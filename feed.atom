<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://looperxx.github.io/ArxivDaily/index.html</id>
    <title>ArxivDaily</title>
    <updated>2021-06-08T02:20:28.223Z</updated>
    <generator>osmosfeed 1.10.2</generator>
    <link rel="alternate" href="https://looperxx.github.io/ArxivDaily/index.html"/>
    <link rel="self" href="https://looperxx.github.io/ArxivDaily/feed.atom"/>
    <entry>
        <title type="html"><![CDATA[Fine-grained Angular Contrastive Learning with Coarse Labels. (arXiv:2012.03515v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.03515</id>
        <link href="http://arxiv.org/abs/2012.03515"/>
        <updated>2021-06-08T02:20:28.134Z</updated>
        <summary type="html"><![CDATA[Few-shot learning methods offer pre-training techniques optimized for easier
later adaptation of the model to new classes (unseen during training) using one
or a few examples. This adaptivity to unseen classes is especially important
for many practical applications where the pre-trained label space cannot remain
fixed for effective use and the model needs to be "specialized" to support new
categories on the fly. One particularly interesting scenario, essentially
overlooked by the few-shot literature, is Coarse-to-Fine Few-Shot (C2FS), where
the training classes (e.g. animals) are of much `coarser granularity' than the
target (test) classes (e.g. breeds). A very practical example of C2FS is when
the target classes are sub-classes of the training classes. Intuitively, it is
especially challenging as (both regular and few-shot) supervised pre-training
tends to learn to ignore intra-class variability which is essential for
separating sub-classes. In this paper, we introduce a novel 'Angular
normalization' module that allows to effectively combine supervised and
self-supervised contrastive pre-training to approach the proposed C2FS task,
demonstrating significant gains in a broad study over multiple baselines and
datasets. We hope that this work will help to pave the way for future research
on this new, challenging, and very practical topic of C2FS classification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bukchin_G/0/1/0/all/0/1"&gt;Guy Bukchin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schwartz_E/0/1/0/all/0/1"&gt;Eli Schwartz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saenko_K/0/1/0/all/0/1"&gt;Kate Saenko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shahar_O/0/1/0/all/0/1"&gt;Ori Shahar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feris_R/0/1/0/all/0/1"&gt;Rogerio Feris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Giryes_R/0/1/0/all/0/1"&gt;Raja Giryes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karlinsky_L/0/1/0/all/0/1"&gt;Leonid Karlinsky&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Relation Alignment for Calibrated Cross-modal Retrieval. (arXiv:2105.13868v2 [cs.CL] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2105.13868</id>
        <link href="http://arxiv.org/abs/2105.13868"/>
        <updated>2021-06-08T02:20:28.103Z</updated>
        <summary type="html"><![CDATA[Despite the achievements of large-scale multimodal pre-training approaches,
cross-modal retrieval, e.g., image-text retrieval, remains a challenging task.
To bridge the semantic gap between the two modalities, previous studies mainly
focus on word-region alignment at the object level, lacking the matching
between the linguistic relation among the words and the visual relation among
the regions. The neglect of such relation consistency impairs the
contextualized representation of image-text pairs and hinders the model
performance and the interpretability. In this paper, we first propose a novel
metric, Intra-modal Self-attention Distance (ISD), to quantify the relation
consistency by measuring the semantic distance between linguistic and visual
relations. In response, we present Inter-modal Alignment on Intra-modal
Self-attentions (IAIS), a regularized training method to optimize the ISD and
calibrate intra-modal self-attentions from the two modalities mutually via
inter-modal alignment. The IAIS regularizer boosts the performance of
prevailing models on Flickr30k and MS COCO datasets by a considerable margin,
which demonstrates the superiority of our approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1"&gt;Shuhuai Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1"&gt;Junyang Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1"&gt;Guangxiang Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Men_R/0/1/0/all/0/1"&gt;Rui Men&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_A/0/1/0/all/0/1"&gt;An Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jingren Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1"&gt;Xu Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Hongxia Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Escaping Saddle Points Faster with Stochastic Momentum. (arXiv:2106.02985v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02985</id>
        <link href="http://arxiv.org/abs/2106.02985"/>
        <updated>2021-06-08T02:20:28.095Z</updated>
        <summary type="html"><![CDATA[Stochastic gradient descent (SGD) with stochastic momentum is popular in
nonconvex stochastic optimization and particularly for the training of deep
neural networks. In standard SGD, parameters are updated by improving along the
path of the gradient at the current iterate on a batch of examples, where the
addition of a ``momentum'' term biases the update in the direction of the
previous change in parameters. In non-stochastic convex optimization one can
show that a momentum adjustment provably reduces convergence time in many
settings, yet such results have been elusive in the stochastic and non-convex
settings. At the same time, a widely-observed empirical phenomenon is that in
training deep networks stochastic momentum appears to significantly improve
convergence time, variants of it have flourished in the development of other
popular update methods, e.g. ADAM [KB15], AMSGrad [RKK18], etc. Yet theoretical
justification for the use of stochastic momentum has remained a significant
open question. In this paper we propose an answer: stochastic momentum improves
deep network training because it modifies SGD to escape saddle points faster
and, consequently, to more quickly find a second order stationary point. Our
theoretical results also shed light on the related question of how to choose
the ideal momentum parameter--our analysis suggests that $\beta \in [0,1)$
should be large (close to 1), which comports with empirical findings. We also
provide experimental findings that further validate these conclusions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jun-Kun Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1"&gt;Chi-Heng Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abernethy_J/0/1/0/all/0/1"&gt;Jacob Abernethy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Manipulating SGD with Data Ordering Attacks. (arXiv:2104.09667v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.09667</id>
        <link href="http://arxiv.org/abs/2104.09667"/>
        <updated>2021-06-08T02:20:28.089Z</updated>
        <summary type="html"><![CDATA[Machine learning is vulnerable to a wide variety of attacks. It is now well
understood that by changing the underlying data distribution, an adversary can
poison the model trained with it or introduce backdoors. In this paper we
present a novel class of training-time attacks that require no changes to the
underlying dataset or model architecture, but instead only change the order in
which data are supplied to the model. In particular, we find that the attacker
can either prevent the model from learning, or poison it to learn behaviours
specified by the attacker. Furthermore, we find that even a single
adversarially-ordered epoch can be enough to slow down model learning, or even
to reset all of the learning progress. Indeed, the attacks presented here are
not specific to the model or dataset, but rather target the stochastic nature
of modern learning procedures. We extensively evaluate our attacks on computer
vision and natural language benchmarks to find that the adversary can disrupt
model training and even introduce backdoors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shumailov_I/0/1/0/all/0/1"&gt;Ilia Shumailov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shumaylov_Z/0/1/0/all/0/1"&gt;Zakhar Shumaylov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kazhdan_D/0/1/0/all/0/1"&gt;Dmitry Kazhdan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yiren Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Papernot_N/0/1/0/all/0/1"&gt;Nicolas Papernot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Erdogdu_M/0/1/0/all/0/1"&gt;Murat A. Erdogdu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anderson_R/0/1/0/all/0/1"&gt;Ross Anderson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Points2Polygons: Context-Based Segmentation from Weak Labels Using Adversarial Networks. (arXiv:2106.02804v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02804</id>
        <link href="http://arxiv.org/abs/2106.02804"/>
        <updated>2021-06-08T02:20:28.082Z</updated>
        <summary type="html"><![CDATA[In applied image segmentation tasks, the ability to provide numerous and
precise labels for training is paramount to the accuracy of the model at
inference time. However, this overhead is often neglected, and recently
proposed segmentation architectures rely heavily on the availability and
fidelity of ground truth labels to achieve state-of-the-art accuracies. Failure
to acknowledge the difficulty in creating adequate ground truths can lead to an
over-reliance on pre-trained models or a lack of adoption in real-world
applications. We introduce Points2Polygons (P2P), a model which makes use of
contextual metric learning techniques that directly addresses this problem.
Points2Polygons performs well against existing fully-supervised segmentation
baselines with limited training data, despite using lightweight segmentation
models (U-Net with a ResNet18 backbone) and having access to only weak labels
in the form of object centroids and no pre-training. We demonstrate this on
several different small but non-trivial datasets. We show that metric learning
using contextual data provides key insights for self-supervised tasks in
general, and allow segmentation models to easily generalize across
traditionally label-intensive domains in computer vision.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_K/0/1/0/all/0/1"&gt;Kuai Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frank_H/0/1/0/all/0/1"&gt;Hakeem Frank&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wilson_D/0/1/0/all/0/1"&gt;Daniel Wilson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SketchGen: Generating Constrained CAD Sketches. (arXiv:2106.02711v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02711</id>
        <link href="http://arxiv.org/abs/2106.02711"/>
        <updated>2021-06-08T02:20:28.076Z</updated>
        <summary type="html"><![CDATA[Computer-aided design (CAD) is the most widely used modeling approach for
technical design. The typical starting point in these designs is 2D sketches
which can later be extruded and combined to obtain complex three-dimensional
assemblies. Such sketches are typically composed of parametric primitives, such
as points, lines, and circular arcs, augmented with geometric constraints
linking the primitives, such as coincidence, parallelism, or orthogonality.
Sketches can be represented as graphs, with the primitives as nodes and the
constraints as edges. Training a model to automatically generate CAD sketches
can enable several novel workflows, but is challenging due to the complexity of
the graphs and the heterogeneity of the primitives and constraints. In
particular, each type of primitive and constraint may require a record of
different size and parameter types. We propose SketchGen as a generative model
based on a transformer architecture to address the heterogeneity problem by
carefully designing a sequential language for the primitives and constraints
that allows distinguishing between different primitive or constraint types and
their parameters, while encouraging our model to re-use information across
related parameters, encoding shared structure. A particular highlight of our
work is the ability to produce primitives linked via constraints that enables
the final output to be further regularized via a constraint solver. We evaluate
our model by demonstrating constraint prediction for given sets of primitives
and full sketch generation from scratch, showing that our approach
significantly out performs the state-of-the-art in CAD sketch generation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Para_W/0/1/0/all/0/1"&gt;Wamiq Reyaz Para&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhat_S/0/1/0/all/0/1"&gt;Shariq Farooq Bhat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guerrero_P/0/1/0/all/0/1"&gt;Paul Guerrero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kelly_T/0/1/0/all/0/1"&gt;Tom Kelly&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mitra_N/0/1/0/all/0/1"&gt;Niloy Mitra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guibas_L/0/1/0/all/0/1"&gt;Leonidas Guibas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wonka_P/0/1/0/all/0/1"&gt;Peter Wonka&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Meta-Learning with Fewer Tasks through Task Interpolation. (arXiv:2106.02695v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02695</id>
        <link href="http://arxiv.org/abs/2106.02695"/>
        <updated>2021-06-08T02:20:28.057Z</updated>
        <summary type="html"><![CDATA[Meta-learning enables algorithms to quickly learn a newly encountered task
with just a few labeled examples by transferring previously learned knowledge.
However, the bottleneck of current meta-learning algorithms is the requirement
of a large number of meta-training tasks, which may not be accessible in
real-world scenarios. To address the challenge that available tasks may not
densely sample the space of tasks, we propose to augment the task set through
interpolation. By meta-learning with task interpolation (MLTI), our approach
effectively generates additional tasks by randomly sampling a pair of tasks and
interpolating the corresponding features and labels. Under both gradient-based
and metric-based meta-learning settings, our theoretical analysis shows MLTI
corresponds to a data-adaptive meta-regularization and further improves the
generalization. Empirically, in our experiments on eight datasets from diverse
domains including image recognition, pose prediction, molecule property
prediction, and medical image classification, we find that the proposed general
MLTI framework is compatible with representative meta-learning algorithms and
consistently outperforms other state-of-the-art strategies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yao_H/0/1/0/all/0/1"&gt;Huaxiu Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Linjun Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1"&gt;Chelsea Finn&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Classification of Very Large Images with Tiny Objects. (arXiv:2106.02694v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02694</id>
        <link href="http://arxiv.org/abs/2106.02694"/>
        <updated>2021-06-08T02:20:28.051Z</updated>
        <summary type="html"><![CDATA[An increasing number of applications in the computer vision domain,
specially, in medical imaging and remote sensing, are challenging when the goal
is to classify very large images with tiny objects. More specifically, these
type of classification tasks face two key challenges: $i$) the size of the
input image in the target dataset is usually in the order of megapixels,
however, existing deep architectures do not easily operate on such big images
due to memory constraints, consequently, we seek a memory-efficient method to
process these images; and $ii$) only a small fraction of the input images are
informative of the label of interest, resulting in low region of interest (ROI)
to image ratio. However, most of the current convolutional neural networks
(CNNs) are designed for image classification datasets that have relatively
large ROIs and small image size (sub-megapixel). Existing approaches have
addressed these two challenges in isolation. We present an end-to-end CNN model
termed Zoom-In network that leverages hierarchical attention sampling for
classification of large images with tiny objects using a single GPU. We
evaluate our method on two large-image datasets and one gigapixel dataset.
Experimental results show that our model achieves higher accuracy than existing
methods while requiring less computing resources.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kong_F/0/1/0/all/0/1"&gt;Fanjie Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Henao_R/0/1/0/all/0/1"&gt;Ricardo Henao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Supercharging Imbalanced Data Learning With Energy-based Contrastive Representation Transfer. (arXiv:2011.12454v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.12454</id>
        <link href="http://arxiv.org/abs/2011.12454"/>
        <updated>2021-06-08T02:20:28.044Z</updated>
        <summary type="html"><![CDATA[Dealing with severe class imbalance poses a major challenge for real-world
applications, especially when the accurate classification and generalization of
minority classes is of primary interest. In computer vision, learning from long
tailed datasets is a recurring theme, especially for natural image datasets.
While existing solutions mostly appeal to sampling or weighting adjustments to
alleviate the pathological imbalance, or imposing inductive bias to prioritize
non-spurious associations, we take novel perspectives to promote sample
efficiency and model generalization based on the invariance principles of
causality. Our proposal posits a meta-distributional scenario, where the data
generating mechanism is invariant across the label-conditional feature
distributions. Such causal assumption enables efficient knowledge transfer from
the dominant classes to their under-represented counterparts, even if the
respective feature distributions show apparent disparities. This allows us to
leverage a causal data inflation procedure to enlarge the representation of
minority classes. Our development is orthogonal to the existing extreme
classification techniques thus can be seamlessly integrated. The utility of our
proposal is validated with an extensive set of synthetic and real-world
computer vision tasks against SOTA solutions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xiu_Z/0/1/0/all/0/1"&gt;Zidi Xiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Junya Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Henao_R/0/1/0/all/0/1"&gt;Ricardo Henao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldstein_B/0/1/0/all/0/1"&gt;Benjamin Goldstein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carin_L/0/1/0/all/0/1"&gt;Lawrence Carin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_C/0/1/0/all/0/1"&gt;Chenyang Tao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AOSLO-net: A deep learning-based method for automatic segmentation of retinal microaneurysms from adaptive optics scanning laser ophthalmoscope images. (arXiv:2106.02800v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.02800</id>
        <link href="http://arxiv.org/abs/2106.02800"/>
        <updated>2021-06-08T02:20:28.038Z</updated>
        <summary type="html"><![CDATA[Adaptive optics scanning laser ophthalmoscopy (AOSLO) provides real-time
retinal images with high resolution down to 2 $\mu m$. This technique enables
detection of the morphologies of individual microaneurysms (MAs), which are one
of the earliest signs of diabetic retinopathy (DR), a frequent complication of
diabetes that can lead to visual impairment and blindness. In contrast to
previous automatic models developed for MA detection on standard fundus
photographs, currently there is no high throughput image protocol available for
automatic analysis of AOSLO photographs. To address this urgency, we introduce
AOSLO-net, a deep neural network framework with customized training policy,
including preprocessing, data augmentation and transfer learning, to
automatically segment MAs from AOSLO images. We evaluate the performance of
AOSLO-net using 87 DR AOSLO images demonstrating very accurate MA detection and
segmentation, leading to correct MA morphological classification, while
outperforming the state-of-the-art both in accuracy and cost.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Qian Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sampani_K/0/1/0/all/0/1"&gt;Konstantina Sampani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xu_M/0/1/0/all/0/1"&gt;Mengjia Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cai_S/0/1/0/all/0/1"&gt;Shengze Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Deng_Y/0/1/0/all/0/1"&gt;Yixiang Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1"&gt;He Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sun_J/0/1/0/all/0/1"&gt;Jennifer K. Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Karniadakis_G/0/1/0/all/0/1"&gt;George Em Karniadakis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Review of Machine Learning Classification Using Quantum Annealing for Real-world Applications. (arXiv:2106.02964v1 [quant-ph])]]></title>
        <id>http://arxiv.org/abs/2106.02964</id>
        <link href="http://arxiv.org/abs/2106.02964"/>
        <updated>2021-06-08T02:20:28.031Z</updated>
        <summary type="html"><![CDATA[Optimizing the training of a machine learning pipeline helps in reducing
training costs and improving model performance. One such optimizing strategy is
quantum annealing, which is an emerging computing paradigm that has shown
potential in optimizing the training of a machine learning model. The
implementation of a physical quantum annealer has been realized by D-Wave
systems and is available to the research community for experiments. Recent
experimental results on a variety of machine learning applications using
quantum annealing have shown interesting results where the performance of
classical machine learning techniques is limited by limited training data and
high dimensional features. This article explores the application of D-Wave's
quantum annealer for optimizing machine learning pipelines for real-world
classification problems. We review the application domains on which a physical
quantum annealer has been used to train machine learning classifiers. We
discuss and analyze the experiments performed on the D-Wave quantum annealer
for applications such as image recognition, remote sensing imagery,
computational biology, and particle physics. We discuss the possible advantages
and the problems for which quantum annealing is likely to be advantageous over
classical computation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Nath_R/0/1/0/all/0/1"&gt;Rajdeep Kumar Nath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Thapliyal_H/0/1/0/all/0/1"&gt;Himanshu Thapliyal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Humble_T/0/1/0/all/0/1"&gt;Travis S. Humble&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TinyTL: Reduce Activations, Not Trainable Parameters for Efficient On-Device Learning. (arXiv:2007.11622v5 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.11622</id>
        <link href="http://arxiv.org/abs/2007.11622"/>
        <updated>2021-06-08T02:20:28.025Z</updated>
        <summary type="html"><![CDATA[On-device learning enables edge devices to continually adapt the AI models to
new data, which requires a small memory footprint to fit the tight memory
constraint of edge devices. Existing work solves this problem by reducing the
number of trainable parameters. However, this doesn't directly translate to
memory saving since the major bottleneck is the activations, not parameters. In
this work, we present Tiny-Transfer-Learning (TinyTL) for memory-efficient
on-device learning. TinyTL freezes the weights while only learns the bias
modules, thus no need to store the intermediate activations. To maintain the
adaptation capacity, we introduce a new memory-efficient bias module, the lite
residual module, to refine the feature extractor by learning small residual
feature maps adding only 3.8% memory overhead. Extensive experiments show that
TinyTL significantly saves the memory (up to 6.5x) with little accuracy loss
compared to fine-tuning the full network. Compared to fine-tuning the last
layer, TinyTL provides significant accuracy improvements (up to 34.1%) with
little memory overhead. Furthermore, combined with feature extractor
adaptation, TinyTL provides 7.3-12.9x memory saving without sacrificing
accuracy compared to fine-tuning the full Inception-V3.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1"&gt;Han Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gan_C/0/1/0/all/0/1"&gt;Chuang Gan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1"&gt;Ligeng Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1"&gt;Song Han&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep learning enables reference-free isotropic super-resolution for volumetric fluorescence microscopy. (arXiv:2104.09435v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.09435</id>
        <link href="http://arxiv.org/abs/2104.09435"/>
        <updated>2021-06-08T02:20:28.005Z</updated>
        <summary type="html"><![CDATA[Volumetric imaging by fluorescence microscopy is often limited by anisotropic
spatial resolution from inferior axial resolution compared to the lateral
resolution. To address this problem, here we present a deep-learning-enabled
unsupervised super-resolution technique that enhances anisotropic images in
volumetric fluorescence microscopy. In contrast to the existing deep learning
approaches that require matched high-resolution target volume images, our
method greatly reduces the effort to put into practice as the training of a
network requires as little as a single 3D image stack, without a priori
knowledge of the image formation process, registration of training data, or
separate acquisition of target data. This is achieved based on the optimal
transport driven cycle-consistent generative adversarial network that learns
from an unpaired matching between high-resolution 2D images in lateral image
plane and low-resolution 2D images in the other planes. Using fluorescence
confocal microscopy and light-sheet microscopy, we demonstrate that the trained
network not only enhances axial resolution, but also restores suppressed visual
details between the imaging planes and removes imaging artifacts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Park_H/0/1/0/all/0/1"&gt;Hyoungjun Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Na_M/0/1/0/all/0/1"&gt;Myeongsu Na&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1"&gt;Bumju Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1"&gt;Soohyun Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1"&gt;Ki Hean Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1"&gt;Sunghoe Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1"&gt;Jong Chul Ye&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lifelong Learning of Hate Speech Classification on Social Media. (arXiv:2106.02821v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02821</id>
        <link href="http://arxiv.org/abs/2106.02821"/>
        <updated>2021-06-08T02:20:27.999Z</updated>
        <summary type="html"><![CDATA[Existing work on automated hate speech classification assumes that the
dataset is fixed and the classes are pre-defined. However, the amount of data
in social media increases every day, and the hot topics changes rapidly,
requiring the classifiers to be able to continuously adapt to new data without
forgetting the previously learned knowledge. This ability, referred to as
lifelong learning, is crucial for the real-word application of hate speech
classifiers in social media. In this work, we propose lifelong learning of hate
speech classification on social media. To alleviate catastrophic forgetting, we
propose to use Variational Representation Learning (VRL) along with a memory
module based on LB-SOINN (Load-Balancing Self-Organizing Incremental Neural
Network). Experimentally, we show that combining variational representation
learning and the LB-SOINN memory module achieves better performance than the
commonly-used lifelong learning techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qian_J/0/1/0/all/0/1"&gt;Jing Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+ElSherief_M/0/1/0/all/0/1"&gt;Mai ElSherief&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1"&gt;Xifeng Yan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GPSfM: Global Projective SFM Using Algebraic Constraints on Multi-View Fundamental Matrices. (arXiv:1812.00426v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1812.00426</id>
        <link href="http://arxiv.org/abs/1812.00426"/>
        <updated>2021-06-08T02:20:27.993Z</updated>
        <summary type="html"><![CDATA[This paper addresses the problem of recovering projective camera matrices
from collections of fundamental matrices in multiview settings. We make two
main contributions. First, given ${n \choose 2}$ fundamental matrices computed
for $n$ images, we provide a complete algebraic characterization in the form of
conditions that are both necessary and sufficient to enabling the recovery of
camera matrices. These conditions are based on arranging the fundamental
matrices as blocks in a single matrix, called the $n$-view fundamental matrix,
and characterizing this matrix in terms of the signs of its eigenvalues and
rank structures. Secondly, we propose a concrete algorithm for projective
structure-from-motion that utilizes this characterization. Given a complete or
partial collection of measured fundamental matrices, our method seeks camera
matrices that minimize a global algebraic error for the measured fundamental
matrices. In contrast to existing methods, our optimization, without any
initialization, produces a consistent set of fundamental matrices that
corresponds to a unique set of cameras (up to a choice of projective frame).
Our experiments indicate that our method achieves state of the art performance
in both accuracy and running time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kasten_Y/0/1/0/all/0/1"&gt;Yoni Kasten&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geifman_A/0/1/0/all/0/1"&gt;Amnon Geifman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Galun_M/0/1/0/all/0/1"&gt;Meirav Galun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Basri_R/0/1/0/all/0/1"&gt;Ronen Basri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Conditional Contrastive Learning: Removing Undesirable Information in Self-Supervised Representations. (arXiv:2106.02866v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02866</id>
        <link href="http://arxiv.org/abs/2106.02866"/>
        <updated>2021-06-08T02:20:27.986Z</updated>
        <summary type="html"><![CDATA[Self-supervised learning is a form of unsupervised learning that leverages
rich information in data to learn representations. However, data sometimes
contains certain information that may be undesirable for downstream tasks. For
instance, gender information may lead to biased decisions on many
gender-irrelevant tasks. In this paper, we develop conditional contrastive
learning to remove undesirable information in self-supervised representations.
To remove the effect of the undesirable variable, our proposed approach
conditions on the undesirable variable (i.e., by fixing the variations of it)
during the contrastive learning process. In particular, inspired by the
contrastive objective InfoNCE, we introduce Conditional InfoNCE (C-InfoNCE),
and its computationally efficient variant, Weak-Conditional InfoNCE
(WeaC-InfoNCE), for conditional contrastive learning. We demonstrate
empirically that our methods can successfully learn self-supervised
representations for downstream tasks while removing a great level of
information related to the undesirable variables. We study three scenarios,
each with a different type of undesirable variables: task-irrelevant
meta-information for self-supervised speech representation learning, sensitive
attributes for fair representation learning, and domain specification for
multi-domain visual representation learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tsai_Y/0/1/0/all/0/1"&gt;Yao-Hung Hubert Tsai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1"&gt;Martin Q. Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1"&gt;Han Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1"&gt;Kun Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morency_L/0/1/0/all/0/1"&gt;Louis-Philippe Morency&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1"&gt;Ruslan Salakhutdinov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[IPS300+: a Challenging Multimodal Dataset for Intersection Perception System. (arXiv:2106.02781v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02781</id>
        <link href="http://arxiv.org/abs/2106.02781"/>
        <updated>2021-06-08T02:20:27.980Z</updated>
        <summary type="html"><![CDATA[Due to the high complexity and occlusion, insufficient perception in the
crowded urban intersection can be a serious safety risk for both human drivers
and autonomous algorithms, whereas CVIS (Cooperative Vehicle Infrastructure
System) is a proposed solution for full-participants perception in this
scenario. However, the research on roadside multimodal perception is still in
its infancy, and there is no open-source dataset for such scenario.
Accordingly, this paper fills the gap. Through an IPS (Intersection Perception
System) installed at the diagonal of the intersection, this paper proposes a
high-quality multimodal dataset for the intersection perception task. The
center of the experimental intersection covers an area of 3000m2, and the
extended distance reaches 300m, which is typical for CVIS. The first batch of
open-source data includes 14198 frames, and each frame has an average of 319.84
labels, which is 9.6 times larger than the most crowded dataset (H3D dataset in
2019) by now. In order to facilitate further study, this dataset tries to keep
the label documents consistent with the KITTI dataset, and a standardized
benchmark is created for algorithm evaluation. Our dataset is available at:
this http URL]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Huanan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xinyu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jun Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhiwei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1"&gt;Lei Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1"&gt;Shuyue Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1"&gt;Yongqiang Deng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ImGAGN:Imbalanced Network Embedding via Generative Adversarial Graph Networks. (arXiv:2106.02817v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02817</id>
        <link href="http://arxiv.org/abs/2106.02817"/>
        <updated>2021-06-08T02:20:27.959Z</updated>
        <summary type="html"><![CDATA[Imbalanced classification on graphs is ubiquitous yet challenging in many
real-world applications, such as fraudulent node detection. Recently, graph
neural networks (GNNs) have shown promising performance on many network
analysis tasks. However, most existing GNNs have almost exclusively focused on
the balanced networks, and would get unappealing performance on the imbalanced
networks. To bridge this gap, in this paper, we present a generative
adversarial graph network model, called ImGAGN to address the imbalanced
classification problem on graphs. It introduces a novel generator for graph
structure data, named GraphGenerator, which can simulate both the minority
class nodes' attribute distribution and network topological structure
distribution by generating a set of synthetic minority nodes such that the
number of nodes in different classes can be balanced. Then a graph
convolutional network (GCN) discriminator is trained to discriminate between
real nodes and fake (i.e., generated) nodes, and also between minority nodes
and majority nodes on the synthetic balanced network. To validate the
effectiveness of the proposed method, extensive experiments are conducted on
four real-world imbalanced network datasets. Experimental results demonstrate
that the proposed method ImGAGN outperforms state-of-the-art algorithms for
semi-supervised imbalanced node classification task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qu_L/0/1/0/all/0/1"&gt;Liang Qu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1"&gt;Huaisheng Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_R/0/1/0/all/0/1"&gt;Ruiqi Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1"&gt;Yuhui Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1"&gt;Hongzhi Yin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Discovery, Control, and Disentanglement of Semantic Attributes with Applications to Anomaly Detection. (arXiv:2002.11169v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.11169</id>
        <link href="http://arxiv.org/abs/2002.11169"/>
        <updated>2021-06-08T02:20:27.950Z</updated>
        <summary type="html"><![CDATA[Our work focuses on unsupervised and generative methods that address the
following goals: (a) learning unsupervised generative representations that
discover latent factors controlling image semantic attributes, (b) studying how
this ability to control attributes formally relates to the issue of latent
factor disentanglement, clarifying related but dissimilar concepts that had
been confounded in the past, and (c) developing anomaly detection methods that
leverage representations learned in (a). For (a), we propose a network
architecture that exploits the combination of multiscale generative models with
mutual information (MI) maximization. For (b), we derive an analytical result
(Lemma 1) that brings clarity to two related but distinct concepts: the ability
of generative networks to control semantic attributes of images they generate,
resulting from MI maximization, and the ability to disentangle latent space
representations, obtained via total correlation minimization. More
specifically, we demonstrate that maximizing semantic attribute control
encourages disentanglement of latent factors. Using Lemma 1 and adopting MI in
our loss function, we then show empirically that, for image generation tasks,
the proposed approach exhibits superior performance as measured in the quality
and disentanglement trade space, when compared to other state of the art
methods, with quality assessed via the Frechet Inception Distance (FID), and
disentanglement via mutual information gap. For (c), we design several systems
for anomaly detection exploiting representations learned in (a), and
demonstrate their performance benefits when compared to state-of-the-art
generative and discriminative algorithms. The above contributions in
representation learning have potential applications in addressing other
important problems in computer vision, such as bias and privacy in AI.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Paul_W/0/1/0/all/0/1"&gt;William Paul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_I/0/1/0/all/0/1"&gt;I-Jeng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alajaji_F/0/1/0/all/0/1"&gt;Fady Alajaji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Burlina_P/0/1/0/all/0/1"&gt;Philippe Burlina&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Visual communication of object concepts at different levels of abstraction. (arXiv:2106.02775v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02775</id>
        <link href="http://arxiv.org/abs/2106.02775"/>
        <updated>2021-06-08T02:20:27.943Z</updated>
        <summary type="html"><![CDATA[People can produce drawings of specific entities (e.g., Garfield), as well as
general categories (e.g., "cat"). What explains this ability to produce such
varied drawings of even highly familiar object concepts? We hypothesized that
drawing objects at different levels of abstraction depends on both sensory
information and representational goals, such that drawings intended to portray
a recently seen object preserve more detail than those intended to represent a
category. Participants drew objects cued either with a photo or a category
label. For each cue type, half the participants aimed to draw a specific
exemplar; the other half aimed to draw the category. We found that label-cued
category drawings were the most recognizable at the basic level, whereas
photo-cued exemplar drawings were the least recognizable. Together, these
findings highlight the importance of task context for explaining how people use
drawings to communicate visual concepts in different ways.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Justin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1"&gt;Judith E. Fan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generative Replay-based Continual Zero-Shot Learning. (arXiv:2101.08894v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.08894</id>
        <link href="http://arxiv.org/abs/2101.08894"/>
        <updated>2021-06-08T02:20:27.934Z</updated>
        <summary type="html"><![CDATA[Zero-shot learning is a new paradigm to classify objects from classes that
are not available at training time. Zero-shot learning (ZSL) methods have
attracted considerable attention in recent years because of their ability to
classify unseen/novel class examples. Most of the existing approaches on ZSL
works when all the samples from seen classes are available to train the model,
which does not suit real life. In this paper, we tackle this hindrance by
developing a generative replay-based continual ZSL (GRCZSL). The proposed
method endows traditional ZSL to learn from streaming data and acquire new
knowledge without forgetting the previous tasks' gained experience. We handle
catastrophic forgetting in GRCZSL by replaying the synthetic samples of seen
classes, which have appeared in the earlier tasks. These synthetic samples are
synthesized using the trained conditional variational autoencoder (VAE) over
the immediate past task. Moreover, we only require the current and immediate
previous VAE at any time for training and testing. The proposed GRZSL method is
developed for a single-head setting of continual learning, simulating a
real-world problem setting. In this setting, task identity is given during
training but unavailable during testing. GRCZSL performance is evaluated on
five benchmark datasets for the generalized setup of ZSL with fixed and dynamic
(incremental class) settings of continual learning. The existing class setting
presented recently in the literature is not suitable for a class-incremental
setting. Therefore, this paper proposes a new setting to address this issue.
Experimental results show that the proposed method significantly outperforms
the baseline and the state-of-the-art method and makes it more suitable for
real-world applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gautam_C/0/1/0/all/0/1"&gt;Chandan Gautam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parameswaran_S/0/1/0/all/0/1"&gt;Sethupathy Parameswaran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mishra_A/0/1/0/all/0/1"&gt;Ashish Mishra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sundaram_S/0/1/0/all/0/1"&gt;Suresh Sundaram&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multimodal Story Generation on Plural Images. (arXiv:2001.10980v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2001.10980</id>
        <link href="http://arxiv.org/abs/2001.10980"/>
        <updated>2021-06-08T02:20:27.927Z</updated>
        <summary type="html"><![CDATA[Traditionally, text generation models take in a sequence of text as input,
and iteratively generate the next most probable word using pre-trained
parameters. In this work, we propose the architecture to use images instead of
text as the input of the text generation model, called StoryGen. In the
architecture, we design a Relational Text Data Generator algorithm that relates
different features from multiple images. The output samples from the model
demonstrate the ability to generate meaningful paragraphs of text containing
the extracted features from the input images. This is an undergraduate project
report. Completed Dec. 2019 at the Cooper Union.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1"&gt;Jing Jiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Feature Flow Regularization: Improving Structured Sparsity in Deep Neural Networks. (arXiv:2106.02914v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02914</id>
        <link href="http://arxiv.org/abs/2106.02914"/>
        <updated>2021-06-08T02:20:27.909Z</updated>
        <summary type="html"><![CDATA[Pruning is a model compression method that removes redundant parameters in
deep neural networks (DNNs) while maintaining accuracy. Most available filter
pruning methods require complex treatments such as iterative pruning, features
statistics/ranking, or additional optimization designs in the training process.
In this paper, we propose a simple and effective regularization strategy from a
new perspective of evolution of features, which we call feature flow
regularization (FFR), for improving structured sparsity and filter pruning in
DNNs. Specifically, FFR imposes controls on the gradient and curvature of
feature flow along the neural network, which implicitly increases the sparsity
of the parameters. The principle behind FFR is that coherent and smooth
evolution of features will lead to an efficient network that avoids redundant
parameters. The high structured sparsity obtained from FFR enables us to prune
filters effectively. Experiments with VGGNets, ResNets on CIFAR-10/100, and
Tiny ImageNet datasets demonstrate that FFR can significantly improve both
unstructured and structured sparsity. Our pruning results in terms of reduction
of parameters and FLOPs are comparable to or even better than those of
state-of-the-art pruning methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yue Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lan_Y/0/1/0/all/0/1"&gt;Yuan Lan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Luchan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiang_Y/0/1/0/all/0/1"&gt;Yang Xiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[i3dLoc: Image-to-range Cross-domain Localization Robust to Inconsistent Environmental Conditions. (arXiv:2105.12883v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.12883</id>
        <link href="http://arxiv.org/abs/2105.12883"/>
        <updated>2021-06-08T02:20:27.903Z</updated>
        <summary type="html"><![CDATA[We present a method for localizing a single camera with respect to a point
cloud map in indoor and outdoor scenes. The problem is challenging because
correspondences of local invariant features are inconsistent across the domains
between image and 3D. The problem is even more challenging as the method must
handle various environmental conditions such as illumination, weather, and
seasonal changes. Our method can match equirectangular images to the 3D range
projections by extracting cross-domain symmetric place descriptors. Our key
insight is to retain condition-invariant 3D geometry features from limited data
samples while eliminating the condition-related features by a designed
Generative Adversarial Network. Based on such features, we further design a
spherical convolution network to learn viewpoint-invariant symmetric place
descriptors. We evaluate our method on extensive self-collected datasets, which
involve \textit{Long-term} (variant appearance conditions),
\textit{Large-scale} (up to $2km$ structure/unstructured environment), and
\textit{Multistory} (four-floor confined space). Our method surpasses other
current state-of-the-arts by achieving around $3$ times higher place retrievals
to inconsistent environments, and above $3$ times accuracy on online
localization. To highlight our method's generalization capabilities, we also
evaluate the recognition across different datasets. With a single trained
model, i3dLoc can demonstrate reliable visual localization in random
conditions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yin_P/0/1/0/all/0/1"&gt;Peng Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1"&gt;Lingyun Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Ji Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choset_H/0/1/0/all/0/1"&gt;Howie Choset&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scherer_S/0/1/0/all/0/1"&gt;Sebastian Scherer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Constrained Generalized Additive 2 Model with Consideration of High-Order Interactions. (arXiv:2106.02836v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02836</id>
        <link href="http://arxiv.org/abs/2106.02836"/>
        <updated>2021-06-08T02:20:27.896Z</updated>
        <summary type="html"><![CDATA[In recent years, machine learning and AI have been introduced in many
industrial fields. In fields such as finance, medicine, and autonomous driving,
where the inference results of a model may have serious consequences, high
interpretability as well as prediction accuracy is required. In this study, we
propose CGA2M+, which is based on the Generalized Additive 2 Model (GA2M) and
differs from it in two major ways. The first is the introduction of
monotonicity. Imposing monotonicity on some functions based on an analyst's
knowledge is expected to improve not only interpretability but also
generalization performance. The second is the introduction of a higher-order
term: given that GA2M considers only second-order interactions, we aim to
balance interpretability and prediction accuracy by introducing a higher-order
term that can capture higher-order interactions. In this way, we can improve
prediction performance without compromising interpretability by applying
learning innovation. Numerical experiments showed that the proposed model has
high predictive performance and interpretability. Furthermore, we confirmed
that generalization performance is improved by introducing monotonicity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Watanabe_A/0/1/0/all/0/1"&gt;Akihisa Watanabe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kuramata_M/0/1/0/all/0/1"&gt;Michiya Kuramata&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Majima_K/0/1/0/all/0/1"&gt;Kaito Majima&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kiyohara_H/0/1/0/all/0/1"&gt;Haruka Kiyohara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kondo_K/0/1/0/all/0/1"&gt;Kensho Kondo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nakata_K/0/1/0/all/0/1"&gt;Kazuhide Nakata&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Soft-Attention Improves Skin Cancer Classification Performance. (arXiv:2105.03358v3 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.03358</id>
        <link href="http://arxiv.org/abs/2105.03358"/>
        <updated>2021-06-08T02:20:27.890Z</updated>
        <summary type="html"><![CDATA[In clinical applications, neural networks must focus on and highlight the
most important parts of an input image. Soft-Attention mechanism enables a
neural network toachieve this goal. This paper investigates the effectiveness
of Soft-Attention in deep neural architectures. The central aim of
Soft-Attention is to boost the value of important features and suppress the
noise-inducing features. We compare the performance of VGG, ResNet,
InceptionResNetv2 and DenseNet architectures with and without the
Soft-Attention mechanism, while classifying skin lesions. The original network
when coupled with Soft-Attention outperforms the baseline[16] by 4.7% while
achieving a precision of 93.7% on HAM10000 dataset [25]. Additionally,
Soft-Attention coupling improves the sensitivity score by 3.8% compared to
baseline[31] and achieves 91.6% on ISIC-2017 dataset [2]. The code is publicly
available at github.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Datta_S/0/1/0/all/0/1"&gt;Soumyya Kanti Datta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shaikh_M/0/1/0/all/0/1"&gt;Mohammad Abuzar Shaikh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Srihari_S/0/1/0/all/0/1"&gt;Sargur N. Srihari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gao_M/0/1/0/all/0/1"&gt;Mingchen Gao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Context-Aware Sparse Deep Coordination Graphs. (arXiv:2106.02886v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02886</id>
        <link href="http://arxiv.org/abs/2106.02886"/>
        <updated>2021-06-08T02:20:27.884Z</updated>
        <summary type="html"><![CDATA[Learning sparse coordination graphs adaptive to the coordination dynamics
among agents is a long-standing problem in cooperative multi-agent learning.
This paper studies this problem by proposing several value-based and
observation-based schemes for learning dynamic topologies and evaluating them
on a new Multi-Agent COordination (MACO) benchmark. The benchmark collects
classic coordination problems in the literature, increases their difficulty,
and classifies them into different types. By analyzing the individual
advantages of each learning scheme on each type of problem and their overall
performance, we propose a novel method using the variance of utility difference
functions to learn context-aware sparse coordination topologies. Moreover, our
method learns action representations that effectively reduce the influence of
utility functions' estimation errors on graph construction. Experiments show
that our method significantly outperforms dense and static topologies across
the MACO and StarCraft II micromanagement benchmark.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1"&gt;Tonghan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_L/0/1/0/all/0/1"&gt;Liang Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_W/0/1/0/all/0/1"&gt;Weijun Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1"&gt;Qianlan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1"&gt;Yang Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chongjie Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Low Budget Active Learning via Wasserstein Distance: An Integer Programming Approach. (arXiv:2106.02968v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02968</id>
        <link href="http://arxiv.org/abs/2106.02968"/>
        <updated>2021-06-08T02:20:27.867Z</updated>
        <summary type="html"><![CDATA[Given restrictions on the availability of data, active learning is the
process of training a model with limited labeled data by selecting a core
subset of an unlabeled data pool to label. Although selecting the most useful
points for training is an optimization problem, the scale of deep learning data
sets forces most selection strategies to employ efficient heuristics. Instead,
we propose a new integer optimization problem for selecting a core set that
minimizes the discrete Wasserstein distance from the unlabeled pool. We
demonstrate that this problem can be tractably solved with a Generalized
Benders Decomposition algorithm. Our strategy requires high-quality latent
features which we obtain by unsupervised learning on the unlabeled pool.
Numerical results on several data sets show that our optimization approach is
competitive with baselines and particularly outperforms them in the low budget
regime where less than one percent of the data set is labeled.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mahmood_R/0/1/0/all/0/1"&gt;Rafid Mahmood&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fidler_S/0/1/0/all/0/1"&gt;Sanja Fidler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Law_M/0/1/0/all/0/1"&gt;Marc T. Law&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PURS: Personalized Unexpected Recommender System for Improving User Satisfaction. (arXiv:2106.02771v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.02771</id>
        <link href="http://arxiv.org/abs/2106.02771"/>
        <updated>2021-06-08T02:20:27.861Z</updated>
        <summary type="html"><![CDATA[Classical recommender system methods typically face the filter bubble problem
when users only receive recommendations of their familiar items, making them
bored and dissatisfied. To address the filter bubble problem, unexpected
recommendations have been proposed to recommend items significantly deviating
from user's prior expectations and thus surprising them by presenting "fresh"
and previously unexplored items to the users. In this paper, we describe a
novel Personalized Unexpected Recommender System (PURS) model that incorporates
unexpectedness into the recommendation process by providing multi-cluster
modeling of user interests in the latent space and personalized unexpectedness
via the self-attention mechanism and via selection of an appropriate unexpected
activation function. Extensive offline experiments on three real-world datasets
illustrate that the proposed PURS model significantly outperforms the
state-of-the-art baseline approaches in terms of both accuracy and
unexpectedness measures. In addition, we conduct an online A/B test at a major
video platform Alibaba-Youku, where our model achieves over 3\% increase in the
average video view per user metric. The proposed model is in the process of
being deployed by the company.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1"&gt;Pan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Que_M/0/1/0/all/0/1"&gt;Maofei Que&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1"&gt;Zhichao Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1"&gt;Yao Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tuzhilin_A/0/1/0/all/0/1"&gt;Alexander Tuzhilin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multimodal Story Generation on Plural Images. (arXiv:2001.10980v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2001.10980</id>
        <link href="http://arxiv.org/abs/2001.10980"/>
        <updated>2021-06-08T02:20:27.854Z</updated>
        <summary type="html"><![CDATA[Traditionally, text generation models take in a sequence of text as input,
and iteratively generate the next most probable word using pre-trained
parameters. In this work, we propose the architecture to use images instead of
text as the input of the text generation model, called StoryGen. In the
architecture, we design a Relational Text Data Generator algorithm that relates
different features from multiple images. The output samples from the model
demonstrate the ability to generate meaningful paragraphs of text containing
the extracted features from the input images. This is an undergraduate project
report. Completed Dec. 2019 at the Cooper Union.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1"&gt;Jing Jiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Sparse Classifiers: Continuous and Mixed Integer Optimization Perspectives. (arXiv:2001.06471v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2001.06471</id>
        <link href="http://arxiv.org/abs/2001.06471"/>
        <updated>2021-06-08T02:20:27.845Z</updated>
        <summary type="html"><![CDATA[We consider a discrete optimization formulation for learning sparse
classifiers, where the outcome depends upon a linear combination of a small
subset of features. Recent work has shown that mixed integer programming (MIP)
can be used to solve (to optimality) $\ell_0$-regularized regression problems
at scales much larger than what was conventionally considered possible. Despite
their usefulness, MIP-based global optimization approaches are significantly
slower compared to the relatively mature algorithms for $\ell_1$-regularization
and heuristics for nonconvex regularized problems. We aim to bridge this gap in
computation times by developing new MIP-based algorithms for
$\ell_0$-regularized classification. We propose two classes of scalable
algorithms: an exact algorithm that can handle $p\approx 50,000$ features in a
few minutes, and approximate algorithms that can address instances with
$p\approx 10^6$ in times comparable to the fast $\ell_1$-based algorithms. Our
exact algorithm is based on the novel idea of \textsl{integrality generation},
which solves the original problem (with $p$ binary variables) via a sequence of
mixed integer programs that involve a small number of binary variables. Our
approximate algorithms are based on coordinate descent and local combinatorial
search. In addition, we present new estimation error bounds for a class of
$\ell_0$-regularized estimators. Experiments on real and synthetic data
demonstrate that our approach leads to models with considerably improved
statistical performance (especially, variable selection) when compared to
competing methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Dedieu_A/0/1/0/all/0/1"&gt;Antoine Dedieu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Hazimeh_H/0/1/0/all/0/1"&gt;Hussein Hazimeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Mazumder_R/0/1/0/all/0/1"&gt;Rahul Mazumder&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Deep Variational Bayesian Framework for Blind Image Deblurring. (arXiv:2106.02884v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.02884</id>
        <link href="http://arxiv.org/abs/2106.02884"/>
        <updated>2021-06-08T02:20:27.839Z</updated>
        <summary type="html"><![CDATA[Blind image deblurring is an important yet very challenging problem in
low-level vision. Traditional optimization based methods generally formulate
this task as a maximum-a-posteriori estimation or variational inference
problem, whose performance highly relies on the handcraft priors for both the
latent image and the blur kernel. In contrast, recent deep learning methods
generally learn, from a large collection of training images, deep neural
networks (DNNs) directly mapping the blurry image to the clean one or to the
blur kernel, paying less attention to the physical degradation process of the
blurry image. In this paper, we present a deep variational Bayesian framework
for blind image deblurring. Under this framework, the posterior of the latent
clean image and blur kernel can be jointly estimated in an amortized inference
fashion with DNNs, and the involved inference DNNs can be trained by fully
considering the physical blur model, together with the supervision of data
driven priors for the clean image and blur kernel, which is naturally led to by
the evidence lower bound objective. Comprehensive experiments are conducted to
substantiate the effectiveness of the proposed framework. The results show that
it can not only achieve a promising performance with relatively simple
networks, but also enhance the performance of existing DNNs for deblurring.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hui Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yue_Z/0/1/0/all/0/1"&gt;Zongsheng Yue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhao_Q/0/1/0/all/0/1"&gt;Qian Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Meng_D/0/1/0/all/0/1"&gt;Deyu Meng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Point Cloud Failure Criterion for Composites using k-Nearest Neighbor Classification. (arXiv:2106.02714v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02714</id>
        <link href="http://arxiv.org/abs/2106.02714"/>
        <updated>2021-06-08T02:20:27.824Z</updated>
        <summary type="html"><![CDATA[Numerous theories of failure have been postulated and implemented in various
commercial programs for composite materials. Even the best theories have had
limited success in predicting damage and failure in validation exercises. In
view of this background, many researchers have started exploring the use of
multiscale modeling to improve the fidelity of the modeling and simulation of
various structural and materials systems. In this paper, a multi-scale modeling
scheme is used to illustrate how a combination of virtual and laboratory
testing programs can be used to generate a point cloud of failure surface data
that can then be queried during finite element analysis at the continuum scale
to ascertain if the onset of failure has occurred. The k-nearest neighbor
(k-NN) classification concept is used to obtain the answer to the query. A
linear, elastic, static finite element example using a unidirectional composite
shows that the framework can be generated and used effectively and efficiently
with the possibility to extend the approach for all types of composite
architectures and behaviors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rajan_S/0/1/0/all/0/1"&gt;Subramaniam Rajan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khaled_B/0/1/0/all/0/1"&gt;Bilal Khaled&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shyamsunder_L/0/1/0/all/0/1"&gt;Loukham Shyamsunder&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Should Graph Convolution Trust Neighbors? A Simple Causal Inference Method. (arXiv:2010.11797v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.11797</id>
        <link href="http://arxiv.org/abs/2010.11797"/>
        <updated>2021-06-08T02:20:26.099Z</updated>
        <summary type="html"><![CDATA[Graph Convolutional Network (GCN) is an emerging technique for information
retrieval (IR) applications. While GCN assumes the homophily property of a
graph, real-world graphs are never perfect: the local structure of a node may
contain discrepancy, e.g., the labels of a node's neighbors could vary. This
pushes us to consider the discrepancy of local structure in GCN modeling.
Existing work approaches this issue by introducing an additional module such as
graph attention, which is expected to learn the contribution of each neighbor.
However, such module may not work reliably as expected, especially when there
lacks supervision signal, e.g., when the labeled data is small. Moreover,
existing methods focus on modeling the nodes in the training data, and never
consider the local structure discrepancy of testing nodes.

This work focuses on the local structure discrepancy issue for testing nodes,
which has received little scrutiny. From a novel perspective of causality, we
investigate whether a GCN should trust the local structure of a testing node
when predicting its label. To this end, we analyze the working mechanism of GCN
with causal graph, estimating the causal effect of a node's local structure for
the prediction. The idea is simple yet effective: given a trained GCN model, we
first intervene the prediction by blocking the graph structure; we then compare
the original prediction with the intervened prediction to assess the causal
effect of the local structure on the prediction. Through this way, we can
eliminate the impact of local structure discrepancy and make more accurate
prediction. Extensive experiments on seven node classification datasets show
that our method effectively enhances the inference stage of GCN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Feng_F/0/1/0/all/0/1"&gt;Fuli Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1"&gt;Weiran Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1"&gt;Xiangnan He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xin_X/0/1/0/all/0/1"&gt;Xin Xin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1"&gt;Qifan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1"&gt;Tat-Seng Chua&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Imitation Learning via Simultaneous Optimization of Policies and Auxiliary Trajectories. (arXiv:2105.03019v2 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.03019</id>
        <link href="http://arxiv.org/abs/2105.03019"/>
        <updated>2021-06-08T02:20:26.066Z</updated>
        <summary type="html"><![CDATA[Imitation learning (IL) is a frequently used approach for data-efficient
policy learning. Many IL methods, such as Dataset Aggregation (DAgger), combat
challenges like distributional shift by interacting with oracular experts.
Unfortunately, assuming access to oracular experts is often unrealistic in
practice; data used in IL frequently comes from offline processes such as
lead-through or teleoperation. In this paper, we present a novel imitation
learning technique called Collocation for Demonstration Encoding (CoDE) that
operates on only a fixed set of trajectory demonstrations. We circumvent
challenges with methods like back-propagation-through-time by introducing an
auxiliary trajectory network, which takes inspiration from collocation
techniques in optimal control. Our method generalizes well and more accurately
reproduces the demonstrated behavior with fewer guiding trajectories when
compared to standard behavioral cloning methods. We present simulation results
on a 7-degree-of-freedom (DoF) robotic manipulator that learns to exhibit
lifting, target-reaching, and obstacle avoidance behaviors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_M/0/1/0/all/0/1"&gt;Mandy Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1"&gt;Anqi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wyk_K/0/1/0/all/0/1"&gt;Karl Van Wyk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dellaert_F/0/1/0/all/0/1"&gt;Frank Dellaert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boots_B/0/1/0/all/0/1"&gt;Byron Boots&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ratliff_N/0/1/0/all/0/1"&gt;Nathan Ratliff&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Privacy-Preserving Kickstarting Deep Reinforcement Learning with Privacy-Aware Learners. (arXiv:2102.09599v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.09599</id>
        <link href="http://arxiv.org/abs/2102.09599"/>
        <updated>2021-06-08T02:20:26.023Z</updated>
        <summary type="html"><![CDATA[Kickstarting deep reinforcement learning algorithms facilitate a
teacher-student relationship among the agents and allow for a well-performing
teacher to share demonstrations with a student to expedite the student's
training. However, despite the known benefits, the demonstrations may contain
sensitive information about the teacher's training data and existing
kickstarting methods do not take any measures to protect it. Therefore, we use
the framework of differential privacy to develop a mechanism that securely
shares the teacher's demonstrations with the student. The mechanism allows for
the teacher to decide upon the accuracy of its demonstrations with respect to
the privacy budget that it consumes, thereby granting the teacher full control
over its data privacy. We then develop a kickstarted deep reinforcement
learning algorithm for the student that is privacy-aware because we calibrate
its objective with the parameters of the teacher's privacy mechanism. The
privacy-aware design of the algorithm makes it possible to kickstart the
student's learning despite the perturbations induced by the privacy mechanism.
From numerical experiments, we highlight three empirical results: (i) the
algorithm succeeds in expediting the student's learning, (ii) the student
converges to a performance level that was not possible without the
demonstrations, and (iii) the student maintains its enhanced performance even
after the teacher stops sharing useful demonstrations due to its privacy budget
constraints.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gohari_P/0/1/0/all/0/1"&gt;Parham Gohari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1"&gt;Bo Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1"&gt;Bo Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hale_M/0/1/0/all/0/1"&gt;Matthew Hale&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Topcu_U/0/1/0/all/0/1"&gt;Ufuk Topcu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hybrid-Attention Guided Network with Multiple Resolution Features for Person Re-Identification. (arXiv:2009.07536v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.07536</id>
        <link href="http://arxiv.org/abs/2009.07536"/>
        <updated>2021-06-08T02:20:26.017Z</updated>
        <summary type="html"><![CDATA[Extracting effective and discriminative features is very important for
addressing the challenging person re-identification (re-ID) task. Prevailing
deep convolutional neural networks (CNNs) usually use high-level features for
identifying pedestrian. However, some essential spatial information resided in
low-level features such as shape, texture and color will be lost when learning
the high-level features, due to extensive padding and pooling operations in the
training stage. In addition, most existing person re-ID methods are mainly
based on hand-craft bounding boxes where images are precisely aligned. It is
unrealistic in practical applications, since the exploited object detection
algorithms often produce inaccurate bounding boxes. This will inevitably
degrade the performance of existing algorithms. To address these problems, we
put forward a novel person re-ID model that fuses high- and low-level
embeddings to reduce the information loss caused in learning high-level
features. Then we divide the fused embedding into several parts and reconnect
them to obtain the global feature and more significant local features, so as to
alleviate the affect caused by the inaccurate bounding boxes. In addition, we
also introduce the spatial and channel attention mechanisms in our model, which
aims to mine more discriminative features related to the target. Finally, we
reconstruct the feature extractor to ensure that our model can obtain more
richer and robust features. Extensive experiments display the superiority of
our approach compared with existing approaches. Our code is available at
https://github.com/libraflower/MutipleFeature-for-PRID.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1"&gt;Guoqing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Junchuan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1"&gt;Yuhui Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yi Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Shengyong Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Enabling Meta-Learning from Target Models. (arXiv:2104.03736v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.03736</id>
        <link href="http://arxiv.org/abs/2104.03736"/>
        <updated>2021-06-08T02:20:25.998Z</updated>
        <summary type="html"><![CDATA[Meta-learning can extract an inductive bias from previous learning experience
and assist the training processes of new tasks. It is often realized through
optimizing a meta-model with the evaluation loss of a series of task-specific
solvers. Most existing algorithms sample non-overlapping $\mathit{support}$
sets and $\mathit{query}$ sets to train and evaluate the solvers respectively
due to simplicity ($\mathcal{S}/\mathcal{Q}$ protocol). However, another
evaluation method that assesses the discrepancy between the solver and a target
model is short of research ($\mathcal{S}/\mathcal{T}$ protocol).
$\mathcal{S}/\mathcal{T}$ protocol has unique advantages such as offering more
informative supervision, but it is computationally expensive. This paper looks
into this special evaluation method and takes a step towards putting it into
practice. We find that with a small ratio of tasks armed with target models,
classic meta-learning algorithms can be improved a lot without consuming many
resources. Furthermore, we empirically verify the effectiveness of
$\mathcal{S}/\mathcal{T}$ protocol in a typical application of meta-learning,
$\mathit{i.e.}$, few-shot learning. In detail, after constructing target models
by fine-tuning the pre-trained network on those hard tasks, we match the
task-specific solvers to target models via knowledge distillation. Experiments
demonstrate the superiority of our proposal.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1"&gt;Su Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1"&gt;Han-Jia Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gan_L/0/1/0/all/0/1"&gt;Le Gan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhan_D/0/1/0/all/0/1"&gt;De-Chuan Zhan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Regret Minimization Experience Replay. (arXiv:2105.07253v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.07253</id>
        <link href="http://arxiv.org/abs/2105.07253"/>
        <updated>2021-06-08T02:20:25.992Z</updated>
        <summary type="html"><![CDATA[In reinforcement learning, experience replay stores past samples for further
reuse. Prioritized sampling is a promising technique to better utilize these
samples. Previous criteria of prioritization include TD error, recentness and
corrective feedback, which are mostly heuristically designed. In this work, we
start from the regret minimization objective, and obtain an optimal
prioritization strategy for Bellman update that can directly maximize the
return of the policy. The theory suggests that data with higher hindsight TD
error, better on-policiness and more accurate Q value should be assigned with
higher weights during sampling. Thus most previous criteria only consider this
strategy partially. We not only provide theoretical justifications for previous
criteria, but also propose two new methods to compute the prioritization
weight, namely ReMERN and ReMERT. ReMERN learns an error network, while ReMERT
exploits the temporal ordering of states. Both methods outperform previous
prioritized sampling algorithms in challenging RL benchmarks, including MuJoCo,
Atari and Meta-World.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xue_Z/0/1/0/all/0/1"&gt;Zhenghai Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xu-Hui Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pang_J/0/1/0/all/0/1"&gt;Jing-Cheng Pang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1"&gt;Shengyi Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1"&gt;Feng Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1"&gt;Yang Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Information Discrepancy in Strategic Learning. (arXiv:2103.01028v3 [cs.GT] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.01028</id>
        <link href="http://arxiv.org/abs/2103.01028"/>
        <updated>2021-06-08T02:20:25.986Z</updated>
        <summary type="html"><![CDATA[We study the effects of information discrepancy across sub-populations on
their ability to simultaneously improve their features in strategic learning
settings. Specifically, we consider a game where a principal deploys a decision
rule in an attempt to optimize the whole population's welfare, and agents
strategically adapt to it to receive better scores. Inspired by real-life
settings, such as loan approvals and college admissions, we remove the typical
assumption made in the strategic learning literature that the decision rule is
fully known to the agents, and focus on settings where it is inaccessible. In
their lack of knowledge, individuals try to infer this rule by learning from
their peers (e.g., friends and acquaintances who previously applied for a
loan), naturally forming groups in the population, each with possibly different
type and level of information about the decision rule. In our equilibrium
analysis, we show that the principal's decision rule optimizing the welfare
across subgroups may cause a surprising negative externality; the true quality
of some of the subgroups can actually deteriorate. On the positive side, we
show that in many natural cases, optimal improvement is guaranteed
simultaneously for all subgroups in equilibrium. We also characterize the
disparity in improvements across subgroups via a measure of their informational
overlap. Finally, we complement our theoretical analysis with experiments on
real-world datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bechavod_Y/0/1/0/all/0/1"&gt;Yahav Bechavod&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Podimata_C/0/1/0/all/0/1"&gt;Chara Podimata&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Zhiwei Steven Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ziani_J/0/1/0/all/0/1"&gt;Juba Ziani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Kernel Smoothing, Mean Shift, and Their Learning Theory with Directional Data. (arXiv:2010.13523v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.13523</id>
        <link href="http://arxiv.org/abs/2010.13523"/>
        <updated>2021-06-08T02:20:25.979Z</updated>
        <summary type="html"><![CDATA[Directional data consist of observations distributed on a (hyper)sphere, and
appear in many applied fields, such as astronomy, ecology, and environmental
science. This paper studies both statistical and computational problems of
kernel smoothing for directional data. We generalize the classical mean shift
algorithm to directional data, which allows us to identify local modes of the
directional kernel density estimator (KDE). The statistical convergence rates
of the directional KDE and its derivatives are derived, and the problem of mode
estimation is examined. We also prove the ascending property of the directional
mean shift algorithm and investigate a general problem of gradient ascent on
the unit hypersphere. To demonstrate the applicability of the algorithm, we
evaluate it as a mode clustering method on both simulated and real-world data
sets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yikun Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yen-Chi Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Representation Matters: Assessing the Importance of Subgroup Allocations in Training Data. (arXiv:2103.03399v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.03399</id>
        <link href="http://arxiv.org/abs/2103.03399"/>
        <updated>2021-06-08T02:20:25.973Z</updated>
        <summary type="html"><![CDATA[Collecting more diverse and representative training data is often touted as a
remedy for the disparate performance of machine learning predictors across
subpopulations. However, a precise framework for understanding how dataset
properties like diversity affect learning outcomes is largely lacking. By
casting data collection as part of the learning process, we demonstrate that
diverse representation in training data is key not only to increasing subgroup
performances, but also to achieving population level objectives. Our analysis
and experiments describe how dataset compositions influence performance and
provide constructive results for using trends in existing data, alongside
domain knowledge, to help guide intentional, objective-aware dataset design.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rolf_E/0/1/0/all/0/1"&gt;Esther Rolf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Worledge_T/0/1/0/all/0/1"&gt;Theodora Worledge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Recht_B/0/1/0/all/0/1"&gt;Benjamin Recht&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jordan_M/0/1/0/all/0/1"&gt;Michael I. Jordan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Principled Simplicial Neural Networks for Trajectory Prediction. (arXiv:2102.10058v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.10058</id>
        <link href="http://arxiv.org/abs/2102.10058"/>
        <updated>2021-06-08T02:20:25.965Z</updated>
        <summary type="html"><![CDATA[We consider the construction of neural network architectures for data on
simplicial complexes. In studying maps on the chain complex of a simplicial
complex, we define three desirable properties of a simplicial neural network
architecture: namely, permutation equivariance, orientation equivariance, and
simplicial awareness. The first two properties respectively account for the
fact that the node indexing and the simplex orientations in a simplicial
complex are arbitrary. The last property encodes the desirable feature that the
output of the neural network depends on the entire simplicial complex and not
on a subset of its dimensions. Based on these properties, we propose a simple
convolutional architecture, rooted in tools from algebraic topology, for the
problem of trajectory prediction, and show that it obeys all three of these
properties when an odd, nonlinear activation function is used. We then
demonstrate the effectiveness of this architecture in extrapolating
trajectories on synthetic and real datasets, with particular emphasis on the
gains in generalizability to unseen trajectories.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Roddenberry_T/0/1/0/all/0/1"&gt;T. Mitchell Roddenberry&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Glaze_N/0/1/0/all/0/1"&gt;Nicholas Glaze&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Segarra_S/0/1/0/all/0/1"&gt;Santiago Segarra&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Model-Agnostic Post-Hoc Adjustment for Balancing Ranking Fairness and Algorithm Utility. (arXiv:2006.08267v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.08267</id>
        <link href="http://arxiv.org/abs/2006.08267"/>
        <updated>2021-06-08T02:20:25.948Z</updated>
        <summary type="html"><![CDATA[Bipartite ranking, which aims to learn a scoring function that ranks positive
individuals higher than negative ones from labeled data, is widely adopted in
various applications where sample prioritization is needed. Recently, there
have been rising concerns on whether the learned scoring function can cause
systematic disparity across different protected groups defined by sensitive
attributes. While there could be trade-off between fairness and performance, in
this paper we propose a model agnostic post-processing framework for balancing
them in the bipartite ranking scenario. Specifically, we maximize a weighted
sum of the utility and fairness by directly adjusting the relative ordering of
samples across groups. By formulating this problem as the identification of an
optimal warping path across different protected groups, we propose a
non-parametric method to search for such an optimal path through a dynamic
programming process. Our method is compatible with various classification
models and applicable to a variety of ranking fairness metrics. Comprehensive
experiments on a suite of benchmark data sets and two real-world patient
electronic health record repositories show that our method can achieve a great
balance between the algorithm utility and ranking fairness. Furthermore, we
experimentally verify the robustness of our method when faced with the fewer
training samples and the difference between training and testing ranking score
distributions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1"&gt;Sen Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_W/0/1/0/all/0/1"&gt;Weishen Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Changshui Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1"&gt;Fei Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Select, Extract and Generate: Neural Keyphrase Generation with Layer-wise Coverage Attention. (arXiv:2008.01739v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.01739</id>
        <link href="http://arxiv.org/abs/2008.01739"/>
        <updated>2021-06-08T02:20:25.941Z</updated>
        <summary type="html"><![CDATA[Natural language processing techniques have demonstrated promising results in
keyphrase generation. However, one of the major challenges in \emph{neural}
keyphrase generation is processing long documents using deep neural networks.
Generally, documents are truncated before given as inputs to neural networks.
Consequently, the models may miss essential points conveyed in the target
document. To overcome this limitation, we propose \emph{SEG-Net}, a neural
keyphrase generation model that is composed of two major components, (1) a
selector that selects the salient sentences in a document and (2) an
extractor-generator that jointly extracts and generates keyphrases from the
selected sentences. SEG-Net uses Transformer, a self-attentive architecture, as
the basic building block with a novel \emph{layer-wise} coverage attention to
summarize most of the points discussed in the document. The experimental
results on seven keyphrase generation benchmarks from scientific and web
documents demonstrate that SEG-Net outperforms the state-of-the-art neural
generative methods by a large margin.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ahmad_W/0/1/0/all/0/1"&gt;Wasi Uddin Ahmad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1"&gt;Xiao Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1"&gt;Soomin Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1"&gt;Kai-Wei Chang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Efficient Representations for Keyword Spotting with Triplet Loss. (arXiv:2101.04792v4 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.04792</id>
        <link href="http://arxiv.org/abs/2101.04792"/>
        <updated>2021-06-08T02:20:25.935Z</updated>
        <summary type="html"><![CDATA[In the past few years, triplet loss-based metric embeddings have become a
de-facto standard for several important computer vision problems, most
no-tably, person reidentification. On the other hand, in the area of speech
recognition the metric embeddings generated by the triplet loss are rarely used
even for classification problems. We fill this gap showing that a combination
of two representation learning techniques: a triplet loss-based embedding and a
variant of kNN for classification instead of cross-entropy loss significantly
(by 26% to 38%) improves the classification accuracy for convolutional networks
on a LibriSpeech-derived LibriWords datasets. To do so, we propose a novel
phonetic similarity based triplet mining approach. We also improve the current
best published SOTA for Google Speech Commands dataset V1 10+2 -class
classification by about 34%, achieving 98.55% accuracy, V2 10+2-class
classification by about 20%, achieving 98.37% accuracy, and V2 35-class
classification by over 50%, achieving 97.0% accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Vygon_R/0/1/0/all/0/1"&gt;Roman Vygon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mikhaylovskiy_N/0/1/0/all/0/1"&gt;Nikolay Mikhaylovskiy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Federated Reconstruction: Partially Local Federated Learning. (arXiv:2102.03448v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.03448</id>
        <link href="http://arxiv.org/abs/2102.03448"/>
        <updated>2021-06-08T02:20:25.930Z</updated>
        <summary type="html"><![CDATA[Personalization methods in federated learning aim to balance the benefits of
federated and local training for data availability, communication cost, and
robustness to client heterogeneity. Approaches that require clients to
communicate all model parameters can be undesirable due to privacy and
communication constraints. Other approaches require always-available or
stateful clients, impractical in large-scale cross-device settings. We
introduce Federated Reconstruction, the first model-agnostic framework for
partially local federated learning suitable for training and inference at
scale. We motivate the framework via a connection to model-agnostic meta
learning, empirically demonstrate its performance over existing approaches for
collaborative filtering and next word prediction, and release an open-source
library for evaluating approaches in this setting. We also describe the
successful deployment of this approach at scale for federated collaborative
filtering in a mobile keyboard application.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Singhal_K/0/1/0/all/0/1"&gt;Karan Singhal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sidahmed_H/0/1/0/all/0/1"&gt;Hakim Sidahmed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Garrett_Z/0/1/0/all/0/1"&gt;Zachary Garrett&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1"&gt;Shanshan Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rush_K/0/1/0/all/0/1"&gt;Keith Rush&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prakash_S/0/1/0/all/0/1"&gt;Sushant Prakash&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Modern Hopfield Networks for Few- and Zero-Shot Reaction Template Prediction. (arXiv:2104.03279v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.03279</id>
        <link href="http://arxiv.org/abs/2104.03279"/>
        <updated>2021-06-08T02:20:25.923Z</updated>
        <summary type="html"><![CDATA[Finding synthesis routes for molecules of interest is an essential step in
the discovery of new drugs and materials. To find such routes,
computer-assisted synthesis planning (CASP) methods are employed which rely on
a model of chemical reactivity. In this study, we model single-step
retrosynthesis in a template-based approach using modern Hopfield networks
(MHNs). We adapt MHNs to associate different modalities, reaction templates and
molecules, which allows the model to leverage structural information about
reaction templates. This approach significantly improves the performance of
template relevance prediction, especially for templates with few or zero
training examples. With inference speed several times faster than that of
baseline methods, we improve predictive performance for top-k exact match
accuracy for $\mathrm{k}\geq5$ in the retrosynthesis benchmark USPTO-50k.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Seidl_P/0/1/0/all/0/1"&gt;Philipp Seidl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Renz_P/0/1/0/all/0/1"&gt;Philipp Renz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dyubankova_N/0/1/0/all/0/1"&gt;Natalia Dyubankova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neves_P/0/1/0/all/0/1"&gt;Paulo Neves&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Verhoeven_J/0/1/0/all/0/1"&gt;Jonas Verhoeven&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Segler_M/0/1/0/all/0/1"&gt;Marwin Segler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wegner_J/0/1/0/all/0/1"&gt;J&amp;#xf6;rg K. Wegner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hochreiter_S/0/1/0/all/0/1"&gt;Sepp Hochreiter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Klambauer_G/0/1/0/all/0/1"&gt;G&amp;#xfc;nter Klambauer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Positive-Negative Momentum: Manipulating Stochastic Gradient Noise to Improve Generalization. (arXiv:2103.17182v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.17182</id>
        <link href="http://arxiv.org/abs/2103.17182"/>
        <updated>2021-06-08T02:20:25.906Z</updated>
        <summary type="html"><![CDATA[It is well-known that stochastic gradient noise (SGN) acts as implicit
regularization for deep learning and is essentially important for both
optimization and generalization of deep networks. Some works attempted to
artificially simulate SGN by injecting random noise to improve deep learning.
However, it turned out that the injected simple random noise cannot work as
well as SGN, which is anisotropic and parameter-dependent. For simulating SGN
at low computational costs and without changing the learning rate or batch
size, we propose the Positive-Negative Momentum (PNM) approach that is a
powerful alternative to conventional Momentum in classic optimizers. The
introduced PNM method maintains two approximate independent momentum terms.
Then, we can control the magnitude of SGN explicitly by adjusting the momentum
difference. We theoretically prove the convergence guarantee and the
generalization advantage of PNM over Stochastic Gradient Descent (SGD). By
incorporating PNM into the two conventional optimizers, SGD with Momentum and
Adam, our extensive experiments empirically verified the significant advantage
of the PNM-based variants over the corresponding conventional Momentum-based
optimizers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1"&gt;Zeke Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1"&gt;Li Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1"&gt;Zhanxing Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sugiyama_M/0/1/0/all/0/1"&gt;Masashi Sugiyama&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LeadCache: Regret-Optimal Caching in Networks. (arXiv:2009.08228v2 [cs.IT] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.08228</id>
        <link href="http://arxiv.org/abs/2009.08228"/>
        <updated>2021-06-08T02:20:25.900Z</updated>
        <summary type="html"><![CDATA[We consider a set-valued online prediction problem in the context of network
caching. Assume that users are connected to a number of caches via a bipartite
network. At any time slot, each user requests some file chosen from a large
catalog. A user's request is met if the requested file is cached in at least
one of the caches connected to the user. The objective is to predict and
optimally store the files on the caches to maximize the total number of cache
hits. We propose $\texttt{LeadCache}$ - an online caching policy based on the
Follow-the-Perturbed-Leader paradigm. We show that the policy is regret-optimal
up to a factor of $\tilde{O}(n^{3/8}),$ where $n$ is the number of users. We
implement the policy by designing a new linear-time Pipage rounding algorithm.
With an additional Strong-Law-type assumption, we show that the total number of
file fetches under $\texttt{LeadCache}$ remains almost surely finite.
Additionally, we derive a tight regret lower bound using results from graph
coloring. Our conclusion is that the proposed learning-based caching policy
decisively outperforms the classical policies both theoretically and
empirically.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Paria_D/0/1/0/all/0/1"&gt;Debjit Paria&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sinha_A/0/1/0/all/0/1"&gt;Abhishek Sinha&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Disentangling a Deep Learned Volume Formula. (arXiv:2012.03955v2 [hep-th] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.03955</id>
        <link href="http://arxiv.org/abs/2012.03955"/>
        <updated>2021-06-08T02:20:25.894Z</updated>
        <summary type="html"><![CDATA[We present a simple phenomenological formula which approximates the
hyperbolic volume of a knot using only a single evaluation of its Jones
polynomial at a root of unity. The average error is just $2.86$% on the first
$1.7$ million knots, which represents a large improvement over previous
formulas of this kind. To find the approximation formula, we use layer-wise
relevance propagation to reverse engineer a black box neural network which
achieves a similar average error for the same approximation task when trained
on $10$% of the total dataset. The particular roots of unity which appear in
our analysis cannot be written as $e^{2\pi i / (k+2)}$ with integer $k$;
therefore, the relevant Jones polynomial evaluations are not given by
unknot-normalized expectation values of Wilson loop operators in conventional
$SU(2)$ Chern$\unicode{x2013}$Simons theory with level $k$. Instead, they
correspond to an analytic continuation of such expectation values to fractional
level. We briefly review the continuation procedure and comment on the presence
of certain Lefschetz thimbles, to which our approximation formula is sensitive,
in the analytically continued Chern$\unicode{x2013}$Simons integration cycle.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/hep-th/1/au:+Craven_J/0/1/0/all/0/1"&gt;Jessica Craven&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-th/1/au:+Jejjala_V/0/1/0/all/0/1"&gt;Vishnu Jejjala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-th/1/au:+Kar_A/0/1/0/all/0/1"&gt;Arjun Kar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Automated Evaluation of Open Domain Dialog via Diverse Reference Augmentation. (arXiv:2106.02833v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02833</id>
        <link href="http://arxiv.org/abs/2106.02833"/>
        <updated>2021-06-08T02:20:25.888Z</updated>
        <summary type="html"><![CDATA[Multiple different responses are often plausible for a given open domain
dialog context. Prior work has shown the importance of having multiple valid
reference responses for meaningful and robust automated evaluations. In such
cases, common practice has been to collect more human written references.
However, such collection can be expensive, time consuming, and not easily
scalable. Instead, we propose a novel technique for automatically expanding a
human generated reference to a set of candidate references. We fetch plausible
references from knowledge sources, and adapt them so that they are more fluent
in context of the dialog instance in question. More specifically, we use (1) a
commonsense knowledge base to elicit a large number of plausible reactions
given the dialog history (2) relevant instances retrieved from dialog corpus,
using similar past as well as future contexts. We demonstrate that our
automatically expanded reference sets lead to large improvements in
correlations of automated metrics with human ratings of system outputs for
DailyDialog dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gangal_V/0/1/0/all/0/1"&gt;Varun Gangal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jhamtani_H/0/1/0/all/0/1"&gt;Harsh Jhamtani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hovy_E/0/1/0/all/0/1"&gt;Eduard Hovy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berg_Kirkpatrick_T/0/1/0/all/0/1"&gt;Taylor Berg-Kirkpatrick&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Framework to Learn with Interpretation. (arXiv:2010.09345v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.09345</id>
        <link href="http://arxiv.org/abs/2010.09345"/>
        <updated>2021-06-08T02:20:25.882Z</updated>
        <summary type="html"><![CDATA[To tackle interpretability in deep learning, we present a novel framework to
jointly learn a predictive model and its associated interpretation model. The
interpreter provides both local and global interpretability about the
predictive model in terms of human-understandable high level attribute
functions, with minimal loss of accuracy. This is achieved by a dedicated
architecture and well chosen regularization penalties. We seek for a small-size
dictionary of high level attribute functions that take as inputs the outputs of
selected hidden layers and whose outputs feed a linear classifier. We impose
strong conciseness on the activation of attributes with an entropy-based
criterion while enforcing fidelity to both inputs and outputs of the predictive
model. A detailed pipeline to visualize the learnt features is also developed.
Moreover, besides generating interpretable models by design, our approach can
be specialized to provide post-hoc interpretations for a pre-trained neural
network. We validate our approach against several state-of-the-art methods on
multiple datasets and show its efficacy on both kinds of tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Parekh_J/0/1/0/all/0/1"&gt;Jayneel Parekh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mozharovskyi_P/0/1/0/all/0/1"&gt;Pavlo Mozharovskyi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+dAlche_Buc_F/0/1/0/all/0/1"&gt;Florence d&amp;#x27;Alch&amp;#xe9;-Buc&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Federated Composite Optimization. (arXiv:2011.08474v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.08474</id>
        <link href="http://arxiv.org/abs/2011.08474"/>
        <updated>2021-06-08T02:20:25.875Z</updated>
        <summary type="html"><![CDATA[Federated Learning (FL) is a distributed learning paradigm that scales
on-device learning collaboratively and privately. Standard FL algorithms such
as FedAvg are primarily geared towards smooth unconstrained settings. In this
paper, we study the Federated Composite Optimization (FCO) problem, in which
the loss function contains a non-smooth regularizer. Such problems arise
naturally in FL applications that involve sparsity, low-rank, monotonicity, or
more general constraints. We first show that straightforward extensions of
primal algorithms such as FedAvg are not well-suited for FCO since they suffer
from the "curse of primal averaging," resulting in poor convergence. As a
solution, we propose a new primal-dual algorithm, Federated Dual Averaging
(FedDualAvg), which by employing a novel server dual averaging procedure
circumvents the curse of primal averaging. Our theoretical analysis and
empirical experiments demonstrate that FedDualAvg outperforms the other
baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1"&gt;Honglin Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zaheer_M/0/1/0/all/0/1"&gt;Manzil Zaheer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reddi_S/0/1/0/all/0/1"&gt;Sashank Reddi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluation of deep learning models for multi-step ahead time series prediction. (arXiv:2103.14250v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.14250</id>
        <link href="http://arxiv.org/abs/2103.14250"/>
        <updated>2021-06-08T02:20:25.858Z</updated>
        <summary type="html"><![CDATA[Time series prediction with neural networks has been the focus of much
research in the past few decades. Given the recent deep learning revolution,
there has been much attention in using deep learning models for time series
prediction, and hence it is important to evaluate their strengths and
weaknesses. In this paper, we present an evaluation study that compares the
performance of deep learning models for multi-step ahead time series
prediction. The deep learning methods comprise simple recurrent neural
networks, long short-term memory (LSTM) networks, bidirectional LSTM networks,
encoder-decoder LSTM networks, and convolutional neural networks. We provide a
further comparison with simple neural networks that use stochastic gradient
descent and adaptive moment estimation (Adam) for training. We focus on
univariate time series for multi-step-ahead prediction from benchmark
time-series datasets and provide a further comparison of the results with
related methods from the literature. The results show that the bidirectional
and encoder-decoder LSTM network provides the best performance in accuracy for
the given time series problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chandra_R/0/1/0/all/0/1"&gt;Rohitash Chandra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goyal_S/0/1/0/all/0/1"&gt;Shaurya Goyal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_R/0/1/0/all/0/1"&gt;Rishabh Gupta&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Provably End-to-end Label-Noise Learning without Anchor Points. (arXiv:2102.02400v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.02400</id>
        <link href="http://arxiv.org/abs/2102.02400"/>
        <updated>2021-06-08T02:20:25.852Z</updated>
        <summary type="html"><![CDATA[In label-noise learning, the transition matrix plays a key role in building
statistically consistent classifiers. Existing consistent estimators for the
transition matrix have been developed by exploiting anchor points. However, the
anchor-point assumption is not always satisfied in real scenarios. In this
paper, we propose an end-to-end framework for solving label-noise learning
without anchor points, in which we simultaneously optimize two objectives: the
cross entropy loss between the prediction by the neural network and the given
noisy label, and the volume of the simplex formed by the columns of the
transition matrix. Our proposed framework can identify the transition matrix if
the clean class-posterior probabilities are sufficiently scattered. This is by
far the mildest assumption under which the transition matrix is provably
identifiable and the learned classifier is statistically consistent.
Experimental results on benchmark datasets demonstrate the effectiveness and
robustness of the proposed method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xuefeng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tongliang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1"&gt;Bo Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niu_G/0/1/0/all/0/1"&gt;Gang Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sugiyama_M/0/1/0/all/0/1"&gt;Masashi Sugiyama&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Meta-Learning with Variational Semantic Memory for Word Sense Disambiguation. (arXiv:2106.02960v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02960</id>
        <link href="http://arxiv.org/abs/2106.02960"/>
        <updated>2021-06-08T02:20:25.844Z</updated>
        <summary type="html"><![CDATA[A critical challenge faced by supervised word sense disambiguation (WSD) is
the lack of large annotated datasets with sufficient coverage of words in their
diversity of senses. This inspired recent research on few-shot WSD using
meta-learning. While such work has successfully applied meta-learning to learn
new word senses from very few examples, its performance still lags behind its
fully supervised counterpart. Aiming to further close this gap, we propose a
model of semantic memory for WSD in a meta-learning setting. Semantic memory
encapsulates prior experiences seen throughout the lifetime of the model, which
aids better generalization in limited data settings. Our model is based on
hierarchical variational inference and incorporates an adaptive memory update
rule via a hypernetwork. We show our model advances the state of the art in
few-shot WSD, supports effective learning in extremely data scarce (e.g.
one-shot) scenarios and produces meaning prototypes that capture similar senses
of distinct words.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1"&gt;Yingjun Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Holla_N/0/1/0/all/0/1"&gt;Nithin Holla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhen_X/0/1/0/all/0/1"&gt;Xiantong Zhen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Snoek_C/0/1/0/all/0/1"&gt;Cees G.M. Snoek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shutova_E/0/1/0/all/0/1"&gt;Ekaterina Shutova&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Revisiting Maximum Entropy Inverse Reinforcement Learning: New Perspectives and Algorithms. (arXiv:2012.00889v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.00889</id>
        <link href="http://arxiv.org/abs/2012.00889"/>
        <updated>2021-06-08T02:20:25.838Z</updated>
        <summary type="html"><![CDATA[We provide new perspectives and inference algorithms for Maximum Entropy
(MaxEnt) Inverse Reinforcement Learning (IRL), which provides a principled
method to find a most non-committal reward function consistent with given
expert demonstrations, among many consistent reward functions.

We first present a generalized MaxEnt formulation based on minimizing a
KL-divergence instead of maximizing an entropy. This improves the previous
heuristic derivation of the MaxEnt IRL model (for stochastic MDPs), allows a
unified view of MaxEnt IRL and Relative Entropy IRL, and leads to a model-free
learning algorithm for the MaxEnt IRL model. Second, a careful review of
existing inference algorithms and implementations showed that they
approximately compute the marginals required for learning the model. We provide
examples to illustrate this, and present an efficient and exact inference
algorithm. Our algorithm can handle variable length demonstrations; in
addition, while a basic version takes time quadratic in the maximum
demonstration length L, an improved version of this algorithm reduces this to
linear using a padding trick.

Experiments show that our exact algorithm improves reward learning as
compared to the approximate ones. Furthermore, our algorithm scales up to a
large, real-world dataset involving driver behaviour forecasting. We provide an
optimized implementation compatible with the OpenAI Gym interface. Our new
insight and algorithms could possibly lead to further interest and exploration
of the original MaxEnt IRL model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Snoswell_A/0/1/0/all/0/1"&gt;Aaron J. Snoswell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1"&gt;Surya P. N. Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_N/0/1/0/all/0/1"&gt;Nan Ye&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Conditional Versus Adversarial Euler-based Generators For Time Series. (arXiv:2102.05313v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.05313</id>
        <link href="http://arxiv.org/abs/2102.05313"/>
        <updated>2021-06-08T02:20:25.820Z</updated>
        <summary type="html"><![CDATA[We introduce three new generative models for time series. Based on Euler
discretization and Wasserstein metrics, they are able to capture time marginal
distributions and temporal dynamics. Two of these methods rely on the
adaptation of generative adversarial networks (GANs) to time series. Both of
them outperform state-of-the-art benchmarks by capturing the underlying
temporal structure on synthetic time series. The third algorithm, called
Conditional Euler Generator (CEGEN), minimizes a dedicated distance between the
transition probability distributions over all time steps. In the context of Ito
processes, we provide theoretical guarantees that minimizing this criterion
implies accurate estimations of the drift and volatility parameters. We
demonstrate empirically that CEGEN outperforms state-of-the-art and GAN
generators on both marginal and temporal dynamics metrics. Besides, it
identifies accurate correlation structures in high dimension. When few data
points are available, we verify the effectiveness of CEGEN, when combined with
transfer learning methods on Monte Carlo simulations. Finally, we illustrate
the robustness of our method on various real-world datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Remlinger_C/0/1/0/all/0/1"&gt;Carl Remlinger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Mikael_J/0/1/0/all/0/1"&gt;Joseph Mikael&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Elie_R/0/1/0/all/0/1"&gt;Romuald Elie&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning High Dimensional Wasserstein Geodesics. (arXiv:2102.02992v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.02992</id>
        <link href="http://arxiv.org/abs/2102.02992"/>
        <updated>2021-06-08T02:20:25.814Z</updated>
        <summary type="html"><![CDATA[We propose a new formulation and learning strategy for computing the
Wasserstein geodesic between two probability distributions in high dimensions.
By applying the method of Lagrange multipliers to the dynamic formulation of
the optimal transport (OT) problem, we derive a minimax problem whose saddle
point is the Wasserstein geodesic. We then parametrize the functions by deep
neural networks and design a sample based bidirectional learning algorithm for
training. The trained networks enable sampling from the Wasserstein geodesic.
As by-products, the algorithm also computes the Wasserstein distance and OT map
between the marginal distributions. We demonstrate the performance of our
algorithms through a series of experiments with both synthetic and realistic
data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Shu Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1"&gt;Shaojun Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yongxin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zha_H/0/1/0/all/0/1"&gt;Hongyuan Zha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1"&gt;Haomin Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Measuring Model Biases in the Absence of Ground Truth. (arXiv:2103.03417v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.03417</id>
        <link href="http://arxiv.org/abs/2103.03417"/>
        <updated>2021-06-08T02:20:25.808Z</updated>
        <summary type="html"><![CDATA[The measurement of bias in machine learning often focuses on model
performance across identity subgroups (such as man and woman) with respect to
groundtruth labels. However, these methods do not directly measure the
associations that a model may have learned, for example between labels and
identity subgroups. Further, measuring a model's bias requires a fully
annotated evaluation dataset which may not be easily available in practice. We
present an elegant mathematical solution that tackles both issues
simultaneously, using image classification as a working example. By treating a
classification model's predictions for a given image as a set of labels
analogous to a bag of words, we rank the biases that a model has learned with
respect to different identity labels. We use (man, woman) as a concrete example
of an identity label set (although this set need not be binary), and present
rankings for the labels that are most biased towards one identity or the other.
We demonstrate how the statistical properties of different association metrics
can lead to different rankings of the most "gender biased" labels, and conclude
that normalized pointwise mutual information (nPMI) is most useful in practice.
Finally, we announce an open-sourced nPMI visualization tool using TensorBoard.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aka_O/0/1/0/all/0/1"&gt;Osman Aka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Burke_K/0/1/0/all/0/1"&gt;Ken Burke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bauerle_A/0/1/0/all/0/1"&gt;Alex B&amp;#xe4;uerle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Greer_C/0/1/0/all/0/1"&gt;Christina Greer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mitchell_M/0/1/0/all/0/1"&gt;Margaret Mitchell&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How to distribute data across tasks for meta-learning?. (arXiv:2103.08463v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.08463</id>
        <link href="http://arxiv.org/abs/2103.08463"/>
        <updated>2021-06-08T02:20:25.801Z</updated>
        <summary type="html"><![CDATA[Meta-learning models transfer the knowledge acquired from previous tasks to
quickly learn new ones. They are trained on benchmarks with a fixed number of
data points per task. This number is usually arbitrary and it is unknown how it
affects performance at testing. Since labelling of data is expensive, finding
the optimal allocation of labels across training tasks may reduce costs. Given
a fixed budget of labels, should we use a small number of highly labelled
tasks, or many tasks with few labels each? Should we allocate more labels to
some tasks and less to others? We show that: 1) If tasks are homogeneous, there
is a uniform optimal allocation, whereby all tasks get the same amount of data;
2) At fixed budget, there is a trade-off between number of tasks and number of
data points per task, with a unique and constant optimum; 3) When trained
separately, harder task should get more data, at the cost of a smaller number
of tasks; 4) When training on a mixture of easy and hard tasks, more data
should be allocated to easy tasks. Interestingly, Neuroscience experiments have
shown that human visual skills also transfer better from easy tasks. We prove
these results mathematically on mixed linear regression, and we show
empirically that the same results hold for few-shot image classification on
CIFAR-FS and mini-ImageNet. Our results provide guidance for allocating labels
across tasks when collecting data for meta-learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cioba_A/0/1/0/all/0/1"&gt;Alexandru Cioba&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bromberg_M/0/1/0/all/0/1"&gt;Michael Bromberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1"&gt;Qian Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niyogi_R/0/1/0/all/0/1"&gt;Ritwik Niyogi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Batzolis_G/0/1/0/all/0/1"&gt;Georgios Batzolis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Garcia_J/0/1/0/all/0/1"&gt;Jezabel Garcia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shiu_D/0/1/0/all/0/1"&gt;Da-shan Shiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bernacchia_A/0/1/0/all/0/1"&gt;Alberto Bernacchia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PeerGAN: Generative Adversarial Networks with a Competing Peer Discriminator. (arXiv:2101.07524v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.07524</id>
        <link href="http://arxiv.org/abs/2101.07524"/>
        <updated>2021-06-08T02:20:25.794Z</updated>
        <summary type="html"><![CDATA[In this paper, we introduce PeerGAN, a generative adversarial network (GAN)
solution to improve the stability of the generated samples and to mitigate mode
collapse. Built upon the Vanilla GAN's two-player game between the
discriminator $D_1$ and the generator $G$, we introduce a peer discriminator
$D_2$ to the min-max game. Similar to previous work using two discriminators,
the first role of both $D_1$, $D_2$ is to distinguish between generated samples
and real ones, while the generator tries to generate high-quality samples which
are able to fool both discriminators. Different from existing methods, we
introduce another game between $D_1$ and $D_2$ to discourage their agreement
and therefore increase the level of diversity of the generated samples. This
property alleviates the issue of early mode collapse by preventing $D_1$ and
$D_2$ from converging too fast. We provide theoretical analysis for the
equilibrium of the min-max game formed among $G, D_1, D_2$. We offer
convergence behavior of PeerGAN as well as stability of the min-max game. It's
worth mentioning that PeerGAN operates in the unsupervised setting, and the
additional game between $D_1$ and $D_2$ does not need any label supervision.
Experiments results on a synthetic dataset and on real-world image datasets
(MNIST, Fashion MNIST, CIFAR-10, STL-10, CelebA, VGG) demonstrate that PeerGAN
outperforms competitive baseline work in generating diverse and high-quality
samples, while only introduces negligible computation cost.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1"&gt;Jiaheng Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1"&gt;Minghao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1"&gt;Jiahao Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1"&gt;Qiutong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Davis_J/0/1/0/all/0/1"&gt;James Davis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yang Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Model-Invariant State Abstractions for Model-Based Reinforcement Learning. (arXiv:2102.09850v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.09850</id>
        <link href="http://arxiv.org/abs/2102.09850"/>
        <updated>2021-06-08T02:20:25.788Z</updated>
        <summary type="html"><![CDATA[Accuracy and generalization of dynamics models is key to the success of
model-based reinforcement learning (MBRL). As the complexity of tasks
increases, so does the sample inefficiency of learning accurate dynamics
models. However, many complex tasks also exhibit sparsity in the dynamics,
i.e., actions have only a local effect on the system dynamics. In this paper,
we exploit this property with a causal invariance perspective in the
single-task setting, introducing a new type of state abstraction called
\textit{model-invariance}. Unlike previous forms of state abstractions, a
model-invariance state abstraction leverages causal sparsity over state
variables. This allows for compositional generalization to unseen states,
something that non-factored forms of state abstractions cannot do. We prove
that an optimal policy can be learned over this model-invariance state
abstraction and show improved generalization in a simple toy domain. Next, we
propose a practical method to approximately learn a model-invariant
representation for complex domains and validate our approach by showing
improved modelling performance over standard maximum likelihood approaches on
challenging tasks, such as the MuJoCo-based Humanoid. Finally, within the MBRL
setting we show strong performance gains with respect to sample efficiency
across a host of other continuous control tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tomar_M/0/1/0/all/0/1"&gt;Manan Tomar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1"&gt;Amy Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Calandra_R/0/1/0/all/0/1"&gt;Roberto Calandra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Taylor_M/0/1/0/all/0/1"&gt;Matthew E. Taylor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pineau_J/0/1/0/all/0/1"&gt;Joelle Pineau&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Resource Allocation in Multi-armed Bandit Exploration: Overcoming Sublinear Scaling with Adaptive Parallelism. (arXiv:2011.00330v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.00330</id>
        <link href="http://arxiv.org/abs/2011.00330"/>
        <updated>2021-06-08T02:20:25.770Z</updated>
        <summary type="html"><![CDATA[We study exploration in stochastic multi-armed bandits when we have access to
a divisible resource that can be allocated in varying amounts to arm pulls. We
focus in particular on the allocation of distributed computing resources, where
we may obtain results faster by allocating more resources per pull, but might
have reduced throughput due to nonlinear scaling. For example, in
simulation-based scientific studies, an expensive simulation can be sped up by
running it on multiple cores. This speed-up however, is partly offset by the
communication among cores, which results in lower throughput than if fewer
cores were allocated per trial to run more trials in parallel. In this paper,
we explore these trade-offs in two settings. First, in a fixed confidence
setting, we need to find the best arm with a given target success probability
as quickly as possible. We propose an algorithm which trades off between
information accumulation and throughput and show that the time taken can be
upper bounded by the solution of a dynamic program whose inputs are the gaps
between the sub-optimal and optimal arms. We also prove a matching hardness
result. Second, we present an algorithm for a fixed deadline setting, where we
are given a time deadline and need to maximize the probability of finding the
best arm. We corroborate our theoretical insights with simulation experiments
that show that the algorithms consistently match or outperform baseline
algorithms on a variety of problem instances.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Thananjeyan_B/0/1/0/all/0/1"&gt;Brijen Thananjeyan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kandasamy_K/0/1/0/all/0/1"&gt;Kirthevasan Kandasamy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stoica_I/0/1/0/all/0/1"&gt;Ion Stoica&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jordan_M/0/1/0/all/0/1"&gt;Michael I. Jordan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldberg_K/0/1/0/all/0/1"&gt;Ken Goldberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gonzalez_J/0/1/0/all/0/1"&gt;Joseph E. Gonzalez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Witness Two-Sample Test. (arXiv:2102.05573v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.05573</id>
        <link href="http://arxiv.org/abs/2102.05573"/>
        <updated>2021-06-08T02:20:25.764Z</updated>
        <summary type="html"><![CDATA[The Maximum Mean Discrepancy (MMD) has been the state-of-the-art
nonparametric test for tackling the two-sample problem. Its statistic is given
by the difference in expectations of the witness function, a real-valued
function defined as a weighted sum of kernel evaluations on a set of basis
points. Typically the kernel is optimized on a training set, and hypothesis
testing is performed on a separate test set to avoid overfitting (i.e., control
type-I error). That is, the test set is used to simultaneously estimate the
expectations and define the basis points, while the training set only serves to
select the kernel and is discarded. In this work, we argue that this data
splitting scheme is overly conservative, and propose to use the training data
to also define the weights and the basis points for better data efficiency. We
show that 1) the new test is consistent and has a well-controlled type-I error;
2) the optimal witness function is given by a precision-weighted mean in the
reproducing kernel Hilbert space associated with the kernel, and is closely
related to kernel Fisher discriminant analysis; and 3) the test power of the
proposed test is comparable or exceeds that of the MMD and other modern tests,
as verified empirically on challenging synthetic and real problems (e.g., Higgs
data).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kubler_J/0/1/0/all/0/1"&gt;Jonas M. K&amp;#xfc;bler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jitkrittum_W/0/1/0/all/0/1"&gt;Wittawat Jitkrittum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1"&gt;Bernhard Sch&amp;#xf6;lkopf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muandet_K/0/1/0/all/0/1"&gt;Krikamol Muandet&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeepCPCFG: Deep Learning and Context Free Grammars for End-to-End Information Extraction. (arXiv:2103.05908v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.05908</id>
        <link href="http://arxiv.org/abs/2103.05908"/>
        <updated>2021-06-08T02:20:25.757Z</updated>
        <summary type="html"><![CDATA[We address the challenge of extracting structured information from business
documents without detailed annotations. We propose Deep Conditional
Probabilistic Context Free Grammars (DeepCPCFG) to parse two-dimensional
complex documents and use Recursive Neural Networks to create an end-to-end
system for finding the most probable parse that represents the structured
information to be extracted. This system is trained end-to-end with scanned
documents as input and only relational-records as labels. The
relational-records are extracted from existing databases avoiding the cost of
annotating documents by hand. We apply this approach to extract information
from scanned invoices achieving state-of-the-art results despite using no
hand-annotations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chua_F/0/1/0/all/0/1"&gt;Freddy C. Chua&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duffy_N/0/1/0/all/0/1"&gt;Nigel P. Duffy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Integral Probability Metric based Regularization for Optimal Transport. (arXiv:2011.05001v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.05001</id>
        <link href="http://arxiv.org/abs/2011.05001"/>
        <updated>2021-06-08T02:20:25.750Z</updated>
        <summary type="html"><![CDATA[Regularization in Optimal Transport (OT) problems has been shown to
critically affect the associated computational and sample complexities. It also
has been observed that regularization effectively helps in handling noisy
marginals as well as marginals with unequal masses. However, existing works on
OT restrict themselves to $\phi$-divergences based regularization. In this
work, we propose and analyze Integral Probability Metric (IPM) based
regularization in OT problems. While it is expected that the well-established
advantages of IPMs are inherited by the IPM-regularized OT variants, we
interestingly observe that some useful aspects of $\phi$-regularization are
preserved. For example, we show that the OT formulation, where the marginal
constraints are relaxed using IPM-regularization, also lifts the ground metric
to that over (perhaps un-normalized) measures. Infact, the lifted metric turns
out to be another IPM whose generating set is the intersection of that of the
IPM employed for regularization and the set of 1-Lipschitz functions under the
ground metric. Also, in the special case where the regularization is squared
maximum mean discrepancy based, the proposed OT variant, as well as the
corresponding Barycenter formulation, turn out to be those of minimizing a
convex quadratic subject to non-negativity/simplex constraints and hence can be
solved efficiently. Simulations confirm that the optimal transport plans/maps
obtained with IPM-regularization are intrinsically different from those
obtained with $\phi$-regularization. Empirical results illustrate the efficacy
of the proposed IPM-regularized OT formulation.

This draft contains the main paper and the Appendices.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Manupriya_P/0/1/0/all/0/1"&gt;Piyushi Manupriya&lt;/a&gt; (IIT Hyderabad, INDIA), &lt;a href="http://arxiv.org/find/cs/1/au:+Nath_J/0/1/0/all/0/1"&gt;J. Saketha Nath&lt;/a&gt; (IIT Hyderabad, INDIA), &lt;a href="http://arxiv.org/find/cs/1/au:+Jawanpuria_P/0/1/0/all/0/1"&gt;Pratik Jawanpuria&lt;/a&gt; (Microsoft IDC, INDIA)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Thresholded Lasso Bandit. (arXiv:2010.11994v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.11994</id>
        <link href="http://arxiv.org/abs/2010.11994"/>
        <updated>2021-06-08T02:20:25.725Z</updated>
        <summary type="html"><![CDATA[In this paper, we revisit the regret minimization problem in sparse
stochastic contextual linear bandits, where feature vectors may be of large
dimension $d$, but where the reward function depends on a few, say $s_0\ll d$,
of these features only. We present Thresholded Lasso bandit, an algorithm that
(i) estimates the vector defining the reward function as well as its sparse
support, i.e., significant feature elements, using the Lasso framework with
thresholding, and (ii) selects an arm greedily according to this estimate
projected on its support. The algorithm does not require prior knowledge of the
sparsity index $s_0$. For this simple algorithm, we establish non-asymptotic
regret upper bounds scaling as $\mathcal{O}( \log d + \sqrt{T} )$ in general,
and as $\mathcal{O}( \log d + \log T)$ under the so-called margin condition (a
setting where arms are well separated). The regret of previous algorithms
scales as $\mathcal{O}( \log d + \sqrt{T \log (d T)})$ and $\mathcal{O}( \log T
\log d)$ in the two settings, respectively. Through numerical experiments, we
confirm that our algorithm outperforms existing methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Ariu_K/0/1/0/all/0/1"&gt;Kaito Ariu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Abe_K/0/1/0/all/0/1"&gt;Kenshi Abe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Proutiere_A/0/1/0/all/0/1"&gt;Alexandre Prouti&amp;#xe8;re&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Anonymizing Machine Learning Models. (arXiv:2007.13086v2 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.13086</id>
        <link href="http://arxiv.org/abs/2007.13086"/>
        <updated>2021-06-08T02:20:25.718Z</updated>
        <summary type="html"><![CDATA[There is a known tension between the need to analyze personal data to drive
business and privacy concerns. Many data protection regulations, including the
EU General Data Protection Regulation (GDPR) and the California Consumer
Protection Act (CCPA), set out strict restrictions and obligations on companies
that collect or process personal data. Moreover, machine learning models
themselves can be used to derive personal information, as demonstrated by
recent membership and attribute inference attacks. Anonymized data, however, is
exempt from data protection principles and obligations. Thus, models built on
anonymized data are also exempt from any privacy obligations, in addition to
providing better protection against such attacks on the training data. Learning
on anonymized data typically results in a significant degradation in accuracy.
We address this challenge by guiding our anonymization using the knowledge
encoded within the model, and targeting it to minimize the impact on the
model's accuracy, a process we call accuracy-guided anonymization. We
demonstrate that by focusing on the model's accuracy rather than information
loss, our method outperforms state of the art k-anonymity methods in terms of
the achieved utility, in particular with high values of k and large numbers of
quasi-identifiers. We also demonstrate that our approach achieves similar
results in its ability to prevent membership inference attacks as alternative
approaches based on differential privacy. This shows that model-guided
anonymization can, in some cases, be a legitimate substitute for such methods,
while averting some of their inherent drawbacks such as complexity, performance
overhead and being fitted to specific model types. As opposed to methods that
rely on adding noise during training, our approach does not rely on making any
modifications to the training algorithm itself.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Goldsteen_A/0/1/0/all/0/1"&gt;Abigail Goldsteen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ezov_G/0/1/0/all/0/1"&gt;Gilad Ezov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shmelkin_R/0/1/0/all/0/1"&gt;Ron Shmelkin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moffie_M/0/1/0/all/0/1"&gt;Micha Moffie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Farkash_A/0/1/0/all/0/1"&gt;Ariel Farkash&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unit Ball Model for Embedding Hierarchical Structures in the Complex Hyperbolic Space. (arXiv:2105.03966v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.03966</id>
        <link href="http://arxiv.org/abs/2105.03966"/>
        <updated>2021-06-08T02:20:25.711Z</updated>
        <summary type="html"><![CDATA[Learning the representation of data with hierarchical structures in the
hyperbolic space attracts increasing attention in recent years. Due to the
constant negative curvature, the hyperbolic space resembles tree metrics and
captures the tree-like properties naturally, which enables the hyperbolic
embeddings to improve over traditional Euclidean models. However, many
real-world hierarchically structured data such as taxonomies and multitree
networks have varying local structures and they are not trees, thus they do not
ubiquitously match the constant curvature property of the hyperbolic space. To
address this limitation of hyperbolic embeddings, we explore the complex
hyperbolic space, which has the variable negative curvature, for representation
learning. Specifically, we propose to learn the embeddings of hierarchically
structured data in the unit ball model of the complex hyperbolic space. The
unit ball model based embeddings have a more powerful representation capacity
to capture a variety of hierarchical structures. Through experiments on
synthetic and real-world data, we show that our approach improves over the
hyperbolic embedding models significantly.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_H/0/1/0/all/0/1"&gt;Huiru Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1"&gt;Caigao Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1"&gt;Yangqiu Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;James Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_J/0/1/0/all/0/1"&gt;Junwu Xiong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Embed Categorical Features without Embedding Tables for Recommendation. (arXiv:2010.10784v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.10784</id>
        <link href="http://arxiv.org/abs/2010.10784"/>
        <updated>2021-06-08T02:20:25.704Z</updated>
        <summary type="html"><![CDATA[Embedding learning of categorical features (e.g. user/item IDs) is at the
core of various recommendation models including matrix factorization and neural
collaborative filtering. The standard approach creates an embedding table where
each row represents a dedicated embedding vector for every unique feature
value. However, this method fails to efficiently handle high-cardinality
features and unseen feature values (e.g. new video ID) that are prevalent in
real-world recommendation systems. In this paper, we propose an alternative
embedding framework Deep Hash Embedding (DHE), replacing embedding tables by a
deep embedding network to compute embeddings on the fly. DHE first encodes the
feature value to a unique identifier vector with multiple hashing functions and
transformations, and then applies a DNN to convert the identifier vector to an
embedding. The encoding module is deterministic, non-learnable, and free of
storage, while the embedding network is updated during the training time to
learn embedding generation. Empirical results show that DHE achieves comparable
AUC against the standard one-hot full embedding, with smaller model sizes. Our
work sheds light on the design of DNN-based alternative embedding schemes for
categorical features without using embedding table lookup.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kang_W/0/1/0/all/0/1"&gt;Wang-Cheng Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_D/0/1/0/all/0/1"&gt;Derek Zhiyuan Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_T/0/1/0/all/0/1"&gt;Tiansheng Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yi_X/0/1/0/all/0/1"&gt;Xinyang Yi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1"&gt;Ting Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hong_L/0/1/0/all/0/1"&gt;Lichan Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chi_E/0/1/0/all/0/1"&gt;Ed H. Chi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TinyTL: Reduce Activations, Not Trainable Parameters for Efficient On-Device Learning. (arXiv:2007.11622v5 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.11622</id>
        <link href="http://arxiv.org/abs/2007.11622"/>
        <updated>2021-06-08T02:20:25.698Z</updated>
        <summary type="html"><![CDATA[On-device learning enables edge devices to continually adapt the AI models to
new data, which requires a small memory footprint to fit the tight memory
constraint of edge devices. Existing work solves this problem by reducing the
number of trainable parameters. However, this doesn't directly translate to
memory saving since the major bottleneck is the activations, not parameters. In
this work, we present Tiny-Transfer-Learning (TinyTL) for memory-efficient
on-device learning. TinyTL freezes the weights while only learns the bias
modules, thus no need to store the intermediate activations. To maintain the
adaptation capacity, we introduce a new memory-efficient bias module, the lite
residual module, to refine the feature extractor by learning small residual
feature maps adding only 3.8% memory overhead. Extensive experiments show that
TinyTL significantly saves the memory (up to 6.5x) with little accuracy loss
compared to fine-tuning the full network. Compared to fine-tuning the last
layer, TinyTL provides significant accuracy improvements (up to 34.1%) with
little memory overhead. Furthermore, combined with feature extractor
adaptation, TinyTL provides 7.3-12.9x memory saving without sacrificing
accuracy compared to fine-tuning the full Inception-V3.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1"&gt;Han Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gan_C/0/1/0/all/0/1"&gt;Chuang Gan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1"&gt;Ligeng Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1"&gt;Song Han&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Differentiable Open-Ended Commonsense Reasoning. (arXiv:2010.14439v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.14439</id>
        <link href="http://arxiv.org/abs/2010.14439"/>
        <updated>2021-06-08T02:20:25.690Z</updated>
        <summary type="html"><![CDATA[Current commonsense reasoning research focuses on developing models that use
commonsense knowledge to answer multiple-choice questions. However, systems
designed to answer multiple-choice questions may not be useful in applications
that do not provide a small list of candidate answers to choose from. As a step
towards making commonsense reasoning research more realistic, we propose to
study open-ended commonsense reasoning (OpenCSR) -- the task of answering a
commonsense question without any pre-defined choices -- using as a resource
only a corpus of commonsense facts written in natural language. OpenCSR is
challenging due to a large decision space, and because many questions require
implicit multi-hop reasoning. As an approach to OpenCSR, we propose DrFact, an
efficient Differentiable model for multi-hop Reasoning over knowledge Facts. To
evaluate OpenCSR methods, we adapt several popular commonsense reasoning
benchmarks, and collect multiple new answers for each test question via
crowd-sourcing. Experiments show that DrFact outperforms strong baseline
methods by a large margin.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1"&gt;Bill Yuchen Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1"&gt;Haitian Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dhingra_B/0/1/0/all/0/1"&gt;Bhuwan Dhingra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zaheer_M/0/1/0/all/0/1"&gt;Manzil Zaheer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1"&gt;Xiang Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cohen_W/0/1/0/all/0/1"&gt;William W. Cohen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Declarative Approaches to Counterfactual Explanations for Classification. (arXiv:2011.07423v2 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.07423</id>
        <link href="http://arxiv.org/abs/2011.07423"/>
        <updated>2021-06-08T02:20:25.684Z</updated>
        <summary type="html"><![CDATA[We propose answer-set programs that specify and compute counterfactual
interventions on entities that are input on a classification model. In relation
to the outcome of the model, the resulting counterfactual entities serve as a
basis for the definition and computation of causality-based explanation scores
for the feature values in the entity under classification, namely
"responsibility scores". The approach and the programs can be applied with
black-box models, and also with models that can be specified as logic programs,
such as rule-based classifiers. The main focus of this work is on the
specification and computation of "best" counterfactual entities, i.e. those
that lead to maximum responsibility scores. From them one can read off the
explanations as maximum responsibility feature values in the original entity.
We also extend the programs to bring into the picture semantic or domain
knowledge. We show how the approach could be extended by means of probabilistic
methods, and how the underlying probability distributions could be modified
through the use of constraints.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bertossi_L/0/1/0/all/0/1"&gt;Leopoldo Bertossi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding the Performance of Knowledge Graph Embeddings in Drug Discovery. (arXiv:2105.10488v2 [q-bio.BM] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.10488</id>
        <link href="http://arxiv.org/abs/2105.10488"/>
        <updated>2021-06-08T02:20:25.677Z</updated>
        <summary type="html"><![CDATA[Knowledge Graphs (KG) and associated Knowledge Graph Embedding (KGE) models
have recently begun to be explored in the context of drug discovery and have
the potential to assist in key challenges such as target identification. In the
drug discovery domain, KGs can be employed as part of a process which can
result in lab-based experiments being performed, or impact on other decisions,
incurring significant time and financial costs and most importantly, ultimately
influencing patient healthcare. For KGE models to have impact in this domain, a
better understanding of not only of performance, but also the various factors
which determine it, is required.

In this study we investigate, over the course of many thousands of
experiments, the predictive performance of five KGE models on two public drug
discovery-oriented KGs. Our goal is not to focus on the best overall model or
configuration, instead we take a deeper look at how performance can be affected
by changes in the training setup, choice of hyperparameters, model parameter
initialisation seed and different splits of the datasets. Our results highlight
that these factors have significant impact on performance and can even affect
the ranking of models. Indeed these factors should be reported along with model
architectures to ensure complete reproducibility and fair comparisons of future
work, and we argue this is critical for the acceptance of use, and impact of
KGEs in a biomedical setting. To aid reproducibility of our own work, we
release all experimentation code.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Bonner_S/0/1/0/all/0/1"&gt;Stephen Bonner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Barrett_I/0/1/0/all/0/1"&gt;Ian P Barrett&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Ye_C/0/1/0/all/0/1"&gt;Cheng Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Swiers_R/0/1/0/all/0/1"&gt;Rowan Swiers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Engkvist_O/0/1/0/all/0/1"&gt;Ola Engkvist&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Hoyt_C/0/1/0/all/0/1"&gt;Charles Tapley Hoyt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Hamilton_W/0/1/0/all/0/1"&gt;William L Hamilton&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Measuring the originality of intellectual property assets based on machine learning outputs. (arXiv:2010.06997v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.06997</id>
        <link href="http://arxiv.org/abs/2010.06997"/>
        <updated>2021-06-08T02:20:25.623Z</updated>
        <summary type="html"><![CDATA[Originality criteria are frequently used to compare assets and, in
particular, to assess the validity of intellectual property (IP) rights such as
copyright and design rights. In this work, the originality of an asset is
formulated as a function of the distances between this asset and its
comparands, using concepts of maximum entropy and surprisal analysis. Namely,
the originality function is defined according to the surprisal associated with
a given asset. Creative assets can be justifiably compared to particles that
repel each other via an electrostatic-like pair potential. This allows a very
simple, suitably bounded formula to be obtained, in which the originality of an
asset writes as the ratio of a reference energy to an interaction energy
imparted to that asset. In particular, the originality of an asset can be
expressed as a ratio of two average distances, i.e., the harmonic mean of the
distances from this asset to its comparands divided by the harmonic mean of the
distances between the sole comparands. Accordingly, the originality of objects
such as IP assets can be simply estimated based on distances computed thanks to
unsupervised machine learning techniques or other distance computation
algorithms. Application is made to various types of assets, including emojis,
typeface designs, paintings, and novel titles.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ragot_S/0/1/0/all/0/1"&gt;S&amp;#xe9;bastien Ragot&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[UNiTE: Unitary N-body Tensor Equivariant Network with Applications to Quantum Chemistry. (arXiv:2105.14655v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14655</id>
        <link href="http://arxiv.org/abs/2105.14655"/>
        <updated>2021-06-08T02:20:25.610Z</updated>
        <summary type="html"><![CDATA[Equivariant neural networks have been successful in incorporating various
types of symmetries, but are mostly limited to vector representations of
geometric objects. Despite the prevalence of higher-order tensors in various
application domains, e.g. in quantum chemistry, equivariant neural networks for
general tensors remain unexplored. Previous strategies for learning equivariant
functions on tensors mostly rely on expensive tensor factorization which is not
scalable when the dimensionality of the problem becomes large. In this work, we
propose unitary $N$-body tensor equivariant neural network (UNiTE), an
architecture for a general class of symmetric tensors called $N$-body tensors.
The proposed neural network is equivariant with respect to the actions of a
unitary group, such as the group of 3D rotations. Furthermore, it has a linear
time complexity with respect to the number of non-zero elements in the tensor.
We also introduce a normalization method, viz., Equivariant Normalization, to
improve generalization of the neural network while preserving symmetry. When
applied to quantum chemistry, UNiTE outperforms all state-of-the-art machine
learning methods of that domain with over 110% average improvements on multiple
benchmarks. Finally, we show that UNiTE achieves a robust zero-shot
generalization performance on diverse down stream chemistry tasks, while being
three orders of magnitude faster than conventional numerical methods with
competitive accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qiao_Z/0/1/0/all/0/1"&gt;Zhuoran Qiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Christensen_A/0/1/0/all/0/1"&gt;Anders S. Christensen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Welborn_M/0/1/0/all/0/1"&gt;Matthew Welborn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Manby_F/0/1/0/all/0/1"&gt;Frederick R. Manby&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1"&gt;Anima Anandkumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Miller_T/0/1/0/all/0/1"&gt;Thomas F. Miller III&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Discrepancy-Based Active Learning for Domain Adaptation. (arXiv:2103.03757v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.03757</id>
        <link href="http://arxiv.org/abs/2103.03757"/>
        <updated>2021-06-08T02:20:25.600Z</updated>
        <summary type="html"><![CDATA[The goal of the paper is to design active learning strategies which lead to
domain adaptation under an assumption of covariate shift in the case of
Lipschitz labeling function. Building on previous work by Mansour et al. (2009)
we adapt the concept of discrepancy distance between source and target
distributions to restrict the maximization over the hypothesis class to a
localized class of functions which are performing accurate labeling on the
source domain. We derive generalization error bounds for such active learning
strategies in terms of Rademacher average and localized discrepancy for general
loss functions which satisfy a regularity condition. A practical K-medoids
algorithm that can address the case of large data set is inferred from the
theoretical bounds. Our numerical experiments show that the proposed algorithm
is competitive against other state-of-the-art active learning techniques in the
context of domain adaptation, in particular on large data sets of around one
hundred thousand images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mathelin_A/0/1/0/all/0/1"&gt;Antoine de Mathelin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deheeger_F/0/1/0/all/0/1"&gt;Francois Deheeger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mougeot_M/0/1/0/all/0/1"&gt;Mathilde Mougeot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vayatis_N/0/1/0/all/0/1"&gt;Nicolas Vayatis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Refining Deep Generative Models via Discriminator Gradient Flow. (arXiv:2012.00780v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.00780</id>
        <link href="http://arxiv.org/abs/2012.00780"/>
        <updated>2021-06-08T02:20:25.585Z</updated>
        <summary type="html"><![CDATA[Deep generative modeling has seen impressive advances in recent years, to the
point where it is now commonplace to see simulated samples (e.g., images) that
closely resemble real-world data. However, generation quality is generally
inconsistent for any given model and can vary dramatically between samples. We
introduce Discriminator Gradient flow (DGflow), a new technique that improves
generated samples via the gradient flow of entropy-regularized f-divergences
between the real and the generated data distributions. The gradient flow takes
the form of a non-linear Fokker-Plank equation, which can be easily simulated
by sampling from the equivalent McKean-Vlasov process. By refining inferior
samples, our technique avoids wasteful sample rejection used by previous
methods (DRS & MH-GAN). Compared to existing works that focus on specific GAN
variants, we show our refinement approach can be applied to GANs with
vector-valued critics and even other deep generative models such as VAEs and
Normalizing Flows. Empirical results on multiple synthetic, image, and text
datasets demonstrate that DGflow leads to significant improvement in the
quality of generated samples for a variety of generative models, outperforming
the state-of-the-art Discriminator Optimal Transport (DOT) and Discriminator
Driven Latent Sampling (DDLS) methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ansari_A/0/1/0/all/0/1"&gt;Abdul Fatir Ansari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ang_M/0/1/0/all/0/1"&gt;Ming Liang Ang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soh_H/0/1/0/all/0/1"&gt;Harold Soh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neograd: Near-Ideal Gradient Descent. (arXiv:2010.07873v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.07873</id>
        <link href="http://arxiv.org/abs/2010.07873"/>
        <updated>2021-06-08T02:20:25.572Z</updated>
        <summary type="html"><![CDATA[The purpose of this paper is to improve upon existing variants of gradient
descent by solving two problems: (1) removing (or reducing) the plateau that
occurs while minimizing the cost function,(2) continually adjusting the
learning rate to an "ideal" value. The approach taken is to approximately solve
for the learning rate as a function of a trust metric. When this technique is
hybridized with momentum, it creates an especially effective gradient descent
variant, called NeogradM. It is shown to outperform Adam on several test
problems, and can easily reach cost function values that are smaller by a
factor of $10^8$.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zimmer_M/0/1/0/all/0/1"&gt;Michael F. Zimmer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Matching in Selective and Balanced Representation Space for Treatment Effects Estimation. (arXiv:2009.06828v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.06828</id>
        <link href="http://arxiv.org/abs/2009.06828"/>
        <updated>2021-06-08T02:20:25.555Z</updated>
        <summary type="html"><![CDATA[The dramatically growing availability of observational data is being
witnessed in various domains of science and technology, which facilitates the
study of causal inference. However, estimating treatment effects from
observational data is faced with two major challenges, missing counterfactual
outcomes and treatment selection bias. Matching methods are among the most
widely used and fundamental approaches to estimating treatment effects, but
existing matching methods have poor performance when facing data with high
dimensional and complicated variables. We propose a feature selection
representation matching (FSRM) method based on deep representation learning and
matching, which maps the original covariate space into a selective, nonlinear,
and balanced representation space, and then conducts matching in the learned
representation space. FSRM adopts deep feature selection to minimize the
influence of irrelevant variables for estimating treatment effects and
incorporates a regularizer based on the Wasserstein distance to learn balanced
representations. We evaluate the performance of our FSRM method on three
datasets, and the results demonstrate superiority over the state-of-the-art
methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Chu_Z/0/1/0/all/0/1"&gt;Zhixuan Chu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Rathbun_S/0/1/0/all/0/1"&gt;Stephen L. Rathbun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Li_S/0/1/0/all/0/1"&gt;Sheng Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding Negative Samples in Instance Discriminative Self-supervised Representation Learning. (arXiv:2102.06866v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.06866</id>
        <link href="http://arxiv.org/abs/2102.06866"/>
        <updated>2021-06-08T02:20:25.537Z</updated>
        <summary type="html"><![CDATA[Instance discriminative self-supervised representation learning has been
attracted attention thanks to its unsupervised nature and informative feature
representation for downstream tasks. In practice, it commonly uses a larger
number of negative samples than the number of supervised classes. However,
there is an inconsistency in the existing analysis; theoretically, a large
number of negative samples degrade classification performance on a downstream
supervised task, while empirically, they improve the performance. We provide a
novel framework to analyze this empirical result regarding negative samples
using the coupon collector's problem. Our bound can implicitly incorporate the
supervised loss of the downstream task in the self-supervised loss by
increasing the number of negative samples. We confirm that our proposed
analysis holds on real-world benchmark datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nozawa_K/0/1/0/all/0/1"&gt;Kento Nozawa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sato_I/0/1/0/all/0/1"&gt;Issei Sato&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Overcoming the curse of dimensionality with Laplacian regularization in semi-supervised learning. (arXiv:2009.04324v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.04324</id>
        <link href="http://arxiv.org/abs/2009.04324"/>
        <updated>2021-06-08T02:20:25.531Z</updated>
        <summary type="html"><![CDATA[As annotations of data can be scarce in large-scale practical problems,
leveraging unlabelled examples is one of the most important aspects of machine
learning. This is the aim of semi-supervised learning. To benefit from the
access to unlabelled data, it is natural to diffuse smoothly knowledge of
labelled data to unlabelled one. This induces to the use of Laplacian
regularization. Yet, current implementations of Laplacian regularization suffer
from several drawbacks, notably the well-known curse of dimensionality. In this
paper, we provide a statistical analysis to overcome those issues, and unveil a
large body of spectral filtering methods that exhibit desirable behaviors. They
are implemented through (reproducing) kernel methods, for which we provide
realistic computational guidelines in order to make our method usable with
large amounts of data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Cabannes_V/0/1/0/all/0/1"&gt;Vivien Cabannes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Pillaud_Vivien_L/0/1/0/all/0/1"&gt;Loucas Pillaud-Vivien&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Bach_F/0/1/0/all/0/1"&gt;Francis Bach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Rudi_A/0/1/0/all/0/1"&gt;Alessandro Rudi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deconditional Downscaling with Gaussian Processes. (arXiv:2105.12909v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.12909</id>
        <link href="http://arxiv.org/abs/2105.12909"/>
        <updated>2021-06-08T02:20:25.525Z</updated>
        <summary type="html"><![CDATA[Refining low-resolution (LR) spatial fields with high-resolution (HR)
information is challenging as the diversity of spatial datasets often prevents
direct matching of observations. Yet, when LR samples are modeled as aggregate
conditional means of HR samples with respect to a mediating variable that is
globally observed, the recovery of the underlying fine-grained field can be
framed as taking an "inverse" of the conditional expectation, namely a
deconditioning problem. In this work, we introduce conditional mean processes
(CMP), a new class of Gaussian Processes describing conditional means. By
treating CMPs as inter-domain features of the underlying field, a posterior for
the latent field can be established as a solution to the deconditioning
problem. Furthermore, we show that this solution can be viewed as a two-staged
vector-valued kernel ridge regressor and show that it has a minimax optimal
convergence rate under mild assumptions. Lastly, we demonstrate its proficiency
in a synthetic and a real-world atmospheric field downscaling problem, showing
substantial improvements over existing methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chau_S/0/1/0/all/0/1"&gt;Siu Lun Chau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bouabid_S/0/1/0/all/0/1"&gt;Shahine Bouabid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sejdinovic_D/0/1/0/all/0/1"&gt;Dino Sejdinovic&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Learning by Self-Transition for Handling Noisy Labels. (arXiv:2012.04337v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.04337</id>
        <link href="http://arxiv.org/abs/2012.04337"/>
        <updated>2021-06-08T02:20:25.519Z</updated>
        <summary type="html"><![CDATA[Real-world data inevitably contains noisy labels, which induce the poor
generalization of deep neural networks. It is known that the network typically
begins to rapidly memorize false-labeled samples after a certain point of
training. Thus, to counter the label noise challenge, we propose a novel
self-transitional learning method called MORPH, which automatically switches
its learning phase at the transition point from seeding to evolution. In the
seeding phase, the network is updated using all the samples to collect a seed
of clean samples. Then, in the evolution phase, the network is updated using
only the set of arguably clean samples, which precisely keeps expanding by the
updated network. Thus, MORPH effectively avoids the overfitting to
false-labeled samples throughout the entire training period. Extensive
experiments using five real-world or synthetic benchmark datasets demonstrate
substantial improvements over state-of-the-art methods in terms of robustness
and efficiency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1"&gt;Hwanjun Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1"&gt;Minseok Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_D/0/1/0/all/0/1"&gt;Dongmin Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shin_Y/0/1/0/all/0/1"&gt;Yooju Shin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jae-Gil Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SAR-U-Net: squeeze-and-excitation block and atrous spatial pyramid pooling based residual U-Net for automatic liver segmentation in Computed Tomography. (arXiv:2103.06419v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.06419</id>
        <link href="http://arxiv.org/abs/2103.06419"/>
        <updated>2021-06-08T02:20:25.512Z</updated>
        <summary type="html"><![CDATA[Background and objective: In this paper, a modified U-Net based framework is
presented, which leverages techniques from Squeeze-and-Excitation (SE) block,
Atrous Spatial Pyramid Pooling (ASPP) and residual learning for accurate and
robust liver CT segmentation, and the effectiveness of the proposed method was
tested on two public datasets LiTS17 and SLiver07.

Methods: A new network architecture called SAR-U-Net was designed. Firstly,
the SE block is introduced to adaptively extract image features after each
convolution in the U-Net encoder, while suppressing irrelevant regions, and
highlighting features of specific segmentation task; Secondly, ASPP was
employed to replace the transition layer and the output layer, and acquire
multi-scale image information via different receptive fields. Thirdly, to
alleviate the degradation problem, the traditional convolution block was
replaced with the residual block and thus prompt the network to gain accuracy
from considerably increased depth.

Results: In the LiTS17 experiment, the mean values of Dice, VOE, RVD, ASD and
MSD were 95.71, 9.52, -0.84, 1.54 and 29.14, respectively. Compared with other
closely related 2D-based models, the proposed method achieved the highest
accuracy. In the experiment of the SLiver07, the mean values of Dice, VOE, RVD,
ASD and MSD were 97.31, 5.37, -1.08, 1.85 and 27.45, respectively. Compared
with other closely related models, the proposed method achieved the highest
segmentation accuracy except for the RVD.

Conclusion: The proposed model enables a great improvement on the accuracy
compared to 2D-based models, and its robustness in circumvent challenging
problems, such as small liver regions, discontinuous liver regions, and fuzzy
liver boundaries, is also well demonstrated and validated.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jinke Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lv_P/0/1/0/all/0/1"&gt;Peiqing Lv&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_H/0/1/0/all/0/1"&gt;Haiying Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shi_C/0/1/0/all/0/1"&gt;Changfa Shi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine Learning Approaches for Binary Classification to Discover Liver Diseases using Clinical Data. (arXiv:2104.12055v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.12055</id>
        <link href="http://arxiv.org/abs/2104.12055"/>
        <updated>2021-06-08T02:20:25.495Z</updated>
        <summary type="html"><![CDATA[For a medical diagnosis, health professionals use different kinds of
pathological ways to make a decision for medical reports in terms of patients
medical condition. In the modern era, because of the advantage of computers and
technologies, one can collect data and visualize many hidden outcomes from
them. Statistical machine learning algorithms based on specific problems can
assist one to make decisions. Machine learning data driven algorithms can be
used to validate existing methods and help researchers to suggest potential new
decisions. In this paper, multiple imputation by chained equations was applied
to deal with missing data, and Principal Component Analysis to reduce the
dimensionality. To reveal significant findings, data visualizations were
implemented. We presented and compared many binary classifier machine learning
algorithms (Artificial Neural Network, Random Forest, Support Vector Machine)
which were used to classify blood donors and non-blood donors with hepatitis,
fibrosis and cirrhosis diseases. From the data published in UCI-MLR [1], all
mentioned techniques were applied to find one better method to classify blood
donors and non-blood donors (hepatitis, fibrosis, and cirrhosis) that can help
health professionals in a laboratory to make better decisions. Our proposed
ML-method showed better accuracy score (e.g. 98.23% for SVM). Thus, it improved
the quality of classification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Mostafa_F/0/1/0/all/0/1"&gt;Fahad B. Mostafa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Hasan_M/0/1/0/all/0/1"&gt;Md Easin Hasan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph Learning with 1D Convolutions on Random Walks. (arXiv:2102.08786v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.08786</id>
        <link href="http://arxiv.org/abs/2102.08786"/>
        <updated>2021-06-08T02:20:25.488Z</updated>
        <summary type="html"><![CDATA[We propose CRaWl (CNNs for Random Walks), a novel neural network architecture
for graph learning. It is based on processing sequences of small subgraphs
induced by random walks with standard 1D CNNs. Thus, CRaWl is fundamentally
different from typical message passing graph neural network architectures. It
is inspired by techniques counting small subgraphs, such as the graphlet kernel
and motif counting, and combines them with random walk based techniques in a
highly efficient and scalable neural architecture. We demonstrate empirically
that CRaWl matches or outperforms state-of-the-art GNN architectures across a
multitude of benchmark datasets for classification and regression on graphs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Toenshoff_J/0/1/0/all/0/1"&gt;Jan Toenshoff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ritzert_M/0/1/0/all/0/1"&gt;Martin Ritzert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wolf_H/0/1/0/all/0/1"&gt;Hinrikus Wolf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grohe_M/0/1/0/all/0/1"&gt;Martin Grohe&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Enhancing Taxonomy Completion with Concept Generation via Fusing Relational Representations. (arXiv:2106.02974v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02974</id>
        <link href="http://arxiv.org/abs/2106.02974"/>
        <updated>2021-06-08T02:20:25.482Z</updated>
        <summary type="html"><![CDATA[Automatic construction of a taxonomy supports many applications in
e-commerce, web search, and question answering. Existing taxonomy expansion or
completion methods assume that new concepts have been accurately extracted and
their embedding vectors learned from the text corpus. However, one critical and
fundamental challenge in fixing the incompleteness of taxonomies is the
incompleteness of the extracted concepts, especially for those whose names have
multiple words and consequently low frequency in the corpus. To resolve the
limitations of extraction-based methods, we propose GenTaxo to enhance taxonomy
completion by identifying positions in existing taxonomies that need new
concepts and then generating appropriate concept names. Instead of relying on
the corpus for concept embeddings, GenTaxo learns the contextual embeddings
from their surrounding graph-based and language-based relational information,
and leverages the corpus for pre-training a concept name generator.
Experimental results demonstrate that GenTaxo improves the completeness of
taxonomies over existing methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_Q/0/1/0/all/0/1"&gt;Qingkai Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1"&gt;Jinfeng Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1"&gt;Wenhao Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cleland_Huang_J/0/1/0/all/0/1"&gt;Jane Cleland-Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1"&gt;Meng Jiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Sample Complexity of Stability Constrained Imitation Learning. (arXiv:2102.09161v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.09161</id>
        <link href="http://arxiv.org/abs/2102.09161"/>
        <updated>2021-06-08T02:20:25.476Z</updated>
        <summary type="html"><![CDATA[We study the following question in the context of imitation learning for
continuous control: how are the underlying stability properties of an expert
policy reflected in the sample-complexity of an imitation learning task? We
provide the first results showing that a surprisingly granular connection can
be made between the underlying expert system's incremental gain stability, a
novel measure of robust convergence between pairs of system trajectories, and
the dependency on the task horizon $T$ of the resulting generalization bounds.
In particular, we propose and analyze incremental gain stability constrained
versions of behavior cloning and a DAgger-like algorithm, and show that the
resulting sample-complexity bounds naturally reflect the underlying stability
properties of the expert system. As a special case, we delineate a class of
systems for which the number of trajectories needed to achieve
$\varepsilon$-suboptimality is sublinear in the task horizon $T$, and do so
without requiring (strong) convexity of the loss function in the policy
parameters. Finally, we conduct numerical experiments demonstrating the
validity of our insights on both a simple nonlinear system for which the
underlying stability properties can be easily tuned, and on a high-dimensional
quadrupedal robotic simulation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tu_S/0/1/0/all/0/1"&gt;Stephen Tu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Robey_A/0/1/0/all/0/1"&gt;Alexander Robey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1"&gt;Tingnan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Matni_N/0/1/0/all/0/1"&gt;Nikolai Matni&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Strength of Minibatch Noise in SGD. (arXiv:2102.05375v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.05375</id>
        <link href="http://arxiv.org/abs/2102.05375"/>
        <updated>2021-06-08T02:20:25.460Z</updated>
        <summary type="html"><![CDATA[The noise in stochastic gradient descent (SGD), caused by minibatch sampling,
is poorly understood despite its practical importance in deep learning. In this
work, we study the nature of SGD noise and fluctuation. We show that some
degree of mismatch between model and data complexity is needed for SGD to
``stir" a noise; such mismatch may be due to a label or input noise,
regularization, or underparametrization. Compared with previous works, the
present work focuses on deriving exactly solvable analytical results. Our work
also motivates a more accurate general formulation to describe minibatch noise,
and we show that the SGD noise takes different shapes and strengths in
different kinds of minima.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ziyin_L/0/1/0/all/0/1"&gt;Liu Ziyin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1"&gt;Kangqiao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mori_T/0/1/0/all/0/1"&gt;Takashi Mori&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ueda_M/0/1/0/all/0/1"&gt;Masahito Ueda&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exact Distribution-Free Hypothesis Tests for the Regression Function of Binary Classification via Conditional Kernel Mean Embeddings. (arXiv:2103.05126v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.05126</id>
        <link href="http://arxiv.org/abs/2103.05126"/>
        <updated>2021-06-08T02:20:25.366Z</updated>
        <summary type="html"><![CDATA[In this paper we suggest two statistical hypothesis tests for the regression
function of binary classification based on conditional kernel mean embeddings.
The regression function is a fundamental object in classification as it
determines both the Bayes optimal classifier and the misclassification
probabilities. A resampling based framework is presented and combined with
consistent point estimators of the conditional kernel mean map, in order to
construct distribution-free hypothesis tests. These tests are introduced in a
flexible manner allowing us to control the exact probability of type I error
for any sample size. We also prove that both proposed techniques are consistent
under weak statistical assumptions, i.e., the type II error probabilities
pointwise converge to zero.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Tamas_A/0/1/0/all/0/1"&gt;Ambrus Tam&amp;#xe1;s&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Csaji_B/0/1/0/all/0/1"&gt;Bal&amp;#xe1;zs Csan&amp;#xe1;d Cs&amp;#xe1;ji&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Beyond Fine-Tuning: Transferring Behavior in Reinforcement Learning. (arXiv:2102.13515v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.13515</id>
        <link href="http://arxiv.org/abs/2102.13515"/>
        <updated>2021-06-08T02:20:25.365Z</updated>
        <summary type="html"><![CDATA[Designing agents that acquire knowledge autonomously and use it to solve new
tasks efficiently is an important challenge in reinforcement learning.
Knowledge acquired during an unsupervised pre-training phase is often
transferred by fine-tuning neural network weights once rewards are exposed, as
is common practice in supervised domains. Given the nature of the reinforcement
learning problem, we argue that standard fine-tuning strategies alone are not
enough for efficient transfer in challenging domains. We introduce Behavior
Transfer (BT), a technique that leverages pre-trained policies for exploration
and that is complementary to transferring neural network weights. Our
experiments show that, when combined with large-scale pre-training in the
absence of rewards, existing intrinsic motivation objectives can lead to the
emergence of complex behaviors. These pre-trained policies can then be
leveraged by BT to discover better solutions than without pre-training, and
combining BT with standard fine-tuning strategies results in additional
benefits. The largest gains are generally observed in domains requiring
structured exploration, including settings where the behavior of the
pre-trained policies is misaligned with the downstream task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Campos_V/0/1/0/all/0/1"&gt;V&amp;#xed;ctor Campos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sprechmann_P/0/1/0/all/0/1"&gt;Pablo Sprechmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hansen_S/0/1/0/all/0/1"&gt;Steven Hansen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barreto_A/0/1/0/all/0/1"&gt;Andre Barreto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kapturowski_S/0/1/0/all/0/1"&gt;Steven Kapturowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vitvitskyi_A/0/1/0/all/0/1"&gt;Alex Vitvitskyi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Badia_A/0/1/0/all/0/1"&gt;Adri&amp;#xe0; Puigdom&amp;#xe8;nech Badia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Blundell_C/0/1/0/all/0/1"&gt;Charles Blundell&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeepCPCFG: Deep Learning and Context Free Grammars for End-to-End Information Extraction. (arXiv:2103.05908v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.05908</id>
        <link href="http://arxiv.org/abs/2103.05908"/>
        <updated>2021-06-08T02:20:25.295Z</updated>
        <summary type="html"><![CDATA[We address the challenge of extracting structured information from business
documents without detailed annotations. We propose Deep Conditional
Probabilistic Context Free Grammars (DeepCPCFG) to parse two-dimensional
complex documents and use Recursive Neural Networks to create an end-to-end
system for finding the most probable parse that represents the structured
information to be extracted. This system is trained end-to-end with scanned
documents as input and only relational-records as labels. The
relational-records are extracted from existing databases avoiding the cost of
annotating documents by hand. We apply this approach to extract information
from scanned invoices achieving state-of-the-art results despite using no
hand-annotations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chua_F/0/1/0/all/0/1"&gt;Freddy C. Chua&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duffy_N/0/1/0/all/0/1"&gt;Nigel P. Duffy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Accelerating Inference for Sparse Extreme Multi-Label Ranking Trees. (arXiv:2106.02697v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02697</id>
        <link href="http://arxiv.org/abs/2106.02697"/>
        <updated>2021-06-08T02:20:25.295Z</updated>
        <summary type="html"><![CDATA[Tree-based models underpin many modern semantic search engines and
recommender systems due to their sub-linear inference times. In industrial
applications, these models operate at extreme scales, where every bit of
performance is critical. Memory constraints at extreme scales also require that
models be sparse, hence tree-based models are often back-ended by sparse matrix
algebra routines. However, there are currently no sparse matrix techniques
specifically designed for the sparsity structure one encounters in tree-based
models for extreme multi-label ranking/classification (XMR/XMC) problems. To
address this issue, we present the masked sparse chunk multiplication (MSCM)
technique, a sparse matrix technique specifically tailored to XMR trees. MSCM
is easy to implement, embarrassingly parallelizable, and offers a significant
performance boost to any existing tree inference pipeline at no cost. We
perform a comprehensive study of MSCM applied to several different sparse
inference schemes and benchmark our methods on a general purpose extreme
multi-label ranking framework. We observe that MSCM gives consistently dramatic
speedups across both the online and batch inference settings, single- and
multi-threaded settings, and on many different tree models and datasets. To
demonstrate its utility in industrial applications, we apply MSCM to an
enterprise-scale semantic product search problem with 100 million products and
achieve sub-millisecond latency of 0.88 ms per query on a single thread -- an
8x reduction in latency over vanilla inference techniques. The MSCM technique
requires absolutely no sacrifices to model accuracy as it gives exactly the
same results as standard sparse matrix techniques. Therefore, we believe that
MSCM will enable users of XMR trees to save a substantial amount of compute
resources in their inference pipelines at very little cost.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Etter_P/0/1/0/all/0/1"&gt;Philip A. Etter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_K/0/1/0/all/0/1"&gt;Kai Zhong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1"&gt;Hsiang-Fu Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ying_L/0/1/0/all/0/1"&gt;Lexing Ying&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dhillon_I/0/1/0/all/0/1"&gt;Inderjit Dhillon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rethink the Connections among Generalization, Memorization and the Spectral Bias of DNNs. (arXiv:2004.13954v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.13954</id>
        <link href="http://arxiv.org/abs/2004.13954"/>
        <updated>2021-06-08T02:20:25.295Z</updated>
        <summary type="html"><![CDATA[Over-parameterized deep neural networks (DNNs) with sufficient capacity to
memorize random noise can achieve excellent generalization performance,
challenging the bias-variance trade-off in classical learning theory. Recent
studies claimed that DNNs first learn simple patterns and then memorize noise;
some other works showed a phenomenon that DNNs have a spectral bias to learn
target functions from low to high frequencies during training. However, we show
that the monotonicity of the learning bias does not always hold: under the
experimental setup of deep double descent, the high-frequency components of
DNNs diminish in the late stage of training, leading to the second descent of
the test error. Besides, we find that the spectrum of DNNs can be applied to
indicating the second descent of the test error, even though it is calculated
from the training set only.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1"&gt;Haoyi Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1"&gt;Dongrui Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Distributed Source Coding. (arXiv:2106.02797v1 [cs.IT])]]></title>
        <id>http://arxiv.org/abs/2106.02797</id>
        <link href="http://arxiv.org/abs/2106.02797"/>
        <updated>2021-06-08T02:20:25.294Z</updated>
        <summary type="html"><![CDATA[Distributed source coding is the task of encoding an input in the absence of
correlated side information that is only available to the decoder. Remarkably,
Slepian and Wolf showed in 1973 that an encoder that has no access to the
correlated side information can asymptotically achieve the same compression
rate as when the side information is available at both the encoder and the
decoder. While there is significant prior work on this topic in information
theory, practical distributed source coding has been limited to synthetic
datasets and specific correlation structures. Here we present a general
framework for lossy distributed source coding that is agnostic to the
correlation structure and can scale to high dimensions. Rather than relying on
hand-crafted source-modeling, our method utilizes a powerful conditional deep
generative model to learn the distributed encoder and decoder. We evaluate our
method on realistic high-dimensional datasets and show substantial improvements
in distributed compression performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Whang_J/0/1/0/all/0/1"&gt;Jay Whang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Acharya_A/0/1/0/all/0/1"&gt;Anish Acharya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1"&gt;Hyeji Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dimakis_A/0/1/0/all/0/1"&gt;Alexandros G. Dimakis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Selective Inference for Latent Block Models. (arXiv:2005.13273v5 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.13273</id>
        <link href="http://arxiv.org/abs/2005.13273"/>
        <updated>2021-06-08T02:20:25.294Z</updated>
        <summary type="html"><![CDATA[Model selection in latent block models has been a challenging but important
task in the field of statistics. Specifically, a major challenge is encountered
when constructing a test on a block structure obtained by applying a specific
clustering algorithm to a finite size matrix. In this case, it becomes crucial
to consider the selective bias in the block structure, that is, the block
structure is selected from all the possible cluster memberships based on some
criterion by the clustering algorithm. To cope with this problem, this study
provides a selective inference method for latent block models. Specifically, we
construct a statistical test on a set of row and column cluster memberships of
a latent block model, which is given by a squared residue minimization
algorithm. The proposed test, by its nature, includes and thus can also be used
as the test on the set of row and column cluster numbers. We also propose an
approximated version of the test based on simulated annealing to avoid
combinatorial explosion in searching the optimal block structure. The results
show that the proposed exact and approximated tests work effectively, compared
to the naive test that did not take the selective bias into account.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Watanabe_C/0/1/0/all/0/1"&gt;Chihiro Watanabe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Suzuki_T/0/1/0/all/0/1"&gt;Taiji Suzuki&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Web based disease prediction and recommender system. (arXiv:2106.02813v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02813</id>
        <link href="http://arxiv.org/abs/2106.02813"/>
        <updated>2021-06-08T02:20:25.285Z</updated>
        <summary type="html"><![CDATA[Worldwide, several cases go undiagnosed due to poor healthcare support in
remote areas. In this context, a centralized system is needed for effective
monitoring and analysis of the medical records. A web-based patient diagnostic
system is a central platform to store the medical history and predict the
possible disease based on the current symptoms experienced by a patient to
ensure faster and accurate diagnosis. Early disease prediction can help the
users determine the severity of the disease and take quick action. The proposed
web-based disease prediction system utilizes machine learning based
classification techniques on a data set acquired from the National Centre of
Disease Control (NCDC). $K$-nearest neighbor (K-NN), random forest and naive
bayes classification approaches are utilized and an ensemble voting algorithm
is also proposed where each classifier is assigned weights dynamically based on
the prediction confidence. The proposed system is also equipped with a
recommendation scheme to recommend the type of tests based on the existing
symptoms of the patient, so that necessary precautions can be taken. A
centralized database ensures that the medical data is preserved and there is
transparency in the system. The tampering into the system is prevented by
giving the no "updation" rights once the diagnosis is created.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rajora_H/0/1/0/all/0/1"&gt;Harish Rajora&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Punn_N/0/1/0/all/0/1"&gt;Narinder Singh Punn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sonbhadra_S/0/1/0/all/0/1"&gt;Sanjay Kumar Sonbhadra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1"&gt;Sonali Agarwal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rethinking Noisy Label Models: Labeler-Dependent Noise with Adversarial Awareness. (arXiv:2105.14083v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14083</id>
        <link href="http://arxiv.org/abs/2105.14083"/>
        <updated>2021-06-08T02:20:25.285Z</updated>
        <summary type="html"><![CDATA[Most studies on learning from noisy labels rely on unrealistic models of
i.i.d. label noise, such as class-conditional transition matrices. More recent
work on instance-dependent noise models are more realistic, but assume a single
generative process for label noise across the entire dataset. We propose a more
principled model of label noise that generalizes instance-dependent noise to
multiple labelers, based on the observation that modern datasets are typically
annotated using distributed crowdsourcing methods. Under our labeler-dependent
model, label noise manifests itself under two modalities: natural error of
good-faith labelers, and adversarial labels provided by malicious actors. We
present two adversarial attack vectors that more accurately reflect the label
noise that may be encountered in real-world settings, and demonstrate that
under our multimodal noisy labels model, state-of-the-art approaches for
learning from noisy labels are defeated by adversarial label attacks. Finally,
we propose a multi-stage, labeler-aware, model-agnostic framework that reliably
filters noisy labels by leveraging knowledge about which data partitions were
labeled by which labeler, and show that our proposed framework remains robust
even in the presence of extreme adversarial label noise.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dawson_G/0/1/0/all/0/1"&gt;Glenn Dawson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Polikar_R/0/1/0/all/0/1"&gt;Robi Polikar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Acoustic Unit Discovery by Leveraging a Language-Independent Subword Discriminative Feature Representation. (arXiv:2104.00994v2 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.00994</id>
        <link href="http://arxiv.org/abs/2104.00994"/>
        <updated>2021-06-08T02:20:25.284Z</updated>
        <summary type="html"><![CDATA[This paper tackles automatically discovering phone-like acoustic units (AUD)
from unlabeled speech data. Past studies usually proposed single-step
approaches. We propose a two-stage approach: the first stage learns a
subword-discriminative feature representation and the second stage applies
clustering to the learned representation and obtains phone-like clusters as the
discovered acoustic units. In the first stage, a recently proposed method in
the task of unsupervised subword modeling is improved by replacing a
monolingual out-of-domain (OOD) ASR system with a multilingual one to create a
subword-discriminative representation that is more language-independent. In the
second stage, segment-level k-means is adopted, and two methods to represent
the variable-length speech segments as fixed-dimension feature vectors are
compared. Experiments on a very low-resource Mboshi language corpus show that
our approach outperforms state-of-the-art AUD in both normalized mutual
information (NMI) and F-score. The multilingual ASR improved upon the
monolingual ASR in providing OOD phone labels and in estimating the phone
boundaries. A comparison of our systems with and without knowing the
ground-truth phone boundaries showed a 16% NMI performance gap, suggesting that
the current approach can significantly benefit from improved phone boundary
estimation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Feng_S/0/1/0/all/0/1"&gt;Siyuan Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zelasko_P/0/1/0/all/0/1"&gt;Piotr &amp;#x17b;elasko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Moro_Velazquez_L/0/1/0/all/0/1"&gt;Laureano Moro-Vel&amp;#xe1;zquez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Scharenborg_O/0/1/0/all/0/1"&gt;Odette Scharenborg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predify: Augmenting deep neural networks with brain-inspired predictive coding dynamics. (arXiv:2106.02749v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02749</id>
        <link href="http://arxiv.org/abs/2106.02749"/>
        <updated>2021-06-08T02:20:25.284Z</updated>
        <summary type="html"><![CDATA[Deep neural networks excel at image classification, but their performance is
far less robust to input perturbations than human perception. In this work we
explore whether this shortcoming may be partly addressed by incorporating
brain-inspired recurrent dynamics in deep convolutional networks. We take
inspiration from a popular framework in neuroscience: 'predictive coding'. At
each layer of the hierarchical model, generative feedback 'predicts' (i.e.,
reconstructs) the pattern of activity in the previous layer. The reconstruction
errors are used to iteratively update the network's representations across
timesteps, and to optimize the network's feedback weights over the natural
image dataset-a form of unsupervised training. We show that implementing this
strategy into two popular networks, VGG16 and EfficientNetB0, improves their
robustness against various corruptions. We hypothesize that other feedforward
networks could similarly benefit from the proposed framework. To promote
research in this direction, we provide an open-sourced PyTorch-based package
called Predify, which can be used to implement and investigate the impacts of
the predictive coding dynamics in any convolutional neural network.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Choksi_B/0/1/0/all/0/1"&gt;Bhavin Choksi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mozafari_M/0/1/0/all/0/1"&gt;Milad Mozafari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+OMay_C/0/1/0/all/0/1"&gt;Callum Biggs O&amp;#x27;May&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ador_B/0/1/0/all/0/1"&gt;Benjamin Ador&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alamia_A/0/1/0/all/0/1"&gt;Andrea Alamia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+VanRullen_R/0/1/0/all/0/1"&gt;Rufin VanRullen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dodrio: Exploring Transformer Models with Interactive Visualization. (arXiv:2103.14625v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.14625</id>
        <link href="http://arxiv.org/abs/2103.14625"/>
        <updated>2021-06-08T02:20:25.284Z</updated>
        <summary type="html"><![CDATA[Why do large pre-trained transformer-based models perform so well across a
wide variety of NLP tasks? Recent research suggests the key may lie in
multi-headed attention mechanism's ability to learn and represent linguistic
information. Understanding how these models represent both syntactic and
semantic knowledge is vital to investigate why they succeed and fail, what they
have learned, and how they can improve. We present Dodrio, an open-source
interactive visualization tool to help NLP researchers and practitioners
analyze attention mechanisms in transformer-based models with linguistic
knowledge. Dodrio tightly integrates an overview that summarizes the roles of
different attention heads, and detailed views that help users compare attention
weights with the syntactic structure and semantic information in the input
text. To facilitate the visual comparison of attention weights and linguistic
knowledge, Dodrio applies different graph visualization techniques to represent
attention weights scalable to longer input text. Case studies highlight how
Dodrio provides insights into understanding the attention mechanism in
transformer-based models. Dodrio is available at
https://poloclub.github.io/dodrio/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zijie J. Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Turko_R/0/1/0/all/0/1"&gt;Robert Turko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chau_D/0/1/0/all/0/1"&gt;Duen Horng Chau&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[hBert + BiasCorp -- Fighting Racism on the Web. (arXiv:2104.02242v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.02242</id>
        <link href="http://arxiv.org/abs/2104.02242"/>
        <updated>2021-06-08T02:20:25.283Z</updated>
        <summary type="html"><![CDATA[Subtle and overt racism is still present both in physical and online
communities today and has impacted many lives in different segments of the
society. In this short piece of work, we present how we're tackling this
societal issue with Natural Language Processing. We are releasing BiasCorp, a
dataset containing 139,090 comments and news segment from three specific
sources - Fox News, BreitbartNews and YouTube. The first batch (45,000 manually
annotated) is ready for publication. We are currently in the final phase of
manually labeling the remaining dataset using Amazon Mechanical Turk. BERT has
been used widely in several downstream tasks. In this work, we present hBERT,
where we modify certain layers of the pretrained BERT model with the new
Hopfield Layer. hBert generalizes well across different distributions with the
added advantage of a reduced model complexity. We are also releasing a
JavaScript library and a Chrome Extension Application, to help developers make
use of our trained model in web applications (say chat application) and for
users to identify and report racially biased contents on the web respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Onabola_O/0/1/0/all/0/1"&gt;Olawale Onabola&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1"&gt;Zhuang Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1"&gt;Yang Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Akera_B/0/1/0/all/0/1"&gt;Benjamin Akera&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ibraheem_A/0/1/0/all/0/1"&gt;Abdulrahman Ibraheem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xue_J/0/1/0/all/0/1"&gt;Jia Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1"&gt;Dianbo Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1"&gt;Yoshua Bengio&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ensemble Learning Based Classification Algorithm Recommendation. (arXiv:2101.05993v1 [cs.IR] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2101.05993</id>
        <link href="http://arxiv.org/abs/2101.05993"/>
        <updated>2021-06-08T02:20:25.282Z</updated>
        <summary type="html"><![CDATA[Recommending appropriate algorithms to a classification problem is one of the
most challenging issues in the field of data mining. The existing algorithm
recommendation models are generally constructed on only one kind of
meta-features by single learners. Considering that i) ensemble learners usually
show better performance and ii) different kinds of meta-features characterize
the classification problems in different viewpoints independently, and further
the models constructed with different sets of meta-features will be
complementary with each other and applicable for ensemble. This paper proposes
an ensemble learning-based algorithm recommendation method. To evaluate the
proposed recommendation method, extensive experiments with 13 well-known
candidate classification algorithms and five different kinds of meta-features
are conducted on 1090 benchmark classification problems. The results show the
effectiveness of the proposed ensemble learning based recommendation method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1"&gt;Guangtao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_Q/0/1/0/all/0/1"&gt;Qinbao Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1"&gt;Xiaoyan Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[OpinionRank: Extracting Ground Truth Labels from Unreliable Expert Opinions with Graph-Based Spectral Ranking. (arXiv:2102.05884v2 [cs.LG] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2102.05884</id>
        <link href="http://arxiv.org/abs/2102.05884"/>
        <updated>2021-06-08T02:20:25.282Z</updated>
        <summary type="html"><![CDATA[As larger and more comprehensive datasets become standard in contemporary
machine learning, it becomes increasingly more difficult to obtain reliable,
trustworthy label information with which to train sophisticated models. To
address this problem, crowdsourcing has emerged as a popular, inexpensive, and
efficient data mining solution for performing distributed label collection.
However, crowdsourced annotations are inherently untrustworthy, as the labels
are provided by anonymous volunteers who may have varying, unreliable
expertise. Worse yet, some participants on commonly used platforms such as
Amazon Mechanical Turk may be adversarial, and provide intentionally incorrect
label information without the end user's knowledge. We discuss three
conventional models of the label generation process, describing their
parameterizations and the model-based approaches used to solve them. We then
propose OpinionRank, a model-free, interpretable, graph-based spectral
algorithm for integrating crowdsourced annotations into reliable labels for
performing supervised or semi-supervised learning. Our experiments show that
OpinionRank performs favorably when compared against more highly parameterized
algorithms. We also show that OpinionRank is scalable to very large datasets
and numbers of label sources, and requires considerably fewer computational
resources than previous approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dawson_G/0/1/0/all/0/1"&gt;Glenn Dawson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Polikar_R/0/1/0/all/0/1"&gt;Robi Polikar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sparsification for Sums of Exponentials and its Algorithmic Applications. (arXiv:2106.02774v1 [cs.DS])]]></title>
        <id>http://arxiv.org/abs/2106.02774</id>
        <link href="http://arxiv.org/abs/2106.02774"/>
        <updated>2021-06-08T02:20:25.281Z</updated>
        <summary type="html"><![CDATA[Many works in signal processing and learning theory operate under the
assumption that the underlying model is simple, e.g. that a signal is
approximately $k$-Fourier-sparse or that a distribution can be approximated by
a mixture model that has at most $k$ components. However the problem of fitting
the parameters of such a model becomes more challenging when the
frequencies/components are too close together.

In this work we introduce new methods for sparsifying sums of exponentials
and give various algorithmic applications. First we study Fourier-sparse
interpolation without a frequency gap, where Chen et al. gave an algorithm for
finding an $\epsilon$-approximate solution which uses $k' = \mbox{poly}(k, \log
1/\epsilon)$ frequencies. Second, we study learning Gaussian mixture models in
one dimension without a separation condition. Kernel density estimators give an
$\epsilon$-approximation that uses $k' = O(k/\epsilon^2)$ components. These
methods both output models that are much more complex than what we started out
with. We show how to post-process to reduce the number of
frequencies/components down to $k' = \widetilde{O}(k)$, which is optimal up to
logarithmic factors. Moreover we give applications to model selection. In
particular, we give the first algorithms for approximately (and robustly)
determining the number of components in a Gaussian mixture model that work
without a separation condition.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jerry Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1"&gt;Allen Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moitra_A/0/1/0/all/0/1"&gt;Ankur Moitra&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Counterfactual Explanations Can Be Manipulated. (arXiv:2106.02666v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02666</id>
        <link href="http://arxiv.org/abs/2106.02666"/>
        <updated>2021-06-08T02:20:25.275Z</updated>
        <summary type="html"><![CDATA[Counterfactual explanations are emerging as an attractive option for
providing recourse to individuals adversely impacted by algorithmic decisions.
As they are deployed in critical applications (e.g. law enforcement, financial
lending), it becomes important to ensure that we clearly understand the
vulnerabilities of these methods and find ways to address them. However, there
is little understanding of the vulnerabilities and shortcomings of
counterfactual explanations. In this work, we introduce the first framework
that describes the vulnerabilities of counterfactual explanations and shows how
they can be manipulated. More specifically, we show counterfactual explanations
may converge to drastically different counterfactuals under a small
perturbation indicating they are not robust. Leveraging this insight, we
introduce a novel objective to train seemingly fair models where counterfactual
explanations find much lower cost recourse under a slight perturbation. We
describe how these models can unfairly provide low-cost recourse for specific
subgroups in the data while appearing fair to auditors. We perform experiments
on loan and violent crime prediction data sets where certain subgroups achieve
up to 20x lower cost recourse under the perturbation. These results raise
concerns regarding the dependability of current counterfactual explanation
techniques, which we hope will inspire investigations in robust counterfactual
explanations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Slack_D/0/1/0/all/0/1"&gt;Dylan Slack&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hilgard_S/0/1/0/all/0/1"&gt;Sophie Hilgard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lakkaraju_H/0/1/0/all/0/1"&gt;Himabindu Lakkaraju&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1"&gt;Sameer Singh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BalaGAN: Image Translation Between Imbalanced Domains via Cross-Modal Transfer. (arXiv:2010.02036v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.02036</id>
        <link href="http://arxiv.org/abs/2010.02036"/>
        <updated>2021-06-08T02:20:25.271Z</updated>
        <summary type="html"><![CDATA[State-of-the-art image-to-image translation methods tend to struggle in an
imbalanced domain setting, where one image domain lacks richness and diversity.
We introduce a new unsupervised translation network, BalaGAN, specifically
designed to tackle the domain imbalance problem. We leverage the latent
modalities of the richer domain to turn the image-to-image translation problem,
between two imbalanced domains, into a balanced, multi-class, and conditional
translation problem, more resembling the style transfer setting. Specifically,
we analyze the source domain and learn a decomposition of it into a set of
latent modes or classes, without any supervision. This leaves us with a
multitude of balanced cross-domain translation tasks, between all pairs of
classes, including the target domain. During inference, the trained network
takes as input a source image, as well as a reference or style image from one
of the modes as a condition, and produces an image which resembles the source
on the pixel-wise level, but shares the same mode as the reference. We show
that employing modalities within the dataset improves the quality of the
translated images, and that BalaGAN outperforms strong baselines of both
unconditioned and style-transfer-based image-to-image translation methods, in
terms of image quality and diversity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Patashnik_O/0/1/0/all/0/1"&gt;Or Patashnik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Danon_D/0/1/0/all/0/1"&gt;Dov Danon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Hao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cohen_Or_D/0/1/0/all/0/1"&gt;Daniel Cohen-Or&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Local Hyper-Flow Diffusion. (arXiv:2102.07945v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.07945</id>
        <link href="http://arxiv.org/abs/2102.07945"/>
        <updated>2021-06-08T02:20:25.270Z</updated>
        <summary type="html"><![CDATA[Recently, hypergraphs have attracted a lot of attention due to their ability
to capture complex relations among entities. The insurgence of hypergraphs has
resulted in data of increasing size and complexity that exhibit interesting
small-scale and local structure, e.g., small-scale communities and localized
node-ranking around a given set of seed nodes. Popular and principled ways to
capture the local structure are the local hypergraph clustering problem and
related seed set expansion problem. In this work, we propose the first local
diffusion method that achieves edge-size-independent Cheeger-type guarantee for
the problem of local hypergraph clustering while applying to a rich class of
higher-order relations that covers many previously studied special cases. Our
method is based on a primal-dual optimization formulation where the primal
problem has a natural network flow interpretation, and the dual problem has a
cut-based interpretation using the $\ell_2$-norm penalty on associated
cut-costs. We demonstrate the new technique is significantly better than
state-of-the-art methods on both synthetic and real-world data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fountoulakis_K/0/1/0/all/0/1"&gt;Kimon Fountoulakis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1"&gt;Pan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1"&gt;Shenghao Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learnable Uncertainty under Laplace Approximations. (arXiv:2010.02720v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.02720</id>
        <link href="http://arxiv.org/abs/2010.02720"/>
        <updated>2021-06-08T02:20:25.225Z</updated>
        <summary type="html"><![CDATA[Laplace approximations are classic, computationally lightweight means for
constructing Bayesian neural networks (BNNs). As in other approximate BNNs, one
cannot necessarily expect the induced predictive uncertainty to be calibrated.
Here we develop a formalism to explicitly "train" the uncertainty in a
decoupled way to the prediction itself. To this end, we introduce uncertainty
units for Laplace-approximated networks: Hidden units associated with a
particular weight structure that can be added to any pre-trained,
point-estimated network. Due to their weights, these units are inactive -- they
do not affect the predictions. But their presence changes the geometry (in
particular the Hessian) of the loss landscape, thereby affecting the network's
uncertainty estimates under a Laplace approximation. We show that such units
can be trained via an uncertainty-aware objective, improving standard Laplace
approximations' performance in various uncertainty quantification tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kristiadi_A/0/1/0/all/0/1"&gt;Agustinus Kristiadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hein_M/0/1/0/all/0/1"&gt;Matthias Hein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hennig_P/0/1/0/all/0/1"&gt;Philipp Hennig&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robustly Learning Mixtures of $k$ Arbitrary Gaussians. (arXiv:2012.02119v3 [cs.DS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.02119</id>
        <link href="http://arxiv.org/abs/2012.02119"/>
        <updated>2021-06-08T02:20:25.176Z</updated>
        <summary type="html"><![CDATA[We give a polynomial-time algorithm for the problem of robustly estimating a
mixture of $k$ arbitrary Gaussians in $\mathbb{R}^d$, for any fixed $k$, in the
presence of a constant fraction of arbitrary corruptions. This resolves the
main open problem in several previous works on algorithmic robust statistics,
which addressed the special cases of robustly estimating (a) a single Gaussian,
(b) a mixture of TV-distance separated Gaussians, and (c) a uniform mixture of
two Gaussians. Our main tools are an efficient \emph{partial clustering}
algorithm that relies on the sum-of-squares method, and a novel \emph{tensor
decomposition} algorithm that allows errors in both Frobenius norm and low-rank
terms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bakshi_A/0/1/0/all/0/1"&gt;Ainesh Bakshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Diakonikolas_I/0/1/0/all/0/1"&gt;Ilias Diakonikolas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_H/0/1/0/all/0/1"&gt;He Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kane_D/0/1/0/all/0/1"&gt;Daniel M. Kane&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kothari_P/0/1/0/all/0/1"&gt;Pravesh K. Kothari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vempala_S/0/1/0/all/0/1"&gt;Santosh S. Vempala&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mutual Information Regularized Identity-aware Facial ExpressionRecognition in Compressed Video. (arXiv:2010.10637v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.10637</id>
        <link href="http://arxiv.org/abs/2010.10637"/>
        <updated>2021-06-08T02:20:25.174Z</updated>
        <summary type="html"><![CDATA[How to extract effective expression representations that invariant to the
identity-specific attributes is a long-lasting problem for facial expression
recognition (FER). Most of the previous methods process the RGB images of a
sequence, while we argue that the off-the-shelf and valuable expression-related
muscle movement is already embedded in the compression format. In this paper,
we target to explore the inter-subject variations eliminated facial expression
representation in the compressed video domain. In the up to two orders of
magnitude compressed domain, we can explicitly infer the expression from the
residual frames and possibly extract identity factors from the I frame with a
pre-trained face recognition network. By enforcing the marginal independence of
them, the expression feature is expected to be purer for the expression and be
robust to identity shifts. Specifically, we propose a novel collaborative
min-min game for mutual information (MI) minimization in latent space. We do
not need the identity label or multiple expression samples from the same person
for identity elimination. Moreover, when the apex frame is annotated in the
dataset, the complementary constraint can be further added to regularize the
feature-level game. In testing, only the compressed residual frames are
required to achieve expression prediction. Our solution can achieve comparable
or better performance than the recent decoded image-based methods on the
typical FER benchmarks with about 3 times faster inference.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiaofeng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1"&gt;Linghao Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1"&gt;Xu Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+You_J/0/1/0/all/0/1"&gt;Jane You&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How Interpretable and Trustworthy are GAMs?. (arXiv:2006.06466v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.06466</id>
        <link href="http://arxiv.org/abs/2006.06466"/>
        <updated>2021-06-08T02:20:25.174Z</updated>
        <summary type="html"><![CDATA[Generalized additive models (GAMs) have become a leading modelclass for
interpretable machine learning. However, there are many algorithms for training
GAMs, and these can learn different or even contradictory models, while being
equally accurate. Which GAM should we trust? In this paper, we quantitatively
and qualitatively investigate a variety of GAM algorithms on real and simulated
datasets. We find that GAMs with high feature sparsity (only using afew
variables to make predictions) can miss patterns in the data and be unfair to
rare subpopulations. Our results suggest that inductive bias plays a crucial
role in what interpretable models learn and that tree-based GAMs represent the
best balance of sparsity, fidelity and accuracy and thus appear to be the most
trustworthy GAM.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chang_C/0/1/0/all/0/1"&gt;Chun-Hao Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_S/0/1/0/all/0/1"&gt;Sarah Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lengerich_B/0/1/0/all/0/1"&gt;Ben Lengerich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldenberg_A/0/1/0/all/0/1"&gt;Anna Goldenberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Caruana_R/0/1/0/all/0/1"&gt;Rich Caruana&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Importance Weight Estimation and Generalization in Domain Adaptation under Label Shift. (arXiv:2011.14251v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.14251</id>
        <link href="http://arxiv.org/abs/2011.14251"/>
        <updated>2021-06-08T02:20:25.174Z</updated>
        <summary type="html"><![CDATA[We study generalization under labeled shift for categorical and general
normed label spaces. We propose a series of methods to estimate the importance
weights from labeled source to unlabeled target domain and provide confidence
bounds for these estimators. We deploy these estimators and provide
generalization bounds in the unlabeled target domain.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Azizzadenesheli_K/0/1/0/all/0/1"&gt;Kamyar Azizzadenesheli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generative Replay-based Continual Zero-Shot Learning. (arXiv:2101.08894v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.08894</id>
        <link href="http://arxiv.org/abs/2101.08894"/>
        <updated>2021-06-08T02:20:25.174Z</updated>
        <summary type="html"><![CDATA[Zero-shot learning is a new paradigm to classify objects from classes that
are not available at training time. Zero-shot learning (ZSL) methods have
attracted considerable attention in recent years because of their ability to
classify unseen/novel class examples. Most of the existing approaches on ZSL
works when all the samples from seen classes are available to train the model,
which does not suit real life. In this paper, we tackle this hindrance by
developing a generative replay-based continual ZSL (GRCZSL). The proposed
method endows traditional ZSL to learn from streaming data and acquire new
knowledge without forgetting the previous tasks' gained experience. We handle
catastrophic forgetting in GRCZSL by replaying the synthetic samples of seen
classes, which have appeared in the earlier tasks. These synthetic samples are
synthesized using the trained conditional variational autoencoder (VAE) over
the immediate past task. Moreover, we only require the current and immediate
previous VAE at any time for training and testing. The proposed GRZSL method is
developed for a single-head setting of continual learning, simulating a
real-world problem setting. In this setting, task identity is given during
training but unavailable during testing. GRCZSL performance is evaluated on
five benchmark datasets for the generalized setup of ZSL with fixed and dynamic
(incremental class) settings of continual learning. The existing class setting
presented recently in the literature is not suitable for a class-incremental
setting. Therefore, this paper proposes a new setting to address this issue.
Experimental results show that the proposed method significantly outperforms
the baseline and the state-of-the-art method and makes it more suitable for
real-world applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gautam_C/0/1/0/all/0/1"&gt;Chandan Gautam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parameswaran_S/0/1/0/all/0/1"&gt;Sethupathy Parameswaran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mishra_A/0/1/0/all/0/1"&gt;Ashish Mishra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sundaram_S/0/1/0/all/0/1"&gt;Suresh Sundaram&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Solving hybrid machine learning tasks by traversing weight space geodesics. (arXiv:2106.02793v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02793</id>
        <link href="http://arxiv.org/abs/2106.02793"/>
        <updated>2021-06-08T02:20:25.173Z</updated>
        <summary type="html"><![CDATA[Machine learning problems have an intrinsic geometric structure as central
objects including a neural network's weight space and the loss function
associated with a particular task can be viewed as encoding the intrinsic
geometry of a given machine learning problem. Therefore, geometric concepts can
be applied to analyze and understand theoretical properties of machine learning
strategies as well as to develop new algorithms. In this paper, we address
three seemingly unrelated open questions in machine learning by viewing them
through a unified framework grounded in differential geometry. Specifically, we
view the weight space of a neural network as a manifold endowed with a
Riemannian metric that encodes performance on specific tasks. By defining a
metric, we can construct geodesic, minimum length, paths in weight space that
represent sets of networks of equivalent or near equivalent functional
performance on a specific task. We, then, traverse geodesic paths while
identifying networks that satisfy a second objective. Inspired by the geometric
insight, we apply our geodesic framework to 3 major applications: (i) Network
sparsification (ii) Mitigating catastrophic forgetting by constructing networks
with high performance on a series of objectives and (iii) Finding high-accuracy
paths connecting distinct local optima of deep networks in the non-convex loss
landscape. Our results are obtained on a wide range of network architectures
(MLP, VGG11/16) trained on MNIST, CIFAR-10/100. Broadly, we introduce a
geometric framework that unifies a range of machine learning objectives and
that can be applied to multiple classes of neural network architectures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Raghavan_G/0/1/0/all/0/1"&gt;Guruprasad Raghavan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thomson_M/0/1/0/all/0/1"&gt;Matt Thomson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bellman Eluder Dimension: New Rich Classes of RL Problems, and Sample-Efficient Algorithms. (arXiv:2102.00815v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.00815</id>
        <link href="http://arxiv.org/abs/2102.00815"/>
        <updated>2021-06-08T02:20:25.173Z</updated>
        <summary type="html"><![CDATA[Finding the minimal structural assumptions that empower sample-efficient
learning is one of the most important research directions in Reinforcement
Learning (RL). This paper advances our understanding of this fundamental
question by introducing a new complexity measure -- Bellman Eluder (BE)
dimension. We show that the family of RL problems of low BE dimension is
remarkably rich, which subsumes a vast majority of existing tractable RL
problems including but not limited to tabular MDPs, linear MDPs, reactive
POMDPs, low Bellman rank problems as well as low Eluder dimension problems.
This paper further designs a new optimization-based algorithm -- GOLF, and
reanalyzes a hypothesis elimination-based algorithm -- OLIVE (proposed in Jiang
et al., 2017). We prove that both algorithms learn the near-optimal policies of
low BE dimension problems in a number of samples that is polynomial in all
relevant parameters, but independent of the size of state-action space. Our
regret and sample complexity results match or improve the best existing results
for several well-known subclasses of low BE dimension problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jin_C/0/1/0/all/0/1"&gt;Chi Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1"&gt;Qinghua Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Miryoosefi_S/0/1/0/all/0/1"&gt;Sobhan Miryoosefi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Federated Accelerated Stochastic Gradient Descent. (arXiv:2006.08950v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.08950</id>
        <link href="http://arxiv.org/abs/2006.08950"/>
        <updated>2021-06-08T02:20:25.166Z</updated>
        <summary type="html"><![CDATA[We propose Federated Accelerated Stochastic Gradient Descent (FedAc), a
principled acceleration of Federated Averaging (FedAvg, also known as Local
SGD) for distributed optimization. FedAc is the first provable acceleration of
FedAvg that improves convergence speed and communication efficiency on various
types of convex functions. For example, for strongly convex and smooth
functions, when using $M$ workers, the previous state-of-the-art FedAvg
analysis can achieve a linear speedup in $M$ if given $M$ rounds of
synchronization, whereas FedAc only requires $M^{\frac{1}{3}}$ rounds.
Moreover, we prove stronger guarantees for FedAc when the objectives are
third-order smooth. Our technique is based on a potential-based perturbed
iterate analysis, a novel stability analysis of generalized accelerated SGD,
and a strategic tradeoff between acceleration and stability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1"&gt;Honglin Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1"&gt;Tengyu Ma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BiToD: A Bilingual Multi-Domain Dataset For Task-Oriented Dialogue Modeling. (arXiv:2106.02787v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02787</id>
        <link href="http://arxiv.org/abs/2106.02787"/>
        <updated>2021-06-08T02:20:25.159Z</updated>
        <summary type="html"><![CDATA[Task-oriented dialogue (ToD) benchmarks provide an important avenue to
measure progress and develop better conversational agents. However, existing
datasets for end-to-end ToD modeling are limited to a single language,
hindering the development of robust end-to-end ToD systems for multilingual
countries and regions. Here we introduce BiToD, the first bilingual
multi-domain dataset for end-to-end task-oriented dialogue modeling. BiToD
contains over 7k multi-domain dialogues (144k utterances) with a large and
realistic bilingual knowledge base. It serves as an effective benchmark for
evaluating bilingual ToD systems and cross-lingual transfer learning
approaches. We provide state-of-the-art baselines under three evaluation
settings (monolingual, bilingual, and cross-lingual). The analysis of our
baselines in different settings highlights 1) the effectiveness of training a
bilingual ToD system compared to two independent monolingual ToD systems, and
2) the potential of leveraging a bilingual knowledge base and cross-lingual
transfer learning to improve the system performance under low resource
condition.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1"&gt;Zhaojiang Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Madotto_A/0/1/0/all/0/1"&gt;Andrea Madotto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Winata_G/0/1/0/all/0/1"&gt;Genta Indra Winata&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1"&gt;Peng Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_F/0/1/0/all/0/1"&gt;Feijun Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1"&gt;Yuxiang Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_C/0/1/0/all/0/1"&gt;Chen Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fung_P/0/1/0/all/0/1"&gt;Pascale Fung&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Perfect reconstruction of sparse signals with piecewise continuous nonconvex penalties and nonconvexity control. (arXiv:1902.07436v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1902.07436</id>
        <link href="http://arxiv.org/abs/1902.07436"/>
        <updated>2021-06-08T02:20:25.147Z</updated>
        <summary type="html"><![CDATA[We consider compressed sensing formulated as a minimization problem of
nonconvex sparse penalties, Smoothly Clipped Absolute deviation (SCAD) and
Minimax Concave Penalty (MCP). The nonconvexity of these penalties is
controlled by nonconvexity parameters, and L1 penalty is contained as a limit
with respect to these parameters. The analytically derived reconstruction limit
overcomes that of L1 and the algorithmic limit in the Bayes-optimal setting,
when the nonconvexity parameters have suitable values. However, for small
nonconvexity parameters, where the reconstruction of the relatively dense
signals is theoretically guaranteed, the corresponding approximate message
passing (AMP) cannot achieve perfect reconstruction. We identify that the
shrinks in the basin of attraction to the perfect reconstruction causes the
discrepancy between the AMP and corresponding theory using state evolution. A
part of the discrepancy is resolved by introducing the control of the
nonconvexity parameters to guide the AMP trajectory to the basin of the
attraction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Sakata_A/0/1/0/all/0/1"&gt;Ayaka Sakata&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Obuchi_T/0/1/0/all/0/1"&gt;Tomoyuki Obuchi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Topology from Synthetic Data for Unsupervised Depth Completion. (arXiv:2106.02994v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02994</id>
        <link href="http://arxiv.org/abs/2106.02994"/>
        <updated>2021-06-08T02:20:25.139Z</updated>
        <summary type="html"><![CDATA[We present a method for inferring dense depth maps from images and sparse
depth measurements by leveraging synthetic data to learn the association of
sparse point clouds with dense natural shapes, and using the image as evidence
to validate the predicted depth map. Our learned prior for natural shapes uses
only sparse depth as input, not images, so the method is not affected by the
covariate shift when attempting to transfer learned models from synthetic data
to real ones. This allows us to use abundant synthetic data with ground truth
to learn the most difficult component of the reconstruction process, which is
topology estimation, and use the image to refine the prediction based on
photometric evidence. Our approach uses fewer parameters than previous methods,
yet, achieves the state of the art on both indoor and outdoor benchmark
datasets. Code available at:
https://github.com/alexklwong/learning-topology-synthetic-data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1"&gt;Alex Wong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cicek_S/0/1/0/all/0/1"&gt;Safa Cicek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1"&gt;Stefano Soatto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Linear Optimization over Wasserstein Balls. (arXiv:2004.07162v2 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.07162</id>
        <link href="http://arxiv.org/abs/2004.07162"/>
        <updated>2021-06-08T02:20:25.138Z</updated>
        <summary type="html"><![CDATA[Wasserstein balls, which contain all probability measures within a
pre-specified Wasserstein distance to a reference measure, have recently
enjoyed wide popularity in the distributionally robust optimization and machine
learning communities to formulate and solve data-driven optimization problems
with rigorous statistical guarantees. In this technical note we prove that the
Wasserstein ball is weakly compact under mild conditions, and we offer
necessary and sufficient conditions for the existence of optimal solutions. We
also characterize the sparsity of solutions if the Wasserstein ball is centred
at a discrete reference measure. In comparison with the existing literature,
which has proved similar results under different conditions, our proofs are
self-contained and shorter, yet mathematically rigorous, and our necessary and
sufficient conditions for the existence of optimal solutions are easily
verifiable in practice.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Yue_M/0/1/0/all/0/1"&gt;Man-Chung Yue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Kuhn_D/0/1/0/all/0/1"&gt;Daniel Kuhn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Wiesemann_W/0/1/0/all/0/1"&gt;Wolfram Wiesemann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Robustness of Vision Transformers to Adversarial Examples. (arXiv:2104.02610v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.02610</id>
        <link href="http://arxiv.org/abs/2104.02610"/>
        <updated>2021-06-08T02:20:25.137Z</updated>
        <summary type="html"><![CDATA[Recent advances in attention-based networks have shown that Vision
Transformers can achieve state-of-the-art or near state-of-the-art results on
many image classification tasks. This puts transformers in the unique position
of being a promising alternative to traditional convolutional neural networks
(CNNs). While CNNs have been carefully studied with respect to adversarial
attacks, the same cannot be said of Vision Transformers. In this paper, we
study the robustness of Vision Transformers to adversarial examples. Our
analyses of transformer security is divided into three parts. First, we test
the transformer under standard white-box and black-box attacks. Second, we
study the transferability of adversarial examples between CNNs and
transformers. We show that adversarial examples do not readily transfer between
CNNs and transformers. Based on this finding, we analyze the security of a
simple ensemble defense of CNNs and transformers. By creating a new attack, the
self-attention blended gradient attack, we show that such an ensemble is not
secure under a white-box adversary. However, under a black-box adversary, we
show that an ensemble can achieve unprecedented robustness without sacrificing
clean accuracy. Our analysis for this work is done using six types of white-box
attacks and two types of black-box attacks. Our study encompasses multiple
Vision Transformers, Big Transfer Models and CNN architectures trained on
CIFAR-10, CIFAR-100 and ImageNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mahmood_K/0/1/0/all/0/1"&gt;Kaleel Mahmood&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mahmood_R/0/1/0/all/0/1"&gt;Rigel Mahmood&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dijk_M/0/1/0/all/0/1"&gt;Marten van Dijk&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Hybrid Inference System for Improved Curvature Estimation in the Level-Set Method Using Machine Learning. (arXiv:2104.02951v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.02951</id>
        <link href="http://arxiv.org/abs/2104.02951"/>
        <updated>2021-06-08T02:20:25.136Z</updated>
        <summary type="html"><![CDATA[We present a novel hybrid strategy based on machine learning to improve
curvature estimation in the level-set method. The proposed inference system
couples enhanced neural networks with standard numerical schemes to compute
curvature more accurately. The core of our hybrid framework is a switching
mechanism that relies on well established numerical techniques to gauge
curvature. If the curvature magnitude is larger than a resolution-dependent
threshold, it uses a neural network to yield a better approximation. Our
networks are multilayer perceptrons fitted to synthetic data sets composed of
sinusoidal- and circular-interface samples at various configurations. To reduce
data set size and training complexity, we leverage the problem's characteristic
symmetry and build our models on just half of the curvature spectrum. These
savings lead to a powerful inference system able to outperform any of its
numerical or neural component alone. Experiments with static, smooth interfaces
show that our hybrid solver is notably superior to conventional numerical
methods in coarse grids and along steep interface regions. Compared to prior
research, we have observed outstanding gains in precision after training the
regression model with data pairs from more than a single interface type and
transforming data with specialized input preprocessing. In particular, our
findings confirm that machine learning is a promising venue for reducing or
removing mass loss in the level-set method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Larios_Cardenas_L/0/1/0/all/0/1"&gt;Luis &amp;#xc1;ngel Larios-C&amp;#xe1;rdenas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gibou_F/0/1/0/all/0/1"&gt;Frederic Gibou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Over-the-Air Adversarial Flickering Attacks against Video Recognition Networks. (arXiv:2002.05123v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.05123</id>
        <link href="http://arxiv.org/abs/2002.05123"/>
        <updated>2021-06-08T02:20:25.135Z</updated>
        <summary type="html"><![CDATA[Deep neural networks for video classification, just like image classification
networks, may be subjected to adversarial manipulation. The main difference
between image classifiers and video classifiers is that the latter usually use
temporal information contained within the video. In this work we present a
manipulation scheme for fooling video classifiers by introducing a flickering
temporal perturbation that in some cases may be unnoticeable by human observers
and is implementable in the real world. After demonstrating the manipulation of
action classification of single videos, we generalize the procedure to make
universal adversarial perturbation, achieving high fooling ratio. In addition,
we generalize the universal perturbation and produce a temporal-invariant
perturbation, which can be applied to the video without synchronizing the
perturbation to the input. The attack was implemented on several target models
and the transferability of the attack was demonstrated. These properties allow
us to bridge the gap between simulated environment and real-world application,
as will be demonstrated in this paper for the first time for an over-the-air
flickering attack.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pony_R/0/1/0/all/0/1"&gt;Roi Pony&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Naeh_I/0/1/0/all/0/1"&gt;Itay Naeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mannor_S/0/1/0/all/0/1"&gt;Shie Mannor&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fed+: A Unified Approach to Robust Personalized Federated Learning. (arXiv:2009.06303v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.06303</id>
        <link href="http://arxiv.org/abs/2009.06303"/>
        <updated>2021-06-08T02:20:25.134Z</updated>
        <summary type="html"><![CDATA[We present a class of methods for robust, personalized federated learning,
called Fed+, that unifies many federated learning algorithms. The principal
advantage of this class of methods is to better accommodate the real-world
characteristics found in federated training, such as the lack of IID data
across parties, the need for robustness to outliers or stragglers, and the
requirement to perform well on party-specific datasets. We achieve this through
a problem formulation that allows the central server to employ robust ways of
aggregating the local models while keeping the structure of local computation
intact. Without making any statistical assumption on the degree of
heterogeneity of local data across parties, we provide convergence guarantees
for Fed+ for convex and non-convex loss functions and robust aggregation. The
Fed+ theory is also equipped to handle heterogeneous computing environments
including stragglers without additional assumptions; specifically, the
convergence results cover the general setting where the number of local update
steps across parties can vary. We demonstrate the benefits of Fed+ through
extensive experiments across standard benchmark datasets as well as on a
challenging real-world problem in financial portfolio management where the
heterogeneity of party-level data can lead to training failure in standard
federated learning approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1"&gt;Pengqian Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kundu_A/0/1/0/all/0/1"&gt;Achintya Kundu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wynter_L/0/1/0/all/0/1"&gt;Laura Wynter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1"&gt;Shiau Hong Lim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Sharp Blockwise Tensor Perturbation Bound for Orthogonal Iteration. (arXiv:2008.02437v2 [math.ST] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.02437</id>
        <link href="http://arxiv.org/abs/2008.02437"/>
        <updated>2021-06-08T02:20:25.133Z</updated>
        <summary type="html"><![CDATA[In this paper, we develop novel perturbation bounds for the high-order
orthogonal iteration (HOOI) [DLDMV00b]. Under mild regularity conditions, we
establish blockwise tensor perturbation bounds for HOOI with guarantees for
both tensor reconstruction in Hilbert-Schmidt norm $\|\widehat{\bcT} - \bcT
\|_{\tHS}$ and mode-$k$ singular subspace estimation in Schatten-$q$ norm $\|
\sin \Theta (\widehat{\U}_k, \U_k) \|_q$ for any $q \geq 1$. We show the upper
bounds of mode-$k$ singular subspace estimation are unilateral and converge
linearly to a quantity characterized by blockwise errors of the perturbation
and signal strength. For the tensor reconstruction error bound, we express the
bound through a simple quantity $\xi$, which depends only on perturbation and
the multilinear rank of the underlying signal. Rate matching deterministic
lower bound for tensor reconstruction, which demonstrates the optimality of
HOOI, is also provided. Furthermore, we prove that one-step HOOI (i.e., HOOI
with only a single iteration) is also optimal in terms of tensor reconstruction
and can be used to lower the computational cost. The perturbation results are
also extended to the case that only partial modes of $\bcT$ have low-rank
structure. We support our theoretical results by extensive numerical studies.
Finally, we apply the novel perturbation bounds of HOOI on two applications,
tensor denoising and tensor co-clustering, from machine learning and
statistics, which demonstrates the superiority of the new perturbation results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Luo_Y/0/1/0/all/0/1"&gt;Yuetian Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Raskutti_G/0/1/0/all/0/1"&gt;Garvesh Raskutti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Yuan_M/0/1/0/all/0/1"&gt;Ming Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Zhang_A/0/1/0/all/0/1"&gt;Anru R. Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[T-Net: Deep Stacked Scale-Iteration Network for Image Dehazing. (arXiv:2106.02809v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02809</id>
        <link href="http://arxiv.org/abs/2106.02809"/>
        <updated>2021-06-08T02:20:25.132Z</updated>
        <summary type="html"><![CDATA[Hazy images reduce the visibility of the image content, and haze will lead to
failure in handling subsequent computer vision tasks. In this paper, we address
the problem of image dehazing by proposing a dehazing network named T-Net,
which consists of a backbone network based on the U-Net architecture and a dual
attention module. And it can achieve multi-scale feature fusion by using skip
connections with a new fusion strategy. Furthermore, by repeatedly unfolding
the plain T-Net, Stack T-Net is proposed to take advantage of the dependence of
deep features across stages via a recursive strategy. In order to reduce
network parameters, the intra-stage recursive computation of ResNet is adopted
in our Stack T-Net. And we take both the stage-wise result and the original
hazy image as input to each T-Net and finally output the prediction of clean
image. Experimental results on both synthetic and real-world images demonstrate
that our plain T-Net and the advanced Stack T-Net perform favorably against the
state-of-the-art dehazing algorithms, and show that our Stack T-Net could
further improve the dehazing effect, demonstrating the effectiveness of the
recursive strategy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1"&gt;Lirong Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yanshan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1"&gt;Kaihao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_W/0/1/0/all/0/1"&gt;Wenhan Luo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Oops I Took A Gradient: Scalable Sampling for Discrete Distributions. (arXiv:2102.04509v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.04509</id>
        <link href="http://arxiv.org/abs/2102.04509"/>
        <updated>2021-06-08T02:20:25.131Z</updated>
        <summary type="html"><![CDATA[We propose a general and scalable approximate sampling strategy for
probabilistic models with discrete variables. Our approach uses gradients of
the likelihood function with respect to its discrete inputs to propose updates
in a Metropolis-Hastings sampler. We show empirically that this approach
outperforms generic samplers in a number of difficult settings including Ising
models, Potts models, restricted Boltzmann machines, and factorial hidden
Markov models. We also demonstrate the use of our improved sampler for training
deep energy-based models on high dimensional discrete data. This approach
outperforms variational auto-encoders and existing energy-based models.
Finally, we give bounds showing that our approach is near-optimal in the class
of samplers which propose local updates.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Grathwohl_W/0/1/0/all/0/1"&gt;Will Grathwohl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Swersky_K/0/1/0/all/0/1"&gt;Kevin Swersky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hashemi_M/0/1/0/all/0/1"&gt;Milad Hashemi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duvenaud_D/0/1/0/all/0/1"&gt;David Duvenaud&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maddison_C/0/1/0/all/0/1"&gt;Chris J. Maddison&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GP-Tree: A Gaussian Process Classifier for Few-Shot Incremental Learning. (arXiv:2102.07868v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.07868</id>
        <link href="http://arxiv.org/abs/2102.07868"/>
        <updated>2021-06-08T02:20:25.130Z</updated>
        <summary type="html"><![CDATA[Gaussian processes (GPs) are non-parametric, flexible, models that work well
in many tasks. Combining GPs with deep learning methods via deep kernel
learning (DKL) is especially compelling due to the strong representational
power induced by the network. However, inference in GPs, whether with or
without DKL, can be computationally challenging on large datasets. Here, we
propose GP-Tree, a novel method for multi-class classification with Gaussian
processes and DKL. We develop a tree-based hierarchical model in which each
internal node of the tree fits a GP to the data using the P\'olya-Gamma
augmentation scheme. As a result, our method scales well with both the number
of classes and data size. We demonstrate the effectiveness of our method
against other Gaussian process training baselines, and we show how our general
GP approach achieves improved accuracy on standard incremental few-shot
learning benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Achituve_I/0/1/0/all/0/1"&gt;Idan Achituve&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Navon_A/0/1/0/all/0/1"&gt;Aviv Navon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yemini_Y/0/1/0/all/0/1"&gt;Yochai Yemini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chechik_G/0/1/0/all/0/1"&gt;Gal Chechik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fetaya_E/0/1/0/all/0/1"&gt;Ethan Fetaya&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Proof of Global Convergence of Gradient Descent for Deep ReLU Networks with Linear Widths. (arXiv:2101.09612v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.09612</id>
        <link href="http://arxiv.org/abs/2101.09612"/>
        <updated>2021-06-08T02:20:25.129Z</updated>
        <summary type="html"><![CDATA[We give a simple proof for the global convergence of gradient descent in
training deep ReLU networks with the standard square loss, and show some of its
improvements over the state-of-the-art. In particular, while prior works
require all the hidden layers to be wide with width at least $\Omega(N^8)$ ($N$
being the number of training samples), we require a single wide layer of
linear, quadratic or cubic width depending on the type of initialization.
Unlike many recent proofs based on the Neural Tangent Kernel (NTK), our proof
need not track the evolution of the entire NTK matrix, or more generally, any
quantities related to the changes of activation patterns during training.
Instead, we only need to track the evolution of the output at the last hidden
layer, which can be done much more easily thanks to the Lipschitz property of
ReLU. Some highlights of our setting: (i) all the layers are trained with
standard gradient descent, (ii) the network has standard parameterization as
opposed to the NTK one, and (iii) the network has a single wide layer as
opposed to having all wide hidden layers as in most of NTK-related results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_Q/0/1/0/all/0/1"&gt;Quynh Nguyen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Orbital MCMC. (arXiv:2010.08047v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.08047</id>
        <link href="http://arxiv.org/abs/2010.08047"/>
        <updated>2021-06-08T02:20:25.125Z</updated>
        <summary type="html"><![CDATA[Markov Chain Monte Carlo (MCMC) algorithms ubiquitously employ complex
deterministic transformations to generate proposal points that are then
filtered by the Metropolis-Hastings-Green (MHG) test. However, the condition of
the target measure invariance puts restrictions on the design of these
transformations. In this paper, we first derive the acceptance test for the
stochastic Markov kernel considering arbitrary deterministic maps as proposal
generators. When applied to the transformations with orbits of period two
(involutions), the test reduces to the MHG test. Based on the derived test we
propose two practical algorithms: one operates by constructing periodic orbits
from any diffeomorphism, another on contractions of the state space (such as
optimization trajectories). Finally, we perform an empirical study
demonstrating the practical advantages of both kernels.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Neklyudov_K/0/1/0/all/0/1"&gt;Kirill Neklyudov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Welling_M/0/1/0/all/0/1"&gt;Max Welling&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rethinking the Implementation Matters in Cooperative Multi-Agent Reinforcement Learning. (arXiv:2102.03479v11 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.03479</id>
        <link href="http://arxiv.org/abs/2102.03479"/>
        <updated>2021-06-08T02:20:25.124Z</updated>
        <summary type="html"><![CDATA[Multi-Agent Reinforcement Learning (MARL) has seen revolutionary
breakthroughs with its successful application to multi-agent cooperative tasks
such as computer games and robot swarms. QMIX, a widely popular MARL algorithm,
has been used to solve cooperative tasks, e.g. Starcraft Multi-Agent Challenge
(SMAC), Difficulty-Enhanced Predator-Prey (DEPP). Recent variants of QMIX
target relaxing the monotonicity constraint of QMIX, allowing for performance
improvement in SMAC. However, in this paper, we investigate the code-level
optimizations of these variants and the monotonicity constraint. We find that
(1) such improvements of the variants are significantly affected by various
code-level optimizations; (2) QMIX with normalized optimizations outperforms
other previous works in SMAC; (3) the monotonicity constraint may improve
sample efficiency in SMAC and DEPP. Last, a discussion with theoretical
analysis is demonstrated about why QMIX works well in SMAC. We open-source the
code at \url{https://github.com/hijkzzz/pymarl2}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1"&gt;Jian Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1"&gt;Siyang Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Harding_S/0/1/0/all/0/1"&gt;Seth Austin Harding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1"&gt;Haibin Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liao_S/0/1/0/all/0/1"&gt;Shih-wei Liao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Draw: Emergent Communication through Sketching. (arXiv:2106.02067v1 [cs.CV] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2106.02067</id>
        <link href="http://arxiv.org/abs/2106.02067"/>
        <updated>2021-06-08T02:20:25.103Z</updated>
        <summary type="html"><![CDATA[Evidence that visual communication preceded written language and provided a
basis for it goes back to prehistory, in forms such as cave and rock paintings
depicting traces of our distant ancestors. Emergent communication research has
sought to explore how agents can learn to communicate in order to
collaboratively solve tasks. Existing research has focused on language, with a
learned communication channel transmitting sequences of discrete tokens between
the agents. In this work, we explore a visual communication channel between
agents that are allowed to draw with simple strokes. Our agents are
parameterised by deep neural networks, and the drawing procedure is
differentiable, allowing for end-to-end training. In the framework of a
referential communication game, we demonstrate that agents can not only
successfully learn to communicate by drawing, but with appropriate inductive
biases, can do so in a fashion that humans can interpret. We hope to encourage
future research to consider visual communication as a more flexible and
directly interpretable alternative of training collaborative agents.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mihai_D/0/1/0/all/0/1"&gt;Daniela Mihai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hare_J/0/1/0/all/0/1"&gt;Jonathon Hare&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Theory of Reinforcement Learning with Once-per-Episode Feedback. (arXiv:2105.14363v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14363</id>
        <link href="http://arxiv.org/abs/2105.14363"/>
        <updated>2021-06-08T02:20:25.097Z</updated>
        <summary type="html"><![CDATA[We study a theory of reinforcement learning (RL) in which the learner
receives binary feedback only once at the end of an episode. While this is an
extreme test case for theory, it is also arguably more representative of
real-world applications than the traditional requirement in RL practice that
the learner receive feedback at every time step. Indeed, in many real-world
applications of reinforcement learning, such as self-driving cars and robotics,
it is easier to evaluate whether a learner's complete trajectory was either
"good" or "bad," but harder to provide a reward signal at each step. To show
that learning is possible in this more challenging setting, we study the case
where trajectory labels are generated by an unknown parametric model, and
provide a statistically and computationally efficient algorithm that achieves
sub-linear regret.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chatterji_N/0/1/0/all/0/1"&gt;Niladri S. Chatterji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pacchiano_A/0/1/0/all/0/1"&gt;Aldo Pacchiano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bartlett_P/0/1/0/all/0/1"&gt;Peter L. Bartlett&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jordan_M/0/1/0/all/0/1"&gt;Michael I. Jordan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning the Positions in CountSketch. (arXiv:2007.09890v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.09890</id>
        <link href="http://arxiv.org/abs/2007.09890"/>
        <updated>2021-06-08T02:20:25.089Z</updated>
        <summary type="html"><![CDATA[We consider sketching algorithms which first quickly compress data by
multiplication with a random sketch matrix, and then apply the sketch to
quickly solve an optimization problem, e.g., low rank approximation. In the
learning-based sketching paradigm proposed by Indyk et al. [2019], the sketch
matrix is found by choosing a random sparse matrix, e.g., the CountSketch, and
then updating the values of the non-zero entries by running gradient descent on
a training data set. Despite the growing body of work on this paradigm, a
noticeable omission is that the locations of the non-zero entries of previous
algorithms were fixed, and only their values were learned. In this work we
propose the first learning algorithm that also optimizes the locations of the
non-zero entries. We show this algorithm gives better accuracy for low rank
approximation than previous work, and apply it to other problems such as
$k$-means clustering for the first time. We show that our algorithm is provably
better in the spiked covariance model and for Zipfian matrices. We also show
the importance of the sketch monotonicity property for combining learned
sketches. Our empirical results show the importance of optimizing not only the
values of the non-zero entries but also their positions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Simin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tianrui Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vakilian_A/0/1/0/all/0/1"&gt;Ali Vakilian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wan_Y/0/1/0/all/0/1"&gt;Yulin Wan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Woodruff_D/0/1/0/all/0/1"&gt;David P. Woodruff&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generalized Jensen-Shannon Divergence Loss for Learning with Noisy Labels. (arXiv:2105.04522v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.04522</id>
        <link href="http://arxiv.org/abs/2105.04522"/>
        <updated>2021-06-08T02:20:25.078Z</updated>
        <summary type="html"><![CDATA[Prior works have found it beneficial to combine provably noise-robust loss
functions e.g., mean absolute error (MAE) with standard categorical loss
function e.g. cross entropy (CE) to improve their learnability. Here, we
propose to use Jensen-Shannon divergence as a noise-robust loss function and
show that it interestingly interpolate between CE and MAE with a controllable
mixing parameter. Furthermore, we make a crucial observation that CE exhibit
lower consistency around noisy data points. Based on this observation, we adopt
a generalized version of the Jensen-Shannon divergence for multiple
distributions to encourage consistency around data points. Using this loss
function, we show state-of-the-art results on both synthetic (CIFAR), and
real-world (WebVision) noise with varying noise rates.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Englesson_E/0/1/0/all/0/1"&gt;Erik Englesson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Azizpour_H/0/1/0/all/0/1"&gt;Hossein Azizpour&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Contextual Biasing of Language Models for Speech Recognition in Goal-Oriented Conversational Agents. (arXiv:2103.10325v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.10325</id>
        <link href="http://arxiv.org/abs/2103.10325"/>
        <updated>2021-06-08T02:20:25.064Z</updated>
        <summary type="html"><![CDATA[Goal-oriented conversational interfaces are designed to accomplish specific
tasks and typically have interactions that tend to span multiple turns adhering
to a pre-defined structure and a goal. However, conventional neural language
models (NLM) in Automatic Speech Recognition (ASR) systems are mostly trained
sentence-wise with limited context. In this paper, we explore different ways to
incorporate context into a LSTM based NLM in order to model long range
dependencies and improve speech recognition. Specifically, we use context carry
over across multiple turns and use lexical contextual cues such as system
dialog act from Natural Language Understanding (NLU) models and the user
provided structure of the chatbot. We also propose a new architecture that
utilizes context embeddings derived from BERT on sample utterances provided
during inference time. Our experiments show a word error rate (WER) relative
reduction of 7% over non-contextual utterance-level NLM rescorers on
goal-oriented audio datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shenoy_A/0/1/0/all/0/1"&gt;Ashish Shenoy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bodapati_S/0/1/0/all/0/1"&gt;Sravan Bodapati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kirchhoff_K/0/1/0/all/0/1"&gt;Katrin Kirchhoff&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PnG BERT: Augmented BERT on Phonemes and Graphemes for Neural TTS. (arXiv:2103.15060v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.15060</id>
        <link href="http://arxiv.org/abs/2103.15060"/>
        <updated>2021-06-08T02:20:25.058Z</updated>
        <summary type="html"><![CDATA[This paper introduces PnG BERT, a new encoder model for neural TTS. This
model is augmented from the original BERT model, by taking both phoneme and
grapheme representations of text as input, as well as the word-level alignment
between them. It can be pre-trained on a large text corpus in a self-supervised
manner, and fine-tuned in a TTS task. Experimental results show that a neural
TTS model using a pre-trained PnG BERT as its encoder yields more natural
prosody and more accurate pronunciation than a baseline model using only
phoneme input with no pre-training. Subjective side-by-side preference
evaluations show that raters have no statistically significant preference
between the speech synthesized using a PnG BERT and ground truth recordings
from professional speakers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1"&gt;Ye Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zen_H/0/1/0/all/0/1"&gt;Heiga Zen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1"&gt;Jonathan Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yonghui Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interpretable UAV Collision Avoidance using Deep Reinforcement Learning. (arXiv:2105.12254v2 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.12254</id>
        <link href="http://arxiv.org/abs/2105.12254"/>
        <updated>2021-06-08T02:20:25.039Z</updated>
        <summary type="html"><![CDATA[The significant components of any successful autonomous flight system are
task completion and collision avoidance. Most deep learning algorithms
successfully execute these aspects under the environment and conditions they
are trained. However, they fail when subjected to novel environments. This
paper presents an autonomous multi-rotor flight algorithm, using Deep
Reinforcement Learning augmented with Self-Attention Models, that can
effectively reason when subjected to varying inputs. In addition to their
reasoning ability, they are also interpretable, enabling it to be used under
real-world conditions. We have tested our algorithm under different weather
conditions and environments and found it robust compared to conventional Deep
Reinforcement Learning algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Thomas_D/0/1/0/all/0/1"&gt;Deepak-George Thomas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Olshanskyi_D/0/1/0/all/0/1"&gt;Daniil Olshanskyi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krueger_K/0/1/0/all/0/1"&gt;Karter Krueger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wongpiromsarn_T/0/1/0/all/0/1"&gt;Tichakorn Wongpiromsarn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jannesari_A/0/1/0/all/0/1"&gt;Ali Jannesari&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers. (arXiv:2105.15203v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.15203</id>
        <link href="http://arxiv.org/abs/2105.15203"/>
        <updated>2021-06-08T02:20:25.030Z</updated>
        <summary type="html"><![CDATA[We present SegFormer, a simple, efficient yet powerful semantic segmentation
framework which unifies Transformers with lightweight multilayer perception
(MLP) decoders. SegFormer has two appealing features: 1) SegFormer comprises a
novel hierarchically structured Transformer encoder which outputs multiscale
features. It does not need positional encoding, thereby avoiding the
interpolation of positional codes which leads to decreased performance when the
testing resolution differs from training. 2) SegFormer avoids complex decoders.
The proposed MLP decoder aggregates information from different layers, and thus
combining both local attention and global attention to render powerful
representations. We show that this simple and lightweight design is the key to
efficient segmentation on Transformers. We scale our approach up to obtain a
series of models from SegFormer-B0 to SegFormer-B5, reaching significantly
better performance and efficiency than previous counterparts. For example,
SegFormer-B4 achieves 50.3% mIoU on ADE20K with 64M parameters, being 5x
smaller and 2.2% better than the previous best method. Our best model,
SegFormer-B5, achieves 84.0% mIoU on Cityscapes validation set and shows
excellent zero-shot robustness on Cityscapes-C. Code will be released at:
github.com/NVlabs/SegFormer.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_E/0/1/0/all/0/1"&gt;Enze Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wenhai Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1"&gt;Zhiding Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1"&gt;Anima Anandkumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alvarez_J/0/1/0/all/0/1"&gt;Jose M. Alvarez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1"&gt;Ping Luo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Navigating to the Best Policy in Markov Decision Processes. (arXiv:2106.02847v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.02847</id>
        <link href="http://arxiv.org/abs/2106.02847"/>
        <updated>2021-06-08T02:20:25.022Z</updated>
        <summary type="html"><![CDATA[We investigate the classical active pure exploration problem in Markov
Decision Processes, where the agent sequentially selects actions and, from the
resulting system trajectory, aims at identifying the best policy as fast as
possible. We propose an information-theoretic lower bound on the average number
of steps required before a correct answer can be given with probability at
least $1-\delta$. This lower bound involves a non-convex optimization problem,
for which we propose a convex relaxation. We further provide an algorithm whose
sample complexity matches the relaxed lower bound up to a factor $2$. This
algorithm addresses general communicating MDPs; we propose a variant with
reduced exploration rate (and hence faster convergence) under an additional
ergodicity assumption. This work extends previous results relative to the
\emph{generative setting}~\cite{marjani2020adaptive}, where the agent could at
each step observe the random outcome of any (state, action) pair. In contrast,
we show here how to deal with the \emph{navigation constraints}. Our analysis
relies on an ergodic theorem for non-homogeneous Markov chains which we
consider of wide interest in the analysis of Markov Decision Processes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Marjani_A/0/1/0/all/0/1"&gt;Aymen Al Marjani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Garivier_A/0/1/0/all/0/1"&gt;Aur&amp;#xe9;lien Garivier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Proutiere_A/0/1/0/all/0/1"&gt;Alexandre Proutiere&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DiscoBox: Weakly Supervised Instance Segmentation and Semantic Correspondence from Box Supervision. (arXiv:2105.06464v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.06464</id>
        <link href="http://arxiv.org/abs/2105.06464"/>
        <updated>2021-06-08T02:20:25.014Z</updated>
        <summary type="html"><![CDATA[We introduce DiscoBox, a novel framework that jointly learns instance
segmentation and semantic correspondence using bounding box supervision.
Specifically, we propose a self-ensembling framework where instance
segmentation and semantic correspondence are jointly guided by a structured
teacher in addition to the bounding box supervision. The teacher is a
structured energy model incorporating a pairwise potential and a cross-image
potential to model the pairwise pixel relationships both within and across the
boxes. Minimizing the teacher energy simultaneously yields refined object masks
and dense correspondences between intra-class objects, which are taken as
pseudo-labels to supervise the task network and provide positive/negative
correspondence pairs for dense constrastive learning. We show a symbiotic
relationship where the two tasks mutually benefit from each other. Our best
model achieves 37.9% AP on COCO instance segmentation, surpassing prior weakly
supervised methods and is competitive to supervised methods. We also obtain
state of the art weakly supervised results on PASCAL VOC12 and PF-PASCAL with
real-time inference.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lan_S/0/1/0/all/0/1"&gt;Shiyi Lan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1"&gt;Zhiding Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choy_C/0/1/0/all/0/1"&gt;Christopher Choy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Radhakrishnan_S/0/1/0/all/0/1"&gt;Subhashree Radhakrishnan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1"&gt;Guilin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yuke Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Davis_L/0/1/0/all/0/1"&gt;Larry S. Davis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1"&gt;Anima Anandkumar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Price of Incentivizing Exploration: A Characterization via Thompson Sampling and Sample Complexity. (arXiv:2002.00558v4 [cs.GT] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.00558</id>
        <link href="http://arxiv.org/abs/2002.00558"/>
        <updated>2021-06-08T02:20:25.006Z</updated>
        <summary type="html"><![CDATA[We consider incentivized exploration: a version of multi-armed bandits where
the choice of arms is controlled by self-interested agents, and the algorithm
can only issue recommendations. The algorithm controls the flow of information,
and the information asymmetry can incentivize the agents to explore. Prior work
achieves optimal regret rates up to multiplicative factors that become
arbitrarily large depending on the Bayesian priors, and scale exponentially in
the number of arms. A more basic problem of sampling each arm once runs into
similar factors.

We focus on the price of incentives: the loss in performance, broadly
construed, incurred for the sake of incentive-compatibility. We prove that
Thompson Sampling, a standard bandit algorithm, is incentive-compatible if
initialized with sufficiently many data points. The performance loss due to
incentives is therefore limited to the initial rounds when these data points
are collected. The problem is largely reduced to that of sample complexity: how
many rounds are needed? We address this question, providing matching upper and
lower bounds and instantiating them in various corollaries. Typically, the
optimal sample complexity is polynomial in the number of arms and exponential
in the "strength of beliefs".]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sellke_M/0/1/0/all/0/1"&gt;Mark Sellke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Slivkins_A/0/1/0/all/0/1"&gt;Aleksandrs Slivkins&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Accelerating Stochastic Simulation with Interactive Neural Processes. (arXiv:2106.02770v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02770</id>
        <link href="http://arxiv.org/abs/2106.02770"/>
        <updated>2021-06-08T02:20:24.988Z</updated>
        <summary type="html"><![CDATA[Stochastic simulations such as large-scale, spatiotemporal, age-structured
epidemic models are computationally expensive at fine-grained resolution. We
propose Interactive Neural Process (INP), an interactive framework to
continuously learn a deep learning surrogate model and accelerate simulation.
Our framework is based on the novel integration of Bayesian active learning,
stochastic simulation and deep sequence modeling. In particular, we develop a
novel spatiotemporal neural process model to mimic the underlying process
dynamics. Our model automatically infers the latent process which describes the
intrinsic uncertainty of the simulator. This also gives rise to a new
acquisition function that can quantify the uncertainty of deep learning
predictions. We design Bayesian active learning algorithms to iteratively query
the simulator, gather more data, and continuously improve the model. We perform
theoretical analysis and demonstrate that our approach reduces sample
complexity compared with random sampling in high dimension. Empirically, we
demonstrate our framework can faithfully imitate the behavior of a complex
infectious disease simulator with a small number of examples, enabling rapid
simulation and scenario exploration.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1"&gt;Dongxia Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chinazzi_M/0/1/0/all/0/1"&gt;Matteo Chinazzi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vespignani_A/0/1/0/all/0/1"&gt;Alessandro Vespignani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1"&gt;Yi-An Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_R/0/1/0/all/0/1"&gt;Rose Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Constrained episodic reinforcement learning in concave-convex and knapsack settings. (arXiv:2006.05051v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.05051</id>
        <link href="http://arxiv.org/abs/2006.05051"/>
        <updated>2021-06-08T02:20:24.977Z</updated>
        <summary type="html"><![CDATA[We propose an algorithm for tabular episodic reinforcement learning with
constraints. We provide a modular analysis with strong theoretical guarantees
for settings with concave rewards and convex constraints, and for settings with
hard constraints (knapsacks). Most of the previous work in constrained
reinforcement learning is limited to linear constraints, and the remaining work
focuses on either the feasibility question or settings with a single episode.
Our experiments demonstrate that the proposed algorithm significantly
outperforms these approaches in existing constrained episodic environments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Brantley_K/0/1/0/all/0/1"&gt;Kiant&amp;#xe9; Brantley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dudik_M/0/1/0/all/0/1"&gt;Miroslav Dudik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lykouris_T/0/1/0/all/0/1"&gt;Thodoris Lykouris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Miryoosefi_S/0/1/0/all/0/1"&gt;Sobhan Miryoosefi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Simchowitz_M/0/1/0/all/0/1"&gt;Max Simchowitz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Slivkins_A/0/1/0/all/0/1"&gt;Aleksandrs Slivkins&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1"&gt;Wen Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Each Part Matters: Local Patterns Facilitate Cross-view Geo-localization. (arXiv:2008.11646v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.11646</id>
        <link href="http://arxiv.org/abs/2008.11646"/>
        <updated>2021-06-08T02:20:24.967Z</updated>
        <summary type="html"><![CDATA[Cross-view geo-localization is to spot images of the same geographic target
from different platforms, e.g., drone-view cameras and satellites. It is
challenging in the large visual appearance changes caused by extreme viewpoint
variations. Existing methods usually concentrate on mining the fine-grained
feature of the geographic target in the image center, but underestimate the
contextual information in neighbor areas. In this work, we argue that neighbor
areas can be leveraged as auxiliary information, enriching discriminative clues
for geolocalization. Specifically, we introduce a simple and effective deep
neural network, called Local Pattern Network (LPN), to take advantage of
contextual information in an end-to-end manner. Without using extra part
estimators, LPN adopts a square-ring feature partition strategy, which provides
the attention according to the distance to the image center. It eases the part
matching and enables the part-wise representation learning. Owing to the
square-ring partition design, the proposed LPN has good scalability to rotation
variations and achieves competitive results on three prevailing benchmarks,
i.e., University-1652, CVUSA and CVACT. Besides, we also show the proposed LPN
can be easily embedded into other frameworks to further boost performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1"&gt;Tingyu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1"&gt;Zhedong Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_C/0/1/0/all/0/1"&gt;Chenggang Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jiyong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1"&gt;Yaoqi Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_B/0/1/0/all/0/1"&gt;Bolun Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yi Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Weighting Adversarial Neural Network for Domain Adaptation in Regression. (arXiv:2006.08251v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.08251</id>
        <link href="http://arxiv.org/abs/2006.08251"/>
        <updated>2021-06-08T02:20:24.958Z</updated>
        <summary type="html"><![CDATA[We present a novel instance-based approach to handle regression tasks in the
context of supervised domain adaptation. The approach developed in this paper
relies on the assumption that the task on the target domain can be efficiently
learned by adequately reweighting the source instances during training phase.
We introduce a novel formulation of the optimization objective for domain
adaptation which relies on a discrepancy distance characterizing the difference
between domains according to a specific task and a class of hypotheses. To
solve this problem, we develop an adversarial network algorithm which learns
both the source weighting scheme and the task in one feed-forward gradient
descent. We provide numerical evidence of the relevance of the method on public
datasets for domain adaptation through reproducible experiments accessible via
an online demo interface at: https://antoinedemathelin.github.io/demo/]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mathelin_A/0/1/0/all/0/1"&gt;Antoine de Mathelin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Richard_G/0/1/0/all/0/1"&gt;Guillaume Richard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deheeger_F/0/1/0/all/0/1"&gt;Francois Deheeger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mougeot_M/0/1/0/all/0/1"&gt;Mathilde Mougeot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vayatis_N/0/1/0/all/0/1"&gt;Nicolas Vayatis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Subgroup Fairness in Two-Sided Markets. (arXiv:2106.02702v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2106.02702</id>
        <link href="http://arxiv.org/abs/2106.02702"/>
        <updated>2021-06-08T02:20:24.947Z</updated>
        <summary type="html"><![CDATA[It is well known that two-sided markets are unfair in a number of ways. For
instance, female workers at Uber earn less than their male colleagues per mile
driven. Similar observations have been made for other minority subgroups in
other two-sided markets. Here, we suggest a novel market-clearing mechanism for
two-sided markets, which promotes equalisation of the pay per hour worked
across multiple subgroups, as well as within each subgroup. In the process, we
introduce a novel notion of subgroup fairness (which we call Inter-fairness),
which can be combined with other notions of fairness within each subgroup
(called Intra-fairness), and the utility for the customers (Customer-Care) in
the objective of the market-clearing problem. While the novel non-linear terms
in the objective complicate market clearing by making the problem non-convex,
we show that a certain non-convex augmented Lagrangian relaxation can be
approximated to any precision in time polynomial in the number of market
participants using semi-definite programming. This makes it possible to
implement the market-clearing mechanism efficiently. On the example of
driver-ride assignment in an Uber-like system, we demonstrate the efficacy and
scalability of the approach, and trade-offs between Inter- and Intra-fairness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1"&gt;Quan Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marecek_J/0/1/0/all/0/1"&gt;Jakub Marecek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shorten_R/0/1/0/all/0/1"&gt;Robert N. Shorten&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the complexity of finding a local minimizer of a quadratic function over a polytope. (arXiv:2008.05558v3 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.05558</id>
        <link href="http://arxiv.org/abs/2008.05558"/>
        <updated>2021-06-08T02:20:24.927Z</updated>
        <summary type="html"><![CDATA[We show that unless P=NP, there cannot be a polynomial-time algorithm that
finds a point within Euclidean distance $c^n$ (for any constant $c \ge 0$) of a
local minimizer of an $n$-variate quadratic function over a polytope. This
result (even with $c=0$) answers a question of Pardalos and Vavasis that
appeared in 1992 on a list of seven open problems in complexity theory for
numerical optimization. Our proof technique also implies that the problem of
deciding whether a quadratic function has a local minimizer over an (unbounded)
polyhedron, and that of deciding if a quartic polynomial has a local minimizer
are NP-hard.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Ahmadi_A/0/1/0/all/0/1"&gt;Amir Ali Ahmadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jeffrey Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Efficient Algorithm For Generalized Linear Bandit: Online Stochastic Gradient Descent and Thompson Sampling. (arXiv:2006.04012v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.04012</id>
        <link href="http://arxiv.org/abs/2006.04012"/>
        <updated>2021-06-08T02:20:24.912Z</updated>
        <summary type="html"><![CDATA[We consider the contextual bandit problem, where a player sequentially makes
decisions based on past observations to maximize the cumulative reward.
Although many algorithms have been proposed for contextual bandit, most of them
rely on finding the maximum likelihood estimator at each iteration, which
requires $O(t)$ time at the $t$-th iteration and are memory inefficient. A
natural way to resolve this problem is to apply online stochastic gradient
descent (SGD) so that the per-step time and memory complexity can be reduced to
constant with respect to $t$, but a contextual bandit policy based on online
SGD updates that balances exploration and exploitation has remained elusive. In
this work, we show that online SGD can be applied to the generalized linear
bandit problem. The proposed SGD-TS algorithm, which uses a single-step SGD
update to exploit past information and uses Thompson Sampling for exploration,
achieves $\tilde{O}(\sqrt{T})$ regret with the total time complexity that
scales linearly in $T$ and $d$, where $T$ is the total number of rounds and $d$
is the number of features. Experimental results show that SGD-TS consistently
outperforms existing algorithms on both synthetic and real datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ding_Q/0/1/0/all/0/1"&gt;Qin Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1"&gt;Cho-Jui Hsieh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharpnack_J/0/1/0/all/0/1"&gt;James Sharpnack&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dodrio: Exploring Transformer Models with Interactive Visualization. (arXiv:2103.14625v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.14625</id>
        <link href="http://arxiv.org/abs/2103.14625"/>
        <updated>2021-06-08T02:20:24.906Z</updated>
        <summary type="html"><![CDATA[Why do large pre-trained transformer-based models perform so well across a
wide variety of NLP tasks? Recent research suggests the key may lie in
multi-headed attention mechanism's ability to learn and represent linguistic
information. Understanding how these models represent both syntactic and
semantic knowledge is vital to investigate why they succeed and fail, what they
have learned, and how they can improve. We present Dodrio, an open-source
interactive visualization tool to help NLP researchers and practitioners
analyze attention mechanisms in transformer-based models with linguistic
knowledge. Dodrio tightly integrates an overview that summarizes the roles of
different attention heads, and detailed views that help users compare attention
weights with the syntactic structure and semantic information in the input
text. To facilitate the visual comparison of attention weights and linguistic
knowledge, Dodrio applies different graph visualization techniques to represent
attention weights scalable to longer input text. Case studies highlight how
Dodrio provides insights into understanding the attention mechanism in
transformer-based models. Dodrio is available at
https://poloclub.github.io/dodrio/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zijie J. Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Turko_R/0/1/0/all/0/1"&gt;Robert Turko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chau_D/0/1/0/all/0/1"&gt;Duen Horng Chau&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Online Adversarial Attacks. (arXiv:2103.02014v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.02014</id>
        <link href="http://arxiv.org/abs/2103.02014"/>
        <updated>2021-06-08T02:20:24.905Z</updated>
        <summary type="html"><![CDATA[Adversarial attacks expose important vulnerabilities of deep learning models,
yet little attention has been paid to settings where data arrives as a stream.
In this paper, we formalize the online adversarial attack problem, emphasizing
two key elements found in real-world use-cases: attackers must operate under
partial knowledge of the target model, and the decisions made by the attacker
are irrevocable since they operate on a transient data stream. We first
rigorously analyze a deterministic variant of the online threat model by
drawing parallels to the well-studied $k$-secretary problem in theoretical
computer science and propose Virtual+, a simple yet practical online algorithm.
Our main theoretical result show Virtual+ yields provably the best competitive
ratio over all single-threshold algorithms for $k<5$ -- extending previous
analysis of the $k$-secretary problem. We also introduce the \textit{stochastic
$k$-secretary} -- effectively reducing online blackbox transfer attacks to a
$k$-secretary problem under noise -- and prove theoretical bounds on the
performance of \textit{any} online algorithms adapted to this setting. Finally,
we complement our theoretical results by conducting experiments on both MNIST
and CIFAR-10 with both vanilla and robust classifiers, revealing not only the
necessity of online algorithms in achieving near-optimal performance but also
the rich interplay of a given attack strategy towards online attack selection,
enabling simple strategies like FGSM to outperform classically strong whitebox
adversaries.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mladenovic_A/0/1/0/all/0/1"&gt;Andjela Mladenovic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bose_A/0/1/0/all/0/1"&gt;Avishek Joey Bose&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berard_H/0/1/0/all/0/1"&gt;Hugo Berard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hamilton_W/0/1/0/all/0/1"&gt;William L. Hamilton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lacoste_Julien_S/0/1/0/all/0/1"&gt;Simon Lacoste-Julien&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vincent_P/0/1/0/all/0/1"&gt;Pascal Vincent&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gidel_G/0/1/0/all/0/1"&gt;Gauthier Gidel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Discrete Auto-regressive Variational Attention Models for Text Modeling. (arXiv:2004.09764v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.09764</id>
        <link href="http://arxiv.org/abs/2004.09764"/>
        <updated>2021-06-08T02:20:24.903Z</updated>
        <summary type="html"><![CDATA[Variational autoencoders (VAEs) have been widely applied for text modeling.
In practice, however, they are troubled by two challenges: information
underrepresentation and posterior collapse. The former arises as only the last
hidden state of LSTM encoder is transformed into the latent space, which is
generally insufficient to summarize the data. The latter is a long-standing
problem during the training of VAEs as the optimization is trapped to a
disastrous local optimum. In this paper, we propose Discrete Auto-regressive
Variational Attention Model (DAVAM) to address the challenges. Specifically, we
introduce an auto-regressive variational attention approach to enrich the
latent space by effectively capturing the semantic dependency from the input.
We further design discrete latent space for the variational attention and
mathematically show that our model is free from posterior collapse. Extensive
experiments on language modeling tasks demonstrate the superiority of DAVAM
against several VAE counterparts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fang_X/0/1/0/all/0/1"&gt;Xianghong Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bai_H/0/1/0/all/0/1"&gt;Haoli Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jian Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zenglin Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lyu_M/0/1/0/all/0/1"&gt;Michael Lyu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+King_I/0/1/0/all/0/1"&gt;Irwin King&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Source Causal Inference Using Control Variates. (arXiv:2103.16689v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.16689</id>
        <link href="http://arxiv.org/abs/2103.16689"/>
        <updated>2021-06-08T02:20:24.903Z</updated>
        <summary type="html"><![CDATA[While many areas of machine learning have benefited from the increasing
availability of large and varied datasets, the benefit to causal inference has
been limited given the strong assumptions needed to ensure identifiability of
causal effects; these are often not satisfied in real-world datasets. For
example, many large observational datasets (e.g., case-control studies in
epidemiology, click-through data in recommender systems) suffer from selection
bias on the outcome, which makes the average treatment effect (ATE)
unidentifiable. We propose a general algorithm to estimate causal effects from
\emph{multiple} data sources, where the ATE may be identifiable only in some
datasets but not others. The key idea is to construct control variates using
the datasets in which the ATE is not identifiable. We show theoretically that
this reduces the variance of the ATE estimate. We apply this framework to
inference from observational data under outcome selection bias, assuming access
to an auxiliary small dataset from which we can obtain a consistent estimate of
the ATE. We construct a control variate by taking the difference of the odds
ratio estimates from the two datasets. Across simulations and two case studies
with real data, we show that this control variate can significantly reduce the
variance of the ATE estimate.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1"&gt;Wenshuo Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Serena Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_P/0/1/0/all/0/1"&gt;Peng Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yixin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jordan_M/0/1/0/all/0/1"&gt;Michael I. Jordan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mirror Descent Policy Optimization. (arXiv:2005.09814v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.09814</id>
        <link href="http://arxiv.org/abs/2005.09814"/>
        <updated>2021-06-08T02:20:24.895Z</updated>
        <summary type="html"><![CDATA[Mirror descent (MD), a well-known first-order method in constrained convex
optimization, has recently been shown as an important tool to analyze
trust-region algorithms in reinforcement learning (RL). However, there remains
a considerable gap between such theoretically analyzed algorithms and the ones
used in practice. Inspired by this, we propose an efficient RL algorithm,
called {\em mirror descent policy optimization} (MDPO). MDPO iteratively
updates the policy by {\em approximately} solving a trust-region problem, whose
objective function consists of two terms: a linearization of the standard RL
objective and a proximity term that restricts two consecutive policies to be
close to each other. Each update performs this approximation by taking multiple
gradient steps on this objective function. We derive {\em on-policy} and {\em
off-policy} variants of MDPO, while emphasizing important design choices
motivated by the existing theory of MD in RL. We highlight the connections
between on-policy MDPO and two popular trust-region RL algorithms: TRPO and
PPO, and show that explicitly enforcing the trust-region constraint is in fact
{\em not} a necessity for high performance gains in TRPO. We then show how the
popular soft actor-critic (SAC) algorithm can be derived by slight
modifications of off-policy MDPO. Overall, MDPO is derived from the MD
principles, offers a unified approach to viewing a number of popular RL
algorithms, and performs better than or on-par with TRPO, PPO, and SAC in a
number of continuous control tasks. Code is available at
\url{https://github.com/manantomar/Mirror-Descent-Policy-Optimization}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tomar_M/0/1/0/all/0/1"&gt;Manan Tomar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shani_L/0/1/0/all/0/1"&gt;Lior Shani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Efroni_Y/0/1/0/all/0/1"&gt;Yonathan Efroni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghavamzadeh_M/0/1/0/all/0/1"&gt;Mohammad Ghavamzadeh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Random Feature Model for Input-Output Maps between Banach Spaces. (arXiv:2005.10224v2 [math.NA] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.10224</id>
        <link href="http://arxiv.org/abs/2005.10224"/>
        <updated>2021-06-08T02:20:24.886Z</updated>
        <summary type="html"><![CDATA[Well known to the machine learning community, the random feature model is a
parametric approximation to kernel interpolation or regression methods. It is
typically used to approximate functions mapping a finite-dimensional input
space to the real line. In this paper, we instead propose a methodology for use
of the random feature model as a data-driven surrogate for operators that map
an input Banach space to an output Banach space. Although the methodology is
quite general, we consider operators defined by partial differential equations
(PDEs); here, the inputs and outputs are themselves functions, with the input
parameters being functions required to specify the problem, such as initial
data or coefficients, and the outputs being solutions of the problem. Upon
discretization, the model inherits several desirable attributes from this
infinite-dimensional viewpoint, including mesh-invariant approximation error
with respect to the true PDE solution map and the capability to be trained at
one mesh resolution and then deployed at different mesh resolutions. We view
the random feature model as a non-intrusive data-driven emulator, provide a
mathematical framework for its interpretation, and demonstrate its ability to
efficiently and accurately approximate the nonlinear parameter-to-solution maps
of two prototypical PDEs arising in physical science and engineering
applications: viscous Burgers' equation and a variable coefficient elliptic
equation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Nelsen_N/0/1/0/all/0/1"&gt;Nicholas H. Nelsen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Stuart_A/0/1/0/all/0/1"&gt;Andrew M. Stuart&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimization Induced Equilibrium Networks. (arXiv:2105.13228v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.13228</id>
        <link href="http://arxiv.org/abs/2105.13228"/>
        <updated>2021-06-08T02:20:24.875Z</updated>
        <summary type="html"><![CDATA[Implicit equilibrium models, i.e., deep neural networks (DNNs) defined by
implicit equations, have been becoming more and more attractive recently. In
this paper, we investigate an emerging question: can an implicit equilibrium
model's equilibrium point be regarded as the solution of an optimization
problem? To this end, we first decompose DNNs into a new class of unit layer
that is the proximal operator of an implicit convex function while keeping its
output unchanged. Then, the equilibrium model of the unit layer can be derived,
named Optimization Induced Equilibrium Networks (OptEq), which can be easily
extended to deep layers. The equilibrium point of OptEq can be theoretically
connected to the solution of its corresponding convex optimization problem with
explicit objectives. Based on this, we can flexibly introduce prior properties
to the equilibrium points: 1) modifying the underlying convex problems
explicitly so as to change the architectures of OptEq; and 2) merging the
information into the fixed point iteration, which guarantees to choose the
desired equilibrium point when the fixed point set is non-singleton. We show
that deep OptEq outperforms previous implicit models even with fewer
parameters. This work establishes the first step towards the
optimization-guided design of deep models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1"&gt;Xingyu Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1"&gt;Qiuhao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ling_Z/0/1/0/all/0/1"&gt;Zenan Ling&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xia Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yisen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1"&gt;Guangcan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1"&gt;Zhouchen Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Into the Unknown: Active Monitoring of Neural Networks. (arXiv:2009.06429v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.06429</id>
        <link href="http://arxiv.org/abs/2009.06429"/>
        <updated>2021-06-08T02:20:24.869Z</updated>
        <summary type="html"><![CDATA[Neural-network classifiers achieve high accuracy when predicting the class of
an input that they were trained to identify. Maintaining this accuracy in
dynamic environments, where inputs frequently fall outside the fixed set of
initially known classes, remains a challenge. The typical approach is to detect
inputs from novel classes and retrain the classifier on an augmented dataset.
However, not only the classifier but also the detection mechanism needs to
adapt in order to distinguish between newly learned and yet unknown input
classes. To address this challenge, we introduce an algorithmic framework for
active monitoring of a neural network. A monitor wrapped in our framework
operates in parallel with the neural network and interacts with a human user
via a series of interpretable labeling queries for incremental adaptation. In
addition, we propose an adaptive quantitative monitor to improve precision. An
experimental evaluation on a diverse set of benchmarks with varying numbers of
classes confirms the benefits of our active monitoring framework in dynamic
scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lukina_A/0/1/0/all/0/1"&gt;Anna Lukina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schilling_C/0/1/0/all/0/1"&gt;Christian Schilling&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Henzinger_T/0/1/0/all/0/1"&gt;Thomas A. Henzinger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Feature Mining via Attention-based BiLSTM-GCN for Human Motor Imagery Recognition. (arXiv:2005.00777v2 [eess.SP] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.00777</id>
        <link href="http://arxiv.org/abs/2005.00777"/>
        <updated>2021-06-08T02:20:24.848Z</updated>
        <summary type="html"><![CDATA[Recognition accuracy and response time are both critically essential ahead of
building practical electroencephalography (EEG) based brain-computer interface
(BCI). Recent approaches, however, have either compromised in the
classification accuracy or responding time. This paper presents a novel deep
learning approach designed towards remarkably accurate and responsive motor
imagery (MI) recognition based on scalp EEG. Bidirectional Long Short-term
Memory (BiLSTM) with the Attention mechanism manages to derive relevant
features from raw EEG signals. The connected graph convolutional neural network
(GCN) promotes the decoding performance by cooperating with the topological
structure of features, which are estimated from the overall data. The
0.4-second detection framework has shown effective and efficient prediction
based on individual and group-wise training, with 98.81% and 94.64% accuracy,
respectively, which outperformed all the state-of-the-art studies. The
introduced deep feature mining approach can precisely recognize human motion
intents from raw EEG signals, which paves the road to translate the EEG based
MI recognition to practical BCI systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Hou_Y/0/1/0/all/0/1"&gt;Yimin Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jia_S/0/1/0/all/0/1"&gt;Shuyue Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lun_X/0/1/0/all/0/1"&gt;Xiangmin Lun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shi_Y/0/1/0/all/0/1"&gt;Yan Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yang Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Temporal Spike Sequence Learning via Backpropagation for Deep Spiking Neural Networks. (arXiv:2002.10085v4 [cs.NE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.10085</id>
        <link href="http://arxiv.org/abs/2002.10085"/>
        <updated>2021-06-08T02:20:24.819Z</updated>
        <summary type="html"><![CDATA[Spiking neural networks (SNNs) are well suited for spatio-temporal learning
and implementations on energy-efficient event-driven neuromorphic processors.
However, existing SNN error backpropagation (BP) methods lack proper handling
of spiking discontinuities and suffer from low performance compared with the BP
methods for traditional artificial neural networks. In addition, a large number
of time steps are typically required to achieve decent performance, leading to
high latency and rendering spike-based computation unscalable to deep
architectures. We present a novel Temporal Spike Sequence Learning
Backpropagation (TSSL-BP) method for training deep SNNs, which breaks down
error backpropagation across two types of inter-neuron and intra-neuron
dependencies and leads to improved temporal learning precision. It captures
inter-neuron dependencies through presynaptic firing times by considering the
all-or-none characteristics of firing activities and captures intra-neuron
dependencies by handling the internal evolution of each neuronal state in time.
TSSL-BP efficiently trains deep SNNs within a much shortened temporal window of
a few steps while improving the accuracy for various image classification
datasets including CIFAR10.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Wenrui Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1"&gt;Peng Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Integrating Auxiliary Information in Self-supervised Learning. (arXiv:2106.02869v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02869</id>
        <link href="http://arxiv.org/abs/2106.02869"/>
        <updated>2021-06-08T02:20:24.807Z</updated>
        <summary type="html"><![CDATA[This paper presents to integrate the auxiliary information (e.g., additional
attributes for data such as the hashtags for Instagram images) in the
self-supervised learning process. We first observe that the auxiliary
information may bring us useful information about data structures: for
instance, the Instagram images with the same hashtags can be semantically
similar. Hence, to leverage the structural information from the auxiliary
information, we present to construct data clusters according to the auxiliary
information. Then, we introduce the Clustering InfoNCE (Cl-InfoNCE) objective
that learns similar representations for augmented variants of data from the
same cluster and dissimilar representations for data from different clusters.
Our approach contributes as follows: 1) Comparing to conventional
self-supervised representations, the auxiliary-information-infused
self-supervised representations bring the performance closer to the supervised
representations; 2) The presented Cl-InfoNCE can also work with unsupervised
constructed clusters (e.g., k-means clusters) and outperform strong
clustering-based self-supervised learning approaches, such as the Prototypical
Contrastive Learning (PCL) method; 3) We show that Cl-InfoNCE may be a better
approach to leverage the data clustering information, by comparing it to the
baseline approach - learning to predict the clustering assignments with
cross-entropy loss. For analysis, we connect the goodness of the learned
representations with the statistical relationships: i) the mutual information
between the labels and the clusters and ii) the conditional entropy of the
clusters given the labels.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tsai_Y/0/1/0/all/0/1"&gt;Yao-Hung Hubert Tsai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1"&gt;Tianqin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Weixin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liao_P/0/1/0/all/0/1"&gt;Peiyuan Liao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1"&gt;Ruslan Salakhutdinov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morency_L/0/1/0/all/0/1"&gt;Louis-Philippe Morency&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimal Bounds between $f$-Divergences and Integral Probability Metrics. (arXiv:2006.05973v3 [math.ST] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.05973</id>
        <link href="http://arxiv.org/abs/2006.05973"/>
        <updated>2021-06-08T02:20:24.801Z</updated>
        <summary type="html"><![CDATA[The families of $f$-divergences (e.g. the Kullback-Leibler divergence) and
Integral Probability Metrics (e.g. total variation distance or maximum mean
discrepancies) are widely used to quantify the similarity between probability
distributions. In this work, we systematically study the relationship between
these two families from the perspective of convex duality. Starting from a
tight variational representation of the $f$-divergence, we derive a
generalization of the moment-generating function, which we show exactly
characterizes the best lower bound of the $f$-divergence as a function of a
given IPM. Using this characterization, we obtain new bounds while also
recovering in a unified manner well-known results, such as Hoeffding's lemma,
Pinsker's inequality and its extension to subgaussian functions, and the
Hammersley-Chapman-Robbins bound. This characterization also allows us to prove
new results on topological properties of the divergence which may be of
independent interest.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Agrawal_R/0/1/0/all/0/1"&gt;Rohit Agrawal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Horel_T/0/1/0/all/0/1"&gt;Thibaut Horel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Curves for SGD on Structured Features. (arXiv:2106.02713v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.02713</id>
        <link href="http://arxiv.org/abs/2106.02713"/>
        <updated>2021-06-08T02:20:24.795Z</updated>
        <summary type="html"><![CDATA[The generalization performance of a machine learning algorithm such as a
neural network depends in a non-trivial way on the structure of the data
distribution. Models of generalization in machine learning theory often ignore
the low-dimensional structure of natural signals, either by considering
data-agnostic bounds or by studying the performance of the algorithm when
trained on uncorrelated features. To analyze the influence of data structure on
test loss dynamics, we study an exactly solveable model of stochastic gradient
descent (SGD) which predicts test loss when training on features with arbitrary
covariance structure. We solve the theory exactly for both Gaussian features
and arbitrary features and we show that the simpler Gaussian model accurately
predicts test loss of nonlinear random-feature models and deep neural networks
trained with SGD on real datasets such as MNIST and CIFAR-10. We show that
modeling the geometry of the data in the induced feature space is indeed
crucial to accurately predict the test error throughout learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Bordelon_B/0/1/0/all/0/1"&gt;Blake Bordelon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Pehlevan_C/0/1/0/all/0/1"&gt;Cengiz Pehlevan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Orthogonal Over-Parameterized Training. (arXiv:2004.04690v6 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.04690</id>
        <link href="http://arxiv.org/abs/2004.04690"/>
        <updated>2021-06-08T02:20:24.778Z</updated>
        <summary type="html"><![CDATA[The inductive bias of a neural network is largely determined by the
architecture and the training algorithm. To achieve good generalization, how to
effectively train a neural network is of great importance. We propose a novel
orthogonal over-parameterized training (OPT) framework that can provably
minimize the hyperspherical energy which characterizes the diversity of neurons
on a hypersphere. By maintaining the minimum hyperspherical energy during
training, OPT can greatly improve the empirical generalization. Specifically,
OPT fixes the randomly initialized weights of the neurons and learns an
orthogonal transformation that applies to these neurons. We consider multiple
ways to learn such an orthogonal transformation, including unrolling
orthogonalization algorithms, applying orthogonal parameterization, and
designing orthogonality-preserving gradient descent. For better scalability, we
propose the stochastic OPT which performs orthogonal transformation
stochastically for partial dimensions of neurons. Interestingly, OPT reveals
that learning a proper coordinate system for neurons is crucial to
generalization. We provide some insights on why OPT yields better
generalization. Extensive experiments validate the superiority of OPT over the
standard training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Weiyang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_R/0/1/0/all/0/1"&gt;Rongmei Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhen Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rehg_J/0/1/0/all/0/1"&gt;James M. Rehg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paull_L/0/1/0/all/0/1"&gt;Liam Paull&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_L/0/1/0/all/0/1"&gt;Li Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1"&gt;Le Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weller_A/0/1/0/all/0/1"&gt;Adrian Weller&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Expressive Power of Invariant and Equivariant Graph Neural Networks. (arXiv:2006.15646v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.15646</id>
        <link href="http://arxiv.org/abs/2006.15646"/>
        <updated>2021-06-08T02:20:24.771Z</updated>
        <summary type="html"><![CDATA[Various classes of Graph Neural Networks (GNN) have been proposed and shown
to be successful in a wide range of applications with graph structured data. In
this paper, we propose a theoretical framework able to compare the expressive
power of these GNN architectures. The current universality theorems only apply
to intractable classes of GNNs. Here, we prove the first approximation
guarantees for practical GNNs, paving the way for a better understanding of
their generalization. Our theoretical results are proved for invariant GNNs
computing a graph embedding (permutation of the nodes of the input graph does
not affect the output) and equivariant GNNs computing an embedding of the nodes
(permutation of the input permutes the output). We show that Folklore Graph
Neural Networks (FGNN), which are tensor based GNNs augmented with matrix
multiplication are the most expressive architectures proposed so far for a
given tensor order. We illustrate our results on the Quadratic Assignment
Problem (a NP-Hard combinatorial problem) by showing that FGNNs are able to
learn how to solve the problem, leading to much better average performances
than existing algorithms (based on spectral, SDP or other GNNs architectures).
On a practical side, we also implement masked tensors to handle batches of
graphs of varying sizes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Azizian_W/0/1/0/all/0/1"&gt;Wa&amp;#xef;ss Azizian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lelarge_M/0/1/0/all/0/1"&gt;Marc Lelarge&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[UFO-BLO: Unbiased First-Order Bilevel Optimization. (arXiv:2006.03631v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.03631</id>
        <link href="http://arxiv.org/abs/2006.03631"/>
        <updated>2021-06-08T02:20:24.763Z</updated>
        <summary type="html"><![CDATA[Bilevel optimization (BLO) is a popular approach with many applications
including hyperparameter optimization, neural architecture search, adversarial
robustness and model-agnostic meta-learning. However, the approach suffers from
time and memory complexity proportional to the length $r$ of its inner
optimization loop, which has led to several modifications being proposed. One
such modification is \textit{first-order} BLO (FO-BLO) which approximates
outer-level gradients by zeroing out second derivative terms, yielding
significant speed gains and requiring only constant memory as $r$ varies.
Despite FO-BLO's popularity, there is a lack of theoretical understanding of
its convergence properties. We make progress by demonstrating a rich family of
examples where FO-BLO-based stochastic optimization does not converge to a
stationary point of the BLO objective. We address this concern by proposing a
new FO-BLO-based unbiased estimate of outer-level gradients, enabling us to
theoretically guarantee this convergence, with no harm to memory and expected
time complexity. Our findings are supported by experimental results on Omniglot
and Mini-ImageNet, popular few-shot meta-learning benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Likhosherstov_V/0/1/0/all/0/1"&gt;Valerii Likhosherstov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1"&gt;Xingyou Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choromanski_K/0/1/0/all/0/1"&gt;Krzysztof Choromanski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Davis_J/0/1/0/all/0/1"&gt;Jared Davis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weller_A/0/1/0/all/0/1"&gt;Adrian Weller&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Churn Reduction via Distillation. (arXiv:2106.02654v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02654</id>
        <link href="http://arxiv.org/abs/2106.02654"/>
        <updated>2021-06-08T02:20:24.754Z</updated>
        <summary type="html"><![CDATA[In real-world systems, models are frequently updated as more data becomes
available, and in addition to achieving high accuracy, the goal is to also
maintain a low difference in predictions compared to the base model (i.e.
predictive ``churn''). If model retraining results in vastly different
behavior, then it could cause negative effects in downstream systems,
especially if this churn can be avoided with limited impact on model accuracy.
In this paper, we show an equivalence between training with distillation using
the base model as the teacher and training with an explicit constraint on the
predictive churn. We then show that distillation performs strongly for low
churn training against a number of recent baselines on a wide range of datasets
and model architectures, including fully-connected networks, convolutional
networks, and transformers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1"&gt;Heinrich Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Narasimhan_H/0/1/0/all/0/1"&gt;Harikrishna Narasimhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bahri_D/0/1/0/all/0/1"&gt;Dara Bahri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cotter_A/0/1/0/all/0/1"&gt;Andrew Cotter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rostamizadeh_A/0/1/0/all/0/1"&gt;Afshin Rostamizadeh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Low-rank Characteristic Tensor Density Estimation Part I: Foundations. (arXiv:2008.12315v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.12315</id>
        <link href="http://arxiv.org/abs/2008.12315"/>
        <updated>2021-06-08T02:20:24.737Z</updated>
        <summary type="html"><![CDATA[Effective non-parametric density estimation is a key challenge in
high-dimensional multivariate data analysis. In this paper,we propose a novel
approach that builds upon tensor factorization tools. Any multivariate density
can be represented by its characteristic function, via the Fourier transform.
If the sought density is compactly supported, then its characteristic function
can be approximated, within controllable error, by a finite tensor of leading
Fourier coefficients, whose size de-pends on the smoothness of the underlying
density. This tensor can be naturally estimated from observed realizations of
the random vector of interest, via sample averaging. In order to circumvent the
curse of dimensionality, we introduce a low-rank model of this characteristic
tensor, which significantly improves the density estimate especially for
high-dimensional data and/or in the sample-starved regime. By virtue of
uniqueness of low-rank tensor decomposition, under certain conditions, our
method enables learning the true data-generating distribution. We demonstrate
the very promising performance of the proposed method using several measured
datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Amiridi_M/0/1/0/all/0/1"&gt;Magda Amiridi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kargas_N/0/1/0/all/0/1"&gt;Nikos Kargas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Sidiropoulos_N/0/1/0/all/0/1"&gt;Nicholas D. Sidiropoulos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Variational (Gradient) Estimate of the Score Function in Energy-based Latent Variable Models. (arXiv:2010.08258v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.08258</id>
        <link href="http://arxiv.org/abs/2010.08258"/>
        <updated>2021-06-08T02:20:24.735Z</updated>
        <summary type="html"><![CDATA[The learning and evaluation of energy-based latent variable models (EBLVMs)
without any structural assumptions are highly challenging, because the true
posteriors and the partition functions in such models are generally
intractable. This paper presents variational estimates of the score function
and its gradient with respect to the model parameters in a general EBLVM,
referred to as VaES and VaGES respectively. The variational posterior is
trained to minimize a certain divergence to the true model posterior and the
bias in both estimates can be bounded by the divergence theoretically. With a
minimal model assumption, VaES and VaGES can be applied to the kernelized Stein
discrepancy (KSD) and score matching (SM)-based methods to learn EBLVMs.
Besides, VaES can also be used to estimate the exact Fisher divergence between
the data and general EBLVMs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bao_F/0/1/0/all/0/1"&gt;Fan Bao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1"&gt;Kun Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chongxuan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hong_L/0/1/0/all/0/1"&gt;Lanqing Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jun Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1"&gt;Bo Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GraphMI: Extracting Private Graph Data from Graph Neural Networks. (arXiv:2106.02820v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02820</id>
        <link href="http://arxiv.org/abs/2106.02820"/>
        <updated>2021-06-08T02:20:24.729Z</updated>
        <summary type="html"><![CDATA[As machine learning becomes more widely used for critical applications, the
need to study its implications in privacy turns to be urgent. Given access to
the target model and auxiliary information, the model inversion attack aims to
infer sensitive features of the training dataset, which leads to great privacy
concerns. Despite its success in grid-like domains, directly applying model
inversion techniques on non-grid domains such as graph achieves poor attack
performance due to the difficulty to fully exploit the intrinsic properties of
graphs and attributes of nodes used in Graph Neural Networks (GNN). To bridge
this gap, we present \textbf{Graph} \textbf{M}odel \textbf{I}nversion attack
(GraphMI), which aims to extract private graph data of the training graph by
inverting GNN, one of the state-of-the-art graph analysis tools. Specifically,
we firstly propose a projected gradient module to tackle the discreteness of
graph edges while preserving the sparsity and smoothness of graph features.
Then we design a graph auto-encoder module to efficiently exploit graph
topology, node attributes, and target model parameters for edge inference. With
the proposed methods, we study the connection between model inversion risk and
edge influence and show that edges with greater influence are more likely to be
recovered. Extensive experiments over several public datasets demonstrate the
effectiveness of our method. We also show that differential privacy in its
canonical form can hardly defend our attack while preserving decent utility.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zaixi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1"&gt;Qi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zhenya Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1"&gt;Chengqiang Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1"&gt;Chuanren Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_E/0/1/0/all/0/1"&gt;Enhong Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Modular Analysis of Provable Acceleration via Polyak's Momentum: Training a Wide ReLU Network and a Deep Linear Network. (arXiv:2010.01618v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.01618</id>
        <link href="http://arxiv.org/abs/2010.01618"/>
        <updated>2021-06-08T02:20:24.710Z</updated>
        <summary type="html"><![CDATA[Incorporating a so-called "momentum" dynamic in gradient descent methods is
widely used in neural net training as it has been broadly observed that, at
least empirically, it often leads to significantly faster convergence. At the
same time, there are very few theoretical guarantees in the literature to
explain this apparent acceleration effect. Even for the classical strongly
convex quadratic problems, several existing results only show Polyak's momentum
has an accelerated linear rate asymptotically. In this paper, we first revisit
the quadratic problems and show a non-asymptotic accelerated linear rate of
Polyak's momentum. Then, we provably show that Polyak's momentum achieves
acceleration for training a one-layer wide ReLU network and a deep linear
network, which are perhaps the two most popular canonical models for studying
optimization and deep learning in the literature. Prior work Du at al. 2019 and
Wu et al. 2019 showed that using vanilla gradient descent, and with an use of
over-parameterization, the error decays as $(1- \Theta(\frac{1}{ \kappa'}))^t$
after $t$ iterations, where $\kappa'$ is the condition number of a Gram Matrix.
Our result shows that with the appropriate choice of parameters Polyak's
momentum has a rate of $(1-\Theta(\frac{1}{\sqrt{\kappa'}}))^t$. For the deep
linear network, prior work Hu et al. 2020 showed that vanilla gradient descent
has a rate of $(1-\Theta(\frac{1}{\kappa}))^t$, where $\kappa$ is the condition
number of a data matrix. Our result shows an acceleration rate $(1-
\Theta(\frac{1}{\sqrt{\kappa}}))^t$ is achievable by Polyak's momentum. All the
results in this work are obtained from a modular analysis, which can be of
independent interest. This work establishes that momentum does indeed speed up
neural net training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jun-Kun Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1"&gt;Chi-Heng Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abernethy_J/0/1/0/all/0/1"&gt;Jacob Abernethy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Custom 7nm CMOS Standard Cell Library for Implementing TNN-based Neuromorphic Processors. (arXiv:2012.05419v2 [cs.AR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.05419</id>
        <link href="http://arxiv.org/abs/2012.05419"/>
        <updated>2021-06-08T02:20:24.710Z</updated>
        <summary type="html"><![CDATA[A set of highly-optimized custom macro extensions is developed for a 7nm CMOS
cell library for implementing Temporal Neural Networks (TNNs) that can mimic
brain-like sensory processing with extreme energy efficiency. A TNN prototype
(13,750 neurons and 315,000 synapses) for MNIST requires only 1.56mm2 die area
and consumes only 1.69mW.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nair_H/0/1/0/all/0/1"&gt;Harideep Nair&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vellaisamy_P/0/1/0/all/0/1"&gt;Prabhu Vellaisamy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhasuthkar_S/0/1/0/all/0/1"&gt;Santha Bhasuthkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1"&gt;John Paul Shen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Characterizing the Latent Space of Molecular Deep Generative Models with Persistent Homology Metrics. (arXiv:2010.08548v2 [q-bio.BM] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.08548</id>
        <link href="http://arxiv.org/abs/2010.08548"/>
        <updated>2021-06-08T02:20:24.709Z</updated>
        <summary type="html"><![CDATA[Deep generative models are increasingly becoming integral parts of the in
silico molecule design pipeline and have dual goals of learning the chemical
and structural features that render candidate molecules viable while also being
flexible enough to generate novel designs. Specifically, Variational Auto
Encoders (VAEs) are generative models in which encoder-decoder network pairs
are trained to reconstruct training data distributions in such a way that the
latent space of the encoder network is smooth. Therefore, novel candidates can
be found by sampling from this latent space. However, the scope of
architectures and hyperparameters is vast and choosing the best combination for
in silico discovery has important implications for downstream success.
Therefore, it is important to develop a principled methodology for
distinguishing how well a given generative model is able to learn salient
molecular features. In this work, we propose a method for measuring how well
the latent space of deep generative models is able to encode structural and
chemical features of molecular datasets by correlating latent space metrics
with metrics from the field of topological data analysis (TDA). We apply our
evaluation methodology to a VAE trained on SMILES strings and show that 3D
topology information is consistently encoded throughout the latent space of the
model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Schiff_Y/0/1/0/all/0/1"&gt;Yair Schiff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Chenthamarakshan_V/0/1/0/all/0/1"&gt;Vijil Chenthamarakshan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Ramamurthy_K/0/1/0/all/0/1"&gt;Karthikeyan Natesan Ramamurthy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Das_P/0/1/0/all/0/1"&gt;Payel Das&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How to Decompose a Tensor with Group Structure. (arXiv:2106.02680v1 [cs.DS])]]></title>
        <id>http://arxiv.org/abs/2106.02680</id>
        <link href="http://arxiv.org/abs/2106.02680"/>
        <updated>2021-06-08T02:20:24.708Z</updated>
        <summary type="html"><![CDATA[In this work we study the orbit recovery problem, which is a natural
abstraction for the problem of recovering a planted signal from noisy
measurements under unknown group actions. Many important inverse problems in
statistics, engineering and the sciences fit into this framework. Prior work
has studied cases when the group is discrete and/or abelian. However
fundamentally new techniques are needed in order to handle more complex group
actions.

Our main result is a quasi-polynomial time algorithm to solve orbit recovery
over $SO(3)$ - i.e. the cryo-electron tomography problem which asks to recover
the three-dimensional structure of a molecule from noisy measurements of
randomly rotated copies of it. We analyze a variant of the frequency marching
heuristic in the framework of smoothed analysis. Our approach exploits the
layered structure of the invariant polynomials, and simultaneously yields a new
class of tensor decomposition algorithms that work in settings when the tensor
is not low-rank but rather where the factors are algebraically related to each
other by a group action.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1"&gt;Allen Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moitra_A/0/1/0/all/0/1"&gt;Ankur Moitra&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Contextual Bandit Bake-off. (arXiv:1802.04064v5 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1802.04064</id>
        <link href="http://arxiv.org/abs/1802.04064"/>
        <updated>2021-06-08T02:20:24.699Z</updated>
        <summary type="html"><![CDATA[Contextual bandit algorithms are essential for solving many real-world
interactive machine learning problems. Despite multiple recent successes on
statistically and computationally efficient methods, the practical behavior of
these algorithms is still poorly understood. We leverage the availability of
large numbers of supervised learning datasets to empirically evaluate
contextual bandit algorithms, focusing on practical methods that learn by
relying on optimization oracles from supervised learning. We find that a recent
method (Foster et al., 2018) using optimism under uncertainty works the best
overall. A surprisingly close second is a simple greedy baseline that only
explores implicitly through the diversity of contexts, followed by a variant of
Online Cover (Agarwal et al., 2014) which tends to be more conservative but
robust to problem specification by design. Along the way, we also evaluate
various components of contextual bandit algorithm design such as loss
estimators. Overall, this is a thorough study and review of contextual bandit
methodology.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Bietti_A/0/1/0/all/0/1"&gt;Alberto Bietti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Agarwal_A/0/1/0/all/0/1"&gt;Alekh Agarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Langford_J/0/1/0/all/0/1"&gt;John Langford&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Layered gradient accumulation and modular pipeline parallelism: fast and efficient training of large language models. (arXiv:2106.02679v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02679</id>
        <link href="http://arxiv.org/abs/2106.02679"/>
        <updated>2021-06-08T02:20:24.693Z</updated>
        <summary type="html"><![CDATA[The advent of the transformer has sparked a quick growth in the size of
language models, far outpacing hardware improvements. (Dense) transformers are
expected to reach the trillion-parameter scale in the near future, for which
training requires thousands or even tens of thousands of GPUs. We investigate
the challenges of training at this scale and beyond on commercially available
hardware. In particular, we analyse the shortest possible training time for
different configurations of distributed training, leveraging empirical scaling
laws for language models to estimate the optimal (critical) batch size.
Contrary to popular belief, we find no evidence for a memory wall, and instead
argue that the real limitation -- other than the cost -- lies in the training
duration.

In addition to this analysis, we introduce two new methods, \textit{layered
gradient accumulation} and \textit{modular pipeline parallelism}, which
together cut the shortest training time by half. The methods also reduce data
movement, lowering the network requirement to a point where a fast InfiniBand
connection is not necessary. This increased network efficiency also improve on
the methods introduced with the ZeRO optimizer, reducing the memory usage to a
tiny fraction of the available GPU memory.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lamy_Poirier_J/0/1/0/all/0/1"&gt;Joel Lamy-Poirier&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sequential Neural Posterior and Likelihood Approximation. (arXiv:2102.06522v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.06522</id>
        <link href="http://arxiv.org/abs/2102.06522"/>
        <updated>2021-06-08T02:20:24.668Z</updated>
        <summary type="html"><![CDATA[We introduce the sequential neural posterior and likelihood approximation
(SNPLA) algorithm. SNPLA is a normalizing flows-based algorithm for inference
in implicit models, and therefore is a simulation-based inference method that
only requires simulations from a generative model. SNPLA avoids Markov chain
Monte Carlo sampling and correction-steps of the parameter proposal function
that are introduced in similar methods, but that can be numerically unstable or
restrictive. By utilizing the reverse KL divergence, SNPLA manages to learn
both the likelihood and the posterior in a sequential manner. Over four
experiments, we show that SNPLA performs competitively when utilizing the same
number of model simulations as used in other methods, even though the inference
problem for SNPLA is more complex due to the joint learning of posterior and
likelihood function. Due to utilizing normalizing flows SNPLA generates
posterior draws much faster (4 orders of magnitude) than MCMC-based methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Wiqvist_S/0/1/0/all/0/1"&gt;Samuel Wiqvist&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Frellsen_J/0/1/0/all/0/1"&gt;Jes Frellsen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Picchini_U/0/1/0/all/0/1"&gt;Umberto Picchini&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PALI at SemEval-2021 Task 2: Fine-Tune XLM-RoBERTa for Word in Context Disambiguation. (arXiv:2104.10375v2 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.10375</id>
        <link href="http://arxiv.org/abs/2104.10375"/>
        <updated>2021-06-08T02:20:24.658Z</updated>
        <summary type="html"><![CDATA[This paper presents the PALI team's winning system for SemEval-2021 Task 2:
Multilingual and Cross-lingual Word-in-Context Disambiguation. We fine-tune
XLM-RoBERTa model to solve the task of word in context disambiguation, i.e., to
determine whether the target word in the two contexts contains the same meaning
or not. In the implementation, we first specifically design an input tag to
emphasize the target word in the contexts. Second, we construct a new vector on
the fine-tuned embeddings from XLM-RoBERTa and feed it to a fully-connected
network to output the probability of whether the target word in the context has
the same meaning or not. The new vector is attained by concatenating the
embedding of the [CLS] token and the embeddings of the target word in the
contexts. In training, we explore several tricks, such as the Ranger optimizer,
data augmentation, and adversarial training, to improve the model prediction.
Consequently, we attain first place in all four cross-lingual tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1"&gt;Shuyi Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1"&gt;Jian Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Haiqin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1"&gt;Lianxin Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mo_Y/0/1/0/all/0/1"&gt;Yang Mo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1"&gt;Jianping Shen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimal Beam Association for High Mobility mmWave Vehicular Networks: Lightweight Parallel Reinforcement Learning Approach. (arXiv:2005.00694v2 [cs.NI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.00694</id>
        <link href="http://arxiv.org/abs/2005.00694"/>
        <updated>2021-06-08T02:20:24.658Z</updated>
        <summary type="html"><![CDATA[In intelligent transportation systems (ITS), vehicles are expected to feature
with advanced applications and services which demand ultra-high data rates and
low-latency communications. For that, the millimeter wave (mmWave)
communication has been emerging as a very promising solution. However,
incorporating the mmWave into ITS is particularly challenging due to the high
mobility of vehicles and the inherent sensitivity of mmWave beams to dynamic
blockages. This article addresses these problems by developing an optimal beam
association framework for mmWave vehicular networks under high mobility.
Specifically, we use the semi-Markov decision process to capture the dynamics
and uncertainty of the environment. The Q-learning algorithm is then often used
to find the optimal policy. However, Q-learning is notorious for its
slow-convergence. Instead of adopting deep reinforcement learning structures
(like most works in the literature), we leverage the fact that there are
usually multiple vehicles on the road to speed up the learning process. To that
end, we develop a lightweight yet very effective parallel Q-learning algorithm
to quickly obtain the optimal policy by simultaneously learning from various
vehicles. Extensive simulations demonstrate that our proposed solution can
increase the data rate by 47% and reduce the disconnection probability by 29%
compared to other solutions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huynh_N/0/1/0/all/0/1"&gt;Nguyen Van Huynh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1"&gt;Diep N. Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoang_D/0/1/0/all/0/1"&gt;Dinh Thai Hoang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dutkiewicz_E/0/1/0/all/0/1"&gt;Eryk Dutkiewicz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Group Invariant Dictionary Learning. (arXiv:2007.07550v2 [eess.SP] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.07550</id>
        <link href="http://arxiv.org/abs/2007.07550"/>
        <updated>2021-06-08T02:20:24.657Z</updated>
        <summary type="html"><![CDATA[The dictionary learning problem concerns the task of representing data as
sparse linear sums drawn from a smaller collection of basic building blocks. In
application domains where such techniques are deployed, we frequently encounter
datasets where some form of symmetry or invariance is present. Motivated by
this observation, we develop a framework for learning dictionaries for data
under the constraint that the collection of basic building blocks remains
invariant under such symmetries. Our procedure for learning such dictionaries
relies on representing the symmetry as the action of a matrix group acting on
the data, and subsequently introducing a convex penalty function so as to
induce sparsity with respect to the collection of matrix group elements. Our
framework specializes to the convolutional dictionary learning problem when we
consider integer shifts. Using properties of positive semidefinite Hermitian
Toeplitz matrices, we develop an extension that learns dictionaries that are
invariant under continuous shifts. Our numerical experiments on synthetic data
and ECG data show that the incorporation of such symmetries as priors are most
valuable when the dataset has few data-points, or when the full range of
symmetries is inadequately expressed in the dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Soh_Y/0/1/0/all/0/1"&gt;Yong Sheng Soh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Collaborative Learning in the Jungle. (arXiv:2008.00742v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.00742</id>
        <link href="http://arxiv.org/abs/2008.00742"/>
        <updated>2021-06-08T02:20:24.643Z</updated>
        <summary type="html"><![CDATA[We study Byzantine collaborative learning, where $n$ nodes seek to
collectively learn from each others' local data. The data distribution may vary
from one node to another. No node is trusted, and $f < n$ nodes can behave
arbitrarily. We prove that collaborative learning is equivalent to a new form
of agreement, which we call averaging agreement. In this problem, nodes start
each with an initial vector and seek to approximately agree on a common vector,
which is close to the average of honest nodes' initial vectors. We present two
asynchronous solutions to averaging agreement, each we prove optimal according
to some dimension. The first, based on the minimum-diameter averaging, requires
$ n \geq 6f+1$, but achieves asymptotically the best-possible averaging
constant up to a multiplicative constant. The second, based on reliable
broadcast and coordinate-wise trimmed mean, achieves optimal Byzantine
resilience, i.e., $n \geq 3f+1$. Each of these algorithms induces an optimal
Byzantine collaborative learning protocol. In particular, our equivalence
yields new impossibility theorems on what any collaborative learning algorithm
can achieve in adversarial and heterogeneous environments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+El_Mhamdi_E/0/1/0/all/0/1"&gt;El-Mahdi El-Mhamdi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Farhadkhani_S/0/1/0/all/0/1"&gt;Sadegh Farhadkhani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guerraoui_R/0/1/0/all/0/1"&gt;Rachid Guerraoui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guirguis_A/0/1/0/all/0/1"&gt;Arsany Guirguis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoang_L/0/1/0/all/0/1"&gt;L&amp;#xea; Nguy&amp;#xea;n Hoang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rouault_S/0/1/0/all/0/1"&gt;S&amp;#xe9;bastien Rouault&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Conv-MPN: Convolutional Message Passing Neural Network for Structured Outdoor Architecture Reconstruction. (arXiv:1912.01756v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1912.01756</id>
        <link href="http://arxiv.org/abs/1912.01756"/>
        <updated>2021-06-08T02:20:24.637Z</updated>
        <summary type="html"><![CDATA[This paper proposes a novel message passing neural (MPN) architecture
Conv-MPN, which reconstructs an outdoor building as a planar graph from a
single RGB image. Conv-MPN is specifically designed for cases where nodes of a
graph have explicit spatial embedding. In our problem, nodes correspond to
building edges in an image. Conv-MPN is different from MPN in that 1) the
feature associated with a node is represented as a feature volume instead of a
1D vector; and 2) convolutions encode messages instead of fully connected
layers. Conv-MPN learns to select a true subset of nodes (i.e., building edges)
to reconstruct a building planar graph. Our qualitative and quantitative
evaluations over 2,000 buildings show that Conv-MPN makes significant
improvements over the existing fully neural solutions. We believe that the
paper has a potential to open a new line of graph neural network research for
structured geometry reconstruction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1"&gt;Fuyang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nauata_N/0/1/0/all/0/1"&gt;Nelson Nauata&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Furukawa_Y/0/1/0/all/0/1"&gt;Yasutaka Furukawa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Logic of Machine Learning. (arXiv:2006.09500v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.09500</id>
        <link href="http://arxiv.org/abs/2006.09500"/>
        <updated>2021-06-08T02:20:24.635Z</updated>
        <summary type="html"><![CDATA[ML is approached from logic point of view as a problem of maximizing
consistency of a hypothesis in a context of a given training set. Nonjudgmental
logic (NjL) with modalities ``It appears that'', ``Assume that'' is introduced
to formalize and quantify the concepts of inconsistency. Two conjectures are
formulated. First, there are only 5 types of steps for all learners. Second,
any learner minimizes a criterion, which can be represented as a measure of
inconsistency in a semantic of NjL. Many popular ML algorithms (from
hierarchical clustering to k-NN and SVM) are shown to corroborate both
conjectures. In addition, it is demonstrated that NjL allows to formalize and
solve several general learning problems which are not considered as ML usually.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sapir_M/0/1/0/all/0/1"&gt;Marina Sapir&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Singular Dynamic Mode Decompositions. (arXiv:2106.02639v1 [eess.SY])]]></title>
        <id>http://arxiv.org/abs/2106.02639</id>
        <link href="http://arxiv.org/abs/2106.02639"/>
        <updated>2021-06-08T02:20:24.615Z</updated>
        <summary type="html"><![CDATA[This manuscript is aimed at addressing several long standing limitations of
dynamic mode decompositions in the application of Koopman analysis. Principle
among these limitations are the convergence of associated Dynamic Mode
Decomposition algorithms and the existence of Koopman modes. To address these
limitations, two major modifications are made, where Koopman operators are
removed from the analysis in light of Liouville operators (known as Koopman
generators in special cases), and these operators are shown to be compact for
certain pairs of Hilbert spaces selected separately as the domain and range of
the operator. While eigenfunctions are discarded in this analysis, a viable
reconstruction algorithm is still demonstrated, and the sacrifice of
eigenfunctions realizes the theoretical goals of DMD analysis that have yet to
be achieved in other contexts. The manuscript concludes with the description of
a Dynamic Mode Decomposition algorithm that converges when a dense collection
of occupation kernels, arising from the data, are leveraged in the analysis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Rosenfeld_J/0/1/0/all/0/1"&gt;Joel A. Rosenfeld&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kamalapurkar_R/0/1/0/all/0/1"&gt;Rushikesh Kamalapurkar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Low-Regret Active learning. (arXiv:2104.02822v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.02822</id>
        <link href="http://arxiv.org/abs/2104.02822"/>
        <updated>2021-06-08T02:20:24.601Z</updated>
        <summary type="html"><![CDATA[We develop an online learning algorithm for identifying unlabeled data points
that are most informative for training (i.e., active learning). By formulating
the active learning problem as the prediction with sleeping experts problem, we
provide a framework for identifying informative data with respect to any given
definition of informativeness. At the core of our work is an efficient
algorithm for sleeping experts that is tailored to achieve low regret on
predictable (easy) instances while remaining resilient to adversarial ones.
This stands in contrast to state-of-the-art active learning methods that are
overwhelmingly based on greedy selection, and hence cannot ensure good
performance across varying problem instances. We present empirical results
demonstrating that our method (i) instantiated with an informativeness measure
consistently outperforms its greedy counterpart and (ii) reliably outperforms
uniform sampling on real-world data sets and models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Baykal_C/0/1/0/all/0/1"&gt;Cenk Baykal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liebenwein_L/0/1/0/all/0/1"&gt;Lucas Liebenwein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feldman_D/0/1/0/all/0/1"&gt;Dan Feldman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rus_D/0/1/0/all/0/1"&gt;Daniela Rus&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Constant Approximation Algorithm for Sequential Random-Order No-Substitution k-Median Clustering. (arXiv:2102.04050v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.04050</id>
        <link href="http://arxiv.org/abs/2102.04050"/>
        <updated>2021-06-08T02:20:24.598Z</updated>
        <summary type="html"><![CDATA[We study k-median clustering under the sequential no-substitution setting. In
this setting, a data stream is sequentially observed, and some of the points
are selected by the algorithm as cluster centers. However, a point can be
selected as a center only immediately after it is observed, before observing
the next point. In addition, a selected center cannot be substituted later. We
give the first algorithm for this setting that obtains a constant approximation
factor on the optimal risk under a random arrival order, an exponential
improvement over previous work. This is also the first constant approximation
guarantee that holds without any structural assumptions on the input data.
Moreover, the number of selected centers is only quasi-linear in k. Our
algorithm and analysis are based on a careful risk estimation that avoids
outliers, a new concept of a linear bin division, and a multiscale approach to
center selection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hess_T/0/1/0/all/0/1"&gt;Tom Hess&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moshkovitz_M/0/1/0/all/0/1"&gt;Michal Moshkovitz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sabato_S/0/1/0/all/0/1"&gt;Sivan Sabato&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Treatment Effects in Panels with General Intervention Patterns. (arXiv:2106.02780v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.02780</id>
        <link href="http://arxiv.org/abs/2106.02780"/>
        <updated>2021-06-08T02:20:24.597Z</updated>
        <summary type="html"><![CDATA[The problem of causal inference with panel data is a central econometric
question. The following is a fundamental version of this problem: Let $M^*$ be
a low rank matrix and $E$ be a zero-mean noise matrix. For a `treatment' matrix
$Z$ with entries in $\{0,1\}$ we observe the matrix $O$ with entries $O_{ij} :=
M^*_{ij} + E_{ij} + \mathcal{T}_{ij} Z_{ij}$ where $\mathcal{T}_{ij} $ are
unknown, heterogenous treatment effects. The problem requires we estimate the
average treatment effect $\tau^* := \sum_{ij} \mathcal{T}_{ij} Z_{ij} /
\sum_{ij} Z_{ij}$. The synthetic control paradigm provides an approach to
estimating $\tau^*$ when $Z$ places support on a single row. This paper extends
that framework to allow rate-optimal recovery of $\tau^*$ for general $Z$, thus
broadly expanding its applicability. Our guarantees are the first of their type
in this general setting. Computational experiments on synthetic and real-world
data show a substantial advantage over competing estimators.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Farias_V/0/1/0/all/0/1"&gt;Vivek F. Farias&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Li_A/0/1/0/all/0/1"&gt;Andrew A. Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Peng_T/0/1/0/all/0/1"&gt;Tianyi Peng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Graph Symmetrisation Bound on Channel Information Leakage under Blowfish Privacy. (arXiv:2007.05975v2 [cs.IT] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.05975</id>
        <link href="http://arxiv.org/abs/2007.05975"/>
        <updated>2021-06-08T02:20:24.589Z</updated>
        <summary type="html"><![CDATA[Blowfish privacy is a recent generalisation of differential privacy that
enables improved utility while maintaining privacy policies with semantic
guarantees, a factor that has driven the popularity of differential privacy in
computer science. This paper relates Blowfish privacy to an important measure
of privacy loss of information channels from the communications theory
community: min-entropy leakage. Symmetry in an input data neighbouring relation
is central to known connections between differential privacy and min-entropy
leakage. But while differential privacy exhibits strong symmetry, Blowfish
neighbouring relations correspond to arbitrary simple graphs owing to the
framework's flexible privacy policies. To bound the min-entropy leakage of
Blowfish-private mechanisms we organise our analysis over symmetrical
partitions corresponding to orbits of graph automorphism groups. A construction
meeting our bound with asymptotic equality demonstrates tightness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Edwards_T/0/1/0/all/0/1"&gt;Tobias Edwards&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rubinstein_B/0/1/0/all/0/1"&gt;Benjamin I. P. Rubinstein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zuhe Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1"&gt;Sanming Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Revisiting Hilbert-Schmidt Information Bottleneck for Adversarial Robustness. (arXiv:2106.02734v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02734</id>
        <link href="http://arxiv.org/abs/2106.02734"/>
        <updated>2021-06-08T02:20:24.576Z</updated>
        <summary type="html"><![CDATA[We investigate the HSIC (Hilbert-Schmidt independence criterion) bottleneck
as a regularizer for learning an adversarially robust deep neural network
classifier. We show that the HSIC bottleneck enhances robustness to adversarial
attacks both theoretically and experimentally. Our experiments on multiple
benchmark datasets and architectures demonstrate that incorporating an HSIC
bottleneck regularizer attains competitive natural accuracy and improves
adversarial robustness, both with and without adversarial examples during
training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zifeng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jian_T/0/1/0/all/0/1"&gt;Tong Jian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Masoomi_A/0/1/0/all/0/1"&gt;Aria Masoomi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ioannidis_S/0/1/0/all/0/1"&gt;Stratis Ioannidis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dy_J/0/1/0/all/0/1"&gt;Jennifer Dy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bandwidth-based Step-Sizes for Non-Convex Stochastic Optimization. (arXiv:2106.02888v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02888</id>
        <link href="http://arxiv.org/abs/2106.02888"/>
        <updated>2021-06-08T02:20:24.569Z</updated>
        <summary type="html"><![CDATA[Many popular learning-rate schedules for deep neural networks combine a
decaying trend with local perturbations that attempt to escape saddle points
and bad local minima. We derive convergence guarantees for bandwidth-based
step-sizes, a general class of learning-rates that are allowed to vary in a
banded region. This framework includes cyclic and non-monotonic step-sizes for
which no theoretical guarantees were previously known. We provide worst-case
guarantees for SGD on smooth non-convex problems under several bandwidth-based
step sizes, including stagewise $1/\sqrt{t}$ and the popular step-decay
(constant and then drop by a constant), which is also shown to be optimal.
Moreover, we show that its momentum variant (SGDM) converges as fast as SGD
with the bandwidth-based step-decay step-size. Finally, we propose some novel
step-size schemes in the bandwidth-based family and verify their efficiency on
several deep neural network training tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaoyu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Johansson_M/0/1/0/all/0/1"&gt;Mikael Johansson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Attacker Behaviour Profiling using Stochastic Ensemble of Hidden Markov Models. (arXiv:1905.11824v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1905.11824</id>
        <link href="http://arxiv.org/abs/1905.11824"/>
        <updated>2021-06-08T02:20:24.551Z</updated>
        <summary type="html"><![CDATA[Cyber threat intelligence is one of the emerging areas of focus in
information security. Much of the recent work has focused on rule-based methods
and detection of network attacks using Intrusion Detection algorithms. In this
paper we propose a framework for inspecting and modelling the behavioural
aspect of an attacker to obtain better insight predictive power on his future
actions. For modelling we propose a novel semi-supervised algorithm called
Fusion Hidden Markov Model (FHMM) which is more robust to noise, requires
comparatively less training time, and utilizes the benefits of ensemble
learning to better model temporal relationships in data. This paper evaluates
the performances of FHMM and compares it with both traditional algorithms like
Markov Chain, Hidden Markov Model (HMM) and recently developed Deep Recurrent
Neural Network (Deep RNN) architectures. We conduct the experiments on dataset
consisting of real data attacks on a Cowrie honeypot system. FHMM provides
accuracy comparable to deep RNN architectures at significant lower training
time. Given these experimental results, we recommend using FHMM for modelling
discrete temporal data for significantly faster training and better performance
than existing methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Deshmukh_S/0/1/0/all/0/1"&gt;Soham Deshmukh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rade_R/0/1/0/all/0/1"&gt;Rahul Rade&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kazi_D/0/1/0/all/0/1"&gt;Dr. Faruk Kazi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FedNL: Making Newton-Type Methods Applicable to Federated Learning. (arXiv:2106.02969v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02969</id>
        <link href="http://arxiv.org/abs/2106.02969"/>
        <updated>2021-06-08T02:20:24.536Z</updated>
        <summary type="html"><![CDATA[Inspired by recent work of Islamov et al (2021), we propose a family of
Federated Newton Learn (FedNL) methods, which we believe is a marked step in
the direction of making second-order methods applicable to FL. In contrast to
the aforementioned work, FedNL employs a different Hessian learning technique
which i) enhances privacy as it does not rely on the training data to be
revealed to the coordinating server, ii) makes it applicable beyond generalized
linear models, and iii) provably works with general contractive compression
operators for compressing the local Hessians, such as Top-$K$ or Rank-$R$,
which are vastly superior in practice. Notably, we do not need to rely on error
feedback for our methods to work with contractive compressors. Moreover, we
develop FedNL-PP, FedNL-CR and FedNL-LS, which are variants of FedNL that
support partial participation, and globalization via cubic regularization and
line search, respectively, and FedNL-BC, which is a variant that can further
benefit from bidirectional compression of gradients and models, i.e., smart
uplink gradient and smart downlink model compression. We prove local
convergence rates that are independent of the condition number, the number of
training data points, and compression variance. Our communication efficient
Hessian learning technique provably learns the Hessian at the optimum. Finally,
we perform a variety of numerical experiments that show that our FedNL methods
have state-of-the-art communication complexity when compared to key baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Safaryan_M/0/1/0/all/0/1"&gt;Mher Safaryan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Islamov_R/0/1/0/all/0/1"&gt;Rustem Islamov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qian_X/0/1/0/all/0/1"&gt;Xun Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Richtarik_P/0/1/0/all/0/1"&gt;Peter Richt&amp;#xe1;rik&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Stochastic Behaviour from Aggregate Data. (arXiv:2002.03513v7 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.03513</id>
        <link href="http://arxiv.org/abs/2002.03513"/>
        <updated>2021-06-08T02:20:24.529Z</updated>
        <summary type="html"><![CDATA[Learning nonlinear dynamics from aggregate data is a challenging problem
because the full trajectory of each individual is not available, namely, the
individual observed at one time may not be observed at the next time point, or
the identity of individual is unavailable. This is in sharp contrast to
learning dynamics with full trajectory data, on which the majority of existing
methods are based. We propose a novel method using the weak form of Fokker
Planck Equation (FPE) -- a partial differential equation -- to describe the
density evolution of data in a sampled form, which is then combined with
Wasserstein generative adversarial network (WGAN) in the training process. In
such a sample-based framework we are able to learn the nonlinear dynamics from
aggregate data without explicitly solving the partial differential equation
(PDE) FPE. We demonstrate our approach in the context of a series of synthetic
and real-world data sets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1"&gt;Shaojun Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Shu Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zha_H/0/1/0/all/0/1"&gt;Hongyuan Zha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1"&gt;Haomin Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Constructive Universal High-Dimensional Distribution Generation through Deep ReLU Networks. (arXiv:2006.16664v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.16664</id>
        <link href="http://arxiv.org/abs/2006.16664"/>
        <updated>2021-06-08T02:20:24.525Z</updated>
        <summary type="html"><![CDATA[We present an explicit deep neural network construction that transforms
uniformly distributed one-dimensional noise into an arbitrarily close
approximation of any two-dimensional Lipschitz-continuous target distribution.
The key ingredient of our design is a generalization of the "space-filling"
property of sawtooth functions discovered in (Bailey & Telgarsky, 2018). We
elicit the importance of depth - in our neural network construction - in
driving the Wasserstein distance between the target distribution and the
approximation realized by the network to zero. An extension to output
distributions of arbitrary dimension is outlined. Finally, we show that the
proposed construction does not incur a cost - in terms of error measured in
Wasserstein-distance - relative to generating $d$-dimensional target
distributions from $d$ independent random variables.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Perekrestenko_D/0/1/0/all/0/1"&gt;Dmytro Perekrestenko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muller_S/0/1/0/all/0/1"&gt;Stephan M&amp;#xfc;ller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bolcskei_H/0/1/0/all/0/1"&gt;Helmut B&amp;#xf6;lcskei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Graph to Graphs Framework for Retrosynthesis Prediction. (arXiv:2003.12725v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.12725</id>
        <link href="http://arxiv.org/abs/2003.12725"/>
        <updated>2021-06-08T02:20:24.516Z</updated>
        <summary type="html"><![CDATA[A fundamental problem in computational chemistry is to find a set of
reactants to synthesize a target molecule, a.k.a. retrosynthesis prediction.
Existing state-of-the-art methods rely on matching the target molecule with a
large set of reaction templates, which are very computationally expensive and
also suffer from the problem of coverage. In this paper, we propose a novel
template-free approach called G2Gs by transforming a target molecular graph
into a set of reactant molecular graphs. G2Gs first splits the target molecular
graph into a set of synthons by identifying the reaction centers, and then
translates the synthons to the final reactant graphs via a variational graph
translation framework. Experimental results show that G2Gs significantly
outperforms existing template-free approaches by up to 63% in terms of the
top-1 accuracy and achieves a performance close to that of state-of-the-art
template based approaches, but does not require domain knowledge and is much
more scalable.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shi_C/0/1/0/all/0/1"&gt;Chence Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1"&gt;Minkai Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1"&gt;Hongyu Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Ming Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1"&gt;Jian Tang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[k-Mixup Regularization for Deep Learning via Optimal Transport. (arXiv:2106.02933v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02933</id>
        <link href="http://arxiv.org/abs/2106.02933"/>
        <updated>2021-06-08T02:20:24.485Z</updated>
        <summary type="html"><![CDATA[Mixup is a popular regularization technique for training deep neural networks
that can improve generalization and increase adversarial robustness. It
perturbs input training data in the direction of other randomly-chosen
instances in the training set. To better leverage the structure of the data, we
extend mixup to \emph{$k$-mixup} by perturbing $k$-batches of training points
in the direction of other $k$-batches using displacement interpolation,
interpolation under the Wasserstein metric. We demonstrate theoretically and in
simulations that $k$-mixup preserves cluster and manifold structures, and we
extend theory studying efficacy of standard mixup. Our empirical results show
that training with $k$-mixup further improves generalization and robustness on
benchmark datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Greenewald_K/0/1/0/all/0/1"&gt;Kristjan Greenewald&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_A/0/1/0/all/0/1"&gt;Anming Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yurochkin_M/0/1/0/all/0/1"&gt;Mikhail Yurochkin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Solomon_J/0/1/0/all/0/1"&gt;Justin Solomon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chien_E/0/1/0/all/0/1"&gt;Edward Chien&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[W-RST: Towards a Weighted RST-style Discourse Framework. (arXiv:2106.02658v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02658</id>
        <link href="http://arxiv.org/abs/2106.02658"/>
        <updated>2021-06-08T02:20:24.469Z</updated>
        <summary type="html"><![CDATA[Aiming for a better integration of data-driven and linguistically-inspired
approaches, we explore whether RST Nuclearity, assigning a binary assessment of
importance between text segments, can be replaced by automatically generated,
real-valued scores, in what we call a Weighted-RST framework. In particular, we
find that weighted discourse trees from auxiliary tasks can benefit key NLP
downstream applications, compared to nuclearity-centered approaches. We further
show that real-valued importance distributions partially and interestingly
align with the assessment and uncertainty of human annotators.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huber_P/0/1/0/all/0/1"&gt;Patrick Huber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_W/0/1/0/all/0/1"&gt;Wen Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carenini_G/0/1/0/all/0/1"&gt;Giuseppe Carenini&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PID-GAN: A GAN Framework based on a Physics-informed Discriminator for Uncertainty Quantification with Physics. (arXiv:2106.02993v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02993</id>
        <link href="http://arxiv.org/abs/2106.02993"/>
        <updated>2021-06-08T02:20:24.456Z</updated>
        <summary type="html"><![CDATA[As applications of deep learning (DL) continue to seep into critical
scientific use-cases, the importance of performing uncertainty quantification
(UQ) with DL has become more pressing than ever before. In scientific
applications, it is also important to inform the learning of DL models with
knowledge of physics of the problem to produce physically consistent and
generalized solutions. This is referred to as the emerging field of
physics-informed deep learning (PIDL). We consider the problem of developing
PIDL formulations that can also perform UQ. To this end, we propose a novel
physics-informed GAN architecture, termed PID-GAN, where the knowledge of
physics is used to inform the learning of both the generator and discriminator
models, making ample use of unlabeled data instances. We show that our proposed
PID-GAN framework does not suffer from imbalance of generator gradients from
multiple loss terms as compared to state-of-the-art. We also empirically
demonstrate the efficacy of our proposed framework on a variety of case studies
involving benchmark physics-based PDEs as well as imperfect physics. All the
code and datasets used in this study have been made available on this link :
https://github.com/arkadaw9/PID-GAN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Daw_A/0/1/0/all/0/1"&gt;Arka Daw&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maruf_M/0/1/0/all/0/1"&gt;M. Maruf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karpatne_A/0/1/0/all/0/1"&gt;Anuj Karpatne&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph Infomax Adversarial Learning for Treatment Effect Estimation with Networked Observational Data. (arXiv:2106.02881v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02881</id>
        <link href="http://arxiv.org/abs/2106.02881"/>
        <updated>2021-06-08T02:20:24.434Z</updated>
        <summary type="html"><![CDATA[Treatment effect estimation from observational data is a critical research
topic across many domains. The foremost challenge in treatment effect
estimation is how to capture hidden confounders. Recently, the growing
availability of networked observational data offers a new opportunity to deal
with the issue of hidden confounders. Unlike networked data in traditional
graph learning tasks, such as node classification and link detection, the
networked data under the causal inference problem has its particularity, i.e.,
imbalanced network structure. In this paper, we propose a Graph Infomax
Adversarial Learning (GIAL) model for treatment effect estimation, which makes
full use of the network structure to capture more information by recognizing
the imbalance in network structure. We evaluate the performance of our GIAL
model on two benchmark datasets, and the results demonstrate superiority over
the state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chu_Z/0/1/0/all/0/1"&gt;Zhixuan Chu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rathbun_S/0/1/0/all/0/1"&gt;Stephen L. Rathbun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Sheng Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Attribute-Aligned Strategy for Learning Speech Representation. (arXiv:2106.02810v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2106.02810</id>
        <link href="http://arxiv.org/abs/2106.02810"/>
        <updated>2021-06-08T02:20:24.391Z</updated>
        <summary type="html"><![CDATA[Advancement in speech technology has brought convenience to our life.
However, the concern is on the rise as speech signal contains multiple personal
attributes, which would lead to either sensitive information leakage or bias
toward decision. In this work, we propose an attribute-aligned learning
strategy to derive speech representation that can flexibly address these issues
by attribute-selection mechanism. Specifically, we propose a
layered-representation variational autoencoder (LR-VAE), which factorizes
speech representation into attribute-sensitive nodes, to derive an
identity-free representation for speech emotion recognition (SER), and an
emotionless representation for speaker verification (SV). Our proposed method
achieves competitive performances on identity-free SER and a better performance
on emotionless SV, comparing to the current state-of-the-art method of using
adversarial learning applied on a large emotion corpora, the MSP-Podcast. Also,
our proposed learning strategy reduces the model and training process needed to
achieve multiple privacy-preserving tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yu-Lin Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Su_B/0/1/0/all/0/1"&gt;Bo-Hao Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hong_Y/0/1/0/all/0/1"&gt;Y.-W. Peter Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lee_C/0/1/0/all/0/1"&gt;Chi-Chun Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Time Attention Networks for Irregularly Sampled Time Series. (arXiv:2101.10318v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.10318</id>
        <link href="http://arxiv.org/abs/2101.10318"/>
        <updated>2021-06-08T02:20:24.112Z</updated>
        <summary type="html"><![CDATA[Irregular sampling occurs in many time series modeling applications where it
presents a significant challenge to standard deep learning models. This work is
motivated by the analysis of physiological time series data in electronic
health records, which are sparse, irregularly sampled, and multivariate. In
this paper, we propose a new deep learning framework for this setting that we
call Multi-Time Attention Networks. Multi-Time Attention Networks learn an
embedding of continuous-time values and use an attention mechanism to produce a
fixed-length representation of a time series containing a variable number of
observations. We investigate the performance of this framework on interpolation
and classification tasks using multiple datasets. Our results show that the
proposed approach performs as well or better than a range of baseline and
recently proposed models while offering significantly faster training times
than current state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shukla_S/0/1/0/all/0/1"&gt;Satya Narayan Shukla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marlin_B/0/1/0/all/0/1"&gt;Benjamin M. Marlin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HMRL: Hyper-Meta Learning for Sparse Reward Reinforcement Learning Problem. (arXiv:2002.04238v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.04238</id>
        <link href="http://arxiv.org/abs/2002.04238"/>
        <updated>2021-06-08T02:20:24.070Z</updated>
        <summary type="html"><![CDATA[In spite of the success of existing meta reinforcement learning methods, they
still have difficulty in learning a meta policy effectively for RL problems
with sparse reward. In this respect, we develop a novel meta reinforcement
learning framework called Hyper-Meta RL(HMRL), for sparse reward RL problems.
It is consisted with three modules including the cross-environment meta state
embedding module which constructs a common meta state space to adapt to
different environments; the meta state based environment-specific meta reward
shaping which effectively extends the original sparse reward trajectory by
cross-environmental knowledge complementarity and as a consequence the meta
policy achieves better generalization and efficiency with the shaped meta
reward. Experiments with sparse-reward environments show the superiority of
HMRL on both transferability and policy learning efficiency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hua_Y/0/1/0/all/0/1"&gt;Yun Hua&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiangfeng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_B/0/1/0/all/0/1"&gt;Bo Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1"&gt;Wenhao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1"&gt;Junchi Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1"&gt;Xiaofeng He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zha_H/0/1/0/all/0/1"&gt;Hongyuan Zha&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Variational Leakage: The Role of Information Complexity in Privacy Leakage. (arXiv:2106.02818v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02818</id>
        <link href="http://arxiv.org/abs/2106.02818"/>
        <updated>2021-06-08T02:20:24.063Z</updated>
        <summary type="html"><![CDATA[We study the role of information complexity in privacy leakage about an
attribute of an adversary's interest, which is not known a priori to the system
designer. Considering the supervised representation learning setup and using
neural networks to parameterize the variational bounds of information
quantities, we study the impact of the following factors on the amount of
information leakage: information complexity regularizer weight, latent space
dimension, the cardinalities of the known utility and unknown sensitive
attribute sets, the correlation between utility and sensitive attributes, and a
potential bias in a sensitive attribute of adversary's interest. We conduct
extensive experiments on Colored-MNIST and CelebA datasets to evaluate the
effect of information complexity on the amount of intrinsic leakage.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Atashin_A/0/1/0/all/0/1"&gt;Amir Ahooye Atashin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Razeghi_B/0/1/0/all/0/1"&gt;Behrooz Razeghi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gunduz_D/0/1/0/all/0/1"&gt;Deniz G&amp;#xfc;nd&amp;#xfc;z&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Voloshynovskiy_S/0/1/0/all/0/1"&gt;Slava Voloshynovskiy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Causal Bandits with Unknown Graph Structure. (arXiv:2106.02988v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.02988</id>
        <link href="http://arxiv.org/abs/2106.02988"/>
        <updated>2021-06-08T02:20:24.056Z</updated>
        <summary type="html"><![CDATA[In causal bandit problems, the action set consists of interventions on
variables of a causal graph. Several researchers have recently studied such
bandit problems and pointed out their practical applications. However, all
existing works rely on a restrictive and impractical assumption that the
learner is given full knowledge of the causal graph structure upfront. In this
paper, we develop novel causal bandit algorithms without knowing the causal
graph. Our algorithms work well for causal trees, causal forests and a general
class of causal graphs. The regret guarantees of our algorithms greatly improve
upon those of standard multi-armed bandit (MAB) algorithms under mild
conditions. Lastly, we prove our mild conditions are necessary: without them
one cannot do better than standard MAB bandit algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Lu_Y/0/1/0/all/0/1"&gt;Yangyi Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Meisami_A/0/1/0/all/0/1"&gt;Amirhossein Meisami&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Tewari_A/0/1/0/all/0/1"&gt;Ambuj Tewari&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Numerical Composition of Differential Privacy. (arXiv:2106.02848v1 [cs.DS])]]></title>
        <id>http://arxiv.org/abs/2106.02848</id>
        <link href="http://arxiv.org/abs/2106.02848"/>
        <updated>2021-06-08T02:20:24.051Z</updated>
        <summary type="html"><![CDATA[We give a fast algorithm to optimally compose privacy guarantees of
differentially private (DP) algorithms to arbitrary accuracy. Our method is
based on the notion of privacy loss random variables to quantify the privacy
loss of DP algorithms. The running time and memory needed for our algorithm to
approximate the privacy curve of a DP algorithm composed with itself $k$ times
is $\tilde{O}(\sqrt{k})$. This improves over the best prior method by Koskela
et al. (2020) which requires $\tilde{\Omega}(k^{1.5})$ running time. We
demonstrate the utility of our algorithm by accurately computing the privacy
loss of DP-SGD algorithm of Abadi et al. (2016) and showing that our algorithm
speeds up the privacy computations by a few orders of magnitude compared to
prior work, while maintaining similar accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gopi_S/0/1/0/all/0/1"&gt;Sivakanth Gopi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1"&gt;Yin Tat Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wutschitz_L/0/1/0/all/0/1"&gt;Lukas Wutschitz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learnable Fourier Features for Multi-DimensionalSpatial Positional Encoding. (arXiv:2106.02795v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02795</id>
        <link href="http://arxiv.org/abs/2106.02795"/>
        <updated>2021-06-08T02:20:24.044Z</updated>
        <summary type="html"><![CDATA[Attentional mechanisms are order-invariant. Positional encoding is a crucial
component to allow attention-based deep model architectures such as Transformer
to address sequences or images where the position of information matters. In
this paper, we propose a novel positional encoding method based on learnable
Fourier features. Instead of hard-coding each position as a token or a vector,
we represent each position, which can be multi-dimensional, as a trainable
encoding based on learnable Fourier feature mapping, modulated with a
multi-layer perceptron. The representation is particularly advantageous for a
spatial multi-dimensional position, e.g., pixel positions on an image, where
$L_2$ distances or more complex positional relationships need to be captured.
Our experiments based on several public benchmark tasks show that our learnable
Fourier feature representation for multi-dimensional positional encoding
outperforms existing methods by both improving the accuracy and allowing faster
convergence.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Si_S/0/1/0/all/0/1"&gt;Si Si&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1"&gt;Gang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1"&gt;Cho-Jui Hsieh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bengio_S/0/1/0/all/0/1"&gt;Samy Bengio&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Accelerated Learning with Robustness to Adversarial Regressors. (arXiv:2005.01529v3 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.01529</id>
        <link href="http://arxiv.org/abs/2005.01529"/>
        <updated>2021-06-08T02:20:24.038Z</updated>
        <summary type="html"><![CDATA[High order momentum-based parameter update algorithms have seen widespread
applications in training machine learning models. Recently, connections with
variational approaches have led to the derivation of new learning algorithms
with accelerated learning guarantees. Such methods however, have only
considered the case of static regressors. There is a significant need for
parameter update algorithms which can be proven stable in the presence of
adversarial time-varying regressors, as is commonplace in control theory. In
this paper, we propose a new discrete time algorithm which 1) provides
stability and asymptotic convergence guarantees in the presence of adversarial
regressors by leveraging insights from adaptive control theory and 2) provides
non-asymptotic accelerated learning guarantees leveraging insights from convex
optimization. In particular, our algorithm reaches an $\epsilon$ sub-optimal
point in at most $\tilde{\mathcal{O}}(1/\sqrt{\epsilon})$ iterations when
regressors are constant - matching lower bounds due to Nesterov of
$\Omega(1/\sqrt{\epsilon})$, up to a $\log(1/\epsilon)$ factor and provides
guaranteed bounds for stability when regressors are time-varying. We provide
numerical experiments for a variant of Nesterov's provably hard convex
optimization problem with time-varying regressors, as well as the problem of
recovering an image with a time-varying blur and noise using streaming data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Gaudio_J/0/1/0/all/0/1"&gt;Joseph E. Gaudio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Annaswamy_A/0/1/0/all/0/1"&gt;Anuradha M. Annaswamy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Moreu_J/0/1/0/all/0/1"&gt;Jos&amp;#xe9; M. Moreu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Bolender_M/0/1/0/all/0/1"&gt;Michael A. Bolender&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Gibson_T/0/1/0/all/0/1"&gt;Travis E. Gibson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Same State, Different Task: Continual Reinforcement Learning without Interference. (arXiv:2106.02940v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02940</id>
        <link href="http://arxiv.org/abs/2106.02940"/>
        <updated>2021-06-08T02:20:24.032Z</updated>
        <summary type="html"><![CDATA[Continual Learning (CL) considers the problem of training an agent
sequentially on a set of tasks while seeking to retain performance on all
previous tasks. A key challenge in CL is catastrophic forgetting, which arises
when performance on a previously mastered task is reduced when learning a new
task. While a variety of methods exist to combat forgetting, in some cases
tasks are fundamentally incompatible with each other and thus cannot be learnt
by a single policy. This can occur, in reinforcement learning (RL) when an
agent may be rewarded for achieving different goals from the same observation.
In this paper we formalize this ``interference'' as distinct from the problem
of forgetting. We show that existing CL methods based on single neural network
predictors with shared replay buffers fail in the presence of interference.
Instead, we propose a simple method, OWL, to address this challenge. OWL learns
a factorized policy, using shared feature extraction layers, but separate
heads, each specializing on a new task. The separate heads in OWL are used to
prevent interference. At test time, we formulate policy selection as a
multi-armed bandit problem, and show it is possible to select the best policy
for an unknown task using feedback from the environment. The use of bandit
algorithms allows the OWL agent to constructively re-use different continually
learnt policies at different times during an episode. We show in multiple RL
environments that existing replay based CL methods fail, while OWL is able to
achieve close to optimal performance when training sequentially.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kessler_S/0/1/0/all/0/1"&gt;Samuel Kessler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parker_Holder_J/0/1/0/all/0/1"&gt;Jack Parker-Holder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ball_P/0/1/0/all/0/1"&gt;Philip Ball&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zohren_S/0/1/0/all/0/1"&gt;Stefan Zohren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roberts_S/0/1/0/all/0/1"&gt;Stephen J. Roberts&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Stochastic Linear Contextual Bandits Under Adversarial Attacks. (arXiv:2106.02978v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.02978</id>
        <link href="http://arxiv.org/abs/2106.02978"/>
        <updated>2021-06-08T02:20:24.025Z</updated>
        <summary type="html"><![CDATA[Stochastic linear contextual bandit algorithms have substantial applications
in practice, such as recommender systems, online advertising, clinical trials,
etc. Recent works show that optimal bandit algorithms are vulnerable to
adversarial attacks and can fail completely in the presence of attacks.
Existing robust bandit algorithms only work for the non-contextual setting
under the attack of rewards and cannot improve the robustness in the general
and popular contextual bandit environment. In addition, none of the existing
methods can defend against attacked context. In this work, we provide the first
robust bandit algorithm for stochastic linear contextual bandit setting under a
fully adaptive and omniscient attack. Our algorithm not only works under the
attack of rewards, but also under attacked context. Moreover, it does not need
any information about the attack budget or the particular form of the attack.
We provide theoretical guarantees for our proposed algorithm and show by
extensive experiments that our proposed algorithm significantly improves the
robustness against various kinds of popular attacks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Ding_Q/0/1/0/all/0/1"&gt;Qin Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Hsieh_C/0/1/0/all/0/1"&gt;Cho-Jui Hsieh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Sharpnack_J/0/1/0/all/0/1"&gt;James Sharpnack&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Denoising Word Embeddings by Averaging in a Shared Space. (arXiv:2106.02954v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02954</id>
        <link href="http://arxiv.org/abs/2106.02954"/>
        <updated>2021-06-08T02:20:24.001Z</updated>
        <summary type="html"><![CDATA[We introduce a new approach for smoothing and improving the quality of word
embeddings. We consider a method of fusing word embeddings that were trained on
the same corpus but with different initializations. We project all the models
to a shared vector space using an efficient implementation of the Generalized
Procrustes Analysis (GPA) procedure, previously used in multilingual word
translation. Our word representation demonstrates consistent improvements over
the raw models as well as their simplistic average, on a range of tasks. As the
new representations are more stable and reliable, there is a noticeable
improvement in rare word evaluations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Caciularu_A/0/1/0/all/0/1"&gt;Avi Caciularu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dagan_I/0/1/0/all/0/1"&gt;Ido Dagan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldberger_J/0/1/0/all/0/1"&gt;Jacob Goldberger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural dSCA: demixing multimodal interaction among brain areas during naturalistic experiments. (arXiv:2106.02948v1 [q-bio.NC])]]></title>
        <id>http://arxiv.org/abs/2106.02948</id>
        <link href="http://arxiv.org/abs/2106.02948"/>
        <updated>2021-06-08T02:20:23.981Z</updated>
        <summary type="html"><![CDATA[Multi-regional interaction among neuronal populations underlies the brain's
processing of rich sensory information in our daily lives. Recent neuroscience
and neuroimaging studies have increasingly used naturalistic stimuli and
experimental design to identify such realistic sensory computation in the
brain. However, existing methods for cross-areal interaction analysis with
dimensionality reduction, such as reduced-rank regression and canonical
correlation analysis, have limited applicability and interpretability in
naturalistic settings because they usually do not appropriately 'demix' neural
interactions into those associated with different types of task parameters or
stimulus features (e.g., visual or audio). In this paper, we develop a new
method for cross-areal interaction analysis that uses the rich task or stimulus
parameters to reveal how and what types of information are shared by different
neural populations. The proposed neural demixed shared component analysis
combines existing dimensionality reduction methods with a practical neural
network implementation of functional analysis of variance with latent
variables, thereby efficiently demixing nonlinear effects of continuous and
multimodal stimuli. We also propose a simplifying alternative under the
assumptions of linear effects and unimodal stimuli. To demonstrate our methods,
we analyzed two human neuroimaging datasets of participants watching
naturalistic videos of movies and dance movements. The results demonstrate that
our methods provide new insights into multi-regional interaction in the brain
during naturalistic sensory inputs, which cannot be captured by conventional
techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Takagi_Y/0/1/0/all/0/1"&gt;Yu Takagi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Hunt_L/0/1/0/all/0/1"&gt;Laurence T. Hunt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Ohata_R/0/1/0/all/0/1"&gt;Ryu Ohata&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Imamizu_H/0/1/0/all/0/1"&gt;Hiroshi Imamizu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Hirayama_J/0/1/0/all/0/1"&gt;Jun-ichiro Hirayama&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spectral Temporal Graph Neural Network for Trajectory Prediction. (arXiv:2106.02930v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02930</id>
        <link href="http://arxiv.org/abs/2106.02930"/>
        <updated>2021-06-08T02:20:23.942Z</updated>
        <summary type="html"><![CDATA[An effective understanding of the contextual environment and accurate motion
forecasting of surrounding agents is crucial for the development of autonomous
vehicles and social mobile robots. This task is challenging since the behavior
of an autonomous agent is not only affected by its own intention, but also by
the static environment and surrounding dynamically interacting agents. Previous
works focused on utilizing the spatial and temporal information in time domain
while not sufficiently taking advantage of the cues in frequency domain. To
this end, we propose a Spectral Temporal Graph Neural Network (SpecTGNN), which
can capture inter-agent correlations and temporal dependency simultaneously in
frequency domain in addition to time domain. SpecTGNN operates on both an agent
graph with dynamic state information and an environment graph with the features
extracted from context images in two streams. The model integrates graph
Fourier transform, spectral graph convolution and temporal gated convolution to
encode history information and forecast future trajectories. Moreover, we
incorporate a multi-head spatio-temporal attention mechanism to mitigate the
effect of error propagation in a long time horizon. We demonstrate the
performance of SpecTGNN on two public trajectory prediction benchmark datasets,
which achieves state-of-the-art performance in terms of prediction accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cao_D/0/1/0/all/0/1"&gt;Defu Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jiachen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1"&gt;Hengbo Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tomizuka_M/0/1/0/all/0/1"&gt;Masayoshi Tomizuka&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Detection of fake news on CoViD-19 on Web Search Engines. (arXiv:2103.11804v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.11804</id>
        <link href="http://arxiv.org/abs/2103.11804"/>
        <updated>2021-06-08T02:20:23.924Z</updated>
        <summary type="html"><![CDATA[In early January 2020, after China reported the first cases of the new
coronavirus (SARS-CoV-2) in the city of Wuhan, unreliable and not fully
accurate information has started spreading faster than the virus itself.
Alongside this pandemic, people have experienced a parallel infodemic, i.e., an
overabundance of information, some of which misleading or even harmful, that
has widely spread around the globe. Although Social Media are increasingly
being used as information source, Web Search Engines, like Google or Yahoo!,
still represent a powerful and trustworthy resource for finding information on
the Web. This is due to their capability to capture the largest amount of
information, helping users quickly identify the most relevant, useful, although
not always the most reliable, results for their search queries. This study aims
to detect potential misleading and fake contents by capturing and analysing
textual information, which flow through Search Engines. By using a real-world
dataset associated with recent CoViD-19 pandemic, we first apply re-sampling
techniques for class imbalance, then we use existing Machine Learning
algorithms for classification of not reliable news. By extracting lexical and
host-based features of associated Uniform Resource Locators (URLs) for news
articles, we show that the proposed methods, so common in phishing and
malicious URLs detection, can improve the efficiency and performance of
classifiers. Based on these findings, we suggest that the use of both textual
and URLs features can improve the effectiveness of fake news detection methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mazzeo_V/0/1/0/all/0/1"&gt;V. Mazzeo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rapisarda_A/0/1/0/all/0/1"&gt;A. Rapisarda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Giuffrida_G/0/1/0/all/0/1"&gt;G. Giuffrida&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DIVERSE: Bayesian Data IntegratiVE learning for precise drug ResponSE prediction. (arXiv:2104.00520v2 [q-bio.QM] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.00520</id>
        <link href="http://arxiv.org/abs/2104.00520"/>
        <updated>2021-06-08T02:20:23.913Z</updated>
        <summary type="html"><![CDATA[Detecting predictive biomarkers from multi-omics data is important for
precision medicine, to improve diagnostics of complex diseases and for better
treatments. This needs substantial experimental efforts that are made difficult
by the heterogeneity of cell lines and huge cost. An effective solution is to
build a computational model over the diverse omics data, including genomic,
molecular, and environmental information. However, choosing informative and
reliable data sources from among the different types of data is a challenging
problem. We propose DIVERSE, a framework of Bayesian importance-weighted tri-
and bi-matrix factorization(DIVERSE3 or DIVERSE2) to predict drug responses
from data of cell lines, drugs, and gene interactions. DIVERSE integrates the
data sources systematically, in a step-wise manner, examining the importance of
each added data set in turn. More specifically, we sequentially integrate five
different data sets, which have not all been combined in earlier bioinformatic
methods for predicting drug responses. Empirical experiments show that DIVERSE
clearly outperformed five other methods including three state-of-the-art
approaches, under cross-validation, particularly in out-of-matrix prediction,
which is closer to the setting of real use cases and more challenging than
simpler in-matrix prediction. Additionally, case studies for discovering new
drugs further confirmed the performance advantage of DIVERSE.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Paltun_B/0/1/0/all/0/1"&gt;Bet&amp;#xfc;l G&amp;#xfc;ven&amp;#xe7; Paltun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Kaski_S/0/1/0/all/0/1"&gt;Samuel Kaski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Mamitsuka_H/0/1/0/all/0/1"&gt;Hiroshi Mamitsuka&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Syndicated Bandits: A Framework for Auto Tuning Hyper-parameters in Contextual Bandit Algorithms. (arXiv:2106.02979v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.02979</id>
        <link href="http://arxiv.org/abs/2106.02979"/>
        <updated>2021-06-08T02:20:23.906Z</updated>
        <summary type="html"><![CDATA[The stochastic contextual bandit problem, which models the trade-off between
exploration and exploitation, has many real applications, including recommender
systems, online advertising and clinical trials. As many other machine learning
algorithms, contextual bandit algorithms often have one or more
hyper-parameters. As an example, in most optimal stochastic contextual bandit
algorithms, there is an unknown exploration parameter which controls the
trade-off between exploration and exploitation. A proper choice of the
hyper-parameters is essential for contextual bandit algorithms to perform well.
However, it is infeasible to use offline tuning methods to select
hyper-parameters in contextual bandit environment since there is no
pre-collected dataset and the decisions have to be made in real time. To tackle
this problem, we first propose a two-layer bandit structure for auto tuning the
exploration parameter and further generalize it to the Syndicated Bandits
framework which can learn multiple hyper-parameters dynamically in contextual
bandit environment. We show our Syndicated Bandits framework can achieve the
optimal regret upper bounds and is general enough to handle the tuning tasks in
many popular contextual bandit algorithms, such as LinUCB, LinTS, UCB-GLM, etc.
Experiments on both synthetic and real datasets validate the effectiveness of
our proposed framework.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Ding_Q/0/1/0/all/0/1"&gt;Qin Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yi-Wei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Hsieh_C/0/1/0/all/0/1"&gt;Cho-Jui Hsieh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Sharpnack_J/0/1/0/all/0/1"&gt;James Sharpnack&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Practical Privacy Filters and Odometers with R\'enyi Differential Privacy and Applications to Differentially Private Deep Learning. (arXiv:2103.01379v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.01379</id>
        <link href="http://arxiv.org/abs/2103.01379"/>
        <updated>2021-06-08T02:20:23.899Z</updated>
        <summary type="html"><![CDATA[Differential Privacy (DP) is the leading approach to privacy preserving deep
learning. As such, there are multiple efforts to provide drop-in integration of
DP into popular frameworks. These efforts, which add noise to each gradient
computation to make it DP, rely on composition theorems to bound the total
privacy loss incurred over this sequence of DP computations.

However, existing composition theorems present a tension between efficiency
and flexibility. Most theorems require all computations in the sequence to have
a predefined DP parameter, called the privacy budget. This prevents the design
of training algorithms that adapt the privacy budget on the fly, or that
terminate early to reduce the total privacy loss. Alternatively, the few
existing composition results for adaptive privacy budgets provide complex
bounds on the privacy loss, with constants too large to be practical.

In this paper, we study DP composition under adaptive privacy budgets through
the lens of R\'enyi Differential Privacy, proving a simpler composition theorem
with smaller constants, making it practical enough to use in algorithm design.
We demonstrate two applications of this theorem for DP deep learning: adapting
the noise or batch size online to improve a model's accuracy within a fixed
total privacy loss, and stopping early when fine-tuning a model to reduce total
privacy loss.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Lecuyer_M/0/1/0/all/0/1"&gt;Mathias L&amp;#xe9;cuyer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[IM-META: Influence Maximization Using Node Metadata in Networks With Unknown Topology. (arXiv:2106.02926v1 [cs.SI])]]></title>
        <id>http://arxiv.org/abs/2106.02926</id>
        <link href="http://arxiv.org/abs/2106.02926"/>
        <updated>2021-06-08T02:20:23.893Z</updated>
        <summary type="html"><![CDATA[In real-world applications of influence maximization (IM), the network
structure is often unknown. In this case, we may identify the most influential
seed nodes by exploring only a part of the underlying network given a small
budget for node queries. Motivated by the fact that collecting node metadata is
more cost-effective than investigating the relationship between nodes via
queried nodes, we develop IM-META, an end-to-end solution to IM in networks
with unknown topology by retrieving information from both queries and node
metadata. However, using such metadata to aid the IM process is not without
risk due to the noisy nature of metadata and uncertainties in connectivity
inference. To tackle these challenges, we formulate an IM problem that aims to
find two sets, i.e., seed nodes and queried nodes. We propose an effective
method that iteratively performs three steps: 1) we learn the relationship
between collected metadata and edges via a Siamese neural network model, 2) we
select a number of inferred influential edges to construct a reinforced graph
used for discovering an optimal seed set, and 3) we identify the next node to
query by maximizing the inferred influence spread using a topology-aware
ranking strategy. By querying only 5% of nodes, IM-META reaches 93% of the
upper bound performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tran_C/0/1/0/all/0/1"&gt;Cong Tran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shin_W/0/1/0/all/0/1"&gt;Won-Yong Shin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Spitz_A/0/1/0/all/0/1"&gt;Andreas Spitz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Proximal Causal Learning with Kernels: Two-Stage Estimation and Moment Restriction. (arXiv:2105.04544v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.04544</id>
        <link href="http://arxiv.org/abs/2105.04544"/>
        <updated>2021-06-08T02:20:23.873Z</updated>
        <summary type="html"><![CDATA[We address the problem of causal effect estimation in the presence of
unobserved confounding, but where proxies for the latent confounder(s) are
observed. We propose two kernel-based methods for nonlinear causal effect
estimation in this setting: (a) a two-stage regression approach, and (b) a
maximum moment restriction approach. We focus on the proximal causal learning
setting, but our methods can be used to solve a wider class of inverse problems
characterised by a Fredholm integral equation. In particular, we provide a
unifying view of two-stage and moment restriction approaches for solving this
problem in a nonlinear setting. We provide consistency guarantees for each
algorithm, and we demonstrate these approaches achieve competitive results on
synthetic data and data simulating a real-world task. In particular, our
approach outperforms earlier methods that are not suited to leveraging proxy
variables.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mastouri_A/0/1/0/all/0/1"&gt;Afsaneh Mastouri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yuchen Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gultchin_L/0/1/0/all/0/1"&gt;Limor Gultchin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Korba_A/0/1/0/all/0/1"&gt;Anna Korba&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Silva_R/0/1/0/all/0/1"&gt;Ricardo Silva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kusner_M/0/1/0/all/0/1"&gt;Matt J. Kusner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gretton_A/0/1/0/all/0/1"&gt;Arthur Gretton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muandet_K/0/1/0/all/0/1"&gt;Krikamol Muandet&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-armed Bandit Algorithms on System-on-Chip: Go Frequentist or Bayesian?. (arXiv:2106.02855v1 [eess.SY])]]></title>
        <id>http://arxiv.org/abs/2106.02855</id>
        <link href="http://arxiv.org/abs/2106.02855"/>
        <updated>2021-06-08T02:20:23.867Z</updated>
        <summary type="html"><![CDATA[Multi-armed Bandit (MAB) algorithms identify the best arm among multiple arms
via exploration-exploitation trade-off without prior knowledge of arm
statistics. Their usefulness in wireless radio, IoT, and robotics demand
deployment on edge devices, and hence, a mapping on system-on-chip (SoC) is
desired. Theoretically, the Bayesian approach-based Thompson Sampling (TS)
algorithm offers better performance than the frequentist approach-based Upper
Confidence Bound (UCB) algorithm. However, TS is not synthesizable due to Beta
function. We address this problem by approximating it via a pseudo-random
number generator-based approach and efficiently realize the TS algorithm on
Zynq SoC. In practice, the type of arms distribution (e.g., Bernoulli,
Gaussian, etc.) is unknown and hence, a single algorithm may not be optimal. We
propose a reconfigurable and intelligent MAB (RI-MAB) framework. Here,
intelligence enables the identification of appropriate MAB algorithms for a
given environment, and reconfigurability allows on-the-fly switching between
algorithms on the SoC. This eliminates the need for parallel implementation of
algorithms resulting in huge savings in resources and power consumption. We
analyze the functional correctness, area, power, and execution time of the
proposed and existing architectures for various arm distributions, word-length,
and hardware-software co-design approaches. We demonstrate the superiority of
the RI-MAB over TS and UCB only architectures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Santosh_S/0/1/0/all/0/1"&gt;S. V. Sai Santosh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Darak_S/0/1/0/all/0/1"&gt;Sumit J. Darak&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sensor Fusion-based GNSS Spoofing Attack Detection Framework for Autonomous Vehicles. (arXiv:2106.02982v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2106.02982</id>
        <link href="http://arxiv.org/abs/2106.02982"/>
        <updated>2021-06-08T02:20:23.860Z</updated>
        <summary type="html"><![CDATA[In this study, a sensor fusion based GNSS spoofing attack detection framework
is presented that consists of three concurrent strategies for an autonomous
vehicle (AV): (i) prediction of location shift, (ii) detection of turns (left
or right), and (iii) recognition of motion state (including standstill state).
Data from multiple low-cost in-vehicle sensors (i.e., accelerometer, steering
angle sensor, speed sensor, and GNSS) are fused and fed into a recurrent neural
network model, which is a long short-term memory (LSTM) network for predicting
the location shift, i.e., the distance that an AV travels between two
consecutive timestamps. We have then combined k-Nearest Neighbors (k-NN) and
Dynamic Time Warping (DTW) algorithms to detect turns using data from the
steering angle sensor. In addition, data from an AV's speed sensor is used to
recognize the AV's motion state including the standstill state. To prove the
efficacy of the sensor fusion-based attack detection framework, attack datasets
are created for three unique and sophisticated spoofing attacks turn by turn,
overshoot, and stop using the publicly available real-world Honda Research
Institute Driving Dataset (HDD). Our analysis reveals that the sensor
fusion-based detection framework successfully detects all three types of
spoofing attacks within the required computational latency threshold.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dasgupta_S/0/1/0/all/0/1"&gt;Sagar Dasgupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1"&gt;Mizanur Rahman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1"&gt;Mhafuzul Islam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chowdhury_M/0/1/0/all/0/1"&gt;Mashrur Chowdhury&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Relative stability toward diffeomorphisms indicates performance in deep nets. (arXiv:2105.02468v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.02468</id>
        <link href="http://arxiv.org/abs/2105.02468"/>
        <updated>2021-06-08T02:20:23.854Z</updated>
        <summary type="html"><![CDATA[Understanding why deep nets can classify data in large dimensions remains a
challenge. It has been proposed that they do so by becoming stable to
diffeomorphisms, yet existing empirical measurements support that it is often
not the case. We revisit this question by defining a maximum-entropy
distribution on diffeomorphisms, that allows to study typical diffeomorphisms
of a given norm. We confirm that stability toward diffeomorphisms does not
strongly correlate to performance on benchmark data sets of images. By
contrast, we find that the stability toward diffeomorphisms relative to that of
generic transformations $R_f$ correlates remarkably with the test error
$\epsilon_t$. It is of order unity at initialization but decreases by several
decades during training for state-of-the-art architectures. For CIFAR10 and 15
known architectures, we find $\epsilon_t\approx 0.2\sqrt{R_f}$, suggesting that
obtaining a small $R_f$ is important to achieve good performance. We study how
$R_f$ depends on the size of the training set and compare it to a simple model
of invariant learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Petrini_L/0/1/0/all/0/1"&gt;Leonardo Petrini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Favero_A/0/1/0/all/0/1"&gt;Alessandro Favero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geiger_M/0/1/0/all/0/1"&gt;Mario Geiger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wyart_M/0/1/0/all/0/1"&gt;Matthieu Wyart&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exposing the Implicit Energy Networks behind Masked Language Models via Metropolis--Hastings. (arXiv:2106.02736v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02736</id>
        <link href="http://arxiv.org/abs/2106.02736"/>
        <updated>2021-06-08T02:20:23.846Z</updated>
        <summary type="html"><![CDATA[While recent work has shown that scores from models trained by the ubiquitous
masked language modeling (MLM) objective effectively discriminate probable and
improbable sequences, it is still an open question if these MLMs specify a
principled probability distribution over the space of possible sequences. In
this paper, we interpret MLMs as energy-based sequence models and propose two
energy parametrizations derivable from the trained MLMs. In order to draw
samples correctly from these models, we develop a tractable \emph{sampling}
scheme based on the Metropolis--Hastings Monte Carlo algorithm. In our
approach, samples are proposed from the same masked conditionals used for
training the masked language models, and they are accepted or rejected based on
their energy values according to the target distribution. We validate the
effectiveness of the proposed parametrizations by exploring the quality of
samples drawn from these energy-based models on the conditional generation task
of machine translation. We theoretically and empirically justify our sampling
algorithm by showing that the masked conditionals on their own do not yield a
Markov chain whose stationary distribution is that of our target distribution,
and our approach generates higher quality samples than other recently proposed
undirected generation approaches (Wang et al., 2019, Ghazvininejad et al.,
2019).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Goyal_K/0/1/0/all/0/1"&gt;Kartik Goyal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dyer_C/0/1/0/all/0/1"&gt;Chris Dyer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berg_Kirkpatrick_T/0/1/0/all/0/1"&gt;Taylor Berg-Kirkpatrick&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MC2G: An Efficient Algorithm for Matrix Completion with Social and Item Similarity Graphs. (arXiv:2006.04373v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.04373</id>
        <link href="http://arxiv.org/abs/2006.04373"/>
        <updated>2021-06-08T02:20:23.828Z</updated>
        <summary type="html"><![CDATA[In this paper, we design and analyze MC2G (Matrix Completion with 2 Graphs),
an algorithm that performs matrix completion in the presence of social and item
similarity graphs. MC2G runs in quasilinear time and is parameter free. It is
based on spectral clustering and local refinement steps. The expected number of
sampled entries required for MC2G to succeed (i.e., recover the clusters in the
graphs and complete the matrix) matches an information-theoretic lower bound up
to a constant factor for a wide range of parameters. We show via extensive
experiments on both synthetic and real datasets that MC2G outperforms other
state-of-the-art matrix completion algorithms that leverage graph side
information.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Qiaosheng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suh_G/0/1/0/all/0/1"&gt;Geewon Suh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suh_C/0/1/0/all/0/1"&gt;Changho Suh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_V/0/1/0/all/0/1"&gt;Vincent Y. F. Tan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Soft-Attention Improves Skin Cancer Classification Performance. (arXiv:2105.03358v3 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.03358</id>
        <link href="http://arxiv.org/abs/2105.03358"/>
        <updated>2021-06-08T02:20:23.821Z</updated>
        <summary type="html"><![CDATA[In clinical applications, neural networks must focus on and highlight the
most important parts of an input image. Soft-Attention mechanism enables a
neural network toachieve this goal. This paper investigates the effectiveness
of Soft-Attention in deep neural architectures. The central aim of
Soft-Attention is to boost the value of important features and suppress the
noise-inducing features. We compare the performance of VGG, ResNet,
InceptionResNetv2 and DenseNet architectures with and without the
Soft-Attention mechanism, while classifying skin lesions. The original network
when coupled with Soft-Attention outperforms the baseline[16] by 4.7% while
achieving a precision of 93.7% on HAM10000 dataset [25]. Additionally,
Soft-Attention coupling improves the sensitivity score by 3.8% compared to
baseline[31] and achieves 91.6% on ISIC-2017 dataset [2]. The code is publicly
available at github.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Datta_S/0/1/0/all/0/1"&gt;Soumyya Kanti Datta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shaikh_M/0/1/0/all/0/1"&gt;Mohammad Abuzar Shaikh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Srihari_S/0/1/0/all/0/1"&gt;Sargur N. Srihari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gao_M/0/1/0/all/0/1"&gt;Mingchen Gao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Federated Learning of User Verification Models Without Sharing Embeddings. (arXiv:2104.08776v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.08776</id>
        <link href="http://arxiv.org/abs/2104.08776"/>
        <updated>2021-06-08T02:20:23.815Z</updated>
        <summary type="html"><![CDATA[We consider the problem of training User Verification (UV) models in
federated setting, where each user has access to the data of only one class and
user embeddings cannot be shared with the server or other users. To address
this problem, we propose Federated User Verification (FedUV), a framework in
which users jointly learn a set of vectors and maximize the correlation of
their instance embeddings with a secret linear combination of those vectors. We
show that choosing the linear combinations from the codewords of an
error-correcting code allows users to collaboratively train the model without
revealing their embedding vectors. We present the experimental results for user
verification with voice, face, and handwriting data and show that FedUV is on
par with existing approaches, while not sharing the embeddings with other users
or the server.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hosseini_H/0/1/0/all/0/1"&gt;Hossein Hosseini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_H/0/1/0/all/0/1"&gt;Hyunsin Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yun_S/0/1/0/all/0/1"&gt;Sungrack Yun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Louizos_C/0/1/0/all/0/1"&gt;Christos Louizos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soriaga_J/0/1/0/all/0/1"&gt;Joseph Soriaga&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Welling_M/0/1/0/all/0/1"&gt;Max Welling&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TARA: Training and Representation Alteration for AI Fairness and Domain Generalization. (arXiv:2012.06387v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.06387</id>
        <link href="http://arxiv.org/abs/2012.06387"/>
        <updated>2021-06-08T02:20:23.807Z</updated>
        <summary type="html"><![CDATA[We propose a novel method for enforcing AI fairness with respect to protected
or sensitive factors. This method uses a dual strategy performing training and
representation alteration (TARA) for the mitigation of prominent causes of AI
bias by including: a) the use of representation learning alteration via
adversarial independence to suppress the bias-inducing dependence of the data
representation from protected factors; and b) training set alteration via
intelligent augmentation to address bias-causing data imbalance, by using
generative models that allow the fine control of sensitive factors related to
underrepresented populations via domain adaptation and latent space
manipulation. When testing our methods on image analytics, experiments
demonstrate that TARA significantly or fully debiases baseline models while
outperforming competing debiasing methods that have the same amount of
information, e.g., with (% overall accuracy, % accuracy gap) = (78.8, 0.5) vs.
the baseline method's score of (71.8, 10.5) for EyePACS, and (73.7, 11.8) vs.
(69.1, 21.7) for CelebA. Furthermore, recognizing certain limitations in
current metrics used for assessing debiasing performance, we propose novel
conjunctive debiasing metrics. Our experiments also demonstrate the ability of
these novel metrics in assessing the Pareto efficiency of the proposed methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Paul_W/0/1/0/all/0/1"&gt;William Paul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hadzic_A/0/1/0/all/0/1"&gt;Armin Hadzic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joshi_N/0/1/0/all/0/1"&gt;Neil Joshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alajaji_F/0/1/0/all/0/1"&gt;Fady Alajaji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Burlina_P/0/1/0/all/0/1"&gt;Phil Burlina&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BERTnesia: Investigating the capture and forgetting of knowledge in BERT. (arXiv:2106.02902v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02902</id>
        <link href="http://arxiv.org/abs/2106.02902"/>
        <updated>2021-06-08T02:20:23.795Z</updated>
        <summary type="html"><![CDATA[Probing complex language models has recently revealed several insights into
linguistic and semantic patterns found in the learned representations. In this
article, we probe BERT specifically to understand and measure the relational
knowledge it captures in its parametric memory. While probing for linguistic
understanding is commonly applied to all layers of BERT as well as fine-tuned
models, this has not been done for factual knowledge. We utilize existing
knowledge base completion tasks (LAMA) to probe every layer of pre-trained as
well as fine-tuned BERT models(ranking, question answering, NER). Our findings
show that knowledge is not just contained in BERT's final layers. Intermediate
layers contribute a significant amount (17-60%) to the total knowledge found.
Probing intermediate layers also reveals how different types of knowledge
emerge at varying rates. When BERT is fine-tuned, relational knowledge is
forgotten. The extent of forgetting is impacted by the fine-tuning objective
and the training data. We found that ranking models forget the least and retain
more knowledge in their final layer compared to masked language modeling and
question-answering. However, masked language modeling performed the best at
acquiring new knowledge from the training data. When it comes to learning
facts, we found that capacity and fact density are key factors. We hope this
initial work will spur further research into understanding the parametric
memory of language models and the effect of training objectives on factual
knowledge. The code to repeat the experiments is publicly available on GitHub.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wallat_J/0/1/0/all/0/1"&gt;Jonas Wallat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_J/0/1/0/all/0/1"&gt;Jaspreet Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anand_A/0/1/0/all/0/1"&gt;Avishek Anand&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Forced Variational Integrator Networks for Prediction and Control of Mechanical Systems. (arXiv:2106.02973v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02973</id>
        <link href="http://arxiv.org/abs/2106.02973"/>
        <updated>2021-06-08T02:20:23.782Z</updated>
        <summary type="html"><![CDATA[As deep learning becomes more prevalent for prediction and control of real
physical systems, it is important that these overparameterized models are
consistent with physically plausible dynamics. This elicits a problem with how
much inductive bias to impose on the model through known physical parameters
and principles to reduce complexity of the learning problem to give us more
reliable predictions. Recent work employs discrete variational integrators
parameterized as a neural network architecture to learn conservative Lagrangian
systems. The learned model captures and enforces global energy preserving
properties of the system from very few trajectories. However, most real systems
are inherently non-conservative and, in practice, we would also like to apply
actuation. In this paper we extend this paradigm to account for general forcing
(e.g. control input and damping) via discrete d'Alembert's principle which may
ultimately be used for control applications. We show that this forced
variational integrator networks (FVIN) architecture allows us to accurately
account for energy dissipation and external forcing while still capturing the
true underlying energy-based passive dynamics. We show that in application this
can result in highly-data efficient model-based control and can predict on real
non-conservative systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Havens_A/0/1/0/all/0/1"&gt;Aaron Havens&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chowdhary_G/0/1/0/all/0/1"&gt;Girish Chowdhary&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bridging the Gap Between Adversarial Robustness and Optimization Bias. (arXiv:2102.08868v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.08868</id>
        <link href="http://arxiv.org/abs/2102.08868"/>
        <updated>2021-06-08T02:20:23.761Z</updated>
        <summary type="html"><![CDATA[We demonstrate that the choice of optimizer, neural network architecture, and
regularizer significantly affect the adversarial robustness of linear neural
networks, providing guarantees without the need for adversarial training. To
this end, we revisit a known result linking maximally robust classifiers and
minimum norm solutions, and combine it with recent results on the implicit bias
of optimizers. First, we show that, under certain conditions, it is possible to
achieve both perfect standard accuracy and a certain degree of robustness,
simply by training an overparametrized model using the implicit bias of the
optimization. In that regime, there is a direct relationship between the type
of the optimizer and the attack to which the model is robust. To the best of
our knowledge, this work is the first to study the impact of optimization
methods such as sign gradient descent and proximal methods on adversarial
robustness. Second, we characterize the robustness of linear convolutional
models, showing that they resist attacks subject to a constraint on the
Fourier-$\ell_\infty$ norm. To illustrate these findings we design a novel
Fourier-$\ell_\infty$ attack that finds adversarial examples with controllable
frequencies. We evaluate Fourier-$\ell_\infty$ robustness of
adversarially-trained deep CIFAR-10 models from the standard RobustBench
benchmark and visualize adversarial perturbations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Faghri_F/0/1/0/all/0/1"&gt;Fartash Faghri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gowal_S/0/1/0/all/0/1"&gt;Sven Gowal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vasconcelos_C/0/1/0/all/0/1"&gt;Cristina Vasconcelos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fleet_D/0/1/0/all/0/1"&gt;David J. Fleet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pedregosa_F/0/1/0/all/0/1"&gt;Fabian Pedregosa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roux_N/0/1/0/all/0/1"&gt;Nicolas Le Roux&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Greedy Approximation Algorithms for Active Sequential Hypothesis Testing. (arXiv:2103.04250v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.04250</id>
        <link href="http://arxiv.org/abs/2103.04250"/>
        <updated>2021-06-08T02:20:23.753Z</updated>
        <summary type="html"><![CDATA[In the problem of active sequential hypotheses testing (ASHT), a learner
seeks to identify the true hypothesis from among a known set of hypotheses. The
learner is given a set of actions and knows the random distribution of the
outcome of any action under any true hypothesis. Given a target error
$\delta>0$, the goal is to sequentially select the fewest number of actions so
as to identify the true hypothesis with probability at least $1 - \delta$.
Motivated by applications in which the number of hypotheses or actions is
massive (e.g. genomics-based cancer detection), we propose efficient (greedy,
in fact) algorithms and provide the first approximation guarantees for ASHT,
under two types of adaptivity. Both of our guarantees are independent of the
number of actions and logarithmic in the number of hypotheses. We numerically
evaluate the performance of our algorithms using both synthetic and real DNA
mutation data, demonstrating that our algorithms outperform previous heuristic
policies by large margins.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gan_K/0/1/0/all/0/1"&gt;Kyra Gan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_S/0/1/0/all/0/1"&gt;Su Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1"&gt;Andrew Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Relation Alignment for Calibrated Cross-modal Retrieval. (arXiv:2105.13868v2 [cs.CL] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2105.13868</id>
        <link href="http://arxiv.org/abs/2105.13868"/>
        <updated>2021-06-08T02:20:23.738Z</updated>
        <summary type="html"><![CDATA[Despite the achievements of large-scale multimodal pre-training approaches,
cross-modal retrieval, e.g., image-text retrieval, remains a challenging task.
To bridge the semantic gap between the two modalities, previous studies mainly
focus on word-region alignment at the object level, lacking the matching
between the linguistic relation among the words and the visual relation among
the regions. The neglect of such relation consistency impairs the
contextualized representation of image-text pairs and hinders the model
performance and the interpretability. In this paper, we first propose a novel
metric, Intra-modal Self-attention Distance (ISD), to quantify the relation
consistency by measuring the semantic distance between linguistic and visual
relations. In response, we present Inter-modal Alignment on Intra-modal
Self-attentions (IAIS), a regularized training method to optimize the ISD and
calibrate intra-modal self-attentions from the two modalities mutually via
inter-modal alignment. The IAIS regularizer boosts the performance of
prevailing models on Flickr30k and MS COCO datasets by a considerable margin,
which demonstrates the superiority of our approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1"&gt;Shuhuai Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1"&gt;Junyang Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1"&gt;Guangxiang Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Men_R/0/1/0/all/0/1"&gt;Rui Men&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_A/0/1/0/all/0/1"&gt;An Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jingren Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1"&gt;Xu Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Hongxia Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reinforcement Learning for Assignment Problem with Time Constraints. (arXiv:2106.02856v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2106.02856</id>
        <link href="http://arxiv.org/abs/2106.02856"/>
        <updated>2021-06-08T02:20:23.721Z</updated>
        <summary type="html"><![CDATA[We present an end-to-end framework for the Assignment Problem with multiple
tasks mapped to a group of workers, using reinforcement learning while
preserving many constraints. Tasks and workers have time constraints and there
is a cost associated with assigning a worker to a task. Each worker can perform
multiple tasks until it exhausts its allowed time units (capacity). We train a
reinforcement learning agent to find near optimal solutions to the problem by
minimizing total cost associated with the assignments while maintaining hard
constraints. We use proximal policy optimization to optimize model parameters.
The model generates a sequence of actions in real-time which correspond to task
assignment to workers, without having to retrain for changes in the dynamic
state of the environment. In our problem setting reward is computed as negative
of the assignment cost. We also demonstrate our results on bin packing and
capacitated vehicle routing problem, using the same framework. Our results
outperform Google OR-Tools using MIP and CP-SAT solvers with large problem
instances, in terms of solution quality and computation time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pathan_S/0/1/0/all/0/1"&gt;Sharmin Pathan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shrivastava_V/0/1/0/all/0/1"&gt;Vyom Shrivastava&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AOSLO-net: A deep learning-based method for automatic segmentation of retinal microaneurysms from adaptive optics scanning laser ophthalmoscope images. (arXiv:2106.02800v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.02800</id>
        <link href="http://arxiv.org/abs/2106.02800"/>
        <updated>2021-06-08T02:20:23.696Z</updated>
        <summary type="html"><![CDATA[Adaptive optics scanning laser ophthalmoscopy (AOSLO) provides real-time
retinal images with high resolution down to 2 $\mu m$. This technique enables
detection of the morphologies of individual microaneurysms (MAs), which are one
of the earliest signs of diabetic retinopathy (DR), a frequent complication of
diabetes that can lead to visual impairment and blindness. In contrast to
previous automatic models developed for MA detection on standard fundus
photographs, currently there is no high throughput image protocol available for
automatic analysis of AOSLO photographs. To address this urgency, we introduce
AOSLO-net, a deep neural network framework with customized training policy,
including preprocessing, data augmentation and transfer learning, to
automatically segment MAs from AOSLO images. We evaluate the performance of
AOSLO-net using 87 DR AOSLO images demonstrating very accurate MA detection and
segmentation, leading to correct MA morphological classification, while
outperforming the state-of-the-art both in accuracy and cost.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Qian Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sampani_K/0/1/0/all/0/1"&gt;Konstantina Sampani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xu_M/0/1/0/all/0/1"&gt;Mengjia Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cai_S/0/1/0/all/0/1"&gt;Shengze Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Deng_Y/0/1/0/all/0/1"&gt;Yixiang Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1"&gt;He Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sun_J/0/1/0/all/0/1"&gt;Jennifer K. Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Karniadakis_G/0/1/0/all/0/1"&gt;George Em Karniadakis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Equivariant message passing for the prediction of tensorial properties and molecular spectra. (arXiv:2102.03150v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.03150</id>
        <link href="http://arxiv.org/abs/2102.03150"/>
        <updated>2021-06-08T02:20:23.689Z</updated>
        <summary type="html"><![CDATA[Message passing neural networks have become a method of choice for learning
on graphs, in particular the prediction of chemical properties and the
acceleration of molecular dynamics studies. While they readily scale to large
training data sets, previous approaches have proven to be less data efficient
than kernel methods. We identify limitations of invariant representations as a
major reason and extend the message passing formulation to rotationally
equivariant representations. On this basis, we propose the polarizable atom
interaction neural network (PaiNN) and improve on common molecule benchmarks
over previous networks, while reducing model size and inference time. We
leverage the equivariant atomwise representations obtained by PaiNN for the
prediction of tensorial properties. Finally, we apply this to the simulation of
molecular spectra, achieving speedups of 4-5 orders of magnitude compared to
the electronic structure reference.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schutt_K/0/1/0/all/0/1"&gt;Kristof T. Sch&amp;#xfc;tt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Unke_O/0/1/0/all/0/1"&gt;Oliver T. Unke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gastegger_M/0/1/0/all/0/1"&gt;Michael Gastegger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fast Walsh-Hadamard Transform and Smooth-Thresholding Based Binary Layers in Deep Neural Networks. (arXiv:2104.07085v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.07085</id>
        <link href="http://arxiv.org/abs/2104.07085"/>
        <updated>2021-06-08T02:20:23.677Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a novel layer based on fast Walsh-Hadamard
transform (WHT) and smooth-thresholding to replace $1\times 1$ convolution
layers in deep neural networks. In the WHT domain, we denoise the transform
domain coefficients using the new smooth-thresholding non-linearity, a smoothed
version of the well-known soft-thresholding operator. We also introduce a
family of multiplication-free operators from the basic 2$\times$2 Hadamard
transform to implement $3\times 3$ depthwise separable convolution layers.
Using these two types of layers, we replace the bottleneck layers in
MobileNet-V2 to reduce the network's number of parameters with a slight loss in
accuracy. For example, by replacing the final third bottleneck layers, we
reduce the number of parameters from 2.270M to 540K. This reduces the accuracy
from 95.21\% to 92.98\% on the CIFAR-10 dataset. Our approach significantly
improves the speed of data processing. The fast Walsh-Hadamard transform has a
computational complexity of $O(m\log_2 m)$. As a result, it is computationally
more efficient than the $1\times1$ convolution layer. The fast Walsh-Hadamard
layer processes a tensor in $\mathbb{R}^{10\times32\times32\times1024}$ about 2
times faster than $1\times1$ convolution layer on NVIDIA Jetson Nano computer
board.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pan_H/0/1/0/all/0/1"&gt;Hongyi Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dabawi_D/0/1/0/all/0/1"&gt;Diaa Dabawi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cetin_A/0/1/0/all/0/1"&gt;Ahmet Enis Cetin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Distributed Inference with Sparse and Quantized Communication. (arXiv:2004.01302v4 [eess.SY] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.01302</id>
        <link href="http://arxiv.org/abs/2004.01302"/>
        <updated>2021-06-08T02:20:23.670Z</updated>
        <summary type="html"><![CDATA[We consider the problem of distributed inference where agents in a network
observe a stream of private signals generated by an unknown state, and aim to
uniquely identify this state from a finite set of hypotheses. We focus on
scenarios where communication between agents is costly, and takes place over
channels with finite bandwidth. To reduce the frequency of communication, we
develop a novel event-triggered distributed learning rule that is based on the
principle of diffusing low beliefs on each false hypothesis. Building on this
principle, we design a trigger condition under which an agent broadcasts only
those components of its belief vector that have adequate innovation, to only
those neighbors that require such information. We prove that our rule
guarantees convergence to the true state exponentially fast almost surely
despite sparse communication, and that it has the potential to significantly
reduce information flow from uninformative agents to informative agents. Next,
to deal with finite-precision communication channels, we propose a distributed
learning rule that leverages the idea of adaptive quantization. We show that by
sequentially refining the range of the quantizers, every agent can learn the
truth exponentially fast almost surely, while using just $1$ bit to encode its
belief on each hypothesis. For both our proposed algorithms, we rigorously
characterize the trade-offs between communication-efficiency and the learning
rate.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Mitra_A/0/1/0/all/0/1"&gt;Aritra Mitra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Richards_J/0/1/0/all/0/1"&gt;John A. Richards&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bagchi_S/0/1/0/all/0/1"&gt;Saurabh Bagchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sundaram_S/0/1/0/all/0/1"&gt;Shreyas Sundaram&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Code-Mixing on Sesame Street: Dawn of the Adversarial Polyglots. (arXiv:2103.09593v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.09593</id>
        <link href="http://arxiv.org/abs/2103.09593"/>
        <updated>2021-06-08T02:20:23.663Z</updated>
        <summary type="html"><![CDATA[Multilingual models have demonstrated impressive cross-lingual transfer
performance. However, test sets like XNLI are monolingual at the example level.
In multilingual communities, it is common for polyglots to code-mix when
conversing with each other. Inspired by this phenomenon, we present two strong
black-box adversarial attacks (one word-level, one phrase-level) for
multilingual models that push their ability to handle code-mixed sentences to
the limit. The former uses bilingual dictionaries to propose perturbations and
translations of the clean example for sense disambiguation. The latter directly
aligns the clean example with its translations before extracting phrases as
perturbations. Our phrase-level attack has a success rate of 89.75% against
XLM-R-large, bringing its average accuracy of 79.85 down to 8.18 on XNLI.
Finally, we propose an efficient adversarial training scheme that trains in the
same number of steps as the original model and show that it improves model
accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tan_S/0/1/0/all/0/1"&gt;Samson Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1"&gt;Shafiq Joty&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Safe Tests and Always-Valid Confidence Intervals for contingency tables and beyond. (arXiv:2106.02693v1 [stat.ME])]]></title>
        <id>http://arxiv.org/abs/2106.02693</id>
        <link href="http://arxiv.org/abs/2106.02693"/>
        <updated>2021-06-08T02:20:23.657Z</updated>
        <summary type="html"><![CDATA[We develop E variables for testing whether two data streams come from the
same source or not, and more generally, whether the difference between the
sources is larger than some minimal effect size. These E variables lead to
tests that remain safe, i.e. keep their Type-I error guarantees, under flexible
sampling scenarios such as optional stopping and continuation. We also develop
the corresponding always-valid confidence intervals. In special cases our E
variables also have an optimal `growth' property under the alternative. We
illustrate the generic construction through the special case of 2x2 contingency
tables, where we also allow for the incorporation of different restrictions on
a composite alternative. Comparison to p-value analysis in simulations and a
real-world example show that E variables, through their flexibility, often
allow for early stopping of data collection, thereby retaining similar power as
classical methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Turner_R/0/1/0/all/0/1"&gt;Rosanne Turner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ly_A/0/1/0/all/0/1"&gt;Alexander Ly&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Grunwald_P/0/1/0/all/0/1"&gt;Peter Gr&amp;#xfc;nwald&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sample Complexity of Uniform Convergence for Multicalibration. (arXiv:2005.01757v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.01757</id>
        <link href="http://arxiv.org/abs/2005.01757"/>
        <updated>2021-06-08T02:20:23.634Z</updated>
        <summary type="html"><![CDATA[There is a growing interest in societal concerns in machine learning systems,
especially in fairness. Multicalibration gives a comprehensive methodology to
address group fairness. In this work, we address the multicalibration error and
decouple it from the prediction error. The importance of decoupling the
fairness metric (multicalibration) and the accuracy (prediction error) is due
to the inherent trade-off between the two, and the societal decision regarding
the "right tradeoff" (as imposed many times by regulators). Our work gives
sample complexity bounds for uniform convergence guarantees of multicalibration
error, which implies that regardless of the accuracy, we can guarantee that the
empirical and (true) multicalibration errors are close. We emphasize that our
results: (1) are more general than previous bounds, as they apply to both
agnostic and realizable settings, and do not rely on a specific type of
algorithm (such as deferentially private), (2) improve over previous
multicalibration sample complexity bounds and (3) implies uniform convergence
guarantees for the classical calibration error.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shabat_E/0/1/0/all/0/1"&gt;Eliran Shabat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cohen_L/0/1/0/all/0/1"&gt;Lee Cohen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mansour_Y/0/1/0/all/0/1"&gt;Yishay Mansour&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Exact Solver for the Weston-Watkins SVM Subproblem. (arXiv:2102.05640v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.05640</id>
        <link href="http://arxiv.org/abs/2102.05640"/>
        <updated>2021-06-08T02:20:23.627Z</updated>
        <summary type="html"><![CDATA[Recent empirical evidence suggests that the Weston-Watkins support vector
machine is among the best performing multiclass extensions of the binary SVM.
Current state-of-the-art solvers repeatedly solve a particular subproblem
approximately using an iterative strategy. In this work, we propose an
algorithm that solves the subproblem exactly using a novel reparametrization of
the Weston-Watkins dual problem. For linear WW-SVMs, our solver shows
significant speed-up over the state-of-the-art solver when the number of
classes is large. Our exact subproblem solver also allows us to prove linear
convergence of the overall solver.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yutong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Scott_C/0/1/0/all/0/1"&gt;Clayton D. Scott&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Differentially Private Multi-Armed Bandits in the Shuffle Model. (arXiv:2106.02900v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02900</id>
        <link href="http://arxiv.org/abs/2106.02900"/>
        <updated>2021-06-08T02:20:23.618Z</updated>
        <summary type="html"><![CDATA[We give an $(\varepsilon,\delta)$-differentially private algorithm for the
multi-armed bandit (MAB) problem in the shuffle model with a
distribution-dependent regret of $O\left(\left(\sum_{a\in
[k]:\Delta_a>0}\frac{\log
T}{\Delta_a}\right)+\frac{k\sqrt{\log\frac{1}{\delta}}\log
T}{\varepsilon}\right)$, and a distribution-independent regret of
$O\left(\sqrt{kT\log T}+\frac{k\sqrt{\log\frac{1}{\delta}}\log
T}{\varepsilon}\right)$, where $T$ is the number of rounds, $\Delta_a$ is the
suboptimality gap of the arm $a$, and $k$ is the total number of arms. Our
upper bound almost matches the regret of the best known algorithms for the
centralized model, and significantly outperforms the best known algorithm in
the local model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1"&gt;Jay Tenenbaum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kaplan_H/0/1/0/all/0/1"&gt;Haim Kaplan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mansour_Y/0/1/0/all/0/1"&gt;Yishay Mansour&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stemmer_U/0/1/0/all/0/1"&gt;Uri Stemmer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Discrete Auto-regressive Variational Attention Models for Text Modeling. (arXiv:2004.09764v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.09764</id>
        <link href="http://arxiv.org/abs/2004.09764"/>
        <updated>2021-06-08T02:20:23.611Z</updated>
        <summary type="html"><![CDATA[Variational autoencoders (VAEs) have been widely applied for text modeling.
In practice, however, they are troubled by two challenges: information
underrepresentation and posterior collapse. The former arises as only the last
hidden state of LSTM encoder is transformed into the latent space, which is
generally insufficient to summarize the data. The latter is a long-standing
problem during the training of VAEs as the optimization is trapped to a
disastrous local optimum. In this paper, we propose Discrete Auto-regressive
Variational Attention Model (DAVAM) to address the challenges. Specifically, we
introduce an auto-regressive variational attention approach to enrich the
latent space by effectively capturing the semantic dependency from the input.
We further design discrete latent space for the variational attention and
mathematically show that our model is free from posterior collapse. Extensive
experiments on language modeling tasks demonstrate the superiority of DAVAM
against several VAE counterparts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fang_X/0/1/0/all/0/1"&gt;Xianghong Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bai_H/0/1/0/all/0/1"&gt;Haoli Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jian Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zenglin Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lyu_M/0/1/0/all/0/1"&gt;Michael Lyu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+King_I/0/1/0/all/0/1"&gt;Irwin King&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ClawCraneNet: Leveraging Object-level Relation for Text-based Video Segmentation. (arXiv:2103.10702v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.10702</id>
        <link href="http://arxiv.org/abs/2103.10702"/>
        <updated>2021-06-08T02:20:23.579Z</updated>
        <summary type="html"><![CDATA[Text-based video segmentation is a challenging task that segments out the
natural language referred objects in videos. It essentially requires semantic
comprehension and fine-grained video understanding. Existing methods introduce
language representation into segmentation models in a bottom-up manner, which
merely conducts vision-language interaction within local receptive fields of
ConvNets. We argue that such interaction is not fulfilled since the model can
barely construct region-level relationships given partial observations, which
is contrary to the description logic of natural language/referring expressions.
In fact, people usually describe a target object using relations with other
objects, which may not be easily understood without seeing the whole video. To
address the issue, we introduce a novel top-down approach by imitating how we
human segment an object with the language guidance. We first figure out all
candidate objects in videos and then choose the refereed one by parsing
relations among those high-level objects. Three kinds of object-level relations
are investigated for precise relationship understanding, i.e., positional
relation, text-guided semantic relation, and temporal relation. Extensive
experiments on A2D Sentences and J-HMDB Sentences show our method outperforms
state-of-the-art methods by a large margin. Qualitative results also show our
results are more explainable. Besides, based on the inspiration, we win the
first place in CVPR2021 Referring Youtube-VOS challenge.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liang_C/0/1/0/all/0/1"&gt;Chen Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yu Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1"&gt;Yawei Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yi Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fast and High-Quality Blind Multi-Spectral Image Pansharpening. (arXiv:2103.09943v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.09943</id>
        <link href="http://arxiv.org/abs/2103.09943"/>
        <updated>2021-06-08T02:20:23.556Z</updated>
        <summary type="html"><![CDATA[Blind pansharpening addresses the problem of generating a high
spatial-resolution multi-spectral (HRMS) image given a low spatial-resolution
multi-spectral (LRMS) image with the guidance of its associated spatially
misaligned high spatial-resolution panchromatic (PAN) image without parametric
side information. In this paper, we propose a fast approach to blind
pansharpening and achieve state-of-the-art image reconstruction quality.
Typical blind pansharpening algorithms are often computationally intensive
since the blur kernel and the target HRMS image are often computed using
iterative solvers and in an alternating fashion. To achieve fast blind
pansharpening, we decouple the solution of the blur kernel and of the HRMS
image. First, we estimate the blur kernel by computing the kernel coefficients
with minimum total generalized variation that blur a downsampled version of the
PAN image to approximate a linear combination of the LRMS image channels. Then,
we estimate each channel of the HRMS image using local Laplacian prior to
regularize the relationship between each HRMS channel and the PAN image.
Solving the HRMS image is accelerated by both parallelizing across the channels
and by fast numerical algorithms for each channel. Due to the fast scheme and
the powerful priors we used on the blur kernel coefficients (total generalized
variation) and on the cross-channel relationship (local Laplacian prior),
numerical experiments demonstrate that our algorithm outperforms
state-of-the-art model-based counterparts in terms of both computational time
and reconstruction quality of the HRMS images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1"&gt;Lantao Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1"&gt;Dehong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mansour_H/0/1/0/all/0/1"&gt;Hassan Mansour&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boufounos_P/0/1/0/all/0/1"&gt;Petros T. Boufounos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multiscale Principle of Relevant Information for Hyperspectral Image Classification. (arXiv:1907.06022v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1907.06022</id>
        <link href="http://arxiv.org/abs/1907.06022"/>
        <updated>2021-06-08T02:20:23.549Z</updated>
        <summary type="html"><![CDATA[This paper proposes a novel architecture, termed multiscale principle of
relevant information (MPRI), to learn discriminative spectral-spatial features
for hyperspectral image (HSI) classification. MPRI inherits the merits of the
principle of relevant information (PRI) to effectively extract multiscale
information embedded in the given data, and also takes advantage of the
multilayer structure to learn representations in a coarse-to-fine manner.
Specifically, MPRI performs spectral-spatial pixel characterization (using PRI)
and feature dimensionality reduction (using regularized linear discriminant
analysis) iteratively and successively. Extensive experiments on three
benchmark data sets demonstrate that MPRI outperforms existing state-of-the-art
methods (including deep learning based ones) qualitatively and quantitatively,
especially in the scenario of limited training samples. Code of MPRI is
available at \url{this http URL}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1"&gt;Yantao Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1"&gt;Shujian Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Giraldo_L/0/1/0/all/0/1"&gt;Luis Sanchez Giraldo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Principe_J/0/1/0/all/0/1"&gt;Jose C. Principe&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Causal Abstractions of Neural Networks. (arXiv:2106.02997v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2106.02997</id>
        <link href="http://arxiv.org/abs/2106.02997"/>
        <updated>2021-06-08T02:20:23.542Z</updated>
        <summary type="html"><![CDATA[Structural analysis methods (e.g., probing and feature attribution) are
increasingly important tools for neural network analysis. We propose a new
structural analysis method grounded in a formal theory of \textit{causal
abstraction} that provides rich characterizations of model-internal
representations and their roles in input/output behavior. In this method,
neural representations are aligned with variables in interpretable causal
models, and then \textit{interchange interventions} are used to experimentally
verify that the neural representations have the causal properties of their
aligned variables. We apply this method in a case study to analyze neural
models trained on Multiply Quantified Natural Language Inference (MQNLI)
corpus, a highly complex NLI dataset that was constructed with a
tree-structured natural logic causal model. We discover that a BERT-based model
with state-of-the-art performance successfully realizes the approximate causal
structure of the natural logic causal model, whereas a simpler baseline model
fails to show any such structure, demonstrating that neural representations
encode the compositional structure of MQNLI examples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Geiger_A/0/1/0/all/0/1"&gt;Atticus Geiger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1"&gt;Hanson Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Icard_T/0/1/0/all/0/1"&gt;Thomas Icard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Potts_C/0/1/0/all/0/1"&gt;Christopher Potts&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Immediate Proximity Detection Using Wi-Fi-Enabled Smartphones. (arXiv:2106.02777v1 [cs.NI])]]></title>
        <id>http://arxiv.org/abs/2106.02777</id>
        <link href="http://arxiv.org/abs/2106.02777"/>
        <updated>2021-06-08T02:20:23.534Z</updated>
        <summary type="html"><![CDATA[Smartphone apps for exposure notification and contact tracing have been shown
to be effective in controlling the COVID-19 pandemic. However, Bluetooth Low
Energy tokens similar to those broadcast by existing apps can still be picked
up far away from the transmitting device. In this paper, we present a new class
of methods for detecting whether or not two Wi-Fi-enabled devices are in
immediate physical proximity, i.e. 2 or fewer meters apart, as established by
the U.S. Centers for Disease Control and Prevention (CDC). Our goal is to
enhance the accuracy of smartphone-based exposure notification and contact
tracing systems. We present a set of binary machine learning classifiers that
take as input pairs of Wi-Fi RSSI fingerprints. We empirically verify that a
single classifier cannot generalize well to a range of different environments
with vastly different numbers of detectable Wi-Fi Access Points (APs). However,
specialized classifiers, tailored to situations where the number of detectable
APs falls within a certain range, are able to detect immediate physical
proximity significantly more accurately. As such, we design three classifiers
for situations with low, medium, and high numbers of detectable APs. These
classifiers distinguish between pairs of RSSI fingerprints recorded 2 or fewer
meters apart and pairs recorded further apart but still in Bluetooth range. We
characterize their balanced accuracy for this task to be between 66.8% and
77.8%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hyfte_Z/0/1/0/all/0/1"&gt;Zach Van Hyfte&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zakhor_A/0/1/0/all/0/1"&gt;Avideh Zakhor&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ImageNet-21K Pretraining for the Masses. (arXiv:2104.10972v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.10972</id>
        <link href="http://arxiv.org/abs/2104.10972"/>
        <updated>2021-06-08T02:20:23.495Z</updated>
        <summary type="html"><![CDATA[ImageNet-1K serves as the primary dataset for pretraining deep learning
models for computer vision tasks. ImageNet-21K dataset, which is bigger and
more diverse, is used less frequently for pretraining, mainly due to its
complexity, low accessibility, and underestimation of its added value. This
paper aims to close this gap, and make high-quality efficient pretraining on
ImageNet-21K available for everyone. Via a dedicated preprocessing stage,
utilization of WordNet hierarchical structure, and a novel training scheme
called semantic softmax, we show that various models significantly benefit from
ImageNet-21K pretraining on numerous datasets and tasks, including small
mobile-oriented models. We also show that we outperform previous ImageNet-21K
pretraining schemes for prominent new models like ViT and Mixer. Our proposed
pretraining pipeline is efficient, accessible, and leads to SoTA reproducible
results, from a publicly available dataset. The training code and pretrained
models are available at: https://github.com/Alibaba-MIIL/ImageNet21K]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ridnik_T/0/1/0/all/0/1"&gt;Tal Ridnik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ben_Baruch_E/0/1/0/all/0/1"&gt;Emanuel Ben-Baruch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Noy_A/0/1/0/all/0/1"&gt;Asaf Noy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zelnik_Manor_L/0/1/0/all/0/1"&gt;Lihi Zelnik-Manor&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tetrad: Actively Secure 4PC for Secure Training and Inference. (arXiv:2106.02850v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2106.02850</id>
        <link href="http://arxiv.org/abs/2106.02850"/>
        <updated>2021-06-08T02:20:23.469Z</updated>
        <summary type="html"><![CDATA[In this work, we design an efficient mixed-protocol framework, Tetrad, with
applications to privacy-preserving machine learning. It is designed for the
four-party setting with at most one active corruption and supports rings.

Our fair multiplication protocol requires communicating only 5 ring elements
improving over the state-of-the-art protocol of Trident (Chaudhari et al.
NDSS'20). The technical highlights of Tetrad include efficient (a) truncation
without any overhead, (b) multi-input multiplication protocols for arithmetic
and boolean worlds, (c) garbled-world, tailor-made for the mixed-protocol
framework, and (d) conversion mechanisms to switch between the computation
styles. The fair framework is also extended to provide robustness without
inflating the costs.

The competence of Tetrad is tested with benchmarks for deep neural networks
such as LeNet and VGG16 and support vector machines. One variant of our
framework aims at minimizing the execution time, while the other focuses on the
monetary cost. We observe improvements up to 6x over Trident across these
parameters.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Koti_N/0/1/0/all/0/1"&gt;Nishat Koti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Patra_A/0/1/0/all/0/1"&gt;Arpita Patra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rachuri_R/0/1/0/all/0/1"&gt;Rahul Rachuri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suresh_A/0/1/0/all/0/1"&gt;Ajith Suresh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Latent Time-Adaptive Drift-Diffusion Model. (arXiv:2106.02742v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02742</id>
        <link href="http://arxiv.org/abs/2106.02742"/>
        <updated>2021-06-08T02:20:23.458Z</updated>
        <summary type="html"><![CDATA[Animals can quickly learn the timing of events with fixed intervals and their
rate of acquisition does not depend on the length of the interval. In contrast,
recurrent neural networks that use gradient based learning have difficulty
predicting the timing of events that depend on stimulus that occurred long ago.
We present the latent time-adaptive drift-diffusion model (LTDDM), an extension
to the time-adaptive drift-diffusion model (TDDM), a model for animal learning
of timing that exhibits behavioural properties consistent with experimental
data from animals. The performance of LTDDM is compared to that of a state of
the art long short-term memory (LSTM) recurrent neural network across three
timing tasks. Differences in the relative performance of these two models is
discussed and it is shown how LTDDM can learn these events time series orders
of magnitude faster than recurrent neural networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cimolino_G/0/1/0/all/0/1"&gt;Gabriele Cimolino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rivest_F/0/1/0/all/0/1"&gt;Francois Rivest&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SpikePropamine: Differentiable Plasticity in Spiking Neural Networks. (arXiv:2106.02681v1 [cs.NE])]]></title>
        <id>http://arxiv.org/abs/2106.02681</id>
        <link href="http://arxiv.org/abs/2106.02681"/>
        <updated>2021-06-08T02:20:23.220Z</updated>
        <summary type="html"><![CDATA[The adaptive changes in synaptic efficacy that occur between spiking neurons
have been demonstrated to play a critical role in learning for biological
neural networks. Despite this source of inspiration, many learning focused
applications using Spiking Neural Networks (SNNs) retain static synaptic
connections, preventing additional learning after the initial training period.
Here, we introduce a framework for simultaneously learning the underlying
fixed-weights and the rules governing the dynamics of synaptic plasticity and
neuromodulated synaptic plasticity in SNNs through gradient descent. We further
demonstrate the capabilities of this framework on a series of challenging
benchmarks, learning the parameters of several plasticity rules including BCM,
Oja's, and their respective set of neuromodulatory variants. The experimental
results display that SNNs augmented with differentiable plasticity are
sufficient for solving a set of challenging temporal learning tasks that a
traditional SNN fails to solve, even in the presence of significant noise.
These networks are also shown to be capable of producing locomotion on a
high-dimensional robotic learning task, where near-minimal degradation in
performance is observed in the presence of novel conditions not seen during the
initial training period.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schmidgall_S/0/1/0/all/0/1"&gt;Samuel Schmidgall&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ashkanazy_J/0/1/0/all/0/1"&gt;Julia Ashkanazy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lawson_W/0/1/0/all/0/1"&gt;Wallace Lawson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hays_J/0/1/0/all/0/1"&gt;Joe Hays&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep learning enables reference-free isotropic super-resolution for volumetric fluorescence microscopy. (arXiv:2104.09435v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.09435</id>
        <link href="http://arxiv.org/abs/2104.09435"/>
        <updated>2021-06-08T02:20:23.214Z</updated>
        <summary type="html"><![CDATA[Volumetric imaging by fluorescence microscopy is often limited by anisotropic
spatial resolution from inferior axial resolution compared to the lateral
resolution. To address this problem, here we present a deep-learning-enabled
unsupervised super-resolution technique that enhances anisotropic images in
volumetric fluorescence microscopy. In contrast to the existing deep learning
approaches that require matched high-resolution target volume images, our
method greatly reduces the effort to put into practice as the training of a
network requires as little as a single 3D image stack, without a priori
knowledge of the image formation process, registration of training data, or
separate acquisition of target data. This is achieved based on the optimal
transport driven cycle-consistent generative adversarial network that learns
from an unpaired matching between high-resolution 2D images in lateral image
plane and low-resolution 2D images in the other planes. Using fluorescence
confocal microscopy and light-sheet microscopy, we demonstrate that the trained
network not only enhances axial resolution, but also restores suppressed visual
details between the imaging planes and removes imaging artifacts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Park_H/0/1/0/all/0/1"&gt;Hyoungjun Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Na_M/0/1/0/all/0/1"&gt;Myeongsu Na&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1"&gt;Bumju Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1"&gt;Soohyun Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1"&gt;Ki Hean Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1"&gt;Sunghoe Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1"&gt;Jong Chul Ye&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adapting Long Context NLM for ASR Rescoring in Conversational Agents. (arXiv:2104.11070v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.11070</id>
        <link href="http://arxiv.org/abs/2104.11070"/>
        <updated>2021-06-08T02:20:23.207Z</updated>
        <summary type="html"><![CDATA[Neural Language Models (NLM), when trained and evaluated with context
spanning multiple utterances, have been shown to consistently outperform both
conventional n-gram language models and NLMs that use limited context. In this
paper, we investigate various techniques to incorporate turn based context
history into both recurrent (LSTM) and Transformer-XL based NLMs. For recurrent
based NLMs, we explore context carry over mechanism and feature based
augmentation, where we incorporate other forms of contextual information such
as bot response and system dialogue acts as classified by a Natural Language
Understanding (NLU) model. To mitigate the sharp nearby, fuzzy far away problem
with contextual NLM, we propose the use of attention layer over lexical
metadata to improve feature based augmentation. Additionally, we adapt our
contextual NLM towards user provided on-the-fly speech patterns by leveraging
encodings from a large pre-trained masked language model and performing fusion
with a Transformer-XL based NLM. We test our proposed models using N-best
rescoring of ASR hypotheses of task-oriented dialogues and also evaluate on
downstream NLU tasks such as intent classification and slot labeling. The best
performing model shows a relative WER between 1.6% and 9.1% and a slot labeling
F1 score improvement of 4% over non-contextual baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shenoy_A/0/1/0/all/0/1"&gt;Ashish Shenoy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bodapati_S/0/1/0/all/0/1"&gt;Sravan Bodapati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sunkara_M/0/1/0/all/0/1"&gt;Monica Sunkara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ronanki_S/0/1/0/all/0/1"&gt;Srikanth Ronanki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kirchhoff_K/0/1/0/all/0/1"&gt;Katrin Kirchhoff&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Technologies for Trustworthy Machine Learning: A Survey in a Socio-Technical Context. (arXiv:2007.08911v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.08911</id>
        <link href="http://arxiv.org/abs/2007.08911"/>
        <updated>2021-06-08T02:20:23.184Z</updated>
        <summary type="html"><![CDATA[Concerns about the societal impact of AI-based services and systems has
encouraged governments and other organisations around the world to propose AI
policy frameworks to address fairness, accountability, transparency and related
topics. To achieve the objectives of these frameworks, the data and software
engineers who build machine-learning systems require knowledge about a variety
of relevant supporting tools and techniques. In this paper we provide an
overview of technologies that support building trustworthy machine learning
systems, i.e., systems whose properties justify that people place trust in
them. We argue that four categories of system properties are instrumental in
achieving the policy objectives, namely fairness, explainability, auditability
and safety & security (FEAS). We discuss how these properties need to be
considered across all stages of the machine learning life cycle, from data
collection through run-time model inference. As a consequence, we survey in
this paper the main technologies with respect to all four of the FEAS
properties, for data-centric as well as model-centric stages of the machine
learning system life cycle. We conclude with an identification of open research
problems, with a particular focus on the connection between trustworthy machine
learning technologies and their implications for individuals and society.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Toreini_E/0/1/0/all/0/1"&gt;Ehsan Toreini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aitken_M/0/1/0/all/0/1"&gt;Mhairi Aitken&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Coopamootoo_K/0/1/0/all/0/1"&gt;Kovila P. L. Coopamootoo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Elliott_K/0/1/0/all/0/1"&gt;Karen Elliott&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zelaya_V/0/1/0/all/0/1"&gt;Vladimiro Gonzalez Zelaya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Missier_P/0/1/0/all/0/1"&gt;Paolo Missier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ng_M/0/1/0/all/0/1"&gt;Magdalene Ng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moorsel_A/0/1/0/all/0/1"&gt;Aad van Moorsel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Birds of a Feather Flock Together: A Close Look at Cooperation Emergence via Multi-Agent RL. (arXiv:2104.11455v2 [cs.MA] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.11455</id>
        <link href="http://arxiv.org/abs/2104.11455"/>
        <updated>2021-06-08T02:20:23.178Z</updated>
        <summary type="html"><![CDATA[How cooperation emerges is a long-standing and interdisciplinary problem.
Game-theoretical studies on social dilemmas reveal that altruistic incentives
are critical to the emergence of cooperation but their analyses are limited to
stateless games. For more realistic scenarios, multi-agent reinforcement
learning has been used to study sequential social dilemmas (SSDs). Recent works
show that learning to incentivize other agents can promote cooperation in SSDs.
However, we find that, with these incentivizing mechanisms, the team
cooperation level does not converge and regularly oscillates between
cooperation and defection during learning. We show that a second-order social
dilemma resulting from the incentive mechanisms is the main reason for such
fragile cooperation. We formally analyze the dynamics of second-order social
dilemmas and find that a typical tendency of humans, called homophily, provides
a promising solution. We propose a novel learning framework to encourage
homophilic incentives and show that it achieves stable cooperation in both SSDs
of public goods and tragedy of the commons.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1"&gt;Heng Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1"&gt;Tonghan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jiayuan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_C/0/1/0/all/0/1"&gt;Chi Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chongjie Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spectral Geometric Matrix Completion. (arXiv:1911.07255v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1911.07255</id>
        <link href="http://arxiv.org/abs/1911.07255"/>
        <updated>2021-06-08T02:20:23.171Z</updated>
        <summary type="html"><![CDATA[Deep Matrix Factorization (DMF) is an emerging approach to the problem of
matrix completion. Recent works have established that gradient descent applied
to a DMF model induces an implicit regularization on the rank of the recovered
matrix. In this work we interpret the DMF model through the lens of spectral
geometry. This allows us to incorporate explicit regularization without
breaking the DMF structure, thus enjoying the best of both worlds. In
particular, we focus on matrix completion problems with underlying geometric or
topological relations between the rows and/or columns. Such relations are
prevalent in matrix completion problems that arise in many applications, such
as recommender systems and drug-target interaction. Our contributions enable
DMF models to exploit these relations, and make them competitive on real
benchmarks, while exhibiting one of the first successful applications of deep
linear networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Boyarski_A/0/1/0/all/0/1"&gt;Amit Boyarski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vedula_S/0/1/0/all/0/1"&gt;Sanketh Vedula&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bronstein_A/0/1/0/all/0/1"&gt;Alex Bronstein&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Heuristic-Guided Reinforcement Learning. (arXiv:2106.02757v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02757</id>
        <link href="http://arxiv.org/abs/2106.02757"/>
        <updated>2021-06-08T02:20:23.164Z</updated>
        <summary type="html"><![CDATA[We provide a framework for accelerating reinforcement learning (RL)
algorithms by heuristics constructed from domain knowledge or offline data.
Tabula rasa RL algorithms require environment interactions or computation that
scales with the horizon of the sequential decision-making task. Using our
framework, we show how heuristic-guided RL induces a much shorter-horizon
subproblem that provably solves the original task. Our framework can be viewed
as a horizon-based regularization for controlling bias and variance in RL under
a finite interaction budget. On the theoretical side, we characterize
properties of a good heuristic and its impact on RL acceleration. In
particular, we introduce the novel concept of an "improvable heuristic" -- a
heuristic that allows an RL agent to extrapolate beyond its prior knowledge. On
the empirical side, we instantiate our framework to accelerate several
state-of-the-art algorithms in simulated robotic control tasks and procedurally
generated games. Our framework complements the rich literature on warm-starting
RL with expert demonstrations or exploratory datasets, and introduces a
principled method for injecting prior knowledge into RL.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_C/0/1/0/all/0/1"&gt;Ching-An Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kolobov_A/0/1/0/all/0/1"&gt;Andrey Kolobov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Swaminathan_A/0/1/0/all/0/1"&gt;Adith Swaminathan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[End-to-end learnable EEG channel selection for deep neural networks with Gumbel-softmax. (arXiv:2102.09050v3 [eess.SP] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.09050</id>
        <link href="http://arxiv.org/abs/2102.09050"/>
        <updated>2021-06-08T02:20:23.158Z</updated>
        <summary type="html"><![CDATA[Many electroencephalography (EEG) applications rely on channel selection
methods to remove the least informative channels, e.g., to reduce the amount of
electrodes to be mounted, to decrease the computational load, or to reduce
overfitting effects and improve performance. Wrapper-based channel selection
methods aim to match the channel selection step to the target model, yet they
require to re-train the model multiple times on different candidate channel
subsets, which often leads to an unacceptably high computational cost,
especially when said model is a (deep) neural network. To alleviate this, we
propose a framework to embed the EEG channel selection in the neural network
itself to jointly learn the network weights and optimal channels in an
end-to-end manner by traditional backpropagation algorithms. We deal with the
discrete nature of this new optimization problem by employing continuous
relaxations of the discrete channel selection parameters based on the
Gumbel-softmax trick. We also propose a regularization method that discourages
selecting channels more than once. This generic approach is evaluated on two
different EEG tasks: motor imagery brain-computer interfaces and auditory
attention decoding. The results demonstrate that our framework is generally
applicable, while being competitive with state-of-the art EEG channel selection
methods, tailored to these tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Strypsteen_T/0/1/0/all/0/1"&gt;Thomas Strypsteen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bertrand_A/0/1/0/all/0/1"&gt;Alexander Bertrand&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Training Robust Graph Neural Networks with Topology Adaptive Edge Dropping. (arXiv:2106.02892v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02892</id>
        <link href="http://arxiv.org/abs/2106.02892"/>
        <updated>2021-06-08T02:20:23.139Z</updated>
        <summary type="html"><![CDATA[Graph neural networks (GNNs) are processing architectures that exploit graph
structural information to model representations from network data. Despite
their success, GNNs suffer from sub-optimal generalization performance given
limited training data, referred to as over-fitting. This paper proposes
Topology Adaptive Edge Dropping (TADropEdge) method as an adaptive data
augmentation technique to improve generalization performance and learn robust
GNN models. We start by explicitly analyzing how random edge dropping increases
the data diversity during training, while indicating i.i.d. edge dropping does
not account for graph structural information and could result in noisy
augmented data degrading performance. To overcome this issue, we consider graph
connectivity as the key property that captures graph topology. TADropEdge
incorporates this factor into random edge dropping such that the edge-dropped
subgraphs maintain similar topology as the underlying graph, yielding more
satisfactory data augmentation. In particular, TADropEdge first leverages the
graph spectrum to assign proper weights to graph edges, which represent their
criticality for establishing the graph connectivity. It then normalizes the
edge weights and drops graph edges adaptively based on their normalized
weights. Besides improving generalization performance, TADropEdge reduces
variance for efficient training and can be applied as a generic method modular
to different GNN models. Intensive experiments on real-life and synthetic
datasets corroborate theory and verify the effectiveness of the proposed
method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1"&gt;Zhan Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhattacharya_S/0/1/0/all/0/1"&gt;Subhrajit Bhattacharya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Leiming Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Blum_R/0/1/0/all/0/1"&gt;Rick S. Blum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ribeiro_A/0/1/0/all/0/1"&gt;Alejandro Ribeiro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sadler_B/0/1/0/all/0/1"&gt;Brian M. Sadler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adaptive Latent Space Tuning for Non-Stationary Distributions. (arXiv:2105.03584v4 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.03584</id>
        <link href="http://arxiv.org/abs/2105.03584"/>
        <updated>2021-06-08T02:20:23.132Z</updated>
        <summary type="html"><![CDATA[Powerful deep learning tools, such as convolutional neural networks (CNN),
are able to learn the input-output relationships of large complicated systems
directly from data. Encoder-decoder deep CNNs are able to extract features
directly from images, mix them with scalar inputs within a general
low-dimensional latent space, and then generate new complex 2D outputs which
represent complex physical phenomenon. One important challenge faced by deep
learning methods is large non-stationary systems whose characteristics change
quickly with time for which re-training is not feasible. In this paper we
present a method for adaptive tuning of the low-dimensional latent space of
deep encoder-decoder style CNNs based on real-time feedback to quickly
compensate for unknown and fast distribution shifts. We demonstrate our
approach for predicting the properties of a time-varying charged particle beam
in a particle accelerator whose components (accelerating electric fields and
focusing magnetic fields) are also quickly changing with time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Scheinker_A/0/1/0/all/0/1"&gt;Alexander Scheinker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Cropp_F/0/1/0/all/0/1"&gt;Frederick Cropp&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Paiagua_S/0/1/0/all/0/1"&gt;Sergio Paiagua&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Filippetto_D/0/1/0/all/0/1"&gt;Daniele Filippetto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dual Attentive Sequential Learning for Cross-Domain Click-Through Rate Prediction. (arXiv:2106.02768v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.02768</id>
        <link href="http://arxiv.org/abs/2106.02768"/>
        <updated>2021-06-08T02:20:23.125Z</updated>
        <summary type="html"><![CDATA[Cross domain recommender system constitutes a powerful method to tackle the
cold-start and sparsity problem by aggregating and transferring user
preferences across multiple category domains. Therefore, it has great potential
to improve click-through-rate prediction performance in online commerce
platforms having many domains of products. While several cross domain
sequential recommendation models have been proposed to leverage information
from a source domain to improve CTR predictions in a target domain, they did
not take into account bidirectional latent relations of user preferences across
source-target domain pairs. As such, they cannot provide enhanced cross-domain
CTR predictions for both domains simultaneously. In this paper, we propose a
novel approach to cross-domain sequential recommendations based on the dual
learning mechanism that simultaneously transfers information between two
related domains in an iterative manner until the learning process stabilizes.
In particular, the proposed Dual Attentive Sequential Learning (DASL) model
consists of two novel components Dual Embedding and Dual Attention, which
jointly establish the two-stage learning process: we first construct dual
latent embeddings that extract user preferences in both domains simultaneously,
and subsequently provide cross-domain recommendations by matching the extracted
latent embeddings with candidate items through dual-attention learning
mechanism. We conduct extensive offline experiments on three real-world
datasets to demonstrate the superiority of our proposed model, which
significantly and consistently outperforms several state-of-the-art baselines
across all experimental settings. We also conduct an online A/B test at a major
video streaming platform Alibaba-Youku, where our proposed model significantly
improves business performance over the latest production system in the company.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1"&gt;Pan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1"&gt;Zhichao Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Que_M/0/1/0/all/0/1"&gt;Maofei Que&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1"&gt;Yao Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tuzhilin_A/0/1/0/all/0/1"&gt;Alexander Tuzhilin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Local Disentanglement in Variational Auto-Encoders Using Jacobian $L_1$ Regularization. (arXiv:2106.02923v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02923</id>
        <link href="http://arxiv.org/abs/2106.02923"/>
        <updated>2021-06-08T02:20:23.112Z</updated>
        <summary type="html"><![CDATA[There have been many recent advances in representation learning; however,
unsupervised representation learning can still struggle with model
identification issues. Variational Auto-Encoders (VAEs) and their extensions
such as $\beta$-VAEs have been shown to locally align latent variables with PCA
directions, which can help to improve model disentanglement under some
conditions. Borrowing inspiration from Independent Component Analysis (ICA) and
sparse coding, we propose applying an $L_1$ loss to the VAE's generative
Jacobian during training to encourage local latent variable alignment with
independent factors of variation in the data. We demonstrate our results on a
variety of datasets, giving qualitative and quantitative results using
information theoretic and modularity measures that show our added $L_1$ cost
encourages local axis alignment of the latent representation with individual
factors of variation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rhodes_T/0/1/0/all/0/1"&gt;Travers Rhodes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1"&gt;Daniel D. Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hierarchical Temperature Imaging Using Pseudo-Inversed Convolutional Neural Network Aided TDLAS Tomography. (arXiv:2106.02901v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.02901</id>
        <link href="http://arxiv.org/abs/2106.02901"/>
        <updated>2021-06-08T02:20:23.105Z</updated>
        <summary type="html"><![CDATA[As an in situ combustion diagnostic tool, Tunable Diode Laser Absorption
Spectroscopy (TDLAS) tomography has been widely used for imaging of
two-dimensional temperature distributions in reactive flows. Compared with the
computational tomographic algorithms, Convolutional Neural Networks (CNNs) have
been proofed to be more robust and accurate for image reconstruction,
particularly in case of limited access of laser beams in the Region of Interest
(RoI). In practice, flame in the RoI that requires to be reconstructed with
good spatial resolution is commonly surrounded by low-temperature background.
Although the background is not of high interest, spectroscopic absorption still
exists due to heat dissipation and gas convection. Therefore, we propose a
Pseudo-Inversed CNN (PI-CNN) for hierarchical temperature imaging that (a) uses
efficiently the training and learning resources for temperature imaging in the
RoI with good spatial resolution, and (b) reconstructs the less spatially
resolved background temperature by adequately addressing the integrity of the
spectroscopic absorption model. In comparison with the traditional CNN, the
newly introduced pseudo inversion of the RoI sensitivity matrix is more
penetrating for revealing the inherent correlation between the projection data
and the RoI to be reconstructed, thus prioritising the temperature imaging in
the RoI with high accuracy and high computational efficiency. In this paper,
the proposed algorithm was validated by both numerical simulation and lab-scale
experiment, indicating good agreement between the phantoms and the
high-fidelity reconstructions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Si_J/0/1/0/all/0/1"&gt;Jingjing Si&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_G/0/1/0/all/0/1"&gt;Guoliang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cheng_Y/0/1/0/all/0/1"&gt;Yinbo Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Rui Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Enemali_G/0/1/0/all/0/1"&gt;Godwin Enemali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liu_C/0/1/0/all/0/1"&gt;Chang Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A novel multi-scale loss function for classification problems in machine learning. (arXiv:2106.02676v1 [math.NA])]]></title>
        <id>http://arxiv.org/abs/2106.02676</id>
        <link href="http://arxiv.org/abs/2106.02676"/>
        <updated>2021-06-08T02:20:23.083Z</updated>
        <summary type="html"><![CDATA[We introduce two-scale loss functions for use in various gradient descent
algorithms applied to classification problems via deep neural networks. This
new method is generic in the sense that it can be applied to a wide range of
machine learning architectures, from deep neural networks to support vector
machines for example. These two-scale loss functions allow to focus the
training onto objects in the training set which are not well classified. This
leads to an increase in several measures of performance for
appropriately-defined two-scale loss functions with respect to the more
classical cross-entropy when tested on traditional deep neural networks on the
MNIST, CIFAR10, and CIFAR100 data-sets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Berlyand_L/0/1/0/all/0/1"&gt;Leonid Berlyand&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Creese_R/0/1/0/all/0/1"&gt;Robert Creese&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Jabin_P/0/1/0/all/0/1"&gt;Pierre-Emmanuel Jabin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Kernel approximation on algebraic varieties. (arXiv:2106.02755v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02755</id>
        <link href="http://arxiv.org/abs/2106.02755"/>
        <updated>2021-06-08T02:20:23.077Z</updated>
        <summary type="html"><![CDATA[Low-rank approximation of kernels is a fundamental mathematical problem with
widespread algorithmic applications. Often the kernel is restricted to an
algebraic variety, e.g., in problems involving sparse or low-rank data. We show
that significantly better approximations are obtainable in this setting: the
rank required to achieve a given error depends on the variety's dimension
rather than the ambient dimension, which is typically much larger. This is true
in both high-precision and high-dimensional regimes. Our results are presented
for smooth isotropic kernels, the predominant class of kernels used in
applications. Our main technical insight is to approximate smooth kernels by
polynomial kernels, and leverage two key properties of polynomial kernels that
hold when they are restricted to a variety. First, their ranks decrease
exponentially in the variety's co-dimension. Second, their maximum values are
governed by their values over a small set of points. Together, our results
provide a general approach for exploiting (approximate) "algebraic structure"
in datasets in order to efficiently solve large-scale data science problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Altschuler_J/0/1/0/all/0/1"&gt;Jason M. Altschuler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parrilo_P/0/1/0/all/0/1"&gt;Pablo A. Parrilo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Neural Networks using a Single Neuron: Folded-in-Time Architecture using Feedback-Modulated Delay Loops. (arXiv:2011.10115v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.10115</id>
        <link href="http://arxiv.org/abs/2011.10115"/>
        <updated>2021-06-08T02:20:23.071Z</updated>
        <summary type="html"><![CDATA[Deep neural networks are among the most widely applied machine learning tools
showing outstanding performance in a broad range of tasks. We present a method
for folding a deep neural network of arbitrary size into a single neuron with
multiple time-delayed feedback loops. This single-neuron deep neural network
comprises only a single nonlinearity and appropriately adjusted modulations of
the feedback signals. The network states emerge in time as a temporal unfolding
of the neuron's dynamics. By adjusting the feedback-modulation within the
loops, we adapt the network's connection weights. These connection weights are
determined via a back-propagation algorithm, where both the delay-induced and
local network connections must be taken into account. Our approach can fully
represent standard Deep Neural Networks (DNN), encompasses sparse DNNs, and
extends the DNN concept toward dynamical systems implementations. The new
method, which we call Folded-in-time DNN (Fit-DNN), exhibits promising
performance in a set of benchmark tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Stelzer_F/0/1/0/all/0/1"&gt;Florian Stelzer&lt;/a&gt; (1, 2 and 4), &lt;a href="http://arxiv.org/find/cs/1/au:+Rohm_A/0/1/0/all/0/1"&gt;Andr&amp;#xe9; R&amp;#xf6;hm&lt;/a&gt; (3), &lt;a href="http://arxiv.org/find/cs/1/au:+Vicente_R/0/1/0/all/0/1"&gt;Raul Vicente&lt;/a&gt; (4), &lt;a href="http://arxiv.org/find/cs/1/au:+Fischer_I/0/1/0/all/0/1"&gt;Ingo Fischer&lt;/a&gt; (3), &lt;a href="http://arxiv.org/find/cs/1/au:+Yanchuk_S/0/1/0/all/0/1"&gt;Serhiy Yanchuk&lt;/a&gt; (1) ((1) Institute of Mathematics, Technische Universit&amp;#xe4;t Berlin, Germany, (2) Department of Mathematics, Humboldt-Universit&amp;#xe4;t zu Berlin, Germany, (3) Instituto de F&amp;#xed;sica Interdisciplinar y Sistemas Complejos, IFISC (UIB-CSIC), Spain, (4) Institute of Computer Science, University of Tartu, Estonia)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Can Subnetwork Structure be the Key to Out-of-Distribution Generalization?. (arXiv:2106.02890v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02890</id>
        <link href="http://arxiv.org/abs/2106.02890"/>
        <updated>2021-06-08T02:20:23.061Z</updated>
        <summary type="html"><![CDATA[Can models with particular structure avoid being biased towards spurious
correlation in out-of-distribution (OOD) generalization? Peters et al. (2016)
provides a positive answer for linear cases. In this paper, we use a functional
modular probing method to analyze deep model structures under OOD setting. We
demonstrate that even in biased models (which focus on spurious correlation)
there still exist unbiased functional subnetworks. Furthermore, we articulate
and demonstrate the functional lottery ticket hypothesis: full network contains
a subnetwork that can achieve better OOD performance. We then propose Modular
Risk Minimization to solve the subnetwork selection problem. Our algorithm
learns the subnetwork structure from a given dataset, and can be combined with
any other OOD regularization methods. Experiments on various OOD generalization
tasks corroborate the effectiveness of our method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1"&gt;Dinghuai Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ahuja_K/0/1/0/all/0/1"&gt;Kartik Ahuja&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yilun Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yisen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Courville_A/0/1/0/all/0/1"&gt;Aaron Courville&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ensemble Defense with Data Diversity: Weak Correlation Implies Strong Robustness. (arXiv:2106.02867v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02867</id>
        <link href="http://arxiv.org/abs/2106.02867"/>
        <updated>2021-06-08T02:20:23.056Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a framework of filter-based ensemble of deep
neuralnetworks (DNNs) to defend against adversarial attacks. The framework
builds an ensemble of sub-models -- DNNs with differentiated preprocessing
filters. From the theoretical perspective of DNN robustness, we argue that
under the assumption of high quality of the filters, the weaker the
correlations of the sensitivity of the filters are, the more robust the
ensemble model tends to be, and this is corroborated by the experiments of
transfer-based attacks. Correspondingly, we propose a principle that chooses
the specific filters with smaller Pearson correlation coefficients, which
ensures the diversity of the inputs received by DNNs, as well as the
effectiveness of the entire framework against attacks. Our ensemble models are
more robust than those constructed by previous defense methods like adversarial
training, and even competitive with the classical ensemble of adversarial
trained DNNs under adversarial attacks when the attacking radius is large.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1"&gt;Renjue Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Hanwei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_P/0/1/0/all/0/1"&gt;Pengfei Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1"&gt;Cheng-Chao Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_A/0/1/0/all/0/1"&gt;Aimin Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xue_B/0/1/0/all/0/1"&gt;Bai Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lijun Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Decentralized Q-Learning in Zero-sum Markov Games. (arXiv:2106.02748v1 [cs.GT])]]></title>
        <id>http://arxiv.org/abs/2106.02748</id>
        <link href="http://arxiv.org/abs/2106.02748"/>
        <updated>2021-06-08T02:20:23.040Z</updated>
        <summary type="html"><![CDATA[We study multi-agent reinforcement learning (MARL) in infinite-horizon
discounted zero-sum Markov games. We focus on the practical but challenging
setting of decentralized MARL, where agents make decisions without coordination
by a centralized controller, but only based on their own payoffs and local
actions executed. The agents need not observe the opponent's actions or
payoffs, possibly being even oblivious to the presence of the opponent, nor be
aware of the zero-sum structure of the underlying game, a setting also referred
to as radically uncoupled in the literature of learning in games. In this
paper, we develop for the first time a radically uncoupled Q-learning dynamics
that is both rational and convergent: the learning dynamics converges to the
best response to the opponent's strategy when the opponent follows an
asymptotically stationary strategy; the value function estimates converge to
the payoffs at a Nash equilibrium when both agents adopt the dynamics. The key
challenge in this decentralized setting is the non-stationarity of the learning
environment from an agent's perspective, since both her own payoffs and the
system evolution depend on the actions of other agents, and each agent adapts
their policies simultaneously and independently. To address this issue, we
develop a two-timescale learning dynamics where each agent updates her local
Q-function and value function estimates concurrently, with the latter happening
at a slower timescale.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sayin_M/0/1/0/all/0/1"&gt;Muhammed O. Sayin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1"&gt;Kaiqing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leslie_D/0/1/0/all/0/1"&gt;David S. Leslie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Basar_T/0/1/0/all/0/1"&gt;Tamer Basar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ozdaglar_A/0/1/0/all/0/1"&gt;Asuman Ozdaglar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Unified Lottery Ticket Hypothesis for Graph Neural Networks. (arXiv:2102.06790v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.06790</id>
        <link href="http://arxiv.org/abs/2102.06790"/>
        <updated>2021-06-08T02:20:23.033Z</updated>
        <summary type="html"><![CDATA[With graphs rapidly growing in size and deeper graph neural networks (GNNs)
emerging, the training and inference of GNNs become increasingly expensive.
Existing network weight pruning algorithms cannot address the main space and
computational bottleneck in GNNs, caused by the size and connectivity of the
graph. To this end, this paper first presents a unified GNN sparsification
(UGS) framework that simultaneously prunes the graph adjacency matrix and the
model weights, for effectively accelerating GNN inference on large-scale
graphs. Leveraging this new tool, we further generalize the recently popular
lottery ticket hypothesis to GNNs for the first time, by defining a graph
lottery ticket (GLT) as a pair of core sub-dataset and sparse sub-network,
which can be jointly identified from the original GNN and the full dense graph
by iteratively applying UGS. Like its counterpart in convolutional neural
networks, GLT can be trained in isolation to match the performance of training
with the full model and graph, and can be drawn from both randomly initialized
and self-supervised pre-trained GNNs. Our proposal has been experimentally
verified across various GNN architectures and diverse tasks, on both
small-scale graph datasets (Cora, Citeseer and PubMed), and large-scale
datasets from the challenging Open Graph Benchmark (OGB). Specifically, for
node classification, our found GLTs achieve the same accuracies with 20%~98%
MACs saving on small graphs and 25%~85% MACs saving on large ones. For link
prediction, GLTs lead to 48%~97% and 70% MACs saving on small and large graph
datasets, respectively, without compromising predictive performance. Codes
available at https://github.com/VITA-Group/Unified-LTH-GNN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1"&gt;Tianlong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sui_Y/0/1/0/all/0/1"&gt;Yongduo Sui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xuxi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1"&gt;Aston Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhangyang Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Principle Bit Analysis: Autoencoding with Schur-Concave Loss. (arXiv:2106.02796v1 [cs.IT])]]></title>
        <id>http://arxiv.org/abs/2106.02796</id>
        <link href="http://arxiv.org/abs/2106.02796"/>
        <updated>2021-06-08T02:20:23.025Z</updated>
        <summary type="html"><![CDATA[We consider a linear autoencoder in which the latent variables are quantized,
or corrupted by noise, and the constraint is Schur-concave in the set of latent
variances. Although finding the optimal encoder/decoder pair for this setup is
a nonconvex optimization problem, we show that decomposing the source into its
principal components is optimal. If the constraint is strictly Schur-concave
and the empirical covariance matrix has only simple eigenvalues, then any
optimal encoder/decoder must decompose the source in this way. As one
application, we consider a strictly Schur-concave constraint that estimates the
number of bits needed to represent the latent variables under fixed-rate
encoding, a setup that we call \emph{Principal Bit Analysis (PBA)}. This yields
a practical, general-purpose, fixed-rate compressor that outperforms existing
algorithms. As a second application, we show that a prototypical
autoencoder-based variable-rate compressor is guaranteed to decompose the
source into its principal components.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bhadane_S/0/1/0/all/0/1"&gt;Sourbh Bhadane&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wagner_A/0/1/0/all/0/1"&gt;Aaron B. Wagner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Acharya_J/0/1/0/all/0/1"&gt;Jayadev Acharya&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SpreadGNN: Serverless Multi-task Federated Learning for Graph Neural Networks. (arXiv:2106.02743v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02743</id>
        <link href="http://arxiv.org/abs/2106.02743"/>
        <updated>2021-06-08T02:20:23.019Z</updated>
        <summary type="html"><![CDATA[Graph Neural Networks (GNNs) are the first choice methods for graph machine
learning problems thanks to their ability to learn state-of-the-art level
representations from graph-structured data. However, centralizing a massive
amount of real-world graph data for GNN training is prohibitive due to
user-side privacy concerns, regulation restrictions, and commercial
competition. Federated Learning is the de-facto standard for collaborative
training of machine learning models over many distributed edge devices without
the need for centralization. Nevertheless, training graph neural networks in a
federated setting is vaguely defined and brings statistical and systems
challenges. This work proposes SpreadGNN, a novel multi-task federated training
framework capable of operating in the presence of partial labels and absence of
a central server for the first time in the literature. SpreadGNN extends
federated multi-task learning to realistic serverless settings for GNNs, and
utilizes a novel optimization algorithm with a convergence guarantee,
Decentralized Periodic Averaging SGD (DPA-SGD), to solve decentralized
multi-task learning problems. We empirically demonstrate the efficacy of our
framework on a variety of non-I.I.D. distributed graph-level molecular property
prediction datasets with partial labels. Our results show that SpreadGNN
outperforms GNN models trained over a central server-dependent federated
learning system, even in constrained topologies. The source code is publicly
available at https://github.com/FedML-AI/SpreadGNN]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+He_C/0/1/0/all/0/1"&gt;Chaoyang He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ceyani_E/0/1/0/all/0/1"&gt;Emir Ceyani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balasubramanian_K/0/1/0/all/0/1"&gt;Keshav Balasubramanian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Annavaram_M/0/1/0/all/0/1"&gt;Murali Annavaram&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Avestimehr_S/0/1/0/all/0/1"&gt;Salman Avestimehr&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[No MCMC for me: Amortized sampling for fast and stable training of energy-based models. (arXiv:2010.04230v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.04230</id>
        <link href="http://arxiv.org/abs/2010.04230"/>
        <updated>2021-06-08T02:20:23.012Z</updated>
        <summary type="html"><![CDATA[Energy-Based Models (EBMs) present a flexible and appealing way to represent
uncertainty. Despite recent advances, training EBMs on high-dimensional data
remains a challenging problem as the state-of-the-art approaches are costly,
unstable, and require considerable tuning and domain expertise to apply
successfully. In this work, we present a simple method for training EBMs at
scale which uses an entropy-regularized generator to amortize the MCMC sampling
typically used in EBM training. We improve upon prior MCMC-based entropy
regularization methods with a fast variational approximation. We demonstrate
the effectiveness of our approach by using it to train tractable likelihood
models. Next, we apply our estimator to the recently proposed Joint Energy
Model (JEM), where we match the original performance with faster and stable
training. This allows us to extend JEM models to semi-supervised classification
on tabular data from a variety of continuous domains.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Grathwohl_W/0/1/0/all/0/1"&gt;Will Grathwohl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kelly_J/0/1/0/all/0/1"&gt;Jacob Kelly&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hashemi_M/0/1/0/all/0/1"&gt;Milad Hashemi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Norouzi_M/0/1/0/all/0/1"&gt;Mohammad Norouzi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Swersky_K/0/1/0/all/0/1"&gt;Kevin Swersky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duvenaud_D/0/1/0/all/0/1"&gt;David Duvenaud&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Network Estimation by Mixing: Adaptivity and More. (arXiv:2106.02803v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.02803</id>
        <link href="http://arxiv.org/abs/2106.02803"/>
        <updated>2021-06-08T02:20:22.994Z</updated>
        <summary type="html"><![CDATA[Networks analysis has been commonly used to study the interactions between
units of complex systems. One problem of particular interest is learning the
network's underlying connection pattern given a single and noisy instantiation.
While many methods have been proposed to address this problem in recent years,
they usually assume that the true model belongs to a known class, which is not
verifiable in most real-world applications. Consequently, network modeling
based on these methods either suffers from model misspecification or relies on
additional model selection procedures that are not well understood in theory
and can potentially be unstable in practice. To address this difficulty, we
propose a mixing strategy that leverages available arbitrary models to improve
their individual performances. The proposed method is computationally efficient
and almost tuning-free; thus, it can be used as an off-the-shelf method for
network modeling. We show that the proposed method performs equally well as the
oracle estimate when the true model is included as individual candidates. More
importantly, the method remains robust and outperforms all current estimates
even when the models are misspecified. Extensive simulation examples are used
to verify the advantage of the proposed mixing method. Evaluation of link
prediction performance on 385 real-world networks from six domains also
demonstrates the universal competitiveness of the mixing method across multiple
domains.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Li_T/0/1/0/all/0/1"&gt;Tianxi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Le_C/0/1/0/all/0/1"&gt;Can M. Le&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Increasing Depth Leads to U-Shaped Test Risk in Over-parameterized Convolutional Networks. (arXiv:2010.09610v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.09610</id>
        <link href="http://arxiv.org/abs/2010.09610"/>
        <updated>2021-06-08T02:20:22.987Z</updated>
        <summary type="html"><![CDATA[Recent works have demonstrated that increasing model capacity through width
in over-parameterized neural networks leads to a decrease in test risk. For
neural networks, however, model capacity can also be increased through depth,
yet understanding the impact of increasing depth on test risk remains an open
question. In this work, we demonstrate that the test risk of over-parameterized
convolutional networks is a U-shaped curve (i.e. monotonically decreasing, then
increasing) with increasing depth. We first provide empirical evidence for this
phenomenon via image classification experiments using both ResNets and the
convolutional neural tangent kernel (CNTK). We then present a novel linear
regression framework for characterizing the impact of depth on test risk, and
show that increasing depth leads to a U-shaped test risk for the linear CNTK.
In particular, we prove that the linear CNTK corresponds to a depth-dependent
linear transformation on the original space and characterize properties of this
transformation. We then analyze over-parameterized linear regression under
arbitrary linear transformations and, in simplified settings, provably identify
the depths which minimize each of the bias and variance terms of the test risk.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nichani_E/0/1/0/all/0/1"&gt;Eshaan Nichani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Radhakrishnan_A/0/1/0/all/0/1"&gt;Adityanarayanan Radhakrishnan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Uhler_C/0/1/0/all/0/1"&gt;Caroline Uhler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[W-RST: Towards a Weighted RST-style Discourse Framework. (arXiv:2106.02658v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02658</id>
        <link href="http://arxiv.org/abs/2106.02658"/>
        <updated>2021-06-08T02:20:22.981Z</updated>
        <summary type="html"><![CDATA[Aiming for a better integration of data-driven and linguistically-inspired
approaches, we explore whether RST Nuclearity, assigning a binary assessment of
importance between text segments, can be replaced by automatically generated,
real-valued scores, in what we call a Weighted-RST framework. In particular, we
find that weighted discourse trees from auxiliary tasks can benefit key NLP
downstream applications, compared to nuclearity-centered approaches. We further
show that real-valued importance distributions partially and interestingly
align with the assessment and uncertainty of human annotators.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huber_P/0/1/0/all/0/1"&gt;Patrick Huber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_W/0/1/0/all/0/1"&gt;Wen Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carenini_G/0/1/0/all/0/1"&gt;Giuseppe Carenini&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Patch Slimming for Efficient Vision Transformers. (arXiv:2106.02852v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02852</id>
        <link href="http://arxiv.org/abs/2106.02852"/>
        <updated>2021-06-08T02:20:22.975Z</updated>
        <summary type="html"><![CDATA[This paper studies the efficiency problem for visual transformers by
excavating redundant calculation in given networks. The recent transformer
architecture has demonstrated its effectiveness for achieving excellent
performance on a series of computer vision tasks. However, similar to that of
convolutional neural networks, the huge computational cost of vision
transformers is still a severe issue. Considering that the attention mechanism
aggregates different patches layer-by-layer, we present a novel patch slimming
approach that discards useless patches in a top-down paradigm. We first
identify the effective patches in the last layer and then use them to guide the
patch selection process of previous layers. For each layer, the impact of a
patch on the final output feature is approximated and patches with less impact
will be removed. Experimental results on benchmark datasets demonstrate that
the proposed method can significantly reduce the computational costs of vision
transformers without affecting their performances. For example, over 45% FLOPs
of the ViT-Ti model can be reduced with only 0.2% top-1 accuracy drop on the
ImageNet dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1"&gt;Yehui Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1"&gt;Kai Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yunhe Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Chang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1"&gt;Jianyuan Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Chao Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1"&gt;Dacheng Tao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Variational Perspective on Diffusion-Based Generative Models and Score Matching. (arXiv:2106.02808v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02808</id>
        <link href="http://arxiv.org/abs/2106.02808"/>
        <updated>2021-06-08T02:20:22.968Z</updated>
        <summary type="html"><![CDATA[Discrete-time diffusion-based generative models and score matching methods
have shown promising results in modeling high-dimensional image data. Recently,
Song et al. (2021) show that diffusion processes that transform data into noise
can be reversed via learning the score function, i.e. the gradient of the
log-density of the perturbed data. They propose to plug the learned score
function into an inverse formula to define a generative diffusion process.
Despite the empirical success, a theoretical underpinning of this procedure is
still lacking. In this work, we approach the (continuous-time) generative
diffusion directly and derive a variational framework for likelihood
estimation, which includes continuous-time normalizing flows as a special case,
and can be seen as an infinitely deep variational autoencoder. Under this
framework, we show that minimizing the score-matching loss is equivalent to
maximizing a lower bound of the likelihood of the plug-in reverse SDE proposed
by Song et al. (2021), bridging the theoretical gap.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1"&gt;Chin-Wei Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lim_J/0/1/0/all/0/1"&gt;Jae Hyun Lim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Courville_A/0/1/0/all/0/1"&gt;Aaron Courville&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Even More Optimal Stochastic Optimization Algorithm: Minibatching and Interpolation Learning. (arXiv:2106.02720v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02720</id>
        <link href="http://arxiv.org/abs/2106.02720"/>
        <updated>2021-06-08T02:20:22.953Z</updated>
        <summary type="html"><![CDATA[We present and analyze an algorithm for optimizing smooth and convex or
strongly convex objectives using minibatch stochastic gradient estimates. The
algorithm is optimal with respect to its dependence on both the minibatch size
and minimum expected loss simultaneously. This improves over the optimal method
of Lan (2012), which is insensitive to the minimum expected loss; over the
optimistic acceleration of Cotter et al. (2011), which has suboptimal
dependence on the minibatch size; and over the algorithm of Liu and Belkin
(2018), which is limited to least squares problems and is also similarly
suboptimal with respect to the minibatch size. Applied to interpolation
learning, the improvement over Cotter et al. and Liu and Belkin translates to a
linear, rather than square-root, parallelization speedup.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Woodworth_B/0/1/0/all/0/1"&gt;Blake Woodworth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srebro_N/0/1/0/all/0/1"&gt;Nathan Srebro&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Coarse-to-Fine Entity Representations for Document-level Relation Extraction. (arXiv:2012.02507v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.02507</id>
        <link href="http://arxiv.org/abs/2012.02507"/>
        <updated>2021-06-08T02:20:22.946Z</updated>
        <summary type="html"><![CDATA[Document-level Relation Extraction (RE) requires extracting relations
expressed within and across sentences. Recent works show that graph-based
methods, usually constructing a document-level graph that captures
document-aware interactions, can obtain useful entity representations thus
helping tackle document-level RE. These methods either focus more on the entire
graph, or pay more attention to a part of the graph, e.g., paths between the
target entity pair. However, we find that document-level RE may benefit from
focusing on both of them simultaneously. Therefore, to obtain more
comprehensive entity representations, we propose the Coarse-to-Fine Entity
Representation model (CFER) that adopts a coarse-to-fine strategy involving two
phases. First, CFER uses graph neural networks to integrate global information
in the entire graph at a coarse level. Next, CFER utilizes the global
information as a guidance to selectively aggregate path information between the
target entity pair at a fine level. In classification, we combine the entity
representations from both two levels into more comprehensive representations
for relation extraction. Experimental results on two document-level RE
datasets, DocRED and CDR, show that CFER outperforms existing models and is
robust to the uneven label distribution.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1"&gt;Damai Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1"&gt;Jing Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_S/0/1/0/all/0/1"&gt;Shuang Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_B/0/1/0/all/0/1"&gt;Baobao Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sui_Z/0/1/0/all/0/1"&gt;Zhifang Sui&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pinball-OCSVM for early-stage COVID-19 diagnosis with limited posteroanterior chest X-ray images. (arXiv:2010.08115v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.08115</id>
        <link href="http://arxiv.org/abs/2010.08115"/>
        <updated>2021-06-08T02:20:22.935Z</updated>
        <summary type="html"><![CDATA[The infection of respiratory coronavirus disease 2019 (COVID-19) starts with
the upper respiratory tract and as the virus grows, the infection can progress
to lungs and develop pneumonia. The conventional way of COVID-19 diagnosis is
reverse transcription polymerase chain reaction (RT-PCR), which is less
sensitive during early stages; especially if the patient is asymptomatic, which
may further cause more severe pneumonia. In this context, several deep learning
models have been proposed to identify pulmonary infections using publicly
available chest X-ray (CXR) image datasets for early diagnosis, better
treatment and quick cure. In these datasets, presence of less number of
COVID-19 positive samples compared to other classes (normal, pneumonia and
Tuberculosis) raises the challenge for unbiased learning of deep learning
models. All deep learning models opted class balancing techniques to solve this
issue; which however should be avoided in any medical diagnosis process.
Moreover, the deep learning models are also data hungry and need massive
computation resources. Therefore for quicker diagnosis, this research proposes
a novel pinball loss function based one-class support vector machine
(PB-OCSVM), that can work in presence of limited COVID-19 positive CXR samples
with objectives to maximize the learning efficiency and to minimize the false
predictions. The performance of the proposed model is compared with
conventional OCSVM and existing deep learning models, and the experimental
results prove that the proposed model outperformed over state-of-the-art
methods. To validate the robustness of the proposed model, experiments are also
performed with noisy CXR images and UCI benchmark datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Sonbhadra_S/0/1/0/all/0/1"&gt;Sanjay Kumar Sonbhadra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Agarwal_S/0/1/0/all/0/1"&gt;Sonali Agarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Nagabhushan_P/0/1/0/all/0/1"&gt;P. Nagabhushan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Distilled One-Shot Federated Learning. (arXiv:2009.07999v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.07999</id>
        <link href="http://arxiv.org/abs/2009.07999"/>
        <updated>2021-06-08T02:20:22.918Z</updated>
        <summary type="html"><![CDATA[Current federated learning algorithms take tens of communication rounds
transmitting unwieldy model weights under ideal circumstances and hundreds when
data is poorly distributed. Inspired by recent work on dataset distillation and
distributed one-shot learning, we propose Distilled One-Shot Federated Learning
(DOSFL) to significantly reduce the communication cost while achieving
comparable performance. In just one round, each client distills their private
dataset, sends the synthetic data (e.g. images or sentences) to the server, and
collectively trains a global model. The distilled data look like noise and are
only useful to the specific model weights, i.e., become useless after the model
updates. With this weight-less and gradient-less design, the total
communication cost of DOSFL is up to three orders of magnitude less than FedAvg
while preserving between 93% to 99% performance of a centralized counterpart.
Afterwards, clients could switch to traditional methods such as FedAvg to
finetune the last few percent to fit personalized local models with local
datasets. Through comprehensive experiments, we show the accuracy and
communication performance of DOSFL on both vision and language tasks with
different models including CNN, LSTM, Transformer, etc. We demonstrate that an
eavesdropping attacker cannot properly train a good model using the leaked
distilled data, without knowing the initial model weights. DOSFL serves as an
inexpensive method to quickly converge on a performant pre-trained model with
less than 0.1% communication cost of traditional methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yanlin Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pu_G/0/1/0/all/0/1"&gt;George Pu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1"&gt;Xiyao Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiaolin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1"&gt;Dapeng Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tensor Normal Training for Deep Learning Models. (arXiv:2106.02925v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02925</id>
        <link href="http://arxiv.org/abs/2106.02925"/>
        <updated>2021-06-08T02:20:22.909Z</updated>
        <summary type="html"><![CDATA[Despite the predominant use of first-order methods for training deep learning
models, second-order methods, and in particular, natural gradient methods,
remain of interest because of their potential for accelerating training through
the use of curvature information. Several methods with non-diagonal
preconditioning matrices, including KFAC and Shampoo, have been proposed and
shown to be effective. Based on the so-called tensor normal (TN) distribution,
we propose and analyze a brand new approximate natural gradient method, Tensor
Normal Training (TNT), which like Shampoo, only requires knowledge on the shape
of the training parameters. By approximating the probabilistically based Fisher
matrix, as opposed to the empirical Fisher matrix, our method uses the
layer-wise covariance of the sampling based gradient as the pre-conditioning
matrix. Moreover, the assumption that the sampling-based (tensor) gradient
follows a TN distribution, ensures that its covariance has a Kronecker
separable structure, which leads to a tractable approximation to the Fisher
matrix. Consequently, TNT's memory requirements and per-iteration computational
costs are only slightly higher than those for first-order methods. In our
experiments, TNT exhibited superior optimization performance to KFAC and
Shampoo, and to state-of-the-art first-order methods. Moreover, TNT
demonstrated its ability to generalize as well as these first-order methods,
using fewer epochs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1"&gt;Yi Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldfarb_D/0/1/0/all/0/1"&gt;Donald Goldfarb&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Traffic Refinery: Cost-Aware Data Representation for Machine Learning on Network Traffic. (arXiv:2010.14605v3 [cs.NI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.14605</id>
        <link href="http://arxiv.org/abs/2010.14605"/>
        <updated>2021-06-08T02:20:22.898Z</updated>
        <summary type="html"><![CDATA[Network management often relies on machine learning to make predictions about
performance and security from network traffic. Often, the representation of the
traffic is as important as the choice of the model. The features that the model
relies on, and the representation of those features, ultimately determine model
accuracy, as well as where and whether the model can be deployed in practice.
Thus, the design and evaluation of these models ultimately requires
understanding not only model accuracy but also the systems costs associated
with deploying the model in an operational network. Towards this goal, this
paper develops a new framework and system that enables a joint evaluation of
both the conventional notions of machine learning performance (e.g., model
accuracy) and the systems-level costs of different representations of network
traffic. We highlight these two dimensions for two practical network management
tasks, video streaming quality inference and malware detection, to demonstrate
the importance of exploring different representations to find the appropriate
operating point. We demonstrate the benefit of exploring a range of
representations of network traffic and present Traffic Refinery, a
proof-of-concept implementation that both monitors network traffic at 10 Gbps
and transforms traffic in real time to produce a variety of feature
representations for machine learning. Traffic Refinery both highlights this
design space and makes it possible to explore different representations for
learning, balancing systems costs related to feature extraction and model
training against model accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bronzino_F/0/1/0/all/0/1"&gt;Francesco Bronzino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schmitt_P/0/1/0/all/0/1"&gt;Paul Schmitt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ayoubi_S/0/1/0/all/0/1"&gt;Sara Ayoubi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1"&gt;Hyojoon Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Teixeira_R/0/1/0/all/0/1"&gt;Renata Teixeira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feamster_N/0/1/0/all/0/1"&gt;Nick Feamster&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GCNs-Net: A Graph Convolutional Neural Network Approach for Decoding Time-resolved EEG Motor Imagery Signals. (arXiv:2006.08924v2 [eess.SP] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.08924</id>
        <link href="http://arxiv.org/abs/2006.08924"/>
        <updated>2021-06-08T02:20:22.781Z</updated>
        <summary type="html"><![CDATA[Towards developing effective and efficient brain-computer interface (BCI)
systems, precise decoding of brain activity measured by electroencephalogram
(EEG), is highly demanded. Traditional works classify EEG signals without
considering the topological relationship among electrodes. However,
neuroscience research has increasingly emphasized network patterns of brain
dynamics. Thus, the Euclidean structure of electrodes might not adequately
reflect the interaction between signals. To fill the gap, a novel deep learning
framework based on the graph convolutional neural networks (GCNs) was presented
to enhance the decoding performance of raw EEG signals during different types
of motor imagery (MI) tasks while cooperating with the functional topological
relationship of electrodes. Based on the absolute Pearson's matrix of overall
signals, the graph Laplacian of EEG electrodes was built up. The GCNs-Net
constructed by graph convolutional layers learns the generalized features. The
followed pooling layers reduce dimensionality, and the fully-connected softmax
layer derives the final prediction. The introduced approach has been shown to
converge for both personalized and group-wise predictions. It has achieved the
highest averaged accuracy, 93.056% and 88.57% (PhysioNet Dataset), 96.24% and
80.89% (High Gamma Dataset), at the subject and group level, respectively,
compared with existing studies, which suggests adaptability and robustness to
individual variability. Moreover, the performance was stably reproducible among
repetitive experiments for cross-validation. To conclude, the GCNs-Net filters
EEG signals based on the functional topological relationship, which manages to
decode relevant features for brain motor imagery.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Lun_X/0/1/0/all/0/1"&gt;Xiangmin Lun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jia_S/0/1/0/all/0/1"&gt;Shuyue Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hou_Y/0/1/0/all/0/1"&gt;Yimin Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shi_Y/0/1/0/all/0/1"&gt;Yan Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yang Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Convergence of Tsetlin Machines for the IDENTITY- and NOT Operators. (arXiv:2007.14268v2 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.14268</id>
        <link href="http://arxiv.org/abs/2007.14268"/>
        <updated>2021-06-08T02:20:22.770Z</updated>
        <summary type="html"><![CDATA[The Tsetlin Machine (TM) is a recent machine learning algorithm with several
distinct properties, such as interpretability, simplicity, and
hardware-friendliness. Although numerous empirical evaluations report on its
performance, the mathematical analysis of its convergence is still open. In
this article, we analyze the convergence of the TM with only one clause
involved for classification. More specifically, we examine two basic logical
operators, namely, the "IDENTITY"- and "NOT" operators. Our analysis reveals
that the TM, with just one clause, can converge correctly to the intended
logical operator, learning from training data over an infinite time horizon.
Besides, it can capture arbitrarily rare patterns and select the most accurate
one when two candidate patterns are incompatible, by configuring a granularity
parameter. The analysis of the convergence of the two basic operators lays the
foundation for analyzing other logical operators. These analyses altogether,
from a mathematical perspective, provide new insights on why TMs have obtained
state-of-the-art performance on several pattern recognition problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xuan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiao_L/0/1/0/all/0/1"&gt;Lei Jiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Granmo_O/0/1/0/all/0/1"&gt;Ole-Christoffer Granmo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goodwin_M/0/1/0/all/0/1"&gt;Morten Goodwin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Topology from Synthetic Data for Unsupervised Depth Completion. (arXiv:2106.02994v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02994</id>
        <link href="http://arxiv.org/abs/2106.02994"/>
        <updated>2021-06-08T02:20:22.729Z</updated>
        <summary type="html"><![CDATA[We present a method for inferring dense depth maps from images and sparse
depth measurements by leveraging synthetic data to learn the association of
sparse point clouds with dense natural shapes, and using the image as evidence
to validate the predicted depth map. Our learned prior for natural shapes uses
only sparse depth as input, not images, so the method is not affected by the
covariate shift when attempting to transfer learned models from synthetic data
to real ones. This allows us to use abundant synthetic data with ground truth
to learn the most difficult component of the reconstruction process, which is
topology estimation, and use the image to refine the prediction based on
photometric evidence. Our approach uses fewer parameters than previous methods,
yet, achieves the state of the art on both indoor and outdoor benchmark
datasets. Code available at:
https://github.com/alexklwong/learning-topology-synthetic-data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1"&gt;Alex Wong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cicek_S/0/1/0/all/0/1"&gt;Safa Cicek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1"&gt;Stefano Soatto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Feature Flow Regularization: Improving Structured Sparsity in Deep Neural Networks. (arXiv:2106.02914v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02914</id>
        <link href="http://arxiv.org/abs/2106.02914"/>
        <updated>2021-06-08T02:20:22.703Z</updated>
        <summary type="html"><![CDATA[Pruning is a model compression method that removes redundant parameters in
deep neural networks (DNNs) while maintaining accuracy. Most available filter
pruning methods require complex treatments such as iterative pruning, features
statistics/ranking, or additional optimization designs in the training process.
In this paper, we propose a simple and effective regularization strategy from a
new perspective of evolution of features, which we call feature flow
regularization (FFR), for improving structured sparsity and filter pruning in
DNNs. Specifically, FFR imposes controls on the gradient and curvature of
feature flow along the neural network, which implicitly increases the sparsity
of the parameters. The principle behind FFR is that coherent and smooth
evolution of features will lead to an efficient network that avoids redundant
parameters. The high structured sparsity obtained from FFR enables us to prune
filters effectively. Experiments with VGGNets, ResNets on CIFAR-10/100, and
Tiny ImageNet datasets demonstrate that FFR can significantly improve both
unstructured and structured sparsity. Our pruning results in terms of reduction
of parameters and FLOPs are comparable to or even better than those of
state-of-the-art pruning methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yue Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lan_Y/0/1/0/all/0/1"&gt;Yuan Lan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Luchan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiang_Y/0/1/0/all/0/1"&gt;Yang Xiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Routines for Effective Off-Policy Reinforcement Learning. (arXiv:2106.02943v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02943</id>
        <link href="http://arxiv.org/abs/2106.02943"/>
        <updated>2021-06-08T02:20:22.697Z</updated>
        <summary type="html"><![CDATA[The performance of reinforcement learning depends upon designing an
appropriate action space, where the effect of each action is measurable, yet,
granular enough to permit flexible behavior. So far, this process involved
non-trivial user choices in terms of the available actions and their execution
frequency. We propose a novel framework for reinforcement learning that
effectively lifts such constraints. Within our framework, agents learn
effective behavior over a routine space: a new, higher-level action space,
where each routine represents a set of 'equivalent' sequences of granular
actions with arbitrary length. Our routine space is learned end-to-end to
facilitate the accomplishment of underlying off-policy reinforcement learning
objectives. We apply our framework to two state-of-the-art off-policy
algorithms and show that the resulting agents obtain relevant performance
improvements while requiring fewer interactions with the environment per
episode, improving computational efficiency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cetin_E/0/1/0/all/0/1"&gt;Edoardo Cetin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Celiktutan_O/0/1/0/all/0/1"&gt;Oya Celiktutan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Extracting Weighted Automata for Approximate Minimization in Language Modelling. (arXiv:2106.02965v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02965</id>
        <link href="http://arxiv.org/abs/2106.02965"/>
        <updated>2021-06-08T02:20:22.691Z</updated>
        <summary type="html"><![CDATA[In this paper we study the approximate minimization problem for language
modelling. We assume we are given some language model as a black box. The
objective is to obtain a weighted finite automaton (WFA) that fits within a
given size constraint and which mimics the behaviour of the original model
while minimizing some notion of distance between the black box and the
extracted WFA. We provide an algorithm for the approximate minimization of
black boxes trained for language modelling of sequential data over a one-letter
alphabet. By reformulating the problem in terms of Hankel matrices, we leverage
classical results on the approximation of Hankel operators, namely the
celebrated Adamyan-Arov-Krein (AAK) theory. This allows us to use the spectral
norm to measure the distance between the black box and the WFA. We provide
theoretical guarantees to study the potentially infinite-rank Hankel matrix of
the black box, without accessing the training data, and we prove that our
method returns an asymptotically-optimal approximation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lacroce_C/0/1/0/all/0/1"&gt;Clara Lacroce&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Panangaden_P/0/1/0/all/0/1"&gt;Prakash Panangaden&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rabusseau_G/0/1/0/all/0/1"&gt;Guillaume Rabusseau&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards High Fidelity Face Relighting with Realistic Shadows. (arXiv:2104.00825v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.00825</id>
        <link href="http://arxiv.org/abs/2104.00825"/>
        <updated>2021-06-08T02:20:22.685Z</updated>
        <summary type="html"><![CDATA[Existing face relighting methods often struggle with two problems:
maintaining the local facial details of the subject and accurately removing and
synthesizing shadows in the relit image, especially hard shadows. We propose a
novel deep face relighting method that addresses both problems. Our method
learns to predict the ratio (quotient) image between a source image and the
target image with the desired lighting, allowing us to relight the image while
maintaining the local facial details. During training, our model also learns to
accurately modify shadows by using estimated shadow masks to emphasize on the
high-contrast shadow borders. Furthermore, we introduce a method to use the
shadow mask to estimate the ambient light intensity in an image, and are thus
able to leverage multiple datasets during training with different global
lighting intensities. With quantitative and qualitative evaluations on the
Multi-PIE and FFHQ datasets, we demonstrate that our proposed method faithfully
maintains the local facial details of the subject and can accurately handle
hard shadows while achieving state-of-the-art face relighting performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hou_A/0/1/0/all/0/1"&gt;Andrew Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Ze Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sarkis_M/0/1/0/all/0/1"&gt;Michel Sarkis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bi_N/0/1/0/all/0/1"&gt;Ning Bi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tong_Y/0/1/0/all/0/1"&gt;Yiying Tong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiaoming Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Relation Alignment for Calibrated Cross-modal Retrieval. (arXiv:2105.13868v2 [cs.CL] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2105.13868</id>
        <link href="http://arxiv.org/abs/2105.13868"/>
        <updated>2021-06-08T02:20:22.677Z</updated>
        <summary type="html"><![CDATA[Despite the achievements of large-scale multimodal pre-training approaches,
cross-modal retrieval, e.g., image-text retrieval, remains a challenging task.
To bridge the semantic gap between the two modalities, previous studies mainly
focus on word-region alignment at the object level, lacking the matching
between the linguistic relation among the words and the visual relation among
the regions. The neglect of such relation consistency impairs the
contextualized representation of image-text pairs and hinders the model
performance and the interpretability. In this paper, we first propose a novel
metric, Intra-modal Self-attention Distance (ISD), to quantify the relation
consistency by measuring the semantic distance between linguistic and visual
relations. In response, we present Inter-modal Alignment on Intra-modal
Self-attentions (IAIS), a regularized training method to optimize the ISD and
calibrate intra-modal self-attentions from the two modalities mutually via
inter-modal alignment. The IAIS regularizer boosts the performance of
prevailing models on Flickr30k and MS COCO datasets by a considerable margin,
which demonstrates the superiority of our approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1"&gt;Shuhuai Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1"&gt;Junyang Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1"&gt;Guangxiang Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Men_R/0/1/0/all/0/1"&gt;Rui Men&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_A/0/1/0/all/0/1"&gt;An Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jingren Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1"&gt;Xu Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Hongxia Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding and Improving Fairness-Accuracy Trade-offs in Multi-Task Learning. (arXiv:2106.02705v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02705</id>
        <link href="http://arxiv.org/abs/2106.02705"/>
        <updated>2021-06-08T02:20:22.670Z</updated>
        <summary type="html"><![CDATA[As multi-task models gain popularity in a wider range of machine learning
applications, it is becoming increasingly important for practitioners to
understand the fairness implications associated with those models. Most
existing fairness literature focuses on learning a single task more fairly,
while how ML fairness interacts with multiple tasks in the joint learning
setting is largely under-explored. In this paper, we are concerned with how
group fairness (e.g., equal opportunity, equalized odds) as an ML fairness
concept plays out in the multi-task scenario. In multi-task learning, several
tasks are learned jointly to exploit task correlations for a more efficient
inductive transfer. This presents a multi-dimensional Pareto frontier on (1)
the trade-off between group fairness and accuracy with respect to each task, as
well as (2) the trade-offs across multiple tasks. We aim to provide a deeper
understanding on how group fairness interacts with accuracy in multi-task
learning, and we show that traditional approaches that mainly focus on
optimizing the Pareto frontier of multi-task accuracy might not perform well on
fairness goals. We propose a new set of metrics to better capture the
multi-dimensional Pareto frontier of fairness-accuracy trade-offs uniquely
presented in a multi-task learning setting. We further propose a
Multi-Task-Aware Fairness (MTA-F) approach to improve fairness in multi-task
learning. Experiments on several real-world datasets demonstrate the
effectiveness of our proposed approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yuyan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xuezhi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Beutel_A/0/1/0/all/0/1"&gt;Alex Beutel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prost_F/0/1/0/all/0/1"&gt;Flavien Prost&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jilin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chi_E/0/1/0/all/0/1"&gt;Ed H. Chi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Autonomous Optimization of Fluid Systems at Varying Length Scales. (arXiv:2105.13553v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.13553</id>
        <link href="http://arxiv.org/abs/2105.13553"/>
        <updated>2021-06-08T02:20:22.664Z</updated>
        <summary type="html"><![CDATA[Autonomous optimization is a process by which hardware conditions are
discovered that generate an optimized experimental product without the guidance
of a domain expert. We design an autonomous optimization framework to discover
the experimental conditions within fluid systems that generate discrete and
uniform droplet patterns. Generating discrete and uniform droplets requires
high-precision control over the experimental conditions of a fluid system.
Fluid stream instabilities, such as Rayleigh-Plateau instability and capillary
instability, drive the separation of a flow into individual droplets. However,
because this phenomenon leverages an instability, by nature the hardware must
be precisely tuned to achieve uniform, repeatable droplets. Typically this
requires a domain expert in the loop and constant re-tuning depending on the
hardware configuration and liquid precursor selection. Herein, we propose a
computer vision-driven Bayesian optimization framework to discover the precise
hardware conditions that generate uniform, reproducible droplets with the
desired features, leveraging flow instability without a domain expert in the
loop. This framework is validated on two fluid systems, at the micrometer and
millimeter length scales, using microfluidic and inkjet systems, respectively,
indicating the application breadth of this approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Siemenn_A/0/1/0/all/0/1"&gt;Alexander E. Siemenn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shaulsky_E/0/1/0/all/0/1"&gt;Evyatar Shaulsky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Beveridge_M/0/1/0/all/0/1"&gt;Matthew Beveridge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Buonassisi_T/0/1/0/all/0/1"&gt;Tonio Buonassisi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hashmi_S/0/1/0/all/0/1"&gt;Sara M. Hashmi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Drori_I/0/1/0/all/0/1"&gt;Iddo Drori&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Manipulating SGD with Data Ordering Attacks. (arXiv:2104.09667v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.09667</id>
        <link href="http://arxiv.org/abs/2104.09667"/>
        <updated>2021-06-08T02:20:22.657Z</updated>
        <summary type="html"><![CDATA[Machine learning is vulnerable to a wide variety of attacks. It is now well
understood that by changing the underlying data distribution, an adversary can
poison the model trained with it or introduce backdoors. In this paper we
present a novel class of training-time attacks that require no changes to the
underlying dataset or model architecture, but instead only change the order in
which data are supplied to the model. In particular, we find that the attacker
can either prevent the model from learning, or poison it to learn behaviours
specified by the attacker. Furthermore, we find that even a single
adversarially-ordered epoch can be enough to slow down model learning, or even
to reset all of the learning progress. Indeed, the attacks presented here are
not specific to the model or dataset, but rather target the stochastic nature
of modern learning procedures. We extensively evaluate our attacks on computer
vision and natural language benchmarks to find that the adversary can disrupt
model training and even introduce backdoors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shumailov_I/0/1/0/all/0/1"&gt;Ilia Shumailov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shumaylov_Z/0/1/0/all/0/1"&gt;Zakhar Shumaylov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kazhdan_D/0/1/0/all/0/1"&gt;Dmitry Kazhdan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yiren Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Papernot_N/0/1/0/all/0/1"&gt;Nicolas Papernot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Erdogdu_M/0/1/0/all/0/1"&gt;Murat A. Erdogdu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anderson_R/0/1/0/all/0/1"&gt;Ross Anderson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using a Supervised Method without supervision for foreground segmentation. (arXiv:2011.07954v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.07954</id>
        <link href="http://arxiv.org/abs/2011.07954"/>
        <updated>2021-06-08T02:20:22.640Z</updated>
        <summary type="html"><![CDATA[Neural networks are a powerful framework for foreground segmentation in video
acquired by static cameras, segmenting moving objects from the background in a
robust way in various challenging scenarios. The premier methods are those
based on supervision requiring a final training stage on a database of tens to
hundreds of manually segmented images from the specific static camera. In this
work, we propose a method to automatically create an "artificial" database that
is sufficient for training the supervised methods so that it performs better
than current unsupervised methods. It is based on combining a weak foreground
segmenter, compared to the supervised method, to extract suitable objects from
the training images and randomly inserting these objects back into a background
image. Test results are shown on the test sequences in CDnet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kassel_L/0/1/0/all/0/1"&gt;Levi Kassel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Werman_M/0/1/0/all/0/1"&gt;Michael Werman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Integrating Expert ODEs into Neural ODEs: Pharmacology and Disease Progression. (arXiv:2106.02875v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02875</id>
        <link href="http://arxiv.org/abs/2106.02875"/>
        <updated>2021-06-08T02:20:22.623Z</updated>
        <summary type="html"><![CDATA[Modeling a system's temporal behaviour in reaction to external stimuli is a
fundamental problem in many areas. Pure Machine Learning (ML) approaches often
fail in the small sample regime and cannot provide actionable insights beyond
predictions. A promising modification has been to incorporate expert domain
knowledge into ML models. The application we consider is predicting the
progression of disease under medications, where a plethora of domain knowledge
is available from pharmacology. Pharmacological models describe the dynamics of
carefully-chosen medically meaningful variables in terms of systems of Ordinary
Differential Equations (ODEs). However, these models only describe a limited
collection of variables, and these variables are often not observable in
clinical environments. To close this gap, we propose the latent hybridisation
model (LHM) that integrates a system of expert-designed ODEs with
machine-learned Neural ODEs to fully describe the dynamics of the system and to
link the expert and latent variables to observable quantities. We evaluated LHM
on synthetic data as well as real-world intensive care data of COVID-19
patients. LHM consistently outperforms previous works, especially when few
training samples are available such as at the beginning of the pandemic.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qian_Z/0/1/0/all/0/1"&gt;Zhaozhi Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zame_W/0/1/0/all/0/1"&gt;William R. Zame&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schaar_M/0/1/0/all/0/1"&gt;Mihaela van der Schaar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fleuren_L/0/1/0/all/0/1"&gt;Lucas M. Fleuren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Elbers_P/0/1/0/all/0/1"&gt;Paul Elbers&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lower Bounds on Cross-Entropy Loss in the Presence of Test-time Adversaries. (arXiv:2104.08382v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.08382</id>
        <link href="http://arxiv.org/abs/2104.08382"/>
        <updated>2021-06-08T02:20:22.603Z</updated>
        <summary type="html"><![CDATA[Understanding the fundamental limits of robust supervised learning has
emerged as a problem of immense interest, from both practical and theoretical
standpoints. In particular, it is critical to determine classifier-agnostic
bounds on the training loss to establish when learning is possible. In this
paper, we determine optimal lower bounds on the cross-entropy loss in the
presence of test-time adversaries, along with the corresponding optimal
classification outputs. Our formulation of the bound as a solution to an
optimization problem is general enough to encompass any loss function depending
on soft classifier outputs. We also propose and provide a proof of correctness
for a bespoke algorithm to compute this lower bound efficiently, allowing us to
determine lower bounds for multiple practical datasets of interest. We use our
lower bounds as a diagnostic tool to determine the effectiveness of current
robust training methods and find a gap from optimality at larger budgets.
Finally, we investigate the possibility of using of optimal classification
outputs as soft labels to empirically improve robust training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bhagoji_A/0/1/0/all/0/1"&gt;Arjun Nitin Bhagoji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cullina_D/0/1/0/all/0/1"&gt;Daniel Cullina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sehwag_V/0/1/0/all/0/1"&gt;Vikash Sehwag&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mittal_P/0/1/0/all/0/1"&gt;Prateek Mittal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Role of Entropy-based Loss for Learning Causal Structures with Continuous Optimization. (arXiv:2106.02835v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02835</id>
        <link href="http://arxiv.org/abs/2106.02835"/>
        <updated>2021-06-08T02:20:22.589Z</updated>
        <summary type="html"><![CDATA[Causal discovery from observational data is an important but challenging task
in many scientific fields. Recently, NOTEARS [Zheng et al., 2018] formulates
the causal structure learning problem as a continuous optimization problem
using least-square loss with an acyclicity constraint. Though the least-square
loss function is well justified under the standard Gaussian noise assumption,
it is limited if the assumption does not hold. In this work, we theoretically
show that the violation of the Gaussian noise assumption will hinder the causal
direction identification, making the causal orientation fully determined by the
causal strength as well as the variances of noises in the linear case and the
noises of strong non-Gaussianity in the nonlinear case. Consequently, we
propose a more general entropy-based loss that is theoretically consistent with
the likelihood score under any noise distribution. We run extensive empirical
evaluations on both synthetic data and real-world data to validate the
effectiveness of the proposed method and show that our method achieves the best
in Structure Hamming Distance, False Discovery Rate, and True Positive Rate
matrices.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cai_R/0/1/0/all/0/1"&gt;Ruichu Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1"&gt;Weilin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiao_J/0/1/0/all/0/1"&gt;Jie Qiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hao_Z/0/1/0/all/0/1"&gt;Zhifeng Hao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dynamic Hawkes Processes for Discovering Time-evolving Communities' States behind Diffusion Processes. (arXiv:2105.11152v2 [cs.SI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.11152</id>
        <link href="http://arxiv.org/abs/2105.11152"/>
        <updated>2021-06-08T02:20:22.582Z</updated>
        <summary type="html"><![CDATA[Sequences of events including infectious disease outbreaks, social network
activities, and crimes are ubiquitous and the data on such events carry
essential information about the underlying diffusion processes between
communities (e.g., regions, online user groups). Modeling diffusion processes
and predicting future events are crucial in many applications including
epidemic control, viral marketing, and predictive policing. Hawkes processes
offer a central tool for modeling the diffusion processes, in which the
influence from the past events is described by the triggering kernel. However,
the triggering kernel parameters, which govern how each community is influenced
by the past events, are assumed to be static over time. In the real world, the
diffusion processes depend not only on the influences from the past, but also
the current (time-evolving) states of the communities, e.g., people's awareness
of the disease and people's current interests. In this paper, we propose a
novel Hawkes process model that is able to capture the underlying dynamics of
community states behind the diffusion processes and predict the occurrences of
events based on the dynamics. Specifically, we model the latent dynamic
function that encodes these hidden dynamics by a mixture of neural networks.
Then we design the triggering kernel using the latent dynamic function and its
integral. The proposed method, termed DHP (Dynamic Hawkes Processes), offers a
flexible way to learn complex representations of the time-evolving communities'
states, while at the same time it allows to computing the exact likelihood,
which makes parameter learning tractable. Extensive experiments on four
real-world event datasets show that DHP outperforms five widely adopted methods
for event prediction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Okawa_M/0/1/0/all/0/1"&gt;Maya Okawa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iwata_T/0/1/0/all/0/1"&gt;Tomoharu Iwata&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tanaka_Y/0/1/0/all/0/1"&gt;Yusuke Tanaka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Toda_H/0/1/0/all/0/1"&gt;Hiroyuki Toda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kurashima_T/0/1/0/all/0/1"&gt;Takeshi Kurashima&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kashima_H/0/1/0/all/0/1"&gt;Hisashi Kashima&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generative Causal Explanations for Graph Neural Networks. (arXiv:2104.06643v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.06643</id>
        <link href="http://arxiv.org/abs/2104.06643"/>
        <updated>2021-06-08T02:20:22.575Z</updated>
        <summary type="html"><![CDATA[This paper presents Gem, a model-agnostic approach for providing
interpretable explanations for any GNNs on various graph learning tasks.
Specifically, we formulate the problem of providing explanations for the
decisions of GNNs as a causal learning task. Then we train a causal explanation
model equipped with a loss function based on Granger causality. Different from
existing explainers for GNNs, Gem explains GNNs on graph-structured data from a
causal perspective. It has better generalization ability as it has no
requirements on the internal structure of the GNNs or prior knowledge on the
graph learning tasks. In addition, Gem, once trained, can be used to explain
the target GNN very quickly. Our theoretical analysis shows that several recent
explainers fall into a unified framework of additive feature attribution
methods. Experimental results on synthetic and real-world datasets show that
Gem achieves a relative increase of the explanation accuracy by up to $30\%$
and speeds up the explanation process by up to $110\times$ as compared to its
state-of-the-art alternatives.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1"&gt;Wanyu Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lan_H/0/1/0/all/0/1"&gt;Hao Lan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Baochun Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Privacy-Preserving Training of Tree Ensembles over Continuous Data. (arXiv:2106.02769v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2106.02769</id>
        <link href="http://arxiv.org/abs/2106.02769"/>
        <updated>2021-06-08T02:20:22.556Z</updated>
        <summary type="html"><![CDATA[Most existing Secure Multi-Party Computation (MPC) protocols for
privacy-preserving training of decision trees over distributed data assume that
the features are categorical. In real-life applications, features are often
numerical. The standard ``in the clear'' algorithm to grow decision trees on
data with continuous values requires sorting of training examples for each
feature in the quest for an optimal cut-point in the range of feature values in
each node. Sorting is an expensive operation in MPC, hence finding secure
protocols that avoid such an expensive step is a relevant problem in
privacy-preserving machine learning. In this paper we propose three more
efficient alternatives for secure training of decision tree based models on
data with continuous features, namely: (1) secure discretization of the data,
followed by secure training of a decision tree over the discretized data; (2)
secure discretization of the data, followed by secure training of a random
forest over the discretized data; and (3) secure training of extremely
randomized trees (``extra-trees'') on the original data. Approaches (2) and (3)
both involve randomizing feature choices. In addition, in approach (3)
cut-points are chosen randomly as well, thereby alleviating the need to sort or
to discretize the data up front. We implemented all proposed solutions in the
semi-honest setting with additive secret sharing based MPC. In addition to
mathematically proving that all proposed approaches are correct and secure, we
experimentally evaluated and compared them in terms of classification accuracy
and runtime. We privately train tree ensembles over data sets with 1000s of
instances or features in a few minutes, with accuracies that are at par with
those obtained in the clear. This makes our solution orders of magnitude more
efficient than the existing approaches, which are based on oblivious sorting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Adams_S/0/1/0/all/0/1"&gt;Samuel Adams&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choudhary_C/0/1/0/all/0/1"&gt;Chaitali Choudhary&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cock_M/0/1/0/all/0/1"&gt;Martine De Cock&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dowsley_R/0/1/0/all/0/1"&gt;Rafael Dowsley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Melanson_D/0/1/0/all/0/1"&gt;David Melanson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nascimento_A/0/1/0/all/0/1"&gt;Anderson C. A. Nascimento&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Railsback_D/0/1/0/all/0/1"&gt;Davis Railsback&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1"&gt;Jianwei Shen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Encoder-Decoder Neural Architecture Optimization for Keyword Spotting. (arXiv:2106.02738v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02738</id>
        <link href="http://arxiv.org/abs/2106.02738"/>
        <updated>2021-06-08T02:20:22.549Z</updated>
        <summary type="html"><![CDATA[Keyword spotting aims to identify specific keyword audio utterances. In
recent years, deep convolutional neural networks have been widely utilized in
keyword spotting systems. However, their model architectures are mainly based
on off-the shelfbackbones such as VGG-Net or ResNet, instead of specially
designed for the task. In this paper, we utilize neural architecture search to
design convolutional neural network models that can boost the performance of
keyword spotting while maintaining an acceptable memory footprint.
Specifically, we search the model operators and their connections in a specific
search space with Encoder-Decoder neural architecture optimization. Extensive
evaluations on Google's Speech Commands Dataset show that the model
architecture searched by our approach achieves a state-of-the-art accuracy of
over 97%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mo_T/0/1/0/all/0/1"&gt;Tong Mo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1"&gt;Bang Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Energy-Based Learning for Cooperative Games, with Applications to Feature/Data/Model Valuations. (arXiv:2106.02938v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02938</id>
        <link href="http://arxiv.org/abs/2106.02938"/>
        <updated>2021-06-08T02:20:22.543Z</updated>
        <summary type="html"><![CDATA[Valuation problems, such as attribution-based feature interpretation, data
valuation and model valuation for ensembles, become increasingly more important
in many machine learning applications. Such problems are commonly solved by
well-known game-theoretic criteria, such as Shapley value or Banzhaf index. In
this work, we present a novel energy-based treatment for cooperative games,
with a theoretical justification by the maximum entropy framework.
Surprisingly, by conducting variational inference of the energy-based model, we
recover various game-theoretic valuation criteria, such as Shapley value and
Banzhaf index, through conducting one-step gradient ascent for maximizing the
mean-field ELBO objective. This observation also verifies the rationality of
existing criteria, as they are all trying to decouple the correlations among
the players through the mean-field approach. By running gradient ascent for
multiple steps, we achieve a trajectory of the valuations, among which we
define the valuation with the best conceivable decoupling error as the
Variational Index. We experimentally demonstrate that the proposed Variational
Index enjoys intriguing properties on certain synthetic and real-world
valuation problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bian_Y/0/1/0/all/0/1"&gt;Yatao Bian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rong_Y/0/1/0/all/0/1"&gt;Yu Rong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1"&gt;Tingyang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1"&gt;Jiaxiang Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krause_A/0/1/0/all/0/1"&gt;Andreas Krause&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1"&gt;Junzhou Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Discrete Variational Derivation of Accelerated Methods in Optimization. (arXiv:2106.02700v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2106.02700</id>
        <link href="http://arxiv.org/abs/2106.02700"/>
        <updated>2021-06-08T02:20:22.537Z</updated>
        <summary type="html"><![CDATA[Many of the new developments in machine learning are connected with
gradient-based optimization methods. Recently, these methods have been studied
using a variational perspective. This has opened up the possibility of
introducing variational and symplectic integration methods using geometric
integrators. In particular, in this paper, we introduce variational integrators
which allow us to derive different methods for optimization. Using both,
Hamilton's principle and Lagrange-d'Alembert's, we derive two families of
optimization methods in one-to-one correspondence that generalize Polyak's
heavy ball and the well known Nesterov accelerated gradient method, mimicking
the behavior of the latter which reduces the oscillations of typical momentum
methods. However, since the systems considered are explicitly time-dependent,
the preservation of symplecticity of autonomous systems occurs here solely on
the fibers. Several experiments exemplify the result.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Campos_C/0/1/0/all/0/1"&gt;C&amp;#xe9;dric M. Campos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Mahillo_A/0/1/0/all/0/1"&gt;Alejandro Mahillo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Diego_D/0/1/0/all/0/1"&gt;David Mart&amp;#xed;n de Diego&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Personalized Transformer for Explainable Recommendation. (arXiv:2105.11601v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.11601</id>
        <link href="http://arxiv.org/abs/2105.11601"/>
        <updated>2021-06-08T02:20:22.530Z</updated>
        <summary type="html"><![CDATA[Personalization of natural language generation plays a vital role in a large
spectrum of tasks, such as explainable recommendation, review summarization and
dialog systems. In these tasks, user and item IDs are important identifiers for
personalization. Transformer, which is demonstrated with strong language
modeling capability, however, is not personalized and fails to make use of the
user and item IDs since the ID tokens are not even in the same semantic space
as the words. To address this problem, we present a PErsonalized Transformer
for Explainable Recommendation (PETER), on which we design a simple and
effective learning objective that utilizes the IDs to predict the words in the
target explanation, so as to endow the IDs with linguistic meanings and to
achieve personalized Transformer. Besides generating explanations, PETER can
also make recommendations, which makes it a unified model for the whole
recommendation-explanation pipeline. Extensive experiments show that our small
unpretrained model outperforms fine-tuned BERT on the generation task, in terms
of both effectiveness and efficiency, which highlights the importance and the
nice utility of our design.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yongfeng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Li Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ImageNet-21K Pretraining for the Masses. (arXiv:2104.10972v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.10972</id>
        <link href="http://arxiv.org/abs/2104.10972"/>
        <updated>2021-06-08T02:20:22.512Z</updated>
        <summary type="html"><![CDATA[ImageNet-1K serves as the primary dataset for pretraining deep learning
models for computer vision tasks. ImageNet-21K dataset, which is bigger and
more diverse, is used less frequently for pretraining, mainly due to its
complexity, low accessibility, and underestimation of its added value. This
paper aims to close this gap, and make high-quality efficient pretraining on
ImageNet-21K available for everyone. Via a dedicated preprocessing stage,
utilization of WordNet hierarchical structure, and a novel training scheme
called semantic softmax, we show that various models significantly benefit from
ImageNet-21K pretraining on numerous datasets and tasks, including small
mobile-oriented models. We also show that we outperform previous ImageNet-21K
pretraining schemes for prominent new models like ViT and Mixer. Our proposed
pretraining pipeline is efficient, accessible, and leads to SoTA reproducible
results, from a publicly available dataset. The training code and pretrained
models are available at: https://github.com/Alibaba-MIIL/ImageNet21K]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ridnik_T/0/1/0/all/0/1"&gt;Tal Ridnik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ben_Baruch_E/0/1/0/all/0/1"&gt;Emanuel Ben-Baruch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Noy_A/0/1/0/all/0/1"&gt;Asaf Noy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zelnik_Manor_L/0/1/0/all/0/1"&gt;Lihi Zelnik-Manor&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Differentially Private Deep Learning under the Fairness Lens. (arXiv:2106.02674v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02674</id>
        <link href="http://arxiv.org/abs/2106.02674"/>
        <updated>2021-06-08T02:20:22.505Z</updated>
        <summary type="html"><![CDATA[Differential Privacy (DP) is an important privacy-enhancing technology for
private machine learning systems. It allows to measure and bound the risk
associated with an individual participation in a computation. However, it was
recently observed that DP learning systems may exacerbate bias and unfairness
for different groups of individuals. This paper builds on these important
observations and sheds light on the causes of the disparate impacts arising in
the problem of differentially private empirical risk minimization. It focuses
on the accuracy disparity arising among groups of individuals in two
well-studied DP learning methods: output perturbation and differentially
private stochastic gradient descent. The paper analyzes which data and model
properties are responsible for the disproportionate impacts, why these aspects
are affecting different groups disproportionately and proposes guidelines to
mitigate these effects. The proposed approach is evaluated on several datasets
and settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tran_C/0/1/0/all/0/1"&gt;Cuong Tran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dinh_M/0/1/0/all/0/1"&gt;My H. Dinh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fioretto_F/0/1/0/all/0/1"&gt;Ferdinando Fioretto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Zero-shot Task Adaptation using Natural Language. (arXiv:2106.02972v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2106.02972</id>
        <link href="http://arxiv.org/abs/2106.02972"/>
        <updated>2021-06-08T02:20:22.499Z</updated>
        <summary type="html"><![CDATA[Imitation learning and instruction-following are two common approaches to
communicate a user's intent to a learning agent. However, as the complexity of
tasks grows, it could be beneficial to use both demonstrations and language to
communicate with an agent. In this work, we propose a novel setting where an
agent is given both a demonstration and a description, and must combine
information from both the modalities. Specifically, given a demonstration for a
task (the source task), and a natural language description of the differences
between the demonstrated task and a related but different task (the target
task), our goal is to train an agent to complete the target task in a zero-shot
setting, that is, without any demonstrations for the target task. To this end,
we introduce Language-Aided Reward and Value Adaptation (LARVA) which, given a
source demonstration and a linguistic description of how the target task
differs, learns to output a reward / value function that accurately describes
the target task. Our experiments show that on a diverse set of adaptations, our
approach is able to complete more than 95% of target tasks when using
template-based descriptions, and more than 70% when using free-form natural
language.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Goyal_P/0/1/0/all/0/1"&gt;Prasoon Goyal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mooney_R/0/1/0/all/0/1"&gt;Raymond J. Mooney&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niekum_S/0/1/0/all/0/1"&gt;Scott Niekum&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data-driven discovery of interacting particle systems using Gaussian processes. (arXiv:2106.02735v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.02735</id>
        <link href="http://arxiv.org/abs/2106.02735"/>
        <updated>2021-06-08T02:20:22.491Z</updated>
        <summary type="html"><![CDATA[Interacting particle or agent systems that display a rich variety of
collection motions are ubiquitous in science and engineering. A fundamental and
challenging goal is to understand the link between individual interaction rules
and collective behaviors. In this paper, we study the data-driven discovery of
distance-based interaction laws in second-order interacting particle systems.
We propose a learning approach that models the latent interaction kernel
functions as Gaussian processes, which can simultaneously fulfill two inference
goals: one is the nonparametric inference of interaction kernel function with
the pointwise uncertainty quantification, and the other one is the inference of
unknown parameters in the non-collective forces of the system. We formulate
learning interaction kernel functions as a statistical inverse problem and
provide a detailed analysis of recoverability conditions, establishing that a
coercivity condition is sufficient for recoverability. We provide a
finite-sample analysis, showing that our posterior mean estimator converges at
an optimal rate equal to the one in the classical 1-dimensional Kernel Ridge
regression. Numerical results on systems that exhibit different collective
behaviors demonstrate efficient learning of our approach from scarce noisy
trajectory data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Feng_J/0/1/0/all/0/1"&gt;Jinchao Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ren_Y/0/1/0/all/0/1"&gt;Yunxiang Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Tang_S/0/1/0/all/0/1"&gt;Sui Tang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Prediction of Apophis Asteroid Flyby Optimal Trajectories and Data Fusion of Earth-Apophis Mission Launch Windows using Deep Neural Networks. (arXiv:2104.06249v2 [astro-ph.IM] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.06249</id>
        <link href="http://arxiv.org/abs/2104.06249"/>
        <updated>2021-06-08T02:20:22.483Z</updated>
        <summary type="html"><![CDATA[In recent years, understanding asteroids has shifted from light worlds to
geological worlds by exploring modern spacecraft and advanced radar and
telescopic surveys. However, flyby in 2029 will be an opportunity to conduct an
internal geophysical study and test the current hypothesis on the effects of
tidal forces on asteroids. The Earth-Apophis mission is driven by additional
factors and scientific goals beyond the unique opportunity for natural
experimentation. However, the internal geophysical structures remain largely
unknown. Understanding the strength and internal integrity of asteroids is not
just a matter of scientific curiosity. It is a practical imperative to advance
knowledge for planetary defense against the possibility of an asteroid impact.
This paper presents a conceptual robotics system required for efficiency at
every stage from entry to post-landing and for asteroid monitoring. In short,
asteroid surveillance missions are futuristic frontiers, with the potential for
technological growth that could revolutionize space exploration. Advanced space
technologies and robotic systems are needed to minimize risk and prepare these
technologies for future missions. A neural network model is implemented to
track and predict asteroids' orbits. Advanced algorithms are also needed to
numerically predict orbital events to minimize error]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/astro-ph/1/au:+Ntumba_M/0/1/0/all/0/1"&gt;Manuel Ntumba&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Gore_S/0/1/0/all/0/1"&gt;Saurabh Gore&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Awanyo_J/0/1/0/all/0/1"&gt;Jean-Baptiste Awanyo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Classification of Very Large Images with Tiny Objects. (arXiv:2106.02694v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02694</id>
        <link href="http://arxiv.org/abs/2106.02694"/>
        <updated>2021-06-08T02:20:22.464Z</updated>
        <summary type="html"><![CDATA[An increasing number of applications in the computer vision domain,
specially, in medical imaging and remote sensing, are challenging when the goal
is to classify very large images with tiny objects. More specifically, these
type of classification tasks face two key challenges: $i$) the size of the
input image in the target dataset is usually in the order of megapixels,
however, existing deep architectures do not easily operate on such big images
due to memory constraints, consequently, we seek a memory-efficient method to
process these images; and $ii$) only a small fraction of the input images are
informative of the label of interest, resulting in low region of interest (ROI)
to image ratio. However, most of the current convolutional neural networks
(CNNs) are designed for image classification datasets that have relatively
large ROIs and small image size (sub-megapixel). Existing approaches have
addressed these two challenges in isolation. We present an end-to-end CNN model
termed Zoom-In network that leverages hierarchical attention sampling for
classification of large images with tiny objects using a single GPU. We
evaluate our method on two large-image datasets and one gigapixel dataset.
Experimental results show that our model achieves higher accuracy than existing
methods while requiring less computing resources.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kong_F/0/1/0/all/0/1"&gt;Fanjie Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Henao_R/0/1/0/all/0/1"&gt;Ricardo Henao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Gradient Fields for Molecular Conformation Generation. (arXiv:2105.03902v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.03902</id>
        <link href="http://arxiv.org/abs/2105.03902"/>
        <updated>2021-06-08T02:20:22.457Z</updated>
        <summary type="html"><![CDATA[We study a fundamental problem in computational chemistry known as molecular
conformation generation, trying to predict stable 3D structures from 2D
molecular graphs. Existing machine learning approaches usually first predict
distances between atoms and then generate a 3D structure satisfying the
distances, where noise in predicted distances may induce extra errors during 3D
coordinate generation. Inspired by the traditional force field methods for
molecular dynamics simulation, in this paper, we propose a novel approach
called ConfGF by directly estimating the gradient fields of the log density of
atomic coordinates. The estimated gradient fields allow directly generating
stable conformations via Langevin dynamics. However, the problem is very
challenging as the gradient fields are roto-translation equivariant. We notice
that estimating the gradient fields of atomic coordinates can be translated to
estimating the gradient fields of interatomic distances, and hence develop a
novel algorithm based on recent score-based generative models to effectively
estimate these gradients. Experimental results across multiple tasks show that
ConfGF outperforms previous state-of-the-art baselines by a significant margin.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shi_C/0/1/0/all/0/1"&gt;Chence Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_S/0/1/0/all/0/1"&gt;Shitong Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1"&gt;Minkai Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1"&gt;Jian Tang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[D-Cliques: Compensating NonIIDness in Decentralized Federated Learning with Topology. (arXiv:2104.07365v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.07365</id>
        <link href="http://arxiv.org/abs/2104.07365"/>
        <updated>2021-06-08T02:20:22.450Z</updated>
        <summary type="html"><![CDATA[The convergence speed of machine learning models trained with Federated
Learning is significantly affected by non-independent and identically
distributed (non-IID) data partitions, even more so in a fully decentralized
setting without a central server. In this paper, we show that the impact of
local class bias, an important type of data non-IIDness, can be significantly
reduced by carefully designing the underlying communication topology. We
present D-Cliques, a novel topology that reduces gradient bias by grouping
nodes in interconnected cliques such that the local joint distribution in a
clique is representative of the global class distribution. We also show how to
adapt the updates of decentralized SGD to obtain unbiased gradients and
implement an effective momentum with D-Cliques. Our empirical evaluation on
MNIST and CIFAR10 demonstrates that our approach provides similar convergence
speed as a fully-connected topology with a significant reduction in the number
of edges and messages. In a 1000-node topology, D-Cliques requires 98% less
edges and 96% less total messages, with further possible gains using a
small-world topology across cliques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bellet_A/0/1/0/all/0/1"&gt;Aur&amp;#xe9;lien Bellet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kermarrec_A/0/1/0/all/0/1"&gt;Anne-Marie Kermarrec&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lavoie_E/0/1/0/all/0/1"&gt;Erick Lavoie&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Maximum Entropy Subspace Clustering Network. (arXiv:2012.03176v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.03176</id>
        <link href="http://arxiv.org/abs/2012.03176"/>
        <updated>2021-06-08T02:20:22.443Z</updated>
        <summary type="html"><![CDATA[Deep subspace clustering networks have attracted much attention in subspace
clustering, in which an auto-encoder non-linearly maps the input data into a
latent space, and a fully connected layer named self-expressiveness module is
introduced to learn the affinity matrix via a typical regularization term
(e.g., sparse or low-rank). However, the adopted regularization terms ignore
the connectivity within each subspace, limiting their clustering performance.
In addition, the adopted framework suffers from the coupling issue between the
auto-encoder module and the self-expressiveness module, making the network
training non-trivial. To tackle these two issues, we propose a novel deep
subspace clustering method named Maximum Entropy Subspace Clustering Network
(MESC-Net). Specifically, MESC-Net maximizes the entropy of the affinity matrix
to promote the connectivity within each subspace, in which its elements
corresponding to the same subspace are uniformly and densely distributed.
Furthermore, we design a novel framework to explicitly decouple the
auto-encoder module and the self-expressiveness module. We also theoretically
prove that the learned affinity matrix satisfies the block-diagonal property
under the independent subspaces. Extensive quantitative and qualitative results
on commonly used benchmark datasets validate MESC-Net significantly outperforms
state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Peng_Z/0/1/0/all/0/1"&gt;Zhihao Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1"&gt;Yuheng Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Hui Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1"&gt;Junhui Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Qingfu Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Chromatic and spatial analysis of one-pixel attacks against an image classifier. (arXiv:2105.13771v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.13771</id>
        <link href="http://arxiv.org/abs/2105.13771"/>
        <updated>2021-06-08T02:20:22.436Z</updated>
        <summary type="html"><![CDATA[One-pixel attack is a curious way of deceiving neural network classifier by
changing only one pixel in the input image. The full potential and boundaries
of this attack method are not yet fully understood. In this research, the
successful and unsuccessful attacks are studied in more detail to illustrate
the working mechanisms of a one-pixel attack created using differential
evolution. The data comes from our earlier studies where we applied the attack
against medical imaging. We used a real breast cancer tissue dataset and a real
classifier as the attack target. This research presents ways to analyze
chromatic and spatial distributions of one-pixel attacks. In addition, we
present one-pixel attack confidence maps to illustrate the behavior of the
target classifier. We show that the more effective attacks change the color of
the pixel more, and that the successful attacks are situated at the center of
the images. This kind of analysis is not only useful for understanding the
behavior of the attack but also the qualities of the classifying neural
network.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alatalo_J/0/1/0/all/0/1"&gt;Janne Alatalo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Korpihalkola_J/0/1/0/all/0/1"&gt;Joni Korpihalkola&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sipola_T/0/1/0/all/0/1"&gt;Tuomo Sipola&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kokkonen_T/0/1/0/all/0/1"&gt;Tero Kokkonen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Less is More: Pay Less Attention in Vision Transformers. (arXiv:2105.14217v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14217</id>
        <link href="http://arxiv.org/abs/2105.14217"/>
        <updated>2021-06-08T02:20:22.415Z</updated>
        <summary type="html"><![CDATA[Transformers have become one of the dominant architectures in deep learning,
particularly as a powerful alternative to convolutional neural networks (CNNs)
in computer vision. However, Transformer training and inference in previous
works can be prohibitively expensive due to the quadratic complexity of
self-attention over a long sequence of representations, especially for
high-resolution dense prediction tasks. To this end, we present a novel Less
attention vIsion Transformer (LIT), building upon the fact that convolutions,
fully-connected (FC) layers, and self-attentions have almost equivalent
mathematical expressions for processing image patch sequences. Specifically, we
propose a hierarchical Transformer where we use pure multi-layer perceptrons
(MLPs) to encode rich local patterns in the early stages while applying
self-attention modules to capture longer dependencies in deeper layers.
Moreover, we further propose a learned deformable token merging module to
adaptively fuse informative patches in a non-uniform manner. The proposed LIT
achieves promising performance on image recognition tasks, including image
classification, object detection and instance segmentation, serving as a strong
backbone for many vision tasks. Code is available at:
https://github.com/MonashAI/LIT]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pan_Z/0/1/0/all/0/1"&gt;Zizheng Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhuang_B/0/1/0/all/0/1"&gt;Bohan Zhuang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1"&gt;Haoyu He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jing Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1"&gt;Jianfei Cai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Policies with Zero or Bounded Constraint Violation for Constrained MDPs. (arXiv:2106.02684v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02684</id>
        <link href="http://arxiv.org/abs/2106.02684"/>
        <updated>2021-06-08T02:20:22.407Z</updated>
        <summary type="html"><![CDATA[We address the issue of safety in reinforcement learning. We pose the problem
in an episodic framework of a constrained Markov decision process. Existing
results have shown that it is possible to achieve a reward regret of
$\tilde{\mathcal{O}}(\sqrt{K})$ while allowing an
$\tilde{\mathcal{O}}(\sqrt{K})$ constraint violation in $K$ episodes. A
critical question that arises is whether it is possible to keep the constraint
violation even smaller. We show that when a strictly safe policy is known, then
one can confine the system to zero constraint violation with arbitrarily high
probability while keeping the reward regret of order
$\tilde{\mathcal{O}}(\sqrt{K})$. The algorithm which does so employs the
principle of optimistic pessimism in the face of uncertainty to achieve safe
exploration. When no strictly safe policy is known, though one is known to
exist, then it is possible to restrict the system to bounded constraint
violation with arbitrarily high probability. This is shown to be realized by a
primal-dual algorithm with an optimistic primal estimate and a pessimistic dual
update.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_R/0/1/0/all/0/1"&gt;Ruida Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kalathil_D/0/1/0/all/0/1"&gt;Dileep Kalathil&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_P/0/1/0/all/0/1"&gt;P. R. Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_C/0/1/0/all/0/1"&gt;Chao Tian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BO-DBA: Query-Efficient Decision-Based Adversarial Attacks via Bayesian Optimization. (arXiv:2106.02732v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02732</id>
        <link href="http://arxiv.org/abs/2106.02732"/>
        <updated>2021-06-08T02:20:22.400Z</updated>
        <summary type="html"><![CDATA[Decision-based attacks (DBA), wherein attackers perturb inputs to spoof
learning algorithms by observing solely the output labels, are a type of severe
adversarial attacks against Deep Neural Networks (DNNs) requiring minimal
knowledge of attackers. State-of-the-art DBA attacks relying on zeroth-order
gradient estimation require an excessive number of queries. Recently, Bayesian
optimization (BO) has shown promising in reducing the number of queries in
score-based attacks (SBA), in which attackers need to observe real-valued
probability scores as outputs. However, extending BO to the setting of DBA is
nontrivial because in DBA only output labels instead of real-valued scores, as
needed by BO, are available to attackers. In this paper, we close this gap by
proposing an efficient DBA attack, namely BO-DBA. Different from existing
approaches, BO-DBA generates adversarial examples by searching so-called
\emph{directions of perturbations}. It then formulates the problem as a BO
problem that minimizes the real-valued distortion of perturbations. With the
optimized perturbation generation process, BO-DBA converges much faster than
the state-of-the-art DBA techniques. Experimental results on pre-trained
ImageNet classifiers show that BO-DBA converges within 200 queries while the
state-of-the-art DBA techniques need over 15,000 queries to achieve the same
level of perturbation distortion. BO-DBA also shows similar attack success
rates even as compared to BO-based SBA attacks but with less distortion.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhuosheng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1"&gt;Shucheng Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers. (arXiv:2105.15203v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.15203</id>
        <link href="http://arxiv.org/abs/2105.15203"/>
        <updated>2021-06-08T02:20:22.389Z</updated>
        <summary type="html"><![CDATA[We present SegFormer, a simple, efficient yet powerful semantic segmentation
framework which unifies Transformers with lightweight multilayer perception
(MLP) decoders. SegFormer has two appealing features: 1) SegFormer comprises a
novel hierarchically structured Transformer encoder which outputs multiscale
features. It does not need positional encoding, thereby avoiding the
interpolation of positional codes which leads to decreased performance when the
testing resolution differs from training. 2) SegFormer avoids complex decoders.
The proposed MLP decoder aggregates information from different layers, and thus
combining both local attention and global attention to render powerful
representations. We show that this simple and lightweight design is the key to
efficient segmentation on Transformers. We scale our approach up to obtain a
series of models from SegFormer-B0 to SegFormer-B5, reaching significantly
better performance and efficiency than previous counterparts. For example,
SegFormer-B4 achieves 50.3% mIoU on ADE20K with 64M parameters, being 5x
smaller and 2.2% better than the previous best method. Our best model,
SegFormer-B5, achieves 84.0% mIoU on Cityscapes validation set and shows
excellent zero-shot robustness on Cityscapes-C. Code will be released at:
github.com/NVlabs/SegFormer.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_E/0/1/0/all/0/1"&gt;Enze Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wenhai Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1"&gt;Zhiding Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1"&gt;Anima Anandkumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alvarez_J/0/1/0/all/0/1"&gt;Jose M. Alvarez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1"&gt;Ping Luo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Real Time Video based Heart and Respiration Rate Monitoring. (arXiv:2106.02669v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.02669</id>
        <link href="http://arxiv.org/abs/2106.02669"/>
        <updated>2021-06-08T02:20:22.368Z</updated>
        <summary type="html"><![CDATA[In recent years, research about monitoring vital signs by smartphones grows
significantly. There are some special sensors like Electrocardiogram (ECG) and
Photoplethysmographic (PPG) to detect heart rate (HR) and respiration rate
(RR). Smartphone cameras also can measure HR by detecting and processing
imaging Photoplethysmographic (iPPG) signals from the video of a user's face.
Indeed, the variation in the intensity of the green channel can be measured by
the iPPG signals of the video. This study aimed to provide a method to extract
heart rate and respiration rate using the video of individuals' faces. The
proposed method is based on measuring fluctuations in the Hue, and can
therefore extract both HR and RR from the video of a user's face. The proposed
method is evaluated by performing on 25 healthy individuals. For each subject,
20 seconds video of his/her face is recorded. Results show that the proposed
approach of measuring iPPG using Hue gives more accurate rates than the Green
channel.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Pourbemany_J/0/1/0/all/0/1"&gt;Jafar Pourbemany&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Essa_A/0/1/0/all/0/1"&gt;Almabrok Essa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Ye Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The R-U-A-Robot Dataset: Helping Avoid Chatbot Deception by Detecting User Questions About Human or Non-Human Identity. (arXiv:2106.02692v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02692</id>
        <link href="http://arxiv.org/abs/2106.02692"/>
        <updated>2021-06-08T02:20:22.359Z</updated>
        <summary type="html"><![CDATA[Humans are increasingly interacting with machines through language, sometimes
in contexts where the user may not know they are talking to a machine (like
over the phone or a text chatbot). We aim to understand how system designers
and researchers might allow their systems to confirm its non-human identity. We
collect over 2,500 phrasings related to the intent of ``Are you a robot?". This
is paired with over 2,500 adversarially selected utterances where only
confirming the system is non-human would be insufficient or disfluent. We
compare classifiers to recognize the intent and discuss the precision/recall
and model complexity tradeoffs. Such classifiers could be integrated into
dialog systems to avoid undesired deception. We then explore how both a
generative research model (Blender) as well as two deployed systems (Amazon
Alexa, Google Assistant) handle this intent, finding that systems often fail to
confirm their non-human identity. Finally, we try to understand what a good
response to the intent would be, and conduct a user study to compare the
important aspects when responding to this intent.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gros_D/0/1/0/all/0/1"&gt;David Gros&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1"&gt;Zhou Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Enabling Meta-Learning from Target Models. (arXiv:2104.03736v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.03736</id>
        <link href="http://arxiv.org/abs/2104.03736"/>
        <updated>2021-06-08T02:20:22.234Z</updated>
        <summary type="html"><![CDATA[Meta-learning can extract an inductive bias from previous learning experience
and assist the training processes of new tasks. It is often realized through
optimizing a meta-model with the evaluation loss of a series of task-specific
solvers. Most existing algorithms sample non-overlapping $\mathit{support}$
sets and $\mathit{query}$ sets to train and evaluate the solvers respectively
due to simplicity ($\mathcal{S}/\mathcal{Q}$ protocol). However, another
evaluation method that assesses the discrepancy between the solver and a target
model is short of research ($\mathcal{S}/\mathcal{T}$ protocol).
$\mathcal{S}/\mathcal{T}$ protocol has unique advantages such as offering more
informative supervision, but it is computationally expensive. This paper looks
into this special evaluation method and takes a step towards putting it into
practice. We find that with a small ratio of tasks armed with target models,
classic meta-learning algorithms can be improved a lot without consuming many
resources. Furthermore, we empirically verify the effectiveness of
$\mathcal{S}/\mathcal{T}$ protocol in a typical application of meta-learning,
$\mathit{i.e.}$, few-shot learning. In detail, after constructing target models
by fine-tuning the pre-trained network on those hard tasks, we match the
task-specific solvers to target models via knowledge distillation. Experiments
demonstrate the superiority of our proposal.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1"&gt;Su Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1"&gt;Han-Jia Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gan_L/0/1/0/all/0/1"&gt;Le Gan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhan_D/0/1/0/all/0/1"&gt;De-Chuan Zhan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ResT: An Efficient Transformer for Visual Recognition. (arXiv:2105.13677v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.13677</id>
        <link href="http://arxiv.org/abs/2105.13677"/>
        <updated>2021-06-08T02:20:22.202Z</updated>
        <summary type="html"><![CDATA[This paper presents an efficient multi-scale vision Transformer, called ResT,
that capably served as a general-purpose backbone for image recognition. Unlike
existing Transformer methods, which employ standard Transformer blocks to
tackle raw images with a fixed resolution, our ResT have several advantages:
(1) A memory-efficient multi-head self-attention is built, which compresses the
memory by a simple depth-wise convolution, and projects the interaction across
the attention-heads dimension while keeping the diversity ability of
multi-heads; (2) Position encoding is constructed as spatial attention, which
is more flexible and can tackle with input images of arbitrary size without
interpolation or fine-tune; (3) Instead of the straightforward tokenization at
the beginning of each stage, we design the patch embedding as a stack of
overlapping convolution operation with stride on the 2D-reshaped token map. We
comprehensively validate ResT on image classification and downstream tasks.
Experimental results show that the proposed ResT can outperform the recently
state-of-the-art backbones by a large margin, demonstrating the potential of
ResT as strong backbones. The code and models will be made publicly available
at https://github.com/wofmanaf/ResT.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Qinglong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yubin Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stylizing 3D Scene via Implicit Representation and HyperNetwork. (arXiv:2105.13016v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.13016</id>
        <link href="http://arxiv.org/abs/2105.13016"/>
        <updated>2021-06-08T02:20:22.196Z</updated>
        <summary type="html"><![CDATA[In this work, we aim to address the 3D scene stylization problem - generating
stylized images of the scene at arbitrary novel view angles. A straightforward
solution is to combine existing novel view synthesis and image/video style
transfer approaches, which often leads to blurry results or inconsistent
appearance. Inspired by the high quality results of the neural radiance fields
(NeRF) method, we propose a joint framework to directly render novel views with
the desired style. Our framework consists of two components: an implicit
representation of the 3D scene with the neural radiance field model, and a
hypernetwork to transfer the style information into the scene representation.
In particular, our implicit representation model disentangles the scene into
the geometry and appearance branches, and the hypernetwork learns to predict
the parameters of the appearance branch from the reference style image. To
alleviate the training difficulties and memory burden, we propose a two-stage
training procedure and a patch sub-sampling approach to optimize the style and
content losses with the neural radiance field model. After optimization, our
model is able to render consistent novel views at arbitrary view angles with
arbitrary style. Both quantitative evaluation and human subject study have
demonstrated that the proposed method generates faithful stylization results
with consistent appearance across different views.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chiang_P/0/1/0/all/0/1"&gt;Pei-Ze Chiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsai_M/0/1/0/all/0/1"&gt;Meng-Shiun Tsai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tseng_H/0/1/0/all/0/1"&gt;Hung-Yu Tseng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lai_W/0/1/0/all/0/1"&gt;Wei-sheng Lai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chiu_W/0/1/0/all/0/1"&gt;Wei-Chen Chiu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SAR-U-Net: squeeze-and-excitation block and atrous spatial pyramid pooling based residual U-Net for automatic liver segmentation in Computed Tomography. (arXiv:2103.06419v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.06419</id>
        <link href="http://arxiv.org/abs/2103.06419"/>
        <updated>2021-06-08T02:20:22.189Z</updated>
        <summary type="html"><![CDATA[Background and objective: In this paper, a modified U-Net based framework is
presented, which leverages techniques from Squeeze-and-Excitation (SE) block,
Atrous Spatial Pyramid Pooling (ASPP) and residual learning for accurate and
robust liver CT segmentation, and the effectiveness of the proposed method was
tested on two public datasets LiTS17 and SLiver07.

Methods: A new network architecture called SAR-U-Net was designed. Firstly,
the SE block is introduced to adaptively extract image features after each
convolution in the U-Net encoder, while suppressing irrelevant regions, and
highlighting features of specific segmentation task; Secondly, ASPP was
employed to replace the transition layer and the output layer, and acquire
multi-scale image information via different receptive fields. Thirdly, to
alleviate the degradation problem, the traditional convolution block was
replaced with the residual block and thus prompt the network to gain accuracy
from considerably increased depth.

Results: In the LiTS17 experiment, the mean values of Dice, VOE, RVD, ASD and
MSD were 95.71, 9.52, -0.84, 1.54 and 29.14, respectively. Compared with other
closely related 2D-based models, the proposed method achieved the highest
accuracy. In the experiment of the SLiver07, the mean values of Dice, VOE, RVD,
ASD and MSD were 97.31, 5.37, -1.08, 1.85 and 27.45, respectively. Compared
with other closely related models, the proposed method achieved the highest
segmentation accuracy except for the RVD.

Conclusion: The proposed model enables a great improvement on the accuracy
compared to 2D-based models, and its robustness in circumvent challenging
problems, such as small liver regions, discontinuous liver regions, and fuzzy
liver boundaries, is also well demonstrated and validated.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jinke Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lv_P/0/1/0/all/0/1"&gt;Peiqing Lv&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_H/0/1/0/all/0/1"&gt;Haiying Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shi_C/0/1/0/all/0/1"&gt;Changfa Shi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Relational Learning with Gated and Attentive Neighbor Aggregator for Few-Shot Knowledge Graph Completion. (arXiv:2104.13095v2 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.13095</id>
        <link href="http://arxiv.org/abs/2104.13095"/>
        <updated>2021-06-08T02:20:22.169Z</updated>
        <summary type="html"><![CDATA[Aiming at expanding few-shot relations' coverage in knowledge graphs (KGs),
few-shot knowledge graph completion (FKGC) has recently gained more research
interests. Some existing models employ a few-shot relation's multi-hop neighbor
information to enhance its semantic representation. However, noise neighbor
information might be amplified when the neighborhood is excessively sparse and
no neighbor is available to represent the few-shot relation. Moreover, modeling
and inferring complex relations of one-to-many (1-N), many-to-one (N-1), and
many-to-many (N-N) by previous knowledge graph completion approaches requires
high model complexity and a large amount of training instances. Thus, inferring
complex relations in the few-shot scenario is difficult for FKGC models due to
limited training instances. In this paper, we propose a few-shot relational
learning with global-local framework to address the above issues. At the global
stage, a novel gated and attentive neighbor aggregator is built for accurately
integrating the semantics of a few-shot relation's neighborhood, which helps
filtering the noise neighbors even if a KG contains extremely sparse
neighborhoods. For the local stage, a meta-learning based TransH (MTransH)
method is designed to model complex relations and train our model in a few-shot
learning fashion. Extensive experiments show that our model outperforms the
state-of-the-art FKGC approaches on the frequently-used benchmark datasets
NELL-One and Wiki-One. Compared with the strong baseline model MetaR, our model
achieves 5-shot FKGC performance improvements of 8.0% on NELL-One and 2.8% on
Wiki-One by the metric Hits@10.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Niu_G/0/1/0/all/0/1"&gt;Guanglin Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1"&gt;Chengguang Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geng_R/0/1/0/all/0/1"&gt;Ruiying Geng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1"&gt;Jian Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1"&gt;Qiao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1"&gt;Jian Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1"&gt;Fei Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1"&gt;Luo Si&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Robustness of Vision Transformers to Adversarial Examples. (arXiv:2104.02610v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.02610</id>
        <link href="http://arxiv.org/abs/2104.02610"/>
        <updated>2021-06-08T02:20:22.163Z</updated>
        <summary type="html"><![CDATA[Recent advances in attention-based networks have shown that Vision
Transformers can achieve state-of-the-art or near state-of-the-art results on
many image classification tasks. This puts transformers in the unique position
of being a promising alternative to traditional convolutional neural networks
(CNNs). While CNNs have been carefully studied with respect to adversarial
attacks, the same cannot be said of Vision Transformers. In this paper, we
study the robustness of Vision Transformers to adversarial examples. Our
analyses of transformer security is divided into three parts. First, we test
the transformer under standard white-box and black-box attacks. Second, we
study the transferability of adversarial examples between CNNs and
transformers. We show that adversarial examples do not readily transfer between
CNNs and transformers. Based on this finding, we analyze the security of a
simple ensemble defense of CNNs and transformers. By creating a new attack, the
self-attention blended gradient attack, we show that such an ensemble is not
secure under a white-box adversary. However, under a black-box adversary, we
show that an ensemble can achieve unprecedented robustness without sacrificing
clean accuracy. Our analysis for this work is done using six types of white-box
attacks and two types of black-box attacks. Our study encompasses multiple
Vision Transformers, Big Transfer Models and CNN architectures trained on
CIFAR-10, CIFAR-100 and ImageNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mahmood_K/0/1/0/all/0/1"&gt;Kaleel Mahmood&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mahmood_R/0/1/0/all/0/1"&gt;Rigel Mahmood&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dijk_M/0/1/0/all/0/1"&gt;Marten van Dijk&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semantic Scene Completion via Integrating Instances and Scene in-the-Loop. (arXiv:2104.03640v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.03640</id>
        <link href="http://arxiv.org/abs/2104.03640"/>
        <updated>2021-06-08T02:20:22.154Z</updated>
        <summary type="html"><![CDATA[Semantic Scene Completion aims at reconstructing a complete 3D scene with
precise voxel-wise semantics from a single-view depth or RGBD image. It is a
crucial but challenging problem for indoor scene understanding. In this work,
we present a novel framework named Scene-Instance-Scene Network
(\textit{SISNet}), which takes advantages of both instance and scene level
semantic information. Our method is capable of inferring fine-grained shape
details as well as nearby objects whose semantic categories are easily
mixed-up. The key insight is that we decouple the instances from a coarsely
completed semantic scene instead of a raw input image to guide the
reconstruction of instances and the overall scene. SISNet conducts iterative
scene-to-instance (SI) and instance-to-scene (IS) semantic completion.
Specifically, the SI is able to encode objects' surrounding context for
effectively decoupling instances from the scene and each instance could be
voxelized into higher resolution to capture finer details. With IS,
fine-grained instance information can be integrated back into the 3D scene and
thus leads to more accurate semantic scene completion. Utilizing such an
iterative mechanism, the scene and instance completion benefits each other to
achieve higher completion accuracy. Extensively experiments show that our
proposed method consistently outperforms state-of-the-art methods on both real
NYU, NYUCAD and synthetic SUNCG-RGBD datasets. The code and the supplementary
material will be available at \url{https://github.com/yjcaimeow/SISNet}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1"&gt;Yingjie Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xuesong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_K/0/1/0/all/0/1"&gt;Kwan-Yee Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaogang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Hongsheng Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ConVEx: Data-Efficient and Few-Shot Slot Labeling. (arXiv:2010.11791v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.11791</id>
        <link href="http://arxiv.org/abs/2010.11791"/>
        <updated>2021-06-08T02:20:22.147Z</updated>
        <summary type="html"><![CDATA[We propose ConVEx (Conversational Value Extractor), an efficient pretraining
and fine-tuning neural approach for slot-labeling dialog tasks. Instead of
relying on more general pretraining objectives from prior work (e.g., language
modeling, response selection), ConVEx's pretraining objective, a novel pairwise
cloze task using Reddit data, is well aligned with its intended usage on
sequence labeling tasks. This enables learning domain-specific slot labelers by
simply fine-tuning decoding layers of the pretrained general-purpose sequence
labeling model, while the majority of the pretrained model's parameters are
kept frozen. We report state-of-the-art performance of ConVEx across a range of
diverse domains and data sets for dialog slot-labeling, with the largest gains
in the most challenging, few-shot setups. We believe that ConVEx's reduced
pretraining times (i.e., only 18 hours on 12 GPUs) and cost, along with its
efficient fine-tuning and strong performance, promise wider portability and
scalability for data-efficient sequence-labeling tasks in general.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Henderson_M/0/1/0/all/0/1"&gt;Matthew Henderson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vulic_I/0/1/0/all/0/1"&gt;Ivan Vuli&amp;#x107;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Each Part Matters: Local Patterns Facilitate Cross-view Geo-localization. (arXiv:2008.11646v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.11646</id>
        <link href="http://arxiv.org/abs/2008.11646"/>
        <updated>2021-06-08T02:20:22.136Z</updated>
        <summary type="html"><![CDATA[Cross-view geo-localization is to spot images of the same geographic target
from different platforms, e.g., drone-view cameras and satellites. It is
challenging in the large visual appearance changes caused by extreme viewpoint
variations. Existing methods usually concentrate on mining the fine-grained
feature of the geographic target in the image center, but underestimate the
contextual information in neighbor areas. In this work, we argue that neighbor
areas can be leveraged as auxiliary information, enriching discriminative clues
for geolocalization. Specifically, we introduce a simple and effective deep
neural network, called Local Pattern Network (LPN), to take advantage of
contextual information in an end-to-end manner. Without using extra part
estimators, LPN adopts a square-ring feature partition strategy, which provides
the attention according to the distance to the image center. It eases the part
matching and enables the part-wise representation learning. Owing to the
square-ring partition design, the proposed LPN has good scalability to rotation
variations and achieves competitive results on three prevailing benchmarks,
i.e., University-1652, CVUSA and CVACT. Besides, we also show the proposed LPN
can be easily embedded into other frameworks to further boost performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1"&gt;Tingyu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1"&gt;Zhedong Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_C/0/1/0/all/0/1"&gt;Chenggang Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jiyong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1"&gt;Yaoqi Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_B/0/1/0/all/0/1"&gt;Bolun Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yi Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generalized Jensen-Shannon Divergence Loss for Learning with Noisy Labels. (arXiv:2105.04522v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.04522</id>
        <link href="http://arxiv.org/abs/2105.04522"/>
        <updated>2021-06-08T02:20:22.114Z</updated>
        <summary type="html"><![CDATA[Prior works have found it beneficial to combine provably noise-robust loss
functions e.g., mean absolute error (MAE) with standard categorical loss
function e.g. cross entropy (CE) to improve their learnability. Here, we
propose to use Jensen-Shannon divergence as a noise-robust loss function and
show that it interestingly interpolate between CE and MAE with a controllable
mixing parameter. Furthermore, we make a crucial observation that CE exhibit
lower consistency around noisy data points. Based on this observation, we adopt
a generalized version of the Jensen-Shannon divergence for multiple
distributions to encourage consistency around data points. Using this loss
function, we show state-of-the-art results on both synthetic (CIFAR), and
real-world (WebVision) noise with varying noise rates.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Englesson_E/0/1/0/all/0/1"&gt;Erik Englesson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Azizpour_H/0/1/0/all/0/1"&gt;Hossein Azizpour&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Unified Conditional Disentanglement Framework for Multimodal Brain MR Image Translation. (arXiv:2101.05434v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.05434</id>
        <link href="http://arxiv.org/abs/2101.05434"/>
        <updated>2021-06-08T02:20:22.107Z</updated>
        <summary type="html"><![CDATA[Multimodal MRI provides complementary and clinically relevant information to
probe tissue condition and to characterize various diseases. However, it is
often difficult to acquire sufficiently many modalities from the same subject
due to limitations in study plans, while quantitative analysis is still
demanded. In this work, we propose a unified conditional disentanglement
framework to synthesize any arbitrary modality from an input modality. Our
framework hinges on a cycle-constrained conditional adversarial training
approach, where it can extract a modality-invariant anatomical feature with a
modality-agnostic encoder and generate a target modality with a conditioned
decoder. We validate our framework on four MRI modalities, including
T1-weighted, T1 contrast enhanced, T2-weighted, and FLAIR MRI, from the
BraTS'18 database, showing superior performance on synthesis quality over the
comparison methods. In addition, we report results from experiments on a tumor
segmentation task carried out with synthesized data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiaofeng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xing_F/0/1/0/all/0/1"&gt;Fangxu Xing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Fakhri_G/0/1/0/all/0/1"&gt;Georges El Fakhri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Woo_J/0/1/0/all/0/1"&gt;Jonghye Woo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Registration of serial sections: An evaluation method based on distortions of the ground truths. (arXiv:2011.11060v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.11060</id>
        <link href="http://arxiv.org/abs/2011.11060"/>
        <updated>2021-06-08T02:20:22.100Z</updated>
        <summary type="html"><![CDATA[Registration of histological serial sections is a challenging task. Serial
sections exhibit distortions and damage from sectioning. Missing information on
how the tissue looked before cutting makes a realistic validation of 2D
registrations extremely difficult.

This work proposes methods for ground-truth-based evaluation of
registrations. Firstly, we present a methodology to generate test data for
registrations. We distort an innately registered image stack in the manner
similar to the cutting distortion of serial sections. Test cases are generated
from existing 3D data sets, thus the ground truth is known. Secondly, our test
case generation premises evaluation of the registrations with known ground
truths. Our methodology for such an evaluation technique distinguishes this
work from other approaches. Both under- and over-registration become evident in
our evaluations. We also survey existing validation efforts.

We present a full-series evaluation across six different registration methods
applied to our distorted 3D data sets of animal lungs. Our distorted and ground
truth data sets are made publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lobachev_O/0/1/0/all/0/1"&gt;Oleg Lobachev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Funatomi_T/0/1/0/all/0/1"&gt;Takuya Funatomi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pfaffenroth_A/0/1/0/all/0/1"&gt;Alexander Pfaffenroth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Forster_R/0/1/0/all/0/1"&gt;Reinhold F&amp;#xf6;rster&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Knudsen_L/0/1/0/all/0/1"&gt;Lars Knudsen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wrede_C/0/1/0/all/0/1"&gt;Christoph Wrede&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guthe_M/0/1/0/all/0/1"&gt;Michael Guthe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Haberthur_D/0/1/0/all/0/1"&gt;David Haberth&amp;#xfc;r&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hlushchuk_R/0/1/0/all/0/1"&gt;Ruslan Hlushchuk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salaets_T/0/1/0/all/0/1"&gt;Thomas Salaets&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Toelen_J/0/1/0/all/0/1"&gt;Jaan Toelen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gaffling_S/0/1/0/all/0/1"&gt;Simone Gaffling&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muhlfeld_C/0/1/0/all/0/1"&gt;Christian M&amp;#xfc;hlfeld&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grothausmann_R/0/1/0/all/0/1"&gt;Roman Grothausmann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Relative stability toward diffeomorphisms indicates performance in deep nets. (arXiv:2105.02468v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.02468</id>
        <link href="http://arxiv.org/abs/2105.02468"/>
        <updated>2021-06-08T02:20:22.090Z</updated>
        <summary type="html"><![CDATA[Understanding why deep nets can classify data in large dimensions remains a
challenge. It has been proposed that they do so by becoming stable to
diffeomorphisms, yet existing empirical measurements support that it is often
not the case. We revisit this question by defining a maximum-entropy
distribution on diffeomorphisms, that allows to study typical diffeomorphisms
of a given norm. We confirm that stability toward diffeomorphisms does not
strongly correlate to performance on benchmark data sets of images. By
contrast, we find that the stability toward diffeomorphisms relative to that of
generic transformations $R_f$ correlates remarkably with the test error
$\epsilon_t$. It is of order unity at initialization but decreases by several
decades during training for state-of-the-art architectures. For CIFAR10 and 15
known architectures, we find $\epsilon_t\approx 0.2\sqrt{R_f}$, suggesting that
obtaining a small $R_f$ is important to achieve good performance. We study how
$R_f$ depends on the size of the training set and compare it to a simple model
of invariant learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Petrini_L/0/1/0/all/0/1"&gt;Leonardo Petrini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Favero_A/0/1/0/all/0/1"&gt;Alessandro Favero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geiger_M/0/1/0/all/0/1"&gt;Mario Geiger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wyart_M/0/1/0/all/0/1"&gt;Matthieu Wyart&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Local Disentanglement in Variational Auto-Encoders Using Jacobian $L_1$ Regularization. (arXiv:2106.02923v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02923</id>
        <link href="http://arxiv.org/abs/2106.02923"/>
        <updated>2021-06-08T02:20:22.084Z</updated>
        <summary type="html"><![CDATA[There have been many recent advances in representation learning; however,
unsupervised representation learning can still struggle with model
identification issues. Variational Auto-Encoders (VAEs) and their extensions
such as $\beta$-VAEs have been shown to locally align latent variables with PCA
directions, which can help to improve model disentanglement under some
conditions. Borrowing inspiration from Independent Component Analysis (ICA) and
sparse coding, we propose applying an $L_1$ loss to the VAE's generative
Jacobian during training to encourage local latent variable alignment with
independent factors of variation in the data. We demonstrate our results on a
variety of datasets, giving qualitative and quantitative results using
information theoretic and modularity measures that show our added $L_1$ cost
encourages local axis alignment of the latent representation with individual
factors of variation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rhodes_T/0/1/0/all/0/1"&gt;Travers Rhodes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1"&gt;Daniel D. Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What If We Only Use Real Datasets for Scene Text Recognition? Toward Scene Text Recognition With Fewer Labels. (arXiv:2103.04400v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.04400</id>
        <link href="http://arxiv.org/abs/2103.04400"/>
        <updated>2021-06-08T02:20:22.061Z</updated>
        <summary type="html"><![CDATA[Scene text recognition (STR) task has a common practice: All state-of-the-art
STR models are trained on large synthetic data. In contrast to this practice,
training STR models only on fewer real labels (STR with fewer labels) is
important when we have to train STR models without synthetic data: for
handwritten or artistic texts that are difficult to generate synthetically and
for languages other than English for which we do not always have synthetic
data. However, there has been implicit common knowledge that training STR
models on real data is nearly impossible because real data is insufficient. We
consider that this common knowledge has obstructed the study of STR with fewer
labels. In this work, we would like to reactivate STR with fewer labels by
disproving the common knowledge. We consolidate recently accumulated public
real data and show that we can train STR models satisfactorily only with real
labeled data. Subsequently, we find simple data augmentation to fully exploit
real data. Furthermore, we improve the models by collecting unlabeled data and
introducing semi- and self-supervised methods. As a result, we obtain a
competitive model to state-of-the-art methods. To the best of our knowledge,
this is the first study that 1) shows sufficient performance by only using real
labels and 2) introduces semi- and self-supervised methods into STR with fewer
labels. Our code and data are available:
https://github.com/ku21fan/STR-Fewer-Labels]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Baek_J/0/1/0/all/0/1"&gt;Jeonghun Baek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Matsui_Y/0/1/0/all/0/1"&gt;Yusuke Matsui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aizawa_K/0/1/0/all/0/1"&gt;Kiyoharu Aizawa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scaling Local Self-Attention for Parameter Efficient Visual Backbones. (arXiv:2103.12731v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.12731</id>
        <link href="http://arxiv.org/abs/2103.12731"/>
        <updated>2021-06-08T02:20:22.054Z</updated>
        <summary type="html"><![CDATA[Self-attention has the promise of improving computer vision systems due to
parameter-independent scaling of receptive fields and content-dependent
interactions, in contrast to parameter-dependent scaling and
content-independent interactions of convolutions. Self-attention models have
recently been shown to have encouraging improvements on accuracy-parameter
trade-offs compared to baseline convolutional models such as ResNet-50. In this
work, we aim to develop self-attention models that can outperform not just the
canonical baseline models, but even the high-performing convolutional models.
We propose two extensions to self-attention that, in conjunction with a more
efficient implementation of self-attention, improve the speed, memory usage,
and accuracy of these models. We leverage these improvements to develop a new
self-attention model family, HaloNets, which reach state-of-the-art accuracies
on the parameter-limited setting of the ImageNet classification benchmark. In
preliminary transfer learning experiments, we find that HaloNet models
outperform much larger models and have better inference performance. On harder
tasks such as object detection and instance segmentation, our simple local
self-attention and convolutional hybrids show improvements over very strong
baselines. These results mark another step in demonstrating the efficacy of
self-attention models on settings traditionally dominated by convolutional
models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vaswani_A/0/1/0/all/0/1"&gt;Ashish Vaswani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramachandran_P/0/1/0/all/0/1"&gt;Prajit Ramachandran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srinivas_A/0/1/0/all/0/1"&gt;Aravind Srinivas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parmar_N/0/1/0/all/0/1"&gt;Niki Parmar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hechtman_B/0/1/0/all/0/1"&gt;Blake Hechtman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shlens_J/0/1/0/all/0/1"&gt;Jonathon Shlens&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DiscoBox: Weakly Supervised Instance Segmentation and Semantic Correspondence from Box Supervision. (arXiv:2105.06464v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.06464</id>
        <link href="http://arxiv.org/abs/2105.06464"/>
        <updated>2021-06-08T02:20:22.044Z</updated>
        <summary type="html"><![CDATA[We introduce DiscoBox, a novel framework that jointly learns instance
segmentation and semantic correspondence using bounding box supervision.
Specifically, we propose a self-ensembling framework where instance
segmentation and semantic correspondence are jointly guided by a structured
teacher in addition to the bounding box supervision. The teacher is a
structured energy model incorporating a pairwise potential and a cross-image
potential to model the pairwise pixel relationships both within and across the
boxes. Minimizing the teacher energy simultaneously yields refined object masks
and dense correspondences between intra-class objects, which are taken as
pseudo-labels to supervise the task network and provide positive/negative
correspondence pairs for dense constrastive learning. We show a symbiotic
relationship where the two tasks mutually benefit from each other. Our best
model achieves 37.9% AP on COCO instance segmentation, surpassing prior weakly
supervised methods and is competitive to supervised methods. We also obtain
state of the art weakly supervised results on PASCAL VOC12 and PF-PASCAL with
real-time inference.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lan_S/0/1/0/all/0/1"&gt;Shiyi Lan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1"&gt;Zhiding Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choy_C/0/1/0/all/0/1"&gt;Christopher Choy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Radhakrishnan_S/0/1/0/all/0/1"&gt;Subhashree Radhakrishnan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1"&gt;Guilin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yuke Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Davis_L/0/1/0/all/0/1"&gt;Larry S. Davis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1"&gt;Anima Anandkumar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sattiy at SemEval-2021 Task 9: An Ensemble Solution for Statement Verification and Evidence Finding with Tables. (arXiv:2104.10366v2 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.10366</id>
        <link href="http://arxiv.org/abs/2104.10366"/>
        <updated>2021-06-08T02:20:22.037Z</updated>
        <summary type="html"><![CDATA[Question answering from semi-structured tables can be seen as a semantic
parsing task and is significant and practical for pushing the boundary of
natural language understanding. Existing research mainly focuses on
understanding contents from unstructured evidence, e.g., news, natural language
sentences, and documents. The task of verification from structured evidence,
such as tables, charts, and databases, is still less explored. This paper
describes sattiy team's system in SemEval-2021 task 9: Statement Verification
and Evidence Finding with Tables (SEM-TAB-FACT). This competition aims to
verify statements and to find evidence from tables for scientific articles and
to promote the proper interpretation of the surrounding article. In this paper,
we exploited ensemble models of pre-trained language models over tables, TaPas
and TaBERT, for Task A and adjust the result based on some rules extracted for
Task B. Finally, in the leaderboard, we attain the F1 scores of 0.8496 and
0.7732 in Task A for the 2-way and 3-way evaluation, respectively, and the F1
score of 0.4856 in Task B.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ruan_X/0/1/0/all/0/1"&gt;Xiaoyi Ruan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_M/0/1/0/all/0/1"&gt;Meizhi Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1"&gt;Jian Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Haiqin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1"&gt;Lianxin Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mo_Y/0/1/0/all/0/1"&gt;Yang Mo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1"&gt;Mengyuan Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dynamic Resolution Network. (arXiv:2106.02898v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02898</id>
        <link href="http://arxiv.org/abs/2106.02898"/>
        <updated>2021-06-08T02:20:22.029Z</updated>
        <summary type="html"><![CDATA[Deep convolutional neural networks (CNNs) are often of sophisticated design
with numerous convolutional layers and learnable parameters for the accuracy
reason. To alleviate the expensive costs of deploying them on mobile devices,
recent works have made huge efforts for excavating redundancy in pre-defined
architectures. Nevertheless, the redundancy on the input resolution of modern
CNNs has not been fully investigated, i.e., the resolution of input image is
fixed. In this paper, we observe that the smallest resolution for accurately
predicting the given image is different using the same neural network. To this
end, we propose a novel dynamic-resolution network (DRNet) in which the
resolution is determined dynamically based on each input sample. Thus, a
resolution predictor with negligible computational costs is explored and
optimized jointly with the desired network. In practice, the predictor learns
the smallest resolution that can retain and even exceed the original
recognition accuracy for each image. During the inference, each input image
will be resized to its predicted resolution for minimizing the overall
computation burden. We then conduct extensive experiments on several benchmark
networks and datasets. The results show that our DRNet can be embedded in any
off-the-shelf network architecture to obtain a considerable reduction in
computational complexity. For instance, DRNet achieves similar performance with
an about 34% computation reduction, while gains 1.4% accuracy increase with 10%
computation reduction compared to the original ResNet-50 on ImageNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1"&gt;Mingjian Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1"&gt;Kai Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_E/0/1/0/all/0/1"&gt;Enhua Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Qiulin Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nie_Y/0/1/0/all/0/1"&gt;Ying Nie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lan_Z/0/1/0/all/0/1"&gt;Zhenzhong Lan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yunhe Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adapting Long Context NLM for ASR Rescoring in Conversational Agents. (arXiv:2104.11070v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.11070</id>
        <link href="http://arxiv.org/abs/2104.11070"/>
        <updated>2021-06-08T02:20:22.023Z</updated>
        <summary type="html"><![CDATA[Neural Language Models (NLM), when trained and evaluated with context
spanning multiple utterances, have been shown to consistently outperform both
conventional n-gram language models and NLMs that use limited context. In this
paper, we investigate various techniques to incorporate turn based context
history into both recurrent (LSTM) and Transformer-XL based NLMs. For recurrent
based NLMs, we explore context carry over mechanism and feature based
augmentation, where we incorporate other forms of contextual information such
as bot response and system dialogue acts as classified by a Natural Language
Understanding (NLU) model. To mitigate the sharp nearby, fuzzy far away problem
with contextual NLM, we propose the use of attention layer over lexical
metadata to improve feature based augmentation. Additionally, we adapt our
contextual NLM towards user provided on-the-fly speech patterns by leveraging
encodings from a large pre-trained masked language model and performing fusion
with a Transformer-XL based NLM. We test our proposed models using N-best
rescoring of ASR hypotheses of task-oriented dialogues and also evaluate on
downstream NLU tasks such as intent classification and slot labeling. The best
performing model shows a relative WER between 1.6% and 9.1% and a slot labeling
F1 score improvement of 4% over non-contextual baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shenoy_A/0/1/0/all/0/1"&gt;Ashish Shenoy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bodapati_S/0/1/0/all/0/1"&gt;Sravan Bodapati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sunkara_M/0/1/0/all/0/1"&gt;Monica Sunkara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ronanki_S/0/1/0/all/0/1"&gt;Srikanth Ronanki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kirchhoff_K/0/1/0/all/0/1"&gt;Katrin Kirchhoff&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spectral Geometric Matrix Completion. (arXiv:1911.07255v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1911.07255</id>
        <link href="http://arxiv.org/abs/1911.07255"/>
        <updated>2021-06-08T02:20:22.003Z</updated>
        <summary type="html"><![CDATA[Deep Matrix Factorization (DMF) is an emerging approach to the problem of
matrix completion. Recent works have established that gradient descent applied
to a DMF model induces an implicit regularization on the rank of the recovered
matrix. In this work we interpret the DMF model through the lens of spectral
geometry. This allows us to incorporate explicit regularization without
breaking the DMF structure, thus enjoying the best of both worlds. In
particular, we focus on matrix completion problems with underlying geometric or
topological relations between the rows and/or columns. Such relations are
prevalent in matrix completion problems that arise in many applications, such
as recommender systems and drug-target interaction. Our contributions enable
DMF models to exploit these relations, and make them competitive on real
benchmarks, while exhibiting one of the first successful applications of deep
linear networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Boyarski_A/0/1/0/all/0/1"&gt;Amit Boyarski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vedula_S/0/1/0/all/0/1"&gt;Sanketh Vedula&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bronstein_A/0/1/0/all/0/1"&gt;Alex Bronstein&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PCT: Point cloud transformer. (arXiv:2012.09688v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.09688</id>
        <link href="http://arxiv.org/abs/2012.09688"/>
        <updated>2021-06-08T02:20:21.995Z</updated>
        <summary type="html"><![CDATA[The irregular domain and lack of ordering make it challenging to design deep
neural networks for point cloud processing. This paper presents a novel
framework named Point Cloud Transformer(PCT) for point cloud learning. PCT is
based on Transformer, which achieves huge success in natural language
processing and displays great potential in image processing. It is inherently
permutation invariant for processing a sequence of points, making it
well-suited for point cloud learning. To better capture local context within
the point cloud, we enhance input embedding with the support of farthest point
sampling and nearest neighbor search. Extensive experiments demonstrate that
the PCT achieves the state-of-the-art performance on shape classification, part
segmentation and normal estimation tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1"&gt;Meng-Hao Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1"&gt;Jun-Xiong Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zheng-Ning Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mu_T/0/1/0/all/0/1"&gt;Tai-Jiang Mu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martin_R/0/1/0/all/0/1"&gt;Ralph R. Martin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1"&gt;Shi-Min Hu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Visual Search Asymmetry: Deep Nets and Humans Share Similar Inherent Biases. (arXiv:2106.02953v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02953</id>
        <link href="http://arxiv.org/abs/2106.02953"/>
        <updated>2021-06-08T02:20:21.988Z</updated>
        <summary type="html"><![CDATA[Visual search is a ubiquitous and often challenging daily task, exemplified
by looking for the car keys at home or a friend in a crowd. An intriguing
property of some classical search tasks is an asymmetry such that finding a
target A among distractors B can be easier than finding B among A. To elucidate
the mechanisms responsible for asymmetry in visual search, we propose a
computational model that takes a target and a search image as inputs and
produces a sequence of eye movements until the target is found. The model
integrates eccentricity-dependent visual recognition with target-dependent
top-down cues. We compared the model against human behavior in six paradigmatic
search tasks that show asymmetry in humans. Without prior exposure to the
stimuli or task-specific training, the model provides a plausible mechanism for
search asymmetry. We hypothesized that the polarity of search asymmetry arises
from experience with the natural environment. We tested this hypothesis by
training the model on an augmented version of ImageNet where the biases of
natural images were either removed or reversed. The polarity of search
asymmetry disappeared or was altered depending on the training protocol. This
study highlights how classical perceptual properties can emerge in neural
network models, without the need for task-specific training, but rather as a
consequence of the statistical properties of the developmental diet fed to the
model. All source code and stimuli are publicly available
https://github.com/kreimanlab/VisualSearchAsymmetry]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1"&gt;Shashi Kant Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Mengmi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1"&gt;Chia-Chien Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wolfe_J/0/1/0/all/0/1"&gt;Jeremy M. Wolfe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kreiman_G/0/1/0/all/0/1"&gt;Gabriel Kreiman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Patch Slimming for Efficient Vision Transformers. (arXiv:2106.02852v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02852</id>
        <link href="http://arxiv.org/abs/2106.02852"/>
        <updated>2021-06-08T02:20:21.981Z</updated>
        <summary type="html"><![CDATA[This paper studies the efficiency problem for visual transformers by
excavating redundant calculation in given networks. The recent transformer
architecture has demonstrated its effectiveness for achieving excellent
performance on a series of computer vision tasks. However, similar to that of
convolutional neural networks, the huge computational cost of vision
transformers is still a severe issue. Considering that the attention mechanism
aggregates different patches layer-by-layer, we present a novel patch slimming
approach that discards useless patches in a top-down paradigm. We first
identify the effective patches in the last layer and then use them to guide the
patch selection process of previous layers. For each layer, the impact of a
patch on the final output feature is approximated and patches with less impact
will be removed. Experimental results on benchmark datasets demonstrate that
the proposed method can significantly reduce the computational costs of vision
transformers without affecting their performances. For example, over 45% FLOPs
of the ViT-Ti model can be reduced with only 0.2% top-1 accuracy drop on the
ImageNet dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1"&gt;Yehui Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1"&gt;Kai Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yunhe Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Chang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1"&gt;Jianyuan Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Chao Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1"&gt;Dacheng Tao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Deep Joint Sparse Non-negative Matrix Factorization Framework for Identifying the Common and Subject-specific Functional Units of Tongue Motion During Speech. (arXiv:2007.04865v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.04865</id>
        <link href="http://arxiv.org/abs/2007.04865"/>
        <updated>2021-06-08T02:20:21.974Z</updated>
        <summary type="html"><![CDATA[Intelligible speech is produced by creating varying internal local muscle
groupings -- i.e., functional units -- that are generated in a systematic and
coordinated manner. There are two major challenges in characterizing and
analyzing functional units.~First, due to the complex and convoluted nature of
tongue structure and function, it is of great importance to develop a method
that can accurately decode complex muscle coordination patterns during speech.
Second, it is challenging to keep identified functional units across subjects
comparable due to their substantial variability. In this work, to address these
challenges, we develop a new deep learning framework to identify common and
subject-specific functional units of tongue motion during speech.~Our framework
hinges on joint deep graph-regularized sparse non-negative matrix factorization
(NMF) using motion quantities derived from displacements by tagged Magnetic
Resonance Imaging. More specifically, we transform NMF with sparse and graph
regularizations into modular architectures akin to deep neural networks by
means of unfolding the Iterative Shrinkage-Thresholding Algorithm to learn
interpretable building blocks and associated weighting map. We then apply
spectral clustering to common and subject-specific weighting maps from which we
jointly determine the common and subject-specific functional units. Experiments
carried out with simulated datasets show that the proposed method achieved on
par or better clustering performance over the comparison methods. Experiments
carried out with in vivo tongue motion data show that the proposed method can
determine the common and subject-specific functional units with increased
interpretability and decreased size variability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Woo_J/0/1/0/all/0/1"&gt;Jonghye Woo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xing_F/0/1/0/all/0/1"&gt;Fangxu Xing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prince_J/0/1/0/all/0/1"&gt;Jerry L. Prince&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stone_M/0/1/0/all/0/1"&gt;Maureen Stone&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gomez_A/0/1/0/all/0/1"&gt;Arnold Gomez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reese_T/0/1/0/all/0/1"&gt;Timothy G. Reese&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wedeen_V/0/1/0/all/0/1"&gt;Van J. Wedeen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fakhri_G/0/1/0/all/0/1"&gt;Georges El Fakhri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Correlation Filters for Unmanned Aerial Vehicle-Based Aerial Tracking: A Review and Experimental Evaluation. (arXiv:2010.06255v5 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.06255</id>
        <link href="http://arxiv.org/abs/2010.06255"/>
        <updated>2021-06-08T02:20:21.955Z</updated>
        <summary type="html"><![CDATA[Aerial tracking, which has exhibited its omnipresent dedication and splendid
performance, is one of the most active applications in the remote sensing
field. Especially, unmanned aerial vehicle (UAV)-based remote sensing system,
equipped with a visual tracking approach, has been widely used in aviation,
navigation, agriculture,transportation, and public security, etc. As is
mentioned above, the UAV-based aerial tracking platform has been gradually
developed from research to practical application stage, reaching one of the
main aerial remote sensing technologies in the future. However, due to the
real-world onerous situations, e.g., harsh external challenges, the vibration
of the UAV mechanical structure (especially under strong wind conditions), the
maneuvering flight in complex environment, and the limited computation
resources onboard, accuracy, robustness, and high efficiency are all crucial
for the onboard tracking methods. Recently, the discriminative correlation
filter (DCF)-based trackers have stood out for their high computational
efficiency and appealing robustness on a single CPU, and have flourished in the
UAV visual tracking community. In this work, the basic framework of the
DCF-based trackers is firstly generalized, based on which, 23 state-of-the-art
DCF-based trackers are orderly summarized according to their innovations for
solving various issues. Besides, exhaustive and quantitative experiments have
been extended on various prevailing UAV tracking benchmarks, i.e., UAV123,
UAV123@10fps, UAV20L, UAVDT, DTB70, and VisDrone2019-SOT, which contain 371,903
frames in total. The experiments show the performance, verify the feasibility,
and demonstrate the current challenges of DCF-based trackers onboard UAV
tracking.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1"&gt;Changhong Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bowen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_F/0/1/0/all/0/1"&gt;Fangqiang Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_F/0/1/0/all/0/1"&gt;Fuling Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_G/0/1/0/all/0/1"&gt;Geng Lu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Feature Mining via Attention-based BiLSTM-GCN for Human Motor Imagery Recognition. (arXiv:2005.00777v2 [eess.SP] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.00777</id>
        <link href="http://arxiv.org/abs/2005.00777"/>
        <updated>2021-06-08T02:20:21.917Z</updated>
        <summary type="html"><![CDATA[Recognition accuracy and response time are both critically essential ahead of
building practical electroencephalography (EEG) based brain-computer interface
(BCI). Recent approaches, however, have either compromised in the
classification accuracy or responding time. This paper presents a novel deep
learning approach designed towards remarkably accurate and responsive motor
imagery (MI) recognition based on scalp EEG. Bidirectional Long Short-term
Memory (BiLSTM) with the Attention mechanism manages to derive relevant
features from raw EEG signals. The connected graph convolutional neural network
(GCN) promotes the decoding performance by cooperating with the topological
structure of features, which are estimated from the overall data. The
0.4-second detection framework has shown effective and efficient prediction
based on individual and group-wise training, with 98.81% and 94.64% accuracy,
respectively, which outperformed all the state-of-the-art studies. The
introduced deep feature mining approach can precisely recognize human motion
intents from raw EEG signals, which paves the road to translate the EEG based
MI recognition to practical BCI systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Hou_Y/0/1/0/all/0/1"&gt;Yimin Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jia_S/0/1/0/all/0/1"&gt;Shuyue Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lun_X/0/1/0/all/0/1"&gt;Xiangmin Lun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shi_Y/0/1/0/all/0/1"&gt;Yan Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yang Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learnable Fourier Features for Multi-DimensionalSpatial Positional Encoding. (arXiv:2106.02795v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02795</id>
        <link href="http://arxiv.org/abs/2106.02795"/>
        <updated>2021-06-08T02:20:21.870Z</updated>
        <summary type="html"><![CDATA[Attentional mechanisms are order-invariant. Positional encoding is a crucial
component to allow attention-based deep model architectures such as Transformer
to address sequences or images where the position of information matters. In
this paper, we propose a novel positional encoding method based on learnable
Fourier features. Instead of hard-coding each position as a token or a vector,
we represent each position, which can be multi-dimensional, as a trainable
encoding based on learnable Fourier feature mapping, modulated with a
multi-layer perceptron. The representation is particularly advantageous for a
spatial multi-dimensional position, e.g., pixel positions on an image, where
$L_2$ distances or more complex positional relationships need to be captured.
Our experiments based on several public benchmark tasks show that our learnable
Fourier feature representation for multi-dimensional positional encoding
outperforms existing methods by both improving the accuracy and allowing faster
convergence.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Si_S/0/1/0/all/0/1"&gt;Si Si&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1"&gt;Gang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1"&gt;Cho-Jui Hsieh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bengio_S/0/1/0/all/0/1"&gt;Samy Bengio&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Category Contrast for Unsupervised Domain Adaptation in Visual Tasks. (arXiv:2106.02885v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02885</id>
        <link href="http://arxiv.org/abs/2106.02885"/>
        <updated>2021-06-08T02:20:21.862Z</updated>
        <summary type="html"><![CDATA[Instance contrast for unsupervised representation learning has achieved great
success in recent years. In this work, we explore the idea of instance
contrastive learning in unsupervised domain adaptation (UDA) and propose a
novel Category Contrast technique (CaCo) that introduces semantic priors on top
of instance discrimination for visual UDA tasks. By considering instance
contrastive learning as a dictionary look-up operation, we construct a
semantics-aware dictionary with samples from both source and target domains
where each target sample is assigned a (pseudo) category label based on the
category priors of source samples. This allows category contrastive learning
(between target queries and the category-level dictionary) for
category-discriminative yet domain-invariant feature representations: samples
of the same category (from either source or target domain) are pulled closer
while those of different categories are pushed apart simultaneously. Extensive
UDA experiments in multiple visual tasks ($e.g.$, segmentation, classification
and detection) show that the simple implementation of CaCo achieves superior
performance as compared with the highly-optimized state-of-the-art methods.
Analytically and empirically, the experiments also demonstrate that CaCo is
complementary to existing UDA methods and generalizable to other learning
setups such as semi-supervised learning, unsupervised model adaptation, etc.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1"&gt;Jiaxing Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guan_D/0/1/0/all/0/1"&gt;Dayan Guan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_A/0/1/0/all/0/1"&gt;Aoran Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1"&gt;Shijian Lu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[S2SD: Simultaneous Similarity-based Self-Distillation for Deep Metric Learning. (arXiv:2009.08348v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.08348</id>
        <link href="http://arxiv.org/abs/2009.08348"/>
        <updated>2021-06-08T02:20:21.830Z</updated>
        <summary type="html"><![CDATA[Deep Metric Learning (DML) provides a crucial tool for visual similarity and
zero-shot applications by learning generalizing embedding spaces, although
recent work in DML has shown strong performance saturation across training
objectives. However, generalization capacity is known to scale with the
embedding space dimensionality. Unfortunately, high dimensional embeddings also
create higher retrieval cost for downstream applications. To remedy this, we
propose \emph{Simultaneous Similarity-based Self-distillation (S2SD). S2SD
extends DML with knowledge distillation from auxiliary, high-dimensional
embedding and feature spaces to leverage complementary context during training
while retaining test-time cost and with negligible changes to the training
time. Experiments and ablations across different objectives and standard
benchmarks show S2SD offers notable improvements of up to 7% in Recall@1, while
also setting a new state-of-the-art. Code available at
https://github.com/MLforHealth/S2SD.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Roth_K/0/1/0/all/0/1"&gt;Karsten Roth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Milbich_T/0/1/0/all/0/1"&gt;Timo Milbich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ommer_B/0/1/0/all/0/1"&gt;Bj&amp;#xf6;rn Ommer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cohen_J/0/1/0/all/0/1"&gt;Joseph Paul Cohen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghassemi_M/0/1/0/all/0/1"&gt;Marzyeh Ghassemi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Region-aware Adaptive Instance Normalization for Image Harmonization. (arXiv:2106.02853v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02853</id>
        <link href="http://arxiv.org/abs/2106.02853"/>
        <updated>2021-06-08T02:20:21.794Z</updated>
        <summary type="html"><![CDATA[Image composition plays a common but important role in photo editing. To
acquire photo-realistic composite images, one must adjust the appearance and
visual style of the foreground to be compatible with the background. Existing
deep learning methods for harmonizing composite images directly learn an image
mapping network from the composite to the real one, without explicit
exploration on visual style consistency between the background and the
foreground images. To ensure the visual style consistency between the
foreground and the background, in this paper, we treat image harmonization as a
style transfer problem. In particular, we propose a simple yet effective
Region-aware Adaptive Instance Normalization (RAIN) module, which explicitly
formulates the visual style from the background and adaptively applies them to
the foreground. With our settings, our RAIN module can be used as a drop-in
module for existing image harmonization networks and is able to bring
significant improvements. Extensive experiments on the existing image
harmonization benchmark datasets show the superior capability of the proposed
method. Code is available at {https://github.com/junleen/RainNet}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ling_J/0/1/0/all/0/1"&gt;Jun Ling&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1"&gt;Han Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1"&gt;Li Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_R/0/1/0/all/0/1"&gt;Rong Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_X/0/1/0/all/0/1"&gt;Xiao Gu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Convolutional Neural Networks with Gated Recurrent Connections. (arXiv:2106.02859v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02859</id>
        <link href="http://arxiv.org/abs/2106.02859"/>
        <updated>2021-06-08T02:20:21.772Z</updated>
        <summary type="html"><![CDATA[The convolutional neural network (CNN) has become a basic model for solving
many computer vision problems. In recent years, a new class of CNNs, recurrent
convolution neural network (RCNN), inspired by abundant recurrent connections
in the visual systems of animals, was proposed. The critical element of RCNN is
the recurrent convolutional layer (RCL), which incorporates recurrent
connections between neurons in the standard convolutional layer. With
increasing number of recurrent computations, the receptive fields (RFs) of
neurons in RCL expand unboundedly, which is inconsistent with biological facts.
We propose to modulate the RFs of neurons by introducing gates to the recurrent
connections. The gates control the amount of context information inputting to
the neurons and the neurons' RFs therefore become adaptive. The resulting layer
is called gated recurrent convolution layer (GRCL). Multiple GRCLs constitute a
deep model called gated RCNN (GRCNN). The GRCNN was evaluated on several
computer vision tasks including object recognition, scene text recognition and
object detection, and obtained much better results than the RCNN. In addition,
when combined with other adaptive RF techniques, the GRCNN demonstrated
competitive performance to the state-of-the-art models on benchmark datasets
for these tasks. The codes are released at
\href{https://github.com/Jianf-Wang/GRCNN}{https://github.com/Jianf-Wang/GRCNN}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jianfeng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1"&gt;Xiaolin Hu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Principle Bit Analysis: Autoencoding with Schur-Concave Loss. (arXiv:2106.02796v1 [cs.IT])]]></title>
        <id>http://arxiv.org/abs/2106.02796</id>
        <link href="http://arxiv.org/abs/2106.02796"/>
        <updated>2021-06-08T02:20:21.763Z</updated>
        <summary type="html"><![CDATA[We consider a linear autoencoder in which the latent variables are quantized,
or corrupted by noise, and the constraint is Schur-concave in the set of latent
variances. Although finding the optimal encoder/decoder pair for this setup is
a nonconvex optimization problem, we show that decomposing the source into its
principal components is optimal. If the constraint is strictly Schur-concave
and the empirical covariance matrix has only simple eigenvalues, then any
optimal encoder/decoder must decompose the source in this way. As one
application, we consider a strictly Schur-concave constraint that estimates the
number of bits needed to represent the latent variables under fixed-rate
encoding, a setup that we call \emph{Principal Bit Analysis (PBA)}. This yields
a practical, general-purpose, fixed-rate compressor that outperforms existing
algorithms. As a second application, we show that a prototypical
autoencoder-based variable-rate compressor is guaranteed to decompose the
source into its principal components.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bhadane_S/0/1/0/all/0/1"&gt;Sourbh Bhadane&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wagner_A/0/1/0/all/0/1"&gt;Aaron B. Wagner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Acharya_J/0/1/0/all/0/1"&gt;Jayadev Acharya&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hierarchical Video Generation for Complex Data. (arXiv:2106.02719v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02719</id>
        <link href="http://arxiv.org/abs/2106.02719"/>
        <updated>2021-06-08T02:20:21.755Z</updated>
        <summary type="html"><![CDATA[Videos can often be created by first outlining a global description of the
scene and then adding local details. Inspired by this we propose a hierarchical
model for video generation which follows a coarse to fine approach. First our
model generates a low resolution video, establishing the global scene
structure, that is then refined by subsequent levels in the hierarchy. We train
each level in our hierarchy sequentially on partial views of the videos. This
reduces the computational complexity of our generative model, which scales to
high-resolution videos beyond a few frames. We validate our approach on
Kinetics-600 and BDD100K, for which we train a three level model capable of
generating 256x256 videos with 48 frames.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Castrejon_L/0/1/0/all/0/1"&gt;Lluis Castrejon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ballas_N/0/1/0/all/0/1"&gt;Nicolas Ballas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Courville_A/0/1/0/all/0/1"&gt;Aaron Courville&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Shellcode_IA32: A Dataset for Automatic Shellcode Generation. (arXiv:2104.13100v2 [cs.SE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.13100</id>
        <link href="http://arxiv.org/abs/2104.13100"/>
        <updated>2021-06-08T02:20:21.731Z</updated>
        <summary type="html"><![CDATA[We take the first step to address the task of automatically generating
shellcodes, i.e., small pieces of code used as a payload in the exploitation of
a software vulnerability, starting from natural language comments. We assemble
and release a novel dataset (Shellcode_IA32), consisting of challenging but
common assembly instructions with their natural language descriptions. We
experiment with standard methods in neural machine translation (NMT) to
establish baseline performance levels on this task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liguori_P/0/1/0/all/0/1"&gt;Pietro Liguori&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Al_Hossami_E/0/1/0/all/0/1"&gt;Erfan Al-Hossami&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cotroneo_D/0/1/0/all/0/1"&gt;Domenico Cotroneo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Natella_R/0/1/0/all/0/1"&gt;Roberto Natella&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cukic_B/0/1/0/all/0/1"&gt;Bojan Cukic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shaikh_S/0/1/0/all/0/1"&gt;Samira Shaikh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sequential Random Network for Fine-grained Image Classification. (arXiv:2103.07230v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.07230</id>
        <link href="http://arxiv.org/abs/2103.07230"/>
        <updated>2021-06-08T02:20:21.716Z</updated>
        <summary type="html"><![CDATA[Deep Convolutional Neural Network (DCNN) and Transformer have achieved
remarkable successes in image recognition. However, their performance in
fine-grained image recognition is still difficult to meet the requirements of
actual needs. This paper proposes a Sequence Random Network (SRN) to enhance
the performance of DCNN. The output of DCNN is one-dimensional features. This
one-dimensional feature abstractly represents image information, but it does
not express well the detailed information of image. To address this issue, we
use the proposed SRN which composed of BiLSTM and several Tanh-Dropout blocks
(called BiLSTM-TDN), to further process DCNN one-dimensional features for
highlighting the detail information of image. After the feature transform by
BiLSTM-TDN, the recognition performance has been greatly improved. We conducted
the experiments on six fine-grained image datasets. Except for FGVC-Aircraft,
the accuracies of the proposed methods on the other datasets exceeded 99%.
Experimental results show that BiLSTM-TDN is far superior to the existing
state-of-the-art methods. In addition to DCNN, BiLSTM-TDN can also be extended
to other models, such as Transformer.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chaorong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Malu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1"&gt;Wei Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_F/0/1/0/all/0/1"&gt;Fengqing Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_A/0/1/0/all/0/1"&gt;Anping Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yuanyuan Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Detection of fake news on CoViD-19 on Web Search Engines. (arXiv:2103.11804v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.11804</id>
        <link href="http://arxiv.org/abs/2103.11804"/>
        <updated>2021-06-08T02:20:21.706Z</updated>
        <summary type="html"><![CDATA[In early January 2020, after China reported the first cases of the new
coronavirus (SARS-CoV-2) in the city of Wuhan, unreliable and not fully
accurate information has started spreading faster than the virus itself.
Alongside this pandemic, people have experienced a parallel infodemic, i.e., an
overabundance of information, some of which misleading or even harmful, that
has widely spread around the globe. Although Social Media are increasingly
being used as information source, Web Search Engines, like Google or Yahoo!,
still represent a powerful and trustworthy resource for finding information on
the Web. This is due to their capability to capture the largest amount of
information, helping users quickly identify the most relevant, useful, although
not always the most reliable, results for their search queries. This study aims
to detect potential misleading and fake contents by capturing and analysing
textual information, which flow through Search Engines. By using a real-world
dataset associated with recent CoViD-19 pandemic, we first apply re-sampling
techniques for class imbalance, then we use existing Machine Learning
algorithms for classification of not reliable news. By extracting lexical and
host-based features of associated Uniform Resource Locators (URLs) for news
articles, we show that the proposed methods, so common in phishing and
malicious URLs detection, can improve the efficiency and performance of
classifiers. Based on these findings, we suggest that the use of both textual
and URLs features can improve the effectiveness of fake news detection methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mazzeo_V/0/1/0/all/0/1"&gt;V. Mazzeo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rapisarda_A/0/1/0/all/0/1"&gt;A. Rapisarda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Giuffrida_G/0/1/0/all/0/1"&gt;G. Giuffrida&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cluster-Former: Clustering-based Sparse Transformer for Long-Range Dependency Encoding. (arXiv:2009.06097v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.06097</id>
        <link href="http://arxiv.org/abs/2009.06097"/>
        <updated>2021-06-08T02:20:21.699Z</updated>
        <summary type="html"><![CDATA[Transformer has become ubiquitous in the deep learning field. One of the key
ingredients that destined its success is the self-attention mechanism, which
allows fully-connected contextual encoding over input tokens. However, despite
its effectiveness in modeling short sequences, self-attention suffers when
handling inputs with extreme long-range dependencies, as its complexity grows
quadratically with respect to the sequence length. Therefore, long sequences
are often encoded by Transformer in chunks using a sliding window. In this
paper, we propose Cluster-Former, a novel clustering-based sparse Transformer
to perform attention across chunked sequences. The proposed framework is
pivoted on two unique types of Transformer layer: Sliding-Window Layer and
Cluster-Former Layer, which encode local sequence information and global
context jointly and iteratively. This new design allows information integration
beyond local windows, which is especially beneficial for question answering
(QA) tasks that rely on long-range dependencies. Experiments show that
Cluster-Former achieves state-of-the-art performance on several major QA
benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shuohang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1"&gt;Luowei Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gan_Z/0/1/0/all/0/1"&gt;Zhe Gan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yen-Chun Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1"&gt;Yuwei Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1"&gt;Siqi Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1"&gt;Yu Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jingjing Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Damaging Contrastive Learning. (arXiv:2106.02990v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02990</id>
        <link href="http://arxiv.org/abs/2106.02990"/>
        <updated>2021-06-08T02:20:21.693Z</updated>
        <summary type="html"><![CDATA[The recent breakthrough achieved by contrastive learning accelerates the pace
for deploying unsupervised training on real-world data applications. However,
unlabeled data in reality is commonly imbalanced and shows a long-tail
distribution, and it is unclear how robustly the latest contrastive learning
methods could perform in the practical scenario. This paper proposes to
explicitly tackle this challenge, via a principled framework called
Self-Damaging Contrastive Learning (SDCLR), to automatically balance the
representation learning without knowing the classes. Our main inspiration is
drawn from the recent finding that deep models have difficult-to-memorize
samples, and those may be exposed through network pruning. It is further
natural to hypothesize that long-tail samples are also tougher for the model to
learn well due to insufficient examples. Hence, the key innovation in SDCLR is
to create a dynamic self-competitor model to contrast with the target model,
which is a pruned version of the latter. During training, contrasting the two
models will lead to adaptive online mining of the most easily forgotten samples
for the current target model, and implicitly emphasize them more in the
contrastive loss. Extensive experiments across multiple datasets and imbalance
settings show that SDCLR significantly improves not only overall accuracies but
also balancedness, in terms of linear evaluation on the full-shot and few-shot
settings. Our code is available at: https://github.com/VITA-Group/SDCLR.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1"&gt;Ziyu Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1"&gt;Tianlong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mortazavi_B/0/1/0/all/0/1"&gt;Bobak Mortazavi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhangyang Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hidden Markov Modeling for Maximum Likelihood Neuron Reconstruction. (arXiv:2106.02701v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02701</id>
        <link href="http://arxiv.org/abs/2106.02701"/>
        <updated>2021-06-08T02:20:21.686Z</updated>
        <summary type="html"><![CDATA[Recent advances in brain clearing and imaging have made it possible to image
entire mammalian brains at sub-micron resolution. These images offer the
potential to assemble brain-wide atlases of projection neuron morphology, but
manual neuron reconstruction remains a bottleneck. Here we present a method
inspired by hidden Markov modeling and appearance modeling of fluorescent
neuron images that can automatically trace neuronal processes. Our method
leverages dynamic programming to scale to terabyte sized image data and can be
applied to images with one or more neurons. We applied our algorithm to the
output of image segmentation models where false negatives severed neuronal
processes, and showed that it can follow axons in the presence of noise or
nearby neurons. Our method has the potential to be integrated into a semi or
fully automated reconstruction pipeline. Additionally, it creates a framework
through which users can intervene with hard constraints to, for example, rule
out certain reconstructions, or assign axons to particular cell bodies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Athey_T/0/1/0/all/0/1"&gt;Thomas L. Athey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tward_D/0/1/0/all/0/1"&gt;Daniel Tward&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mueller_U/0/1/0/all/0/1"&gt;Ulrich Mueller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Miller_M/0/1/0/all/0/1"&gt;Michael I. Miller&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DEEPMIR: A DEEP neural network for differential detection of cerebral Microbleeds and IRon deposits in MRI. (arXiv:2010.00148v3 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.00148</id>
        <link href="http://arxiv.org/abs/2010.00148"/>
        <updated>2021-06-08T02:20:21.659Z</updated>
        <summary type="html"><![CDATA[Lobar cerebral microbleeds (CMBs) and localized non-hemorrhage iron deposits
in the basal ganglia have been associated with brain aging, vascular disease
and neurodegenerative disorders. Particularly, CMBs are small lesions and
require multiple neuroimaging modalities for accurate detection. Quantitative
susceptibility mapping (QSM) derived from in vivo magnetic resonance imaging
(MRI) is necessary to differentiate between iron content and mineralization. We
set out to develop a deep learning-based segmentation method suitable for
segmenting both CMBs and iron deposits. We included a convenience sample of 24
participants from the MESA cohort and used T2-weighted images, susceptibility
weighted imaging (SWI), and QSM to segment the two types of lesions. We
developed a protocol for simultaneous manual annotation of CMBs and
non-hemorrhage iron deposits in the basal ganglia. This manual annotation was
then used to train a deep convolution neural network (CNN). Specifically, we
adapted the U-Net model with a higher number of resolution layers to be able to
detect small lesions such as CMBs from standard resolution MRI. We tested
different combinations of the three modalities to determine the most
informative data sources for the detection tasks. In the detection of CMBs
using single class and multiclass models, we achieved an average sensitivity
and precision of between 0.84-0.88 and 0.40-0.59, respectively. The same
framework detected non-hemorrhage iron deposits with an average sensitivity and
precision of about 0.75-0.81 and 0.62-0.75, respectively. Our results showed
that deep learning could automate the detection of small vessel disease lesions
and including multimodal MR data (particularly QSM) can improve the detection
of CMB and non-hemorrhage iron deposits with sensitivity and precision that is
compatible with use in large-scale research studies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Rashid_T/0/1/0/all/0/1"&gt;Tanweer Rashid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Abdulkadir_A/0/1/0/all/0/1"&gt;Ahmed Abdulkadir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Nasrallah_I/0/1/0/all/0/1"&gt;Ilya M. Nasrallah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ware_J/0/1/0/all/0/1"&gt;Jeffrey B. Ware&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liu_H/0/1/0/all/0/1"&gt;Hangfan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Spincemaille_P/0/1/0/all/0/1"&gt;Pascal Spincemaille&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Romero_J/0/1/0/all/0/1"&gt;J. Rafael Romero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bryan_R/0/1/0/all/0/1"&gt;R. Nick Bryan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Heckbert_S/0/1/0/all/0/1"&gt;Susan R. Heckbert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Habes_M/0/1/0/all/0/1"&gt;Mohamad Habes&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Coarse-to-Fine Entity Representations for Document-level Relation Extraction. (arXiv:2012.02507v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.02507</id>
        <link href="http://arxiv.org/abs/2012.02507"/>
        <updated>2021-06-08T02:20:21.652Z</updated>
        <summary type="html"><![CDATA[Document-level Relation Extraction (RE) requires extracting relations
expressed within and across sentences. Recent works show that graph-based
methods, usually constructing a document-level graph that captures
document-aware interactions, can obtain useful entity representations thus
helping tackle document-level RE. These methods either focus more on the entire
graph, or pay more attention to a part of the graph, e.g., paths between the
target entity pair. However, we find that document-level RE may benefit from
focusing on both of them simultaneously. Therefore, to obtain more
comprehensive entity representations, we propose the Coarse-to-Fine Entity
Representation model (CFER) that adopts a coarse-to-fine strategy involving two
phases. First, CFER uses graph neural networks to integrate global information
in the entire graph at a coarse level. Next, CFER utilizes the global
information as a guidance to selectively aggregate path information between the
target entity pair at a fine level. In classification, we combine the entity
representations from both two levels into more comprehensive representations
for relation extraction. Experimental results on two document-level RE
datasets, DocRED and CDR, show that CFER outperforms existing models and is
robust to the uneven label distribution.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1"&gt;Damai Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1"&gt;Jing Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_S/0/1/0/all/0/1"&gt;Shuang Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_B/0/1/0/all/0/1"&gt;Baobao Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sui_Z/0/1/0/all/0/1"&gt;Zhifang Sui&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Non-Volume Preserving-based Fusion to Group-Level Emotion Recognition on Crowd Videos. (arXiv:1811.11849v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1811.11849</id>
        <link href="http://arxiv.org/abs/1811.11849"/>
        <updated>2021-06-08T02:20:21.643Z</updated>
        <summary type="html"><![CDATA[Group-level emotion recognition (ER) is a growing research area as the
demands for assessing crowds of all sizes are becoming an interest in both the
security arena as well as social media. This work extends the earlier ER
investigations, which focused on either group-level ER on single images or
within a video, by fully investigating group-level expression recognition on
crowd videos. In this paper, we propose an effective deep feature level fusion
mechanism to model the spatial-temporal information in the crowd videos. In our
approach, the fusing process is performed on the deep feature domain by a
generative probabilistic model, Non-Volume Preserving Fusion (NVPF), that
models spatial information relationships. Furthermore, we extend our proposed
spatial NVPF approach to the spatial-temporal NVPF approach to learn the
temporal information between frames. To demonstrate the robustness and
effectiveness of each component in the proposed approach, three experiments
were conducted: (i) evaluation on AffectNet database to benchmark the proposed
EmoNet for recognizing facial expression; (ii) evaluation on EmotiW2018 to
benchmark the proposed deep feature level fusion mechanism NVPF; and, (iii)
examine the proposed TNVPF on an innovative Group-level Emotion on Crowd Videos
(GECV) dataset composed of 627 videos collected from publicly available
sources. GECV dataset is a collection of videos containing crowds of people.
Each video is labeled with emotion categories at three levels: individual
faces, group of people, and the entire video frame.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Quach_K/0/1/0/all/0/1"&gt;Kha Gia Quach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Le_N/0/1/0/all/0/1"&gt;Ngan Le&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duong_C/0/1/0/all/0/1"&gt;Chi Nhan Duong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jalata_I/0/1/0/all/0/1"&gt;Ibsa Jalata&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roy_K/0/1/0/all/0/1"&gt;Kaushik Roy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luu_K/0/1/0/all/0/1"&gt;Khoa Luu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Orthogonal Over-Parameterized Training. (arXiv:2004.04690v6 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.04690</id>
        <link href="http://arxiv.org/abs/2004.04690"/>
        <updated>2021-06-08T02:20:21.636Z</updated>
        <summary type="html"><![CDATA[The inductive bias of a neural network is largely determined by the
architecture and the training algorithm. To achieve good generalization, how to
effectively train a neural network is of great importance. We propose a novel
orthogonal over-parameterized training (OPT) framework that can provably
minimize the hyperspherical energy which characterizes the diversity of neurons
on a hypersphere. By maintaining the minimum hyperspherical energy during
training, OPT can greatly improve the empirical generalization. Specifically,
OPT fixes the randomly initialized weights of the neurons and learns an
orthogonal transformation that applies to these neurons. We consider multiple
ways to learn such an orthogonal transformation, including unrolling
orthogonalization algorithms, applying orthogonal parameterization, and
designing orthogonality-preserving gradient descent. For better scalability, we
propose the stochastic OPT which performs orthogonal transformation
stochastically for partial dimensions of neurons. Interestingly, OPT reveals
that learning a proper coordinate system for neurons is crucial to
generalization. We provide some insights on why OPT yields better
generalization. Extensive experiments validate the superiority of OPT over the
standard training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Weiyang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_R/0/1/0/all/0/1"&gt;Rongmei Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhen Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rehg_J/0/1/0/all/0/1"&gt;James M. Rehg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paull_L/0/1/0/all/0/1"&gt;Liam Paull&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_L/0/1/0/all/0/1"&gt;Li Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1"&gt;Le Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weller_A/0/1/0/all/0/1"&gt;Adrian Weller&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Over-the-Air Adversarial Flickering Attacks against Video Recognition Networks. (arXiv:2002.05123v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.05123</id>
        <link href="http://arxiv.org/abs/2002.05123"/>
        <updated>2021-06-08T02:20:21.618Z</updated>
        <summary type="html"><![CDATA[Deep neural networks for video classification, just like image classification
networks, may be subjected to adversarial manipulation. The main difference
between image classifiers and video classifiers is that the latter usually use
temporal information contained within the video. In this work we present a
manipulation scheme for fooling video classifiers by introducing a flickering
temporal perturbation that in some cases may be unnoticeable by human observers
and is implementable in the real world. After demonstrating the manipulation of
action classification of single videos, we generalize the procedure to make
universal adversarial perturbation, achieving high fooling ratio. In addition,
we generalize the universal perturbation and produce a temporal-invariant
perturbation, which can be applied to the video without synchronizing the
perturbation to the input. The attack was implemented on several target models
and the transferability of the attack was demonstrated. These properties allow
us to bridge the gap between simulated environment and real-world application,
as will be demonstrated in this paper for the first time for an over-the-air
flickering attack.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pony_R/0/1/0/all/0/1"&gt;Roi Pony&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Naeh_I/0/1/0/all/0/1"&gt;Itay Naeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mannor_S/0/1/0/all/0/1"&gt;Shie Mannor&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DEFT: Detection Embeddings for Tracking. (arXiv:2102.02267v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.02267</id>
        <link href="http://arxiv.org/abs/2102.02267"/>
        <updated>2021-06-08T02:20:21.611Z</updated>
        <summary type="html"><![CDATA[Most modern multiple object tracking (MOT) systems follow the
tracking-by-detection paradigm, consisting of a detector followed by a method
for associating detections into tracks. There is a long history in tracking of
combining motion and appearance features to provide robustness to occlusions
and other challenges, but typically this comes with the trade-off of a more
complex and slower implementation. Recent successes on popular 2D tracking
benchmarks indicate that top-scores can be achieved using a state-of-the-art
detector and relatively simple associations relying on single-frame spatial
offsets -- notably outperforming contemporary methods that leverage learned
appearance features to help re-identify lost tracks. In this paper, we propose
an efficient joint detection and tracking model named DEFT, or "Detection
Embeddings for Tracking." Our approach relies on an appearance-based object
matching network jointly-learned with an underlying object detection network.
An LSTM is also added to capture motion constraints. DEFT has comparable
accuracy and speed to the top methods on 2D online tracking leaderboards while
having significant advantages in robustness when applied to more challenging
tracking data. DEFT raises the bar on the nuScenes monocular 3D tracking
challenge, more than doubling the performance of the previous top method. Code
is publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chaabane_M/0/1/0/all/0/1"&gt;Mohamed Chaabane&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1"&gt;Peter Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Beveridge_J/0/1/0/all/0/1"&gt;J. Ross Beveridge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+OHara_S/0/1/0/all/0/1"&gt;Stephen O&amp;#x27;Hara&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Middle-level Fusion for Lightweight RGB-D Salient Object Detection. (arXiv:2104.11543v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.11543</id>
        <link href="http://arxiv.org/abs/2104.11543"/>
        <updated>2021-06-08T02:20:21.601Z</updated>
        <summary type="html"><![CDATA[Most existing lightweight RGB-D salient object detection (SOD) models are
based on two-stream structure or single-stream structure. The former one first
uses two sub-networks to extract unimodal features from RGB and depth images,
respectively, and then fuses them for SOD. While, the latter one directly
extracts multi-modal features from the input RGB-D images and then focuses on
exploiting cross-level complementary information. However, two-stream structure
based models inevitably require more parameters and single-stream structure
based ones cannot well exploit the cross-modal complementary information since
they ignore the modality difference. To address these issues, we propose to
employ the middle-level fusion structure for designing lightweight RGB-D SOD
model in this paper, which first employs two sub-networks to extract low- and
middle-level unimodal features, respectively, and then fuses those extracted
middle-level unimodal features for extracting corresponding high-level
multi-modal features in the subsequent sub-network. Different from existing
models, this structure can effectively exploit the cross-modal complementary
information and significantly reduce the network's parameters, simultaneously.
Therefore, a novel lightweight SOD model is designed, which contains a
information-aware multi-modal feature fusion (IMFF) module for effectively
capturing the cross-modal complementary information and a lightweight
feature-level and decision-level feature fusion (LFDF) module for aggregating
the feature-level and the decision-level saliency information in different
stages with less parameters. Our proposed model has only 3.9M parameters and
runs at 33 FPS. The experimental results on several benchmark datasets verify
the effectiveness and superiority of the proposed method over some
state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_N/0/1/0/all/0/1"&gt;Nianchang Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Qiang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1"&gt;Jungong Han&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Real Time Video based Heart and Respiration Rate Monitoring. (arXiv:2106.02669v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.02669</id>
        <link href="http://arxiv.org/abs/2106.02669"/>
        <updated>2021-06-08T02:20:21.593Z</updated>
        <summary type="html"><![CDATA[In recent years, research about monitoring vital signs by smartphones grows
significantly. There are some special sensors like Electrocardiogram (ECG) and
Photoplethysmographic (PPG) to detect heart rate (HR) and respiration rate
(RR). Smartphone cameras also can measure HR by detecting and processing
imaging Photoplethysmographic (iPPG) signals from the video of a user's face.
Indeed, the variation in the intensity of the green channel can be measured by
the iPPG signals of the video. This study aimed to provide a method to extract
heart rate and respiration rate using the video of individuals' faces. The
proposed method is based on measuring fluctuations in the Hue, and can
therefore extract both HR and RR from the video of a user's face. The proposed
method is evaluated by performing on 25 healthy individuals. For each subject,
20 seconds video of his/her face is recorded. Results show that the proposed
approach of measuring iPPG using Hue gives more accurate rates than the Green
channel.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Pourbemany_J/0/1/0/all/0/1"&gt;Jafar Pourbemany&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Essa_A/0/1/0/all/0/1"&gt;Almabrok Essa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Ye Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Intent Classification and Slot Filling for Privacy Policies. (arXiv:2101.00123v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.00123</id>
        <link href="http://arxiv.org/abs/2101.00123"/>
        <updated>2021-06-08T02:20:21.587Z</updated>
        <summary type="html"><![CDATA[Understanding privacy policies is crucial for users as it empowers them to
learn about the information that matters to them. Sentences written in a
privacy policy document explain privacy practices, and the constituent text
spans convey further specific information about that practice. We refer to
predicting the privacy practice explained in a sentence as intent
classification and identifying the text spans sharing specific information as
slot filling. In this work, we propose PolicyIE, an English corpus consisting
of 5,250 intent and 11,788 slot annotations spanning 31 privacy policies of
websites and mobile applications. PolicyIE corpus is a challenging real-world
benchmark with limited labeled examples reflecting the cost of collecting
large-scale annotations from domain experts. We present two alternative neural
approaches as baselines, (1) intent classification and slot filling as a joint
sequence tagging and (2) modeling them as a sequence-to-sequence (Seq2Seq)
learning task. The experiment results show that both approaches perform
comparably in intent classification, while the Seq2Seq method outperforms the
sequence tagging approach in slot filling by a large margin. We perform a
detailed error analysis to reveal the challenges of the proposed corpus.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ahmad_W/0/1/0/all/0/1"&gt;Wasi Uddin Ahmad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chi_J/0/1/0/all/0/1"&gt;Jianfeng Chi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1"&gt;Tu Le&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Norton_T/0/1/0/all/0/1"&gt;Thomas Norton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1"&gt;Yuan Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1"&gt;Kai-Wei Chang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RegionViT: Regional-to-Local Attention for Vision Transformers. (arXiv:2106.02689v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02689</id>
        <link href="http://arxiv.org/abs/2106.02689"/>
        <updated>2021-06-08T02:20:21.580Z</updated>
        <summary type="html"><![CDATA[Vision transformer (ViT) has recently showed its strong capability in
achieving comparable results to convolutional neural networks (CNNs) on image
classification. However, vanilla ViT simply inherits the same architecture from
the natural language processing directly, which is often not optimized for
vision applications. Motivated by this, in this paper, we propose a new
architecture that adopts the pyramid structure and employ a novel
regional-to-local attention rather than global self-attention in vision
transformers. More specifically, our model first generates regional tokens and
local tokens from an image with different patch sizes, where each regional
token is associated with a set of local tokens based on the spatial location.
The regional-to-local attention includes two steps: first, the regional
self-attention extract global information among all regional tokens and then
the local self-attention exchanges the information among one regional token and
the associated local tokens via self-attention. Therefore, even though local
self-attention confines the scope in a local region but it can still receive
global information. Extensive experiments on three vision tasks, including
image classification, object detection and action recognition, show that our
approach outperforms or is on par with state-of-the-art ViT variants including
many concurrent works. Our source codes and models will be publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Chun-Fu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Panda_R/0/1/0/all/0/1"&gt;Rameswar Panda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_Q/0/1/0/all/0/1"&gt;Quanfu Fan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Differentiable Open-Ended Commonsense Reasoning. (arXiv:2010.14439v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.14439</id>
        <link href="http://arxiv.org/abs/2010.14439"/>
        <updated>2021-06-08T02:20:21.562Z</updated>
        <summary type="html"><![CDATA[Current commonsense reasoning research focuses on developing models that use
commonsense knowledge to answer multiple-choice questions. However, systems
designed to answer multiple-choice questions may not be useful in applications
that do not provide a small list of candidate answers to choose from. As a step
towards making commonsense reasoning research more realistic, we propose to
study open-ended commonsense reasoning (OpenCSR) -- the task of answering a
commonsense question without any pre-defined choices -- using as a resource
only a corpus of commonsense facts written in natural language. OpenCSR is
challenging due to a large decision space, and because many questions require
implicit multi-hop reasoning. As an approach to OpenCSR, we propose DrFact, an
efficient Differentiable model for multi-hop Reasoning over knowledge Facts. To
evaluate OpenCSR methods, we adapt several popular commonsense reasoning
benchmarks, and collect multiple new answers for each test question via
crowd-sourcing. Experiments show that DrFact outperforms strong baseline
methods by a large margin.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1"&gt;Bill Yuchen Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1"&gt;Haitian Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dhingra_B/0/1/0/all/0/1"&gt;Bhuwan Dhingra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zaheer_M/0/1/0/all/0/1"&gt;Manzil Zaheer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1"&gt;Xiang Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cohen_W/0/1/0/all/0/1"&gt;William W. Cohen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bridging the Gap Between Adversarial Robustness and Optimization Bias. (arXiv:2102.08868v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.08868</id>
        <link href="http://arxiv.org/abs/2102.08868"/>
        <updated>2021-06-08T02:20:21.553Z</updated>
        <summary type="html"><![CDATA[We demonstrate that the choice of optimizer, neural network architecture, and
regularizer significantly affect the adversarial robustness of linear neural
networks, providing guarantees without the need for adversarial training. To
this end, we revisit a known result linking maximally robust classifiers and
minimum norm solutions, and combine it with recent results on the implicit bias
of optimizers. First, we show that, under certain conditions, it is possible to
achieve both perfect standard accuracy and a certain degree of robustness,
simply by training an overparametrized model using the implicit bias of the
optimization. In that regime, there is a direct relationship between the type
of the optimizer and the attack to which the model is robust. To the best of
our knowledge, this work is the first to study the impact of optimization
methods such as sign gradient descent and proximal methods on adversarial
robustness. Second, we characterize the robustness of linear convolutional
models, showing that they resist attacks subject to a constraint on the
Fourier-$\ell_\infty$ norm. To illustrate these findings we design a novel
Fourier-$\ell_\infty$ attack that finds adversarial examples with controllable
frequencies. We evaluate Fourier-$\ell_\infty$ robustness of
adversarially-trained deep CIFAR-10 models from the standard RobustBench
benchmark and visualize adversarial perturbations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Faghri_F/0/1/0/all/0/1"&gt;Fartash Faghri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gowal_S/0/1/0/all/0/1"&gt;Sven Gowal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vasconcelos_C/0/1/0/all/0/1"&gt;Cristina Vasconcelos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fleet_D/0/1/0/all/0/1"&gt;David J. Fleet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pedregosa_F/0/1/0/all/0/1"&gt;Fabian Pedregosa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roux_N/0/1/0/all/0/1"&gt;Nicolas Le Roux&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cascaded Context Enhancement Network for Automatic Skin Lesion Segmentation. (arXiv:2004.08107v3 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.08107</id>
        <link href="http://arxiv.org/abs/2004.08107"/>
        <updated>2021-06-08T02:20:21.547Z</updated>
        <summary type="html"><![CDATA[Skin lesion segmentation is an important step for automatic melanoma
diagnosis. Due to the non-negligible diversity of lesions from different
patients, extracting powerful context for fine-grained semantic segmentation is
still challenging today. Although the deep convolutional neural network (CNNs)
have made significant improvements on skin lesion segmentation, they often fail
to reserve the spatial details and long-range dependencies context due to
consecutive convolution striding and pooling operations inside CNNs. In this
paper, we formulate a cascaded context enhancement neural network for automatic
skin lesion segmentation. A new cascaded context aggregation (CCA) module with
a gate-based information integration approach is proposed to sequentially and
selectively aggregate original image and multi-level features from the encoder
sub-network. The generated context is further utilized to guide discriminative
features extraction by the designed context-guided local affinity (CGL) module.
Furthermore, an auxiliary loss is added to the CCA module for refining the
prediction. In our work, we evaluate our approach on four public skin
dermoscopy image datasets. The proposed method achieves the Jaccard Index (JA)
of 87.1%, 80.3%, 83.4%, and 86.6% on ISIC-2016, ISIC-2017, ISIC-2018, and PH2
datasets, which are higher than other state-of-the-art models respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Wang_R/0/1/0/all/0/1"&gt;Ruxin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chen_S/0/1/0/all/0/1"&gt;Shuyuan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ji_C/0/1/0/all/0/1"&gt;Chaojie Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1"&gt;Ye Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Learning by Self-Transition for Handling Noisy Labels. (arXiv:2012.04337v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.04337</id>
        <link href="http://arxiv.org/abs/2012.04337"/>
        <updated>2021-06-08T02:20:21.531Z</updated>
        <summary type="html"><![CDATA[Real-world data inevitably contains noisy labels, which induce the poor
generalization of deep neural networks. It is known that the network typically
begins to rapidly memorize false-labeled samples after a certain point of
training. Thus, to counter the label noise challenge, we propose a novel
self-transitional learning method called MORPH, which automatically switches
its learning phase at the transition point from seeding to evolution. In the
seeding phase, the network is updated using all the samples to collect a seed
of clean samples. Then, in the evolution phase, the network is updated using
only the set of arguably clean samples, which precisely keeps expanding by the
updated network. Thus, MORPH effectively avoids the overfitting to
false-labeled samples throughout the entire training period. Extensive
experiments using five real-world or synthetic benchmark datasets demonstrate
substantial improvements over state-of-the-art methods in terms of robustness
and efficiency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1"&gt;Hwanjun Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1"&gt;Minseok Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_D/0/1/0/all/0/1"&gt;Dongmin Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shin_Y/0/1/0/all/0/1"&gt;Yooju Shin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jae-Gil Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Facial Image Deformation Based on Landmark Detection. (arXiv:1910.13671v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1910.13671</id>
        <link href="http://arxiv.org/abs/1910.13671"/>
        <updated>2021-06-08T02:20:21.524Z</updated>
        <summary type="html"><![CDATA[In this work, we use facial landmarks to make the deformation for facial
images more authentic. The deformation includes the expansion of eyes and the
shrinking of noses, mouths, and cheeks. An advanced 106-point facial landmark
detector is utilized to provide control points for deformation. Bilinear
interpolation is used in the expansion and Moving Least Squares methods (MLS)
including Affine Deformation, Similarity Deformation and Rigid Deformation are
used in the shrinking. We compare the running time as well as the quality of
deformed images using different MLS methods. The experimental results show that
the Rigid Deformation which can keep other parts of the images unchanged
performs better even if it takes the longest time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1"&gt;Chaoyue Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yugang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shulai Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ni_B/0/1/0/all/0/1"&gt;Bingbing Ni&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[3D Human Mesh Regression with Dense Correspondence. (arXiv:2006.05734v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.05734</id>
        <link href="http://arxiv.org/abs/2006.05734"/>
        <updated>2021-06-08T02:20:21.516Z</updated>
        <summary type="html"><![CDATA[Estimating 3D mesh of the human body from a single 2D image is an important
task with many applications such as augmented reality and Human-Robot
interaction. However, prior works reconstructed 3D mesh from global image
feature extracted by using convolutional neural network (CNN), where the dense
correspondences between the mesh surface and the image pixels are missing,
leading to suboptimal solution. This paper proposes a model-free 3D human mesh
estimation framework, named DecoMR, which explicitly establishes the dense
correspondence between the mesh and the local image features in the UV space
(i.e. a 2D space used for texture mapping of 3D mesh). DecoMR first predicts
pixel-to-surface dense correspondence map (i.e., IUV image), with which we
transfer local features from the image space to the UV space. Then the
transferred local image features are processed in the UV space to regress a
location map, which is well aligned with transferred features. Finally we
reconstruct 3D human mesh from the regressed location map with a predefined
mapping function. We also observe that the existing discontinuous UV map are
unfriendly to the learning of network. Therefore, we propose a novel UV map
that maintains most of the neighboring relations on the original mesh surface.
Experiments demonstrate that our proposed local feature alignment and
continuous UV map outperforms existing 3D mesh based methods on multiple public
benchmarks. Code will be made available at
https://github.com/zengwang430521/DecoMR]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1"&gt;Wang Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1"&gt;Wanli Ouyang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1"&gt;Ping Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wentao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaogang Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spectral Temporal Graph Neural Network for Trajectory Prediction. (arXiv:2106.02930v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02930</id>
        <link href="http://arxiv.org/abs/2106.02930"/>
        <updated>2021-06-08T02:20:21.508Z</updated>
        <summary type="html"><![CDATA[An effective understanding of the contextual environment and accurate motion
forecasting of surrounding agents is crucial for the development of autonomous
vehicles and social mobile robots. This task is challenging since the behavior
of an autonomous agent is not only affected by its own intention, but also by
the static environment and surrounding dynamically interacting agents. Previous
works focused on utilizing the spatial and temporal information in time domain
while not sufficiently taking advantage of the cues in frequency domain. To
this end, we propose a Spectral Temporal Graph Neural Network (SpecTGNN), which
can capture inter-agent correlations and temporal dependency simultaneously in
frequency domain in addition to time domain. SpecTGNN operates on both an agent
graph with dynamic state information and an environment graph with the features
extracted from context images in two streams. The model integrates graph
Fourier transform, spectral graph convolution and temporal gated convolution to
encode history information and forecast future trajectories. Moreover, we
incorporate a multi-head spatio-temporal attention mechanism to mitigate the
effect of error propagation in a long time horizon. We demonstrate the
performance of SpecTGNN on two public trajectory prediction benchmark datasets,
which achieves state-of-the-art performance in terms of prediction accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cao_D/0/1/0/all/0/1"&gt;Defu Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jiachen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1"&gt;Hengbo Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tomizuka_M/0/1/0/all/0/1"&gt;Masayoshi Tomizuka&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FS-Net: Fast Shape-based Network for Category-Level 6D Object Pose Estimation with Decoupled Rotation Mechanism. (arXiv:2103.07054v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.07054</id>
        <link href="http://arxiv.org/abs/2103.07054"/>
        <updated>2021-06-08T02:20:21.501Z</updated>
        <summary type="html"><![CDATA[In this paper, we focus on category-level 6D pose and size estimation from
monocular RGB-D image. Previous methods suffer from inefficient category-level
pose feature extraction which leads to low accuracy and inference speed. To
tackle this problem, we propose a fast shape-based network (FS-Net) with
efficient category-level feature extraction for 6D pose estimation. First, we
design an orientation aware autoencoder with 3D graph convolution for latent
feature extraction. The learned latent feature is insensitive to point shift
and object size thanks to the shift and scale-invariance properties of the 3D
graph convolution. Then, to efficiently decode category-level rotation
information from the latent feature, we propose a novel decoupled rotation
mechanism that employs two decoders to complementarily access the rotation
information. Meanwhile, we estimate translation and size by two residuals,
which are the difference between the mean of object points and ground truth
translation, and the difference between the mean size of the category and
ground truth size, respectively. Finally, to increase the generalization
ability of FS-Net, we propose an online box-cage based 3D deformation mechanism
to augment the training data. Extensive experiments on two benchmark datasets
show that the proposed method achieves state-of-the-art performance in both
category- and instance-level 6D object pose estimation. Especially in
category-level pose estimation, without extra synthetic data, our method
outperforms existing methods by 6.3% on the NOCS-REAL dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1"&gt;Wei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1"&gt;Xi Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1"&gt;Hyung Jin Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duan_J/0/1/0/all/0/1"&gt;Jinming Duan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1"&gt;Linlin Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leonardis_A/0/1/0/all/0/1"&gt;Ales Leonardis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Guidance and Teaching Network for Video Salient Object Detection. (arXiv:2105.10110v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.10110</id>
        <link href="http://arxiv.org/abs/2105.10110"/>
        <updated>2021-06-08T02:20:21.494Z</updated>
        <summary type="html"><![CDATA[Owing to the difficulties of mining spatial-temporal cues, the existing
approaches for video salient object detection (VSOD) are limited in
understanding complex and noisy scenarios, and often fail in inferring
prominent objects. To alleviate such shortcomings, we propose a simple yet
efficient architecture, termed Guidance and Teaching Network (GTNet), to
independently distil effective spatial and temporal cues with implicit guidance
and explicit teaching at feature- and decision-level, respectively. To be
specific, we (a) introduce a temporal modulator to implicitly bridge features
from motion into the appearance branch, which is capable of fusing cross-modal
features collaboratively, and (b) utilise motion-guided mask to propagate the
explicit cues during the feature aggregation. This novel learning strategy
achieves satisfactory results via decoupling the complex spatial-temporal cues
and mapping informative cues across different modalities. Extensive experiments
on three challenging benchmarks show that the proposed method can run at ~28
fps on a single TITAN Xp GPU and perform competitively against 14 cutting-edge
baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiao_Y/0/1/0/all/0/1"&gt;Yingxia Jiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chou_Y/0/1/0/all/0/1"&gt;Yu-Cheng Chou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1"&gt;Shouyuan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_G/0/1/0/all/0/1"&gt;Ge-Peng Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_R/0/1/0/all/0/1"&gt;Rong Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_G/0/1/0/all/0/1"&gt;Ge Gao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[End-to-end Prostate Cancer Detection in bpMRI via 3D CNNs: Effects of Attention Mechanisms, Clinical Priori and Decoupled False Positive Reduction. (arXiv:2101.03244v8 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.03244</id>
        <link href="http://arxiv.org/abs/2101.03244"/>
        <updated>2021-06-08T02:20:21.487Z</updated>
        <summary type="html"><![CDATA[We present a multi-stage 3D computer-aided detection and diagnosis (CAD)
model for automated localization of clinically significant prostate cancer
(csPCa) in bi-parametric MR imaging (bpMRI). Deep attention mechanisms drive
its detection network, targeting salient structures and highly discriminative
feature dimensions across multiple resolutions. Its goal is to accurately
identify csPCa lesions from indolent cancer and the wide range of benign
pathology that can afflict the prostate gland. Simultaneously, a decoupled
residual classifier is used to achieve consistent false positive reduction,
without sacrificing high sensitivity or computational efficiency. In order to
guide model generalization with domain-specific clinical knowledge, a
probabilistic anatomical prior is used to encode the spatial prevalence and
zonal distinction of csPCa. Using a large dataset of 1950 prostate bpMRI paired
with radiologically-estimated annotations, we hypothesize that such CNN-based
models can be trained to detect biopsy-confirmed malignancies in an independent
cohort.

For 486 institutional testing scans, the 3D CAD system achieves
83.69$\pm$5.22% and 93.19$\pm$2.96% detection sensitivity at 0.50 and 1.46
false positive(s) per patient, respectively, with 0.882$\pm$0.030 AUROC in
patient-based diagnosis $-$significantly outperforming four state-of-the-art
baseline architectures (U-SEResNet, UNet++, nnU-Net, Attention U-Net) from
recent literature. For 296 external biopsy-confirmed testing scans, the
ensembled CAD system shares moderate agreement with a consensus of expert
radiologists (76.69%; $kappa$ $=$ 0.51$\pm$0.04) and independent pathologists
(81.08%; $kappa$ $=$ 0.56$\pm$0.06); demonstrating strong generalization to
histologically-confirmed csPCa diagnosis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Saha_A/0/1/0/all/0/1"&gt;Anindo Saha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hosseinzadeh_M/0/1/0/all/0/1"&gt;Matin Hosseinzadeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Huisman_H/0/1/0/all/0/1"&gt;Henkjan Huisman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[3D Segmentation Learning from Sparse Annotations and Hierarchical Descriptors. (arXiv:2105.12885v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.12885</id>
        <link href="http://arxiv.org/abs/2105.12885"/>
        <updated>2021-06-08T02:20:21.471Z</updated>
        <summary type="html"><![CDATA[One of the main obstacles to 3D semantic segmentation is the significant
amount of endeavor required to generate expensive point-wise annotations for
fully supervised training. To alleviate manual efforts, we propose GIDSeg, a
novel approach that can simultaneously learn segmentation from sparse
annotations via reasoning global-regional structures and individual-vicinal
properties. GIDSeg depicts global- and individual- relation via a dynamic edge
convolution network coupled with a kernelized identity descriptor. The ensemble
effects are obtained by endowing a fine-grained receptive field to a
low-resolution voxelized map. In our GIDSeg, an adversarial learning module is
also designed to further enhance the conditional constraint of identity
descriptors within the joint feature distribution. Despite the apparent
simplicity, our proposed approach achieves superior performance over
state-of-the-art for inferencing 3D dense segmentation with only sparse
annotations. Particularly, with $5\%$ annotations of raw data, GIDSeg
outperforms other 3D segmentation methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yin_P/0/1/0/all/0/1"&gt;Peng Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1"&gt;Lingyun Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_J/0/1/0/all/0/1"&gt;Jianmin Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scherer_S/0/1/0/all/0/1"&gt;Sebastian Scherer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choset_H/0/1/0/all/0/1"&gt;Howie Choset&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pinball-OCSVM for early-stage COVID-19 diagnosis with limited posteroanterior chest X-ray images. (arXiv:2010.08115v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.08115</id>
        <link href="http://arxiv.org/abs/2010.08115"/>
        <updated>2021-06-08T02:20:21.444Z</updated>
        <summary type="html"><![CDATA[The infection of respiratory coronavirus disease 2019 (COVID-19) starts with
the upper respiratory tract and as the virus grows, the infection can progress
to lungs and develop pneumonia. The conventional way of COVID-19 diagnosis is
reverse transcription polymerase chain reaction (RT-PCR), which is less
sensitive during early stages; especially if the patient is asymptomatic, which
may further cause more severe pneumonia. In this context, several deep learning
models have been proposed to identify pulmonary infections using publicly
available chest X-ray (CXR) image datasets for early diagnosis, better
treatment and quick cure. In these datasets, presence of less number of
COVID-19 positive samples compared to other classes (normal, pneumonia and
Tuberculosis) raises the challenge for unbiased learning of deep learning
models. All deep learning models opted class balancing techniques to solve this
issue; which however should be avoided in any medical diagnosis process.
Moreover, the deep learning models are also data hungry and need massive
computation resources. Therefore for quicker diagnosis, this research proposes
a novel pinball loss function based one-class support vector machine
(PB-OCSVM), that can work in presence of limited COVID-19 positive CXR samples
with objectives to maximize the learning efficiency and to minimize the false
predictions. The performance of the proposed model is compared with
conventional OCSVM and existing deep learning models, and the experimental
results prove that the proposed model outperformed over state-of-the-art
methods. To validate the robustness of the proposed model, experiments are also
performed with noisy CXR images and UCI benchmark datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Sonbhadra_S/0/1/0/all/0/1"&gt;Sanjay Kumar Sonbhadra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Agarwal_S/0/1/0/all/0/1"&gt;Sonali Agarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Nagabhushan_P/0/1/0/all/0/1"&gt;P. Nagabhushan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TARA: Training and Representation Alteration for AI Fairness and Domain Generalization. (arXiv:2012.06387v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.06387</id>
        <link href="http://arxiv.org/abs/2012.06387"/>
        <updated>2021-06-08T02:20:21.419Z</updated>
        <summary type="html"><![CDATA[We propose a novel method for enforcing AI fairness with respect to protected
or sensitive factors. This method uses a dual strategy performing training and
representation alteration (TARA) for the mitigation of prominent causes of AI
bias by including: a) the use of representation learning alteration via
adversarial independence to suppress the bias-inducing dependence of the data
representation from protected factors; and b) training set alteration via
intelligent augmentation to address bias-causing data imbalance, by using
generative models that allow the fine control of sensitive factors related to
underrepresented populations via domain adaptation and latent space
manipulation. When testing our methods on image analytics, experiments
demonstrate that TARA significantly or fully debiases baseline models while
outperforming competing debiasing methods that have the same amount of
information, e.g., with (% overall accuracy, % accuracy gap) = (78.8, 0.5) vs.
the baseline method's score of (71.8, 10.5) for EyePACS, and (73.7, 11.8) vs.
(69.1, 21.7) for CelebA. Furthermore, recognizing certain limitations in
current metrics used for assessing debiasing performance, we propose novel
conjunctive debiasing metrics. Our experiments also demonstrate the ability of
these novel metrics in assessing the Pareto efficiency of the proposed methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Paul_W/0/1/0/all/0/1"&gt;William Paul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hadzic_A/0/1/0/all/0/1"&gt;Armin Hadzic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joshi_N/0/1/0/all/0/1"&gt;Neil Joshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alajaji_F/0/1/0/all/0/1"&gt;Fady Alajaji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Burlina_P/0/1/0/all/0/1"&gt;Phil Burlina&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An End-to-End Breast Tumour Classification Model Using Context-Based Patch Modelling- A BiLSTM Approach for Image Classification. (arXiv:2106.02864v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02864</id>
        <link href="http://arxiv.org/abs/2106.02864"/>
        <updated>2021-06-08T02:20:21.393Z</updated>
        <summary type="html"><![CDATA[Researchers working on computational analysis of Whole Slide Images (WSIs) in
histopathology have primarily resorted to patch-based modelling due to large
resolution of each WSI. The large resolution makes WSIs infeasible to be fed
directly into the machine learning models due to computational constraints.
However, due to patch-based analysis, most of the current methods fail to
exploit the underlying spatial relationship among the patches. In our work, we
have tried to integrate this relationship along with feature-based correlation
among the extracted patches from the particular tumorous region. For the given
task of classification, we have used BiLSTMs to model both forward and backward
contextual relationship. RNN based models eliminate the limitation of sequence
size by allowing the modelling of variable size images within a deep learning
model. We have also incorporated the effect of spatial continuity by exploring
different scanning techniques used to sample patches. To establish the
efficiency of our approach, we trained and tested our model on two datasets,
microscopy images and WSI tumour regions. After comparing with contemporary
literature we achieved the better performance with accuracy of 90% for
microscopy image dataset. For WSI tumour region dataset, we compared the
classification results with deep learning networks such as ResNet, DenseNet,
and InceptionV3 using maximum voting technique. We achieved the highest
performance accuracy of 84%. We found out that BiLSTMs with CNN features have
performed much better in modelling patches into an end-to-end Image
classification network. Additionally, the variable dimensions of WSI tumour
regions were used for classification without the need for resizing. This
suggests that our method is independent of tumour image size and can process
large dimensional images without losing the resolution details.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tripathi_S/0/1/0/all/0/1"&gt;Suvidha Tripathi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1"&gt;Satish Kumar Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1"&gt;Hwee Kuan Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Measuring Model Biases in the Absence of Ground Truth. (arXiv:2103.03417v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.03417</id>
        <link href="http://arxiv.org/abs/2103.03417"/>
        <updated>2021-06-08T02:20:21.386Z</updated>
        <summary type="html"><![CDATA[The measurement of bias in machine learning often focuses on model
performance across identity subgroups (such as man and woman) with respect to
groundtruth labels. However, these methods do not directly measure the
associations that a model may have learned, for example between labels and
identity subgroups. Further, measuring a model's bias requires a fully
annotated evaluation dataset which may not be easily available in practice. We
present an elegant mathematical solution that tackles both issues
simultaneously, using image classification as a working example. By treating a
classification model's predictions for a given image as a set of labels
analogous to a bag of words, we rank the biases that a model has learned with
respect to different identity labels. We use (man, woman) as a concrete example
of an identity label set (although this set need not be binary), and present
rankings for the labels that are most biased towards one identity or the other.
We demonstrate how the statistical properties of different association metrics
can lead to different rankings of the most "gender biased" labels, and conclude
that normalized pointwise mutual information (nPMI) is most useful in practice.
Finally, we announce an open-sourced nPMI visualization tool using TensorBoard.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aka_O/0/1/0/all/0/1"&gt;Osman Aka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Burke_K/0/1/0/all/0/1"&gt;Ken Burke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bauerle_A/0/1/0/all/0/1"&gt;Alex B&amp;#xe4;uerle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Greer_C/0/1/0/all/0/1"&gt;Christina Greer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mitchell_M/0/1/0/all/0/1"&gt;Margaret Mitchell&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NeuralLog: Natural Language Inference with Joint Neural and Logical Reasoning. (arXiv:2105.14167v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14167</id>
        <link href="http://arxiv.org/abs/2105.14167"/>
        <updated>2021-06-08T02:20:21.356Z</updated>
        <summary type="html"><![CDATA[Deep learning (DL) based language models achieve high performance on various
benchmarks for Natural Language Inference (NLI). And at this time, symbolic
approaches to NLI are receiving less attention. Both approaches (symbolic and
DL) have their advantages and weaknesses. However, currently, no method
combines them in a system to solve the task of NLI. To merge symbolic and deep
learning methods, we propose an inference framework called NeuralLog, which
utilizes both a monotonicity-based logical inference engine and a neural
network language model for phrase alignment. Our framework models the NLI task
as a classic search problem and uses the beam search algorithm to search for
optimal inference paths. Experiments show that our joint logic and neural
inference system improves accuracy on the NLI task and can achieve state-of-art
accuracy on the SICK and MED datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zeming Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Q/0/1/0/all/0/1"&gt;Qiyue Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moss_L/0/1/0/all/0/1"&gt;Lawrence S. Moss&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ZeroWaste Dataset: Towards Automated Waste Recycling. (arXiv:2106.02740v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02740</id>
        <link href="http://arxiv.org/abs/2106.02740"/>
        <updated>2021-06-08T02:20:21.296Z</updated>
        <summary type="html"><![CDATA[Less than 35% of recyclable waste is being actually recycled in the US, which
leads to increased soil and sea pollution and is one of the major concerns of
environmental researchers as well as the common public. At the heart of the
problem is the inefficiencies of the waste sorting process (separating paper,
plastic, metal, glass, etc.) due to the extremely complex and cluttered nature
of the waste stream. Automated waste detection strategies have a great
potential to enable more efficient, reliable and safer waste sorting practices,
but the literature lacks comprehensive datasets and methodology for the
industrial waste sorting solutions. In this paper, we take a step towards
computer-aided waste detection and present the first in-the-wild
industrial-grade waste detection and segmentation dataset, ZeroWaste. This
dataset contains over1800fully segmented video frames collected from a real
waste sorting plant along with waste material labels for training and
evaluation of the segmentation methods, as well as over6000unlabeled frames
that can be further used for semi-supervised and self-supervised learning
techniques. ZeroWaste also provides frames of the conveyor belt before and
after the sorting process, comprising a novel setup that can be used for
weakly-supervised segmentation. We present baselines for fully-, semi- and
weakly-supervised segmentation methods. Our experimental results demonstrate
that state-of-the-art segmentation methods struggle to correctly detect and
classify target objects which suggests the challenging nature of our proposed
in-the-wild dataset. We believe that ZeroWastewill catalyze research in object
detection and semantic segmentation in extreme clutter as well as applications
in the recycling domain. Our project page can be found
atthis http URL]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bashkirova_D/0/1/0/all/0/1"&gt;Dina Bashkirova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1"&gt;Ziliang Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Akl_J/0/1/0/all/0/1"&gt;James Akl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alladkani_F/0/1/0/all/0/1"&gt;Fadi Alladkani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_P/0/1/0/all/0/1"&gt;Ping Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ablavsky_V/0/1/0/all/0/1"&gt;Vitaly Ablavsky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Calli_B/0/1/0/all/0/1"&gt;Berk Calli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bargal_S/0/1/0/all/0/1"&gt;Sarah Adel Bargal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saenko_K/0/1/0/all/0/1"&gt;Kate Saenko&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RDA: Robust Domain Adaptation via Fourier Adversarial Attacking. (arXiv:2106.02874v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02874</id>
        <link href="http://arxiv.org/abs/2106.02874"/>
        <updated>2021-06-08T02:20:21.234Z</updated>
        <summary type="html"><![CDATA[Unsupervised domain adaptation (UDA) involves a supervised loss in a labeled
source domain and an unsupervised loss in an unlabeled target domain, which
often faces more severe overfitting (than classical supervised learning) as the
supervised source loss has clear domain gap and the unsupervised target loss is
often noisy due to the lack of annotations. This paper presents RDA, a robust
domain adaptation technique that introduces adversarial attacking to mitigate
overfitting in UDA. We achieve robust domain adaptation by a novel Fourier
adversarial attacking (FAA) method that allows large magnitude of perturbation
noises but has minimal modification of image semantics, the former is critical
to the effectiveness of its generated adversarial samples due to the existence
of 'domain gaps'. Specifically, FAA decomposes images into multiple frequency
components (FCs) and generates adversarial samples by just perturbating certain
FCs that capture little semantic information. With FAA-generated samples, the
training can continue the 'random walk' and drift into an area with a flat loss
landscape, leading to more robust domain adaptation. Extensive experiments over
multiple domain adaptation tasks show that RDA can work with different computer
vision tasks with superior performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1"&gt;Jiaxing Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guan_D/0/1/0/all/0/1"&gt;Dayan Guan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_A/0/1/0/all/0/1"&gt;Aoran Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1"&gt;Shijian Lu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multimodal Story Generation on Plural Images. (arXiv:2001.10980v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2001.10980</id>
        <link href="http://arxiv.org/abs/2001.10980"/>
        <updated>2021-06-08T02:20:21.199Z</updated>
        <summary type="html"><![CDATA[Traditionally, text generation models take in a sequence of text as input,
and iteratively generate the next most probable word using pre-trained
parameters. In this work, we propose the architecture to use images instead of
text as the input of the text generation model, called StoryGen. In the
architecture, we design a Relational Text Data Generator algorithm that relates
different features from multiple images. The output samples from the model
demonstrate the ability to generate meaningful paragraphs of text containing
the extracted features from the input images. This is an undergraduate project
report. Completed Dec. 2019 at the Cooper Union.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1"&gt;Jing Jiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Radar-Camera Pixel Depth Association for Depth Completion. (arXiv:2106.02778v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02778</id>
        <link href="http://arxiv.org/abs/2106.02778"/>
        <updated>2021-06-08T02:20:21.190Z</updated>
        <summary type="html"><![CDATA[While radar and video data can be readily fused at the detection level,
fusing them at the pixel level is potentially more beneficial. This is also
more challenging in part due to the sparsity of radar, but also because
automotive radar beams are much wider than a typical pixel combined with a
large baseline between camera and radar, which results in poor association
between radar pixels and color pixel. A consequence is that depth completion
methods designed for LiDAR and video fare poorly for radar and video. Here we
propose a radar-to-pixel association stage which learns a mapping from radar
returns to pixels. This mapping also serves to densify radar returns. Using
this as a first stage, followed by a more traditional depth completion method,
we are able to achieve image-guided depth completion with radar and video. We
demonstrate performance superior to camera and radar alone on the nuScenes
dataset. Our source code is available at https://github.com/longyunf/rc-pda.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Long_Y/0/1/0/all/0/1"&gt;Yunfei Long&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morris_D/0/1/0/all/0/1"&gt;Daniel Morris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiaoming Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Castro_M/0/1/0/all/0/1"&gt;Marcos Castro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chakravarty_P/0/1/0/all/0/1"&gt;Punarjay Chakravarty&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Narayanan_P/0/1/0/all/0/1"&gt;Praveen Narayanan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Zero-shot Task Adaptation using Natural Language. (arXiv:2106.02972v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2106.02972</id>
        <link href="http://arxiv.org/abs/2106.02972"/>
        <updated>2021-06-08T02:20:21.183Z</updated>
        <summary type="html"><![CDATA[Imitation learning and instruction-following are two common approaches to
communicate a user's intent to a learning agent. However, as the complexity of
tasks grows, it could be beneficial to use both demonstrations and language to
communicate with an agent. In this work, we propose a novel setting where an
agent is given both a demonstration and a description, and must combine
information from both the modalities. Specifically, given a demonstration for a
task (the source task), and a natural language description of the differences
between the demonstrated task and a related but different task (the target
task), our goal is to train an agent to complete the target task in a zero-shot
setting, that is, without any demonstrations for the target task. To this end,
we introduce Language-Aided Reward and Value Adaptation (LARVA) which, given a
source demonstration and a linguistic description of how the target task
differs, learns to output a reward / value function that accurately describes
the target task. Our experiments show that on a diverse set of adaptations, our
approach is able to complete more than 95% of target tasks when using
template-based descriptions, and more than 70% when using free-form natural
language.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Goyal_P/0/1/0/all/0/1"&gt;Prasoon Goyal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mooney_R/0/1/0/all/0/1"&gt;Raymond J. Mooney&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niekum_S/0/1/0/all/0/1"&gt;Scott Niekum&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semi-Supervised Domain Adaptation via Adaptive and Progressive Feature Alignment. (arXiv:2106.02845v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02845</id>
        <link href="http://arxiv.org/abs/2106.02845"/>
        <updated>2021-06-08T02:20:21.174Z</updated>
        <summary type="html"><![CDATA[Contemporary domain adaptive semantic segmentation aims to address data
annotation challenges by assuming that target domains are completely
unannotated. However, annotating a few target samples is usually very
manageable and worthwhile especially if it improves the adaptation performance
substantially. This paper presents SSDAS, a Semi-Supervised Domain Adaptive
image Segmentation network that employs a few labeled target samples as anchors
for adaptive and progressive feature alignment between labeled source samples
and unlabeled target samples. We position the few labeled target samples as
references that gauge the similarity between source and target features and
guide adaptive inter-domain alignment for learning more similar source
features. In addition, we replace the dissimilar source features by
high-confidence target features continuously during the iterative training
process, which achieves progressive intra-domain alignment between confident
and unconfident target features. Extensive experiments show the proposed SSDAS
greatly outperforms a number of baselines, i.e., UDA-based semantic
segmentation and SSDA-based image classification. In addition, SSDAS is
complementary and can be easily incorporated into UDA-based methods with
consistent improvements in domain adaptive semantic segmentation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1"&gt;Jiaxing Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guan_D/0/1/0/all/0/1"&gt;Dayan Guan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_A/0/1/0/all/0/1"&gt;Aoran Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1"&gt;Shijian Lu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Table2Charts: Recommending Charts by Learning Shared Table Representations. (arXiv:2008.11015v3 [cs.DB] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.11015</id>
        <link href="http://arxiv.org/abs/2008.11015"/>
        <updated>2021-06-08T02:20:21.152Z</updated>
        <summary type="html"><![CDATA[It is common for people to create different types of charts to explore a
multi-dimensional dataset (table). However, to recommend commonly composed
charts in real world, one should take the challenges of efficiency, imbalanced
data and table context into consideration. In this paper, we propose
Table2Charts framework which learns common patterns from a large corpus of
(table, charts) pairs. Based on deep Q-learning with copying mechanism and
heuristic searching, Table2Charts does table-to-sequence generation, where each
sequence follows a chart template. On a large spreadsheet corpus with 165k
tables and 266k charts, we show that Table2Charts could learn a shared
representation of table fields so that recommendation tasks on different chart
types could mutually enhance each other. Table2Charts outperforms other chart
recommendation systems in both multi-type task (with doubled recall numbers
R@3=0.61 and R@1=0.43) and human evaluations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1"&gt;Mengyu Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1"&gt;Qingtao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1"&gt;Xinyi He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yuejiang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yibo Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_W/0/1/0/all/0/1"&gt;Wei Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1"&gt;Shi Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yining Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1"&gt;Daxin Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1"&gt;Dongmei Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Code-Mixing on Sesame Street: Dawn of the Adversarial Polyglots. (arXiv:2103.09593v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.09593</id>
        <link href="http://arxiv.org/abs/2103.09593"/>
        <updated>2021-06-08T02:20:21.141Z</updated>
        <summary type="html"><![CDATA[Multilingual models have demonstrated impressive cross-lingual transfer
performance. However, test sets like XNLI are monolingual at the example level.
In multilingual communities, it is common for polyglots to code-mix when
conversing with each other. Inspired by this phenomenon, we present two strong
black-box adversarial attacks (one word-level, one phrase-level) for
multilingual models that push their ability to handle code-mixed sentences to
the limit. The former uses bilingual dictionaries to propose perturbations and
translations of the clean example for sense disambiguation. The latter directly
aligns the clean example with its translations before extracting phrases as
perturbations. Our phrase-level attack has a success rate of 89.75% against
XLM-R-large, bringing its average accuracy of 79.85 down to 8.18 on XNLI.
Finally, we propose an efficient adversarial training scheme that trains in the
same number of steps as the original model and show that it improves model
accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tan_S/0/1/0/all/0/1"&gt;Samson Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1"&gt;Shafiq Joty&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Making CNNs Interpretable by Building Dynamic Sequential Decision Forests with Top-down Hierarchy Learning. (arXiv:2106.02824v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02824</id>
        <link href="http://arxiv.org/abs/2106.02824"/>
        <updated>2021-06-08T02:20:21.130Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a generic model transfer scheme to make
Convlutional Neural Networks (CNNs) interpretable, while maintaining their high
classification accuracy. We achieve this by building a differentiable decision
forest on top of CNNs, which enjoys two characteristics: 1) During training,
the tree hierarchies of the forest are learned in a top-down manner under the
guidance from the category semantics embedded in the pre-trained CNN weights;
2) During inference, a single decision tree is dynamically selected from the
forest for each input sample, enabling the transferred model to make sequential
decisions corresponding to the attributes shared by semantically-similar
categories, rather than directly performing flat classification. We name the
transferred model deep Dynamic Sequential Decision Forest (dDSDF). Experimental
results show that dDSDF not only achieves higher classification accuracy than
its conuterpart, i.e., the original CNN, but has much better interpretability,
as qualitatively it has plausible hierarchies and quantitatively it leads to
more precise saliency maps.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yilin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1"&gt;Shaozuo Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xiaokang Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1"&gt;Wei Shen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Challenges in Information-Seeking QA: Unanswerable Questions and Paragraph Retrieval. (arXiv:2010.11915v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.11915</id>
        <link href="http://arxiv.org/abs/2010.11915"/>
        <updated>2021-06-08T02:20:21.119Z</updated>
        <summary type="html"><![CDATA[Recent pretrained language models "solved" many reading comprehension
benchmarks, where questions are written with access to the evidence document.
However, datasets containing information-seeking queries where evidence
documents are provided after the queries are written independently remain
challenging. We analyze why answering information-seeking queries is more
challenging and where their prevalent unanswerabilities arise, on Natural
Questions and TyDi QA. Our controlled experiments suggest two headrooms --
paragraph selection and answerability prediction, i.e. whether the paired
evidence document contains the answer to the query or not. When provided with a
gold paragraph and knowing when to abstain from answering, existing models
easily outperform a human annotator. However, predicting answerability itself
remains challenging. We manually annotate 800 unanswerable examples across six
languages on what makes them challenging to answer. With this new data, we
conduct per-category answerability prediction, revealing issues in the current
dataset collection as well as task formulation. Together, our study points to
avenues for future research in information-seeking question answering, both for
dataset creation and model development.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Asai_A/0/1/0/all/0/1"&gt;Akari Asai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_E/0/1/0/all/0/1"&gt;Eunsol Choi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MagicPai at SemEval-2021 Task 7: Method for Detecting and Rating Humor Based on Multi-Task Adversarial Training. (arXiv:2104.10336v2 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.10336</id>
        <link href="http://arxiv.org/abs/2104.10336"/>
        <updated>2021-06-08T02:20:21.109Z</updated>
        <summary type="html"><![CDATA[This paper describes MagicPai's system for SemEval 2021 Task 7, HaHackathon:
Detecting and Rating Humor and Offense. This task aims to detect whether the
text is humorous and how humorous it is. There are four subtasks in the
competition. In this paper, we mainly present our solution, a multi-task
learning model based on adversarial examples, for task 1a and 1b. More
specifically, we first vectorize the cleaned dataset and add the perturbation
to obtain more robust embedding representations. We then correct the loss via
the confidence level. Finally, we perform interactive joint learning on
multiple tasks to capture the relationship between whether the text is humorous
and how humorous it is. The final result shows the effectiveness of our system.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1"&gt;Jian Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1"&gt;Shuyi Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Haiqin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1"&gt;Lianxin Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1"&gt;Mengyuan Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ruan_X/0/1/0/all/0/1"&gt;Xiaoyi Ruan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mo_Y/0/1/0/all/0/1"&gt;Yang Mo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SketchGen: Generating Constrained CAD Sketches. (arXiv:2106.02711v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02711</id>
        <link href="http://arxiv.org/abs/2106.02711"/>
        <updated>2021-06-08T02:20:21.086Z</updated>
        <summary type="html"><![CDATA[Computer-aided design (CAD) is the most widely used modeling approach for
technical design. The typical starting point in these designs is 2D sketches
which can later be extruded and combined to obtain complex three-dimensional
assemblies. Such sketches are typically composed of parametric primitives, such
as points, lines, and circular arcs, augmented with geometric constraints
linking the primitives, such as coincidence, parallelism, or orthogonality.
Sketches can be represented as graphs, with the primitives as nodes and the
constraints as edges. Training a model to automatically generate CAD sketches
can enable several novel workflows, but is challenging due to the complexity of
the graphs and the heterogeneity of the primitives and constraints. In
particular, each type of primitive and constraint may require a record of
different size and parameter types. We propose SketchGen as a generative model
based on a transformer architecture to address the heterogeneity problem by
carefully designing a sequential language for the primitives and constraints
that allows distinguishing between different primitive or constraint types and
their parameters, while encouraging our model to re-use information across
related parameters, encoding shared structure. A particular highlight of our
work is the ability to produce primitives linked via constraints that enables
the final output to be further regularized via a constraint solver. We evaluate
our model by demonstrating constraint prediction for given sets of primitives
and full sketch generation from scratch, showing that our approach
significantly out performs the state-of-the-art in CAD sketch generation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Para_W/0/1/0/all/0/1"&gt;Wamiq Reyaz Para&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhat_S/0/1/0/all/0/1"&gt;Shariq Farooq Bhat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guerrero_P/0/1/0/all/0/1"&gt;Paul Guerrero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kelly_T/0/1/0/all/0/1"&gt;Tom Kelly&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mitra_N/0/1/0/all/0/1"&gt;Niloy Mitra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guibas_L/0/1/0/all/0/1"&gt;Leonidas Guibas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wonka_P/0/1/0/all/0/1"&gt;Peter Wonka&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Camera Vehicle Counting Using Edge-AI. (arXiv:2106.02842v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02842</id>
        <link href="http://arxiv.org/abs/2106.02842"/>
        <updated>2021-06-08T02:20:21.079Z</updated>
        <summary type="html"><![CDATA[This paper presents a novel solution to automatically count vehicles in a
parking lot using images captured by smart cameras. Unlike most of the
literature on this task, which focuses on the analysis of single images, this
paper proposes the use of multiple visual sources to monitor a wider parking
area from different perspectives. The proposed multi-camera system is capable
of automatically estimate the number of cars present in the entire parking lot
directly on board the edge devices. It comprises an on-device deep
learning-based detector that locates and counts the vehicles from the captured
images and a decentralized geometric-based approach that can analyze the
inter-camera shared areas and merge the data acquired by all the devices. We
conduct the experimental evaluation on an extended version of the CNRPark-EXT
dataset, a collection of images taken from the parking lot on the campus of the
National Research Council (CNR) in Pisa, Italy. We show that our system is
robust and takes advantage of the redundant information deriving from the
different cameras, improving the overall performance without requiring any
extra geometrical information of the monitored scene.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ciampi_L/0/1/0/all/0/1"&gt;Luca Ciampi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gennaro_C/0/1/0/all/0/1"&gt;Claudio Gennaro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carrara_F/0/1/0/all/0/1"&gt;Fabio Carrara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Falchi_F/0/1/0/all/0/1"&gt;Fabrizio Falchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vairo_C/0/1/0/all/0/1"&gt;Claudio Vairo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Amato_G/0/1/0/all/0/1"&gt;Giuseppe Amato&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DISCO: accurate Discrete Scale Convolutions. (arXiv:2106.02733v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02733</id>
        <link href="http://arxiv.org/abs/2106.02733"/>
        <updated>2021-06-08T02:20:21.069Z</updated>
        <summary type="html"><![CDATA[Scale is often seen as a given, disturbing factor in many vision tasks. When
doing so it is one of the factors why we need more data during learning. In
recent work scale equivariance was added to convolutional neural networks. It
was shown to be effective for a range of tasks. We aim for accurate
scale-equivariant convolutional neural networks (SE-CNNs) applicable for
problems where high granularity of scale and small filter sizes are required.
Current SE-CNNs rely on weight sharing and filter rescaling, the latter of
which is accurate for integer scales only. To reach accurate scale
equivariance, we derive general constraints under which scale-convolution
remains equivariant to discrete rescaling. We find the exact solution for all
cases where it exists, and compute the approximation for the rest. The discrete
scale-convolution pays off, as demonstrated in a new state-of-the-art
classification on MNIST-scale and improving the results on STL-10. With the
same SE scheme, we also improve the computational effort of a scale-equivariant
Siamese tracker on OTB-13.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sosnovik_I/0/1/0/all/0/1"&gt;Ivan Sosnovik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moskalev_A/0/1/0/all/0/1"&gt;Artem Moskalev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smeulders_A/0/1/0/all/0/1"&gt;Arnold Smeulders&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-hop Question Answering via Reasoning Chains. (arXiv:1910.02610v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1910.02610</id>
        <link href="http://arxiv.org/abs/1910.02610"/>
        <updated>2021-06-08T02:20:21.060Z</updated>
        <summary type="html"><![CDATA[Multi-hop question answering requires models to gather information from
different parts of a text to answer a question. Most current approaches learn
to address this task in an end-to-end way with neural networks, without
maintaining an explicit representation of the reasoning process. We propose a
method to extract a discrete reasoning chain over the text, which consists of a
series of sentences leading to the answer. We then feed the extracted chains to
a BERT-based QA model to do final answer prediction. Critically, we do not rely
on gold annotated chains or "supporting facts:" at training time, we derive
pseudogold reasoning chains using heuristics based on named entity recognition
and coreference resolution. Nor do we rely on these annotations at test time,
as our model learns to extract chains from raw text alone. We test our approach
on two recently proposed large multi-hop question answering datasets: WikiHop
and HotpotQA, and achieve state-of-art performance on WikiHop and strong
performance on HotpotQA. Our analysis shows the properties of chains that are
crucial for high performance: in particular, modeling extraction sequentially
is important, as is dealing with each candidate sentence in a context-aware
way. Furthermore, human evaluation shows that our extracted chains allow humans
to give answers with high confidence, indicating that these are a strong
intermediate abstraction for this task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jifan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1"&gt;Shih-ting Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Durrett_G/0/1/0/all/0/1"&gt;Greg Durrett&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GLSD: The Global Large-Scale Ship Database and Baseline Evaluations. (arXiv:2106.02773v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02773</id>
        <link href="http://arxiv.org/abs/2106.02773"/>
        <updated>2021-06-08T02:20:21.051Z</updated>
        <summary type="html"><![CDATA[In this paper, we introduce a challenging global large-scale ship database
(called GLSD), designed specifically for ship detection tasks. The designed
GLSD database includes a total of 140,616 annotated instances from 100,729
images. Based on the collected images, we propose 13 categories that widely
exists in international routes. These categories include sailing boat, fishing
boat, passenger ship, war ship, general cargo ship, container ship, bulk cargo
carrier, barge, ore carrier, speed boat, canoe, oil carrier, and tug. The
motivations of developing GLSD include the following: 1) providing a refined
ship detection database; 2) providing the worldwide researchers of ship
detection and exhaustive label information (bounding box and ship class label)
in one uniform global database; and 3) providing a large-scale ship database
with geographic information (port and country information) that benefits
multi-modal analysis. In addition, we discuss the evaluation protocols given
image characteristics in GLSD and analyze the performance of selected
state-of-the-art object detection algorithms on GSLD, providing baselines for
future studies. More information regarding the designed GLSD can be found at
https://github.com/jiaming-wang/GLSD.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shao_Z/0/1/0/all/0/1"&gt;Zhenfeng Shao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jiaming Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_L/0/1/0/all/0/1"&gt;Lianbing Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1"&gt;Xiao Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1"&gt;Tao Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Ruiqian Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lv_X/0/1/0/all/0/1"&gt;Xianwei Lv&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_Q/0/1/0/all/0/1"&gt;Qing Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhiqiang Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Step Inference for Reasoning Over Paragraphs. (arXiv:2004.02995v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.02995</id>
        <link href="http://arxiv.org/abs/2004.02995"/>
        <updated>2021-06-08T02:20:21.026Z</updated>
        <summary type="html"><![CDATA[Complex reasoning over text requires understanding and chaining together
free-form predicates and logical connectives. Prior work has largely tried to
do this either symbolically or with black-box transformers. We present a middle
ground between these two extremes: a compositional model reminiscent of neural
module networks that can perform chained logical reasoning. This model first
finds relevant sentences in the context and then chains them together using
neural modules. Our model gives significant performance improvements (up to
29\% relative error reduction when comfibined with a reranker) on ROPES, a
recently introduced complex reasoning dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jiangming Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gardner_M/0/1/0/all/0/1"&gt;Matt Gardner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cohen_S/0/1/0/all/0/1"&gt;Shay B. Cohen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lapata_M/0/1/0/all/0/1"&gt;Mirella Lapata&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GO FIGURE: A Meta Evaluation of Factuality in Summarization. (arXiv:2010.12834v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.12834</id>
        <link href="http://arxiv.org/abs/2010.12834"/>
        <updated>2021-06-08T02:20:21.017Z</updated>
        <summary type="html"><![CDATA[While neural language models can generate text with remarkable fluency and
coherence, controlling for factual correctness in generation remains an open
research question. This major discrepancy between the surface-level fluency and
the content-level correctness of neural generation has motivated a new line of
research that seeks automatic metrics for evaluating the factuality of machine
text. In this paper, we introduce GO FIGURE, a meta-evaluation framework for
evaluating factuality evaluation metrics. We propose five necessary and
intuitive conditions to evaluate factuality metrics on diagnostic factuality
data across three different summarization tasks. Our benchmark analysis on ten
factuality metrics reveals that our meta-evaluation framework provides a robust
and efficient evaluation that is extensible to multiple types of factual
consistency and standard generation metrics, including QA metrics. It also
reveals that while QA metrics generally improve over standard metrics that
measure factuality across domains, performance is highly dependent on the way
in which questions are generated.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gabriel_S/0/1/0/all/0/1"&gt;Saadia Gabriel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Celikyilmaz_A/0/1/0/all/0/1"&gt;Asli Celikyilmaz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jha_R/0/1/0/all/0/1"&gt;Rahul Jha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1"&gt;Yejin Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1"&gt;Jianfeng Gao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lookup-Table Recurrent Language Models for Long Tail Speech Recognition. (arXiv:2104.04552v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.04552</id>
        <link href="http://arxiv.org/abs/2104.04552"/>
        <updated>2021-06-08T02:20:21.004Z</updated>
        <summary type="html"><![CDATA[We introduce Lookup-Table Language Models (LookupLM), a method for scaling up
the size of RNN language models with only a constant increase in the floating
point operations, by increasing the expressivity of the embedding table. In
particular, we instantiate an (additional) embedding table which embeds the
previous n-gram token sequence, rather than a single token. This allows the
embedding table to be scaled up arbitrarily -- with a commensurate increase in
performance -- without changing the token vocabulary. Since embeddings are
sparsely retrieved from the table via a lookup; increasing the size of the
table adds neither extra operations to each forward pass nor extra parameters
that need to be stored on limited GPU/TPU memory. We explore scaling n-gram
embedding tables up to nearly a billion parameters. When trained on a 3-billion
sentence corpus, we find that LookupLM improves long tail log perplexity by
2.44 and long tail WER by 23.4% on a downstream speech recognition task over a
standard RNN language model baseline, an improvement comparable to a scaling up
the baseline by 6.2x the number of floating point operations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1"&gt;W. Ronny Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sainath_T/0/1/0/all/0/1"&gt;Tara N. Sainath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peyser_C/0/1/0/all/0/1"&gt;Cal Peyser&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1"&gt;Shankar Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rybach_D/0/1/0/all/0/1"&gt;David Rybach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Strohman_T/0/1/0/all/0/1"&gt;Trevor Strohman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Better Robustness by More Coverage: Adversarial Training with Mixup Augmentation for Robust Fine-tuning. (arXiv:2012.15699v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.15699</id>
        <link href="http://arxiv.org/abs/2012.15699"/>
        <updated>2021-06-08T02:20:20.995Z</updated>
        <summary type="html"><![CDATA[Pretrained language models (PLMs) perform poorly under adversarial attacks.
To improve the adversarial robustness, adversarial data augmentation (ADA) has
been widely adopted to cover more search space of adversarial attacks by adding
textual adversarial examples during training. However, the number of
adversarial examples for text augmentation is still extremely insufficient due
to the exponentially large attack search space. In this work, we propose a
simple and effective method to cover a much larger proportion of the attack
search space, called Adversarial and Mixup Data Augmentation (AMDA).
Specifically, AMDA linearly interpolates the representations of pairs of
training samples to form new virtual samples, which are more abundant and
diverse than the discrete text adversarial examples in conventional ADA.
Moreover, to fairly evaluate the robustness of different models, we adopt a
challenging evaluation setup, which generates a new set of adversarial examples
targeting each model. In text classification experiments of BERT and RoBERTa,
AMDA achieves significant robustness gains under two strong adversarial attacks
and alleviates the performance degradation of ADA on the clean data. Our code
is available at: https://github.com/thunlp/MixADA .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Si_C/0/1/0/all/0/1"&gt;Chenglei Si&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhengyan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_F/0/1/0/all/0/1"&gt;Fanchao Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhiyuan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yasheng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1"&gt;Qun Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1"&gt;Maosong Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Coarse-to-Fine Entity Representations for Document-level Relation Extraction. (arXiv:2012.02507v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.02507</id>
        <link href="http://arxiv.org/abs/2012.02507"/>
        <updated>2021-06-08T02:20:20.962Z</updated>
        <summary type="html"><![CDATA[Document-level Relation Extraction (RE) requires extracting relations
expressed within and across sentences. Recent works show that graph-based
methods, usually constructing a document-level graph that captures
document-aware interactions, can obtain useful entity representations thus
helping tackle document-level RE. These methods either focus more on the entire
graph, or pay more attention to a part of the graph, e.g., paths between the
target entity pair. However, we find that document-level RE may benefit from
focusing on both of them simultaneously. Therefore, to obtain more
comprehensive entity representations, we propose the Coarse-to-Fine Entity
Representation model (CFER) that adopts a coarse-to-fine strategy involving two
phases. First, CFER uses graph neural networks to integrate global information
in the entire graph at a coarse level. Next, CFER utilizes the global
information as a guidance to selectively aggregate path information between the
target entity pair at a fine level. In classification, we combine the entity
representations from both two levels into more comprehensive representations
for relation extraction. Experimental results on two document-level RE
datasets, DocRED and CDR, show that CFER outperforms existing models and is
robust to the uneven label distribution.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1"&gt;Damai Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1"&gt;Jing Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_S/0/1/0/all/0/1"&gt;Shuang Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_B/0/1/0/all/0/1"&gt;Baobao Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sui_Z/0/1/0/all/0/1"&gt;Zhifang Sui&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Global-aware Beam Search for Neural Abstractive Summarization. (arXiv:2009.06891v4 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.06891</id>
        <link href="http://arxiv.org/abs/2009.06891"/>
        <updated>2021-06-08T02:20:20.879Z</updated>
        <summary type="html"><![CDATA[This study develops a calibrated beam-based algorithm with global awareness
for neural abstractive summarization, aiming to improve the local optimality
problem of the original beam search in a rigorous way. Specifically, a novel
global protocol is proposed based on the attention distribution to stipulate
how a global optimal hypothesis should attend to the source. A global scoring
function is then developed to regulate beam search to generate summaries in a
more near-global optimal fashion. This novel design enjoys a distinctive
property, i.e. the global attention distribution could be predicted before
inference, enabling stepwise improvements on the beam search through the global
scoring function. Extensive experiments on $9$ datasets show that the
global-aware inference significantly improves state-of-the-art summarization
models even using empirical hyper-parameters. The algorithm is also proven
robust as it remains to generate meaningful texts with corrupted attention
distributions. The codes and a comprehensive set of examples are available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1"&gt;Ye Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lan_Z/0/1/0/all/0/1"&gt;Zixun Lan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zong_L/0/1/0/all/0/1"&gt;Lu Zong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1"&gt;Kaizhu Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Denoising Word Embeddings by Averaging in a Shared Space. (arXiv:2106.02954v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02954</id>
        <link href="http://arxiv.org/abs/2106.02954"/>
        <updated>2021-06-08T02:20:20.848Z</updated>
        <summary type="html"><![CDATA[We introduce a new approach for smoothing and improving the quality of word
embeddings. We consider a method of fusing word embeddings that were trained on
the same corpus but with different initializations. We project all the models
to a shared vector space using an efficient implementation of the Generalized
Procrustes Analysis (GPA) procedure, previously used in multilingual word
translation. Our word representation demonstrates consistent improvements over
the raw models as well as their simplistic average, on a range of tasks. As the
new representations are more stable and reliable, there is a noticeable
improvement in rare word evaluations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Caciularu_A/0/1/0/all/0/1"&gt;Avi Caciularu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dagan_I/0/1/0/all/0/1"&gt;Ido Dagan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldberger_J/0/1/0/all/0/1"&gt;Jacob Goldberger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Weakly-Supervised Methods for Suicide Risk Assessment: Role of Related Domains. (arXiv:2106.02792v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02792</id>
        <link href="http://arxiv.org/abs/2106.02792"/>
        <updated>2021-06-08T02:20:20.839Z</updated>
        <summary type="html"><![CDATA[Social media has become a valuable resource for the study of suicidal
ideation and the assessment of suicide risk. Among social media platforms,
Reddit has emerged as the most promising one due to its anonymity and its focus
on topic-based communities (subreddits) that can be indicative of someone's
state of mind or interest regarding mental health disorders such as
r/SuicideWatch, r/Anxiety, r/depression. A challenge for previous work on
suicide risk assessment has been the small amount of labeled data. We propose
an empirical investigation into several classes of weakly-supervised
approaches, and show that using pseudo-labeling based on related issues around
mental health (e.g., anxiety, depression) helps improve model performance for
suicide risk assessment.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1"&gt;Chenghao Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yudong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muresan_S/0/1/0/all/0/1"&gt;Smaranda Muresan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lifelong Learning of Hate Speech Classification on Social Media. (arXiv:2106.02821v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02821</id>
        <link href="http://arxiv.org/abs/2106.02821"/>
        <updated>2021-06-08T02:20:20.826Z</updated>
        <summary type="html"><![CDATA[Existing work on automated hate speech classification assumes that the
dataset is fixed and the classes are pre-defined. However, the amount of data
in social media increases every day, and the hot topics changes rapidly,
requiring the classifiers to be able to continuously adapt to new data without
forgetting the previously learned knowledge. This ability, referred to as
lifelong learning, is crucial for the real-word application of hate speech
classifiers in social media. In this work, we propose lifelong learning of hate
speech classification on social media. To alleviate catastrophic forgetting, we
propose to use Variational Representation Learning (VRL) along with a memory
module based on LB-SOINN (Load-Balancing Self-Organizing Incremental Neural
Network). Experimentally, we show that combining variational representation
learning and the LB-SOINN memory module achieves better performance than the
commonly-used lifelong learning techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qian_J/0/1/0/all/0/1"&gt;Jing Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+ElSherief_M/0/1/0/all/0/1"&gt;Mai ElSherief&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1"&gt;Xifeng Yan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A novel method for recommendation systems using invasive weed optimization. (arXiv:2106.02831v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.02831</id>
        <link href="http://arxiv.org/abs/2106.02831"/>
        <updated>2021-06-08T02:20:20.805Z</updated>
        <summary type="html"><![CDATA[One of the popular approaches in recommendation systems is Collaborative
Filtering (CF). The most significant step in CF is choosing the appropriate set
of users. For this purpose, similarity measures are usually used for computing
the similarity between a specific user and the other users. This paper proposes
a new invasive weed optimization (IWO) based CF approach that uses users'
context to identify important and effective users set. By using a newly defined
similarity measure based on both rating values and a measure values called
confidence, the proposed approach calculates the similarity between users and
thus identifies and filters the most similar users to a specific user. It then
uses IWO to calculate the importance degree of users and finally, by using the
identified important users and their importance degrees it predicts unknown
ratings. To evaluate the proposed method, several experiments have been
performed on two known real world datasets and the results show that the
proposed method improves the state of the art results up to 15% in terms of
Root Mean Square Error (RMSE) and Mean Absolute Error (MAE).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Soltaninejad_F/0/1/0/all/0/1"&gt;Fahimeh Soltaninejad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bidgoly_A/0/1/0/all/0/1"&gt;Amir Jalaly Bidgoly&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MergeDistill: Merging Pre-trained Language Models using Distillation. (arXiv:2106.02834v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02834</id>
        <link href="http://arxiv.org/abs/2106.02834"/>
        <updated>2021-06-08T02:20:20.787Z</updated>
        <summary type="html"><![CDATA[Pre-trained multilingual language models (LMs) have achieved state-of-the-art
results in cross-lingual transfer, but they often lead to an inequitable
representation of languages due to limited capacity, skewed pre-training data,
and sub-optimal vocabularies. This has prompted the creation of an ever-growing
pre-trained model universe, where each model is trained on large amounts of
language or domain specific data with a carefully curated, linguistically
informed vocabulary. However, doing so brings us back full circle and prevents
one from leveraging the benefits of multilinguality. To address the gaps at
both ends of the spectrum, we propose MergeDistill, a framework to merge
pre-trained LMs in a way that can best leverage their assets with minimal
dependencies, using task-agnostic knowledge distillation. We demonstrate the
applicability of our framework in a practical setting by leveraging
pre-existing teacher LMs and training student LMs that perform competitively
with or even outperform teacher LMs trained on several orders of magnitude more
data and with a fixed model capacity. We also highlight the importance of
teacher selection and its impact on student model performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khanuja_S/0/1/0/all/0/1"&gt;Simran Khanuja&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Johnson_M/0/1/0/all/0/1"&gt;Melvin Johnson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Talukdar_P/0/1/0/all/0/1"&gt;Partha Talukdar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MultiOpEd: A Corpus of Multi-Perspective News Editorials. (arXiv:2106.02725v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02725</id>
        <link href="http://arxiv.org/abs/2106.02725"/>
        <updated>2021-06-08T02:20:20.776Z</updated>
        <summary type="html"><![CDATA[We propose MultiOpEd, an open-domain news editorial corpus that supports
various tasks pertaining to the argumentation structure in news editorials,
focusing on automatic perspective discovery. News editorial is a genre of
persuasive text, where the argumentation structure is usually implicit.
However, the arguments presented in an editorial typically center around a
concise, focused thesis, which we refer to as their perspective. MultiOpEd aims
at supporting the study of multiple tasks relevant to automatic perspective
discovery, where a system is expected to produce a single-sentence thesis
statement summarizing the arguments presented. We argue that identifying and
abstracting such natural language perspectives from editorials is a crucial
step toward studying the implicit argumentation structure in news editorials.
We first discuss the challenges and define a few conceptual tasks towards our
goal. To demonstrate the utility of MultiOpEd and the induced tasks, we study
the problem of perspective summarization in a multi-task learning setting, as a
case study. We show that, with the induced tasks as auxiliary tasks, we can
improve the quality of the perspective summary generated. We hope that
MultiOpEd will be a useful resource for future studies on argumentation in the
news editorial domain.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Siyi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Sihao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Uyttendaele_X/0/1/0/all/0/1"&gt;Xander Uyttendaele&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roth_D/0/1/0/all/0/1"&gt;Dan Roth&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exposing the Implicit Energy Networks behind Masked Language Models via Metropolis--Hastings. (arXiv:2106.02736v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02736</id>
        <link href="http://arxiv.org/abs/2106.02736"/>
        <updated>2021-06-08T02:20:20.767Z</updated>
        <summary type="html"><![CDATA[While recent work has shown that scores from models trained by the ubiquitous
masked language modeling (MLM) objective effectively discriminate probable and
improbable sequences, it is still an open question if these MLMs specify a
principled probability distribution over the space of possible sequences. In
this paper, we interpret MLMs as energy-based sequence models and propose two
energy parametrizations derivable from the trained MLMs. In order to draw
samples correctly from these models, we develop a tractable \emph{sampling}
scheme based on the Metropolis--Hastings Monte Carlo algorithm. In our
approach, samples are proposed from the same masked conditionals used for
training the masked language models, and they are accepted or rejected based on
their energy values according to the target distribution. We validate the
effectiveness of the proposed parametrizations by exploring the quality of
samples drawn from these energy-based models on the conditional generation task
of machine translation. We theoretically and empirically justify our sampling
algorithm by showing that the masked conditionals on their own do not yield a
Markov chain whose stationary distribution is that of our target distribution,
and our approach generates higher quality samples than other recently proposed
undirected generation approaches (Wang et al., 2019, Ghazvininejad et al.,
2019).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Goyal_K/0/1/0/all/0/1"&gt;Kartik Goyal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dyer_C/0/1/0/all/0/1"&gt;Chris Dyer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berg_Kirkpatrick_T/0/1/0/all/0/1"&gt;Taylor Berg-Kirkpatrick&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PURS: Personalized Unexpected Recommender System for Improving User Satisfaction. (arXiv:2106.02771v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.02771</id>
        <link href="http://arxiv.org/abs/2106.02771"/>
        <updated>2021-06-08T02:20:20.759Z</updated>
        <summary type="html"><![CDATA[Classical recommender system methods typically face the filter bubble problem
when users only receive recommendations of their familiar items, making them
bored and dissatisfied. To address the filter bubble problem, unexpected
recommendations have been proposed to recommend items significantly deviating
from user's prior expectations and thus surprising them by presenting "fresh"
and previously unexplored items to the users. In this paper, we describe a
novel Personalized Unexpected Recommender System (PURS) model that incorporates
unexpectedness into the recommendation process by providing multi-cluster
modeling of user interests in the latent space and personalized unexpectedness
via the self-attention mechanism and via selection of an appropriate unexpected
activation function. Extensive offline experiments on three real-world datasets
illustrate that the proposed PURS model significantly outperforms the
state-of-the-art baseline approaches in terms of both accuracy and
unexpectedness measures. In addition, we conduct an online A/B test at a major
video platform Alibaba-Youku, where our model achieves over 3\% increase in the
average video view per user metric. The proposed model is in the process of
being deployed by the company.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1"&gt;Pan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Que_M/0/1/0/all/0/1"&gt;Maofei Que&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1"&gt;Zhichao Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1"&gt;Yao Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tuzhilin_A/0/1/0/all/0/1"&gt;Alexander Tuzhilin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Emergent Communication of Generalizations. (arXiv:2106.02668v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02668</id>
        <link href="http://arxiv.org/abs/2106.02668"/>
        <updated>2021-06-08T02:20:20.733Z</updated>
        <summary type="html"><![CDATA[To build agents that can collaborate effectively with others, recent research
has trained artificial agents to communicate with each other in Lewis-style
referential games. However, this often leads to successful but uninterpretable
communication. We argue that this is due to the game objective: communicating
about a single object in a shared visual context is prone to overfitting and
does not encourage language useful beyond concrete reference. In contrast,
human language conveys a rich variety of abstract ideas. To promote such
skills, we propose games that require communicating generalizations over sets
of objects representing abstract visual concepts, optionally with separate
contexts for each agent. We find that these games greatly improve systematicity
and interpretability of the learned languages, according to several metrics in
the literature. Finally, we propose a method for identifying logical operations
embedded in the emergent languages by learning an approximate compositional
reconstruction of the language.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mu_J/0/1/0/all/0/1"&gt;Jesse Mu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goodman_N/0/1/0/all/0/1"&gt;Noah Goodman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Layered gradient accumulation and modular pipeline parallelism: fast and efficient training of large language models. (arXiv:2106.02679v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02679</id>
        <link href="http://arxiv.org/abs/2106.02679"/>
        <updated>2021-06-08T02:20:20.719Z</updated>
        <summary type="html"><![CDATA[The advent of the transformer has sparked a quick growth in the size of
language models, far outpacing hardware improvements. (Dense) transformers are
expected to reach the trillion-parameter scale in the near future, for which
training requires thousands or even tens of thousands of GPUs. We investigate
the challenges of training at this scale and beyond on commercially available
hardware. In particular, we analyse the shortest possible training time for
different configurations of distributed training, leveraging empirical scaling
laws for language models to estimate the optimal (critical) batch size.
Contrary to popular belief, we find no evidence for a memory wall, and instead
argue that the real limitation -- other than the cost -- lies in the training
duration.

In addition to this analysis, we introduce two new methods, \textit{layered
gradient accumulation} and \textit{modular pipeline parallelism}, which
together cut the shortest training time by half. The methods also reduce data
movement, lowering the network requirement to a point where a fast InfiniBand
connection is not necessary. This increased network efficiency also improve on
the methods introduced with the ZeRO optimizer, reducing the memory usage to a
tiny fraction of the available GPU memory.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lamy_Poirier_J/0/1/0/all/0/1"&gt;Joel Lamy-Poirier&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ensemble Learning Based Classification Algorithm Recommendation. (arXiv:2101.05993v1 [cs.IR] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2101.05993</id>
        <link href="http://arxiv.org/abs/2101.05993"/>
        <updated>2021-06-08T02:20:20.580Z</updated>
        <summary type="html"><![CDATA[Recommending appropriate algorithms to a classification problem is one of the
most challenging issues in the field of data mining. The existing algorithm
recommendation models are generally constructed on only one kind of
meta-features by single learners. Considering that i) ensemble learners usually
show better performance and ii) different kinds of meta-features characterize
the classification problems in different viewpoints independently, and further
the models constructed with different sets of meta-features will be
complementary with each other and applicable for ensemble. This paper proposes
an ensemble learning-based algorithm recommendation method. To evaluate the
proposed recommendation method, extensive experiments with 13 well-known
candidate classification algorithms and five different kinds of meta-features
are conducted on 1090 benchmark classification problems. The results show the
effectiveness of the proposed ensemble learning based recommendation method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1"&gt;Guangtao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_Q/0/1/0/all/0/1"&gt;Qinbao Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1"&gt;Xiaoyan Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bidirectional Distillation for Top-K Recommender System. (arXiv:2106.02870v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.02870</id>
        <link href="http://arxiv.org/abs/2106.02870"/>
        <updated>2021-06-08T02:20:20.569Z</updated>
        <summary type="html"><![CDATA[Recommender systems (RS) have started to employ knowledge distillation, which
is a model compression technique training a compact model (student) with the
knowledge transferred from a cumbersome model (teacher). The state-of-the-art
methods rely on unidirectional distillation transferring the knowledge only
from the teacher to the student, with an underlying assumption that the teacher
is always superior to the student. However, we demonstrate that the student
performs better than the teacher on a significant proportion of the test set,
especially for RS. Based on this observation, we propose Bidirectional
Distillation (BD) framework whereby both the teacher and the student
collaboratively improve with each other. Specifically, each model is trained
with the distillation loss that makes to follow the other's prediction along
with its original loss function. For effective bidirectional distillation, we
propose rank discrepancy-aware sampling scheme to distill only the informative
knowledge that can fully enhance each other. The proposed scheme is designed to
effectively cope with a large performance gap between the teacher and the
student. Trained in the bidirectional way, it turns out that both the teacher
and the student are significantly improved compared to when being trained
separately. Our extensive experiments on real-world datasets show that our
proposed framework consistently outperforms the state-of-the-art competitors.
We also provide analyses for an in-depth understanding of BD and ablation
studies to verify the effectiveness of each proposed component.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kweon_W/0/1/0/all/0/1"&gt;Wonbin Kweon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kang_S/0/1/0/all/0/1"&gt;SeongKu Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1"&gt;Hwanjo Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dual Attentive Sequential Learning for Cross-Domain Click-Through Rate Prediction. (arXiv:2106.02768v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.02768</id>
        <link href="http://arxiv.org/abs/2106.02768"/>
        <updated>2021-06-08T02:20:20.522Z</updated>
        <summary type="html"><![CDATA[Cross domain recommender system constitutes a powerful method to tackle the
cold-start and sparsity problem by aggregating and transferring user
preferences across multiple category domains. Therefore, it has great potential
to improve click-through-rate prediction performance in online commerce
platforms having many domains of products. While several cross domain
sequential recommendation models have been proposed to leverage information
from a source domain to improve CTR predictions in a target domain, they did
not take into account bidirectional latent relations of user preferences across
source-target domain pairs. As such, they cannot provide enhanced cross-domain
CTR predictions for both domains simultaneously. In this paper, we propose a
novel approach to cross-domain sequential recommendations based on the dual
learning mechanism that simultaneously transfers information between two
related domains in an iterative manner until the learning process stabilizes.
In particular, the proposed Dual Attentive Sequential Learning (DASL) model
consists of two novel components Dual Embedding and Dual Attention, which
jointly establish the two-stage learning process: we first construct dual
latent embeddings that extract user preferences in both domains simultaneously,
and subsequently provide cross-domain recommendations by matching the extracted
latent embeddings with candidate items through dual-attention learning
mechanism. We conduct extensive offline experiments on three real-world
datasets to demonstrate the superiority of our proposed model, which
significantly and consistently outperforms several state-of-the-art baselines
across all experimental settings. We also conduct an online A/B test at a major
video streaming platform Alibaba-Youku, where our proposed model significantly
improves business performance over the latest production system in the company.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1"&gt;Pan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1"&gt;Zhichao Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Que_M/0/1/0/all/0/1"&gt;Maofei Que&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1"&gt;Yao Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tuzhilin_A/0/1/0/all/0/1"&gt;Alexander Tuzhilin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mutual Information Regularized Identity-aware Facial ExpressionRecognition in Compressed Video. (arXiv:2010.10637v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.10637</id>
        <link href="http://arxiv.org/abs/2010.10637"/>
        <updated>2021-06-08T02:20:20.501Z</updated>
        <summary type="html"><![CDATA[How to extract effective expression representations that invariant to the
identity-specific attributes is a long-lasting problem for facial expression
recognition (FER). Most of the previous methods process the RGB images of a
sequence, while we argue that the off-the-shelf and valuable expression-related
muscle movement is already embedded in the compression format. In this paper,
we target to explore the inter-subject variations eliminated facial expression
representation in the compressed video domain. In the up to two orders of
magnitude compressed domain, we can explicitly infer the expression from the
residual frames and possibly extract identity factors from the I frame with a
pre-trained face recognition network. By enforcing the marginal independence of
them, the expression feature is expected to be purer for the expression and be
robust to identity shifts. Specifically, we propose a novel collaborative
min-min game for mutual information (MI) minimization in latent space. We do
not need the identity label or multiple expression samples from the same person
for identity elimination. Moreover, when the apex frame is annotated in the
dataset, the complementary constraint can be further added to regularize the
feature-level game. In testing, only the compressed residual frames are
required to achieve expression prediction. Our solution can achieve comparable
or better performance than the recent decoded image-based methods on the
typical FER benchmarks with about 3 times faster inference.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiaofeng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1"&gt;Linghao Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1"&gt;Xu Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+You_J/0/1/0/all/0/1"&gt;Jane You&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Auditing Source Diversity Bias in Video Search Results Using Virtual Agents. (arXiv:2106.02715v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.02715</id>
        <link href="http://arxiv.org/abs/2106.02715"/>
        <updated>2021-06-08T02:20:20.489Z</updated>
        <summary type="html"><![CDATA[We audit the presence of domain-level source diversity bias in video search
results. Using a virtual agent-based approach, we compare outputs of four
Western and one non-Western search engines for English and Russian queries. Our
findings highlight that source diversity varies substantially depending on the
language with English queries returning more diverse outputs. We also find
disproportionately high presence of a single platform, YouTube, in top search
outputs for all Western search engines except Google. At the same time, we
observe that Youtube's major competitors such as Vimeo or Dailymotion do not
appear in the sampled Google's video search results. This finding suggests that
Google might be downgrading the results from the main competitors of
Google-owned Youtube and highlights the necessity for further studies focusing
on the presence of own-content bias in Google's search results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Urman_A/0/1/0/all/0/1"&gt;Aleksandra Urman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Makhortykh_M/0/1/0/all/0/1"&gt;Mykola Makhortykh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ulloa_R/0/1/0/all/0/1"&gt;Roberto Ulloa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Model-Agnostic Counterfactual Reasoning for Eliminating Popularity Bias in Recommender System. (arXiv:2010.15363v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.15363</id>
        <link href="http://arxiv.org/abs/2010.15363"/>
        <updated>2021-06-08T02:20:20.471Z</updated>
        <summary type="html"><![CDATA[The general aim of the recommender system is to provide personalized
suggestions to users, which is opposed to suggesting popular items. However,
the normal training paradigm, i.e., fitting a recommender model to recover the
user behavior data with pointwise or pairwise loss, makes the model biased
towards popular items. This results in the terrible Matthew effect, making
popular items be more frequently recommended and become even more popular.
Existing work addresses this issue with Inverse Propensity Weighting (IPW),
which decreases the impact of popular items on the training and increases the
impact of long-tail items. Although theoretically sound, IPW methods are highly
sensitive to the weighting strategy, which is notoriously difficult to tune. In
this work, we explore the popularity bias issue from a novel and fundamental
perspective -- cause-effect. We identify that popularity bias lies in the
direct effect from the item node to the ranking score, such that an item's
intrinsic property is the cause of mistakenly assigning it a higher ranking
score. To eliminate popularity bias, it is essential to answer the
counterfactual question that what the ranking score would be if the model only
uses item property. To this end, we formulate a causal graph to describe the
important cause-effect relations in the recommendation process. During
training, we perform multi-task learning to achieve the contribution of each
cause; during testing, we perform counterfactual inference to remove the effect
of item popularity. Remarkably, our solution amends the learning process of
recommendation which is agnostic to a wide range of models -- it can be easily
implemented in existing methods. We demonstrate it on Matrix Factorization (MF)
and LightGCN [20]. Experiments on five real-world datasets demonstrate the
effectiveness of our method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wei_T/0/1/0/all/0/1"&gt;Tianxin Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_F/0/1/0/all/0/1"&gt;Fuli Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jiawei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Ziwei Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yi_J/0/1/0/all/0/1"&gt;Jinfeng Yi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1"&gt;Xiangnan He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MC2G: An Efficient Algorithm for Matrix Completion with Social and Item Similarity Graphs. (arXiv:2006.04373v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.04373</id>
        <link href="http://arxiv.org/abs/2006.04373"/>
        <updated>2021-06-08T02:20:20.446Z</updated>
        <summary type="html"><![CDATA[In this paper, we design and analyze MC2G (Matrix Completion with 2 Graphs),
an algorithm that performs matrix completion in the presence of social and item
similarity graphs. MC2G runs in quasilinear time and is parameter free. It is
based on spectral clustering and local refinement steps. The expected number of
sampled entries required for MC2G to succeed (i.e., recover the clusters in the
graphs and complete the matrix) matches an information-theoretic lower bound up
to a constant factor for a wide range of parameters. We show via extensive
experiments on both synthetic and real datasets that MC2G outperforms other
state-of-the-art matrix completion algorithms that leverage graph side
information.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Qiaosheng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suh_G/0/1/0/all/0/1"&gt;Geewon Suh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suh_C/0/1/0/all/0/1"&gt;Changho Suh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_V/0/1/0/all/0/1"&gt;Vincent Y. F. Tan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Embed Categorical Features without Embedding Tables for Recommendation. (arXiv:2010.10784v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.10784</id>
        <link href="http://arxiv.org/abs/2010.10784"/>
        <updated>2021-06-08T02:20:20.433Z</updated>
        <summary type="html"><![CDATA[Embedding learning of categorical features (e.g. user/item IDs) is at the
core of various recommendation models including matrix factorization and neural
collaborative filtering. The standard approach creates an embedding table where
each row represents a dedicated embedding vector for every unique feature
value. However, this method fails to efficiently handle high-cardinality
features and unseen feature values (e.g. new video ID) that are prevalent in
real-world recommendation systems. In this paper, we propose an alternative
embedding framework Deep Hash Embedding (DHE), replacing embedding tables by a
deep embedding network to compute embeddings on the fly. DHE first encodes the
feature value to a unique identifier vector with multiple hashing functions and
transformations, and then applies a DNN to convert the identifier vector to an
embedding. The encoding module is deterministic, non-learnable, and free of
storage, while the embedding network is updated during the training time to
learn embedding generation. Empirical results show that DHE achieves comparable
AUC against the standard one-hot full embedding, with smaller model sizes. Our
work sheds light on the design of DNN-based alternative embedding schemes for
categorical features without using embedding table lookup.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kang_W/0/1/0/all/0/1"&gt;Wang-Cheng Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_D/0/1/0/all/0/1"&gt;Derek Zhiyuan Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_T/0/1/0/all/0/1"&gt;Tiansheng Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yi_X/0/1/0/all/0/1"&gt;Xinyang Yi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1"&gt;Ting Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hong_L/0/1/0/all/0/1"&gt;Lichan Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chi_E/0/1/0/all/0/1"&gt;Ed H. Chi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Detection of fake news on CoViD-19 on Web Search Engines. (arXiv:2103.11804v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.11804</id>
        <link href="http://arxiv.org/abs/2103.11804"/>
        <updated>2021-06-08T02:20:20.418Z</updated>
        <summary type="html"><![CDATA[In early January 2020, after China reported the first cases of the new
coronavirus (SARS-CoV-2) in the city of Wuhan, unreliable and not fully
accurate information has started spreading faster than the virus itself.
Alongside this pandemic, people have experienced a parallel infodemic, i.e., an
overabundance of information, some of which misleading or even harmful, that
has widely spread around the globe. Although Social Media are increasingly
being used as information source, Web Search Engines, like Google or Yahoo!,
still represent a powerful and trustworthy resource for finding information on
the Web. This is due to their capability to capture the largest amount of
information, helping users quickly identify the most relevant, useful, although
not always the most reliable, results for their search queries. This study aims
to detect potential misleading and fake contents by capturing and analysing
textual information, which flow through Search Engines. By using a real-world
dataset associated with recent CoViD-19 pandemic, we first apply re-sampling
techniques for class imbalance, then we use existing Machine Learning
algorithms for classification of not reliable news. By extracting lexical and
host-based features of associated Uniform Resource Locators (URLs) for news
articles, we show that the proposed methods, so common in phishing and
malicious URLs detection, can improve the efficiency and performance of
classifiers. Based on these findings, we suggest that the use of both textual
and URLs features can improve the effectiveness of fake news detection methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mazzeo_V/0/1/0/all/0/1"&gt;V. Mazzeo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rapisarda_A/0/1/0/all/0/1"&gt;A. Rapisarda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Giuffrida_G/0/1/0/all/0/1"&gt;G. Giuffrida&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeepQTMT: A Deep Learning Approach for Fast QTMT-based CU Partition of Intra-mode VVC. (arXiv:2006.13125v3 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.13125</id>
        <link href="http://arxiv.org/abs/2006.13125"/>
        <updated>2021-06-08T02:20:20.377Z</updated>
        <summary type="html"><![CDATA[Versatile Video Coding (VVC), as the latest standard, significantly improves
the coding efficiency over its ancestor standard High Efficiency Video Coding
(HEVC), but at the expense of sharply increased complexity. In VVC, the
quad-tree plus multi-type tree (QTMT) structure of coding unit (CU) partition
accounts for over 97% of the encoding time, due to the brute-force search for
recursive rate-distortion (RD) optimization. Instead of the brute-force QTMT
search, this paper proposes a deep learning approach to predict the QTMT-based
CU partition, for drastically accelerating the encoding process of intra-mode
VVC. First, we establish a large-scale database containing sufficient CU
partition patterns with diverse video content, which can facilitate the
data-driven VVC complexity reduction. Next, we propose a multi-stage exit CNN
(MSE-CNN) model with an early-exit mechanism to determine the CU partition, in
accord with the flexible QTMT structure at multiple stages. Then, we design an
adaptive loss function for training the MSE-CNN model, synthesizing both the
uncertain number of split modes and the target on minimized RD cost. Finally, a
multi-threshold decision scheme is developed, achieving desirable trade-off
between complexity and RD performance. Experimental results demonstrate that
our approach can reduce the encoding time of VVC by 44.65%-66.88% with the
negligible Bj{\o}ntegaard delta bit-rate (BD-BR) of 1.322%-3.188%, which
significantly outperforms other state-of-the-art approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Li_T/0/1/0/all/0/1"&gt;Tianyi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xu_M/0/1/0/all/0/1"&gt;Mai Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tang_R/0/1/0/all/0/1"&gt;Runzhi Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Ying Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xing_Q/0/1/0/all/0/1"&gt;Qunliang Xing&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An End-to-End Breast Tumour Classification Model Using Context-Based Patch Modelling- A BiLSTM Approach for Image Classification. (arXiv:2106.02864v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02864</id>
        <link href="http://arxiv.org/abs/2106.02864"/>
        <updated>2021-06-08T02:20:20.291Z</updated>
        <summary type="html"><![CDATA[Researchers working on computational analysis of Whole Slide Images (WSIs) in
histopathology have primarily resorted to patch-based modelling due to large
resolution of each WSI. The large resolution makes WSIs infeasible to be fed
directly into the machine learning models due to computational constraints.
However, due to patch-based analysis, most of the current methods fail to
exploit the underlying spatial relationship among the patches. In our work, we
have tried to integrate this relationship along with feature-based correlation
among the extracted patches from the particular tumorous region. For the given
task of classification, we have used BiLSTMs to model both forward and backward
contextual relationship. RNN based models eliminate the limitation of sequence
size by allowing the modelling of variable size images within a deep learning
model. We have also incorporated the effect of spatial continuity by exploring
different scanning techniques used to sample patches. To establish the
efficiency of our approach, we trained and tested our model on two datasets,
microscopy images and WSI tumour regions. After comparing with contemporary
literature we achieved the better performance with accuracy of 90% for
microscopy image dataset. For WSI tumour region dataset, we compared the
classification results with deep learning networks such as ResNet, DenseNet,
and InceptionV3 using maximum voting technique. We achieved the highest
performance accuracy of 84%. We found out that BiLSTMs with CNN features have
performed much better in modelling patches into an end-to-end Image
classification network. Additionally, the variable dimensions of WSI tumour
regions were used for classification without the need for resizing. This
suggests that our method is independent of tumour image size and can process
large dimensional images without losing the resolution details.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tripathi_S/0/1/0/all/0/1"&gt;Suvidha Tripathi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1"&gt;Satish Kumar Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1"&gt;Hwee Kuan Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Encoder-Decoder Neural Architecture Optimization for Keyword Spotting. (arXiv:2106.02738v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02738</id>
        <link href="http://arxiv.org/abs/2106.02738"/>
        <updated>2021-06-08T02:20:20.207Z</updated>
        <summary type="html"><![CDATA[Keyword spotting aims to identify specific keyword audio utterances. In
recent years, deep convolutional neural networks have been widely utilized in
keyword spotting systems. However, their model architectures are mainly based
on off-the shelfbackbones such as VGG-Net or ResNet, instead of specially
designed for the task. In this paper, we utilize neural architecture search to
design convolutional neural network models that can boost the performance of
keyword spotting while maintaining an acceptable memory footprint.
Specifically, we search the model operators and their connections in a specific
search space with Encoder-Decoder neural architecture optimization. Extensive
evaluations on Google's Speech Commands Dataset show that the model
architecture searched by our approach achieves a state-of-the-art accuracy of
over 97%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mo_T/0/1/0/all/0/1"&gt;Tong Mo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1"&gt;Bang Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[About Explicit Variance Minimization: Training Neural Networks for Medical Imaging With Limited Data Annotations. (arXiv:2105.14117v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14117</id>
        <link href="http://arxiv.org/abs/2105.14117"/>
        <updated>2021-06-07T23:29:40.150Z</updated>
        <summary type="html"><![CDATA[Self-supervised learning methods for computer vision have demonstrated the
effectiveness of pre-training feature representations, resulting in
well-generalizing Deep Neural Networks, even if the annotated data are limited.
However, representation learning techniques require a significant amount of
time for model training, with most of it time spent on precise hyper-parameter
optimization and selection of augmentation techniques. We hypothesized that if
the annotated dataset has enough morphological diversity to capture the general
population's as is common in medical imaging, for example, due to conserved
similarities of tissue mythologies, the variance error of the trained model is
the prevalent component of the Bias-Variance Trade-off. We propose the Variance
Aware Training (VAT) method that exploits this property by introducing the
variance error into the model loss function, i.e., enabling minimizing the
variance explicitly. Additionally, we provide the theoretical formulation and
proof of the proposed method to aid in interpreting the approach. Our method
requires selecting only one hyper-parameter and was able to match or improve
the state-of-the-art performance of self-supervised methods while achieving an
order of magnitude reduction in the GPU training time. We validated VAT on
three medical imaging datasets from diverse domains and various learning
objectives. These included a Magnetic Resonance Imaging (MRI) dataset for the
heart semantic segmentation (MICCAI 2017 ACDC challenge), fundus photography
dataset for ordinary regression of diabetic retinopathy progression (Kaggle
2019 APTOS Blindness Detection challenge), and classification of
histopathologic scans of lymph node sections (PatchCamelyon dataset).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shubin_D/0/1/0/all/0/1"&gt;Dmitrii Shubin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eytan_D/0/1/0/all/0/1"&gt;Danny Eytan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goodfellow_S/0/1/0/all/0/1"&gt;Sebastian D. Goodfellow&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[More Is Better: An Analysis of Instance Quantity/Quality Trade-off in Rehearsal-based Continual Learning. (arXiv:2105.14106v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14106</id>
        <link href="http://arxiv.org/abs/2105.14106"/>
        <updated>2021-06-07T23:29:40.127Z</updated>
        <summary type="html"><![CDATA[The design of machines and algorithms capable of learning in a dynamically
changing environment has become an increasingly topical problem with the
increase of the size and heterogeneity of data available to learning systems.
As a consequence, the key issue of Continual Learning has become that of
addressing the stability-plasticity dilemma of connectionist systems, as they
need to adapt their model without forgetting previously acquired knowledge.
Within this context, rehearsal-based methods i.e., solutions in where the
learner exploits memory to revisit past data, has proven to be very effective,
leading to performance at the state-of-the-art. In our study, we propose an
analysis of the memory quantity/quality trade-off adopting various data
reduction approaches to increase the number of instances storable in memory. In
particular, we investigate complex instance compression techniques such as deep
encoders, but also trivial approaches such as image resizing and linear
dimensionality reduction. Our findings suggest that the optimal trade-off is
severely skewed toward instance quantity, where rehearsal approaches with
several heavily compressed instances easily outperform state-of-the-art
approaches with the same amount of memory at their disposal. Further, in high
memory configurations, deep approaches extracting spatial structure combined
with extreme resizing (of the order of $8\times8$ images) yield the best
results, while in memory-constrained configurations where deep approaches
cannot be used due to their memory requirement in training, Extreme Learning
Machines (ELM) offer a clear advantage.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pelosin_F/0/1/0/all/0/1"&gt;Francesco Pelosin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Torsello_A/0/1/0/all/0/1"&gt;Andrea Torsello&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Near-optimal Offline and Streaming Algorithms for Learning Non-Linear Dynamical Systems. (arXiv:2105.11558v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.11558</id>
        <link href="http://arxiv.org/abs/2105.11558"/>
        <updated>2021-06-07T22:33:05.470Z</updated>
        <summary type="html"><![CDATA[We consider the setting of vector valued non-linear dynamical systems
$X_{t+1} = \phi(A^* X_t) + \eta_t$, where $\eta_t$ is unbiased noise and $\phi
: \mathbb{R} \to \mathbb{R}$ is a known link function that satisfies certain
{\em expansivity property}. The goal is to learn $A^*$ from a single trajectory
$X_1,\cdots,X_T$ of {\em dependent or correlated} samples. While the problem is
well-studied in the linear case, where $\phi$ is identity, with optimal error
rates even for non-mixing systems, existing results in the non-linear case hold
only for mixing systems. In this work, we improve existing results for learning
nonlinear systems in a number of ways: a) we provide the first offline
algorithm that can learn non-linear dynamical systems without the mixing
assumption, b) we significantly improve upon the sample complexity of existing
results for mixing systems, c) in the much harder one-pass, streaming setting
we study a SGD with Reverse Experience Replay ($\mathsf{SGD-RER}$) method, and
demonstrate that for mixing systems, it achieves the same sample complexity as
our offline algorithm, d) we justify the expansivity assumption by showing that
for the popular ReLU link function -- a non-expansive but easy to learn link
function with i.i.d. samples -- any method would require exponentially many
samples (with respect to dimension of $X_t$) from the dynamical system. We
validate our results via. simulations and demonstrate that a naive application
of SGD can be highly sub-optimal. Indeed, our work demonstrates that for
correlated data, specialized methods designed for the dependency structure in
data can significantly outperform standard SGD based methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jain_P/0/1/0/all/0/1"&gt;Prateek Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kowshik_S/0/1/0/all/0/1"&gt;Suhas S Kowshik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nagaraj_D/0/1/0/all/0/1"&gt;Dheeraj Nagaraj&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Netrapalli_P/0/1/0/all/0/1"&gt;Praneeth Netrapalli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Equilibrium and non-Equilibrium regimes in the learning of Restricted Boltzmann Machines. (arXiv:2105.13889v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.13889</id>
        <link href="http://arxiv.org/abs/2105.13889"/>
        <updated>2021-06-07T22:33:05.420Z</updated>
        <summary type="html"><![CDATA[Training Restricted Boltzmann Machines (RBMs) has been challenging for a long
time due to the difficulty of computing precisely the log-likelihood gradient.
Over the past decades, many works have proposed more or less successful
training recipes but without studying the crucial quantity of the problem: the
mixing time i.e. the number of Monte Carlo iterations needed to sample new
configurations from a model. In this work, we show that this mixing time plays
a crucial role in the dynamics and stability of the trained model, and that
RBMs operate in two well-defined regimes, namely equilibrium and
out-of-equilibrium, depending on the interplay between this mixing time of the
model and the number of steps, $k$, used to approximate the gradient. We
further show empirically that this mixing time increases with the learning,
which often implies a transition from one regime to another as soon as $k$
becomes smaller than this time. In particular, we show that using the popular
$k$ (persistent) contrastive divergence approaches, with $k$ small, the
dynamics of the learned model are extremely slow and often dominated by strong
out-of-equilibrium effects. On the contrary, RBMs trained in equilibrium
display faster dynamics, and a smooth convergence to dataset-like
configurations during the sampling. Finally we discuss how to exploit in
practice both regimes depending on the task one aims to fulfill: (i) short $k$s
can be used to generate convincing samples in short times, (ii) large $k$ (or
increasingly large) must be used to learn the correct equilibrium distribution
of the RBM.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Decelle_A/0/1/0/all/0/1"&gt;Aur&amp;#xe9;lien Decelle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Furtlehner_C/0/1/0/all/0/1"&gt;Cyril Furtlehner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seoane_B/0/1/0/all/0/1"&gt;Beatriz Seoane&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Online-Bandit Strategies for Minimax Learning Problems. (arXiv:2105.13939v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.13939</id>
        <link href="http://arxiv.org/abs/2105.13939"/>
        <updated>2021-06-07T22:33:05.409Z</updated>
        <summary type="html"><![CDATA[Several learning problems involve solving min-max problems, e.g., empirical
distributional robust learning or learning with non-standard aggregated losses.
More specifically, these problems are convex-linear problems where the
minimization is carried out over the model parameters $w\in\mathcal{W}$ and the
maximization over the empirical distribution $p\in\mathcal{K}$ of the training
set indexes, where $\mathcal{K}$ is the simplex or a subset of it. To design
efficient methods, we let an online learning algorithm play against a
(combinatorial) bandit algorithm. We argue that the efficiency of such
approaches critically depends on the structure of $\mathcal{K}$ and propose two
properties of $\mathcal{K}$ that facilitate designing efficient algorithms. We
focus on a specific family of sets $\mathcal{S}_{n,k}$ encompassing various
learning applications and provide high-probability convergence guarantees to
the minimax values.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Roux_C/0/1/0/all/0/1"&gt;Christophe Roux&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wirth_E/0/1/0/all/0/1"&gt;Elias Wirth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pokutta_S/0/1/0/all/0/1"&gt;Sebastian Pokutta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kerdreux_T/0/1/0/all/0/1"&gt;Thomas Kerdreux&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A nearly Blackwell-optimal policy gradient method. (arXiv:2105.13609v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.13609</id>
        <link href="http://arxiv.org/abs/2105.13609"/>
        <updated>2021-06-07T22:33:05.399Z</updated>
        <summary type="html"><![CDATA[For continuing environments, reinforcement learning methods commonly maximize
a discounted reward criterion with discount factor close to 1 in order to
approximate the steady-state reward (the gain). However, such a criterion only
considers the long-run performance, ignoring the transient behaviour. In this
work, we develop a policy gradient method that optimizes the gain, then the
bias (which indicates the transient performance and is important to capably
select from policies with equal gain). We derive expressions that enable
sampling for the gradient of the bias, and its preconditioning Fisher matrix.
We further propose an algorithm that solves the corresponding bi-level
optimization using a logarithmic barrier. Experimental results provide insights
into the fundamental mechanisms of our proposal.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dewanto_V/0/1/0/all/0/1"&gt;Vektor Dewanto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gallagher_M/0/1/0/all/0/1"&gt;Marcus Gallagher&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cascading Bandit under Differential Privacy. (arXiv:2105.11126v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.11126</id>
        <link href="http://arxiv.org/abs/2105.11126"/>
        <updated>2021-06-07T22:33:05.386Z</updated>
        <summary type="html"><![CDATA[This paper studies \emph{differential privacy (DP)} and \emph{local
differential privacy (LDP)} in cascading bandits. Under DP, we propose an
algorithm which guarantees $\epsilon$-indistinguishability and a regret of
$\mathcal{O}((\frac{\log T}{\epsilon})^{1+\xi})$ for an arbitrarily small
$\xi$. This is a significant improvement from the previous work of
$\mathcal{O}(\frac{\log^3 T}{\epsilon})$ regret. Under
($\epsilon$,$\delta$)-LDP, we relax the $K^2$ dependence through the tradeoff
between privacy budget $\epsilon$ and error probability $\delta$, and obtain a
regret of $\mathcal{O}(\frac{K\log (1/\delta) \log T}{\epsilon^2})$, where $K$
is the size of the arm subset. This result holds for both Gaussian mechanism
and Laplace mechanism by analyses on the composition. Our results extend to
combinatorial semi-bandit. We show respective lower bounds for DP and LDP
cascading bandits. Extensive experiments corroborate our theoretic findings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1"&gt;Kun Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1"&gt;Jing Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1"&gt;Baoxiang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shuai Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_S/0/1/0/all/0/1"&gt;Shuo Shao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Feedback Capacity and a Variant of the Kalman Filter with ARMA Gaussian Noises: Explicit Bounds and Feedback Coding Design. (arXiv:2001.03108v6 [cs.IT] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2001.03108</id>
        <link href="http://arxiv.org/abs/2001.03108"/>
        <updated>2021-06-07T22:33:05.322Z</updated>
        <summary type="html"><![CDATA[In this paper, we relate a feedback channel with any finite-order
autoregressive moving-average (ARMA) Gaussian noises to a variant of the Kalman
filter. In light of this, we obtain relatively explicit lower bounds on the
feedback capacity for such colored Gaussian noises, and the bounds are seen to
be consistent with various existing results in the literature. Meanwhile, this
variant of the Kalman filter also leads to explicit recursive coding schemes
with clear structures to achieve the lower bounds. In general, our results
provide an alternative perspective while pointing to potentially tighter bounds
for the feedback capacity problem.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fang_S/0/1/0/all/0/1"&gt;Song Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1"&gt;Quanyan Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Value Function is All You Need: A Unified Learning Framework for Ride Hailing Platforms. (arXiv:2105.08791v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.08791</id>
        <link href="http://arxiv.org/abs/2105.08791"/>
        <updated>2021-06-07T22:33:05.308Z</updated>
        <summary type="html"><![CDATA[Large ride-hailing platforms, such as DiDi, Uber and Lyft, connect tens of
thousands of vehicles in a city to millions of ride demands throughout the day,
providing great promises for improving transportation efficiency through the
tasks of order dispatching and vehicle repositioning. Existing studies,
however, usually consider the two tasks in simplified settings that hardly
address the complex interactions between the two, the real-time fluctuations
between supply and demand, and the necessary coordinations due to the
large-scale nature of the problem. In this paper we propose a unified
value-based dynamic learning framework (V1D3) for tackling both tasks. At the
center of the framework is a globally shared value function that is updated
continuously using online experiences generated from real-time platform
transactions. To improve the sample-efficiency and the robustness, we further
propose a novel periodic ensemble method combining the fast online learning
with a large-scale offline training scheme that leverages the abundant
historical driver trajectory data. This allows the proposed framework to adapt
quickly to the highly dynamic environment, to generalize robustly to recurrent
patterns and to drive implicit coordinations among the population of managed
vehicles. Extensive experiments based on real-world datasets show considerably
improvements over other recently proposed methods on both tasks. Particularly,
V1D3 outperforms the first prize winners of both dispatching and repositioning
tracks in the KDD Cup 2020 RL competition, achieving state-of-the-art results
on improving both total driver income and user experience related metrics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1"&gt;Xiaocheng Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1"&gt;Fan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1"&gt;Zhiwei Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yansheng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_D/0/1/0/all/0/1"&gt;Dingyuan Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_B/0/1/0/all/0/1"&gt;Bingchen Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tong_Y/0/1/0/all/0/1"&gt;Yongxin Tong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1"&gt;Hongtu Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1"&gt;Jieping Ye&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Efficient Training Approach for Very Large Scale Face Recognition. (arXiv:2105.10375v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.10375</id>
        <link href="http://arxiv.org/abs/2105.10375"/>
        <updated>2021-06-07T22:33:05.238Z</updated>
        <summary type="html"><![CDATA[Face recognition has achieved significant progress in deep-learning era due
to the ultra-large-scale and well-labeled datasets.

However, training on ultra-large-scale datasets is time-consuming and takes
up a lot of hardware resource.

Therefore, designing an efficient training approach is crucial and
indispensable.

The heavy computational and memory costs mainly result from the high
dimensionality of the Fully-Connected (FC) layer.

Specifically, the dimensionality is determined by the number of face
identities, which can be million-level or even more.

To this end, we propose a novel training approach for ultra-large-scale face
datasets, termed Faster Face Classification (F$^2$C).

In F$^2$C, we first define a Gallery Net and a Probe Net that are used to
generate identities' centers and extract faces' features for face recognition,
respectively.

Gallery Net has the same structure as Probe Net and inherits the parameters
from Probe Net with a moving average paradigm.

After that, to reduce the training time and hardware costs of the FC layer,
we propose a Dynamic Class Pool (DCP) that stores the features from Gallery Net
and calculates the inner product (logits) with positive samples (whose
identities are in the DCP) in each mini-batch.

DCP can be regarded as a substitute for the FC layer but it is far smaller,
thus greatly reducing the computational and memory costs.

For negative samples (whose identities are not in DCP), we minimize the
cosine similarities between negative samples and those in DCP.

Then, to improve the update efficiency of DCP's parameters, we design a dual
data-loader including identity-based and instance-based loaders to generate a
certain of identities and samples in mini-batches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1"&gt;Kai Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shuo Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1"&gt;Zhipeng Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaobo Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1"&gt;Xiaojiang Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1"&gt;Baigui Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Hao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1"&gt;Yang You&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SiamRCR: Reciprocal Classification and Regression for Visual Object Tracking. (arXiv:2105.11237v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.11237</id>
        <link href="http://arxiv.org/abs/2105.11237"/>
        <updated>2021-06-07T22:33:05.218Z</updated>
        <summary type="html"><![CDATA[Recently, most siamese network based trackers locate targets via object
classification and bounding-box regression. Generally, they select the
bounding-box with maximum classification confidence as the final prediction.
This strategy may miss the right result due to the accuracy misalignment
between classification and regression. In this paper, we propose a novel
siamese tracking algorithm called SiamRCR, addressing this problem with a
simple, light and effective solution. It builds reciprocal links between
classification and regression branches, which can dynamically re-weight their
losses for each positive sample. In addition, we add a localization branch to
predict the localization accuracy, so that it can work as the replacement of
the regression assistance link during inference. This branch makes the training
and inference more consistent. Extensive experimental results demonstrate the
effectiveness of SiamRCR and its superiority over the state-of-the-art
competitors on GOT-10k, LaSOT, TrackingNet, OTB-2015, VOT-2018 and VOT-2019.
Moreover, our SiamRCR runs at 65 FPS, far above the real-time requirement.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1"&gt;Jinlong Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1"&gt;Zhengkai Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1"&gt;Yueyang Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yang Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yabiao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tai_Y/0/1/0/all/0/1"&gt;Ying Tai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chengjie Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1"&gt;Weiyao Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SHD360: A Benchmark Dataset for Salient Human Detection in 360{\deg} Videos. (arXiv:2105.11578v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.11578</id>
        <link href="http://arxiv.org/abs/2105.11578"/>
        <updated>2021-06-07T22:33:05.198Z</updated>
        <summary type="html"><![CDATA[Salient human detection (SHD) in dynamic 360{\deg} immersive videos is of
great importance for various applications such as robotics, inter-human and
human-object interaction in augmented reality. However, 360{\deg} video SHD has
been seldom discussed in the computer vision community due to a lack of
datasets with large-scale omnidirectional videos and rich annotations. To this
end, we propose SHD360, the first 360{\deg} video SHD dataset containing
various real-life daily scenes borrowed from this http URL, with
hierarchical annotations for 6,268 key frames uniformly sampled from 37,403
omnidirectional video frames at 4K resolution. Since so far there is no method
proposed for 360{\deg} image/video SHD, we systematically benchmark 11
representative state-of-the-art salient object detection approaches on our
SHD360. We hope our proposed dataset and benchmark could serve as a good
starting point for advancing human-centric researches towards 360{\deg}
panoramic data. Our dataset and benchmark will be publicly available at
https://github.com/PanoAsh/SHD360.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1"&gt;Kang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hamidouche_W/0/1/0/all/0/1"&gt;Wassim Hamidouche&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deforges_O/0/1/0/all/0/1"&gt;Olivier Deforges&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Image Distribution Estimation with Random Field and Random Cluster Theories. (arXiv:2104.10762v5 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.10762</id>
        <link href="http://arxiv.org/abs/2104.10762"/>
        <updated>2021-06-07T22:33:05.164Z</updated>
        <summary type="html"><![CDATA[Random field and random cluster theory is used to prove certain mathematical
results concerning the probability distribution of images characterized as
generic $2D$ integer arrays during simultaneous learning. Example models in
image classification and object segmentation illustrate the mathematical
results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Murphy_R/0/1/0/all/0/1"&gt;Robert A. Murphy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robustness Testing of Language Understanding in Task-Oriented Dialog. (arXiv:2012.15262v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.15262</id>
        <link href="http://arxiv.org/abs/2012.15262"/>
        <updated>2021-06-07T22:33:05.098Z</updated>
        <summary type="html"><![CDATA[Most language understanding models in task-oriented dialog systems are
trained on a small amount of annotated training data, and evaluated in a small
set from the same distribution. However, these models can lead to system
failure or undesirable output when being exposed to natural language
perturbation or variation in practice. In this paper, we conduct comprehensive
evaluation and analysis with respect to the robustness of natural language
understanding models, and introduce three important aspects related to language
understanding in real-world dialog systems, namely, language variety, speech
characteristics, and noise perturbation. We propose a model-agnostic toolkit
LAUG to approximate natural language perturbations for testing the robustness
issues in task-oriented dialog. Four data augmentation approaches covering the
three aspects are assembled in LAUG, which reveals critical robustness issues
in state-of-the-art models. The augmented dataset through LAUG can be used to
facilitate future research on the robustness testing of language understanding
in task-oriented dialog.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jiexi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Takanobu_R/0/1/0/all/0/1"&gt;Ryuichi Takanobu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1"&gt;Jiaxin Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wan_D/0/1/0/all/0/1"&gt;Dazhen Wan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Hongguang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nie_W/0/1/0/all/0/1"&gt;Weiran Nie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Cheng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_W/0/1/0/all/0/1"&gt;Wei Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1"&gt;Minlie Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sigma-Delta and Distributed Noise-Shaping Quantization Methods for Random Fourier Features. (arXiv:2106.02614v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02614</id>
        <link href="http://arxiv.org/abs/2106.02614"/>
        <updated>2021-06-07T03:06:17.007Z</updated>
        <summary type="html"><![CDATA[We propose the use of low bit-depth Sigma-Delta and distributed noise-shaping
methods for quantizing the Random Fourier features (RFFs) associated with
shift-invariant kernels. We prove that our quantized RFFs -- even in the case
of $1$-bit quantization -- allow a high accuracy approximation of the
underlying kernels, and the approximation error decays at least polynomially
fast as the dimension of the RFFs increases. We also show that the quantized
RFFs can be further compressed, yielding an excellent trade-off between memory
use and accuracy. Namely, the approximation error now decays exponentially as a
function of the bits used. Moreover, we empirically show by testing the
performance of our methods on several machine learning tasks that our method
compares favorably to other state of the art quantization methods in this
context.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jinjie Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cloninger_A/0/1/0/all/0/1"&gt;Alexander Cloninger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saab_R/0/1/0/all/0/1"&gt;Rayan Saab&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ES-ENAS: Controller-Based Architecture Search for Evolutionary Reinforcement Learning. (arXiv:2101.07415v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.07415</id>
        <link href="http://arxiv.org/abs/2101.07415"/>
        <updated>2021-06-07T03:06:17.000Z</updated>
        <summary type="html"><![CDATA[We introduce ES-ENAS, a simple yet general evolutionary joint optimization
procedure by combining continuous optimization via Evolutionary Strategies (ES)
and combinatorial optimization via Efficient NAS (ENAS) in a highly scalable
and intuitive way. Our main insight is noticing that ES is already a highly
distributed algorithm involving hundreds of forward passes which can not only
be used for training neural network weights, but also for jointly training a
NAS controller, both in a blackbox fashion. By doing so, we also bridge the gap
from NAS research in supervised learning settings to the reinforcement learning
scenario through this relatively simple marriage between two different yet
common lines of research. We demonstrate the utility and effectiveness of our
method over a large search space by training highly combinatorial neural
network architectures for RL problems in continuous control, via edge pruning
and quantization. We also incorporate a wide variety of popular techniques from
modern NAS literature including multiobjective optimization along with various
controller methods, to showcase their promise in the RL field and discuss
possible extensions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1"&gt;Xingyou Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choromanski_K/0/1/0/all/0/1"&gt;Krzysztof Choromanski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parker_Holder_J/0/1/0/all/0/1"&gt;Jack Parker-Holder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1"&gt;Yunhao Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_D/0/1/0/all/0/1"&gt;Daiyi Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jain_D/0/1/0/all/0/1"&gt;Deepali Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1"&gt;Wenbo Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pacchiano_A/0/1/0/all/0/1"&gt;Aldo Pacchiano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sarlos_T/0/1/0/all/0/1"&gt;Tamas Sarlos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yuxiang Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SmartON: Just-in-Time Active Event Detection on Energy Harvesting Systems. (arXiv:2103.00749v2 [eess.SP] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.00749</id>
        <link href="http://arxiv.org/abs/2103.00749"/>
        <updated>2021-06-07T03:06:16.993Z</updated>
        <summary type="html"><![CDATA[We propose SmartON, a batteryless system that learns to wake up proactively
at the right moment in order to detect events of interest. It does so by
adapting the duty cycle to match the distribution of event arrival times under
the constraints of harvested energy. While existing energy harvesting systems
either wake up periodically at a fixed rate to sense and process the data, or
wake up only in accordance with the availability of the energy source, SmartON
employs a three-phase learning framework to learn the energy harvesting pattern
as well as the pattern of events at run-time, and uses that knowledge to wake
itself up when events are most likely to occur. The three-phase learning
framework enables rapid adaptation to environmental changes in both short and
long terms. Being able to remain asleep more often than a CTID
(charging-then-immediate-discharging) wake-up system and adapt to the event
pattern, SmartON is able to reduce energy waste, increase energy efficiency,
and capture more events. To realize SmartON we have developed a dedicated
hardware platform whose power management module activates capacitors on-the-fly
to dynamically increase its storage capacitance. We conduct both
simulation-driven and real-system experiments to demonstrate that SmartON
captures 1X--7X more events and is 8X--17X more energy-efficient than a CTID
system.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Luo_Y/0/1/0/all/0/1"&gt;Yubo Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Nirjon_S/0/1/0/all/0/1"&gt;Shahriar Nirjon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Convergent and Dimension-Independent First-Order Algorithm for Min-Max Optimization. (arXiv:2006.12376v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.12376</id>
        <link href="http://arxiv.org/abs/2006.12376"/>
        <updated>2021-06-07T03:06:16.987Z</updated>
        <summary type="html"><![CDATA[Motivated by the recent work of Mangoubi and Vishnoi (STOC 2021), we propose
a variant of the min-max optimization framework where the max-player is
constrained to update the maximization variable in a greedy manner until it
reaches a *first-order* stationary point. We present an algorithm that provably
converges to an approximate local equilibrium for our framework from any
initialization and for nonconvex-nonconcave loss functions. Compared to the
second-order algorithm of Mangoubi and Vishnoi, whose iteration bound is
polynomial in the dimension, our algorithm is first-order and its iteration
bound is independent of dimension. We empirically evaluate our algorithm on
challenging nonconvex-nonconcave test-functions and loss functions that arise
in GAN training. Our algorithm converges on these test functions and, when used
to train GANs on synthetic and real-world datasets, trains stably and avoids
mode collapse.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Keswani_V/0/1/0/all/0/1"&gt;Vijay Keswani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mangoubi_O/0/1/0/all/0/1"&gt;Oren Mangoubi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sachdeva_S/0/1/0/all/0/1"&gt;Sushant Sachdeva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vishnoi_N/0/1/0/all/0/1"&gt;Nisheeth K. Vishnoi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hyperparameter Optimization Is Deceiving Us, and How to Stop It. (arXiv:2102.03034v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.03034</id>
        <link href="http://arxiv.org/abs/2102.03034"/>
        <updated>2021-06-07T03:06:16.981Z</updated>
        <summary type="html"><![CDATA[Recent empirical work shows that inconsistent results, based on choice of
hyperparameter optimization (HPO) configuration, are a widespread problem in ML
research. When comparing two algorithms J and K, searching one subspace can
yield the conclusion that J outperforms K, whereas searching another can entail
the opposite. In short, the way we choose hyperparameters can deceive us. We
provide a theoretical complement to this prior work, arguing that, to avoid
such deception, the process of drawing conclusions from HPO should be made more
rigorous. We call this process epistemic hyperparameter optimization (EHPO),
and put forth a logical framework to capture its semantics and how it can lead
to inconsistent conclusions about performance. Our framework enables us to
prove EHPO methods that are guaranteed to be defended against deception. We
demonstrate its utility by proving and empirically validating a defended
variant of random search.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cooper_A/0/1/0/all/0/1"&gt;A. Feder Cooper&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1"&gt;Yucheng Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Forde_J/0/1/0/all/0/1"&gt;Jessica Zosa Forde&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sa_C/0/1/0/all/0/1"&gt;Christopher De Sa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fair Exploration via Axiomatic Bargaining. (arXiv:2106.02553v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02553</id>
        <link href="http://arxiv.org/abs/2106.02553"/>
        <updated>2021-06-07T03:06:16.974Z</updated>
        <summary type="html"><![CDATA[Motivated by the consideration of fairly sharing the cost of exploration
between multiple groups in learning problems, we develop the Nash bargaining
solution in the context of multi-armed bandits. Specifically, the 'grouped'
bandit associated with any multi-armed bandit problem associates, with each
time step, a single group from some finite set of groups. The utility gained by
a given group under some learning policy is naturally viewed as the reduction
in that group's regret relative to the regret that group would have incurred
'on its own'. We derive policies that yield the Nash bargaining solution
relative to the set of incremental utilities possible under any policy. We show
that on the one hand, the 'price of fairness' under such policies is limited,
while on the other hand, regret optimal policies are arbitrarily unfair under
generic conditions. Our theoretical development is complemented by a case study
on contextual bandits for warfarin dosing where we are concerned with the cost
of exploration across multiple races and age groups.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Baek_J/0/1/0/all/0/1"&gt;Jackie Baek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Farias_V/0/1/0/all/0/1"&gt;Vivek F. Farias&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Influence Estimation and Maximization via Neural Mean-Field Dynamics. (arXiv:2106.02608v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02608</id>
        <link href="http://arxiv.org/abs/2106.02608"/>
        <updated>2021-06-07T03:06:16.956Z</updated>
        <summary type="html"><![CDATA[We propose a novel learning framework using neural mean-field (NMF) dynamics
for inference and estimation problems on heterogeneous diffusion networks. Our
new framework leverages the Mori-Zwanzig formalism to obtain an exact evolution
equation of the individual node infection probabilities, which renders a delay
differential equation with memory integral approximated by learnable time
convolution operators. Directly using information diffusion cascade data, our
framework can simultaneously learn the structure of the diffusion network and
the evolution of node infection probabilities. Connections between parameter
learning and optimal control are also established, leading to a rigorous and
implementable algorithm for training NMF. Moreover, we show that the projected
gradient descent method can be employed to solve the challenging influence
maximization problem, where the gradient is computed extremely fast by
integrating NMF forward in time just once in each iteration. Extensive
empirical studies show that our approach is versatile and robust to variations
of the underlying diffusion network models, and significantly outperform
existing approaches in accuracy and efficiency on both synthetic and real-world
data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1"&gt;Shushan He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zha_H/0/1/0/all/0/1"&gt;Hongyuan Zha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1"&gt;Xiaojing Ye&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Graph Models for Retrosynthesis Prediction. (arXiv:2006.07038v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.07038</id>
        <link href="http://arxiv.org/abs/2006.07038"/>
        <updated>2021-06-07T03:06:16.949Z</updated>
        <summary type="html"><![CDATA[Retrosynthesis prediction is a fundamental problem in organic synthesis,
where the task is to identify precursor molecules that can be used to
synthesize a target molecule. A key consideration in building neural models for
this task is aligning model design with strategies adopted by chemists.
Building on this viewpoint, this paper introduces a graph-based approach that
capitalizes on the idea that the graph topology of precursor molecules is
largely unaltered during a chemical reaction. The model first predicts the set
of graph edits transforming the target into incomplete molecules called
synthons. Next, the model learns to expand synthons into complete molecules by
attaching relevant leaving groups. This decomposition simplifies the
architecture, making its predictions more interpretable, and also amenable to
manual correction. Our model achieves a top-1 accuracy of $53.7\%$,
outperforming previous template-free and semi-template-based methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Somnath_V/0/1/0/all/0/1"&gt;Vignesh Ram Somnath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bunne_C/0/1/0/all/0/1"&gt;Charlotte Bunne&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Coley_C/0/1/0/all/0/1"&gt;Connor W. Coley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krause_A/0/1/0/all/0/1"&gt;Andreas Krause&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barzilay_R/0/1/0/all/0/1"&gt;Regina Barzilay&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Procedural World Generation Framework for Systematic Evaluation of Continual Learning. (arXiv:2106.02585v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02585</id>
        <link href="http://arxiv.org/abs/2106.02585"/>
        <updated>2021-06-07T03:06:16.942Z</updated>
        <summary type="html"><![CDATA[Several families of continual learning techniques have been proposed to
alleviate catastrophic interference in deep neural network training on
non-stationary data. However, a comprehensive comparison and analysis of
limitations remains largely open due to the inaccessibility to suitable
datasets. Empirical examination not only varies immensely between individual
works, it further currently relies on contrived composition of benchmarks
through subdivision and concatenation of various prevalent static vision
datasets. In this work, our goal is to bridge this gap by introducing a
computer graphics simulation framework that repeatedly renders only upcoming
urban scene fragments in an endless real-time procedural world generation
process. At its core lies a modular parametric generative model with adaptable
generative factors. The latter can be used to flexibly compose data streams,
which significantly facilitates a detailed analysis and allows for effortless
investigation of various continual learning schemes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hess_T/0/1/0/all/0/1"&gt;Timm Hess&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mundt_M/0/1/0/all/0/1"&gt;Martin Mundt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pliushch_I/0/1/0/all/0/1"&gt;Iuliia Pliushch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramesh_V/0/1/0/all/0/1"&gt;Visvanathan Ramesh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Causally-motivated Shortcut Removal Using Auxiliary Labels. (arXiv:2105.06422v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.06422</id>
        <link href="http://arxiv.org/abs/2105.06422"/>
        <updated>2021-06-07T03:06:16.936Z</updated>
        <summary type="html"><![CDATA[Robustness to certain forms of distribution shift is a key concern in many ML
applications. Often, robustness can be formulated as enforcing invariances to
particular interventions on the data generating process. Here, we study a
flexible, causally-motivated approach to enforcing such invariances, paying
special attention to shortcut learning, where a robust predictor can achieve
optimal i.i.d generalization in principle, but instead it relies on spurious
correlations or shortcuts in practice. Our approach uses auxiliary labels,
typically available at training time, to enforce conditional independences
between the latent factors that determine these labels. We show both
theoretically and empirically that causally-motivated regularization schemes
(a) lead to more robust estimators that generalize well under distribution
shift, and (b) have better finite sample efficiency compared to usual
regularization schemes, even in the absence of distribution shifts. Our
analysis highlights important theoretical properties of training techniques
commonly used in causal inference, fairness, and disentanglement literature.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Makar_M/0/1/0/all/0/1"&gt;Maggie Makar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Packer_B/0/1/0/all/0/1"&gt;Ben Packer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moldovan_D/0/1/0/all/0/1"&gt;Dan Moldovan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Blalock_D/0/1/0/all/0/1"&gt;Davis Blalock&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Halpern_Y/0/1/0/all/0/1"&gt;Yoni Halpern&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+DAmour_A/0/1/0/all/0/1"&gt;Alexander D&amp;#x27;Amour&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Growing 3D Artefacts and Functional Machines with Neural Cellular Automata. (arXiv:2103.08737v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.08737</id>
        <link href="http://arxiv.org/abs/2103.08737"/>
        <updated>2021-06-07T03:06:16.920Z</updated>
        <summary type="html"><![CDATA[Neural Cellular Automata (NCAs) have been proven effective in simulating
morphogenetic processes, the continuous construction of complex structures from
very few starting cells. Recent developments in NCAs lie in the 2D domain,
namely reconstructing target images from a single pixel or infinitely growing
2D textures. In this work, we propose an extension of NCAs to 3D, utilizing 3D
convolutions in the proposed neural network architecture. Minecraft is selected
as the environment for our automaton since it allows the generation of both
static structures and moving machines. We show that despite their simplicity,
NCAs are capable of growing complex entities such as castles, apartment blocks,
and trees, some of which are composed of over 3,000 blocks. Additionally, when
trained for regeneration, the system is able to regrow parts of simple
functional machines, significantly expanding the capabilities of simulated
morphogenetic systems. The code for the experiment in this paper can be found
at: https://github.com/real-itu/3d-artefacts-nca.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sudhakaran_S/0/1/0/all/0/1"&gt;Shyam Sudhakaran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grbic_D/0/1/0/all/0/1"&gt;Djordje Grbic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Siyan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Katona_A/0/1/0/all/0/1"&gt;Adam Katona&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Najarro_E/0/1/0/all/0/1"&gt;Elias Najarro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Glanois_C/0/1/0/all/0/1"&gt;Claire Glanois&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Risi_S/0/1/0/all/0/1"&gt;Sebastian Risi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How do Quadratic Regularizers Prevent Catastrophic Forgetting: The Role of Interpolation. (arXiv:2102.02805v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.02805</id>
        <link href="http://arxiv.org/abs/2102.02805"/>
        <updated>2021-06-07T03:06:16.914Z</updated>
        <summary type="html"><![CDATA[Catastrophic forgetting undermines the effectiveness of deep neural networks
(DNNs) in scenarios such as continual learning and lifelong learning. While
several methods have been proposed to tackle this problem, there is limited
work explaining why these methods work well. This paper has the goal of better
explaining a popularly used technique for avoiding catastrophic forgetting:
quadratic regularization. We show that quadratic regularizers prevent
forgetting of past tasks by interpolating current and previous values of model
parameters at every training iteration. Over multiple training iterations, this
interpolation operation reduces the learning rates of more important model
parameters, thereby minimizing their movement. Our analysis also reveals two
drawbacks of quadratic regularization: (a) dependence of parameter
interpolation on training hyperparameters, which often leads to training
instability and (b) assignment of lower importance to deeper layers, which are
generally the place forgetting occurs in DNNs. Via a simple modification to the
order of operations, we show these drawbacks can be easily avoided, resulting
in 6.2% higher average accuracy at 4.5% lower average forgetting. Code
available at \url{https://github.com/EkdeepSLubana/QRforgetting}]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lubana_E/0/1/0/all/0/1"&gt;Ekdeep Singh Lubana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Trivedi_P/0/1/0/all/0/1"&gt;Puja Trivedi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koutra_D/0/1/0/all/0/1"&gt;Danai Koutra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dick_R/0/1/0/all/0/1"&gt;Robert P. Dick&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Painless Stochastic Gradient: Interpolation, Line-Search, and Convergence Rates. (arXiv:1905.09997v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1905.09997</id>
        <link href="http://arxiv.org/abs/1905.09997"/>
        <updated>2021-06-07T03:06:16.908Z</updated>
        <summary type="html"><![CDATA[Recent works have shown that stochastic gradient descent (SGD) achieves the
fast convergence rates of full-batch gradient descent for over-parameterized
models satisfying certain interpolation conditions. However, the step-size used
in these works depends on unknown quantities and SGD's practical performance
heavily relies on the choice of this step-size. We propose to use line-search
techniques to automatically set the step-size when training models that can
interpolate the data. In the interpolation setting, we prove that SGD with a
stochastic variant of the classic Armijo line-search attains the deterministic
convergence rates for both convex and strongly-convex functions. Under
additional assumptions, SGD with Armijo line-search is shown to achieve fast
convergence for non-convex functions. Furthermore, we show that stochastic
extra-gradient with a Lipschitz line-search attains linear convergence for an
important class of non-convex functions and saddle-point problems satisfying
interpolation. To improve the proposed methods' practical performance, we give
heuristics to use larger step-sizes and acceleration. We compare the proposed
algorithms against numerous optimization methods on standard classification
tasks using both kernel methods and deep networks. The proposed methods result
in competitive performance across all models and datasets, while being robust
to the precise choices of hyper-parameters. For multi-class classification
using deep networks, SGD with Armijo line-search results in both faster
convergence and better generalization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vaswani_S/0/1/0/all/0/1"&gt;Sharan Vaswani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mishkin_A/0/1/0/all/0/1"&gt;Aaron Mishkin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Laradji_I/0/1/0/all/0/1"&gt;Issam Laradji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schmidt_M/0/1/0/all/0/1"&gt;Mark Schmidt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gidel_G/0/1/0/all/0/1"&gt;Gauthier Gidel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lacoste_Julien_S/0/1/0/all/0/1"&gt;Simon Lacoste-Julien&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Effects of Quantisation on Model Uncertainty in Bayesian Neural Networks. (arXiv:2102.11062v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.11062</id>
        <link href="http://arxiv.org/abs/2102.11062"/>
        <updated>2021-06-07T03:06:16.902Z</updated>
        <summary type="html"><![CDATA[Bayesian neural networks (BNNs) are making significant progress in many
research areas where decision-making needs to be accompanied by uncertainty
estimation. Being able to quantify uncertainty while making decisions is
essential for understanding when the model is over-/under-confident, and hence
BNNs are attracting interest in safety-critical applications, such as
autonomous driving, healthcare, and robotics. Nevertheless, BNNs have not been
as widely used in industrial practice, mainly because of their increased memory
and compute costs. In this work, we investigate quantisation of BNNs by
compressing 32-bit floating-point weights and activations to their integer
counterparts, that has already been successful in reducing the compute demand
in standard pointwise neural networks. We study three types of quantised BNNs,
we evaluate them under a wide range of different settings, and we empirically
demonstrate that a uniform quantisation scheme applied to BNNs does not
substantially decrease their quality of uncertainty estimation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ferianc_M/0/1/0/all/0/1"&gt;Martin Ferianc&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maji_P/0/1/0/all/0/1"&gt;Partha Maji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mattina_M/0/1/0/all/0/1"&gt;Matthew Mattina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rodrigues_M/0/1/0/all/0/1"&gt;Miguel Rodrigues&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Collection and harmonization of system logs and prototypal Analytics services with the Elastic (ELK) suite at the INFN-CNAF computing centre. (arXiv:2106.02612v1 [cs.DC])]]></title>
        <id>http://arxiv.org/abs/2106.02612</id>
        <link href="http://arxiv.org/abs/2106.02612"/>
        <updated>2021-06-07T03:06:16.895Z</updated>
        <summary type="html"><![CDATA[The distributed Grid infrastructure for High Energy Physics experiments at
the Large Hadron Collider (LHC) in Geneva comprises a set of computing centres,
spread all over the world, as part of the Worldwide LHC Computing Grid (WLCG).
In Italy, the Tier-1 functionalities are served by the INFN-CNAF data center,
which provides also computing and storage resources to more than twenty non-LHC
experiments. For this reason, a high amount of logs are collected each day from
various sources, which are highly heterogeneous and difficult to harmonize. In
this contribution, a working implementation of a system that collects, parses
and displays the log information from CNAF data sources and the investigation
of a Machine Learning based predictive maintenance system, is presented.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Diotalevi_T/0/1/0/all/0/1"&gt;Tommaso Diotalevi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Falabella_A/0/1/0/all/0/1"&gt;Antonio Falabella&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martelli_B/0/1/0/all/0/1"&gt;Barbara Martelli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Michelotto_D/0/1/0/all/0/1"&gt;Diego Michelotto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morganti_L/0/1/0/all/0/1"&gt;Lucia Morganti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bonacorsi_D/0/1/0/all/0/1"&gt;Daniele Bonacorsi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Giommi_L/0/1/0/all/0/1"&gt;Luca Giommi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tisbeni_S/0/1/0/all/0/1"&gt;Simone Rossi Tisbeni&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semi-Supervised Multi-aspect Detection of Misinformation using Hierarchical Joint Decomposition. (arXiv:2005.04310v2 [cs.SI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.04310</id>
        <link href="http://arxiv.org/abs/2005.04310"/>
        <updated>2021-06-07T03:06:16.886Z</updated>
        <summary type="html"><![CDATA[Distinguishing between misinformation and real information is one of the most
challenging problems in today's interconnected world. The vast majority of the
state-of-the-art in detecting misinformation is fully supervised, requiring a
large number of high-quality human annotations. However, the availability of
such annotations cannot be taken for granted, since it is very costly,
time-consuming, and challenging to do so in a way that keeps up with the
proliferation of misinformation. In this work, we are interested in exploring
scenarios where the number of annotations is limited. In such scenarios, we
investigate how tapping on a diverse number of resources that characterize a
news article, henceforth referred to as "aspects" can compensate for the lack
of labels. In particular, our contributions in this paper are twofold: 1) We
propose the use of three different aspects: article content, context of social
sharing behaviors, and host website/domain features, and 2) We introduce a
principled tensor based embedding framework that combines all those aspects
effectively. We propose HiJoD a 2-level decomposition pipeline which not only
outperforms state-of-the-art methods with F1-scores of 74% and 81% on Twitter
and Politifact datasets respectively but also is an order of magnitude faster
than similar ensemble approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Abdali_S/0/1/0/all/0/1"&gt;Sara Abdali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shah_N/0/1/0/all/0/1"&gt;Neil Shah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Papalexakis_E/0/1/0/all/0/1"&gt;Evangelos E. Papalexakis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fundamental tradeoffs between memorization and robustness in random features and neural tangent regimes. (arXiv:2106.02630v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.02630</id>
        <link href="http://arxiv.org/abs/2106.02630"/>
        <updated>2021-06-07T03:06:16.878Z</updated>
        <summary type="html"><![CDATA[This work studies the (non)robustness of two-layer neural networks in various
high-dimensional linearized regimes. We establish fundamental trade-offs
between memorization and robustness, as measured by the Sobolev-seminorm of the
model w.r.t the data distribution, i.e the square root of the average squared
$L_2$-norm of the gradients of the model w.r.t the its input. More precisely,
if $n$ is the number of training examples, $d$ is the input dimension, and $k$
is the number of hidden neurons in a two-layer neural network, we prove for a
large class of activation functions that, if the model memorizes even a
fraction of the training, then its Sobolev-seminorm is lower-bounded by (i)
$\sqrt{n}$ in case of infinite-width random features (RF) or neural tangent
kernel (NTK) with $d \gtrsim n$; (ii) $\sqrt{n}$ in case of finite-width RF
with proportionate scaling of $d$ and $k$; and (iii) $\sqrt{n/k}$ in case of
finite-width NTK with proportionate scaling of $d$ and $k$. Moreover, all of
these lower-bounds are tight: they are attained by the min-norm / least-squares
interpolator (when $n$, $d$, and $k$ are in the appropriate interpolating
regime). All our results hold as soon as data is log-concave isotropic, and
there is label-noise, i.e the target variable is not a deterministic function
of the data / features. We empirically validate our theoretical results with
experiments. Accidentally, these experiments also reveal for the first time,
(iv) a multiple-descent phenomenon in the robustness of the min-norm
interpolator.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Dohmatob_E/0/1/0/all/0/1"&gt;Elvis Dohmatob&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DoubleML -- An Object-Oriented Implementation of Double Machine Learning in R. (arXiv:2103.09603v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.09603</id>
        <link href="http://arxiv.org/abs/2103.09603"/>
        <updated>2021-06-07T03:06:16.871Z</updated>
        <summary type="html"><![CDATA[The R package DoubleML implements the double/debiased machine learning
framework of Chernozhukov et al. (2018). It provides functionalities to
estimate parameters in causal models based on machine learning methods. The
double machine learning framework consist of three key ingredients: Neyman
orthogonality, high-quality machine learning estimation and sample splitting.
Estimation of nuisance components can be performed by various state-of-the-art
machine learning methods that are available in the mlr3 ecosystem. DoubleML
makes it possible to perform inference in a variety of causal models, including
partially linear and interactive regression models and their extensions to
instrumental variable estimation. The object-oriented implementation of
DoubleML enables a high flexibility for the model specification and makes it
easily extendable. This paper serves as an introduction to the double machine
learning framework and the R package DoubleML. In reproducible code examples
with simulated and real data sets, we demonstrate how DoubleML users can
perform valid inference based on machine learning methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Bach_P/0/1/0/all/0/1"&gt;Philipp Bach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Chernozhukov_V/0/1/0/all/0/1"&gt;Victor Chernozhukov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kurz_M/0/1/0/all/0/1"&gt;Malte S. Kurz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Spindler_M/0/1/0/all/0/1"&gt;Martin Spindler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Low-Precision Reinforcement Learning: Running Soft Actor-Critic in Half Precision. (arXiv:2102.13565v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.13565</id>
        <link href="http://arxiv.org/abs/2102.13565"/>
        <updated>2021-06-07T03:06:16.840Z</updated>
        <summary type="html"><![CDATA[Low-precision training has become a popular approach to reduce compute
requirements, memory footprint, and energy consumption in supervised learning.
In contrast, this promising approach has not yet enjoyed similarly widespread
adoption within the reinforcement learning (RL) community, partly because RL
agents can be notoriously hard to train even in full precision. In this paper
we consider continuous control with the state-of-the-art SAC agent and
demonstrate that a na\"ive adaptation of low-precision methods from supervised
learning fails. We propose a set of six modifications, all straightforward to
implement, that leaves the underlying agent and its hyperparameters unchanged
but improves the numerical stability dramatically. The resulting modified SAC
agent has lower memory and compute requirements while matching full-precision
rewards, demonstrating that low-precision training can substantially accelerate
state-of-the-art RL without parameter tuning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bjorck_J/0/1/0/all/0/1"&gt;Johan Bjorck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xiangyu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sa_C/0/1/0/all/0/1"&gt;Christopher De Sa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gomes_C/0/1/0/all/0/1"&gt;Carla P. Gomes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weinberger_K/0/1/0/all/0/1"&gt;Kilian Q. Weinberger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Natural Way to Overcome the Catastrophic Forgetting in Neural Networks. (arXiv:2005.07107v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.07107</id>
        <link href="http://arxiv.org/abs/2005.07107"/>
        <updated>2021-06-07T03:06:16.822Z</updated>
        <summary type="html"><![CDATA[Not so long ago, a method was discovered that successfully overcomes the
catastrophic forgetting in neural networks. Although we know about the cases of
using this method to preserve skills when adapting pre-trained networks to
particular tasks, it has not obtained widespread distribution yet. In this
paper, we would like to propose an alternative method of overcoming
catastrophic forgetting based on the total absolute signal passing through each
connection in the network. This method has a simple implementation and seems to
us essentially close to the processes occurring in the brain of animals to
preserve previously learned skills during subsequent learning. We hope that the
ease of implementation of this method will serve its wide application.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kutalev_A/0/1/0/all/0/1"&gt;Alexey Kutalev&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Relevance-Guided Modeling of Object Dynamics for Reinforcement Learning. (arXiv:2003.01384v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.01384</id>
        <link href="http://arxiv.org/abs/2003.01384"/>
        <updated>2021-06-07T03:06:16.801Z</updated>
        <summary type="html"><![CDATA[Current deep reinforcement learning (RL) approaches incorporate minimal prior
knowledge about the environment, limiting computational and sample efficiency.
\textit{Objects} provide a succinct and causal description of the world, and
many recent works have proposed unsupervised object representation learning
using priors and losses over static object properties like visual consistency.
However, object dynamics and interactions are also critical cues for
objectness. In this paper we propose a framework for reasoning about object
dynamics and behavior to rapidly determine minimal and task-specific object
representations. To demonstrate the need to reason over object behavior and
dynamics, we introduce a suite of RGBD MuJoCo object collection and avoidance
tasks that, while intuitive and visually simple, confound state-of-the-art
unsupervised object representation learning algorithms. We also highlight the
potential of this framework on several Atari games, using our object
representation and standard RL and planning algorithms to learn dramatically
faster than existing deep RL algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Agnew_W/0/1/0/all/0/1"&gt;William Agnew&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Domingos_P/0/1/0/all/0/1"&gt;Pedro Domingos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fundamental Limits of Controlled Stochastic Dynamical Systems: An Information-Theoretic Approach. (arXiv:2012.12174v6 [eess.SY] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.12174</id>
        <link href="http://arxiv.org/abs/2012.12174"/>
        <updated>2021-06-07T03:06:16.779Z</updated>
        <summary type="html"><![CDATA[In this paper, we examine the fundamental performance limitations in the
control of stochastic dynamical systems; more specifically, we derive generic
$\mathcal{L}_p$ bounds that hold for any causal (stabilizing) controllers and
any stochastic disturbances, by an information-theoretic analysis. We first
consider the scenario where the plant (i.e., the dynamical system to be
controlled) is linear time-invariant, and it is seen in general that the lower
bounds are characterized by the unstable poles (or nonminimum-phase zeros) of
the plant as well as the conditional entropy of the disturbance. We then
analyze the setting where the plant is assumed to be (strictly) causal, for
which case the lower bounds are determined by the conditional entropy of the
disturbance. We also discuss the special cases of $p = 2$ and $p = \infty$,
which correspond to minimum-variance control and controlling the maximum
deviations, respectively. In addition, we investigate the power-spectral
characterization of the lower bounds as well as its relation to the
Kolmogorov-Szeg\"o formula.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Fang_S/0/1/0/all/0/1"&gt;Song Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhu_Q/0/1/0/all/0/1"&gt;Quanyan Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MUSBO: Model-based Uncertainty Regularized and Sample Efficient Batch Optimization for Deployment Constrained Reinforcement Learning. (arXiv:2102.11448v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.11448</id>
        <link href="http://arxiv.org/abs/2102.11448"/>
        <updated>2021-06-07T03:06:16.770Z</updated>
        <summary type="html"><![CDATA[In many contemporary applications such as healthcare, finance, robotics, and
recommendation systems, continuous deployment of new policies for data
collection and online learning is either cost ineffective or impractical. We
consider a setting that lies between pure offline reinforcement learning (RL)
and pure online RL called deployment constrained RL in which the number of
policy deployments for data sampling is limited. To solve this challenging
task, we propose a new algorithmic learning framework called Model-based
Uncertainty regularized and Sample Efficient Batch Optimization (MUSBO). Our
framework discovers novel and high quality samples for each deployment to
enable efficient data collection. During each offline training session, we
bootstrap the policy update by quantifying the amount of uncertainty within our
collected data. In the high support region (low uncertainty), we encourage our
policy by taking an aggressive update. In the low support region (high
uncertainty) when the policy bootstraps into the out-of-distribution region, we
downweight it by our estimated uncertainty quantification. Experimental results
show that MUSBO achieves state-of-the-art performance in the deployment
constrained RL setting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Su_D/0/1/0/all/0/1"&gt;DiJia Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jason D. Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mulvey_J/0/1/0/all/0/1"&gt;John M. Mulvey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Poor_H/0/1/0/all/0/1"&gt;H. Vincent Poor&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FjORD: Fair and Accurate Federated Learning under heterogeneous targets with Ordered Dropout. (arXiv:2102.13451v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.13451</id>
        <link href="http://arxiv.org/abs/2102.13451"/>
        <updated>2021-06-07T03:06:16.751Z</updated>
        <summary type="html"><![CDATA[Federated Learning (FL) has been gaining significant traction across
different ML tasks, ranging from vision to keyboard predictions. In large-scale
deployments, client heterogeneity is a fact, and constitutes a primary problem
for fairness, training performance and accuracy. Although significant efforts
have been made into tackling statistical data heterogeneity, the diversity in
the processing capabilities and network bandwidth of clients, termed as system
heterogeneity, has remained largely unexplored. Current solutions either
disregard a large portion of available devices or set a uniform limit on the
model's capacity, restricted by the least capable participants. In this work,
we introduce Ordered Dropout, a mechanism that achieves an ordered, nested
representation of knowledge in Neural Networks and enables the extraction of
lower footprint submodels without the need of retraining. We further show that
for linear maps our Ordered Dropout is equivalent to SVD. We employ this
technique, along with a self-distillation methodology, in the realm of FL in a
framework called FjORD. FjORD alleviates the problem of client system
heterogeneity by tailoring the model width to the client's capabilities.
Extensive evaluation on both CNNs and RNNs across diverse modalities shows that
FjORD consistently leads to significant performance gains over state-of-the-art
baselines, while maintaining its nested structure.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Horvath_S/0/1/0/all/0/1"&gt;Samuel Horvath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Laskaridis_S/0/1/0/all/0/1"&gt;Stefanos Laskaridis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Almeida_M/0/1/0/all/0/1"&gt;Mario Almeida&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leontiadis_I/0/1/0/all/0/1"&gt;Ilias Leontiadis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Venieris_S/0/1/0/all/0/1"&gt;Stylianos I. Venieris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lane_N/0/1/0/all/0/1"&gt;Nicholas D. Lane&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[KACC: A Multi-task Benchmark for Knowledge Abstraction, Concretization and Completion. (arXiv:2004.13631v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.13631</id>
        <link href="http://arxiv.org/abs/2004.13631"/>
        <updated>2021-06-07T03:06:16.744Z</updated>
        <summary type="html"><![CDATA[A comprehensive knowledge graph (KG) contains an instance-level entity graph
and an ontology-level concept graph. The two-view KG provides a testbed for
models to "simulate" human's abilities on knowledge abstraction,
concretization, and completion (KACC), which are crucial for human to recognize
the world and manage learned knowledge. Existing studies mainly focus on
partial aspects of KACC. In order to promote thorough analyses for KACC
abilities of models, we propose a unified KG benchmark by improving existing
benchmarks in terms of dataset scale, task coverage, and difficulty.
Specifically, we collect new datasets that contain larger concept graphs,
abundant cross-view links as well as dense entity graphs. Based on the
datasets, we propose novel tasks such as multi-hop knowledge abstraction (MKA),
multi-hop knowledge concretization (MKC) and then design a comprehensive
benchmark. For MKA and MKC tasks, we further annotate multi-hop hierarchical
triples as harder samples. The experimental results of existing methods
demonstrate the challenges of our benchmark. The resource is available at
https://github.com/thunlp/KACC.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jie Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1"&gt;Shengding Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lv_X/0/1/0/all/0/1"&gt;Xin Lv&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1"&gt;Cheng Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhiyuan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1"&gt;Wei Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1"&gt;Jie Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Juanzi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1"&gt;Maosong Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning One Representation to Optimize All Rewards. (arXiv:2103.07945v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.07945</id>
        <link href="http://arxiv.org/abs/2103.07945"/>
        <updated>2021-06-07T03:06:16.728Z</updated>
        <summary type="html"><![CDATA[We introduce the forward-backward (FB) representation of the dynamics of a
reward-free Markov decision process. It provides explicit near-optimal policies
for any reward specified a posteriori. During an unsupervised phase, we use
reward-free interactions with the environment to learn two representations via
off-the-shelf deep learning methods and temporal difference (TD) learning. In
the test phase, a reward representation is estimated either from observations
or an explicit reward description (e.g., a target state). The optimal policy
for that reward is directly obtained from these representations, with no
planning. We assume access to an exploration scheme or replay buffer for the
first phase.

The unsupervised FB loss is well-principled: if training is perfect, the
policies obtained are provably optimal for any reward function. With imperfect
training, the sub-optimality is proportional to the unsupervised approximation
error. The FB representation learns long-range relationships between states and
actions, via a predictive occupancy map, without having to synthesize states as
in model-based approaches.

This is a step towards learning controllable agents in arbitrary black-box
stochastic environments. This approach compares well to goal-oriented RL
algorithms on discrete and continuous mazes, pixel-based MsPacman, and the
FetchReach virtual robot arm. We also illustrate how the agent can immediately
adapt to new tasks beyond goal-oriented RL.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Touati_A/0/1/0/all/0/1"&gt;Ahmed Touati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ollivier_Y/0/1/0/all/0/1"&gt;Yann Ollivier&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Contrastive Learning for Debiased Candidate Generation in Large-Scale Recommender Systems. (arXiv:2005.12964v9 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.12964</id>
        <link href="http://arxiv.org/abs/2005.12964"/>
        <updated>2021-06-07T03:06:16.706Z</updated>
        <summary type="html"><![CDATA[Deep candidate generation (DCG) that narrows down the collection of relevant
items from billions to hundreds via representation learning has become
prevalent in industrial recommender systems. Standard approaches approximate
maximum likelihood estimation (MLE) through sampling for better scalability and
address the problem of DCG in a way similar to language modeling. However, live
recommender systems face severe exposure bias and have a vocabulary several
orders of magnitude larger than that of natural language, implying that MLE
will preserve and even exacerbate the exposure bias in the long run in order to
faithfully fit the observed samples. In this paper, we theoretically prove that
a popular choice of contrastive loss is equivalent to reducing the exposure
bias via inverse propensity weighting, which provides a new perspective for
understanding the effectiveness of contrastive learning. Based on the
theoretical discovery, we design CLRec, a contrastive learning method to
improve DCG in terms of fairness, effectiveness and efficiency in recommender
systems with extremely large candidate size. We further improve upon CLRec and
propose Multi-CLRec, for accurate multi-intention aware bias reduction. Our
methods have been successfully deployed in Taobao, where at least four-month
online A/B tests and offline analyses demonstrate its substantial improvements,
including a dramatic reduction in the Matthew effect.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1"&gt;Chang Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1"&gt;Jianxin Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jianwei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jingren Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Hongxia Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Online Convex Optimization Perspective for Learning from Dynamically Revealed Preferences. (arXiv:2008.10460v3 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.10460</id>
        <link href="http://arxiv.org/abs/2008.10460"/>
        <updated>2021-06-07T03:06:16.665Z</updated>
        <summary type="html"><![CDATA[We study the problem of online learning (OL) from revealed preferences: a
learner wishes to learn a non-strategic agent's private utility function
through observing the agent's utility-maximizing actions in a changing
environment. We adopt an online inverse optimization setup, where the learner
observes a stream of agent's actions in an online fashion and the learning
performance is measured by regret associated with a loss function. We first
characterize a special but broad class of agent's utility functions, then
utilize this structure in designing a new convex loss function. We establish
that the regret with respect to our new loss function also bounds the regret
with respect to all other usual loss functions in the literature. This allows
us to design a flexible OL framework that enables a unified treatment of loss
functions and supports a variety of online convex optimization algorithms. We
demonstrate with theoretical and empirical evidence that our framework based on
the new loss function (in particular online Mirror Descent) has significant
advantages in terms of regret performance and solution time over other OL
algorithms from the literature and bypasses the previous technical assumptions
as well.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Chen_V/0/1/0/all/0/1"&gt;Violet Xinying Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Kilinc_Karzan_F/0/1/0/all/0/1"&gt;Fatma K&amp;#x131;l&amp;#x131;n&amp;#xe7;-Karzan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Active Covering. (arXiv:2106.02552v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02552</id>
        <link href="http://arxiv.org/abs/2106.02552"/>
        <updated>2021-06-07T03:06:16.658Z</updated>
        <summary type="html"><![CDATA[We analyze the problem of active covering, where the learner is given an
unlabeled dataset and can sequentially label query examples. The objective is
to label query all of the positive examples in the fewest number of total label
queries. We show under standard non-parametric assumptions that a classical
support estimator can be repurposed as an offline algorithm attaining an excess
query cost of $\widetilde{\Theta}(n^{D/(D+1)})$ compared to the optimal
learner, where $n$ is the number of datapoints and $D$ is the dimension. We
then provide a simple active learning method that attains an improved excess
query cost of $\widetilde{O}(n^{(D-1)/D})$. Furthermore, the proposed
algorithms only require access to the positive labeled examples, which in
certain settings provides additional computational and privacy benefits.
Finally, we show that the active learning method consistently outperforms
offline methods as well as a variety of baselines on a wide range of benchmark
image-based datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1"&gt;Heinrich Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rostamizadeh_A/0/1/0/all/0/1"&gt;Afshin Rostamizadeh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Equivalence of Dataflow Graphs via Rewrite Rules Using a Graph-to-Sequence Neural Model. (arXiv:2002.06799v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.06799</id>
        <link href="http://arxiv.org/abs/2002.06799"/>
        <updated>2021-06-07T03:06:16.651Z</updated>
        <summary type="html"><![CDATA[In this work we target the problem of provably computing the equivalence
between two programs represented as dataflow graphs. To this end, we formalize
the problem of equivalence between two programs as finding a set of
semantics-preserving rewrite rules from one into the other, such that after the
rewrite the two programs are structurally identical, and therefore trivially
equivalent. We then develop the first graph-to-sequence neural network system
for program equivalence, trained to produce such rewrite sequences from a
carefully crafted automatic example generation algorithm. We extensively
evaluate our system on a rich multi-type linear algebra expression language,
using arbitrary combinations of 100+ graph-rewriting axioms of equivalence. Our
system outputs via inference a correct rewrite sequence for 96% of the 10,000
program pairs isolated for testing, using 30-term programs. And in all cases,
the validity of the sequence produced and therefore the provable assertion of
program equivalence is computable, in negligible time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kommrusch_S/0/1/0/all/0/1"&gt;Steve Kommrusch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barollet_T/0/1/0/all/0/1"&gt;Th&amp;#xe9;o Barollet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pouchet_L/0/1/0/all/0/1"&gt;Louis-No&amp;#xeb;l Pouchet&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MERLOT: Multimodal Neural Script Knowledge Models. (arXiv:2106.02636v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02636</id>
        <link href="http://arxiv.org/abs/2106.02636"/>
        <updated>2021-06-07T03:06:16.634Z</updated>
        <summary type="html"><![CDATA[As humans, we understand events in the visual world contextually, performing
multimodal reasoning across time to make inferences about the past, present,
and future. We introduce MERLOT, a model that learns multimodal script
knowledge by watching millions of YouTube videos with transcribed speech -- in
an entirely label-free, self-supervised manner. By pretraining with a mix of
both frame-level (spatial) and video-level (temporal) objectives, our model not
only learns to match images to temporally corresponding words, but also to
contextualize what is happening globally over time. As a result, MERLOT
exhibits strong out-of-the-box representations of temporal commonsense, and
achieves state-of-the-art performance on 12 different video QA datasets when
finetuned. It also transfers well to the world of static images, allowing
models to reason about the dynamic context behind visual scenes. On Visual
Commonsense Reasoning, MERLOT answers questions correctly with 80.6% accuracy,
outperforming state-of-the-art models of similar size by over 3%, even those
that make heavy use of auxiliary supervised data (like object bounding boxes).

Ablation analyses demonstrate the complementary importance of: 1) training on
videos versus static images; 2) scaling the magnitude and diversity of the
pretraining video corpus; and 3) using diverse objectives that encourage
full-stack multimodal reasoning, from the recognition to cognition level.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zellers_R/0/1/0/all/0/1"&gt;Rowan Zellers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1"&gt;Ximing Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hessel_J/0/1/0/all/0/1"&gt;Jack Hessel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1"&gt;Youngjae Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1"&gt;Jae Sung Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1"&gt;Jize Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Farhadi_A/0/1/0/all/0/1"&gt;Ali Farhadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1"&gt;Yejin Choi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Estimation of Derivatives Using Plug-in KRR Estimators. (arXiv:2006.01350v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.01350</id>
        <link href="http://arxiv.org/abs/2006.01350"/>
        <updated>2021-06-07T03:06:16.624Z</updated>
        <summary type="html"><![CDATA[We study the problem of estimating the derivatives of the regression
function, which has a wide range of applications as a key nonparametric
functional of unknown functions. Standard analysis may be tailored to specific
derivative orders, and parameter tuning remains a daunting challenge
particularly for high-order derivatives. In this article, we propose a simple
plug-in kernel ridge regression (KRR) estimator in nonparametric regression
with random design that is broadly applicable for multi-dimensional support and
arbitrary mixed-partial derivatives. We provide a non-asymptotic analysis to
study the behavior of the proposed estimator, leading to two error bounds for a
general class of kernels under the strong $L_\infty$ norm. In a concrete
example specialized to kernels with polynomially decaying eigenvalues, the
proposed estimator recovers the minimax optimal rate up to a logarithmic factor
for estimating derivatives of functions in H\"older class. Interestingly, the
proposed estimator achieves the optimal rate of convergence with the same
choice of tuning parameter for any order of derivatives. Hence, the proposed
estimator enjoys a remarkable \textit{plug-in property} for derivatives in that
it automatically adapts to the order of derivatives to be estimated, enabling
easy tuning in practice. Our simulation studies show favorable finite sample
performance of the proposed method relative to several existing methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zejian Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Li_M/0/1/0/all/0/1"&gt;Meng Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Aggregation Functions. (arXiv:2012.08482v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.08482</id>
        <link href="http://arxiv.org/abs/2012.08482"/>
        <updated>2021-06-07T03:06:16.611Z</updated>
        <summary type="html"><![CDATA[Learning on sets is increasingly gaining attention in the machine learning
community, due to its widespread applicability. Typically, representations over
sets are computed by using fixed aggregation functions such as sum or maximum.
However, recent results showed that universal function representation by sum-
(or max-) decomposition requires either highly discontinuous (and thus poorly
learnable) mappings, or a latent dimension equal to the maximum number of
elements in the set. To mitigate this problem, we introduce a learnable
aggregation function (LAF) for sets of arbitrary cardinality. LAF can
approximate several extensively used aggregators (such as average, sum,
maximum) as well as more complex functions (e.g., variance and skewness). We
report experiments on semi-synthetic and real data showing that LAF outperforms
state-of-the-art sum- (max-) decomposition architectures such as DeepSets and
library-based architectures like Principal Neighborhood Aggregation, and can be
effectively combined with attention-based architectures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pellegrini_G/0/1/0/all/0/1"&gt;Giovanni Pellegrini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tibo_A/0/1/0/all/0/1"&gt;Alessandro Tibo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frasconi_P/0/1/0/all/0/1"&gt;Paolo Frasconi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Passerini_A/0/1/0/all/0/1"&gt;Andrea Passerini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jaeger_M/0/1/0/all/0/1"&gt;Manfred Jaeger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Editing Conditional Radiance Fields. (arXiv:2105.06466v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.06466</id>
        <link href="http://arxiv.org/abs/2105.06466"/>
        <updated>2021-06-07T03:06:16.515Z</updated>
        <summary type="html"><![CDATA[A neural radiance field (NeRF) is a scene model supporting high-quality view
synthesis, optimized per scene. In this paper, we explore enabling user editing
of a category-level NeRF - also known as a conditional radiance field - trained
on a shape category. Specifically, we introduce a method for propagating coarse
2D user scribbles to the 3D space, to modify the color or shape of a local
region. First, we propose a conditional radiance field that incorporates new
modular network components, including a shape branch that is shared across
object instances. Observing multiple instances of the same category, our model
learns underlying part semantics without any supervision, thereby allowing the
propagation of coarse 2D user scribbles to the entire 3D region (e.g., chair
seat). Next, we propose a hybrid network update strategy that targets specific
network components, which balances efficiency and accuracy. During user
interaction, we formulate an optimization problem that both satisfies the
user's constraints and preserves the original object structure. We demonstrate
our approach on various editing tasks over three shape datasets and show that
it outperforms prior neural editing approaches. Finally, we edit the appearance
and shape of a real photograph and show that the edit propagates to
extrapolated novel views.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Steven Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiuming Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhoutong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Richard Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jun-Yan Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Russell_B/0/1/0/all/0/1"&gt;Bryan Russell&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Importance Weighted Policy Learning and Adaptation. (arXiv:2009.04875v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.04875</id>
        <link href="http://arxiv.org/abs/2009.04875"/>
        <updated>2021-06-07T03:06:16.506Z</updated>
        <summary type="html"><![CDATA[The ability to exploit prior experience to solve novel problems rapidly is a
hallmark of biological learning systems and of great practical importance for
artificial ones. In the meta reinforcement learning literature much recent work
has focused on the problem of optimizing the learning process itself. In this
paper we study a complementary approach which is conceptually simple, general,
modular and built on top of recent improvements in off-policy learning. The
framework is inspired by ideas from the probabilistic inference literature and
combines robust off-policy learning with a behavior prior, or default behavior
that constrains the space of solutions and serves as a bias for exploration; as
well as a representation for the value function, both of which are easily
learned from a number of training tasks in a multi-task scenario. Our approach
achieves competitive adaptation performance on hold-out tasks compared to meta
reinforcement learning baselines and can scale to complex sparse-reward
scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Galashov_A/0/1/0/all/0/1"&gt;Alexandre Galashov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sygnowski_J/0/1/0/all/0/1"&gt;Jakub Sygnowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Desjardins_G/0/1/0/all/0/1"&gt;Guillaume Desjardins&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Humplik_J/0/1/0/all/0/1"&gt;Jan Humplik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hasenclever_L/0/1/0/all/0/1"&gt;Leonard Hasenclever&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jeong_R/0/1/0/all/0/1"&gt;Rae Jeong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Teh_Y/0/1/0/all/0/1"&gt;Yee Whye Teh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heess_N/0/1/0/all/0/1"&gt;Nicolas Heess&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Some observations on high-dimensional partial differential equations with Barron data. (arXiv:2012.01484v3 [math.AP] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.01484</id>
        <link href="http://arxiv.org/abs/2012.01484"/>
        <updated>2021-06-07T03:06:16.243Z</updated>
        <summary type="html"><![CDATA[We use explicit representation formulas to show that solutions to certain
partial differential equations lie in Barron spaces or multilayer spaces if the
PDE data lie in such function spaces. Consequently, these solutions can be
represented efficiently using artificial neural networks, even in high
dimension. Conversely, we present examples in which the solution fails to lie
in the function space associated to a neural network under consideration.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+E_W/0/1/0/all/0/1"&gt;Weinan E&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Wojtowytsch_S/0/1/0/all/0/1"&gt;Stephan Wojtowytsch&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Why does CTC result in peaky behavior?. (arXiv:2105.14849v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14849</id>
        <link href="http://arxiv.org/abs/2105.14849"/>
        <updated>2021-06-07T03:06:16.242Z</updated>
        <summary type="html"><![CDATA[The peaky behavior of CTC models is well known experimentally. However, an
understanding about why peaky behavior occurs is missing, and whether this is a
good property. We provide a formal analysis of the peaky behavior and gradient
descent convergence properties of the CTC loss and related training criteria.
Our analysis provides a deep understanding why peaky behavior occurs and when
it is suboptimal. On a simple example which should be trivial to learn for any
model, we prove that a feed-forward neural network trained with CTC from
uniform initialization converges towards peaky behavior with a 100% error rate.
Our analysis further explains why CTC only works well together with the blank
label. We further demonstrate that peaky behavior does not occur on other
related losses including a label prior model, and that this improves
convergence.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zeyer_A/0/1/0/all/0/1"&gt;Albert Zeyer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schluter_R/0/1/0/all/0/1"&gt;Ralf Schl&amp;#xfc;ter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ney_H/0/1/0/all/0/1"&gt;Hermann Ney&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Regular Expressions for Fast-response COVID-19 Text Classification. (arXiv:2102.09507v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.09507</id>
        <link href="http://arxiv.org/abs/2102.09507"/>
        <updated>2021-06-07T03:06:16.231Z</updated>
        <summary type="html"><![CDATA[Text classifiers are at the core of many NLP applications and use a variety
of algorithmic approaches and software. This paper introduces infrastructure
and methodologies for text classifiers based on large-scale regular
expressions. In particular, we describe how Facebook determines if a given
piece of text - anything from a hashtag to a post - belongs to a narrow topic
such as COVID-19. To fully define a topic and evaluate classifier performance
we employ human-guided iterations of keyword discovery, but do not require
labeled data. For COVID-19, we build two sets of regular expressions: (1) for
66 languages, with 99% precision and recall >50%, (2) for the 11 most common
languages, with precision >90% and recall >90%. Regular expressions enable
low-latency queries from multiple platforms. Response to challenges like
COVID-19 is fast and so are revisions. Comparisons to a DNN classifier show
explainable results, higher precision and recall, and less overfitting. Our
learnings can be applied to other narrow-topic classifiers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Markov_I/0/1/0/all/0/1"&gt;Igor L. Markov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jacqueline Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vagner_A/0/1/0/all/0/1"&gt;Adam Vagner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Model-agnostic and Scalable Counterfactual Explanations via Reinforcement Learning. (arXiv:2106.02597v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02597</id>
        <link href="http://arxiv.org/abs/2106.02597"/>
        <updated>2021-06-07T03:06:16.230Z</updated>
        <summary type="html"><![CDATA[Counterfactual instances are a powerful tool to obtain valuable insights into
automated decision processes, describing the necessary minimal changes in the
input space to alter the prediction towards a desired target. Most previous
approaches require a separate, computationally expensive optimization procedure
per instance, making them impractical for both large amounts of data and
high-dimensional data. Moreover, these methods are often restricted to certain
subclasses of machine learning models (e.g. differentiable or tree-based
models). In this work, we propose a deep reinforcement learning approach that
transforms the optimization procedure into an end-to-end learnable process,
allowing us to generate batches of counterfactual instances in a single forward
pass. Our experiments on real-world data show that our method i) is
model-agnostic (does not assume differentiability), relying only on feedback
from model predictions; ii) allows for generating target-conditional
counterfactual instances; iii) allows for flexible feature range constraints
for numerical and categorical attributes, including the immutability of
protected features (e.g. gender, race); iv) is easily extended to other data
modalities such as images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Samoilescu_R/0/1/0/all/0/1"&gt;Robert-Florian Samoilescu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Looveren_A/0/1/0/all/0/1"&gt;Arnaud Van Looveren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Klaise_J/0/1/0/all/0/1"&gt;Janis Klaise&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NF-GNN: Network Flow Graph Neural Networks for Malware Detection and Classification. (arXiv:2103.03939v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.03939</id>
        <link href="http://arxiv.org/abs/2103.03939"/>
        <updated>2021-06-07T03:06:16.224Z</updated>
        <summary type="html"><![CDATA[Malicious software (malware) poses an increasing threat to the security of
communication systems as the number of interconnected mobile devices increases
exponentially. While some existing malware detection and classification
approaches successfully leverage network traffic data, they treat network flows
between pairs of endpoints independently and thus fail to leverage rich
communication patterns present in the complete network. Our approach first
extracts flow graphs and subsequently classifies them using a novel edge
feature-based graph neural network model. We present three variants of our base
model, which support malware detection and classification in supervised and
unsupervised settings. We evaluate our approach on flow graphs that we extract
from a recently published dataset for mobile malware detection that addresses
several issues with previously available datasets. Experiments on four
different prediction tasks consistently demonstrate the advantages of our
approach and show that our graph neural network model can boost detection
performance by a significant margin.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Busch_J/0/1/0/all/0/1"&gt;Julian Busch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kocheturov_A/0/1/0/all/0/1"&gt;Anton Kocheturov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tresp_V/0/1/0/all/0/1"&gt;Volker Tresp&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seidl_T/0/1/0/all/0/1"&gt;Thomas Seidl&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Maximum Likelihood Training of Score-Based Diffusion Models. (arXiv:2101.09258v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.09258</id>
        <link href="http://arxiv.org/abs/2101.09258"/>
        <updated>2021-06-07T03:06:16.211Z</updated>
        <summary type="html"><![CDATA[Score-based diffusion models synthesize samples by reversing a stochastic
process that diffuses data to noise, and are trained by minimizing a weighted
combination of score matching losses. The log-likelihood of score-based models
can be tractably computed through a connection to continuous normalizing flows,
but log-likelihood is not directly optimized by the weighted combination of
score matching losses. We show that for a specific weighting scheme, the
objective upper bounds the negative log-likelihood, thus enabling approximate
maximum likelihood training of score-based models. We empirically observe that
maximum likelihood training consistently improves the likelihood of score-based
models across multiple datasets, stochastic processes, and model architectures.
Our best models achieve negative log-likelihoods of 2.74 and 3.76 bits/dim on
CIFAR-10 and down-sampled ImageNet, outperforming all existing likelihood-based
models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Song_Y/0/1/0/all/0/1"&gt;Yang Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Durkan_C/0/1/0/all/0/1"&gt;Conor Durkan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Murray_I/0/1/0/all/0/1"&gt;Iain Murray&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ermon_S/0/1/0/all/0/1"&gt;Stefano Ermon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Alert Classification for the ALeRCE Broker System: The Real-time Stamp Classifier. (arXiv:2008.03309v2 [astro-ph.IM] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.03309</id>
        <link href="http://arxiv.org/abs/2008.03309"/>
        <updated>2021-06-07T03:06:16.204Z</updated>
        <summary type="html"><![CDATA[We present a real-time stamp classifier of astronomical events for the ALeRCE
(Automatic Learning for the Rapid Classification of Events) broker. The
classifier is based on a convolutional neural network, trained on alerts
ingested from the Zwicky Transient Facility (ZTF). Using only the
\textit{science, reference} and \textit{difference} images of the first
detection as inputs, along with the metadata of the alert as features, the
classifier is able to correctly classify alerts from active galactic nuclei,
supernovae (SNe), variable stars, asteroids and bogus classes, with high
accuracy ($\sim$94\%) in a balanced test set. In order to find and analyze SN
candidates selected by our classifier from the ZTF alert stream, we designed
and deployed a visualization tool called SN Hunter, where relevant information
about each possible SN is displayed for the experts to choose among candidates
to report to the Transient Name Server database. From June 26th 2019 to
February 28th 2021, we have reported 6846 SN candidates to date (11.8
candidates per day on average), of which 971 have been confirmed
spectroscopically. Our ability to report objects using only a single detection
means that 70\% of the reported SNe occurred within one day after the first
detection. ALeRCE has only reported candidates not otherwise detected or
selected by other groups, therefore adding new early transients to the bulk of
objects available for early follow-up. Our work represents an important
milestone toward rapid alert classifications with the next generation of large
etendue telescopes, such as the Vera C. Rubin Observatory.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/astro-ph/1/au:+Carrasco_Davis_R/0/1/0/all/0/1"&gt;Rodrigo Carrasco-Davis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Reyes_E/0/1/0/all/0/1"&gt;Esteban Reyes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Valenzuela_C/0/1/0/all/0/1"&gt;Camilo Valenzuela&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Forster_F/0/1/0/all/0/1"&gt;Francisco F&amp;#xf6;rster&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Estevez_P/0/1/0/all/0/1"&gt;Pablo A. Est&amp;#xe9;vez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Pignata_G/0/1/0/all/0/1"&gt;Giuliano Pignata&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Bauer_F/0/1/0/all/0/1"&gt;Franz E. Bauer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Reyes_I/0/1/0/all/0/1"&gt;Ignacio Reyes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Sanchez_Saez_P/0/1/0/all/0/1"&gt;Paula S&amp;#xe1;nchez-S&amp;#xe1;ez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Cabrera_Vives_G/0/1/0/all/0/1"&gt;Guillermo Cabrera-Vives&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Eyheramendy_S/0/1/0/all/0/1"&gt;Susana Eyheramendy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Catelan_M/0/1/0/all/0/1"&gt;M&amp;#xe1;rcio Catelan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Arredondo_J/0/1/0/all/0/1"&gt;Javier Arredondo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Castillo_Navarrete_E/0/1/0/all/0/1"&gt;Ernesto Castillo-Navarrete&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Rodriguez_Mancini_D/0/1/0/all/0/1"&gt;Diego Rodr&amp;#xed;guez-Mancini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Ruz_Mieres_D/0/1/0/all/0/1"&gt;Daniela Ruz-Mieres&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Moya_A/0/1/0/all/0/1"&gt;Alberto Moya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Sabatini_Gacitua_L/0/1/0/all/0/1"&gt;Luis Sabatini-Gacit&amp;#xfa;a&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Sepulveda_Cobo_C/0/1/0/all/0/1"&gt;Crist&amp;#xf3;bal Sep&amp;#xfa;lveda-Cobo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Mahabal_A/0/1/0/all/0/1"&gt;Ashish A. Mahabal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Silva_Farfan_J/0/1/0/all/0/1"&gt;Javier Silva-Farf&amp;#xe1;n&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Camacho_Iniquez_E/0/1/0/all/0/1"&gt;Ernesto Camacho-I&amp;#xf1;iquez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Galbany_L/0/1/0/all/0/1"&gt;Llu&amp;#xed;s Galbany&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Defending Democracy: Using Deep Learning to Identify and Prevent Misinformation. (arXiv:2106.02607v1 [cs.SI])]]></title>
        <id>http://arxiv.org/abs/2106.02607</id>
        <link href="http://arxiv.org/abs/2106.02607"/>
        <updated>2021-06-07T03:06:16.183Z</updated>
        <summary type="html"><![CDATA[The rise in online misinformation in recent years threatens democracies by
distorting authentic public discourse and causing confusion, fear, and even, in
extreme cases, violence. There is a need to understand the spread of false
content through online networks for developing interventions that disrupt
misinformation before it achieves virality. Using a Deep Bidirectional
Transformer for Language Understanding (BERT) and propagation graphs, this
study classifies and visualizes the spread of misinformation on a social media
network using publicly available Twitter data. The results confirm prior
research around user clusters and the virality of false content while improving
the precision of deep learning models for misinformation detection. The study
further demonstrates the suitability of BERT for providing a scalable model for
false information detection, which can contribute to the development of more
timely and accurate interventions to slow the spread of misinformation in
online environments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Trivedi_A/0/1/0/all/0/1"&gt;Anusua Trivedi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suhm_A/0/1/0/all/0/1"&gt;Alyssa Suhm&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mahankal_P/0/1/0/all/0/1"&gt;Prathamesh Mahankal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mukuntharaj_S/0/1/0/all/0/1"&gt;Subhiksha Mukuntharaj&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parab_M/0/1/0/all/0/1"&gt;Meghana D. Parab&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mohan_M/0/1/0/all/0/1"&gt;Malvika Mohan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berger_M/0/1/0/all/0/1"&gt;Meredith Berger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sethumadhavan_A/0/1/0/all/0/1"&gt;Arathi Sethumadhavan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jaiman_A/0/1/0/all/0/1"&gt;Ashish Jaiman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dodhia_R/0/1/0/all/0/1"&gt;Rahul Dodhia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Image Enhanced Rotation Prediction for Self-Supervised Learning. (arXiv:1912.11603v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1912.11603</id>
        <link href="http://arxiv.org/abs/1912.11603"/>
        <updated>2021-06-07T03:06:16.176Z</updated>
        <summary type="html"><![CDATA[The rotation prediction (Rotation) is a simple pretext-task for
self-supervised learning (SSL), where models learn useful representations for
target vision tasks by solving pretext-tasks. Although Rotation captures
information of object shapes, it hardly captures information of textures. To
tackle this problem, we introduce a novel pretext-task called image enhanced
rotation prediction (IE-Rot) for SSL. IE-Rot simultaneously solves Rotation and
another pretext-task based on image enhancement (e.g., sharpening and
solarizing) while maintaining simplicity. Through the simultaneous prediction
of rotation and image enhancement, models learn representations to capture the
information of not only object shapes but also textures. Our experimental
results show that IE-Rot models outperform Rotation on various standard
benchmarks including ImageNet classification, PASCAL-VOC detection, and COCO
detection/segmentation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Yamaguchi_S/0/1/0/all/0/1"&gt;Shin&amp;#x27;ya Yamaguchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kanai_S/0/1/0/all/0/1"&gt;Sekitoshi Kanai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Shioda_T/0/1/0/all/0/1"&gt;Tetsuya Shioda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Takeda_S/0/1/0/all/0/1"&gt;Shoichiro Takeda&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bilinear Classes: A Structural Framework for Provable Generalization in RL. (arXiv:2103.10897v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.10897</id>
        <link href="http://arxiv.org/abs/2103.10897"/>
        <updated>2021-06-07T03:06:16.169Z</updated>
        <summary type="html"><![CDATA[This work introduces Bilinear Classes, a new structural framework, which
permit generalization in reinforcement learning in a wide variety of settings
through the use of function approximation. The framework incorporates nearly
all existing models in which a polynomial sample complexity is achievable, and,
notably, also includes new models, such as the Linear $Q^*/V^*$ model in which
both the optimal $Q$-function and the optimal $V$-function are linear in some
known feature space. Our main result provides an RL algorithm which has
polynomial sample complexity for Bilinear Classes; notably, this sample
complexity is stated in terms of a reduction to the generalization error of an
underlying supervised learning sub-problem. These bounds nearly match the best
known sample complexity bounds for existing models. Furthermore, this framework
also extends to the infinite dimensional (RKHS) setting: for the the Linear
$Q^*/V^*$ model, linear MDPs, and linear mixture MDPs, we provide sample
complexities that have no explicit dependence on the explicit feature dimension
(which could be infinite), but instead depends only on information theoretic
quantities.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Du_S/0/1/0/all/0/1"&gt;Simon S. Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kakade_S/0/1/0/all/0/1"&gt;Sham M. Kakade&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jason D. Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lovett_S/0/1/0/all/0/1"&gt;Shachar Lovett&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mahajan_G/0/1/0/all/0/1"&gt;Gaurav Mahajan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1"&gt;Wen Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1"&gt;Ruosong Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fundamental Limits of Prediction, Generalization, and Recursion: An Entropic-Innovations Perspective. (arXiv:2001.03813v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2001.03813</id>
        <link href="http://arxiv.org/abs/2001.03813"/>
        <updated>2021-06-07T03:06:16.162Z</updated>
        <summary type="html"><![CDATA[In this paper, we examine the fundamental performance limits of prediction,
with or without side information. More specifically, we derive generic lower
bounds on the $\mathcal{L}_p$ norms of the prediction errors that are valid for
any prediction algorithms and for any data distributions. Meanwhile, we combine
the entropic analysis from information theory and the innovations approach from
prediction/estimation theory to characterize the conditions (in terms of, e.g.,
directed information or mutual information) to achieve the bounds. We also
investigate the implications of the results in analyzing the fundamental limits
of generalization in fitting (learning) problems from the perspective of
prediction with side information, as well as the fundamental limits of
recursive algorithms by viewing them as generalized prediction problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fang_S/0/1/0/all/0/1"&gt;Song Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1"&gt;Quanyan Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Slice Low-Rank Tensor Decomposition Based Multi-Atlas Segmentation: Application to Automatic Pathological Liver CT Segmentation. (arXiv:2102.12056v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.12056</id>
        <link href="http://arxiv.org/abs/2102.12056"/>
        <updated>2021-06-07T03:06:16.150Z</updated>
        <summary type="html"><![CDATA[Liver segmentation from abdominal CT images is an essential step for liver
cancer computer-aided diagnosis and surgical planning. However, both the
accuracy and robustness of existing liver segmentation methods cannot meet the
requirements of clinical applications. In particular, for the common clinical
cases where the liver tissue contains major pathology, current segmentation
methods show poor performance. In this paper, we propose a novel low-rank
tensor decomposition (LRTD) based multi-atlas segmentation (MAS) framework that
achieves accurate and robust pathological liver segmentation of CT images.
Firstly, we propose a multi-slice LRTD scheme to recover the underlying
low-rank structure embedded in 3D medical images. It performs the LRTD on small
image segments consisting of multiple consecutive image slices. Then, we
present an LRTD-based atlas construction method to generate tumor-free liver
atlases that mitigates the performance degradation of liver segmentation due to
the presence of tumors. Finally, we introduce an LRTD-based MAS algorithm to
derive patient-specific liver atlases for each test image, and to achieve
accurate pairwise image registration and label propagation. Extensive
experiments on three public databases of pathological liver cases validate the
effectiveness of the proposed method. Both qualitative and quantitative results
demonstrate that, in the presence of major pathology, the proposed method is
more accurate and robust than state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Shi_C/0/1/0/all/0/1"&gt;Changfa Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xian_M/0/1/0/all/0/1"&gt;Min Xian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhou_X/0/1/0/all/0/1"&gt;Xiancheng Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_H/0/1/0/all/0/1"&gt;Haotian Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cheng_H/0/1/0/all/0/1"&gt;Heng-Da Cheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Difficulty of Unbiased Alpha Divergence Minimization. (arXiv:2010.09541v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.09541</id>
        <link href="http://arxiv.org/abs/2010.09541"/>
        <updated>2021-06-07T03:06:16.132Z</updated>
        <summary type="html"><![CDATA[Several approximate inference algorithms have been proposed to minimize an
alpha-divergence between an approximating distribution and a target
distribution. Many of these algorithms introduce bias, the magnitude of which
becomes problematic in high dimensions. Other algorithms are unbiased. These
often seem to suffer from high variance, but little is rigorously known. In
this work we study unbiased methods for alpha-divergence minimization through
the Signal-to-Noise Ratio (SNR) of the gradient estimator. We study several
representative scenarios where strong analytical results are possible, such as
fully-factorized or Gaussian distributions. We find that when alpha is not
zero, the SNR worsens exponentially in the dimensionality of the problem. This
casts doubt on the practicality of these methods. We empirically confirm these
theoretical results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Geffner_T/0/1/0/all/0/1"&gt;Tomas Geffner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Domke_J/0/1/0/all/0/1"&gt;Justin Domke&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stochastic Iterative Graph Matching. (arXiv:2106.02206v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02206</id>
        <link href="http://arxiv.org/abs/2106.02206"/>
        <updated>2021-06-07T03:06:16.126Z</updated>
        <summary type="html"><![CDATA[Recent works leveraging Graph Neural Networks to approach graph matching
tasks have shown promising results. Recent progress in learning discrete
distributions poses new opportunities for learning graph matching models. In
this work, we propose a new model, Stochastic Iterative Graph MAtching (SIGMA),
to address the graph matching problem. Our model defines a distribution of
matchings for a graph pair so the model can explore a wide range of possible
matchings. We further introduce a novel multi-step matching procedure, which
learns how to refine a graph pair's matching results incrementally. The model
also includes dummy nodes so that the model does not have to find matchings for
nodes without correspondence. We fit this model to data via scalable stochastic
optimization. We conduct extensive experiments across synthetic graph datasets
as well as biochemistry and computer vision applications. Across all tasks, our
results show that SIGMA can produce significantly improved graph matching
results compared to state-of-the-art models. Ablation studies verify that each
of our components (stochastic training, iterative matching, and dummy nodes)
offers noticeable improvement.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Linfeng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hughes_M/0/1/0/all/0/1"&gt;Michael C. Hughes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hassoun_S/0/1/0/all/0/1"&gt;Soha Hassoun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Li-Ping Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Detect the Interactions that Matter in Matter: Geometric Attention for Many-Body Systems. (arXiv:2106.02549v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02549</id>
        <link href="http://arxiv.org/abs/2106.02549"/>
        <updated>2021-06-07T03:06:16.120Z</updated>
        <summary type="html"><![CDATA[Attention mechanisms are developing into a viable alternative to
convolutional layers as elementary building block of NNs. Their main advantage
is that they are not restricted to capture local dependencies in the input, but
can draw arbitrary connections. This unprecedented capability coincides with
the long-standing problem of modeling global atomic interactions in molecular
force fields and other many-body problems. In its original formulation,
however, attention is not applicable to the continuous domains in which the
atoms live. For this purpose we propose a variant to describe geometric
relations for arbitrary atomic configurations in Euclidean space that also
respects all relevant physical symmetries. We furthermore demonstrate, how the
successive application of our learned attention matrices effectively translates
the molecular geometry into a set of individual atomic contributions
on-the-fly.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Frank_T/0/1/0/all/0/1"&gt;Thorben Frank&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chmiela_S/0/1/0/all/0/1"&gt;Stefan Chmiela&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PCA Initialization for Approximate Message Passing in Rotationally Invariant Models. (arXiv:2106.02356v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.02356</id>
        <link href="http://arxiv.org/abs/2106.02356"/>
        <updated>2021-06-07T03:06:16.112Z</updated>
        <summary type="html"><![CDATA[We study the problem of estimating a rank-$1$ signal in the presence of
rotationally invariant noise-a class of perturbations more general than
Gaussian noise. Principal Component Analysis (PCA) provides a natural
estimator, and sharp results on its performance have been obtained in the
high-dimensional regime. Recently, an Approximate Message Passing (AMP)
algorithm has been proposed as an alternative estimator with the potential to
improve the accuracy of PCA. However, the existing analysis of AMP requires an
initialization that is both correlated with the signal and independent of the
noise, which is often unrealistic in practice. In this work, we combine the two
methods, and propose to initialize AMP with PCA. Our main result is a rigorous
asymptotic characterization of the performance of this estimator. Both the AMP
algorithm and its analysis differ from those previously derived in the Gaussian
setting: at every iteration, our AMP algorithm requires a specific term to
account for PCA initialization, while in the Gaussian case, PCA initialization
affects only the first iteration of AMP. The proof is based on a two-phase
artificial AMP that first approximates the PCA estimator and then mimics the
true AMP. Our numerical simulations show an excellent agreement between AMP
results and theoretical predictions, and suggest an interesting open direction
on achieving Bayes-optimal performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Mondelli_M/0/1/0/all/0/1"&gt;Marco Mondelli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Venkataramanan_R/0/1/0/all/0/1"&gt;Ramji Venkataramanan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Streaming Linear System Identification with Reverse Experience Replay. (arXiv:2103.05896v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.05896</id>
        <link href="http://arxiv.org/abs/2103.05896"/>
        <updated>2021-06-07T03:06:16.106Z</updated>
        <summary type="html"><![CDATA[We consider the problem of estimating a linear time-invariant (LTI) dynamical
system from a single trajectory via streaming algorithms, which is encountered
in several applications including reinforcement learning (RL) and time-series
analysis. While the LTI system estimation problem is well-studied in the {\em
offline} setting, the practically important streaming/online setting has
received little attention. Standard streaming methods like stochastic gradient
descent (SGD) are unlikely to work since streaming points can be highly
correlated. In this work, we propose a novel streaming algorithm, SGD with
Reverse Experience Replay ($\mathsf{SGD}-\mathsf{RER}$), that is inspired by
the experience replay (ER) technique popular in the RL literature.
$\mathsf{SGD}-\mathsf{RER}$ divides data into small buffers and runs SGD
backwards on the data stored in the individual buffers. We show that this
algorithm exactly deconstructs the dependency structure and obtains information
theoretically optimal guarantees for both parameter error and prediction error.
Thus, we provide the first -- to the best of our knowledge -- optimal SGD-style
algorithm for the classical problem of linear system identification with a
first order oracle. Furthermore, $\mathsf{SGD}-\mathsf{RER}$ can be applied to
more general settings like sparse LTI identification with known sparsity
pattern, and non-linear dynamical systems. Our work demonstrates that the
knowledge of data dependency structure can aid us in designing statistically
and computationally efficient algorithms which can "decorrelate" streaming
samples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jain_P/0/1/0/all/0/1"&gt;Prateek Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kowshik_S/0/1/0/all/0/1"&gt;Suhas S Kowshik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nagaraj_D/0/1/0/all/0/1"&gt;Dheeraj Nagaraj&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Netrapalli_P/0/1/0/all/0/1"&gt;Praneeth Netrapalli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning phylogenetic trees as hyperbolic point configurations. (arXiv:2104.11430v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.11430</id>
        <link href="http://arxiv.org/abs/2104.11430"/>
        <updated>2021-06-07T03:06:16.086Z</updated>
        <summary type="html"><![CDATA[We propose a novel method for the inference of phylogenetic trees that
utilises point configurations on hyperbolic space as its optimisation
landscape. Each taxon corresponds to a point of the point configuration, while
the evolutionary distance between taxa is represented by the geodesic distance
between their corresponding points. The point configuration is iteratively
modified to increase an objective function that additively combines pairwise
log-likelihood terms. After convergence, the final tree is derived from the
inter-point distances using a standard distance-based method. The objective
function, which is shown to mimic the log-likelihood on tree space, is a
differentiable function on a Riemannian manifold. Thus gradient-based
optimisation techniques can be applied, avoiding the need for combinatorial
rearrangements of tree topology.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wilson_B/0/1/0/all/0/1"&gt;Benjamin Wilson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sobolev Norm Learning Rates for Conditional Mean Embeddings. (arXiv:2105.07446v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.07446</id>
        <link href="http://arxiv.org/abs/2105.07446"/>
        <updated>2021-06-07T03:06:16.080Z</updated>
        <summary type="html"><![CDATA[We develop novel learning rates for conditional mean embeddings by applying
the theory of interpolation for reproducing kernel Hilbert spaces (RKHS). We
derive explicit, adaptive convergence rates for the sample estimator under the
misspecifed setting, where the target operator is not Hilbert-Schmidt or
bounded with respect to the input/output RKHSs. We demonstrate that in certain
parameter regimes, we can achieve uniform convergence rates in the output RKHS.
We hope our analyses will allow the much broader application of conditional
mean embeddings to more complex ML/RL settings involving infinite dimensional
RKHSs and continuous state spaces.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Talwai_P/0/1/0/all/0/1"&gt;Prem Talwai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Shameli_A/0/1/0/all/0/1"&gt;Ali Shameli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Simchi_Levi_D/0/1/0/all/0/1"&gt;David Simchi-Levi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Representation formulas and pointwise properties for Barron functions. (arXiv:2006.05982v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.05982</id>
        <link href="http://arxiv.org/abs/2006.05982"/>
        <updated>2021-06-07T03:06:16.074Z</updated>
        <summary type="html"><![CDATA[We study the natural function space for infinitely wide two-layer neural
networks with ReLU activation (Barron space) and establish different
representation formulae. In two cases, we describe the space explicitly up to
isomorphism.

Using a convenient representation, we study the pointwise properties of
two-layer networks and show that functions whose singular set is fractal or
curved (for example distance functions from smooth submanifolds) cannot be
represented by infinitely wide two-layer networks with finite path-norm. We use
this structure theorem to show that the only $C^1$-diffeomorphisms which Barron
space are affine.

Furthermore, we show that every Barron function can be decomposed as the sum
of a bounded and a positively one-homogeneous function and that there exist
Barron functions which decay rapidly at infinity and are globally
Lebesgue-integrable. This result suggests that two-layer neural networks may be
able to approximate a greater variety of functions than commonly believed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+E_W/0/1/0/all/0/1"&gt;Weinan E&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wojtowytsch_S/0/1/0/all/0/1"&gt;Stephan Wojtowytsch&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Be Considerate: Objectives, Side Effects, and Deciding How to Act. (arXiv:2106.02617v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2106.02617</id>
        <link href="http://arxiv.org/abs/2106.02617"/>
        <updated>2021-06-07T03:06:16.068Z</updated>
        <summary type="html"><![CDATA[Recent work in AI safety has highlighted that in sequential decision making,
objectives are often underspecified or incomplete. This gives discretion to the
acting agent to realize the stated objective in ways that may result in
undesirable outcomes. We contend that to learn to act safely, a reinforcement
learning (RL) agent should include contemplation of the impact of its actions
on the wellbeing and agency of others in the environment, including other
acting agents and reactive processes. We endow RL agents with the ability to
contemplate such impact by augmenting their reward based on expectation of
future return by others in the environment, providing different criteria for
characterizing impact. We further endow these agents with the ability to
differentially factor this impact into their decision making, manifesting
behavior that ranges from self-centred to self-less, as demonstrated by
experiments in gridworld environments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alamdari_P/0/1/0/all/0/1"&gt;Parand Alizadeh Alamdari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Klassen_T/0/1/0/all/0/1"&gt;Toryn Q. Klassen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Icarte_R/0/1/0/all/0/1"&gt;Rodrigo Toro Icarte&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McIlraith_S/0/1/0/all/0/1"&gt;Sheila A. McIlraith&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Enabling Lightweight Fine-tuning for Pre-trained Language Model Compression based on Matrix Product Operators. (arXiv:2106.02205v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02205</id>
        <link href="http://arxiv.org/abs/2106.02205"/>
        <updated>2021-06-07T03:06:16.062Z</updated>
        <summary type="html"><![CDATA[This paper presents a novel pre-trained language models (PLM) compression
approach based on the matrix product operator (short as MPO) from quantum
many-body physics. It can decompose an original matrix into central tensors
(containing the core information) and auxiliary tensors (with only a small
proportion of parameters). With the decomposed MPO structure, we propose a
novel fine-tuning strategy by only updating the parameters from the auxiliary
tensors, and design an optimization algorithm for MPO-based approximation over
stacked network architectures. Our approach can be applied to the original or
the compressed PLMs in a general way, which derives a lighter network and
significantly reduces the parameters to be fine-tuned. Extensive experiments
have demonstrated the effectiveness of the proposed approach in model
compression, especially the reduction in finetuning parameters (91% reduction
on average).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1"&gt;Peiyu Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1"&gt;Ze-Feng Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1"&gt;Wayne Xin Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1"&gt;Z.Y. Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1"&gt;Zhong-Yi Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1"&gt;Ji-Rong Wen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improve the Interpretability of Attention: A Fast, Accurate, and Interpretable High-Resolution Attention Model. (arXiv:2106.02566v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02566</id>
        <link href="http://arxiv.org/abs/2106.02566"/>
        <updated>2021-06-07T03:06:16.056Z</updated>
        <summary type="html"><![CDATA[The prevalence of employing attention mechanisms has brought along concerns
on the interpretability of attention distributions. Although it provides
insights about how a model is operating, utilizing attention as the explanation
of model predictions is still highly dubious. The community is still seeking
more interpretable strategies for better identifying local active regions that
contribute the most to the final decision. To improve the interpretability of
existing attention models, we propose a novel Bilinear Representative
Non-Parametric Attention (BR-NPA) strategy that captures the task-relevant
human-interpretable information. The target model is first distilled to have
higher-resolution intermediate feature maps. From which, representative
features are then grouped based on local pairwise feature similarity, to
produce finer-grained, more precise attention maps highlighting task-relevant
parts of the input. The obtained attention maps are ranked according to the
`active level' of the compound feature, which provides information regarding
the important level of the highlighted regions. The proposed model can be
easily adapted in a wide variety of modern deep models, where classification is
involved. It is also more accurate, faster, and with a smaller memory footprint
than usual neural attention modules. Extensive experiments showcase more
comprehensive visual explanations compared to the state-of-the-art
visualization model across multiple tasks including few-shot classification,
person re-identification, fine-grained image classification. The proposed
visualization model sheds imperative light on how neural networks `pay their
attention' differently in different tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gomez_T/0/1/0/all/0/1"&gt;Tristan Gomez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ling_S/0/1/0/all/0/1"&gt;Suiyi Ling&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Freour_T/0/1/0/all/0/1"&gt;Thomas Fr&amp;#xe9;our&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mouchere_H/0/1/0/all/0/1"&gt;Harold Mouch&amp;#xe8;re&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI Driven Road Maintenance Inspection. (arXiv:2106.02567v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02567</id>
        <link href="http://arxiv.org/abs/2106.02567"/>
        <updated>2021-06-07T03:06:16.030Z</updated>
        <summary type="html"><![CDATA[Road infrastructure maintenance inspection is typically a labour-intensive
and critical task to ensure the safety of all the road users. In this work, we
propose a detailed methodology to use state-of-the-art techniques in artificial
intelligence and computer vision to automate a sizeable portion of the
maintenance inspection subtasks and reduce the labour costs. The proposed
methodology uses state-of-the-art computer vision techniques such as object
detection and semantic segmentation to automate inspections on primary road
structures such as the road surface, markings, barriers (guardrails) and
traffic signs. The models are mostly trained on commercially viable datasets
and augmented with proprietary data. We demonstrate that our AI models can not
only automate and scale maintenance inspections on primary road structures but
also result in higher recall compared to traditional manual inspections.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mukherjee_R/0/1/0/all/0/1"&gt;Ratnajit Mukherjee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iqbal_H/0/1/0/all/0/1"&gt;Haris Iqbal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marzban_S/0/1/0/all/0/1"&gt;Shabbir Marzban&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Badar_A/0/1/0/all/0/1"&gt;Ahmed Badar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brouns_T/0/1/0/all/0/1"&gt;Terence Brouns&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gowda_S/0/1/0/all/0/1"&gt;Shruthi Gowda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arani_E/0/1/0/all/0/1"&gt;Elahe Arani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zonooz_B/0/1/0/all/0/1"&gt;Bahram Zonooz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Contracting Neural-Newton Solver. (arXiv:2106.02543v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02543</id>
        <link href="http://arxiv.org/abs/2106.02543"/>
        <updated>2021-06-07T03:06:16.024Z</updated>
        <summary type="html"><![CDATA[Recent advances in deep learning have set the focus on neural networks (NNs)
that can successfully replace traditional numerical solvers in many
applications, achieving impressive computing gains. One such application is
time domain simulation, which is indispensable for the design, analysis and
operation of many engineering systems. Simulating dynamical systems with
implicit Newton-based solvers is a computationally heavy task, as it requires
the solution of a parameterized system of differential and algebraic equations
at each time step. A variety of NN-based methodologies have been shown to
successfully approximate the dynamical trajectories computed by numerical time
domain solvers at a fraction of the time. However, so far no previous NN-based
model has explicitly captured the fact that any predicted point on the time
domain trajectory also represents the fixed point of the numerical solver
itself. As we show, explicitly capturing this property can lead to
significantly increased NN accuracy and much smaller NN sizes. In this paper,
we model the Newton solver at the heart of an implicit Runge-Kutta integrator
as a contracting map iteratively seeking this fixed point. Our primary
contribution is to develop a recurrent NN simulation tool, termed the
Contracting Neural-Newton Solver (CoNNS), which explicitly captures the
contracting nature of these Newton iterations. To build CoNNS, we train a
feedforward NN and mimic this contraction behavior by embedding a series of
training constraints which guarantee the mapping provided by the NN satisfies
the Banach fixed-point theorem; thus, we are able to prove that successive
passes through the NN are guaranteed to converge to a unique, fixed point.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chevalier_S/0/1/0/all/0/1"&gt;Samuel Chevalier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stiasny_J/0/1/0/all/0/1"&gt;Jochen Stiasny&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chatzivasileiadis_S/0/1/0/all/0/1"&gt;Spyros Chatzivasileiadis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph-based Deep Learning for Communication Networks: A Survey. (arXiv:2106.02533v1 [cs.NI])]]></title>
        <id>http://arxiv.org/abs/2106.02533</id>
        <link href="http://arxiv.org/abs/2106.02533"/>
        <updated>2021-06-07T03:06:16.018Z</updated>
        <summary type="html"><![CDATA[Communication networks are important infrastructures in contemporary society.
There are still many challenges that are not fully solved and new solutions are
proposed continuously in this active research area. In recent years, to model
the network topology, graph-based deep learning has achieved state-of-the-art
performance in a series of problems in communication networks. In this survey,
we review the rapidly growing body of research using different graph-based deep
learning models, e.g. graph convolutional and graph attention networks, in
various problems from different communication networks, e.g. wireless networks,
wired networks, and software-defined networks. We also present a well-organized
list of the problem and solution for each study and identify future research
directions. To the best of our knowledge, this paper is the first survey that
focuses on the application of graph-based deep learning methods in
communication networks. To track the follow-up research, a public GitHub
repository is created, where the relevant papers will be updated continuously.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1"&gt;Weiwei Jiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep learning-based synthetic-CT generation in radiotherapy and PET: a review. (arXiv:2102.02734v2 [physics.med-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.02734</id>
        <link href="http://arxiv.org/abs/2102.02734"/>
        <updated>2021-06-07T03:06:16.011Z</updated>
        <summary type="html"><![CDATA[Recently, deep learning (DL)-based methods for the generation of synthetic
computed tomography (sCT) have received significant research attention as an
alternative to classical ones. We present here a systematic review of these
methods by grouping them into three categories, according to their clinical
applications: I) To replace CT in magnetic resonance (MR)-based treatment
planning. II) Facilitate cone-beam computed tomography (CBCT)-based
image-guided adaptive radiotherapy. III) Derive attenuation maps for the
correction of positron emission tomography (PET). Appropriate database
searching was performed on journal articles published between January 2014 and
December 2020. The DL methods' key characteristics were extracted from each
eligible study, and a comprehensive comparison among network architectures and
metrics was reported. A detailed review of each category was given,
highlighting essential contributions, identifying specific challenges, and
summarising the achievements. Lastly, the statistics of all the cited works
from various aspects were analysed, revealing the popularity and future trends,
and the potential of DL-based sCT generation. The current status of DL-based
sCT generation was evaluated, assessing the clinical readiness of the presented
methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Spadea_M/0/1/0/all/0/1"&gt;Maria Francesca Spadea&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Maspero_M/0/1/0/all/0/1"&gt;Matteo Maspero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Zaffino_P/0/1/0/all/0/1"&gt;Paolo Zaffino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Seco_J/0/1/0/all/0/1"&gt;Joao Seco&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bag of Tricks for Node Classification with Graph Neural Networks. (arXiv:2103.13355v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.13355</id>
        <link href="http://arxiv.org/abs/2103.13355"/>
        <updated>2021-06-07T03:06:16.001Z</updated>
        <summary type="html"><![CDATA[Over the past few years, graph neural networks (GNN) and label
propagation-based methods have made significant progress in addressing node
classification tasks on graphs. However, in addition to their reliance on
elaborate architectures and algorithms, there are several key technical details
that are frequently overlooked, and yet nonetheless can play a vital role in
achieving satisfactory performance. In this paper, we first summarize a series
of existing tricks-of-the-trade, and then propose several new ones related to
label usage, loss function formulation, and model design that can significantly
improve various GNN architectures. We empirically evaluate their impact on
final node classification accuracy by conducting ablation studies and
demonstrate consistently-improved performance, often to an extent that
outweighs the gains from more dramatic changes in the underlying GNN
architecture. Notably, many of the top-ranked models on the Open Graph
Benchmark (OGB) leaderboard benefit from our techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yangkun Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_J/0/1/0/all/0/1"&gt;Jiarui Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Weinan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1"&gt;Yong Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zheng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wipf_D/0/1/0/all/0/1"&gt;David Wipf&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fairness-Aware Unsupervised Feature Selection. (arXiv:2106.02216v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02216</id>
        <link href="http://arxiv.org/abs/2106.02216"/>
        <updated>2021-06-07T03:06:15.981Z</updated>
        <summary type="html"><![CDATA[Feature selection is a prevalent data preprocessing paradigm for various
learning tasks. Due to the expensive cost of acquiring supervision information,
unsupervised feature selection sparks great interests recently. However,
existing unsupervised feature selection algorithms do not have fairness
considerations and suffer from a high risk of amplifying discrimination by
selecting features that are over associated with protected attributes such as
gender, race, and ethnicity. In this paper, we make an initial investigation of
the fairness-aware unsupervised feature selection problem and develop a
principled framework, which leverages kernel alignment to find a subset of
high-quality features that can best preserve the information in the original
feature space while being minimally correlated with protected attributes.
Specifically, different from the mainstream in-processing debiasing methods,
our proposed framework can be regarded as a model-agnostic debiasing strategy
that eliminates biases and discrimination before downstream learning algorithms
are involved. Experimental results on multiple real-world datasets demonstrate
that our framework achieves a good trade-off between utility maximization and
fairness promotion.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xing_X/0/1/0/all/0/1"&gt;Xiaoying Xing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Hongfu Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Chen Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jundong Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Convergence of Gradient Algorithms for Nonconvex C^{1+alpha} Cost Functions. (arXiv:2012.00628v2 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.00628</id>
        <link href="http://arxiv.org/abs/2012.00628"/>
        <updated>2021-06-07T03:06:15.975Z</updated>
        <summary type="html"><![CDATA[This paper is concerned with convergence of stochastic gradient algorithms
with momentum terms in the nonconvex setting. A class of stochastic momentum
methods, including stochastic gradient descent, heavy ball, and Nesterov's
accelerated gradient, is analyzed in a general framework under mild
assumptions. Based on the convergence result of expected gradients, we prove
the almost sure convergence by a detailed discussion of the effects of momentum
and the number of upcrossings. It is worth noting that there are not additional
restrictions imposed on the objective function and stepsize. Another
improvement over previous results is that the existing Lipschitz condition of
the gradient is relaxed into the condition of Holder continuity. As a
byproduct, we apply a localization procedure to extend our results to
stochastic stepsizes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zixuan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Tang_S/0/1/0/all/0/1"&gt;Shanjian Tang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Inferring Granger Causality from Irregularly Sampled Time Series. (arXiv:2106.02600v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02600</id>
        <link href="http://arxiv.org/abs/2106.02600"/>
        <updated>2021-06-07T03:06:15.968Z</updated>
        <summary type="html"><![CDATA[Continuous, automated surveillance systems that incorporate machine learning
models are becoming increasingly more common in healthcare environments. These
models can capture temporally dependent changes across multiple patient
variables and can enhance a clinician's situational awareness by providing an
early warning alarm of an impending adverse event such as sepsis. However, most
commonly used methods, e.g., XGBoost, fail to provide an interpretable
mechanism for understanding why a model produced a sepsis alarm at a given
time. The black-box nature of many models is a severe limitation as it prevents
clinicians from independently corroborating those physiologic features that
have contributed to the sepsis alarm. To overcome this limitation, we propose a
generalized linear model (GLM) approach to fit a Granger causal graph based on
the physiology of several major sepsis-associated derangements (SADs). We adopt
a recently developed stochastic monotone variational inequality-based estimator
coupled with forwarding feature selection to learn the graph structure from
both continuous and discrete-valued as well as regularly and irregularly
sampled time series. Most importantly, we develop a non-asymptotic upper bound
on the estimation error for any monotone link function in the GLM. We conduct
real-data experiments and demonstrate that our proposed method can achieve
comparable performance to popular and powerful prediction methods such as
XGBoost while simultaneously maintaining a high level of interpretability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wei_S/0/1/0/all/0/1"&gt;Song Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1"&gt;Yao Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Josef_C/0/1/0/all/0/1"&gt;Christopher S. Josef&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kamaleswaran_R/0/1/0/all/0/1"&gt;Rishikesan Kamaleswaran&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Light Field Networks: Neural Scene Representations with Single-Evaluation Rendering. (arXiv:2106.02634v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02634</id>
        <link href="http://arxiv.org/abs/2106.02634"/>
        <updated>2021-06-07T03:06:15.958Z</updated>
        <summary type="html"><![CDATA[Inferring representations of 3D scenes from 2D observations is a fundamental
problem of computer graphics, computer vision, and artificial intelligence.
Emerging 3D-structured neural scene representations are a promising approach to
3D scene understanding. In this work, we propose a novel neural scene
representation, Light Field Networks or LFNs, which represent both geometry and
appearance of the underlying 3D scene in a 360-degree, four-dimensional light
field parameterized via a neural implicit representation. Rendering a ray from
an LFN requires only a *single* network evaluation, as opposed to hundreds of
evaluations per ray for ray-marching or volumetric based renderers in
3D-structured neural scene representations. In the setting of simple scenes, we
leverage meta-learning to learn a prior over LFNs that enables multi-view
consistent light field reconstruction from as little as a single image
observation. This results in dramatic reductions in time and memory complexity,
and enables real-time rendering. The cost of storing a 360-degree light field
via an LFN is two orders of magnitude lower than conventional methods such as
the Lumigraph. Utilizing the analytical differentiability of neural implicit
representations and a novel parameterization of light space, we further
demonstrate the extraction of sparse depth maps from LFNs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sitzmann_V/0/1/0/all/0/1"&gt;Vincent Sitzmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rezchikov_S/0/1/0/all/0/1"&gt;Semon Rezchikov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Freeman_W/0/1/0/all/0/1"&gt;William T. Freeman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1"&gt;Joshua B. Tenenbaum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Durand_F/0/1/0/all/0/1"&gt;Fredo Durand&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Do Syntactic Probes Probe Syntax? Experiments with Jabberwocky Probing. (arXiv:2106.02559v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02559</id>
        <link href="http://arxiv.org/abs/2106.02559"/>
        <updated>2021-06-07T03:06:15.952Z</updated>
        <summary type="html"><![CDATA[Analysing whether neural language models encode linguistic information has
become popular in NLP. One method of doing so, which is frequently cited to
support the claim that models like BERT encode syntax, is called probing;
probes are small supervised models trained to extract linguistic information
from another model's output. If a probe is able to predict a particular
structure, it is argued that the model whose output it is trained on must have
implicitly learnt to encode it. However, drawing a generalisation about a
model's linguistic knowledge about a specific phenomena based on what a probe
is able to learn may be problematic: in this work, we show that semantic cues
in training data means that syntactic probes do not properly isolate syntax. We
generate a new corpus of semantically nonsensical but syntactically well-formed
Jabberwocky sentences, which we use to evaluate two probes trained on normal
data. We train the probes on several popular language models (BERT, GPT, and
RoBERTa), and find that in all settings they perform worse when evaluated on
these data, for one probe by an average of 15.4 UUAS points absolute. Although
in most cases they still outperform the baselines, their lead is reduced
substantially, e.g. by 53% in the case of BERT for one probe. This begs the
question: what empirical scores constitute knowing syntax?]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Maudslay_R/0/1/0/all/0/1"&gt;Rowan Hall Maudslay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1"&gt;Ryan Cotterell&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ViViT: Curvature access through the generalized Gauss-Newton's low-rank structure. (arXiv:2106.02624v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02624</id>
        <link href="http://arxiv.org/abs/2106.02624"/>
        <updated>2021-06-07T03:06:15.929Z</updated>
        <summary type="html"><![CDATA[Curvature in form of the Hessian or its generalized Gauss-Newton (GGN)
approximation is valuable for algorithms that rely on a local model for the
loss to train, compress, or explain deep networks. Existing methods based on
implicit multiplication via automatic differentiation or Kronecker-factored
block diagonal approximations do not consider noise in the mini-batch. We
present ViViT, a curvature model that leverages the GGN's low-rank structure
without further approximations. It allows for efficient computation of
eigenvalues, eigenvectors, as well as per-sample first- and second-order
directional derivatives. The representation is computed in parallel with
gradients in one backward pass and offers a fine-grained cost-accuracy
trade-off, which allows it to scale. As examples for ViViT's usefulness, we
investigate the directional gradients and curvatures during training, and how
noise information can be used to improve the stability of second-order methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dangel_F/0/1/0/all/0/1"&gt;Felix Dangel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tatzel_L/0/1/0/all/0/1"&gt;Lukas Tatzel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hennig_P/0/1/0/all/0/1"&gt;Philipp Hennig&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Consensus Multiplicative Weights Update: Learning to Learn using Projector-based Game Signatures. (arXiv:2106.02615v1 [cs.GT])]]></title>
        <id>http://arxiv.org/abs/2106.02615</id>
        <link href="http://arxiv.org/abs/2106.02615"/>
        <updated>2021-06-07T03:06:15.922Z</updated>
        <summary type="html"><![CDATA[Recently, Optimistic Multiplicative Weights Update (OMWU) was proven to be
the first constant step-size algorithm in the online no-regret framework to
enjoy last-iterate convergence to Nash Equilibria in the constrained zero-sum
bimatrix case, where weights represent the probabilities of playing pure
strategies. We introduce the second such algorithm, \textit{Consensus MWU}, for
which we prove local convergence and show empirically that it enjoys faster and
more robust convergence than OMWU. Our algorithm shows the importance of a new
object, the \textit{simplex Hessian}, as well as of the interaction of the game
with the (eigen)space of vectors summing to zero, which we believe future
research can build on. As for OMWU, CMWU has convergence guarantees in the
zero-sum case only, but Cheung and Piliouras (2020) recently showed that OMWU
and MWU display opposite convergence properties depending on whether the game
is zero-sum or cooperative. Inspired by this work and the recent literature on
learning to optimize for single functions, we extend CMWU to non zero-sum games
by introducing a new framework for online learning in games, where the update
rule's gradient and Hessian coefficients along a trajectory are learnt by a
reinforcement learning policy that is conditioned on the nature of the game:
\textit{the game signature}. We construct the latter using a new canonical
decomposition of two-player games into eight components corresponding to
commutative projection operators, generalizing and unifying recent game
concepts studied in the literature. We show empirically that our new learning
policy is able to exploit the game signature across a wide range of game types.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vadori_N/0/1/0/all/0/1"&gt;Nelson Vadori&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Savani_R/0/1/0/all/0/1"&gt;Rahul Savani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Spooner_T/0/1/0/all/0/1"&gt;Thomas Spooner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ganesh_S/0/1/0/all/0/1"&gt;Sumitra Ganesh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving the Robustness of QA Models to Challenge Sets with Variational Question-Answer Pair Generation. (arXiv:2004.03238v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.03238</id>
        <link href="http://arxiv.org/abs/2004.03238"/>
        <updated>2021-06-07T03:06:15.914Z</updated>
        <summary type="html"><![CDATA[Question answering (QA) models for reading comprehension have achieved
human-level accuracy on in-distribution test sets. However, they have been
demonstrated to lack robustness to challenge sets, whose distribution is
different from that of training sets. Existing data augmentation methods
mitigate this problem by simply augmenting training sets with synthetic
examples sampled from the same distribution as the challenge sets. However,
these methods assume that the distribution of a challenge set is known a
priori, making them less applicable to unseen challenge sets. In this study, we
focus on question-answer pair generation (QAG) to mitigate this problem. While
most existing QAG methods aim to improve the quality of synthetic examples, we
conjecture that diversity-promoting QAG can mitigate the sparsity of training
sets and lead to better robustness. We present a variational QAG model that
generates multiple diverse QA pairs from a paragraph. Our experiments show that
our method can improve the accuracy of 12 challenge sets, as well as the
in-distribution accuracy. Our code and data are available at
https://github.com/KazutoshiShinoda/VQAG.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shinoda_K/0/1/0/all/0/1"&gt;Kazutoshi Shinoda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sugawara_S/0/1/0/all/0/1"&gt;Saku Sugawara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aizawa_A/0/1/0/all/0/1"&gt;Akiko Aizawa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semi-Empirical Objective Functions for MCMC Proposal Optimization. (arXiv:2106.02104v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02104</id>
        <link href="http://arxiv.org/abs/2106.02104"/>
        <updated>2021-06-07T03:06:15.898Z</updated>
        <summary type="html"><![CDATA[We introduce and demonstrate a semi-empirical procedure for determining
approximate objective functions suitable for optimizing arbitrarily
parameterized proposal distributions in MCMC methods. Our proposed Ab Initio
objective functions consist of the weighted combination of functions following
constraints on their global optima and of coordinate invariance that we argue
should be upheld by general measures of MCMC efficiency for use in proposal
optimization. The coefficients of Ab Initio objective functions are determined
so as to recover the optimal MCMC behavior prescribed by established
theoretical analysis for chosen reference problems. Our experimental results
demonstrate that Ab Initio objective functions maintain favorable performance
and preferable optimization behavior compared to existing objective functions
for MCMC optimization when optimizing highly expressive proposal distributions.
We argue that Ab Initio objective functions are sufficiently robust to enable
the confident optimization of MCMC proposal distributions parameterized by deep
generative networks that extend beyond the traditional limitations of
individual MCMC schemes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cannella_C/0/1/0/all/0/1"&gt;Chris Cannella&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tarokh_V/0/1/0/all/0/1"&gt;Vahid Tarokh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Visual Question Rewriting for Increasing Response Rate. (arXiv:2106.02257v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02257</id>
        <link href="http://arxiv.org/abs/2106.02257"/>
        <updated>2021-06-07T03:06:15.889Z</updated>
        <summary type="html"><![CDATA[When a human asks questions online, or when a conversational virtual agent
asks human questions, questions triggering emotions or with details might more
likely to get responses or answers. we explore how to automatically rewrite
natural language questions to improve the response rate from people. In
particular, a new task of Visual Question Rewriting(VQR) task is introduced to
explore how visual information can be used to improve the new questions. A data
set containing around 4K bland questions, attractive questions and images
triples is collected. We developed some baseline sequence to sequence models
and more advanced transformer based models, which take a bland question and a
related image as input and output a rewritten question that is expected to be
more attractive. Offline experiments and mechanical Turk based evaluations show
that it is possible to rewrite bland questions in a more detailed and
attractive way to increase the response rate, and images can be helpful.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1"&gt;Jiayi Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xilian Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xin Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Principled change point detection via representation learning. (arXiv:2106.02602v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02602</id>
        <link href="http://arxiv.org/abs/2106.02602"/>
        <updated>2021-06-07T03:06:15.872Z</updated>
        <summary type="html"><![CDATA[Change points are abrupt alterations in the distribution of sequential data.
A change-point detection (CPD) model aims at quick detection of such changes.
Classic approaches perform poorly for semi-structured sequential data because
of the absence of adequate data representation learning. To deal with it, we
introduce a principled differentiable loss function that considers the
specificity of the CPD task. The theoretical results suggest that this function
approximates well classic rigorous solutions. For such loss function, we
propose an end-to-end method for the training of deep representation learning
CPD models. Our experiments provide evidence that the proposed approach
improves baseline results of change point detection for various data types,
including real-world videos and image sequences, and improve representations
for them.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Romanenkova_E/0/1/0/all/0/1"&gt;Evgenia Romanenkova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zaytsev_A/0/1/0/all/0/1"&gt;Alexey Zaytsev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zainulin_R/0/1/0/all/0/1"&gt;Ramil Zainulin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morozov_M/0/1/0/all/0/1"&gt;Matvey Morozov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning transition times in event sequences: the Event-Based Hidden Markov Model of disease progression. (arXiv:2011.01023v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.01023</id>
        <link href="http://arxiv.org/abs/2011.01023"/>
        <updated>2021-06-07T03:06:15.865Z</updated>
        <summary type="html"><![CDATA[Progressive diseases worsen over time and are characterised by monotonic
change in features that track disease progression. Here we connect ideas from
two formerly separate methodologies -- event-based and hidden Markov modelling
-- to derive a new generative model of disease progression. Our model can
uniquely infer the most likely group-level sequence and timing of events
(natural history) from limited datasets. Moreover, it can infer and predict
individual-level trajectories (prognosis) even when data are missing, giving it
high clinical utility. Here we derive the model and provide an inference scheme
based on the expectation maximisation algorithm. We use clinical, imaging and
biofluid data from the Alzheimer's Disease Neuroimaging Initiative to
demonstrate the validity and utility of our model. First, we train our model to
uncover a new group-level sequence of feature changes in Alzheimer's disease
over a period of ${\sim}17.3$ years. Next, we demonstrate that our model
provides improved utility over a continuous time hidden Markov model by area
under the receiver operator characteristic curve ${\sim}0.23$. Finally, we
demonstrate that our model maintains predictive accuracy with up to $50\%$
missing data. These results support the clinical validity of our model and its
broader utility in resource-limited medical applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wijeratne_P/0/1/0/all/0/1"&gt;Peter A. Wijeratne&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alexander_D/0/1/0/all/0/1"&gt;Daniel C. Alexander&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Topological Graph Neural Networks. (arXiv:2102.07835v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.07835</id>
        <link href="http://arxiv.org/abs/2102.07835"/>
        <updated>2021-06-07T03:06:15.851Z</updated>
        <summary type="html"><![CDATA[Graph neural networks (GNNs) are a powerful architecture for tackling graph
learning tasks, yet have been shown to be oblivious to eminent substructures,
such as cycles. We present TOGL, a novel layer that incorporates global
topological information of a graph using persistent homology. TOGL can be
easily integrated into any type of GNN and is strictly more expressive in terms
of the Weisfeiler--Lehman test of isomorphism. Augmenting GNNs with our layer
leads to beneficial predictive performance for graph and node classification
tasks, both on synthetic data sets, which can be classified by humans using
their topology but not by ordinary GNNs, and on real-world data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Horn_M/0/1/0/all/0/1"&gt;Max Horn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brouwer_E/0/1/0/all/0/1"&gt;Edward De Brouwer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moor_M/0/1/0/all/0/1"&gt;Michael Moor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moreau_Y/0/1/0/all/0/1"&gt;Yves Moreau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rieck_B/0/1/0/all/0/1"&gt;Bastian Rieck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Borgwardt_K/0/1/0/all/0/1"&gt;Karsten Borgwardt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Homological Time Series Analysis of Sensor Signals from Power Plants. (arXiv:2106.02493v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02493</id>
        <link href="http://arxiv.org/abs/2106.02493"/>
        <updated>2021-06-07T03:06:15.840Z</updated>
        <summary type="html"><![CDATA[In this paper, we use topological data analysis techniques to construct a
suitable neural network classifier for the task of learning sensor signals of
entire power plants according to their reference designation system. We use
representations of persistence diagrams to derive necessary preprocessing steps
and visualize the large amounts of data. We derive architectures with deep
one-dimensional convolutional layers combined with stacked long short-term
memories as residual networks suitable for processing the persistence features.
We combine three separate sub-networks, obtaining as input the time series
itself and a representation of the persistent homology for the zeroth and first
dimension. We give a mathematical derivation for most of the used
hyper-parameters. For validation, numerical experiments were performed with
sensor data from four power plants of the same construction type.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Melodia_L/0/1/0/all/0/1"&gt;Luciano Melodia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lenz_R/0/1/0/all/0/1"&gt;Richard Lenz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Distributional Sliced Embedding Discrepancy for Incomparable Distributions. (arXiv:2106.02542v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02542</id>
        <link href="http://arxiv.org/abs/2106.02542"/>
        <updated>2021-06-07T03:06:15.828Z</updated>
        <summary type="html"><![CDATA[Gromov-Wasserstein (GW) distance is a key tool for manifold learning and
cross-domain learning, allowing the comparison of distributions that do not
live in the same metric space. Because of its high computational complexity,
several approximate GW distances have been proposed based on entropy
regularization or on slicing, and one-dimensional GW computation. In this
paper, we propose a novel approach for comparing two incomparable
distributions, that hinges on the idea of distributional slicing, embeddings,
and on computing the closed-form Wasserstein distance between the sliced
distributions. We provide a theoretical analysis of this new divergence, called
distributional sliced embedding (DSE) discrepancy, and we show that it
preserves several interesting properties of GW distance including
rotation-invariance. We show that the embeddings involved in DSE can be
efficiently learned. Finally, we provide a large set of experiments
illustrating the behavior of DSE as a divergence in the context of generative
modeling and in query framework.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alaya_M/0/1/0/all/0/1"&gt;Mokhtar Z. Alaya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gasso_G/0/1/0/all/0/1"&gt;Gilles Gasso&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berar_M/0/1/0/all/0/1"&gt;Maxime Berar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rakotomamonjy_A/0/1/0/all/0/1"&gt;Alain Rakotomamonjy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Meta-Learning Guarantees for Online Receding Horizon Learning Control. (arXiv:2010.11327v12 [eess.SY] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.11327</id>
        <link href="http://arxiv.org/abs/2010.11327"/>
        <updated>2021-06-07T03:06:15.820Z</updated>
        <summary type="html"><![CDATA[In this paper we provide provable regret guarantees for an online
meta-learning receding horizon control algorithm in an iterative control
setting. We consider the setting where, in each iteration the system to be
controlled is a linear deterministic system that is different and unknown, the
cost for the controller in an iteration is a general additive cost function and
there are affine control input constraints. By analysing conditions under which
sub-linear regret is achievable, we prove that the online receding horizon
controller achieves a regret for the controller cost and constraint violation
that are $\tilde{O}(T^{3/4})$ with respect to the best policy that satisfies
the control input control constraints, when the preview of the cost functions
is limited to an interval and the interval size is doubled from one to the
next. We then show that the average of the regret for the controller cost and
constraint violation with respect to the same policy vary as
$\tilde{O}((1+1/\sqrt{N})T^{3/4})$ with the number of iterations $N$, under the
same setting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Muthirayan_D/0/1/0/all/0/1"&gt;Deepan Muthirayan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Khargonekar_P/0/1/0/all/0/1"&gt;Pramod P. Khargonekar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interpretable COVID-19 Chest X-Ray Classification via Orthogonality Constraint. (arXiv:2102.08360v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.08360</id>
        <link href="http://arxiv.org/abs/2102.08360"/>
        <updated>2021-06-07T03:06:15.801Z</updated>
        <summary type="html"><![CDATA[Deep neural networks have increasingly been used as an auxiliary tool in
healthcare applications, due to their ability to improve performance of several
diagnosis tasks. However, these methods are not widely adopted in clinical
settings due to the practical limitations in the reliability, generalizability,
and interpretability of deep learning based systems. As a result, methods have
been developed that impose additional constraints during network training to
gain more control as well as improve interpretabilty, facilitating their
acceptance in healthcare community. In this work, we investigate the benefit of
using Orthogonal Spheres (OS) constraint for classification of COVID-19 cases
from chest X-ray images. The OS constraint can be written as a simple
orthonormality term which is used in conjunction with the standard
cross-entropy loss during classification network training. Previous studies
have demonstrated significant benefits in applying such constraints to deep
learning models. Our findings corroborate these observations, indicating that
the orthonormality loss function effectively produces improved semantic
localization via GradCAM visualizations, enhanced classification performance,
and reduced model calibration error. Our approach achieves an improvement in
accuracy of 1.6% and 4.8% for two- and three-class classification,
respectively; similar results are found for models with data augmentation
applied. In addition to these findings, our work also presents a new
application of the OS regularizer in healthcare, increasing the post-hoc
interpretability and performance of deep learning models for COVID-19
classification to facilitate adoption of these methods in clinical settings. We
also identify the limitations of our strategy that can be explored for further
research in future.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_E/0/1/0/all/0/1"&gt;Ella Y. Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Som_A/0/1/0/all/0/1"&gt;Anirudh Som&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shukla_A/0/1/0/all/0/1"&gt;Ankita Shukla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_H/0/1/0/all/0/1"&gt;Hongjun Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Turaga_P/0/1/0/all/0/1"&gt;Pavan Turaga&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Holistic Approach to Interpretability in Financial Lending: Models, Visualizations, and Summary-Explanations. (arXiv:2106.02605v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02605</id>
        <link href="http://arxiv.org/abs/2106.02605"/>
        <updated>2021-06-07T03:06:15.797Z</updated>
        <summary type="html"><![CDATA[Lending decisions are usually made with proprietary models that provide
minimally acceptable explanations to users. In a future world without such
secrecy, what decision support tools would one want to use for justified
lending decisions? This question is timely, since the economy has dramatically
shifted due to a pandemic, and a massive number of new loans will be necessary
in the short term. We propose a framework for such decisions, including a
globally interpretable machine learning model, an interactive visualization of
it, and several types of summaries and explanations for any given decision. The
machine learning model is a two-layer additive risk model, which resembles a
two-layer neural network, but is decomposable into subscales. In this model,
each node in the first (hidden) layer represents a meaningful subscale model,
and all of the nonlinearities are transparent. Our online visualization tool
allows exploration of this model, showing precisely how it came to its
conclusion. We provide three types of explanations that are simpler than, but
consistent with, the global model: case-based reasoning explanations that use
neighboring past cases, a set of features that were the most important for the
model's prediction, and summary-explanations that provide a customized sparse
explanation for any particular lending decision made by the model. Our
framework earned the FICO recognition award for the Explainable Machine
Learning Challenge, which was the first public challenge in the domain of
explainable machine learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Chaofan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_K/0/1/0/all/0/1"&gt;Kangcheng Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rudin_C/0/1/0/all/0/1"&gt;Cynthia Rudin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shaposhnik_Y/0/1/0/all/0/1"&gt;Yaron Shaposhnik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Sijia Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1"&gt;Tong Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Approximation Algorithms for Sparse Principal Component Analysis. (arXiv:2006.12748v2 [cs.DS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.12748</id>
        <link href="http://arxiv.org/abs/2006.12748"/>
        <updated>2021-06-07T03:06:15.789Z</updated>
        <summary type="html"><![CDATA[Principal component analysis (PCA) is a widely used dimension reduction
technique in machine learning and multivariate statistics. To improve the
interpretability of PCA, various approaches to obtain sparse principal
direction loadings have been proposed, which are termed Sparse Principal
Component Analysis (SPCA). In this paper, we present thresholding as a provably
accurate, polynomial time, approximation algorithm for the SPCA problem,
without imposing any restrictive assumptions on the input covariance matrix.
Our first thresholding algorithm using the Singular Value Decomposition is
conceptually simple; is faster than current state-of-the-art; and performs well
in practice. On the negative side, our (novel) theoretical bounds do not
accurately predict the strong practical performance of this approach. The
second algorithm solves a well-known semidefinite programming relaxation and
then uses a novel, two step, deterministic thresholding scheme to compute a
sparse principal vector. It works very well in practice and, remarkably, this
solid practical performance is accurately predicted by our theoretical bounds,
which bridge the theory-practice gap better than current state-of-the-art.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chowdhury_A/0/1/0/all/0/1"&gt;Agniva Chowdhury&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Drineas_P/0/1/0/all/0/1"&gt;Petros Drineas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Woodruff_D/0/1/0/all/0/1"&gt;David P. Woodruff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1"&gt;Samson Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bonsai-Net: One-Shot Neural Architecture Search via Differentiable Pruners. (arXiv:2006.09264v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.09264</id>
        <link href="http://arxiv.org/abs/2006.09264"/>
        <updated>2021-06-07T03:06:15.769Z</updated>
        <summary type="html"><![CDATA[One-shot Neural Architecture Search (NAS) aims to minimize the computational
expense of discovering state-of-the-art models. However, in the past year
attention has been drawn to the comparable performance of naive random search
across the same search spaces used by leading NAS algorithms. To address this,
we explore the effects of drastically relaxing the NAS search space, and we
present Bonsai-Net, an efficient one-shot NAS method to explore our relaxed
search space. Bonsai-Net is built around a modified differential pruner and can
consistently discover state-of-the-art architectures that are significantly
better than random search with fewer parameters than other state-of-the-art
methods. Additionally, Bonsai-Net performs simultaneous model search and
training, dramatically reducing the total time it takes to generate
fully-trained models from scratch.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Geada_R/0/1/0/all/0/1"&gt;Rob Geada&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prangle_D/0/1/0/all/0/1"&gt;Dennis Prangle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McGough_A/0/1/0/all/0/1"&gt;Andrew Stephen McGough&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Manifold-Aware Deep Clustering: Maximizing Angles between Embedding Vectors Based on Regular Simplex. (arXiv:2106.02331v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2106.02331</id>
        <link href="http://arxiv.org/abs/2106.02331"/>
        <updated>2021-06-07T03:06:15.758Z</updated>
        <summary type="html"><![CDATA[This paper presents a new deep clustering (DC) method called manifold-aware
DC (M-DC) that can enhance hyperspace utilization more effectively than the
original DC. The original DC has a limitation in that a pair of two speakers
has to be embedded having an orthogonal relationship due to its use of the
one-hot vector-based loss function, while our method derives a unique loss
function aimed at maximizing the target angle in the hyperspace based on the
nature of a regular simplex. Our proposed loss imposes a higher penalty than
the original DC when the speaker is assigned incorrectly. The change from DC to
M-DC can be easily achieved by rewriting just one term in the loss function of
DC, without any other modifications to the network architecture or model
parameters. As such, our method has high practicability because it does not
affect the original inference part. The experimental results show that the
proposed method improves the performances of the original DC and its expansion
method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Tanaka_K/0/1/0/all/0/1"&gt;Keitaro Tanaka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sawata_R/0/1/0/all/0/1"&gt;Ryosuke Sawata&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Takahashi_S/0/1/0/all/0/1"&gt;Shusuke Takahashi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multitask Online Mirror Descent. (arXiv:2106.02393v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02393</id>
        <link href="http://arxiv.org/abs/2106.02393"/>
        <updated>2021-06-07T03:06:15.741Z</updated>
        <summary type="html"><![CDATA[We introduce and analyze MT-OMD, a multitask generalization of Online Mirror
Descent (OMD) which operates by sharing updates between tasks. We prove that
the regret of MT-OMD is of order $\sqrt{1 + \sigma^2(N-1)}\sqrt{T}$, where
$\sigma^2$ is the task variance according to the geometry induced by the
regularizer, $N$ is the number of tasks, and $T$ is the time horizon. Whenever
tasks are similar, that is, $\sigma^2 \le 1$, this improves upon the
$\sqrt{NT}$ bound obtained by running independent OMDs on each task. Our
multitask extensions of Online Gradient Descent and Exponentiated Gradient, two
important instances of OMD, are shown to enjoy closed-form updates, making them
easy to use in practice. Finally, we provide numerical experiments on four
real-world datasets which support our theoretical findings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cesa_Bianchi_N/0/1/0/all/0/1"&gt;Nicol&amp;#xf2; Cesa-Bianchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Laforgue_P/0/1/0/all/0/1"&gt;Pierre Laforgue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paudice_A/0/1/0/all/0/1"&gt;Andrea Paudice&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pontil_M/0/1/0/all/0/1"&gt;Massimiliano Pontil&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Discovery of Causal Additive Models in the Presence of Unobserved Variables. (arXiv:2106.02234v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02234</id>
        <link href="http://arxiv.org/abs/2106.02234"/>
        <updated>2021-06-07T03:06:15.702Z</updated>
        <summary type="html"><![CDATA[Causal discovery from data affected by unobserved variables is an important
but difficult problem to solve. The effects that unobserved variables have on
the relationships between observed variables are more complex in nonlinear
cases than in linear cases. In this study, we focus on causal additive models
in the presence of unobserved variables. Causal additive models exhibit
structural equations that are additive in the variables and error terms. We
take into account the presence of not only unobserved common causes but also
unobserved intermediate variables. Our theoretical results show that, when the
causal relationships are nonlinear and there are unobserved variables, it is
not possible to identify all the causal relationships between observed
variables through regression and independence tests. However, our theoretical
results also show that it is possible to avoid incorrect inferences. We propose
a method to identify all the causal relationships that are theoretically
possible to identify without being biased by unobserved variables. The
empirical results using artificial data and simulated functional magnetic
resonance imaging (fMRI) data show that our method effectively infers causal
structures in the presence of unobserved variables.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Maeda_T/0/1/0/all/0/1"&gt;Takashi Nicholas Maeda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shimizu_S/0/1/0/all/0/1"&gt;Shohei Shimizu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Top-$k$ Regularization for Supervised Feature Selection. (arXiv:2106.02197v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02197</id>
        <link href="http://arxiv.org/abs/2106.02197"/>
        <updated>2021-06-07T03:06:15.657Z</updated>
        <summary type="html"><![CDATA[Feature selection identifies subsets of informative features and reduces
dimensions in the original feature space, helping provide insights into data
generation or a variety of domain problems. Existing methods mainly depend on
feature scoring functions or sparse regularizations; nonetheless, they have
limited ability to reconcile the representativeness and inter-correlations of
features. In this paper, we introduce a novel, simple yet effective
regularization approach, named top-$k$ regularization, to supervised feature
selection in regression and classification tasks. Structurally, the top-$k$
regularization induces a sub-architecture on the architecture of a learning
model to boost its ability to select the most informative features and model
complex nonlinear relationships simultaneously. Theoretically, we derive and
mathematically prove a uniform approximation error bound for using this
approach to approximate high-dimensional sparse functions. Extensive
experiments on a wide variety of benchmarking datasets show that the top-$k$
regularization is effective and stable for supervised feature selection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1"&gt;Xinxing Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_Q/0/1/0/all/0/1"&gt;Qiang Cheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Provably Strict Generalisation Benefit for Invariance in Kernel Methods. (arXiv:2106.02346v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.02346</id>
        <link href="http://arxiv.org/abs/2106.02346"/>
        <updated>2021-06-07T03:06:15.572Z</updated>
        <summary type="html"><![CDATA[It is a commonly held belief that enforcing invariance improves
generalisation. Although this approach enjoys widespread popularity, it is only
very recently that a rigorous theoretical demonstration of this benefit has
been established. In this work we build on the function space perspective of
Elesedy and Zaidi arXiv:2102.10333 to derive a strictly non-zero generalisation
benefit of incorporating invariance in kernel ridge regression when the target
is invariant to the action of a compact group. We study invariance enforced by
feature averaging and find that generalisation is governed by a notion of
effective dimension that arises from the interplay between the kernel and the
group. In building towards this result, we find that the action of the group
induces an orthogonal decomposition of both the reproducing kernel Hilbert
space and its kernel, which may be of interest in its own right.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Elesedy_B/0/1/0/all/0/1"&gt;Bryn Elesedy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimal Rates of (Locally) Differentially Private Heavy-tailed Multi-Armed Bandits. (arXiv:2106.02575v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02575</id>
        <link href="http://arxiv.org/abs/2106.02575"/>
        <updated>2021-06-07T03:06:15.207Z</updated>
        <summary type="html"><![CDATA[In this paper we study the problem of stochastic multi-armed bandits (MAB) in
the (local) differential privacy (DP/LDP) model. Unlike the previous results
which need to assume bounded reward distributions, here we mainly focus on the
case the reward distribution of each arm only has $(1+v)$-th moment with some
$v\in (0, 1]$. In the first part, we study the problem in the central
$\epsilon$-DP model. We first provide a near-optimal result by developing a
private and robust Upper Confidence Bound (UCB) algorithm. Then, we improve the
result via a private and robust version of the Successive Elimination (SE)
algorithm. Finally, we show that the instance-dependent regret bound of our
improved algorithm is optimal by showing its lower bound. In the second part of
the paper, we study the problem in the $\epsilon$-LDP model. We propose an
algorithm which could be seen as locally private and robust version of the SE
algorithm, and show it could achieve (near) optimal rates for both
instance-dependent and instance-independent regrets. All of the above results
can also reveal the differences between the problem of private MAB with bounded
rewards and heavy-tailed rewards. To achieve these (near) optimal rates, we
develop several new hard instances and private robust estimators as byproducts,
which might could be used to other related problems. Finally, experimental
results also support our theoretical analysis and show the effectiveness of our
algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tao_Y/0/1/0/all/0/1"&gt;Youming Tao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yulian Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1"&gt;Peng Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1"&gt;Di Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Regressive Domain Adaptation for Unsupervised Keypoint Detection. (arXiv:2103.06175v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.06175</id>
        <link href="http://arxiv.org/abs/2103.06175"/>
        <updated>2021-06-07T03:06:15.201Z</updated>
        <summary type="html"><![CDATA[Domain adaptation (DA) aims at transferring knowledge from a labeled source
domain to an unlabeled target domain. Though many DA theories and algorithms
have been proposed, most of them are tailored into classification settings and
may fail in regression tasks, especially in the practical keypoint detection
task. To tackle this difficult but significant task, we present a method of
regressive domain adaptation (RegDA) for unsupervised keypoint detection.
Inspired by the latest theoretical work, we first utilize an adversarial
regressor to maximize the disparity on the target domain and train a feature
generator to minimize this disparity. However, due to the high dimension of the
output space, this regressor fails to detect samples that deviate from the
support of the source. To overcome this problem, we propose two important
ideas. First, based on our observation that the probability density of the
output space is sparse, we introduce a spatial probability distribution to
describe this sparsity and then use it to guide the learning of the adversarial
regressor. Second, to alleviate the optimization difficulty in the
high-dimensional space, we innovatively convert the minimax game in the
adversarial training to the minimization of two opposite goals. Extensive
experiments show that our method brings large improvement by 8% to 11% in terms
of PCK on different datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1"&gt;Junguang Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1"&gt;Yifei Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Ximei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yufeng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jianmin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Long_M/0/1/0/all/0/1"&gt;Mingsheng Long&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Wide Network Learning with Differential Privacy. (arXiv:2103.01294v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.01294</id>
        <link href="http://arxiv.org/abs/2103.01294"/>
        <updated>2021-06-07T03:06:15.194Z</updated>
        <summary type="html"><![CDATA[Despite intense interest and considerable effort, the current generation of
neural networks suffers a significant loss of accuracy under most practically
relevant privacy training regimes. One particularly challenging class of neural
networks are the wide ones, such as those deployed for NLP typeahead prediction
or recommender systems. Observing that these models share something in
common--an embedding layer that reduces the dimensionality of the input--we
focus on developing a general approach towards training these models that takes
advantage of the sparsity of the gradients. More abstractly, we address the
problem of differentially private empirical risk minimization (ERM) for models
that admit sparse gradients. We demonstrate that for non-convex ERM problems,
the loss is logarithmically dependent on the number of parameters, in contrast
with polynomial dependence for the general case. Following the same intuition,
we propose a novel algorithm for privately training neural networks. Finally,
we provide an empirical study of a DP wide neural network on a real-world
dataset, which has been rarely explored in the previous work.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Huanyu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mironov_I/0/1/0/all/0/1"&gt;Ilya Mironov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hejazinia_M/0/1/0/all/0/1"&gt;Meisam Hejazinia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cybersecurity Threats in Connected and Automated Vehicles based Federated Learning Systems. (arXiv:2102.13256v3 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.13256</id>
        <link href="http://arxiv.org/abs/2102.13256"/>
        <updated>2021-06-07T03:06:15.186Z</updated>
        <summary type="html"><![CDATA[Federated learning (FL) is a machine learning technique that aims at training
an algorithm across decentralized entities holding their local data private.
Wireless mobile networks allow users to communicate with other fixed or mobile
users. The road traffic network represents an infrastructure-based
configuration of a wireless mobile network where the Connected and Automated
Vehicles (CAV) represent the communicating entities. Applying FL in a wireless
mobile network setting gives rise to a new threat in the mobile environment
that is very different from the traditional fixed networks. The threat is due
to the intrinsic characteristics of the wireless medium and is caused by the
characteristics of the vehicular networks such as high node-mobility and
rapidly changing topology. Most cyber defense techniques depend on highly
reliable and connected networks. This paper explores falsified information
attacks, which target the FL process that is ongoing at the RSU. We identified
a number of attack strategies conducted by the malicious CAVs to disrupt the
training of the global model in vehicular networks. We show that the attacks
were able to increase the convergence time and decrease the accuracy the model.
We demonstrate that our attacks bypass FL defense strategies in their primary
form and highlight the need for novel poisoning resilience defense mechanisms
in the wireless mobile setting of the future road networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mallah_R/0/1/0/all/0/1"&gt;Ranwa Al Mallah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Badu_Marfo_G/0/1/0/all/0/1"&gt;Godwin Badu-Marfo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Farooq_B/0/1/0/all/0/1"&gt;Bilal Farooq&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FedCCEA : A Practical Approach of Client Contribution Evaluation for Federated Learning. (arXiv:2106.02310v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02310</id>
        <link href="http://arxiv.org/abs/2106.02310"/>
        <updated>2021-06-07T03:06:15.169Z</updated>
        <summary type="html"><![CDATA[Client contribution evaluation, also known as data valuation, is a crucial
approach in federated learning(FL) for client selection and incentive
allocation. However, due to restrictions of accessibility of raw data, only
limited information such as local weights and local data size of each client is
open for quantifying the client contribution. Using data size from available
information, we introduce an empirical evaluation method called Federated
Client Contribution Evaluation through Accuracy Approximation(FedCCEA). This
method builds the Accuracy Approximation Model(AAM), which estimates a
simulated test accuracy using inputs of sampled data size and extracts the
clients' data quality and data size to measure client contribution. FedCCEA
strengthens some advantages: (1) enablement of data size selection to the
clients, (2) feasible evaluation time regardless of the number of clients, and
(3) precise estimation in non-IID settings. We demonstrate the superiority of
FedCCEA compared to previous methods through several experiments: client
contribution distribution, client removal, and robustness test to partial
participation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shyn_S/0/1/0/all/0/1"&gt;Sung Kuk Shyn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1"&gt;Donghee Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1"&gt;Kwangsu Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Network Surrogate Models for Absorptivity and Emissivity Spectra of Multiple Elements. (arXiv:2106.02528v1 [physics.plasm-ph])]]></title>
        <id>http://arxiv.org/abs/2106.02528</id>
        <link href="http://arxiv.org/abs/2106.02528"/>
        <updated>2021-06-07T03:06:15.163Z</updated>
        <summary type="html"><![CDATA[Simulations of high energy density physics are expensive in terms of
computational resources. In particular, the computation of opacities of
plasmas, which are needed to accurately compute radiation transport in the
non-local thermal equilibrium (NLTE) regime, are expensive to the point of
easily requiring multiple times the sum-total compute time of all other
components of the simulation. As such, there is great interest in finding ways
to accelerate NLTE computations. Previous work has demonstrated that a
combination of fully-connected autoencoders and a deep jointly-informed neural
network (DJINN) can successfully replace the standard NLTE calculations for the
opacity of krypton. This work expands this idea to multiple elements in
demonstrating that individual surrogate models can be also be generated for
other elements with the focus being on creating autoencoders that can
accurately encode and decode the absorptivity and emissivity spectra.
Furthermore, this work shows that multiple elements across a large range of
atomic numbers can be combined into a single autoencoder when using a
convolutional autoencoder while maintaining accuracy that is comparable to
individual fully-connected autoencoders. Lastly, it is demonstrated that DJINN
can effectively learn the latent space of a convolutional autoencoder that can
encode multiple elements allowing the combination to effectively function as a
surrogate model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Wal_M/0/1/0/all/0/1"&gt;Michael D. Vander Wal&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/physics/1/au:+McClarren_R/0/1/0/all/0/1"&gt;Ryan G. McClarren&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/physics/1/au:+Humbird_K/0/1/0/all/0/1"&gt;Kelli D. Humbird&lt;/a&gt; (2) ((1) University of Notre Dame, (2) Lawrence Livermore National Laboratory)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Analysis of the robustness of NMF algorithms. (arXiv:2106.02213v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02213</id>
        <link href="http://arxiv.org/abs/2106.02213"/>
        <updated>2021-06-07T03:06:15.157Z</updated>
        <summary type="html"><![CDATA[We examine three non-negative matrix factorization techniques; L2-norm,
L1-norm, and L2,1-norm. Our aim is to establish the performance of these
different approaches, and their robustness in real-world applications such as
feature selection while managing computational complexity, sensitivity to noise
and more. We thoroughly examine each approach from a theoretical perspective,
and examine the performance of each using a series of experiments drawing on
both the ORL and YaleB datasets. We examine the Relative Reconstruction Errors
(RRE), Average Accuracy and Normalized Mutual Information (NMI) as criteria
under a range of simulated noise scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Diaz_A/0/1/0/all/0/1"&gt;Alex D&amp;#xed;az&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Steele_D/0/1/0/all/0/1"&gt;Damian Steele&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluation of Local Model-Agnostic Explanations Using Ground Truth. (arXiv:2106.02488v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02488</id>
        <link href="http://arxiv.org/abs/2106.02488"/>
        <updated>2021-06-07T03:06:15.152Z</updated>
        <summary type="html"><![CDATA[Explanation techniques are commonly evaluated using human-grounded methods,
limiting the possibilities for large-scale evaluations and rapid progress in
the development of new techniques. We propose a functionally-grounded
evaluation procedure for local model-agnostic explanation techniques. In our
approach, we generate ground truth for explanations when the black-box model is
Logistic Regression and Gaussian Naive Bayes and compare how similar each
explanation is to the extracted ground truth. In our empirical study,
explanations of Local Interpretable Model-agnostic Explanations (LIME), SHapley
Additive exPlanations (SHAP), and Local Permutation Importance (LPI) are
compared in terms of how similar they are to the extracted ground truth. In the
case of Logistic Regression, we find that the performance of the explanation
techniques is highly dependent on the normalization of the data. In contrast,
Local Permutation Importance outperforms the other techniques on Naive Bayes,
irrespective of normalization. We hope that this work lays the foundation for
further research into functionally-grounded evaluation methods for explanation
techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rahnama_A/0/1/0/all/0/1"&gt;Amir Hossein Akhavan Rahnama&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Butepage_J/0/1/0/all/0/1"&gt;Judith Butepage&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geurts_P/0/1/0/all/0/1"&gt;Pierre Geurts&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bostrom_H/0/1/0/all/0/1"&gt;Henrik Bostrom&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Deep Local and Global Scene-Graph Matching for Image-Text Retrieval. (arXiv:2106.02400v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02400</id>
        <link href="http://arxiv.org/abs/2106.02400"/>
        <updated>2021-06-07T03:06:15.145Z</updated>
        <summary type="html"><![CDATA[Conventional approaches to image-text retrieval mainly focus on indexing
visual objects appearing in pictures but ignore the interactions between these
objects. Such objects occurrences and interactions are equivalently useful and
important in this field as they are usually mentioned in the text. Scene graph
presentation is a suitable method for the image-text matching challenge and
obtained good results due to its ability to capture the inter-relationship
information. Both images and text are represented in scene graph levels and
formulate the retrieval challenge as a scene graph matching challenge. In this
paper, we introduce the Local and Global Scene Graph Matching (LGSGM) model
that enhances the state-of-the-art method by integrating an extra graph
convolution network to capture the general information of a graph.
Specifically, for a pair of scene graphs of an image and its caption, two
separate models are used to learn the features of each graph's nodes and edges.
Then a Siamese-structure graph convolution model is employed to embed graphs
into vector forms. We finally combine the graph-level and the vector-level to
calculate the similarity of this image-text pair. The empirical experiments
show that our enhancement with the combination of levels can improve the
performance of the baseline method by increasing the recall by more than 10% on
the Flickr30k dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_M/0/1/0/all/0/1"&gt;Manh-Duy Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_B/0/1/0/all/0/1"&gt;Binh T. Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gurrin_C/0/1/0/all/0/1"&gt;Cathal Gurrin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Event Classification with Multi-step Machine Learning. (arXiv:2106.02301v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02301</id>
        <link href="http://arxiv.org/abs/2106.02301"/>
        <updated>2021-06-07T03:06:15.138Z</updated>
        <summary type="html"><![CDATA[The usefulness and value of Multi-step Machine Learning (ML), where a task is
organized into connected sub-tasks with known intermediate inference goals, as
opposed to a single large model learned end-to-end without intermediate
sub-tasks, is presented. Pre-optimized ML models are connected and better
performance is obtained by re-optimizing the connected one. The selection of an
ML model from several small ML model candidates for each sub-task has been
performed by using the idea based on Neural Architecture Search (NAS). In this
paper, Differentiable Architecture Search (DARTS) and Single Path One-Shot NAS
(SPOS-NAS) are tested, where the construction of loss functions is improved to
keep all ML models smoothly learning. Using DARTS and SPOS-NAS as an
optimization and selection as well as the connections for multi-step machine
learning systems, we find that (1) such a system can quickly and successfully
select highly performant model combinations, and (2) the selected models are
consistent with baseline algorithms, such as grid search, and their outputs are
well controlled.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Saito_M/0/1/0/all/0/1"&gt;Masahiko Saito&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kishimoto_T/0/1/0/all/0/1"&gt;Tomoe Kishimoto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kaneta_Y/0/1/0/all/0/1"&gt;Yuya Kaneta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Itoh_T/0/1/0/all/0/1"&gt;Taichi Itoh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Umeda_Y/0/1/0/all/0/1"&gt;Yoshiaki Umeda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tanaka_J/0/1/0/all/0/1"&gt;Junichi Tanaka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iiyama_Y/0/1/0/all/0/1"&gt;Yutaro Iiyama&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sawada_R/0/1/0/all/0/1"&gt;Ryu Sawada&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Terashi_K/0/1/0/all/0/1"&gt;Koji Terashi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Equal Gender Representation in the Annotations of Toxic Language Detection. (arXiv:2106.02183v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02183</id>
        <link href="http://arxiv.org/abs/2106.02183"/>
        <updated>2021-06-07T03:06:15.120Z</updated>
        <summary type="html"><![CDATA[Classifiers tend to propagate biases present in the data on which they are
trained. Hence, it is important to understand how the demographic identities of
the annotators of comments affect the fairness of the resulting model. In this
paper, we focus on the differences in the ways men and women annotate comments
for toxicity, investigating how these differences result in models that amplify
the opinions of male annotators. We find that the BERT model as-sociates toxic
comments containing offensive words with male annotators, causing the model to
predict 67.7% of toxic comments as having been annotated by men. We show that
this disparity between gender predictions can be mitigated by removing
offensive words and highly toxic comments from the training data. We then apply
the learned associations between gender and language to toxic language
classifiers, finding that models trained exclusively on female-annotated data
perform 1.8% better than those trained solely on male-annotated data and that
training models on data after removing all offensive words reduces bias in the
model by 55.5% while increasing the sensitivity by 0.4%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Excell_E/0/1/0/all/0/1"&gt;Elizabeth Excell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moubayed_N/0/1/0/all/0/1"&gt;Noura Al Moubayed&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Collapse Under MSE Loss: Proximity to and Dynamics on the Central Path. (arXiv:2106.02073v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02073</id>
        <link href="http://arxiv.org/abs/2106.02073"/>
        <updated>2021-06-07T03:06:15.112Z</updated>
        <summary type="html"><![CDATA[Recent work [Papyan, Han, and Donoho, 2020] discovered a phenomenon called
Neural Collapse (NC) that occurs pervasively in today's deep net training
paradigm of driving cross-entropy loss towards zero. In this phenomenon, the
last-layer features collapse to their class-means, both the classifiers and
class-means collapse to the same Simplex Equiangular Tight Frame (ETF), and the
behavior of the last-layer classifier converges to that of the
nearest-class-mean decision rule. Since then, follow-ups-such as Mixon et al.
[2020] and Poggio and Liao [2020a,b]-formally analyzed this inductive bias by
replacing the hard-to-study cross-entropy by the more tractable mean squared
error (MSE) loss. But, these works stopped short of demonstrating the empirical
reality of MSE-NC on benchmark datasets and canonical networks-as had been done
in Papyan, Han, and Donoho [2020] for the cross-entropy loss. In this work, we
establish the empirical reality of MSE-NC by reporting experimental
observations for three prototypical networks and five canonical datasets with
code for reproducing NC. Following this, we develop three main contributions
inspired by MSE-NC. Firstly, we show a new theoretical decomposition of the MSE
loss into (A) a term assuming the last-layer classifier is exactly the
least-squares or Webb and Lowe [1990] classifier and (B) a term capturing the
deviation from this least-squares classifier. Secondly, we exhibit experiments
on canonical datasets and networks demonstrating that, during training,
term-(B) is negligible. This motivates a new theoretical construct: the central
path, where the linear classifier stays MSE-optimal-for the given feature
activations-throughout the dynamics. Finally, through our study of continually
renormalized gradient flow along the central path, we produce closed-form
dynamics that predict full Neural Collapse in an unconstrained features model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1"&gt;X.Y. Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Papyan_V/0/1/0/all/0/1"&gt;Vardan Papyan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Donoho_D/0/1/0/all/0/1"&gt;David L. Donoho&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Can convolutional ResNets approximately preserve input distances? A frequency analysis perspective. (arXiv:2106.02469v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02469</id>
        <link href="http://arxiv.org/abs/2106.02469"/>
        <updated>2021-06-07T03:06:15.106Z</updated>
        <summary type="html"><![CDATA[ResNets constrained to be bi-Lipschitz, that is, approximately distance
preserving, have been a crucial component of recently proposed techniques for
deterministic uncertainty quantification in neural models. We show that
theoretical justifications for recent regularisation schemes trying to enforce
such a constraint suffer from a crucial flaw -- the theoretical link between
the regularisation scheme used and bi-Lipschitzness is only valid under
conditions which do not hold in practice, rendering existing theory of limited
use, despite the strong empirical performance of these models. We provide a
theoretical explanation for the effectiveness of these regularisation schemes
using a frequency analysis perspective, showing that under mild conditions
these schemes will enforce a lower Lipschitz bound on the low-frequency
projection of images. We then provide empirical evidence supporting our
theoretical claims, and perform further experiments which demonstrate that our
broader conclusions appear to hold when some of the mathematical assumptions of
our proof are relaxed, corresponding to the setup used in prior work. In
addition, we present a simple constructive algorithm to search for counter
examples to the distance preservation condition, and discuss possible
implications of our theory for future model design.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Smith_L/0/1/0/all/0/1"&gt;Lewis Smith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Amersfoort_J/0/1/0/all/0/1"&gt;Joost van Amersfoort&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1"&gt;Haiwen Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roberts_S/0/1/0/all/0/1"&gt;Stephen Roberts&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gal_Y/0/1/0/all/0/1"&gt;Yarin Gal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Intelligent Transportation Systems to Mitigate Road Traffic Congestion. (arXiv:2106.02315v1 [eess.SY])]]></title>
        <id>http://arxiv.org/abs/2106.02315</id>
        <link href="http://arxiv.org/abs/2106.02315"/>
        <updated>2021-06-07T03:06:15.100Z</updated>
        <summary type="html"><![CDATA[Intelligent transport systems have efficiently and effectively proved
themselves in settling up the problem of traffic congestion around the world.
The multi-agent based transportation system is one of the most important
intelligent transport systems, which represents an interaction among the
neighbouring vehicles, drivers, roads, infrastructure and vehicles. In this
paper, two traffic management models have been created to mitigate congestion
and to ensure that emergency vehicles arrive as quickly as possible. A
tool-chain SUMO-JADE is employed to create a microscopic simulation symbolizing
the interactions of traffic. The simulation model has showed a significant
reduction of at least 50% in the average time delay and thus a real improvement
in the entire journey time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Hamadeh_N/0/1/0/all/0/1"&gt;Nizar Hamadeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Karouni_A/0/1/0/all/0/1"&gt;Ali Karouni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Farhat_Z/0/1/0/all/0/1"&gt;Zeinab Farhat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ghor_H/0/1/0/all/0/1"&gt;Hussein El Ghor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ghor_M/0/1/0/all/0/1"&gt;Mohamad El Ghor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Katea_I/0/1/0/all/0/1"&gt;Israa Katea&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Guided Visual Exploration of Relations in Data Sets. (arXiv:1905.02515v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1905.02515</id>
        <link href="http://arxiv.org/abs/1905.02515"/>
        <updated>2021-06-07T03:06:15.084Z</updated>
        <summary type="html"><![CDATA[Efficient explorative data analysis systems must take into account both what
a user knows and wants to know. This paper proposes a principled framework for
interactive visual exploration of relations in data, through views most
informative given the user's current knowledge and objectives. The user can
input pre-existing knowledge of relations in the data and also formulate
specific exploration interests, which are then taken into account in the
exploration. The idea is to steer the exploration process towards the interests
of the user, instead of showing uninteresting or already known relations. The
user's knowledge is modelled by a distribution over data sets parametrised by
subsets of rows and columns of data, called tile constraints. We provide a
computationally efficient implementation of this concept based on constrained
randomisation. Furthermore, we describe a novel dimensionality reduction method
for finding the views most informative to the user, which at the limit of no
background knowledge and with generic objectives reduces to PCA. We show that
the method is suitable for interactive use and is robust to noise, outperforms
standard projection pursuit visualisation methods, and gives understandable and
useful results in analysis of real-world data. We provide an open-source
implementation of the framework.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Puolamaki_K/0/1/0/all/0/1"&gt;Kai Puolam&amp;#xe4;ki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Oikarinen_E/0/1/0/all/0/1"&gt;Emilia Oikarinen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Henelius_A/0/1/0/all/0/1"&gt;Andreas Henelius&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tractable Regularization of Probabilistic Circuits. (arXiv:2106.02264v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02264</id>
        <link href="http://arxiv.org/abs/2106.02264"/>
        <updated>2021-06-07T03:06:15.078Z</updated>
        <summary type="html"><![CDATA[Probabilistic Circuits (PCs) are a promising avenue for probabilistic
modeling. They combine advantages of probabilistic graphical models (PGMs) with
those of neural networks (NNs). Crucially, however, they are tractable
probabilistic models, supporting efficient and exact computation of many
probabilistic inference queries, such as marginals and MAP. Further, since PCs
are structured computation graphs, they can take advantage of
deep-learning-style parameter updates, which greatly improves their
scalability. However, this innovation also makes PCs prone to overfitting,
which has been observed in many standard benchmarks. Despite the existence of
abundant regularization techniques for both PGMs and NNs, they are not
effective enough when applied to PCs. Instead, we re-think regularization for
PCs and propose two intuitive techniques, data softening and entropy
regularization, that both take advantage of PCs' tractability and still have an
efficient implementation as a computation graph. Specifically, data softening
provides a principled way to add uncertainty in datasets in closed form, which
implicitly regularizes PC parameters. To learn parameters from a softened
dataset, PCs only need linear time by virtue of their tractability. In entropy
regularization, the exact entropy of the distribution encoded by a PC can be
regularized directly, which is again infeasible for most other density
estimation models. We show that both methods consistently improve the
generalization performance of a wide variety of PCs. Moreover, when paired with
a simple PC structure, we achieved state-of-the-art results on 10 out of 20
standard discrete density estimation benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1"&gt;Anji Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Broeck_G/0/1/0/all/0/1"&gt;Guy Van den Broeck&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Survey on Deep Domain Adaptation for LiDAR Perception. (arXiv:2106.02377v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02377</id>
        <link href="http://arxiv.org/abs/2106.02377"/>
        <updated>2021-06-07T03:06:15.072Z</updated>
        <summary type="html"><![CDATA[Scalable systems for automated driving have to reliably cope with an
open-world setting. This means, the perception systems are exposed to drastic
domain shifts, like changes in weather conditions, time-dependent aspects, or
geographic regions. Covering all domains with annotated data is impossible
because of the endless variations of domains and the time-consuming and
expensive annotation process. Furthermore, fast development cycles of the
system additionally introduce hardware changes, such as sensor types and
vehicle setups, and the required knowledge transfer from simulation. To enable
scalable automated driving, it is therefore crucial to address these domain
shifts in a robust and efficient manner. Over the last years, a vast amount of
different domain adaptation techniques evolved. There already exists a number
of survey papers for domain adaptation on camera images, however, a survey for
LiDAR perception is absent. Nevertheless, LiDAR is a vital sensor for automated
driving that provides detailed 3D scans of the vehicle's surroundings. To
stimulate future research, this paper presents a comprehensive review of recent
progress in domain adaptation methods and formulates interesting research
questions specifically targeted towards LiDAR perception.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Triess_L/0/1/0/all/0/1"&gt;Larissa T. Triess&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dreissig_M/0/1/0/all/0/1"&gt;Mariella Dreissig&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rist_C/0/1/0/all/0/1"&gt;Christoph B. Rist&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zollner_J/0/1/0/all/0/1"&gt;J. Marius Z&amp;#xf6;llner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Regularizing Deep Networks with Semantic Data Augmentation. (arXiv:2007.10538v5 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.10538</id>
        <link href="http://arxiv.org/abs/2007.10538"/>
        <updated>2021-06-07T03:06:15.065Z</updated>
        <summary type="html"><![CDATA[Data augmentation is widely known as a simple yet surprisingly effective
technique for regularizing deep networks. Conventional data augmentation
schemes, e.g., flipping, translation or rotation, are low-level,
data-independent and class-agnostic operations, leading to limited diversity
for augmented samples. To this end, we propose a novel semantic data
augmentation algorithm to complement traditional approaches. The proposed
method is inspired by the intriguing property that deep networks are effective
in learning linearized features, i.e., certain directions in the deep feature
space correspond to meaningful semantic transformations, e.g., changing the
background or view angle of an object. Based on this observation, translating
training samples along many such directions in the feature space can
effectively augment the dataset for more diversity. To implement this idea, we
first introduce a sampling based method to obtain semantically meaningful
directions efficiently. Then, an upper bound of the expected cross-entropy (CE)
loss on the augmented training set is derived by assuming the number of
augmented samples goes to infinity, yielding a highly efficient algorithm. In
fact, we show that the proposed implicit semantic data augmentation (ISDA)
algorithm amounts to minimizing a novel robust CE loss, which adds minimal
extra computational cost to a normal training procedure. In addition to
supervised learning, ISDA can be applied to semi-supervised learning tasks
under the consistency regularization framework, where ISDA amounts to
minimizing the upper bound of the expected KL-divergence between the augmented
features and the original features. Although being simple, ISDA consistently
improves the generalization performance of popular deep models (e.g., ResNets
and DenseNets) on a variety of datasets, i.e., CIFAR-10, CIFAR-100, SVHN,
ImageNet, and Cityscapes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yulin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1"&gt;Gao Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1"&gt;Shiji Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1"&gt;Xuran Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1"&gt;Yitong Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1"&gt;Cheng Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Privately Learning Mixtures of Axis-Aligned Gaussians. (arXiv:2106.02162v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02162</id>
        <link href="http://arxiv.org/abs/2106.02162"/>
        <updated>2021-06-07T03:06:15.059Z</updated>
        <summary type="html"><![CDATA[We consider the problem of learning mixtures of Gaussians under the
constraint of approximate differential privacy. We prove that
$\widetilde{O}(k^2 d \log^{3/2}(1/\delta) / \alpha^2 \varepsilon)$ samples are
sufficient to learn a mixture of $k$ axis-aligned Gaussians in $\mathbb{R}^d$
to within total variation distance $\alpha$ while satisfying $(\varepsilon,
\delta)$-differential privacy. This is the first result for privately learning
mixtures of unbounded axis-aligned (or even unbounded univariate) Gaussians. If
the covariance matrices of each of the Gaussians is the identity matrix, we
show that $\widetilde{O}(kd/\alpha^2 + kd \log(1/\delta) / \alpha \varepsilon)$
samples are sufficient.

Recently, the "local covering" technique of Bun, Kamath, Steinke, and Wu has
been successfully used for privately learning high-dimensional Gaussians with a
known covariance matrix and extended to privately learning general
high-dimensional Gaussians by Aden-Ali, Ashtiani, and Kamath. Given these
positive results, this approach has been proposed as a promising direction for
privately learning mixtures of Gaussians. Unfortunately, we show that this is
not possible.

We design a new technique for privately learning mixture distributions. A
class of distributions $\mathcal{F}$ is said to be list-decodable if there is
an algorithm that, given "heavily corrupted" samples from $f\in \mathcal{F}$,
outputs a list of distributions, $\widehat{\mathcal{F}}$, such that one of the
distributions in $\widehat{\mathcal{F}}$ approximates $f$. We show that if
$\mathcal{F}$ is privately list-decodable, then we can privately learn mixtures
of distributions in $\mathcal{F}$. Finally, we show axis-aligned Gaussian
distributions are privately list-decodable, thereby proving mixtures of such
distributions are privately learnable.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aden_Ali_I/0/1/0/all/0/1"&gt;Ishaq Aden-Ali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ashtiani_H/0/1/0/all/0/1"&gt;Hassan Ashtiani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liaw_C/0/1/0/all/0/1"&gt;Christopher Liaw&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Materials Representation and Transfer Learning for Multi-Property Prediction. (arXiv:2106.02225v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02225</id>
        <link href="http://arxiv.org/abs/2106.02225"/>
        <updated>2021-06-07T03:06:15.053Z</updated>
        <summary type="html"><![CDATA[The adoption of machine learning in materials science has rapidly transformed
materials property prediction. Hurdles limiting full capitalization of recent
advancements in machine learning include the limited development of methods to
learn the underlying interactions of multiple elements, as well as the
relationships among multiple properties, to facilitate property prediction in
new composition spaces. To address these issues, we introduce the Hierarchical
Correlation Learning for Multi-property Prediction (H-CLMP) framework that
seamlessly integrates (i) prediction using only a material's composition, (ii)
learning and exploitation of correlations among target properties in
multi-target regression, and (iii) leveraging training data from tangential
domains via generative transfer learning. The model is demonstrated for
prediction of spectral optical absorption of complex metal oxides spanning 69
3-cation metal oxide composition spaces. H-CLMP accurately predicts non-linear
composition-property relationships in composition spaces for which no training
data is available, which broadens the purview of machine learning to the
discovery of materials with exceptional properties. This achievement results
from the principled integration of latent embedding learning, property
correlation learning, generative transfer learning, and attention models. The
best performance is obtained using H-CLMP with Transfer learning (H-CLMP(T))
wherein a generative adversarial network is trained on computational density of
states data and deployed in the target domain to augment prediction of optical
absorption from composition. H-CLMP(T) aggregates multiple knowledge sources
with a framework that is well-suited for multi-target regression across the
physical sciences.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kong_S/0/1/0/all/0/1"&gt;Shufeng Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guevarra_D/0/1/0/all/0/1"&gt;Dan Guevarra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gomes_C/0/1/0/all/0/1"&gt;Carla P. Gomes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gregoire_J/0/1/0/all/0/1"&gt;John M. Gregoire&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Temporally coherent video anonymization through GAN inpainting. (arXiv:2106.02328v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02328</id>
        <link href="http://arxiv.org/abs/2106.02328"/>
        <updated>2021-06-07T03:06:15.045Z</updated>
        <summary type="html"><![CDATA[This work tackles the problem of temporally coherent face anonymization in
natural video streams.We propose JaGAN, a two-stage system starting with
detecting and masking out faces with black image patches in all individual
frames of the video. The second stage leverages a privacy-preserving Video
Generative Adversarial Network designed to inpaint the missing image patches
with artificially generated faces. Our initial experiments reveal that image
based generative models are not capable of inpainting patches showing temporal
coherent appearance across neighboring video frames. To address this issue we
introduce a newly curated video collection, which is made publicly available
for the research community along with this paper. We also introduce the
Identity Invariance Score IdI as a means to quantify temporal coherency between
neighboring frames.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Balaji_T/0/1/0/all/0/1"&gt;Thangapavithraa Balaji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Blies_P/0/1/0/all/0/1"&gt;Patrick Blies&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gori_G/0/1/0/all/0/1"&gt;Georg G&amp;#xf6;ri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mitsch_R/0/1/0/all/0/1"&gt;Raphael Mitsch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wasserer_M/0/1/0/all/0/1"&gt;Marcel Wasserer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schon_T/0/1/0/all/0/1"&gt;Torsten Sch&amp;#xf6;n&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Entity Concept-enhanced Few-shot Relation Extraction. (arXiv:2106.02401v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02401</id>
        <link href="http://arxiv.org/abs/2106.02401"/>
        <updated>2021-06-07T03:06:15.039Z</updated>
        <summary type="html"><![CDATA[Few-shot relation extraction (FSRE) is of great importance in long-tail
distribution problem, especially in special domain with low-resource data. Most
existing FSRE algorithms fail to accurately classify the relations merely based
on the information of the sentences together with the recognized entity pairs,
due to limited samples and lack of knowledge. To address this problem, in this
paper, we proposed a novel entity CONCEPT-enhanced FEw-shot Relation Extraction
scheme (ConceptFERE), which introduces the inherent concepts of entities to
provide clues for relation prediction and boost the relations classification
performance. Firstly, a concept-sentence attention module is developed to
select the most appropriate concept from multiple concepts of each entity by
calculating the semantic similarity between sentences and concepts. Secondly, a
self-attention based fusion module is presented to bridge the gap of concept
embedding and sentence embedding from different semantic spaces. Extensive
experiments on the FSRE benchmark dataset FewRel have demonstrated the
effectiveness and the superiority of the proposed ConceptFERE scheme as
compared to the state-of-the-art baselines. Code is available at
https://github.com/LittleGuoKe/ConceptFERE.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1"&gt;Shan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yongfei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niu_G/0/1/0/all/0/1"&gt;Guanglin Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1"&gt;Qinghua Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pu_S/0/1/0/all/0/1"&gt;Shiliang Pu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Counterfactual Graph Learning for Link Prediction. (arXiv:2106.02172v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02172</id>
        <link href="http://arxiv.org/abs/2106.02172"/>
        <updated>2021-06-07T03:06:15.009Z</updated>
        <summary type="html"><![CDATA[Learning to predict missing links is important for many graph-based
applications. Existing methods were designed to learn the observed association
between two sets of variables: (1) the observed graph structure and (2) the
existence of link between a pair of nodes. However, the causal relationship
between these variables was ignored and we visit the possibility of learning it
by simply asking a counterfactual question: "would the link exist or not if the
observed graph structure became different?" To answer this question by causal
inference, we consider the information of the node pair as context, global
graph structural properties as treatment, and link existence as outcome. In
this work, we propose a novel link prediction method that enhances graph
learning by the counterfactual inference. It creates counterfactual links from
the observed ones, and our method learns representations from both of them.
Experiments on a number of benchmark datasets show that our proposed method
achieves the state-of-the-art performance on link prediction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1"&gt;Tong Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1"&gt;Gang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1"&gt;Daheng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1"&gt;Wenhao Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1"&gt;Meng Jiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Proving Equivalence Between Complex Expressions Using Graph-to-Sequence Neural Models. (arXiv:2106.02452v1 [cs.PL])]]></title>
        <id>http://arxiv.org/abs/2106.02452</id>
        <link href="http://arxiv.org/abs/2106.02452"/>
        <updated>2021-06-07T03:06:14.983Z</updated>
        <summary type="html"><![CDATA[We target the problem of provably computing the equivalence between two
complex expression trees. To this end, we formalize the problem of equivalence
between two such programs as finding a set of semantics-preserving rewrite
rules from one into the other, such that after the rewrite the two programs are
structurally identical, and therefore trivially equivalent.We then develop a
graph-to-sequence neural network system for program equivalence, trained to
produce such rewrite sequences from a carefully crafted automatic example
generation algorithm. We extensively evaluate our system on a rich multi-type
linear algebra expression language, using arbitrary combinations of 100+
graph-rewriting axioms of equivalence. Our machine learning system guarantees
correctness for all true negatives, and ensures 0 false positive by design. It
outputs via inference a valid proof of equivalence for 93% of the 10,000
equivalent expression pairs isolated for testing, using up to 50-term
expressions. In all cases, the validity of the sequence produced and therefore
the provable assertion of program equivalence is always computable, in
negligible time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kommrusch_S/0/1/0/all/0/1"&gt;Steve Kommrusch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barollet_T/0/1/0/all/0/1"&gt;Th&amp;#xe9;o Barollet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pouchet_L/0/1/0/all/0/1"&gt;Louis-No&amp;#xeb;l Pouchet&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stochastic gradient descent with noise of machine learning type. Part II: Continuous time analysis. (arXiv:2106.02588v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02588</id>
        <link href="http://arxiv.org/abs/2106.02588"/>
        <updated>2021-06-07T03:06:14.972Z</updated>
        <summary type="html"><![CDATA[The representation of functions by artificial neural networks depends on a
large number of parameters in a non-linear fashion. Suitable parameters of
these are found by minimizing a 'loss functional', typically by stochastic
gradient descent (SGD) or an advanced SGD-based algorithm.

In a continuous time model for SGD with noise that follows the 'machine
learning scaling', we show that in a certain noise regime, the optimization
algorithm prefers 'flat' minima of the objective function in a sense which is
different from the flat minimum selection of continuous time SGD with
homogeneous noise.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wojtowytsch_S/0/1/0/all/0/1"&gt;Stephan Wojtowytsch&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Signed Cumulative Distribution Transform for 1-D Signal Analysis and Classification. (arXiv:2106.02146v1 [cs.IT])]]></title>
        <id>http://arxiv.org/abs/2106.02146</id>
        <link href="http://arxiv.org/abs/2106.02146"/>
        <updated>2021-06-07T03:06:14.964Z</updated>
        <summary type="html"><![CDATA[This paper presents a new mathematical signal transform that is especially
suitable for decoding information related to non-rigid signal displacements. We
provide a measure theoretic framework to extend the existing Cumulative
Distribution Transform [ACHA 45 (2018), no. 3, 616-641] to arbitrary (signed)
signals on $\overline{\mathbb{R}}$. We present both forward (analysis) and
inverse (synthesis) formulas for the transform, and describe several of its
properties including translation, scaling, convexity, linear separability and
others. Finally, we describe a metric in transform space, and demonstrate the
application of the transform in classifying (detecting) signals under random
displacements.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aldroubi_A/0/1/0/all/0/1"&gt;Akram Aldroubi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martin_R/0/1/0/all/0/1"&gt;Rocio Diaz Martin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Medri_I/0/1/0/all/0/1"&gt;Ivan Medri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rohde_G/0/1/0/all/0/1"&gt;Gustavo K. Rohde&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thareja_S/0/1/0/all/0/1"&gt;Sumati Thareja&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fuzzy Clustering with Similarity Queries. (arXiv:2106.02212v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02212</id>
        <link href="http://arxiv.org/abs/2106.02212"/>
        <updated>2021-06-07T03:06:14.945Z</updated>
        <summary type="html"><![CDATA[The fuzzy or soft $k$-means objective is a popular generalization of the
well-known $k$-means problem, extending the clustering capability of the
$k$-means to datasets that are uncertain, vague, and otherwise hard to cluster.
In this paper, we propose a semi-supervised active clustering framework, where
the learner is allowed to interact with an oracle (domain expert), asking for
the similarity between a certain set of chosen items. We study the query and
computational complexities of clustering in this framework. We prove that
having a few of such similarity queries enables one to get a polynomial-time
approximation algorithm to an otherwise conjecturally NP-hard problem. In
particular, we provide probabilistic algorithms for fuzzy clustering in this
setting that asks $O(\mathsf{poly}(k)\log n)$ similarity queries and run with
polynomial-time-complexity, where $n$ is the number of items. The fuzzy
$k$-means objective is nonconvex, with $k$-means as a special case, and is
equivalent to some other generic nonconvex problem such as non-negative matrix
factorization. The ubiquitous Lloyd-type algorithms (or,
expectation-maximization algorithm) can get stuck at a local minima. Our
results show that by making few similarity queries, the problem becomes easier
to solve. Finally, we test our algorithms over real-world datasets, showing
their effectiveness in real-world applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huleihel_W/0/1/0/all/0/1"&gt;Wasim Huleihel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mazumdar_A/0/1/0/all/0/1"&gt;Arya Mazumdar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pal_S/0/1/0/all/0/1"&gt;Soumyabrata Pal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Intelligent Resource Reservation for Crowdsourced Live Video Streaming Applications in Geo-Distributed Cloud Environment. (arXiv:2106.02420v1 [cs.NI])]]></title>
        <id>http://arxiv.org/abs/2106.02420</id>
        <link href="http://arxiv.org/abs/2106.02420"/>
        <updated>2021-06-07T03:06:14.933Z</updated>
        <summary type="html"><![CDATA[Crowdsourced live video streaming (livecast) services such as Facebook Live,
YouNow, Douyu and Twitch are gaining more momentum recently. Allocating the
limited resources in a cost-effective manner while maximizing the Quality of
Service (QoS) through real-time delivery and the provision of the appropriate
representations for all viewers is a challenging problem. In our paper, we
introduce a machine-learning based predictive resource allocation framework for
geo-distributed cloud sites, considering the delay and quality constraints to
guarantee the maximum QoS for viewers and the minimum cost for content
providers. First, we present an offline optimization that decides the required
transcoding resources in distributed regions near the viewers with a trade-off
between the QoS and the overall cost. Second, we use machine learning to build
forecasting models that proactively predict the approximate transcoding
resources to be reserved at each cloud site ahead of time. Finally, we develop
a Greedy Nearest and Cheapest algorithm (GNCA) to perform the resource
allocation of real-time broadcasted videos on the rented resources. Extensive
simulations have shown that GNCA outperforms the state-of-the art resource
allocation approaches for crowdsourced live streaming by achieving more than
20% gain in terms of system cost while serving the viewers with relatively
lower latency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Baccour_E/0/1/0/all/0/1"&gt;Emna Baccour&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Haouari_F/0/1/0/all/0/1"&gt;Fatima Haouari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Erbad_A/0/1/0/all/0/1"&gt;Aiman Erbad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mohamed_A/0/1/0/all/0/1"&gt;Amr Mohamed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bilal_K/0/1/0/all/0/1"&gt;Kashif Bilal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guizani_M/0/1/0/all/0/1"&gt;Mohsen Guizani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hamdi_M/0/1/0/all/0/1"&gt;Mounir Hamdi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Beyond Target Networks: Improving Deep $Q$-learning with Functional Regularization. (arXiv:2106.02613v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.02613</id>
        <link href="http://arxiv.org/abs/2106.02613"/>
        <updated>2021-06-07T03:06:14.915Z</updated>
        <summary type="html"><![CDATA[Target networks are at the core of recent success in Reinforcement Learning.
They stabilize the training by using old parameters to estimate the $Q$-values,
but this also limits the propagation of newly-encountered rewards which could
ultimately slow down the training. In this work, we propose an alternative
training method based on functional regularization which does not have this
deficiency. Unlike target networks, our method uses up-to-date parameters to
estimate the target $Q$-values, thereby speeding up training while maintaining
stability. Surprisingly, in some cases, we can show that target networks are a
special, restricted type of functional regularizers. Using this approach, we
show empirical improvements in sample efficiency and performance across a range
of Atari and simulated robotics environments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Piche_A/0/1/0/all/0/1"&gt;Alexandre Pich&amp;#xe9;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Marino_J/0/1/0/all/0/1"&gt;Joseph Marino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Marconi_G/0/1/0/all/0/1"&gt;Gian Maria Marconi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Pal_C/0/1/0/all/0/1"&gt;Christopher Pal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Khan_M/0/1/0/all/0/1"&gt;Mohammad Emtiyaz Khan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spatial Graph Attention and Curiosity-driven Policy for Antiviral Drug Discovery. (arXiv:2106.02190v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02190</id>
        <link href="http://arxiv.org/abs/2106.02190"/>
        <updated>2021-06-07T03:06:14.892Z</updated>
        <summary type="html"><![CDATA[We developed Distilled Graph Attention Policy Networks (DGAPNs), a
curiosity-driven reinforcement learning model to generate novel
graph-structured chemical representations that optimize user-defined objectives
by efficiently navigating a physically constrained domain. The framework is
examined on the task of generating molecules that are designed to bind,
noncovalently, to functional sites of SARS-CoV-2 proteins. We present a spatial
Graph Attention Network (sGAT) that leverages self-attention over both node and
edge attributes as well as encoding spatial structure -- this capability is of
considerable interest in areas such as molecular and synthetic biology and drug
discovery. An attentional policy network is then introduced to learn decision
rules for a dynamic, fragment-based chemical environment, and state-of-the-art
policy gradient techniques are employed to train the network with enhanced
stability. Exploration is efficiently encouraged by incorporating innovation
reward bonuses learned and proposed by random network distillation. In
experiments, our framework achieved outstanding results compared to
state-of-the-art algorithms, while increasing the diversity of proposed
molecules and reducing the complexity of paths to chemical synthesis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yulun Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choma_N/0/1/0/all/0/1"&gt;Nicholas Choma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1"&gt;Andrew Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cashman_M/0/1/0/all/0/1"&gt;Mikaela Cashman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prates_E/0/1/0/all/0/1"&gt;&amp;#xc9;rica T. Prates&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1"&gt;Manesh Shah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vergara_V/0/1/0/all/0/1"&gt;Ver&amp;#xf3;nica G. Melesse Vergara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Clyde_A/0/1/0/all/0/1"&gt;Austin Clyde&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brettin_T/0/1/0/all/0/1"&gt;Thomas S. Brettin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jong_W/0/1/0/all/0/1"&gt;Wibe A. de Jong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_N/0/1/0/all/0/1"&gt;Neeraj Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Head_M/0/1/0/all/0/1"&gt;Martha S. Head&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stevens_R/0/1/0/all/0/1"&gt;Rick L. Stevens&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nugent_P/0/1/0/all/0/1"&gt;Peter Nugent&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jacobson_D/0/1/0/all/0/1"&gt;Daniel A. Jacobson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brown_J/0/1/0/all/0/1"&gt;James B. Brown&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Learning of General-Purpose Embeddings for Code Changes. (arXiv:2106.02087v1 [cs.SE])]]></title>
        <id>http://arxiv.org/abs/2106.02087</id>
        <link href="http://arxiv.org/abs/2106.02087"/>
        <updated>2021-06-07T03:06:14.886Z</updated>
        <summary type="html"><![CDATA[A lot of problems in the field of software engineering - bug fixing, commit
message generation, etc. - require analyzing not only the code itself but
specifically code changes. Applying machine learning models to these tasks
requires us to create numerical representations of the changes, i.e.
embeddings. Recent studies demonstrate that the best way to obtain these
embeddings is to pre-train a deep neural network in an unsupervised manner on a
large volume of unlabeled data and then further fine-tune it for a specific
task.

In this work, we propose an approach for obtaining such embeddings of code
changes during pre-training and evaluate them on two different downstream tasks
- applying changes to code and commit message generation. The pre-training
consists of the model learning to apply the given change (an edit sequence) to
the code in a correct way, and therefore requires only the code change itself.
To increase the quality of the obtained embeddings, we only consider the
changed tokens in the edit sequence. In the task of applying code changes, our
model outperforms the model that uses full edit sequences by 5.9 percentage
points in accuracy. As for the commit message generation, our model
demonstrated the same results as supervised models trained for this specific
task, which indicates that it can encode code changes well and can be improved
in the future by pre-training on a larger dataset of easily gathered code
changes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pravilov_M/0/1/0/all/0/1"&gt;Mikhail Pravilov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bogomolov_E/0/1/0/all/0/1"&gt;Egor Bogomolov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Golubev_Y/0/1/0/all/0/1"&gt;Yaroslav Golubev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bryksin_T/0/1/0/all/0/1"&gt;Timofey Bryksin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hallucination In Object Detection -- A Study In Visual Part Verification. (arXiv:2106.02523v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02523</id>
        <link href="http://arxiv.org/abs/2106.02523"/>
        <updated>2021-06-07T03:06:14.880Z</updated>
        <summary type="html"><![CDATA[We show that object detectors can hallucinate and detect missing objects;
potentially even accurately localized at their expected, but non-existing,
position. This is particularly problematic for applications that rely on visual
part verification: detecting if an object part is present or absent. We show
how popular object detectors hallucinate objects in a visual part verification
task and introduce the first visual part verification dataset: DelftBikes,
which has 10,000 bike photographs, with 22 densely annotated parts per image,
where some parts may be missing. We explicitly annotated an extra object state
label for each part to reflect if a part is missing or intact. We propose to
evaluate visual part verification by relying on recall and compare popular
object detectors on DelftBikes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kayhan_O/0/1/0/all/0/1"&gt;Osman Semih Kayhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vredebregt_B/0/1/0/all/0/1"&gt;Bart Vredebregt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gemert_J/0/1/0/all/0/1"&gt;Jan C. van Gemert&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robustifying Reinforcement Learning Policies with $\mathcal{L}_1$ Adaptive Control. (arXiv:2106.02249v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02249</id>
        <link href="http://arxiv.org/abs/2106.02249"/>
        <updated>2021-06-07T03:06:14.874Z</updated>
        <summary type="html"><![CDATA[A reinforcement learning (RL) policy trained in a nominal environment could
fail in a new/perturbed environment due to the existence of dynamic variations.
Existing robust methods try to obtain a fixed policy for all envisioned dynamic
variation scenarios through robust or adversarial training. These methods could
lead to conservative performance due to emphasis on the worst case, and often
involve tedious modifications to the training environment. We propose an
approach to robustifying a pre-trained non-robust RL policy with
$\mathcal{L}_1$ adaptive control. Leveraging the capability of an
$\mathcal{L}_1$ control law in the fast estimation of and active compensation
for dynamic variations, our approach can significantly improve the robustness
of an RL policy trained in a standard (i.e., non-robust) way, either in a
simulator or in the real world. Numerical experiments are provided to validate
the efficacy of the proposed approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1"&gt;Yikun Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1"&gt;Pan Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gandhi_M/0/1/0/all/0/1"&gt;Manan Gandhi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bo Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Theodorou_E/0/1/0/all/0/1"&gt;Evangelos Theodorou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hovakimyan_N/0/1/0/all/0/1"&gt;Naira Hovakimyan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How Good Is NLP? A Sober Look at NLP Tasks through the Lens of Social Impact. (arXiv:2106.02359v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02359</id>
        <link href="http://arxiv.org/abs/2106.02359"/>
        <updated>2021-06-07T03:06:14.867Z</updated>
        <summary type="html"><![CDATA[Recent years have seen many breakthroughs in natural language processing
(NLP), transitioning it from a mostly theoretical field to one with many
real-world applications. Noting the rising number of applications of other
machine learning and AI techniques with pervasive societal impact, we
anticipate the rising importance of developing NLP technologies for social
good. Inspired by theories in moral philosophy and global priorities research,
we aim to promote a guideline for social good in the context of NLP. We lay the
foundations via moral philosophy's definition of social good, propose a
framework to evaluate NLP tasks' direct and indirect real-world impact, and
adopt the methodology of global priorities research to identify priority causes
for NLP research. Finally, we use our theoretical framework to provide some
practical guidelines for future NLP research for social good. Our data and
codes are available at this http URL]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1"&gt;Zhijing Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chauhan_G/0/1/0/all/0/1"&gt;Geeticka Chauhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tse_B/0/1/0/all/0/1"&gt;Brian Tse&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1"&gt;Mrinmaya Sachan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mihalcea_R/0/1/0/all/0/1"&gt;Rada Mihalcea&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Double Descent Optimization Pattern and Aliasing: Caveats of Noisy Labels. (arXiv:2106.02100v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02100</id>
        <link href="http://arxiv.org/abs/2106.02100"/>
        <updated>2021-06-07T03:06:14.849Z</updated>
        <summary type="html"><![CDATA[Optimization plays a key role in the training of deep neural networks.
Deciding when to stop training can have a substantial impact on the performance
of the network during inference. Under certain conditions, the generalization
error can display a double descent pattern during training: the learning curve
is non-monotonic and seemingly diverges before converging again after
additional epochs. This optimization pattern can lead to early stopping
procedures to stop training before the second convergence and consequently
select a suboptimal set of parameters for the network, with worse performance
during inference. In this work, in addition to confirming that double descent
occurs with small datasets and noisy labels as evidenced by others, we show
that noisy labels must be present both in the training and generalization sets
to observe a double descent pattern. We also show that the learning rate has an
influence on double descent, and study how different optimizers and optimizer
parameters influence the apparition of double descent. Finally, we show that
increasing the learning rate can create an aliasing effect that masks the
double descent pattern without suppressing it. We study this phenomenon through
extensive experiments on variants of CIFAR-10 and show that they translate to a
real world application: the forecast of seizure events in epileptic patients
from continuous electroencephalographic recordings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dubost_F/0/1/0/all/0/1"&gt;Florian Dubost&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saab_K/0/1/0/all/0/1"&gt;Khaled Kamal Saab&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hong_E/0/1/0/all/0/1"&gt;Erin Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_D/0/1/0/all/0/1"&gt;Daniel Yang Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pike_M/0/1/0/all/0/1"&gt;Max Pike&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_S/0/1/0/all/0/1"&gt;Siddharth Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1"&gt;Siyi Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhaskhar_N/0/1/0/all/0/1"&gt;Nandita Bhaskhar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_Messer_C/0/1/0/all/0/1"&gt;Christopher Lee-Messer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rubin_D/0/1/0/all/0/1"&gt;Daniel Rubin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Closer Look at the Worst-case Behavior of Multi-armed Bandit Algorithms. (arXiv:2106.02126v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02126</id>
        <link href="http://arxiv.org/abs/2106.02126"/>
        <updated>2021-06-07T03:06:14.842Z</updated>
        <summary type="html"><![CDATA[One of the key drivers of complexity in the classical (stochastic)
multi-armed bandit (MAB) problem is the difference between mean rewards in the
top two arms, also known as the instance gap. The celebrated Upper Confidence
Bound (UCB) policy is among the simplest optimism-based MAB algorithms that
naturally adapts to this gap: for a horizon of play n, it achieves optimal
O(log n) regret in instances with "large" gaps, and a near-optimal O(\sqrt{n
log n}) minimax regret when the gap can be arbitrarily "small." This paper
provides new results on the arm-sampling behavior of UCB, leading to several
important insights. Among these, it is shown that arm-sampling rates under UCB
are asymptotically deterministic, regardless of the problem complexity. This
discovery facilitates new sharp asymptotics and a novel alternative proof for
the O(\sqrt{n log n}) minimax regret of UCB. Furthermore, the paper also
provides the first complete process-level characterization of the MAB problem
under UCB in the conventional diffusion scaling. Among other things, the
"small" gap worst-case lens adopted in this paper also reveals profound
distinctions between the behavior of UCB and Thompson Sampling, such as an
"incomplete learning" phenomenon characteristic of the latter.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kalvit_A/0/1/0/all/0/1"&gt;Anand Kalvit&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeevi_A/0/1/0/all/0/1"&gt;Assaf Zeevi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SAND-mask: An Enhanced Gradient Masking Strategy for the Discovery of Invariances in Domain Generalization. (arXiv:2106.02266v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02266</id>
        <link href="http://arxiv.org/abs/2106.02266"/>
        <updated>2021-06-07T03:06:14.835Z</updated>
        <summary type="html"><![CDATA[A major bottleneck in the real-world applications of machine learning models
is their failure in generalizing to unseen domains whose data distribution is
not i.i.d to the training domains. This failure often stems from learning
non-generalizable features in the training domains that are spuriously
correlated with the label of data. To address this shortcoming, there has been
a growing surge of interest in learning good explanations that are hard to
vary, which is studied under the notion of Out-of-Distribution (OOD)
Generalization. The search for good explanations that are \textit{invariant}
across different domains can be seen as finding local (global) minimas in the
loss landscape that hold true across all of the training domains. In this
paper, we propose a masking strategy, which determines a continuous weight
based on the agreement of gradients that flow in each edge of network, in order
to control the amount of update received by the edge in each step of
optimization. Particularly, our proposed technique referred to as "Smoothed-AND
(SAND)-masking", not only validates the agreement in the direction of gradients
but also promotes the agreement among their magnitudes to further ensure the
discovery of invariances across training domains. SAND-mask is validated over
the Domainbed benchmark for domain generalization and significantly improves
the state-of-the-art accuracy on the Colored MNIST dataset while providing
competitive results on other domain generalization datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shahtalebi_S/0/1/0/all/0/1"&gt;Soroosh Shahtalebi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gagnon_Audet_J/0/1/0/all/0/1"&gt;Jean-Christophe Gagnon-Audet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Laleh_T/0/1/0/all/0/1"&gt;Touraj Laleh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Faramarzi_M/0/1/0/all/0/1"&gt;Mojtaba Faramarzi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ahuja_K/0/1/0/all/0/1"&gt;Kartik Ahuja&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rish_I/0/1/0/all/0/1"&gt;Irina Rish&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Finding and Fixing Spurious Patterns with Explanations. (arXiv:2106.02112v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02112</id>
        <link href="http://arxiv.org/abs/2106.02112"/>
        <updated>2021-06-07T03:06:14.827Z</updated>
        <summary type="html"><![CDATA[Machine learning models often use spurious patterns such as "relying on the
presence of a person to detect a tennis racket," which do not generalize. In
this work, we present an end-to-end pipeline for identifying and mitigating
spurious patterns for image classifiers. We start by finding patterns such as
"the model's prediction for tennis racket changes 63% of the time if we hide
the people." Then, if a pattern is spurious, we mitigate it via a novel form of
data augmentation. We demonstrate that this approach identifies a diverse set
of spurious patterns and that it mitigates them by producing a model that is
both more accurate on a distribution where the spurious pattern is not helpful
and more robust to distribution shift.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Plumb_G/0/1/0/all/0/1"&gt;Gregory Plumb&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ribeiro_M/0/1/0/all/0/1"&gt;Marco Tulio Ribeiro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Talwalkar_A/0/1/0/all/0/1"&gt;Ameet Talwalkar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Provably Efficient Online Hyperparameter Optimization with Population-Based Bandits. (arXiv:2002.02518v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.02518</id>
        <link href="http://arxiv.org/abs/2002.02518"/>
        <updated>2021-06-07T03:06:14.822Z</updated>
        <summary type="html"><![CDATA[Many of the recent triumphs in machine learning are dependent on well-tuned
hyperparameters. This is particularly prominent in reinforcement learning (RL)
where a small change in the configuration can lead to failure. Despite the
importance of tuning hyperparameters, it remains expensive and is often done in
a naive and laborious way. A recent solution to this problem is Population
Based Training (PBT) which updates both weights and hyperparameters in a single
training run of a population of agents. PBT has been shown to be particularly
effective in RL, leading to widespread use in the field. However, PBT lacks
theoretical guarantees since it relies on random heuristics to explore the
hyperparameter space. This inefficiency means it typically requires vast
computational resources, which is prohibitive for many small and medium sized
labs. In this work, we introduce the first provably efficient PBT-style
algorithm, Population-Based Bandits (PB2). PB2 uses a probabilistic model to
guide the search in an efficient way, making it possible to discover high
performing hyperparameter configurations with far fewer agents than typically
required by PBT. We show in a series of RL experiments that PB2 is able to
achieve high performance with a modest computational budget.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Parker_Holder_J/0/1/0/all/0/1"&gt;Jack Parker-Holder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_V/0/1/0/all/0/1"&gt;Vu Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roberts_S/0/1/0/all/0/1"&gt;Stephen Roberts&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Laplacian-Based Dimensionality Reduction Including Spectral Clustering, Laplacian Eigenmap, Locality Preserving Projection, Graph Embedding, and Diffusion Map: Tutorial and Survey. (arXiv:2106.02154v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.02154</id>
        <link href="http://arxiv.org/abs/2106.02154"/>
        <updated>2021-06-07T03:06:14.802Z</updated>
        <summary type="html"><![CDATA[This is a tutorial and survey paper for nonlinear dimensionality and feature
extraction methods which are based on the Laplacian of graph of data. We first
introduce adjacency matrix, definition of Laplacian matrix, and the
interpretation of Laplacian. Then, we cover the cuts of graph and spectral
clustering which applies clustering in a subspace of data. Different
optimization variants of Laplacian eigenmap and its out-of-sample extension are
explained. Thereafter, we introduce the locality preserving projection and its
kernel variant as linear special cases of Laplacian eigenmap. Versions of graph
embedding are then explained which are generalized versions of Laplacian
eigenmap and locality preserving projection. Finally, diffusion map is
introduced which is a method based on Laplacian of data and random walks on the
data graph.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Ghojogh_B/0/1/0/all/0/1"&gt;Benyamin Ghojogh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ghodsi_A/0/1/0/all/0/1"&gt;Ali Ghodsi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Karray_F/0/1/0/all/0/1"&gt;Fakhri Karray&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Crowley_M/0/1/0/all/0/1"&gt;Mark Crowley&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Nara: Learning Network-Aware Resource Allocation Algorithms for Cloud Data Centres. (arXiv:2106.02412v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02412</id>
        <link href="http://arxiv.org/abs/2106.02412"/>
        <updated>2021-06-07T03:06:14.795Z</updated>
        <summary type="html"><![CDATA[Data centres (DCs) underline many prominent future technological trends such
as distributed training of large scale machine learning models and
internet-of-things based platforms. DCs will soon account for over 3\% of
global energy demand, so efficient use of DC resources is essential. Robust DC
networks (DCNs) are essential to form the large scale systems needed to handle
this demand, but can bottleneck how efficiently DC-server resources can be used
when servers with insufficient connectivity between them cannot be jointly
allocated to a job. However, allocating servers' resources whilst accounting
for their inter-connectivity maps to an NP-hard combinatorial optimisation
problem, and so is often ignored in DC resource management schemes. We present
Nara, a framework based on reinforcement learning (RL) and graph neural
networks (GNN) to learn network-aware allocation policies that increase the
number of requests allocated over time compared to previous methods. Unique to
our solution is the use of a GNN to generate representations of server-nodes in
the DCN, which are then interpreted as actions by a RL policy-network which
chooses from which servers resources will be allocated to incoming requests.
Nara is agnostic to the topology size and shape and is trained end-to-end. The
method can accept up to 33\% more requests than the best baseline when deployed
on DCNs with up to the order of $10\times$ more compute nodes than the DCN seen
during training and is able to maintain its policy's performance on DCNs with
the order of $100\times$ more servers than seen during training. It also
generalises to unseen DCN topologies with varied network structure and unseen
request distributions without re-training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shabka_Z/0/1/0/all/0/1"&gt;Zacharaya Shabka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zervas_G/0/1/0/all/0/1"&gt;Georgios Zervas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Solving Schr\"odinger Bridges via Maximum Likelihood. (arXiv:2106.02081v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.02081</id>
        <link href="http://arxiv.org/abs/2106.02081"/>
        <updated>2021-06-07T03:06:14.788Z</updated>
        <summary type="html"><![CDATA[The Schr\"odinger bridge problem (SBP) finds the most likely stochastic
evolution between two probability distributions given a prior stochastic
evolution. As well as applications in the natural sciences, problems of this
kind have important applications in machine learning such as dataset alignment
and hypothesis testing. Whilst the theory behind this problem is relatively
mature, scalable numerical recipes to estimate the Schr\"odinger bridge remain
an active area of research. We prove an equivalence between the SBP and maximum
likelihood estimation enabling direct application of successful machine
learning techniques. We propose a numerical procedure to estimate SBPs using
Gaussian process and demonstrate the practical usage of our approach in
numerical simulations and experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Vargas_F/0/1/0/all/0/1"&gt;Francisco Vargas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Thodoroff_P/0/1/0/all/0/1"&gt;Pierre Thodoroff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lawrence_N/0/1/0/all/0/1"&gt;Neil D. Lawrence&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lamacraft_A/0/1/0/all/0/1"&gt;Austen Lamacraft&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ambulatory blood pressure monitoring versus office blood pressure measurement: Are there sex differences?. (arXiv:2106.02392v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2106.02392</id>
        <link href="http://arxiv.org/abs/2106.02392"/>
        <updated>2021-06-07T03:06:14.782Z</updated>
        <summary type="html"><![CDATA[The accurate measurement of blood pressure (BP) is an important prerequisite
for the reliable diagnosis and efficient management of hypertension and other
medical conditions. Office Blood Pressure Measurement (OBP) is a technique
performed in-office with the sphygmomanometer, while Ambulatory Blood Pressure
Monitoring (ABPM) is a technique that measures blood pressure during 24h. The
BP fluctuations also depend on other factors such as physical activity,
temperature, mood, age, sex, any pathologies, a hormonal activity that may
intrinsically influence the differences between OBP and ABPM. The aim of this
study is to examine the possible influence of sex on the discrepancies between
OBP and ABPM in 872 subjects with known or suspected hypertension. A
significant correlation was observed between OBP and ABPM mean values
calculated during the day, night and 24h (ABPMday, ABPMnight, ABPM24h) in both
groups (p<0.0001). The main finding of this study is that no difference between
sexes was observed in the relation between OBP and mean ABMP values except
between systolic OBP and systolic ABPM during the night. In addition, this
study showed a moderate correlation between BPs obtained with the two
approaches with a great dispersion around the regression line which suggests
that the two approaches cannot be used interchangeably.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Miladinovic_A/0/1/0/all/0/1"&gt;Aleksandar Miladinovi&amp;#x107;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ajcevic_M/0/1/0/all/0/1"&gt;Milo&amp;#x161; Aj&amp;#x10d;evi&amp;#x107;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Siveri_G/0/1/0/all/0/1"&gt;Giulia Siveri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liguori_L/0/1/0/all/0/1"&gt;Laura Liguori&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Pascazio_L/0/1/0/all/0/1"&gt;Lorenzo Pascazio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Accardo_A/0/1/0/all/0/1"&gt;Agostino Accardo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Teaching keyword spotters to spot new keywords with limited examples. (arXiv:2106.02443v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2106.02443</id>
        <link href="http://arxiv.org/abs/2106.02443"/>
        <updated>2021-06-07T03:06:14.775Z</updated>
        <summary type="html"><![CDATA[Learning to recognize new keywords with just a few examples is essential for
personalizing keyword spotting (KWS) models to a user's choice of keywords.
However, modern KWS models are typically trained on large datasets and
restricted to a small vocabulary of keywords, limiting their transferability to
a broad range of unseen keywords. Towards easily customizable KWS models, we
present KeySEM (Keyword Speech EMbedding), a speech embedding model pre-trained
on the task of recognizing a large number of keywords. Speech representations
offered by KeySEM are highly effective for learning new keywords from a limited
number of examples. Comparisons with a diverse range of related work across
several datasets show that our method achieves consistently superior
performance with fewer training examples. Although KeySEM was pre-trained only
on English utterances, the performance gains also extend to datasets from four
other languages indicating that KeySEM learns useful representations well
aligned with the task of keyword spotting. Finally, we demonstrate KeySEM's
ability to learn new keywords sequentially without requiring to re-train on
previously learned keywords. Our experimental observations suggest that KeySEM
is well suited to on-device environments where post-deployment learning and
ease of customization are often desirable.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Awasthi_A/0/1/0/all/0/1"&gt;Abhijeet Awasthi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kilgour_K/0/1/0/all/0/1"&gt;Kevin Kilgour&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rom_H/0/1/0/all/0/1"&gt;Hassan Rom&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Out-of-Distribution Generalization in Kernel Regression. (arXiv:2106.02261v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.02261</id>
        <link href="http://arxiv.org/abs/2106.02261"/>
        <updated>2021-06-07T03:06:14.755Z</updated>
        <summary type="html"><![CDATA[In real word applications, data generating process for training a machine
learning model often differs from what the model encounters in the test stage.
Understanding how and whether machine learning models generalize under such
distributional shifts have been a theoretical challenge. Here, we study
generalization in kernel regression when the training and test distributions
are different using methods from statistical physics. Using the replica method,
we derive an analytical formula for the out-of-distribution generalization
error applicable to any kernel and real datasets. We identify an overlap matrix
that quantifies the mismatch between distributions for a given kernel as a key
determinant of generalization performance under distribution shift. Using our
analytical expressions we elucidate various generalization phenomena including
possible improvement in generalization when there is a mismatch. We develop
procedures for optimizing training and test distributions for a given data
budget to find best and worst case generalizations under the shift. We present
applications of our theory to real and synthetic datasets and for many kernels.
We compare results of our theory applied to Neural Tangent Kernel with
simulations of wide networks and show agreement. We analyze linear regression
in further depth.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Canatar_A/0/1/0/all/0/1"&gt;Abdulkadir Canatar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Bordelon_B/0/1/0/all/0/1"&gt;Blake Bordelon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Pehlevan_C/0/1/0/all/0/1"&gt;Cengiz Pehlevan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Prospective Observational Study to Investigate Performance of a Chest X-ray Artificial Intelligence Diagnostic Support Tool Across 12 U.S. Hospitals. (arXiv:2106.02118v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.02118</id>
        <link href="http://arxiv.org/abs/2106.02118"/>
        <updated>2021-06-07T03:06:14.749Z</updated>
        <summary type="html"><![CDATA[Importance: An artificial intelligence (AI)-based model to predict COVID-19
likelihood from chest x-ray (CXR) findings can serve as an important adjunct to
accelerate immediate clinical decision making and improve clinical decision
making. Despite significant efforts, many limitations and biases exist in
previously developed AI diagnostic models for COVID-19. Utilizing a large set
of local and international CXR images, we developed an AI model with high
performance on temporal and external validation.

Conclusions and Relevance: AI-based diagnostic tools may serve as an adjunct,
but not replacement, for clinical decision support of COVID-19 diagnosis, which
largely hinges on exposure history, signs, and symptoms. While AI-based tools
have not yet reached full diagnostic potential in COVID-19, they may still
offer valuable information to clinicians taken into consideration along with
clinical signs and symptoms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Sun_J/0/1/0/all/0/1"&gt;Ju Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Peng_L/0/1/0/all/0/1"&gt;Le Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_T/0/1/0/all/0/1"&gt;Taihui Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Adila_D/0/1/0/all/0/1"&gt;Dyah Adila&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zaiman_Z/0/1/0/all/0/1"&gt;Zach Zaiman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Melton_G/0/1/0/all/0/1"&gt;Genevieve B. Melton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ingraham_N/0/1/0/all/0/1"&gt;Nicholas Ingraham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Murray_E/0/1/0/all/0/1"&gt;Eric Murray&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Boley_D/0/1/0/all/0/1"&gt;Daniel Boley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Switzer_S/0/1/0/all/0/1"&gt;Sean Switzer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Burns_J/0/1/0/all/0/1"&gt;John L. Burns&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Huang_K/0/1/0/all/0/1"&gt;Kun Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Allen_T/0/1/0/all/0/1"&gt;Tadashi Allen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Steenburg_S/0/1/0/all/0/1"&gt;Scott D. Steenburg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gichoya_J/0/1/0/all/0/1"&gt;Judy Wawira Gichoya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kummerfeld_E/0/1/0/all/0/1"&gt;Erich Kummerfeld&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tignanelli_C/0/1/0/all/0/1"&gt;Christopher Tignanelli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ukiyo-e Analysis and Creativity with Attribute and Geometry Annotation. (arXiv:2106.02267v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02267</id>
        <link href="http://arxiv.org/abs/2106.02267"/>
        <updated>2021-06-07T03:06:14.742Z</updated>
        <summary type="html"><![CDATA[The study of Ukiyo-e, an important genre of pre-modern Japanese art, focuses
on the object and style like other artwork researches. Such study has benefited
from the renewed interest by the machine learning community in culturally
important topics, leading to interdisciplinary works including collections of
images, quantitative approaches, and machine learning-based creativities. They,
however, have several drawbacks, and it remains challenging to integrate these
works into a comprehensive view. To bridge this gap, we propose a holistic
approach We first present a large-scale Ukiyo-e dataset with coherent semantic
labels and geometric annotations, then show its value in a quantitative study
of Ukiyo-e paintings' object using these labels and annotations. We further
demonstrate the machine learning methods could help style study through soft
color decomposition of Ukiyo-e, and finally provides joint insights into object
and style by composing sketches and colors using colorization. Dataset
available at https://github.com/rois-codh/arc-ukiyoe-faces]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1"&gt;Yingtao Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Clanuwat_T/0/1/0/all/0/1"&gt;Tarin Clanuwat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suzuki_C/0/1/0/all/0/1"&gt;Chikahiko Suzuki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kitamoto_A/0/1/0/all/0/1"&gt;Asanobu Kitamoto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Disentangling Dense Multi-Cable Knots. (arXiv:2106.02252v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2106.02252</id>
        <link href="http://arxiv.org/abs/2106.02252"/>
        <updated>2021-06-07T03:06:14.737Z</updated>
        <summary type="html"><![CDATA[Disentangling two or more cables requires many steps to remove crossings
between and within cables. We formalize the problem of disentangling multiple
cables and present an algorithm, Iterative Reduction Of Non-planar Multiple
cAble kNots (IRON-MAN), that outputs robot actions to remove crossings from
multi-cable knotted structures. We instantiate this algorithm with a learned
perception system, inspired by prior work in single-cable untying that given an
image input, can disentangle two-cable twists, three-cable braids, and knots of
two or three cables, such as overhand, square, carrick bend, sheet bend, crown,
and fisherman's knots. IRON-MAN keeps track of task-relevant keypoints
corresponding to target cable endpoints and crossings and iteratively
disentangles the cables by identifying and undoing crossings that are critical
to knot structure. Using a da Vinci surgical robot, we experimentally evaluate
the effectiveness of IRON-MAN on untangling multi-cable knots of types that
appear in the training data, as well as generalizing to novel classes of
multi-cable knots. Results suggest that IRON-MAN is effective in disentangling
knots involving up to three cables with 80.5% success and generalizing to knot
types that are not present during training, with cables of both distinct or
identical colors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Viswanath_V/0/1/0/all/0/1"&gt;Vainavi Viswanath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grannen_J/0/1/0/all/0/1"&gt;Jennifer Grannen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sundaresan_P/0/1/0/all/0/1"&gt;Priya Sundaresan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thananjeyan_B/0/1/0/all/0/1"&gt;Brijen Thananjeyan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balakrishna_A/0/1/0/all/0/1"&gt;Ashwin Balakrishna&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Novoseller_E/0/1/0/all/0/1"&gt;Ellen Novoseller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ichnowski_J/0/1/0/all/0/1"&gt;Jeffrey Ichnowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Laskey_M/0/1/0/all/0/1"&gt;Michael Laskey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gonzalez_J/0/1/0/all/0/1"&gt;Joseph E. Gonzalez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldberg_K/0/1/0/all/0/1"&gt;Ken Goldberg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Switching State Space Model (DS$^3$M) for Nonlinear Time Series Forecasting with Regime Switching. (arXiv:2106.02329v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02329</id>
        <link href="http://arxiv.org/abs/2106.02329"/>
        <updated>2021-06-07T03:06:14.730Z</updated>
        <summary type="html"><![CDATA[We propose a deep switching state space model (DS$^3$M) for efficient
inference and forecasting of nonlinear time series with irregularly switching
among various regimes. The switching among regimes is captured by both discrete
and continuous latent variables with recurrent neural networks. The model is
estimated with variational inference using a reparameterization trick. We test
the approach on a variety of simulated and real datasets. In all cases, DS$^3$M
achieves competitive performance compared to several state-of-the-art methods
(e.g. GRU, SRNN, DSARF, SNLDS), with superior forecasting accuracy, convincing
interpretability of the discrete latent variables, and powerful representation
of the continuous latent variables for different kinds of time series.
Specifically, the MAPE values increase by 0.09\% to 15.71\% against the
second-best performing alternative models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1"&gt;Xiuqin Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Ying Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stochastic Multi-Armed Bandits with Unrestricted Delay Distributions. (arXiv:2106.02436v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02436</id>
        <link href="http://arxiv.org/abs/2106.02436"/>
        <updated>2021-06-07T03:06:14.724Z</updated>
        <summary type="html"><![CDATA[We study the stochastic Multi-Armed Bandit (MAB) problem with random delays
in the feedback received by the algorithm. We consider two settings: the
reward-dependent delay setting, where realized delays may depend on the
stochastic rewards, and the reward-independent delay setting. Our main
contribution is algorithms that achieve near-optimal regret in each of the
settings, with an additional additive dependence on the quantiles of the delay
distribution. Our results do not make any assumptions on the delay
distributions: in particular, we do not assume they come from any parametric
family of distributions and allow for unbounded support and expectation; we
further allow for infinite delays where the algorithm might occasionally not
observe any feedback.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lancewicki_T/0/1/0/all/0/1"&gt;Tal Lancewicki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Segal_S/0/1/0/all/0/1"&gt;Shahar Segal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koren_T/0/1/0/all/0/1"&gt;Tomer Koren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mansour_Y/0/1/0/all/0/1"&gt;Yishay Mansour&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Subdivision-Based Mesh Convolution Networks. (arXiv:2106.02285v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02285</id>
        <link href="http://arxiv.org/abs/2106.02285"/>
        <updated>2021-06-07T03:06:14.684Z</updated>
        <summary type="html"><![CDATA[Convolutional neural networks (CNNs) have made great breakthroughs in 2D
computer vision. However, the irregular structure of meshes makes it hard to
exploit the power of CNNs directly. A subdivision surface provides a
hierarchical multi-resolution structure, and each face in a closed 2-manifold
triangle mesh is exactly adjacent to three faces. Motivated by these two
properties, this paper introduces a novel and flexible CNN framework, named
SubdivNet, for 3D triangle meshes with Loop subdivision sequence connectivity.
Making an analogy between mesh faces and pixels in a 2D image allows us to
present a mesh convolution operator to aggregate local features from adjacent
faces. By exploiting face neighborhoods, this convolution can support standard
2D convolutional network concepts, e.g. variable kernel size, stride, and
dilation. Based on the multi-resolution hierarchy, we propose a spatial uniform
pooling layer which merges four faces into one and an upsampling method which
splits one face into four. As a result, many popular 2D CNN architectures can
be readily adapted to processing 3D meshes. Meshes with arbitrary connectivity
can be remeshed to hold Loop subdivision sequence connectivity via
self-parameterization, making SubdivNet a general approach. Experiments on mesh
classification, segmentation, correspondence, and retrieval from the real-world
demonstrate the effectiveness and efficiency of SubdivNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1"&gt;Shi-Min Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zheng-Ning Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1"&gt;Meng-Hao Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1"&gt;Jun-Xiong Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1"&gt;Jiahui Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mu_T/0/1/0/all/0/1"&gt;Tai-Jiang Mu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martin_R/0/1/0/all/0/1"&gt;Ralph R. Martin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RL-DARTS: Differentiable Architecture Search for Reinforcement Learning. (arXiv:2106.02229v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02229</id>
        <link href="http://arxiv.org/abs/2106.02229"/>
        <updated>2021-06-07T03:06:14.659Z</updated>
        <summary type="html"><![CDATA[We introduce RL-DARTS, one of the first applications of Differentiable
Architecture Search (DARTS) in reinforcement learning (RL) to search for
convolutional cells, applied to the Procgen benchmark. We outline the initial
difficulties of applying neural architecture search techniques in RL, and
demonstrate that by simply replacing the image encoder with a DARTS supernet,
our search method is sample-efficient, requires minimal extra compute
resources, and is also compatible with off-policy and on-policy RL algorithms,
needing only minor changes in preexisting code. Surprisingly, we find that the
supernet can be used as an actor for inference to generate replay data in
standard RL training loops, and thus train end-to-end. Throughout this training
process, we show that the supernet gradually learns better cells, leading to
alternative architectures which can be highly competitive against manually
designed policies, but also verify previous design choices for RL policies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Miao_Y/0/1/0/all/0/1"&gt;Yingjie Miao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1"&gt;Xingyou Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_D/0/1/0/all/0/1"&gt;Daiyi Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yue_S/0/1/0/all/0/1"&gt;Summer Yue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brevdo_E/0/1/0/all/0/1"&gt;Eugene Brevdo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Faust_A/0/1/0/all/0/1"&gt;Aleksandra Faust&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Learning-based Optimal Market Bidding Strategy for Price-Maker Energy Storage. (arXiv:2106.02396v1 [eess.SY])]]></title>
        <id>http://arxiv.org/abs/2106.02396</id>
        <link href="http://arxiv.org/abs/2106.02396"/>
        <updated>2021-06-07T03:06:14.610Z</updated>
        <summary type="html"><![CDATA[Load serving entities with storage units reach sizes and performances that
can significantly impact clearing prices in electricity markets. Nevertheless,
price endogeneity is rarely considered in storage bidding strategies and
modeling the electricity market is a challenging task. Meanwhile, model-free
reinforcement learning such as the Actor-Critic are becoming increasingly
popular for designing energy system controllers. Yet implementation frequently
requires lengthy, data-intense, and unsafe trial-and-error training. To fill
these gaps, we implement an online Supervised Actor-Critic (SAC) algorithm,
supervised with a model-based controller -- Model Predictive Control (MPC). The
energy storage agent is trained with this algorithm to optimally bid while
learning and adjusting to its impact on the market clearing prices. We compare
the supervised Actor-Critic algorithm with the MPC algorithm as a supervisor,
finding that the former reaps higher profits via learning. Our contribution,
thus, is an online and safe SAC algorithm that outperforms the current
model-based state-of-the-art.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Badoual_M/0/1/0/all/0/1"&gt;Mathilde D. Badoual&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Moura_S/0/1/0/all/0/1"&gt;Scott J. Moura&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph Barlow Twins: A self-supervised representation learning framework for graphs. (arXiv:2106.02466v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02466</id>
        <link href="http://arxiv.org/abs/2106.02466"/>
        <updated>2021-06-07T03:06:14.601Z</updated>
        <summary type="html"><![CDATA[The self-supervised learning (SSL) paradigm is an essential exploration area,
which tries to eliminate the need for expensive data labeling. Despite the
great success of SSL methods in computer vision and natural language
processing, most of them employ contrastive learning objectives that require
negative samples, which are hard to define. This becomes even more challenging
in the case of graphs and is a bottleneck for achieving robust representations.
To overcome such limitations, we propose a framework for self-supervised graph
representation learning -- Graph Barlow Twins, which utilizes a
cross-correlation-based loss function instead of negative samples. Moreover, it
does not rely on non-symmetric neural network architectures -- in contrast to
state-of-the-art self-supervised graph representation learning method BGRL. We
show that our method achieves as competitive results as BGRL, best
self-supervised methods, and fully supervised ones while requiring
substantially fewer hyperparameters and converging in an order of magnitude
training steps earlier.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bielak_P/0/1/0/all/0/1"&gt;Piotr Bielak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kajdanowicz_T/0/1/0/all/0/1"&gt;Tomasz Kajdanowicz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chawla_N/0/1/0/all/0/1"&gt;Nitesh V. Chawla&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Attention Between Datapoints: Going Beyond Individual Input-Output Pairs in Deep Learning. (arXiv:2106.02584v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02584</id>
        <link href="http://arxiv.org/abs/2106.02584"/>
        <updated>2021-06-07T03:06:14.434Z</updated>
        <summary type="html"><![CDATA[We challenge a common assumption underlying most supervised deep learning:
that a model makes a prediction depending only on its parameters and the
features of a single input. To this end, we introduce a general-purpose deep
learning architecture that takes as input the entire dataset instead of
processing one datapoint at a time. Our approach uses self-attention to reason
about relationships between datapoints explicitly, which can be seen as
realizing non-parametric models using parametric attention mechanisms. However,
unlike conventional non-parametric models, we let the model learn end-to-end
from the data how to make use of other datapoints for prediction. Empirically,
our models solve cross-datapoint lookup and complex reasoning tasks unsolvable
by traditional deep learning models. We show highly competitive results on
tabular data, early results on CIFAR-10, and give insight into how the model
makes use of the interactions between points.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kossen_J/0/1/0/all/0/1"&gt;Jannik Kossen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Band_N/0/1/0/all/0/1"&gt;Neil Band&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lyle_C/0/1/0/all/0/1"&gt;Clare Lyle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gomez_A/0/1/0/all/0/1"&gt;Aidan N. Gomez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rainforth_T/0/1/0/all/0/1"&gt;Tom Rainforth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gal_Y/0/1/0/all/0/1"&gt;Yarin Gal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DRIBO: Robust Deep Reinforcement Learning via Multi-View Information Bottleneck. (arXiv:2102.13268v2 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.13268</id>
        <link href="http://arxiv.org/abs/2102.13268"/>
        <updated>2021-06-07T03:06:14.355Z</updated>
        <summary type="html"><![CDATA[Deep reinforcement learning (DRL) agents are often sensitive to visual
changes that were unseen in their training environments. To address this
problem, we leverage the sequential nature of RL to learn robust
representations that encode only task-relevant information from observations
based on the unsupervised multi-view setting. Specifically, we introduce an
auxiliary objective based on the multi-view in-formation bottleneck (MIB)
principle which quantifies the amount of task-irrelevant information and
encourages learning representations that are both predictive of the future and
less sensitive to task-irrelevant distractions. This enables us to train
high-performance policies that are robust to visual distractions and can
generalize to unseen environments. We demonstrate that our approach can achieve
SOTA performance on diverse visual control tasks on the DeepMind Control Suite,
even when the background is replaced with natural videos. In addition, we show
that our approach outperforms well-established baselines for generalization to
unseen environments on the Procgen benchmark. Our code is open-sourced and
available at https://github.com/JmfanBU/DRIBO.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1"&gt;Jiameng Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1"&gt;Wenchao Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Forward Super-Resolution: How Can GANs Learn Hierarchical Generative Models for Real-World Distributions. (arXiv:2106.02619v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02619</id>
        <link href="http://arxiv.org/abs/2106.02619"/>
        <updated>2021-06-07T03:06:14.330Z</updated>
        <summary type="html"><![CDATA[Generative adversarial networks (GANs) are among the most successful models
for learning high-complexity, real-world distributions. However, in theory, due
to the highly non-convex, non-concave landscape of the minmax training
objective, GAN remains one of the least understood deep learning models. In
this work, we formally study how GANs can efficiently learn certain
hierarchically generated distributions that are close to the distribution of
images in practice. We prove that when a distribution has a structure that we
refer to as Forward Super-Resolution, then simply training generative
adversarial networks using gradient descent ascent (GDA) can indeed learn this
distribution efficiently, both in terms of sample and time complexities. We
also provide concrete empirical evidence that not only our assumption "forward
super-resolution" is very natural in practice, but also the underlying learning
mechanisms that we study in this paper (to allow us efficiently train GAN via
GDA in theory) simulates the actual learning process of GANs in practice on
real-world problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Allen_Zhu_Z/0/1/0/all/0/1"&gt;Zeyuan Allen-Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yuanzhi Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Contextual Learners for Protein Networks. (arXiv:2106.02246v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02246</id>
        <link href="http://arxiv.org/abs/2106.02246"/>
        <updated>2021-06-07T03:06:14.321Z</updated>
        <summary type="html"><![CDATA[Spatial context is central to understanding health and disease. Yet reference
protein interaction networks lack such contextualization, thereby limiting the
study of where protein interactions likely occur in the human body.
Contextualized protein interactions could better characterize genes with
disease-specific interactions and elucidate diseases' manifestation in specific
cell types. Here, we introduce AWARE, a graph neural message passing approach
to inject cellular and tissue context into protein embeddings. AWARE optimizes
for a multi-scale embedding space, whose structure reflects the topology of
cell type specific networks. We construct a multi-scale network of the Human
Cell Atlas and apply AWARE to learn protein, cell type, and tissue embeddings
that uphold cell type and tissue hierarchies. We demonstrate AWARE on the novel
task of predicting whether a gene is associated with a disease and where it
most likely manifests in the human body. AWARE embeddings outperform global
embeddings by at least 12.5%, highlighting the importance of contextual
learners for protein networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1"&gt;Michelle M. Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zitnik_M/0/1/0/all/0/1"&gt;Marinka Zitnik&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CAFLOW: Conditional Autoregressive Flows. (arXiv:2106.02531v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02531</id>
        <link href="http://arxiv.org/abs/2106.02531"/>
        <updated>2021-06-07T03:06:14.264Z</updated>
        <summary type="html"><![CDATA[We introduce CAFLOW, a new diverse image-to-image translation model that
simultaneously leverages the power of auto-regressive modeling and the modeling
efficiency of conditional normalizing flows. We transform the conditioning
image into a sequence of latent encodings using a multi-scale normalizing flow
and repeat the process for the conditioned image. We model the conditional
distribution of the latent encodings by modeling the auto-regressive
distributions with an efficient multi-scale normalizing flow, where each
conditioning factor affects image synthesis at its respective resolution scale.
Our proposed framework performs well on a range of image-to-image translation
tasks. It outperforms former designs of conditional flows because of its
expressive auto-regressive structure.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Batzolis_G/0/1/0/all/0/1"&gt;Georgios Batzolis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carioni_M/0/1/0/all/0/1"&gt;Marcello Carioni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Etmann_C/0/1/0/all/0/1"&gt;Christian Etmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Afyouni_S/0/1/0/all/0/1"&gt;Soroosh Afyouni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kourtzi_Z/0/1/0/all/0/1"&gt;Zoe Kourtzi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schonlieb_C/0/1/0/all/0/1"&gt;Carola Bibiane Sch&amp;#xf6;nlieb&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generative Text Modeling through Short Run Inference. (arXiv:2106.02513v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02513</id>
        <link href="http://arxiv.org/abs/2106.02513"/>
        <updated>2021-06-07T03:06:14.221Z</updated>
        <summary type="html"><![CDATA[Latent variable models for text, when trained successfully, accurately model
the data distribution and capture global semantic and syntactic features of
sentences. The prominent approach to train such models is variational
autoencoders (VAE). It is nevertheless challenging to train and often results
in a trivial local optimum where the latent variable is ignored and its
posterior collapses into the prior, an issue known as posterior collapse.
Various techniques have been proposed to mitigate this issue. Most of them
focus on improving the inference model to yield latent codes of higher quality.
The present work proposes a short run dynamics for inference. It is initialized
from the prior distribution of the latent variable and then runs a small number
(e.g., 20) of Langevin dynamics steps guided by its posterior distribution. The
major advantage of our method is that it does not require a separate inference
model or assume simple geometry of the posterior distribution, thus rendering
an automatic, natural and flexible inference engine. We show that the models
trained with short run dynamics more accurately model the data, compared to
strong language model and VAE baselines, and exhibit no sign of posterior
collapse. Analyses of the latent space show that interpolation in the latent
space is able to generate coherent sentences with smooth transition and
demonstrate improved classification over strong baselines with latent features
from unsupervised pretraining. These results together expose a well-structured
latent space of our generative model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pang_B/0/1/0/all/0/1"&gt;Bo Pang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nijkamp_E/0/1/0/all/0/1"&gt;Erik Nijkamp&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_T/0/1/0/all/0/1"&gt;Tian Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Ying Nian Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Influence of cognitive, geographical, and collaborative proximity on knowledge production of Canadian nanotechnology. (arXiv:2106.02110v1 [physics.soc-ph])]]></title>
        <id>http://arxiv.org/abs/2106.02110</id>
        <link href="http://arxiv.org/abs/2106.02110"/>
        <updated>2021-06-07T03:06:14.212Z</updated>
        <summary type="html"><![CDATA[Incorporating existing knowledge is vital for innovating, discovering, and
generating new ideas. Knowledge production through research and invention is
the key to scientific and technological development. As an emerging technology,
nanotechnology has already proved its great potential for the global economy,
attracting considerable federal investments. Canada is reported as one of the
major players in producing nanotechnology research. In this paper, we focused
on the main drivers of knowledge production and diffusion by analyzing Canadian
nanotechnology researchers. We hypothesized that knowledge production in
Canadian nanotechnology is influenced by three key proximity factors, namely
cognitive, geographical, and collaborative. Using statistical analysis, social
network analysis, and machine learning techniques we comprehensively assessed
the influence of the proximity factors on academic knowledge production. Our
results not only prove a significant impact of the three key proximity factors
but also their predictive potential.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Neira_E/0/1/0/all/0/1"&gt;Elva Luz Crespo Neira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Ebadi_A/0/1/0/all/0/1"&gt;Ashkan Ebadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Beaudry_C/0/1/0/all/0/1"&gt;Catherine Beaudry&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Schiffauerova_A/0/1/0/all/0/1"&gt;Andrea Schiffauerova&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Domain-Adversarial and Conditional State Space Model for Imitation Learning. (arXiv:2001.11628v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2001.11628</id>
        <link href="http://arxiv.org/abs/2001.11628"/>
        <updated>2021-06-07T03:06:14.203Z</updated>
        <summary type="html"><![CDATA[State representation learning (SRL) in partially observable Markov decision
processes has been studied to learn abstract features of data useful for robot
control tasks. For SRL, acquiring domain-agnostic states is essential for
achieving efficient imitation learning. Without these states, imitation
learning is hampered by domain-dependent information useless for control.
However, existing methods fail to remove such disturbances from the states when
the data from experts and agents show large domain shifts. To overcome this
issue, we propose a domain-adversarial and conditional state space model
(DAC-SSM) that enables control systems to obtain domain-agnostic and task- and
dynamics-aware states. DAC-SSM jointly optimizes the state inference,
observation reconstruction, forward dynamics, and reward models. To remove
domain-dependent information from the states, the model is trained with domain
discriminators in an adversarial manner, and the reconstruction is conditioned
on domain labels. We experimentally evaluated the model predictive control
performance via imitation learning for continuous control of sparse reward
tasks in simulators and compared it with the performance of the existing SRL
method. The agents from DAC-SSM achieved performance comparable to experts and
more than twice the baselines. We conclude domain-agnostic states are essential
for imitation learning that has large domain shifts and can be obtained using
DAC-SSM.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Okumura_R/0/1/0/all/0/1"&gt;Ryo Okumura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Okada_M/0/1/0/all/0/1"&gt;Masashi Okada&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Taniguchi_T/0/1/0/all/0/1"&gt;Tadahiro Taniguchi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Regularization and Reparameterization Avoid Vanishing Gradients in Sigmoid-Type Networks. (arXiv:2106.02260v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02260</id>
        <link href="http://arxiv.org/abs/2106.02260"/>
        <updated>2021-06-07T03:06:14.197Z</updated>
        <summary type="html"><![CDATA[Deep learning requires several design choices, such as the nodes' activation
functions and the widths, types, and arrangements of the layers. One
consideration when making these choices is the vanishing-gradient problem,
which is the phenomenon of algorithms getting stuck at suboptimal points due to
small gradients. In this paper, we revisit the vanishing-gradient problem in
the context of sigmoid-type activation. We use mathematical arguments to
highlight two different sources of the phenomenon, namely large individual
parameters and effects across layers, and to illustrate two simple remedies,
namely regularization and rescaling. We then demonstrate the effectiveness of
the two remedies in practice. In view of the vanishing-gradient problem being a
main reason why tanh and other sigmoid-type activation has become much less
popular than relu-type activation, our results bring sigmoid-type activation
back to the table.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ven_L/0/1/0/all/0/1"&gt;Leni Ven&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lederer_J/0/1/0/all/0/1"&gt;Johannes Lederer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Price graphs: Utilizing the structural information of financial time series for stock prediction. (arXiv:2106.02522v1 [q-fin.ST])]]></title>
        <id>http://arxiv.org/abs/2106.02522</id>
        <link href="http://arxiv.org/abs/2106.02522"/>
        <updated>2021-06-07T03:06:14.186Z</updated>
        <summary type="html"><![CDATA[Stock prediction, with the purpose of forecasting the future price trends of
stocks, is crucial for maximizing profits from stock investments. While great
research efforts have been devoted to exploiting deep neural networks for
improved stock prediction, the existing studies still suffer from two major
issues. First, the long-range dependencies in time series are not sufficiently
captured. Second, the chaotic property of financial time series fundamentally
lowers prediction performance. In this study, we propose a novel framework to
address both issues regarding stock prediction. Specifically, in terms of
transforming time series into complex networks, we convert market price series
into graphs. Then, structural information, referring to associations among
temporal points and the node weights, is extracted from the mapped graphs to
resolve the problems regarding long-range dependencies and the chaotic
property. We take graph embeddings to represent the associations among temporal
points as the prediction model inputs. Node weights are used as a priori
knowledge to enhance the learning of temporal attention. The effectiveness of
our proposed framework is validated using real-world stock data, and our
approach obtains the best performance among several state-of-the-art
benchmarks. Moreover, in the conducted trading simulations, our framework
further obtains the highest cumulative profits. Our results supplement the
existing applications of complex network methods in the financial realm and
provide insightful implications for investment applications regarding decision
support in financial markets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-fin/1/au:+Wu_J/0/1/0/all/0/1"&gt;Junran Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Xu_K/0/1/0/all/0/1"&gt;Ke Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xueyuan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Li_S/0/1/0/all/0/1"&gt;Shangzhe Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Zhao_J/0/1/0/all/0/1"&gt;Jichang Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adam in Private: Secure and Fast Training of Deep Neural Networks with Adaptive Moment Estimation. (arXiv:2106.02203v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2106.02203</id>
        <link href="http://arxiv.org/abs/2106.02203"/>
        <updated>2021-06-07T03:06:14.163Z</updated>
        <summary type="html"><![CDATA[Privacy-preserving machine learning (PPML) aims at enabling machine learning
(ML) algorithms to be used on sensitive data. We contribute to this line of
research by proposing a framework that allows efficient and secure evaluation
of full-fledged state-of-the-art ML algorithms via secure multi-party
computation (MPC). This is in contrast to most prior works, which substitute ML
algorithms with approximated "MPC-friendly" variants. A drawback of the latter
approach is that fine-tuning of the combined ML and MPC algorithms is required,
which might lead to less efficient algorithms or inferior quality ML. This is
an issue for secure deep neural networks (DNN) training in particular, as this
involves arithmetic algorithms thought to be "MPC-unfriendly", namely, integer
division, exponentiation, inversion, and square root. In this work, we propose
secure and efficient protocols for the above seemingly MPC-unfriendly
computations. Our protocols are three-party protocols in the honest-majority
setting, and we propose both passively secure and actively secure with abort
variants. A notable feature of our protocols is that they simultaneously
provide high accuracy and efficiency. This framework enables us to efficiently
and securely compute modern ML algorithms such as Adam and the softmax function
"as is", without resorting to approximations. As a result, we obtain secure DNN
training that outperforms state-of-the-art three-party systems; our full
training is up to 6.7 times faster than just the online phase of the recently
proposed FALCON@PETS'21 on a standard benchmark network. We further perform
measurements on real-world DNNs, AlexNet and VGG16. The performance of our
framework is up to a factor of about 12-14 faster for AlexNet and 46-48 faster
for VGG16 to achieve an accuracy of 70% and 75%, respectively, when compared to
FALCON.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Attrapadung_N/0/1/0/all/0/1"&gt;Nuttapong Attrapadung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hamada_K/0/1/0/all/0/1"&gt;Koki Hamada&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ikarashi_D/0/1/0/all/0/1"&gt;Dai Ikarashi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kikuchi_R/0/1/0/all/0/1"&gt;Ryo Kikuchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Matsuda_T/0/1/0/all/0/1"&gt;Takahiro Matsuda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mishina_I/0/1/0/all/0/1"&gt;Ibuki Mishina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morita_H/0/1/0/all/0/1"&gt;Hiraku Morita&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schuldt_J/0/1/0/all/0/1"&gt;Jacob C. N. Schuldt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Minimum Word Error Rate Training with Language Model Fusion for End-to-End Speech Recognition. (arXiv:2106.02302v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2106.02302</id>
        <link href="http://arxiv.org/abs/2106.02302"/>
        <updated>2021-06-07T03:06:14.152Z</updated>
        <summary type="html"><![CDATA[Integrating external language models (LMs) into end-to-end (E2E) models
remains a challenging task for domain-adaptive speech recognition. Recently,
internal language model estimation (ILME)-based LM fusion has shown significant
word error rate (WER) reduction from Shallow Fusion by subtracting a weighted
internal LM score from an interpolation of E2E model and external LM scores
during beam search. However, on different test sets, the optimal LM
interpolation weights vary over a wide range and have to be tuned extensively
on well-matched validation sets. In this work, we perform LM fusion in the
minimum WER (MWER) training of an E2E model to obviate the need for LM weights
tuning during inference. Besides MWER training with Shallow Fusion (MWER-SF),
we propose a novel MWER training with ILME (MWER-ILME) where the ILME-based
fusion is conducted to generate N-best hypotheses and their posteriors.
Additional gradient is induced when internal LM is engaged in MWER-ILME loss
computation. During inference, LM weights pre-determined in MWER training
enable robust LM integrations on test sets from different domains. Experimented
with 30K-hour trained transformer transducers, MWER-ILME achieves on average
8.8% and 5.8% relative WER reductions from MWER and MWER-SF training,
respectively, on 6 different test sets]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Meng_Z/0/1/0/all/0/1"&gt;Zhong Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yu Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kanda_N/0/1/0/all/0/1"&gt;Naoyuki Kanda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lu_L/0/1/0/all/0/1"&gt;Liang Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xie Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ye_G/0/1/0/all/0/1"&gt;Guoli Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sun_E/0/1/0/all/0/1"&gt;Eric Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1"&gt;Jinyu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gong_Y/0/1/0/all/0/1"&gt;Yifan Gong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the emergence of simplex symmetry in the final and penultimate layers of neural network classifiers. (arXiv:2012.05420v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.05420</id>
        <link href="http://arxiv.org/abs/2012.05420"/>
        <updated>2021-06-07T03:06:14.139Z</updated>
        <summary type="html"><![CDATA[A recent numerical study observed that neural network classifiers enjoy a
large degree of symmetry in the penultimate layer. Namely, if $h(x) = Af(x) +b$
where $A$ is a linear map and $f$ is the output of the penultimate layer of the
network (after activation), then all data points $x_{i, 1}, \dots, x_{i, N_i}$
in a class $C_i$ are mapped to a single point $y_i$ by $f$ and the points $y_i$
are located at the vertices of a regular $k-1$-dimensional standard simplex in
a high-dimensional Euclidean space.

We explain this observation analytically in toy models for highly expressive
deep neural networks. In complementary examples, we demonstrate rigorously that
even the final output of the classifier $h$ is not uniform over data samples
from a class $C_i$ if $h$ is a shallow network (or if the deeper layers do not
bring the data samples into a convenient geometric configuration).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+E_W/0/1/0/all/0/1"&gt;Weinan E&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wojtowytsch_S/0/1/0/all/0/1"&gt;Stephan Wojtowytsch&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DOCTOR: A Simple Method for Detecting Misclassification Errors. (arXiv:2106.02395v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02395</id>
        <link href="http://arxiv.org/abs/2106.02395"/>
        <updated>2021-06-07T03:06:14.132Z</updated>
        <summary type="html"><![CDATA[Deep neural networks (DNNs) have shown to perform very well on large scale
object recognition problems and lead to widespread use for real-world
applications, including situations where DNN are implemented as "black boxes".
A promising approach to secure their use is to accept decisions that are likely
to be correct while discarding the others. In this work, we propose DOCTOR, a
simple method that aims to identify whether the prediction of a DNN classifier
should (or should not) be trusted so that, consequently, it would be possible
to accept it or to reject it. Two scenarios are investigated: Totally Black Box
(TBB) where only the soft-predictions are available and Partially Black Box
(PBB) where gradient-propagation to perform input pre-processing is allowed.
Empirically, we show that DOCTOR outperforms all state-of-the-art methods on
various well-known images and sentiment analysis datasets. In particular, we
observe a reduction of up to $4\%$ of the false rejection rate (FRR) in the PBB
scenario. DOCTOR can be applied to any pre-trained model, it does not require
prior information about the underlying dataset and is as simple as the simplest
available methods in the literature.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Granese_F/0/1/0/all/0/1"&gt;Federica Granese&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Romanelli_M/0/1/0/all/0/1"&gt;Marco Romanelli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gorla_D/0/1/0/all/0/1"&gt;Daniele Gorla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Palamidessi_C/0/1/0/all/0/1"&gt;Catuscia Palamidessi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Piantanida_P/0/1/0/all/0/1"&gt;Pablo Piantanida&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adaptive Epidemic Forecasting and Community Risk Evaluation of COVID-19. (arXiv:2106.02094v1 [cs.CY])]]></title>
        <id>http://arxiv.org/abs/2106.02094</id>
        <link href="http://arxiv.org/abs/2106.02094"/>
        <updated>2021-06-07T03:06:14.126Z</updated>
        <summary type="html"><![CDATA[Pandemic control measures like lock-down, restrictions on restaurants and
gatherings, social-distancing have shown to be effective in curtailing the
spread of COVID-19. However, their sustained enforcement has negative economic
effects. To craft strategies and policies that reduce the hardship on the
people and the economy while being effective against the pandemic, authorities
need to understand the disease dynamics at the right geo-spatial granularity.
Considering factors like the hospitals' ability to handle the fluctuating
demands, evaluating various reopening scenarios, and accurate forecasting of
cases are vital to decision making. Towards this end, we present a flexible
end-to-end solution that seamlessly integrates public health data with tertiary
client data to accurately estimate the risk of reopening a community. At its
core lies a state-of-the-art prediction model that auto-captures changing
trends in transmission and mobility. Benchmarking against various published
baselines confirm the superiority of our forecasting algorithm. Combined with
the ability to extend to multiple client-specific requirements and perform
deductive reasoning through counter-factual analysis, this solution provides
actionable insights to multiple client domains ranging from government to
educational institutions, hospitals, and commercial establishments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gopalakrishnan_V/0/1/0/all/0/1"&gt;Vishrawas Gopalakrishnan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Navalekar_S/0/1/0/all/0/1"&gt;Sayali Navalekar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_P/0/1/0/all/0/1"&gt;Pan Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hooley_R/0/1/0/all/0/1"&gt;Ryan Hooley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Miller_J/0/1/0/all/0/1"&gt;Jacob Miller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srinivasan_R/0/1/0/all/0/1"&gt;Raman Srinivasan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deshpande_A/0/1/0/all/0/1"&gt;Ajay Deshpande&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xuan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bianco_S/0/1/0/all/0/1"&gt;Simone Bianco&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kaufman_J/0/1/0/all/0/1"&gt;James H. Kaufman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using latent space regression to analyze and leverage compositionality in GANs. (arXiv:2103.10426v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.10426</id>
        <link href="http://arxiv.org/abs/2103.10426"/>
        <updated>2021-06-07T03:06:14.106Z</updated>
        <summary type="html"><![CDATA[In recent years, Generative Adversarial Networks have become ubiquitous in
both research and public perception, but how GANs convert an unstructured
latent code to a high quality output is still an open question. In this work,
we investigate regression into the latent space as a probe to understand the
compositional properties of GANs. We find that combining the regressor and a
pretrained generator provides a strong image prior, allowing us to create
composite images from a collage of random image parts at inference time while
maintaining global consistency. To compare compositional properties across
different generators, we measure the trade-offs between reconstruction of the
unrealistic input and image quality of the regenerated samples. We find that
the regression approach enables more localized editing of individual image
parts compared to direct editing in the latent space, and we conduct
experiments to quantify this independence effect. Our method is agnostic to the
semantics of edits, and does not require labels or predefined concepts during
training. Beyond image composition, our method extends to a number of related
applications, such as image inpainting or example-based image editing, which we
demonstrate on several GANs and datasets, and because it uses only a single
forward pass, it can operate in real-time. Code is available on our project
page: https://chail.github.io/latent-composition/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chai_L/0/1/0/all/0/1"&gt;Lucy Chai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wulff_J/0/1/0/all/0/1"&gt;Jonas Wulff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Isola_P/0/1/0/all/0/1"&gt;Phillip Isola&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adiabatic Quantum Feature Selection for Sparse Linear Regression. (arXiv:2106.02357v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02357</id>
        <link href="http://arxiv.org/abs/2106.02357"/>
        <updated>2021-06-07T03:06:14.099Z</updated>
        <summary type="html"><![CDATA[Linear regression is a popular machine learning approach to learn and predict
real valued outputs or dependent variables from independent variables or
features. In many real world problems, its beneficial to perform sparse linear
regression to identify important features helpful in predicting the dependent
variable. It not only helps in getting interpretable results but also avoids
overfitting when the number of features is large, and the amount of data is
small. The most natural way to achieve this is by using `best subset selection'
which penalizes non-zero model parameters by adding $\ell_0$ norm over
parameters to the least squares loss. However, this makes the objective
function non-convex and intractable even for a small number of features. This
paper aims to address the intractability of sparse linear regression with
$\ell_0$ norm using adiabatic quantum computing, a quantum computing paradigm
that is particularly useful for solving optimization problems faster. We
formulate the $\ell_0$ optimization problem as a Quadratic Unconstrained Binary
Optimization (QUBO) problem and solve it using the D-Wave adiabatic quantum
computer. We study and compare the quality of QUBO solution on synthetic and
real world datasets. The results demonstrate the effectiveness of the proposed
adiabatic quantum computing approach in finding the optimal solution. The QUBO
solution matches the optimal solution for a wide range of sparsity penalty
values across the datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Desu_S/0/1/0/all/0/1"&gt;Surya Sai Teja Desu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srijith_P/0/1/0/all/0/1"&gt;P.K. Srijith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rao_M/0/1/0/all/0/1"&gt;M.V. Panduranga Rao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sivadasan_N/0/1/0/all/0/1"&gt;Naveen Sivadasan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CLIP: A Dataset for Extracting Action Items for Physicians from Hospital Discharge Notes. (arXiv:2106.02524v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02524</id>
        <link href="http://arxiv.org/abs/2106.02524"/>
        <updated>2021-06-07T03:06:14.093Z</updated>
        <summary type="html"><![CDATA[Continuity of care is crucial to ensuring positive health outcomes for
patients discharged from an inpatient hospital setting, and improved
information sharing can help. To share information, caregivers write discharge
notes containing action items to share with patients and their future
caregivers, but these action items are easily lost due to the lengthiness of
the documents. In this work, we describe our creation of a dataset of clinical
action items annotated over MIMIC-III, the largest publicly available dataset
of real clinical notes. This dataset, which we call CLIP, is annotated by
physicians and covers 718 documents representing 100K sentences. We describe
the task of extracting the action items from these documents as multi-aspect
extractive summarization, with each aspect representing a type of action to be
taken. We evaluate several machine learning models on this task, and show that
the best models exploit in-domain language model pre-training on 59K
unannotated documents, and incorporate context from neighboring sentences. We
also propose an approach to pre-training data selection that allows us to
explore the trade-off between size and domain-specificity of pre-training
datasets for this task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mullenbach_J/0/1/0/all/0/1"&gt;James Mullenbach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pruksachatkun_Y/0/1/0/all/0/1"&gt;Yada Pruksachatkun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Adler_S/0/1/0/all/0/1"&gt;Sean Adler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seale_J/0/1/0/all/0/1"&gt;Jennifer Seale&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Swartz_J/0/1/0/all/0/1"&gt;Jordan Swartz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McKelvey_T/0/1/0/all/0/1"&gt;T. Greg McKelvey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_H/0/1/0/all/0/1"&gt;Hui Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yi Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sontag_D/0/1/0/all/0/1"&gt;David Sontag&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Online reinforcement learning with sparse rewards through an active inference capsule. (arXiv:2106.02390v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02390</id>
        <link href="http://arxiv.org/abs/2106.02390"/>
        <updated>2021-06-07T03:06:14.085Z</updated>
        <summary type="html"><![CDATA[Intelligent agents must pursue their goals in complex environments with
partial information and often limited computational capacity. Reinforcement
learning methods have achieved great success by creating agents that optimize
engineered reward functions, but which often struggle to learn in sparse-reward
environments, generally require many environmental interactions to perform
well, and are typically computationally very expensive. Active inference is a
model-based approach that directs agents to explore uncertain states while
adhering to a prior model of their goal behaviour. This paper introduces an
active inference agent which minimizes the novel free energy of the expected
future. Our model is capable of solving sparse-reward problems with a very high
sample efficiency due to its objective function, which encourages directed
exploration of uncertain states. Moreover, our model is computationally very
light and can operate in a fully online manner while achieving comparable
performance to offline RL methods. We showcase the capabilities of our model by
solving the mountain car problem, where we demonstrate its superior exploration
properties and its robustness to observation noise, which in fact improves
performance. We also introduce a novel method for approximating the prior model
from the reward function, which simplifies the expression of complex objectives
and improves performance over previous active inference approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Noel_A/0/1/0/all/0/1"&gt;Alejandro Daniel Noel&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Hoof_C/0/1/0/all/0/1"&gt;Charel van Hoof&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Millidge_B/0/1/0/all/0/1"&gt;Beren Millidge&lt;/a&gt; (2) ((1) Delft University of Technology, (2) University of Oxford)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Earth Mover's Pinball Loss: Quantiles for Histogram-Valued Regression. (arXiv:2106.02051v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02051</id>
        <link href="http://arxiv.org/abs/2106.02051"/>
        <updated>2021-06-07T03:06:14.079Z</updated>
        <summary type="html"><![CDATA[Although ubiquitous in the sciences, histogram data have not received much
attention by the Deep Learning community. Whilst regression and classification
tasks for scalar and vector data are routinely solved by neural networks, a
principled approach for estimating histogram labels as a function of an input
vector or image is lacking in the literature. We present a dedicated method for
Deep Learning-based histogram regression, which incorporates cross-bin
information and yields distributions over possible histograms, expressed by
$\tau$-quantiles of the cumulative histogram in each bin. The crux of our
approach is a new loss function obtained by applying the pinball loss to the
cumulative histogram, which for 1D histograms reduces to the Earth Mover's
distance (EMD) in the special case of the median ($\tau = 0.5$), and
generalizes it to arbitrary quantiles. We validate our method with an
illustrative toy example, a football-related task, and an astrophysical
computer vision problem. We show that with our loss function, the accuracy of
the predicted median histograms is very similar to the standard EMD case (and
higher than for per-bin loss functions such as cross-entropy), while the
predictions become much more informative at almost no additional computational
cost.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+List_F/0/1/0/all/0/1"&gt;Florian List&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How to select and use tools? : Active Perception of Target Objects Using Multimodal Deep Learning. (arXiv:2106.02445v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2106.02445</id>
        <link href="http://arxiv.org/abs/2106.02445"/>
        <updated>2021-06-07T03:06:14.061Z</updated>
        <summary type="html"><![CDATA[Selection of appropriate tools and use of them when performing daily tasks is
a critical function for introducing robots for domestic applications. In
previous studies, however, adaptability to target objects was limited, making
it difficult to accordingly change tools and adjust actions. To manipulate
various objects with tools, robots must both understand tool functions and
recognize object characteristics to discern a tool-object-action relation. We
focus on active perception using multimodal sensorimotor data while a robot
interacts with objects, and allow the robot to recognize their extrinsic and
intrinsic characteristics. We construct a deep neural networks (DNN) model that
learns to recognize object characteristics, acquires tool-object-action
relations, and generates motions for tool selection and handling. As an example
tool-use situation, the robot performs an ingredients transfer task, using a
turner or ladle to transfer an ingredient from a pot to a bowl. The results
confirm that the robot recognizes object characteristics and servings even when
the target ingredients are unknown. We also examine the contributions of
images, force, and tactile data and show that learning a variety of multimodal
information results in rich perception for tool use.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Saito_N/0/1/0/all/0/1"&gt;Namiko Saito&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ogata_T/0/1/0/all/0/1"&gt;Tetsuya Ogata&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Funabashi_S/0/1/0/all/0/1"&gt;Satoshi Funabashi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mori_H/0/1/0/all/0/1"&gt;Hiroki Mori&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sugano_S/0/1/0/all/0/1"&gt;Shigeki Sugano&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quantum Perceptron Revisited: Computational-Statistical Tradeoffs. (arXiv:2106.02496v1 [quant-ph])]]></title>
        <id>http://arxiv.org/abs/2106.02496</id>
        <link href="http://arxiv.org/abs/2106.02496"/>
        <updated>2021-06-07T03:06:14.055Z</updated>
        <summary type="html"><![CDATA[Quantum machine learning algorithms could provide significant speed-ups over
their classical counterparts; however, whether they could also achieve good
generalization remains unclear. Recently, two quantum perceptron models which
give a quadratic improvement over the classical perceptron algorithm using
Grover's search have been proposed by Wiebe et al. arXiv:1602.04799 . While the
first model reduces the complexity with respect to the size of the training
set, the second one improves the bound on the number of mistakes made by the
perceptron. In this paper, we introduce a hybrid quantum-classical perceptron
algorithm with lower complexity and better generalization ability than the
classical perceptron. We show a quadratic improvement over the classical
perceptron in both the number of samples and the margin of the data. We derive
a bound on the expected error of the hypothesis returned by our algorithm,
which compares favorably to the one obtained with the classical online
perceptron. We use numerical experiments to illustrate the trade-off between
computational complexity and statistical accuracy in quantum perceptron
learning and discuss some of the key practical issues surrounding the
implementation of quantum perceptron models into near-term quantum devices,
whose practical implementation represents a serious challenge due to inherent
noise. However, the potential benefits make correcting this worthwhile.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Roget_M/0/1/0/all/0/1"&gt;Mathieu Roget&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Molfetta_G/0/1/0/all/0/1"&gt;Giuseppe Di Molfetta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Kadri_H/0/1/0/all/0/1"&gt;Hachem Kadri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[F-Drop&Match: GANs with a Dead Zone in the High-Frequency Domain. (arXiv:2106.02343v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02343</id>
        <link href="http://arxiv.org/abs/2106.02343"/>
        <updated>2021-06-07T03:06:14.049Z</updated>
        <summary type="html"><![CDATA[Generative adversarial networks built from deep convolutional neural networks
(GANs) lack the ability to exactly replicate the high-frequency components of
natural images. To alleviate this issue, we introduce two novel training
techniques called frequency dropping (F-Drop) and frequency matching (F-Match).
The key idea of F-Drop is to filter out unnecessary high-frequency components
from the input images of the discriminators. This simple modification prevents
the discriminators from being confused by perturbations of the high-frequency
components. In addition, F-Drop makes the GANs focus on fitting in the
low-frequency domain, in which there are the dominant components of natural
images. F-Match minimizes the difference between real and fake images in the
frequency domain for generating more realistic images. F-Match is implemented
as a regularization term in the objective functions of the generators; it
penalizes the batch mean error in the frequency domain. F-Match helps the
generators to fit in the high-frequency domain filtered out by F-Drop to the
real image. We experimentally demonstrate that the combination of F-Drop and
F-Match improves the generative performance of GANs in both the frequency and
spatial domain on multiple image benchmarks (CIFAR, TinyImageNet, STL-10,
CelebA, and ImageNet).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yamaguchi_S/0/1/0/all/0/1"&gt;Shin&amp;#x27;ya Yamaguchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kanai_S/0/1/0/all/0/1"&gt;Sekitoshi Kanai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Debiasing a First-order Heuristic for Approximate Bi-level Optimization. (arXiv:2106.02487v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02487</id>
        <link href="http://arxiv.org/abs/2106.02487"/>
        <updated>2021-06-07T03:06:14.041Z</updated>
        <summary type="html"><![CDATA[Approximate bi-level optimization (ABLO) consists of (outer-level)
optimization problems, involving numerical (inner-level) optimization loops.
While ABLO has many applications across deep learning, it suffers from time and
memory complexity proportional to the length $r$ of its inner optimization
loop. To address this complexity, an earlier first-order method (FOM) was
proposed as a heuristic that omits second derivative terms, yielding
significant speed gains and requiring only constant memory. Despite FOM's
popularity, there is a lack of theoretical understanding of its convergence
properties. We contribute by theoretically characterizing FOM's gradient bias
under mild assumptions. We further demonstrate a rich family of examples where
FOM-based SGD does not converge to a stationary point of the ABLO objective. We
address this concern by proposing an unbiased FOM (UFOM) enjoying constant
memory complexity as a function of $r$. We characterize the introduced
time-variance tradeoff, demonstrate convergence bounds, and find an optimal
UFOM for a given ABLO problem. Finally, we propose an efficient adaptive UFOM
scheme.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Likhosherstov_V/0/1/0/all/0/1"&gt;Valerii Likhosherstov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1"&gt;Xingyou Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choromanski_K/0/1/0/all/0/1"&gt;Krzysztof Choromanski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Davis_J/0/1/0/all/0/1"&gt;Jared Davis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weller_A/0/1/0/all/0/1"&gt;Adrian Weller&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-supervised Dialogue Learning for Spoken Conversational Question Answering. (arXiv:2106.02182v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02182</id>
        <link href="http://arxiv.org/abs/2106.02182"/>
        <updated>2021-06-07T03:06:14.035Z</updated>
        <summary type="html"><![CDATA[In spoken conversational question answering (SCQA), the answer to the
corresponding question is generated by retrieving and then analyzing a fixed
spoken document, including multi-part conversations. Most SCQA systems have
considered only retrieving information from ordered utterances. However, the
sequential order of dialogue is important to build a robust spoken
conversational question answering system, and the changes of utterances order
may severely result in low-quality and incoherent corpora. To this end, we
introduce a self-supervised learning approach, including incoherence
discrimination, insertion detection, and question prediction, to explicitly
capture the coreference resolution and dialogue coherence among spoken
documents. Specifically, we design a joint learning framework where the
auxiliary self-supervised tasks can enable the pre-trained SCQA systems towards
more coherent and meaningful spoken dialogue learning. We also utilize the
proposed self-supervised learning tasks to capture intra-sentence coherence.
Experimental results demonstrate that our proposed method provides more
coherent, meaningful, and appropriate responses, yielding superior performance
gains compared to the original pre-trained language models. Our method achieves
state-of-the-art results on the Spoken-CoQA dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1"&gt;Nuo Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1"&gt;Chenyu You&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1"&gt;Yuexian Zou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fre-GAN: Adversarial Frequency-consistent Audio Synthesis. (arXiv:2106.02297v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2106.02297</id>
        <link href="http://arxiv.org/abs/2106.02297"/>
        <updated>2021-06-07T03:06:14.028Z</updated>
        <summary type="html"><![CDATA[Although recent works on neural vocoder have improved the quality of
synthesized audio, there still exists a gap between generated and ground-truth
audio in frequency space. This difference leads to spectral artifacts such as
hissing noise or robotic sound, and thus degrades the sample quality. In this
paper, we propose Fre-GAN which achieves frequency-consistent audio synthesis
with highly improved generation quality. Specifically, we first present
resolution-connected generator and resolution-wise discriminators, which help
learn various scales of spectral distributions over multiple frequency bands.
Additionally, to reproduce high-frequency components accurately, we leverage
discrete wavelet transform in the discriminators. From our experiments, Fre-GAN
achieves high-fidelity waveform generation with a gap of only 0.03 MOS compared
to ground-truth audio while outperforming standard models in quality.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Kim_J/0/1/0/all/0/1"&gt;Ji-Hoon Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lee_S/0/1/0/all/0/1"&gt;Sang-Hoon Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lee_J/0/1/0/all/0/1"&gt;Ji-Hyun Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lee_S/0/1/0/all/0/1"&gt;Seong-Whan Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Heterogeneous Noisy Short Signal Camouflage in Multi-Domain Environment Decision-Making. (arXiv:2106.02044v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02044</id>
        <link href="http://arxiv.org/abs/2106.02044"/>
        <updated>2021-06-07T03:06:14.010Z</updated>
        <summary type="html"><![CDATA[Data transmission between two or more digital devices in industry and
government demands secure and agile technology. Digital information
distribution often requires deployment of Internet of Things (IoT) devices and
Data Fusion techniques which have also gained popularity in both, civilian and
military environments, such as, emergence of Smart Cities and Internet of
Battlefield Things (IoBT). This usually requires capturing and consolidating
data from multiple sources. Because datasets do not necessarily originate from
identical sensors, fused data typically results in a complex Big Data problem.
Due to potentially sensitive nature of IoT datasets, Blockchain technology is
used to facilitate secure sharing of IoT datasets, which allows digital
information to be distributed, but not copied. However, blockchain has several
limitations related to complexity, scalability, and excessive energy
consumption. We propose an approach to hide information (sensor signal) by
transforming it to an image or an audio signal. In one of the latest attempts
to the military modernization, we investigate sensor fusion approach by
investigating the challenges of enabling an intelligent identification and
detection operation and demonstrates the feasibility of the proposed Deep
Learning and Anomaly Detection models that can support future application for
specific hand gesture alert system from wearable devices.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_P/0/1/0/all/0/1"&gt;Piyush K. Sharma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Next Generation Multitarget Trackers: Random Finite Set Methods vs Transformer-based Deep Learning. (arXiv:2104.00734v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.00734</id>
        <link href="http://arxiv.org/abs/2104.00734"/>
        <updated>2021-06-07T03:06:14.004Z</updated>
        <summary type="html"><![CDATA[Multitarget Tracking (MTT) is the problem of tracking the states of an
unknown number of objects using noisy measurements, with important applications
to autonomous driving, surveillance, robotics, and others. In the model-based
Bayesian setting, there are conjugate priors that enable us to express the
multi-object posterior in closed form, which could theoretically provide
Bayes-optimal estimates. However, the posterior involves a super-exponential
growth of the number of hypotheses over time, forcing state-of-the-art methods
to resort to approximations for remaining tractable, which can impact their
performance in complex scenarios. Model-free methods based on deep-learning
provide an attractive alternative, as they can, in principle, learn the optimal
filter from data, but to the best of our knowledge were never compared to
current state-of-the-art Bayesian filters, specially not in contexts where
accurate models are available. In this paper, we propose a high-performing
deep-learning method for MTT based on the Transformer architecture and compare
it to two state-of-the-art Bayesian filters, in a setting where we assume the
correct model is provided. Although this gives an edge to the model-based
filters, it also allows us to generate unlimited training data. We show that
the proposed model outperforms state-of-the-art Bayesian filters in complex
scenarios, while matching their performance in simpler cases, which validates
the applicability of deep-learning also in the model-based regime. The code for
all our implementations is made available at
https://github.com/JulianoLagana/MT3 .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pinto_J/0/1/0/all/0/1"&gt;Juliano Pinto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hess_G/0/1/0/all/0/1"&gt;Georg Hess&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ljungbergh_W/0/1/0/all/0/1"&gt;William Ljungbergh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1"&gt;Yuxuan Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Svensson_L/0/1/0/all/0/1"&gt;Lennart Svensson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wymeersch_H/0/1/0/all/0/1"&gt;Henk Wymeersch&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Near-Optimal Confidence Sequences for Bounded Random Variables. (arXiv:2006.05022v3 [math.ST] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.05022</id>
        <link href="http://arxiv.org/abs/2006.05022"/>
        <updated>2021-06-07T03:06:13.996Z</updated>
        <summary type="html"><![CDATA[Many inference problems, such as sequential decision problems like A/B
testing, adaptive sampling schemes like bandit selection, are often online in
nature. The fundamental problem for online inference is to provide a sequence
of confidence intervals that are valid uniformly over the growing-into-infinity
sample sizes. To address this question, we provide a near-optimal confidence
sequence for bounded random variables by utilizing Bentkus' concentration
results. We show that it improves on the existing approaches that use the
Cram{\'e}r-Chernoff technique such as the Hoeffding, Bernstein, and Bennett
inequalities. The resulting confidence sequence is confirmed to be favorable in
both synthetic coverage problems and an application to adaptive stopping
algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Kuchibhotla_A/0/1/0/all/0/1"&gt;Arun Kumar Kuchibhotla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Zheng_Q/0/1/0/all/0/1"&gt;Qinqing Zheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Learning via Persistency of Excitation. (arXiv:2106.02078v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.02078</id>
        <link href="http://arxiv.org/abs/2106.02078"/>
        <updated>2021-06-07T03:06:13.988Z</updated>
        <summary type="html"><![CDATA[Improving adversarial robustness of neural networks remains a major
challenge. Fundamentally, training a network is a parameter estimation problem.
In adaptive control theory, maintaining persistency of excitation (PoE) is
integral to ensuring convergence of parameter estimates in dynamical systems to
their robust optima. In this work, we show that network training using gradient
descent is equivalent to a dynamical system parameter estimation problem.
Leveraging this relationship, we prove a sufficient condition for PoE of
gradient descent is achieved when the learning rate is less than the inverse of
the Lipschitz constant of the gradient of loss function. We provide an
efficient technique for estimating the corresponding Lipschitz constant using
extreme value theory and demonstrate that by only scaling the learning rate
schedule we can increase adversarial accuracy by up to 15% on benchmark
datasets. Our approach also universally increases the adversarial accuracy by
0.1% to 0.3% in various state-of-the-art adversarially trained models on the
AutoAttack benchmark, where every small margin of improvement is significant.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Sridhar_K/0/1/0/all/0/1"&gt;Kaustubh Sridhar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Sokolsky_O/0/1/0/all/0/1"&gt;Oleg Sokolsky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lee_I/0/1/0/all/0/1"&gt;Insup Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Weimer_J/0/1/0/all/0/1"&gt;James Weimer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Musical Prosody-Driven Emotion Classification: Interpreting Vocalists Portrayal of Emotions Through Machine Learning. (arXiv:2106.02556v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2106.02556</id>
        <link href="http://arxiv.org/abs/2106.02556"/>
        <updated>2021-06-07T03:06:13.980Z</updated>
        <summary type="html"><![CDATA[The task of classifying emotions within a musical track has received
widespread attention within the Music Information Retrieval (MIR) community.
Music emotion recognition has traditionally relied on the use of acoustic
features, verbal features, and metadata-based filtering. The role of musical
prosody remains under-explored despite several studies demonstrating a strong
connection between prosody and emotion. In this study, we restrict the input of
traditional machine learning algorithms to the features of musical prosody.
Furthermore, our proposed approach builds upon the prior by classifying
emotions under an expanded emotional taxonomy, using the Geneva Wheel of
Emotion. We utilize a methodology for individual data collection from
vocalists, and personal ground truth labeling by the artist themselves. We
found that traditional machine learning algorithms when limited to the features
of musical prosody (1) achieve high accuracies for a single singer, (2)
maintain high accuracy when the dataset is expanded to multiple singers, and
(3) achieve high accuracies when trained on a reduced subset of the total
features.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nicholas_F/0/1/0/all/0/1"&gt;Farris Nicholas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brian_M/0/1/0/all/0/1"&gt;Model Brian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Richard_S/0/1/0/all/0/1"&gt;Savery Richard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gil_W/0/1/0/all/0/1"&gt;Weinberg Gil&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Strategyproof Learning: Building Trustworthy User-Generated Datasets. (arXiv:2106.02398v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02398</id>
        <link href="http://arxiv.org/abs/2106.02398"/>
        <updated>2021-06-07T03:06:13.964Z</updated>
        <summary type="html"><![CDATA[Today's large-scale machine learning algorithms harness massive amounts of
user-generated data to train large models. However, especially in the context
of content recommendation with enormous social, economical and political
incentives to promote specific views, products or ideologies, strategic users
might be tempted to fabricate or mislabel data in order to bias algorithms in
their favor. Unfortunately, today's learning schemes strongly incentivize such
strategic data misreporting. This is a major concern, as it endangers the
trustworthiness of the entire training datasets, and questions the safety of
any algorithm trained on such datasets. In this paper, we show that, perhaps
surprisingly, incentivizing data misreporting is not a fatality. We propose the
first personalized collaborative learning framework, Licchavi, with provable
strategyproofness guarantees through a careful design of the underlying loss
function. Interestingly, we also prove that Licchavi is Byzantine resilient: it
tolerates a minority of users that provide arbitrary data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Farhadkhani_S/0/1/0/all/0/1"&gt;Sadegh Farhadkhani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guerraoui_R/0/1/0/all/0/1"&gt;Rachid Guerraoui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoang_L/0/1/0/all/0/1"&gt;L&amp;#xea;-Nguy&amp;#xea;n Hoang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Local Adaptivity in Federated Learning: Convergence and Consistency. (arXiv:2106.02305v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02305</id>
        <link href="http://arxiv.org/abs/2106.02305"/>
        <updated>2021-06-07T03:06:13.958Z</updated>
        <summary type="html"><![CDATA[The federated learning (FL) framework trains a machine learning model using
decentralized data stored at edge client devices by periodically aggregating
locally trained models. Popular optimization algorithms of FL use vanilla
(stochastic) gradient descent for both local updates at clients and global
updates at the aggregating server. Recently, adaptive optimization methods such
as AdaGrad have been studied for server updates. However, the effect of using
adaptive optimization methods for local updates at clients is not yet
understood. We show in both theory and practice that while local adaptive
methods can accelerate convergence, they can cause a non-vanishing solution
bias, where the final converged solution may be different from the stationary
point of the global objective function. We propose correction techniques to
overcome this inconsistency and complement the local adaptive methods for FL.
Extensive experiments on realistic federated training tasks show that the
proposed algorithms can achieve faster convergence and higher test accuracy
than the baselines without local adaptivity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jianyu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zheng Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Garrett_Z/0/1/0/all/0/1"&gt;Zachary Garrett&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Charles_Z/0/1/0/all/0/1"&gt;Zachary Charles&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Luyang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joshi_G/0/1/0/all/0/1"&gt;Gauri Joshi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[COLD: Concurrent Loads Disaggregator for Non-Intrusive Load Monitoring. (arXiv:2106.02352v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2106.02352</id>
        <link href="http://arxiv.org/abs/2106.02352"/>
        <updated>2021-06-07T03:06:13.951Z</updated>
        <summary type="html"><![CDATA[The modern artificial intelligence techniques show the outstanding
performances in the field of Non-Intrusive Load Monitoring (NILM). However, the
problem related to the identification of a large number of appliances working
simultaneously is underestimated. One of the reasons is the absence of a
specific data. In this research we propose the Synthesizer of Normalized
Signatures (SNS) algorithm to simulate the aggregated consumption with up to 10
concurrent loads. The results show that the synthetic data provides the models
with at least as a powerful identification accuracy as the real-world
measurements. We have developed the neural architecture named Concurrent Loads
Disaggregator (COLD) which is relatively simple and easy to understand in
comparison to the previous approaches. Our model allows identifying from 1 to
10 appliances working simultaneously with mean F1-score 78.95%. The source code
of the experiments performed is available at
https://github.com/arx7ti/cold-nilm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Kamyshev_I/0/1/0/all/0/1"&gt;Ilia Kamyshev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kriukov_D/0/1/0/all/0/1"&gt;Dmitrii Kriukov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gryazina_E/0/1/0/all/0/1"&gt;Elena Gryazina&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Differentiable Dynamic Quantization with Mixed Precision and Adaptive Resolution. (arXiv:2106.02295v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02295</id>
        <link href="http://arxiv.org/abs/2106.02295"/>
        <updated>2021-06-07T03:06:13.945Z</updated>
        <summary type="html"><![CDATA[Model quantization is challenging due to many tedious hyper-parameters such
as precision (bitwidth), dynamic range (minimum and maximum discrete values)
and stepsize (interval between discrete values). Unlike prior arts that
carefully tune these values, we present a fully differentiable approach to
learn all of them, named Differentiable Dynamic Quantization (DDQ), which has
several benefits. (1) DDQ is able to quantize challenging lightweight
architectures like MobileNets, where different layers prefer different
quantization parameters. (2) DDQ is hardware-friendly and can be easily
implemented using low-precision matrix-vector multiplication, making it capable
in many hardware such as ARM. (3) Extensive experiments show that DDQ
outperforms prior arts on many networks and benchmarks, especially when models
are already efficient and compact. e.g., DDQ is the first approach that
achieves lossless 4-bit quantization for MobileNetV2 on ImageNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhaoyang_Z/0/1/0/all/0/1"&gt;Zhang Zhaoyang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wenqi_S/0/1/0/all/0/1"&gt;Shao Wenqi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jinwei_G/0/1/0/all/0/1"&gt;Gu Jinwei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiaogang_W/0/1/0/all/0/1"&gt;Wang Xiaogang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ping_L/0/1/0/all/0/1"&gt;Luo Ping&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning domain-agnostic visual representation for computational pathology using medically-irrelevant style transfer augmentation. (arXiv:2102.01678v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.01678</id>
        <link href="http://arxiv.org/abs/2102.01678"/>
        <updated>2021-06-07T03:06:13.938Z</updated>
        <summary type="html"><![CDATA[Suboptimal generalization of machine learning models on unseen data is a key
challenge which hampers the clinical applicability of such models to medical
imaging. Although various methods such as domain adaptation and domain
generalization have evolved to combat this challenge, learning robust and
generalizable representations is core to medical image understanding, and
continues to be a problem. Here, we propose STRAP (Style TRansfer Augmentation
for histoPathology), a form of data augmentation based on random style transfer
from non-medical style source such as artistic paintings, for learning
domain-agnostic visual representations in computational pathology. Style
transfer replaces the low-level texture content of an image with the
uninformative style of randomly selected style source image, while preserving
the original high-level semantic content. This improves robustness to domain
shift and can be used as a simple yet powerful tool for learning
domain-agnostic representations. We demonstrate that STRAP leads to
state-of-the-art performance, particularly in the presence of domain shifts, on
two particular classification tasks in computational pathology.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Yamashita_R/0/1/0/all/0/1"&gt;Rikiya Yamashita&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Long_J/0/1/0/all/0/1"&gt;Jin Long&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Banda_S/0/1/0/all/0/1"&gt;Snikitha Banda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shen_J/0/1/0/all/0/1"&gt;Jeanne Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rubin_D/0/1/0/all/0/1"&gt;Daniel L. Rubin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transferable and Distributed User Association Policies for 5G and Beyond Networks. (arXiv:2106.02540v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02540</id>
        <link href="http://arxiv.org/abs/2106.02540"/>
        <updated>2021-06-07T03:06:13.932Z</updated>
        <summary type="html"><![CDATA[We study the problem of user association, namely finding the optimal
assignment of user equipment to base stations to achieve a targeted network
performance. In this paper, we focus on the knowledge transferability of
association policies. Indeed, traditional non-trivial user association schemes
are often scenario-specific or deployment-specific and require a policy
re-design or re-learning when the number or the position of the users change.
In contrast, transferability allows to apply a single user association policy,
devised for a specific scenario, to other distinct user deployments, without
needing a substantial re-learning or re-design phase and considerably reducing
its computational and management complexity. To achieve transferability, we
first cast user association as a multi-agent reinforcement learning problem.
Then, based on a neural attention mechanism that we specifically conceived for
this context, we propose a novel distributed policy network architecture, which
is transferable among users with zero-shot generalization capability i.e.,
without requiring additional training.Numerical results show the effectiveness
of our solution in terms of overall network communication rate, outperforming
centralized benchmarks even when the number of users doubles with respect to
the initial training point.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sana_M/0/1/0/all/0/1"&gt;Mohamed Sana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pietro_N/0/1/0/all/0/1"&gt;Nicola di Pietro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Strinati_E/0/1/0/all/0/1"&gt;Emilio Calvanese Strinati&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Shape-Preserving Dimensionality Reduction : An Algorithm and Measures of Topological Equivalence. (arXiv:2106.02096v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.02096</id>
        <link href="http://arxiv.org/abs/2106.02096"/>
        <updated>2021-06-07T03:06:13.913Z</updated>
        <summary type="html"><![CDATA[We introduce a linear dimensionality reduction technique preserving
topological features via persistent homology. The method is designed to find
linear projection $L$ which preserves the persistent diagram of a point cloud
$\mathbb{X}$ via simulated annealing. The projection $L$ induces a set of
canonical simplicial maps from the Rips (or \v{C}ech) filtration of
$\mathbb{X}$ to that of $L\mathbb{X}$. In addition to the distance between
persistent diagrams, the projection induces a map between filtrations, called
filtration homomorphism. Using the filtration homomorphism, one can measure the
difference between shapes of two filtrations directly comparing simplicial
complexes with respect to quasi-isomorphism $\mu_{\operatorname{quasi-iso}}$ or
strong homotopy equivalence $\mu_{\operatorname{equiv}}$. These
$\mu_{\operatorname{quasi-iso}}$ and $\mu_{\operatorname{equiv}}$ measures how
much portion of corresponding simplicial complexes is quasi-isomorphic or
homotopy equivalence respectively. We validate the effectiveness of our
framework with simple examples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Yu_B/0/1/0/all/0/1"&gt;Byeongsu Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+You_K/0/1/0/all/0/1"&gt;Kisung You&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Hard Optimization Problems: A Data Generation Perspective. (arXiv:2106.02601v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2106.02601</id>
        <link href="http://arxiv.org/abs/2106.02601"/>
        <updated>2021-06-07T03:06:13.902Z</updated>
        <summary type="html"><![CDATA[Optimization problems are ubiquitous in our societies and are present in
almost every segment of the economy. Most of these optimization problems are
NP-hard and computationally demanding, often requiring approximate solutions
for large-scale instances. Machine learning frameworks that learn to
approximate solutions to such hard optimization problems are a potentially
promising avenue to address these difficulties, particularly when many closely
related problem instances must be solved repeatedly. Supervised learning
frameworks can train a model using the outputs of pre-solved instances.
However, when the outputs are themselves approximations, when the optimization
problem has symmetric solutions, and/or when the solver uses randomization,
solutions to closely related instances may exhibit large differences and the
learning task can become inherently more difficult. This paper demonstrates
this critical challenge, connects the volatility of the training data to the
ability of a model to approximate it, and proposes a method for producing
(exact or approximate) solutions to optimization problems that are more
amenable to supervised learning tasks. The effectiveness of the method is
tested on hard non-linear nonconvex and discrete combinatorial problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Kotary_J/0/1/0/all/0/1"&gt;James Kotary&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Fioretto_F/0/1/0/all/0/1"&gt;Ferdinando Fioretto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Hentenryck_P/0/1/0/all/0/1"&gt;Pascal Van Hentenryck&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Slice-Aware Representations with Mixture of Attentions. (arXiv:2106.02363v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02363</id>
        <link href="http://arxiv.org/abs/2106.02363"/>
        <updated>2021-06-07T03:06:13.894Z</updated>
        <summary type="html"><![CDATA[Real-world machine learning systems are achieving remarkable performance in
terms of coarse-grained metrics like overall accuracy and F-1 score. However,
model improvement and development often require fine-grained modeling on
individual data subsets or slices, for instance, the data slices where the
models have unsatisfactory results. In practice, it gives tangible values for
developing such models that can pay extra attention to critical or interested
slices while retaining the original overall performance. This work extends the
recent slice-based learning (SBL)~\cite{chen2019slice} with a mixture of
attentions (MoA) to learn slice-aware dual attentive representations. We
empirically show that the MoA approach outperforms the baseline method as well
as the original SBL approach on monitored slices with two natural language
understanding (NLU) tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Cheng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1"&gt;Sungjin Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1"&gt;Sunghyun Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Han Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1"&gt;Young-Bum Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sarikaya_R/0/1/0/all/0/1"&gt;Ruhi Sarikaya&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fluctuation-dissipation Type Theorem in Stochastic Linear Learning. (arXiv:2106.02220v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02220</id>
        <link href="http://arxiv.org/abs/2106.02220"/>
        <updated>2021-06-07T03:06:13.888Z</updated>
        <summary type="html"><![CDATA[The fluctuation-dissipation theorem (FDT) is a simple yet powerful
consequence of the first-order differential equation governing the dynamics of
systems subject simultaneously to dissipative and stochastic forces. The linear
learning dynamics, in which the input vector maps to the output vector by a
linear matrix whose elements are the subject of learning, has a stochastic
version closely mimicking the Langevin dynamics when a full-batch gradient
descent scheme is replaced by that of stochastic gradient descent. We derive a
generalized FDT for the stochastic linear learning dynamics and verify its
validity among the well-known machine learning data sets such as MNIST,
CIFAR-10 and EMNIST.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Han_M/0/1/0/all/0/1"&gt;Manhyung Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1"&gt;Jeonghyeok Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_T/0/1/0/all/0/1"&gt;Taewoong Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1"&gt;Jung Hoon Han&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Consciousness-Inspired Planning Agent for Model-Based Reinforcement Learning. (arXiv:2106.02097v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2106.02097</id>
        <link href="http://arxiv.org/abs/2106.02097"/>
        <updated>2021-06-07T03:06:13.871Z</updated>
        <summary type="html"><![CDATA[We present an end-to-end, model-based deep reinforcement learning agent which
dynamically attends to relevant parts of its state, in order to plan and to
generalize better out-of-distribution. The agent's architecture uses a set
representation and a bottleneck mechanism, forcing the number of entities to
which the agent attends at each planning step to be small. In experiments with
customized MiniGrid environments with different dynamics, we observe that the
design allows agents to learn to plan effectively, by attending to the relevant
objects, leading to better out-of-distribution generalization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1"&gt;Mingde Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhen Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luan_S/0/1/0/all/0/1"&gt;Sitao Luan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shuyuan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Precup_D/0/1/0/all/0/1"&gt;Doina Precup&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1"&gt;Yoshua Bengio&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Architecture Search via Bregman Iterations. (arXiv:2106.02479v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02479</id>
        <link href="http://arxiv.org/abs/2106.02479"/>
        <updated>2021-06-07T03:06:13.865Z</updated>
        <summary type="html"><![CDATA[We propose a novel strategy for Neural Architecture Search (NAS) based on
Bregman iterations. Starting from a sparse neural network our gradient-based
one-shot algorithm gradually adds relevant parameters in an inverse scale space
manner. This allows the network to choose the best architecture in the search
space which makes it well-designed for a given task, e.g., by adding neurons or
skip connections. We demonstrate that using our approach one can unveil, for
instance, residual autoencoders for denoising, deblurring, and classification
tasks. Code is available at https://github.com/TimRoith/BregmanLearning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bungert_L/0/1/0/all/0/1"&gt;Leon Bungert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roith_T/0/1/0/all/0/1"&gt;Tim Roith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tenbrinck_D/0/1/0/all/0/1"&gt;Daniel Tenbrinck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Burger_M/0/1/0/all/0/1"&gt;Martin Burger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Little Robustness Goes a Long Way: Leveraging Universal Features for Targeted Transfer Attacks. (arXiv:2106.02105v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02105</id>
        <link href="http://arxiv.org/abs/2106.02105"/>
        <updated>2021-06-07T03:06:13.858Z</updated>
        <summary type="html"><![CDATA[Adversarial examples for neural network image classifiers are known to be
transferable: examples optimized to be misclassified by a source classifier are
often misclassified as well by classifiers with different architectures.
However, targeted adversarial examples -- optimized to be classified as a
chosen target class -- tend to be less transferable between architectures.
While prior research on constructing transferable targeted attacks has focused
on improving the optimization procedure, in this work we examine the role of
the source classifier. Here, we show that training the source classifier to be
"slightly robust" -- that is, robust to small-magnitude adversarial examples --
substantially improves the transferability of targeted attacks, even between
architectures as different as convolutional neural networks and transformers.
We argue that this result supports a non-intuitive hypothesis: on the spectrum
from non-robust (standard) to highly robust classifiers, those that are only
slightly robust exhibit the most universal features -- ones that tend to
overlap with the features learned by other classifiers trained on the same
dataset. The results we present provide insight into the nature of adversarial
examples as well as the mechanisms underlying so-called "robust" classifiers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Springer_J/0/1/0/all/0/1"&gt;Jacob M. Springer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mitchell_M/0/1/0/all/0/1"&gt;Melanie Mitchell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kenyon_G/0/1/0/all/0/1"&gt;Garrett T. Kenyon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Celebrating Diversity in Shared Multi-Agent Reinforcement Learning. (arXiv:2106.02195v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02195</id>
        <link href="http://arxiv.org/abs/2106.02195"/>
        <updated>2021-06-07T03:06:13.852Z</updated>
        <summary type="html"><![CDATA[Recently, deep multi-agent reinforcement learning (MARL) has shown the
promise to solve complex cooperative tasks. Its success is partly because of
parameter sharing among agents. However, such sharing may lead agents to behave
similarly and limit their coordination capacity. In this paper, we aim to
introduce diversity in both optimization and representation of shared
multi-agent reinforcement learning. Specifically, we propose an
information-theoretical regularization to maximize the mutual information
between agents' identities and their trajectories, encouraging extensive
exploration and diverse individualized behaviors. In representation, we
incorporate agent-specific modules in the shared neural network architecture,
which are regularized by L1-norm to promote learning sharing among agents while
keeping necessary diversity. Empirical results show that our method achieves
state-of-the-art performance on Google Research Football and super hard
StarCraft II micromanagement tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chenghao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+WU_C/0/1/0/all/0/1"&gt;Chengjie WU&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1"&gt;Tonghan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jun Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1"&gt;Qianchuan Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chongjie Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Surprising Power of Graph Neural Networks with Random Node Initialization. (arXiv:2010.01179v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.01179</id>
        <link href="http://arxiv.org/abs/2010.01179"/>
        <updated>2021-06-07T03:06:13.844Z</updated>
        <summary type="html"><![CDATA[Graph neural networks (GNNs) are effective models for representation learning
on relational data. However, standard GNNs are limited in their expressive
power, as they cannot distinguish graphs beyond the capability of the
Weisfeiler-Leman graph isomorphism heuristic. In order to break this
expressiveness barrier, GNNs have been enhanced with random node initialization
(RNI), where the idea is to train and run the models with randomized initial
node features. In this work, we analyze the expressive power of GNNs with RNI,
and prove that these models are universal, a first such result for GNNs not
relying on computationally demanding higher-order properties. This universality
result holds even with partially randomized initial node features, and
preserves the invariance properties of GNNs in expectation. We then empirically
analyze the effect of RNI on GNNs, based on carefully constructed datasets. Our
empirical findings support the superior performance of GNNs with RNI over
standard GNNs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Abboud_R/0/1/0/all/0/1"&gt;Ralph Abboud&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ceylan_I/0/1/0/all/0/1"&gt;&amp;#x130;smail &amp;#x130;lkan Ceylan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grohe_M/0/1/0/all/0/1"&gt;Martin Grohe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lukasiewicz_T/0/1/0/all/0/1"&gt;Thomas Lukasiewicz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The impact of using biased performance metrics on software defect prediction research. (arXiv:2103.10201v3 [cs.SE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.10201</id>
        <link href="http://arxiv.org/abs/2103.10201"/>
        <updated>2021-06-07T03:06:13.837Z</updated>
        <summary type="html"><![CDATA[Context: Software engineering researchers have undertaken many experiments
investigating the potential of software defect prediction algorithms.
Unfortunately, some widely used performance metrics are known to be
problematic, most notably F1, but nevertheless F1 is widely used.

Objective: To investigate the potential impact of using F1 on the validity of
this large body of research.

Method: We undertook a systematic review to locate relevant experiments and
then extract all pairwise comparisons of defect prediction performance using F1
and the un-biased Matthews correlation coefficient (MCC).

Results: We found a total of 38 primary studies. These contain 12,471 pairs
of results. Of these, 21.95% changed direction when the MCC metric is used
instead of the biased F1 metric. Unfortunately, we also found evidence
suggesting that F1 remains widely used in software defect prediction research.

Conclusions: We reiterate the concerns of statisticians that the F1 is a
problematic metric outside of an information retrieval context, since we are
concerned about both classes (defect-prone and not defect-prone units). This
inappropriate usage has led to a substantial number (more than one fifth) of
erroneous (in terms of direction) results. Therefore we urge researchers to (i)
use an unbiased metric and (ii) publish detailed results including confusion
matrices such that alternative analyses become possible.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1"&gt;Jingxiu Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shepperd_M/0/1/0/all/0/1"&gt;Martin Shepperd&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Novel Semi-supervised Framework for Call Center Agent Malpractice Detection via Neural Feature Learning. (arXiv:2106.02433v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02433</id>
        <link href="http://arxiv.org/abs/2106.02433"/>
        <updated>2021-06-07T03:06:13.820Z</updated>
        <summary type="html"><![CDATA[This work presents a practical solution to the problem of call center agent
malpractice. A semi-supervised framework comprising of non-linear power
transformation, neural feature learning and k-means clustering is outlined. We
put these building blocks together and tune the parameters so that the best
performance was obtained. The data used in the experiments is obtained from our
in-house call center. It is made up of recorded agent-customer conversations
which have been annotated using a convolutional neural network based segmenter.
The methods provided a means of tuning the parameters of the neural network to
achieve a desirable result. We show that, using our proposed framework, it is
possible to significantly reduce the malpractice classification error of a
k-means-only clustering model which would serve the same purpose. Additionally,
by presenting the amount of silence per call as a key performance indicator, we
show that the proposed system has enhanced agents performance at our call
center since deployment.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ozan_S/0/1/0/all/0/1"&gt;&amp;#x15e;&amp;#xfc;kr&amp;#xfc; Ozan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iheme_L/0/1/0/all/0/1"&gt;Leonardo Obinna Iheme&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Identifying Misinformation from Website Screenshots. (arXiv:2102.07849v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.07849</id>
        <link href="http://arxiv.org/abs/2102.07849"/>
        <updated>2021-06-07T03:06:13.813Z</updated>
        <summary type="html"><![CDATA[Can the look and the feel of a website give information about the
trustworthiness of an article? In this paper, we propose to use a promising,
yet neglected aspect in detecting the misinformativeness: the overall look of
the domain webpage. To capture this overall look, we take screenshots of news
articles served by either misinformative or trustworthy web domains and
leverage a tensor decomposition based semi-supervised classification technique.
The proposed approach i.e., VizFake is insensitive to a number of image
transformations such as converting the image to grayscale, vectorizing the
image and losing some parts of the screenshots. VizFake leverages a very small
amount of known labels, mirroring realistic and practical scenarios, where
labels (especially for known misinformative articles), are scarce and quickly
become dated. The F1 score of VizFake on a dataset of 50k screenshots of news
articles spanning more than 500 domains is roughly 85% using only 5% of ground
truth labels. Furthermore, tensor representations of VizFake, obtained in an
unsupervised manner, allow for exploratory analysis of the data that provides
valuable insights into the problem. Finally, we compare VizFake with deep
transfer learning, since it is a very popular black-box approach for image
classification and also well-known text text-based methods. VizFake achieves
competitive accuracy with deep transfer learning models while being two orders
of magnitude faster and not requiring laborious hyper-parameter tuning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Abdali_S/0/1/0/all/0/1"&gt;Sara Abdali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gurav_R/0/1/0/all/0/1"&gt;Rutuja Gurav&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Menon_S/0/1/0/all/0/1"&gt;Siddharth Menon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fonseca_D/0/1/0/all/0/1"&gt;Daniel Fonseca&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Entezari_N/0/1/0/all/0/1"&gt;Negin Entezari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shah_N/0/1/0/all/0/1"&gt;Neil Shah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Papalexakis_E/0/1/0/all/0/1"&gt;Evangelos E. Papalexakis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Segmental Contrastive Predictive Coding for Unsupervised Word Segmentation. (arXiv:2106.02170v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2106.02170</id>
        <link href="http://arxiv.org/abs/2106.02170"/>
        <updated>2021-06-07T03:06:13.788Z</updated>
        <summary type="html"><![CDATA[Automatic detection of phoneme or word-like units is one of the core
objectives in zero-resource speech processing. Recent attempts employ
self-supervised training methods, such as contrastive predictive coding (CPC),
where the next frame is predicted given past context. However, CPC only looks
at the audio signal's frame-level structure. We overcome this limitation with a
segmental contrastive predictive coding (SCPC) framework that can model the
signal structure at a higher level e.g. at the phoneme level. In this
framework, a convolutional neural network learns frame-level representation
from the raw waveform via noise-contrastive estimation (NCE). A differentiable
boundary detector finds variable-length segments, which are then used to
optimize a segment encoder via NCE to learn segment representations. The
differentiable boundary detector allows us to train frame-level and
segment-level encoders jointly. Typically, phoneme and word segmentation are
treated as separate tasks. We unify them and experimentally show that our
single model outperforms existing phoneme and word segmentation methods on
TIMIT and Buckeye datasets. We analyze the impact of boundary threshold and
when is the right time to include the segmental loss in the learning process.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Bhati_S/0/1/0/all/0/1"&gt;Saurabhchand Bhati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Villalba_J/0/1/0/all/0/1"&gt;Jes&amp;#xfa;s Villalba&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zelasko_P/0/1/0/all/0/1"&gt;Piotr &amp;#x17b;elasko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Moro_Velazquez_L/0/1/0/all/0/1"&gt;Laureano Moro-Velazquez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dehak_N/0/1/0/all/0/1"&gt;Najim Dehak&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[$\ell_2$-norm Flow Diffusion in Near-Linear Time. (arXiv:2105.14629v2 [cs.DS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14629</id>
        <link href="http://arxiv.org/abs/2105.14629"/>
        <updated>2021-06-07T03:06:13.772Z</updated>
        <summary type="html"><![CDATA[Diffusion is a fundamental graph procedure and has been a basic building
block in a wide range of theoretical and empirical applications such as graph
partitioning and semi-supervised learning on graphs. In this paper, we study
computationally efficient diffusion primitives beyond random walk.

We design an $\widetilde{O}(m)$-time randomized algorithm for the
$\ell_2$-norm flow diffusion problem, a recently proposed diffusion model based
on network flow with demonstrated graph clustering related applications both in
theory and in practice. Examples include finding locally-biased low conductance
cuts. Using a known connection between the optimal dual solution of the flow
diffusion problem and the local cut structure, our algorithm gives an
alternative approach for finding such cuts in nearly linear time.

From a technical point of view, our algorithm contributes a novel way of
dealing with inequality constraints in graph optimization problems. It adapts
the high-level algorithmic framework of nearly linear time Laplacian system
solvers, but requires several new tools: vertex elimination under constraints,
a new family of graph ultra-sparsifiers, and accelerated proximal gradient
methods with inexact proximal mapping computation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Li Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_R/0/1/0/all/0/1"&gt;Richard Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1"&gt;Di Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Extreme sparsity gives rise to functional specialization. (arXiv:2106.02626v1 [q-bio.NC])]]></title>
        <id>http://arxiv.org/abs/2106.02626</id>
        <link href="http://arxiv.org/abs/2106.02626"/>
        <updated>2021-06-07T03:06:13.761Z</updated>
        <summary type="html"><![CDATA[Modularity of neural networks -- both biological and artificial -- can be
thought of either structurally or functionally, and the relationship between
these is an open question. We show that enforcing structural modularity via
sparse connectivity between two dense sub-networks which need to communicate to
solve the task leads to functional specialization of the sub-networks, but only
at extreme levels of sparsity. With even a moderate number of interconnections,
the sub-networks become functionally entangled. Defining functional
specialization is in itself a challenging problem without a universally agreed
solution. To address this, we designed three different measures of
specialization (based on weight masks, retraining and correlation) and found
them to qualitatively agree. Our results have implications in both neuroscience
and machine learning. For neuroscience, it shows that we cannot conclude that
there is functional modularity simply by observing moderate levels of
structural modularity: knowing the brain's connectome is not sufficient for
understanding how it breaks down into functional modules. For machine learning,
using structure to promote functional modularity -- which may be important for
robustness and generalization -- may require extremely narrow bottlenecks
between modules.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Bena_G/0/1/0/all/0/1"&gt;Gabriel B&amp;#xe9;na&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Goodman_D/0/1/0/all/0/1"&gt;Dan F. M. Goodman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incomplete Graph Representation and Learning via Partial Graph Neural Networks. (arXiv:2003.10130v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.10130</id>
        <link href="http://arxiv.org/abs/2003.10130"/>
        <updated>2021-06-07T03:06:13.648Z</updated>
        <summary type="html"><![CDATA[Graph Neural Networks (GNNs) are gaining increasing attention on graph data
learning tasks in recent years. However, in many applications, graph may be
coming in an incomplete form where attributes of graph nodes are partially
unknown/missing. Existing GNNs are generally designed on complete graphs which
can not deal with attribute-incomplete graph data directly. To address this
problem, we develop a novel partial aggregation based GNNs, named Partial Graph
Neural Networks (PaGNNs), for attribute-incomplete graph representation and
learning. Our work is motivated by the observation that the neighborhood
aggregation function in standard GNNs can be equivalently viewed as the
neighborhood reconstruction formulation. Based on it, we define two novel
partial aggregation (reconstruction) functions on incomplete graph and derive
PaGNNs for incomplete graph data learning. Extensive experiments on several
datasets demonstrate the effectiveness and efficiency of the proposed PaGNNs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_B/0/1/0/all/0/1"&gt;Bo Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Ziyan Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cross-Trajectory Representation Learning for Zero-Shot Generalization in RL. (arXiv:2106.02193v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02193</id>
        <link href="http://arxiv.org/abs/2106.02193"/>
        <updated>2021-06-07T03:06:13.624Z</updated>
        <summary type="html"><![CDATA[A highly desirable property of a reinforcement learning (RL) agent -- and a
major difficulty for deep RL approaches -- is the ability to generalize
policies learned on a few tasks over a high-dimensional observation space to
similar tasks not seen during training. Many promising approaches to this
challenge consider RL as a process of training two functions simultaneously: a
complex nonlinear encoder that maps high-dimensional observations to a latent
representation space, and a simple linear policy over this space. We posit that
a superior encoder for zero-shot generalization in RL can be trained by using
solely an auxiliary SSL objective if the training process encourages the
encoder to map behaviorally similar observations to similar representations, as
reward-based signal can cause overfitting in the encoder (Raileanu et al.,
2021). We propose Cross-Trajectory Representation Learning (CTRL), a method
that runs within an RL agent and conditions its encoder to recognize behavioral
similarity in observations by applying a novel SSL objective to pairs of
trajectories from the agent's policies. CTRL can be viewed as having the same
effect as inducing a pseudo-bisimulation metric but, crucially, avoids the use
of rewards and associated overfitting risks. Our experiments ablate various
components of CTRL and demonstrate that in combination with PPO it achieves
better generalization performance on the challenging Procgen benchmark suite
(Cobbe et al., 2020).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mazoure_B/0/1/0/all/0/1"&gt;Bogdan Mazoure&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ahmed_A/0/1/0/all/0/1"&gt;Ahmed M. Ahmed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+MacAlpine_P/0/1/0/all/0/1"&gt;Patrick MacAlpine&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hjelm_R/0/1/0/all/0/1"&gt;R Devon Hjelm&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kolobov_A/0/1/0/all/0/1"&gt;Andrey Kolobov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using latent space regression to analyze and leverage compositionality in GANs. (arXiv:2103.10426v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.10426</id>
        <link href="http://arxiv.org/abs/2103.10426"/>
        <updated>2021-06-07T03:06:13.443Z</updated>
        <summary type="html"><![CDATA[In recent years, Generative Adversarial Networks have become ubiquitous in
both research and public perception, but how GANs convert an unstructured
latent code to a high quality output is still an open question. In this work,
we investigate regression into the latent space as a probe to understand the
compositional properties of GANs. We find that combining the regressor and a
pretrained generator provides a strong image prior, allowing us to create
composite images from a collage of random image parts at inference time while
maintaining global consistency. To compare compositional properties across
different generators, we measure the trade-offs between reconstruction of the
unrealistic input and image quality of the regenerated samples. We find that
the regression approach enables more localized editing of individual image
parts compared to direct editing in the latent space, and we conduct
experiments to quantify this independence effect. Our method is agnostic to the
semantics of edits, and does not require labels or predefined concepts during
training. Beyond image composition, our method extends to a number of related
applications, such as image inpainting or example-based image editing, which we
demonstrate on several GANs and datasets, and because it uses only a single
forward pass, it can operate in real-time. Code is available on our project
page: https://chail.github.io/latent-composition/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chai_L/0/1/0/all/0/1"&gt;Lucy Chai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wulff_J/0/1/0/all/0/1"&gt;Jonas Wulff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Isola_P/0/1/0/all/0/1"&gt;Phillip Isola&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interpretable COVID-19 Chest X-Ray Classification via Orthogonality Constraint. (arXiv:2102.08360v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.08360</id>
        <link href="http://arxiv.org/abs/2102.08360"/>
        <updated>2021-06-07T03:06:13.428Z</updated>
        <summary type="html"><![CDATA[Deep neural networks have increasingly been used as an auxiliary tool in
healthcare applications, due to their ability to improve performance of several
diagnosis tasks. However, these methods are not widely adopted in clinical
settings due to the practical limitations in the reliability, generalizability,
and interpretability of deep learning based systems. As a result, methods have
been developed that impose additional constraints during network training to
gain more control as well as improve interpretabilty, facilitating their
acceptance in healthcare community. In this work, we investigate the benefit of
using Orthogonal Spheres (OS) constraint for classification of COVID-19 cases
from chest X-ray images. The OS constraint can be written as a simple
orthonormality term which is used in conjunction with the standard
cross-entropy loss during classification network training. Previous studies
have demonstrated significant benefits in applying such constraints to deep
learning models. Our findings corroborate these observations, indicating that
the orthonormality loss function effectively produces improved semantic
localization via GradCAM visualizations, enhanced classification performance,
and reduced model calibration error. Our approach achieves an improvement in
accuracy of 1.6% and 4.8% for two- and three-class classification,
respectively; similar results are found for models with data augmentation
applied. In addition to these findings, our work also presents a new
application of the OS regularizer in healthcare, increasing the post-hoc
interpretability and performance of deep learning models for COVID-19
classification to facilitate adoption of these methods in clinical settings. We
also identify the limitations of our strategy that can be explored for further
research in future.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_E/0/1/0/all/0/1"&gt;Ella Y. Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Som_A/0/1/0/all/0/1"&gt;Anirudh Som&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shukla_A/0/1/0/all/0/1"&gt;Ankita Shukla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_H/0/1/0/all/0/1"&gt;Hongjun Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Turaga_P/0/1/0/all/0/1"&gt;Pavan Turaga&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TS-CAM: Token Semantic Coupled Attention Map for Weakly Supervised Object Localization. (arXiv:2103.14862v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.14862</id>
        <link href="http://arxiv.org/abs/2103.14862"/>
        <updated>2021-06-07T03:06:13.409Z</updated>
        <summary type="html"><![CDATA[Weakly supervised object localization (WSOL) is a challenging problem when
given image category labels but requires to learn object localization models.
Optimizing a convolutional neural network (CNN) for classification tends to
activate local discriminative regions while ignoring complete object extent,
causing the partial activation issue. In this paper, we argue that partial
activation is caused by the intrinsic characteristics of CNN, where the
convolution operations produce local receptive fields and experience difficulty
to capture long-range feature dependency among pixels. We introduce the token
semantic coupled attention map (TS-CAM) to take full advantage of the
self-attention mechanism in visual transformer for long-range dependency
extraction. TS-CAM first splits an image into a sequence of patch tokens for
spatial embedding, which produce attention maps of long-range visual dependency
to avoid partial activation. TS-CAM then re-allocates category-related
semantics for patch tokens, enabling each of them to be aware of object
categories. TS-CAM finally couples the patch tokens with the semantic-agnostic
attention map to achieve semantic-aware localization. Experiments on the
ILSVRC/CUB-200-2011 datasets show that TS-CAM outperforms its CNN-CAM
counterparts by 7.1%/27.1% for WSOL, achieving state-of-the-art performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1"&gt;Wei Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wan_F/0/1/0/all/0/1"&gt;Fang Wan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1"&gt;Xingjia Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_Z/0/1/0/all/0/1"&gt;Zhiliang Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1"&gt;Qi Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1"&gt;Zhenjun Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1"&gt;Bolei Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1"&gt;Qixiang Ye&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Good Practices and A Strong Baseline for Traffic Anomaly Detection. (arXiv:2105.03827v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.03827</id>
        <link href="http://arxiv.org/abs/2105.03827"/>
        <updated>2021-06-07T03:06:13.374Z</updated>
        <summary type="html"><![CDATA[The detection of traffic anomalies is a critical component of the intelligent
city transportation management system. Previous works have proposed a variety
of notable insights and taken a step forward in this field, however, dealing
with the complex traffic environment remains a challenge. Moreover, the lack of
high-quality data and the complexity of the traffic scene, motivate us to study
this problem from a hand-crafted perspective. In this paper, we propose a
straightforward and efficient framework that includes pre-processing, a dynamic
track module, and post-processing. With video stabilization, background
modeling, and vehicle detection, the pro-processing phase aims to generate
candidate anomalies. The dynamic tracking module seeks and locates the start
time of anomalies by utilizing vehicle motion patterns and spatiotemporal
status. Finally, we use post-processing to fine-tune the temporal boundary of
anomalies. Not surprisingly, our proposed framework was ranked $1^{st}$ in the
NVIDIA AI CITY 2021 leaderboard for traffic anomaly detection. The code is
available at: https://github.com/Endeavour10020/AICity2021-Anomaly-Detection .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yuxiang Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1"&gt;Wenhao Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1"&gt;Yue He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yingying Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1"&gt;Xiao Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Shifeng Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-level Knowledge Distillation via Knowledge Alignment and Correlation. (arXiv:2012.00573v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.00573</id>
        <link href="http://arxiv.org/abs/2012.00573"/>
        <updated>2021-06-07T03:06:13.351Z</updated>
        <summary type="html"><![CDATA[Knowledge distillation (KD) has become an important technique for model
compression and knowledge transfer. In this work, we first perform a
comprehensive analysis of the knowledge transferred by different KD methods. We
demonstrate that traditional KD methods, which minimize the KL divergence of
softmax outputs between networks, are related to the knowledge alignment of an
individual sample only. Meanwhile, recent contrastive learning-based KD methods
mainly transfer relational knowledge between different samples, namely,
knowledge correlation. While it is important to transfer the full knowledge
from teacher to student, we introduce the Multi-level Knowledge Distillation
(MLKD) by effectively considering both knowledge alignment and correlation.
MLKD is task-agnostic and model-agnostic, and can easily transfer knowledge
from supervised or self-supervised pretrained teachers. We show that MLKD can
improve the reliability and transferability of learned representations.
Experiments demonstrate that MLKD outperforms other state-of-the-art methods on
a large number of experimental settings including different (a) pretraining
strategies (b) network architectures (c) datasets (d) tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ding_F/0/1/0/all/0/1"&gt;Fei Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1"&gt;Hongxin Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krovi_V/0/1/0/all/0/1"&gt;Venkat Krovi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_F/0/1/0/all/0/1"&gt;Feng Luo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Editing Conditional Radiance Fields. (arXiv:2105.06466v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.06466</id>
        <link href="http://arxiv.org/abs/2105.06466"/>
        <updated>2021-06-07T03:06:13.345Z</updated>
        <summary type="html"><![CDATA[A neural radiance field (NeRF) is a scene model supporting high-quality view
synthesis, optimized per scene. In this paper, we explore enabling user editing
of a category-level NeRF - also known as a conditional radiance field - trained
on a shape category. Specifically, we introduce a method for propagating coarse
2D user scribbles to the 3D space, to modify the color or shape of a local
region. First, we propose a conditional radiance field that incorporates new
modular network components, including a shape branch that is shared across
object instances. Observing multiple instances of the same category, our model
learns underlying part semantics without any supervision, thereby allowing the
propagation of coarse 2D user scribbles to the entire 3D region (e.g., chair
seat). Next, we propose a hybrid network update strategy that targets specific
network components, which balances efficiency and accuracy. During user
interaction, we formulate an optimization problem that both satisfies the
user's constraints and preserves the original object structure. We demonstrate
our approach on various editing tasks over three shape datasets and show that
it outperforms prior neural editing approaches. Finally, we edit the appearance
and shape of a real photograph and show that the edit propagates to
extrapolated novel views.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Steven Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiuming Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhoutong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Richard Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jun-Yan Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Russell_B/0/1/0/all/0/1"&gt;Bryan Russell&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Regularizing Deep Networks with Semantic Data Augmentation. (arXiv:2007.10538v5 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.10538</id>
        <link href="http://arxiv.org/abs/2007.10538"/>
        <updated>2021-06-07T03:06:13.323Z</updated>
        <summary type="html"><![CDATA[Data augmentation is widely known as a simple yet surprisingly effective
technique for regularizing deep networks. Conventional data augmentation
schemes, e.g., flipping, translation or rotation, are low-level,
data-independent and class-agnostic operations, leading to limited diversity
for augmented samples. To this end, we propose a novel semantic data
augmentation algorithm to complement traditional approaches. The proposed
method is inspired by the intriguing property that deep networks are effective
in learning linearized features, i.e., certain directions in the deep feature
space correspond to meaningful semantic transformations, e.g., changing the
background or view angle of an object. Based on this observation, translating
training samples along many such directions in the feature space can
effectively augment the dataset for more diversity. To implement this idea, we
first introduce a sampling based method to obtain semantically meaningful
directions efficiently. Then, an upper bound of the expected cross-entropy (CE)
loss on the augmented training set is derived by assuming the number of
augmented samples goes to infinity, yielding a highly efficient algorithm. In
fact, we show that the proposed implicit semantic data augmentation (ISDA)
algorithm amounts to minimizing a novel robust CE loss, which adds minimal
extra computational cost to a normal training procedure. In addition to
supervised learning, ISDA can be applied to semi-supervised learning tasks
under the consistency regularization framework, where ISDA amounts to
minimizing the upper bound of the expected KL-divergence between the augmented
features and the original features. Although being simple, ISDA consistently
improves the generalization performance of popular deep models (e.g., ResNets
and DenseNets) on a variety of datasets, i.e., CIFAR-10, CIFAR-100, SVHN,
ImageNet, and Cityscapes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yulin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1"&gt;Gao Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1"&gt;Shiji Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1"&gt;Xuran Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1"&gt;Yitong Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1"&gt;Cheng Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incomplete Graph Representation and Learning via Partial Graph Neural Networks. (arXiv:2003.10130v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.10130</id>
        <link href="http://arxiv.org/abs/2003.10130"/>
        <updated>2021-06-07T03:06:13.317Z</updated>
        <summary type="html"><![CDATA[Graph Neural Networks (GNNs) are gaining increasing attention on graph data
learning tasks in recent years. However, in many applications, graph may be
coming in an incomplete form where attributes of graph nodes are partially
unknown/missing. Existing GNNs are generally designed on complete graphs which
can not deal with attribute-incomplete graph data directly. To address this
problem, we develop a novel partial aggregation based GNNs, named Partial Graph
Neural Networks (PaGNNs), for attribute-incomplete graph representation and
learning. Our work is motivated by the observation that the neighborhood
aggregation function in standard GNNs can be equivalently viewed as the
neighborhood reconstruction formulation. Based on it, we define two novel
partial aggregation (reconstruction) functions on incomplete graph and derive
PaGNNs for incomplete graph data learning. Extensive experiments on several
datasets demonstrate the effectiveness and efficiency of the proposed PaGNNs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_B/0/1/0/all/0/1"&gt;Bo Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Ziyan Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Improved Attention for Visual Question Answering. (arXiv:2011.02164v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.02164</id>
        <link href="http://arxiv.org/abs/2011.02164"/>
        <updated>2021-06-07T03:06:13.310Z</updated>
        <summary type="html"><![CDATA[We consider the problem of Visual Question Answering (VQA). Given an image
and a free-form, open-ended, question, expressed in natural language, the goal
of VQA system is to provide accurate answer to this question with respect to
the image. The task is challenging because it requires simultaneous and
intricate understanding of both visual and textual information. Attention,
which captures intra- and inter-modal dependencies, has emerged as perhaps the
most widely used mechanism for addressing these challenges. In this paper, we
propose an improved attention-based architecture to solve VQA. We incorporate
an Attention on Attention (AoA) module within encoder-decoder framework, which
is able to determine the relation between attention results and queries.
Attention module generates weighted average for each query. On the other hand,
AoA module first generates an information vector and an attention gate using
attention results and current context; and then adds another attention to
generate final attended information by multiplying the two. We also propose
multimodal fusion module to combine both visual and textual information. The
goal of this fusion module is to dynamically decide how much information should
be considered from each modality. Extensive experiments on VQA-v2 benchmark
dataset show that our method achieves the state-of-the-art performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rahman_T/0/1/0/all/0/1"&gt;Tanzila Rahman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chou_S/0/1/0/all/0/1"&gt;Shih-Han Chou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sigal_L/0/1/0/all/0/1"&gt;Leonid Sigal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carenini_G/0/1/0/all/0/1"&gt;Giuseppe Carenini&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Slice Low-Rank Tensor Decomposition Based Multi-Atlas Segmentation: Application to Automatic Pathological Liver CT Segmentation. (arXiv:2102.12056v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.12056</id>
        <link href="http://arxiv.org/abs/2102.12056"/>
        <updated>2021-06-07T03:06:13.303Z</updated>
        <summary type="html"><![CDATA[Liver segmentation from abdominal CT images is an essential step for liver
cancer computer-aided diagnosis and surgical planning. However, both the
accuracy and robustness of existing liver segmentation methods cannot meet the
requirements of clinical applications. In particular, for the common clinical
cases where the liver tissue contains major pathology, current segmentation
methods show poor performance. In this paper, we propose a novel low-rank
tensor decomposition (LRTD) based multi-atlas segmentation (MAS) framework that
achieves accurate and robust pathological liver segmentation of CT images.
Firstly, we propose a multi-slice LRTD scheme to recover the underlying
low-rank structure embedded in 3D medical images. It performs the LRTD on small
image segments consisting of multiple consecutive image slices. Then, we
present an LRTD-based atlas construction method to generate tumor-free liver
atlases that mitigates the performance degradation of liver segmentation due to
the presence of tumors. Finally, we introduce an LRTD-based MAS algorithm to
derive patient-specific liver atlases for each test image, and to achieve
accurate pairwise image registration and label propagation. Extensive
experiments on three public databases of pathological liver cases validate the
effectiveness of the proposed method. Both qualitative and quantitative results
demonstrate that, in the presence of major pathology, the proposed method is
more accurate and robust than state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Shi_C/0/1/0/all/0/1"&gt;Changfa Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xian_M/0/1/0/all/0/1"&gt;Min Xian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhou_X/0/1/0/all/0/1"&gt;Xiancheng Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_H/0/1/0/all/0/1"&gt;Haotian Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cheng_H/0/1/0/all/0/1"&gt;Heng-Da Cheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-interactive Dual-decoder for RGB-thermal Salient Object Detection. (arXiv:2005.02315v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.02315</id>
        <link href="http://arxiv.org/abs/2005.02315"/>
        <updated>2021-06-07T03:06:13.294Z</updated>
        <summary type="html"><![CDATA[RGB-thermal salient object detection (SOD) aims to segment the common
prominent regions of visible image and corresponding thermal infrared image
that we call it RGBT SOD. Existing methods don't fully explore and exploit the
potentials of complementarity of different modalities and multi-type cues of
image contents, which play a vital role in achieving accurate results. In this
paper, we propose a multi-interactive dual-decoder to mine and model the
multi-type interactions for accurate RGBT SOD. In specific, we first encode two
modalities into multi-level multi-modal feature representations. Then, we
design a novel dual-decoder to conduct the interactions of multi-level
features, two modalities and global contexts. With these interactions, our
method works well in diversely challenging scenarios even in the presence of
invalid modality. Finally, we carry out extensive experiments on public RGBT
and RGBD SOD datasets, and the results show that the proposed method achieves
the outstanding performance against state-of-the-art algorithms. The source
code has been released
at:https://github.com/lz118/Multi-interactive-Dual-decoder.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1"&gt;Zhengzheng Tu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhun Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chenglong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lang_Y/0/1/0/all/0/1"&gt;Yang Lang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1"&gt;Jin Tang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning domain-agnostic visual representation for computational pathology using medically-irrelevant style transfer augmentation. (arXiv:2102.01678v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.01678</id>
        <link href="http://arxiv.org/abs/2102.01678"/>
        <updated>2021-06-07T03:06:13.277Z</updated>
        <summary type="html"><![CDATA[Suboptimal generalization of machine learning models on unseen data is a key
challenge which hampers the clinical applicability of such models to medical
imaging. Although various methods such as domain adaptation and domain
generalization have evolved to combat this challenge, learning robust and
generalizable representations is core to medical image understanding, and
continues to be a problem. Here, we propose STRAP (Style TRansfer Augmentation
for histoPathology), a form of data augmentation based on random style transfer
from non-medical style source such as artistic paintings, for learning
domain-agnostic visual representations in computational pathology. Style
transfer replaces the low-level texture content of an image with the
uninformative style of randomly selected style source image, while preserving
the original high-level semantic content. This improves robustness to domain
shift and can be used as a simple yet powerful tool for learning
domain-agnostic representations. We demonstrate that STRAP leads to
state-of-the-art performance, particularly in the presence of domain shifts, on
two particular classification tasks in computational pathology.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Yamashita_R/0/1/0/all/0/1"&gt;Rikiya Yamashita&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Long_J/0/1/0/all/0/1"&gt;Jin Long&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Banda_S/0/1/0/all/0/1"&gt;Snikitha Banda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shen_J/0/1/0/all/0/1"&gt;Jeanne Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rubin_D/0/1/0/all/0/1"&gt;Daniel L. Rubin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fusing CNNs and statistical indicators to improve image classification. (arXiv:2012.11049v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.11049</id>
        <link href="http://arxiv.org/abs/2012.11049"/>
        <updated>2021-06-07T03:06:13.271Z</updated>
        <summary type="html"><![CDATA[Convolutional Networks have dominated the field of computer vision for the
last ten years, exhibiting extremely powerful feature extraction capabilities
and outstanding classification performance. The main strategy to prolong this
trend relies on further upscaling networks in size. However, costs increase
rapidly while performance improvements may be marginal. We hypothesise that
adding heterogeneous sources of information may be more cost-effective to a CNN
than building a bigger network. In this paper, an ensemble method is proposed
for accurate image classification, fusing automatically detected features
through Convolutional Neural Network architectures with a set of manually
defined statistical indicators. Through a combination of the predictions of a
CNN and a secondary classifier trained on statistical features, better
classification performance can be cheaply achieved. We test multiple learning
algorithms and CNN architectures on a diverse number of datasets to validate
our proposal, making public all our code and data via GitHub. According to our
results, the inclusion of additional indicators and an ensemble classification
approach helps to increase the performance in 8 of 9 datasets, with a
remarkable increase of more than 10% precision in two of them.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huertas_Tato_J/0/1/0/all/0/1"&gt;Javier Huertas-Tato&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martin_A/0/1/0/all/0/1"&gt;Alejandro Mart&amp;#xed;n&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fierrez_J/0/1/0/all/0/1"&gt;Juli&amp;#xe1;n Fierrez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Camacho_D/0/1/0/all/0/1"&gt;David Camacho&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Prediction or Comparison: Toward Interpretable Qualitative Reasoning. (arXiv:2106.02399v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02399</id>
        <link href="http://arxiv.org/abs/2106.02399"/>
        <updated>2021-06-07T03:06:13.263Z</updated>
        <summary type="html"><![CDATA[Qualitative relationships illustrate how changing one property (e.g., moving
velocity) affects another (e.g., kinetic energy) and constitutes a considerable
portion of textual knowledge. Current approaches use either semantic parsers to
transform natural language inputs into logical expressions or a "black-box"
model to solve them in one step. The former has a limited application range,
while the latter lacks interpretability. In this work, we categorize
qualitative reasoning tasks into two types: prediction and comparison. In
particular, we adopt neural network modules trained in an end-to-end manner to
simulate the two reasoning processes. Experiments on two qualitative reasoning
question answering datasets, QuaRTz and QuaRel, show our methods' effectiveness
and generalization capability, and the intermediate outputs provided by the
modules make the reasoning process interpretable.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ren_M/0/1/0/all/0/1"&gt;Mucheng Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1"&gt;Heyan Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1"&gt;Yang Gao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Image Enhanced Rotation Prediction for Self-Supervised Learning. (arXiv:1912.11603v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1912.11603</id>
        <link href="http://arxiv.org/abs/1912.11603"/>
        <updated>2021-06-07T03:06:13.256Z</updated>
        <summary type="html"><![CDATA[The rotation prediction (Rotation) is a simple pretext-task for
self-supervised learning (SSL), where models learn useful representations for
target vision tasks by solving pretext-tasks. Although Rotation captures
information of object shapes, it hardly captures information of textures. To
tackle this problem, we introduce a novel pretext-task called image enhanced
rotation prediction (IE-Rot) for SSL. IE-Rot simultaneously solves Rotation and
another pretext-task based on image enhancement (e.g., sharpening and
solarizing) while maintaining simplicity. Through the simultaneous prediction
of rotation and image enhancement, models learn representations to capture the
information of not only object shapes but also textures. Our experimental
results show that IE-Rot models outperform Rotation on various standard
benchmarks including ImageNet classification, PASCAL-VOC detection, and COCO
detection/segmentation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Yamaguchi_S/0/1/0/all/0/1"&gt;Shin&amp;#x27;ya Yamaguchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kanai_S/0/1/0/all/0/1"&gt;Sekitoshi Kanai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Shioda_T/0/1/0/all/0/1"&gt;Tetsuya Shioda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Takeda_S/0/1/0/all/0/1"&gt;Shoichiro Takeda&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Prototype Completion with Primitive Knowledge for Few-Shot Learning. (arXiv:2009.04960v5 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.04960</id>
        <link href="http://arxiv.org/abs/2009.04960"/>
        <updated>2021-06-07T03:06:13.249Z</updated>
        <summary type="html"><![CDATA[Few-shot learning is a challenging task, which aims to learn a classifier for
novel classes with few examples. Pre-training based meta-learning methods
effectively tackle the problem by pre-training a feature extractor and then
fine-tuning it through the nearest centroid based meta-learning. However,
results show that the fine-tuning step makes very marginal improvements. In
this paper, 1) we figure out the key reason, i.e., in the pre-trained feature
space, the base classes already form compact clusters while novel classes
spread as groups with large variances, which implies that fine-tuning the
feature extractor is less meaningful; 2) instead of fine-tuning the feature
extractor, we focus on estimating more representative prototypes during
meta-learning. Consequently, we propose a novel prototype completion based
meta-learning framework. This framework first introduces primitive knowledge
(i.e., class-level part or attribute annotations) and extracts representative
attribute features as priors. Then, we design a prototype completion network to
learn to complete prototypes with these priors. To avoid the prototype
completion error caused by primitive knowledge noises or class differences, we
further develop a Gaussian based prototype fusion strategy that combines the
mean-based and completed prototypes by exploiting the unlabeled samples.
Extensive experiments show that our method: (i) can obtain more accurate
prototypes; (ii) outperforms state-of-the-art techniques by 2% - 9% in terms of
classification accuracy. Our code is available online.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1"&gt;Baoquan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xutao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1"&gt;Yunming Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zhichao Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lisai Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[F-Drop&Match: GANs with a Dead Zone in the High-Frequency Domain. (arXiv:2106.02343v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02343</id>
        <link href="http://arxiv.org/abs/2106.02343"/>
        <updated>2021-06-07T03:06:13.231Z</updated>
        <summary type="html"><![CDATA[Generative adversarial networks built from deep convolutional neural networks
(GANs) lack the ability to exactly replicate the high-frequency components of
natural images. To alleviate this issue, we introduce two novel training
techniques called frequency dropping (F-Drop) and frequency matching (F-Match).
The key idea of F-Drop is to filter out unnecessary high-frequency components
from the input images of the discriminators. This simple modification prevents
the discriminators from being confused by perturbations of the high-frequency
components. In addition, F-Drop makes the GANs focus on fitting in the
low-frequency domain, in which there are the dominant components of natural
images. F-Match minimizes the difference between real and fake images in the
frequency domain for generating more realistic images. F-Match is implemented
as a regularization term in the objective functions of the generators; it
penalizes the batch mean error in the frequency domain. F-Match helps the
generators to fit in the high-frequency domain filtered out by F-Drop to the
real image. We experimentally demonstrate that the combination of F-Drop and
F-Match improves the generative performance of GANs in both the frequency and
spatial domain on multiple image benchmarks (CIFAR, TinyImageNet, STL-10,
CelebA, and ImageNet).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yamaguchi_S/0/1/0/all/0/1"&gt;Shin&amp;#x27;ya Yamaguchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kanai_S/0/1/0/all/0/1"&gt;Sekitoshi Kanai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RGBT Tracking via Multi-Adapter Network with Hierarchical Divergence Loss. (arXiv:2011.07189v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.07189</id>
        <link href="http://arxiv.org/abs/2011.07189"/>
        <updated>2021-06-07T03:06:13.225Z</updated>
        <summary type="html"><![CDATA[RGBT tracking has attracted increasing attention since RGB and thermal
infrared data have strong complementary advantages, which could make trackers
all-day and all-weather work. However, how to effectively represent RGBT data
for visual tracking remains unstudied well. Existing works usually focus on
extracting modality-shared or modality-specific information, but the potentials
of these two cues are not well explored and exploited in RGBT tracking. In this
paper, we propose a novel multi-adapter network to jointly perform
modality-shared, modality-specific and instance-aware target representation
learning for RGBT tracking. To this end, we design three kinds of adapters
within an end-to-end deep learning framework. In specific, we use the modified
VGG-M as the generality adapter to extract the modality-shared target
representations.To extract the modality-specific features while reducing the
computational complexity, we design a modality adapter, which adds a small
block to the generality adapter in each layer and each modality in a parallel
manner. Such a design could learn multilevel modality-specific representations
with a modest number of parameters as the vast majority of parameters are
shared with the generality adapter. We also design instance adapter to capture
the appearance properties and temporal variations of a certain target.
Moreover, to enhance the shared and specific features, we employ the loss of
multiple kernel maximum mean discrepancy to measure the distribution divergence
of different modal features and integrate it into each layer for more robust
representation learning. Extensive experiments on two RGBT tracking benchmark
datasets demonstrate the outstanding performance of the proposed tracker
against the state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lu_A/0/1/0/all/0/1"&gt;Andong Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chenglong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1"&gt;Yuqing Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1"&gt;Jin Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_B/0/1/0/all/0/1"&gt;Bin Luo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reactive Human-to-Robot Handovers of Arbitrary Objects. (arXiv:2011.08961v2 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.08961</id>
        <link href="http://arxiv.org/abs/2011.08961"/>
        <updated>2021-06-07T03:06:13.218Z</updated>
        <summary type="html"><![CDATA[Human-robot object handovers have been an actively studied area of robotics
over the past decade; however, very few techniques and systems have addressed
the challenge of handing over diverse objects with arbitrary appearance, size,
shape, and rigidity. In this paper, we present a vision-based system that
enables reactive human-to-robot handovers of unknown objects. Our approach
combines closed-loop motion planning with real-time, temporally-consistent
grasp generation to ensure reactivity and motion smoothness. Our system is
robust to different object positions and orientations, and can grasp both rigid
and non-rigid objects. We demonstrate the generalizability, usability, and
robustness of our approach on a novel benchmark set of 26 diverse household
objects, a user study with naive users (N=6) handing over a subset of 15
objects, and a systematic evaluation examining different ways of handing
objects. More results and videos can be found at
https://sites.google.com/nvidia.com/handovers-of-arbitrary-objects.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1"&gt;Wei Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paxton_C/0/1/0/all/0/1"&gt;Chris Paxton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mousavian_A/0/1/0/all/0/1"&gt;Arsalan Mousavian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chao_Y/0/1/0/all/0/1"&gt;Yu-Wei Chao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cakmak_M/0/1/0/all/0/1"&gt;Maya Cakmak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fox_D/0/1/0/all/0/1"&gt;Dieter Fox&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Light Field Networks: Neural Scene Representations with Single-Evaluation Rendering. (arXiv:2106.02634v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02634</id>
        <link href="http://arxiv.org/abs/2106.02634"/>
        <updated>2021-06-07T03:06:13.212Z</updated>
        <summary type="html"><![CDATA[Inferring representations of 3D scenes from 2D observations is a fundamental
problem of computer graphics, computer vision, and artificial intelligence.
Emerging 3D-structured neural scene representations are a promising approach to
3D scene understanding. In this work, we propose a novel neural scene
representation, Light Field Networks or LFNs, which represent both geometry and
appearance of the underlying 3D scene in a 360-degree, four-dimensional light
field parameterized via a neural implicit representation. Rendering a ray from
an LFN requires only a *single* network evaluation, as opposed to hundreds of
evaluations per ray for ray-marching or volumetric based renderers in
3D-structured neural scene representations. In the setting of simple scenes, we
leverage meta-learning to learn a prior over LFNs that enables multi-view
consistent light field reconstruction from as little as a single image
observation. This results in dramatic reductions in time and memory complexity,
and enables real-time rendering. The cost of storing a 360-degree light field
via an LFN is two orders of magnitude lower than conventional methods such as
the Lumigraph. Utilizing the analytical differentiability of neural implicit
representations and a novel parameterization of light space, we further
demonstrate the extraction of sparse depth maps from LFNs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sitzmann_V/0/1/0/all/0/1"&gt;Vincent Sitzmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rezchikov_S/0/1/0/all/0/1"&gt;Semon Rezchikov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Freeman_W/0/1/0/all/0/1"&gt;William T. Freeman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1"&gt;Joshua B. Tenenbaum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Durand_F/0/1/0/all/0/1"&gt;Fredo Durand&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SkeletonNet: A Topology-Preserving Solution for Learning Mesh Reconstruction of Object Surfaces from RGB Images. (arXiv:2008.05742v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.05742</id>
        <link href="http://arxiv.org/abs/2008.05742"/>
        <updated>2021-06-07T03:06:13.206Z</updated>
        <summary type="html"><![CDATA[This paper focuses on the challenging task of learning 3D object surface
reconstructions from RGB images. Existingmethods achieve varying degrees of
success by using different surface representations. However, they all have
their own drawbacks,and cannot properly reconstruct the surface shapes of
complex topologies, arguably due to a lack of constraints on the
topologicalstructures in their learning frameworks. To this end, we propose to
learn and use the topology-preserved, skeletal shape representationto assist
the downstream task of object surface reconstruction from RGB images.
Technically, we propose the novelSkeletonNetdesign that learns a volumetric
representation of a skeleton via a bridged learning of a skeletal point set,
where we use paralleldecoders each responsible for the learning of points on 1D
skeletal curves and 2D skeletal sheets, as well as an efficient module
ofglobally guided subvolume synthesis for a refined, high-resolution skeletal
volume; we present a differentiablePoint2Voxellayer tomake SkeletonNet
end-to-end and trainable. With the learned skeletal volumes, we propose two
models, the Skeleton-Based GraphConvolutional Neural Network (SkeGCNN) and the
Skeleton-Regularized Deep Implicit Surface Network (SkeDISN), which
respectivelybuild upon and improve over the existing frameworks of explicit
mesh deformation and implicit field learning for the downstream
surfacereconstruction task. We conduct thorough experiments that verify the
efficacy of our proposed SkeletonNet. SkeGCNN and SkeDISNoutperform existing
methods as well, and they have their own merits when measured by different
metrics. Additional results ingeneralized task settings further demonstrate the
usefulness of our proposed methods. We have made both our implementation
codeand the ShapeNet-Skeleton dataset publicly available at ble at
https://github.com/tangjiapeng/SkeletonNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1"&gt;Jiapeng Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1"&gt;Xiaoguang Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1"&gt;Mingkui Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tong_X/0/1/0/all/0/1"&gt;Xin Tong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_K/0/1/0/all/0/1"&gt;Kui Jia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Semi-Supervised Gross Target Volume of Nasopharyngeal Carcinoma Segmentation via Uncertainty Rectified Pyramid Consistency. (arXiv:2012.07042v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.07042</id>
        <link href="http://arxiv.org/abs/2012.07042"/>
        <updated>2021-06-07T03:06:13.197Z</updated>
        <summary type="html"><![CDATA[Gross Target Volume (GTV) segmentation plays an irreplaceable role in
radiotherapy planning for Nasopharyngeal Carcinoma (NPC). Despite that
Convolutional Neural Networks (CNN) have achieved good performance for this
task, they rely on a large set of labeled images for training, which is
expensive and time-consuming to acquire. In this paper, we propose a novel
framework with Uncertainty Rectified Pyramid Consistency (URPC) regularization
for semi-supervised NPC GTV segmentation. Concretely, we extend a backbone
segmentation network to produce pyramid predictions at different scales. The
pyramid predictions network (PPNet) is supervised by the ground truth of
labeled images and a multi-scale consistency loss for unlabeled images,
motivated by the fact that prediction at different scales for the same input
should be similar and consistent. However, due to the different resolution of
these predictions, encouraging them to be consistent at each pixel directly has
low robustness and may lose some fine details. To address this problem, we
further design a novel uncertainty rectifying module to enable the framework to
gradually learn from meaningful and reliable consensual regions at different
scales. Experimental results on a dataset with 258 NPC MR images showed that
with only 10% or 20% images labeled, our method largely improved the
segmentation performance by leveraging the unlabeled images, and it also
outperformed five state-of-the-art semi-supervised segmentation methods.
Moreover, when only 50% images labeled, URPC achieved an average Dice score of
82.74% that was close to fully supervised learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1"&gt;Xiangde Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liao_W/0/1/0/all/0/1"&gt;Wenjun Liao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jieneng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_T/0/1/0/all/0/1"&gt;Tao Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yinan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shichuan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1"&gt;Nianyong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1"&gt;Guotai Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shaoting Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Regressive Domain Adaptation for Unsupervised Keypoint Detection. (arXiv:2103.06175v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.06175</id>
        <link href="http://arxiv.org/abs/2103.06175"/>
        <updated>2021-06-07T03:06:13.180Z</updated>
        <summary type="html"><![CDATA[Domain adaptation (DA) aims at transferring knowledge from a labeled source
domain to an unlabeled target domain. Though many DA theories and algorithms
have been proposed, most of them are tailored into classification settings and
may fail in regression tasks, especially in the practical keypoint detection
task. To tackle this difficult but significant task, we present a method of
regressive domain adaptation (RegDA) for unsupervised keypoint detection.
Inspired by the latest theoretical work, we first utilize an adversarial
regressor to maximize the disparity on the target domain and train a feature
generator to minimize this disparity. However, due to the high dimension of the
output space, this regressor fails to detect samples that deviate from the
support of the source. To overcome this problem, we propose two important
ideas. First, based on our observation that the probability density of the
output space is sparse, we introduce a spatial probability distribution to
describe this sparsity and then use it to guide the learning of the adversarial
regressor. Second, to alleviate the optimization difficulty in the
high-dimensional space, we innovatively convert the minimax game in the
adversarial training to the minimization of two opposite goals. Extensive
experiments show that our method brings large improvement by 8% to 11% in terms
of PCK on different datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1"&gt;Junguang Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1"&gt;Yifei Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Ximei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yufeng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jianmin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Long_M/0/1/0/all/0/1"&gt;Mingsheng Long&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sign-Agnostic Implicit Learning of Surface Self-Similarities for Shape Modeling and Reconstruction from Raw Point Clouds. (arXiv:2012.07498v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.07498</id>
        <link href="http://arxiv.org/abs/2012.07498"/>
        <updated>2021-06-07T03:06:13.173Z</updated>
        <summary type="html"><![CDATA[Shape modeling and reconstruction from raw point clouds of objects stand as a
fundamental challenge in vision and graphics research. Classical methods
consider analytic shape priors; however, their performance degraded when the
scanned points deviate from the ideal conditions of cleanness and completeness.
Important progress has been recently made by data-driven approaches, which
learn global and/or local models of implicit surface representations from
auxiliary sets of training shapes. Motivated from a universal phenomenon that
self-similar shape patterns of local surface patches repeat across the entire
surface of an object, we aim to push forward the data-driven strategies and
propose to learn a local implicit surface network for a shared, adaptive
modeling of the entire surface for a direct surface reconstruction from raw
point cloud; we also enhance the leveraging of surface self-similarities by
improving correlations among the optimized latent codes of individual surface
patches. Given that orientations of raw points could be unavailable or noisy,
we extend sign agnostic learning into our local implicit model, which enables
our recovery of signed implicit fields of local surfaces from the unsigned
inputs. We term our framework as Sign-Agnostic Implicit Learning of Surface
Self-Similarities (SAIL-S3). With a global post-optimization of local sign
flipping, SAIL-S3 is able to directly model raw, un-oriented point clouds and
reconstruct high-quality object surfaces. Experiments show its superiority over
existing methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1"&gt;Wenbin Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1"&gt;Jiabao Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1"&gt;Yuxin Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jianguo Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_K/0/1/0/all/0/1"&gt;Kui Jia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Effective Training of Convolutional Neural Networks with Low-bitwidth Weights and Activations. (arXiv:1908.04680v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1908.04680</id>
        <link href="http://arxiv.org/abs/1908.04680"/>
        <updated>2021-06-07T03:06:13.167Z</updated>
        <summary type="html"><![CDATA[This paper tackles the problem of training a deep convolutional neural
network of both low-bitwidth weights and activations. Optimizing a
low-precision network is very challenging due to the non-differentiability of
the quantizer, which may result in substantial accuracy loss. To address this,
we propose three practical approaches, including (i) progressive quantization;
(ii) stochastic precision; and (iii) joint knowledge distillation to improve
the network training. First, for progressive quantization, we propose two
schemes to progressively find good local minima. Specifically, we propose to
first optimize a net with quantized weights and subsequently quantize
activations. This is in contrast to the traditional methods which optimize them
simultaneously. Furthermore, we propose a second progressive quantization
scheme which gradually decreases the bit-width from high-precision to
low-precision during training. Second, to alleviate the excessive training
burden due to the multi-round training stages, we further propose a one-stage
stochastic precision strategy to randomly sample and quantize sub-networks
while keeping other parts in full-precision. Finally, we adopt a novel learning
scheme to jointly train a full-precision model alongside the low-precision one.
By doing so, the full-precision model provides hints to guide the low-precision
model training and significantly improves the performance of the low-precision
network. Extensive experiments on various datasets (e.g., CIFAR-100, ImageNet)
show the effectiveness of the proposed methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhuang_B/0/1/0/all/0/1"&gt;Bohan Zhuang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jing Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1"&gt;Mingkui Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Lingqiao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reid_I/0/1/0/all/0/1"&gt;Ian Reid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1"&gt;Chunhua Shen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Signed Distance Function Computation from an Implicit Surface. (arXiv:2104.08057v2 [cs.GR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.08057</id>
        <link href="http://arxiv.org/abs/2104.08057"/>
        <updated>2021-06-07T03:06:13.160Z</updated>
        <summary type="html"><![CDATA[We describe in this short note a technique to convert an implicit surface
into a Signed Distance Function (SDF) while exactly preserving the zero
level-set of the implicit. The proposed approach relies on embedding the input
implicit in the final layer of a neural network, which is trained to minimize a
loss function characterizing the SDF.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fayolle_P/0/1/0/all/0/1"&gt;Pierre-Alain Fayolle&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Weakly- and Semi-Supervised Probabilistic Segmentation and Quantification of Ultrasound Needle-Reverberation Artifacts to Allow Better AI Understanding of Tissue Beneath Needles. (arXiv:2011.11958v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.11958</id>
        <link href="http://arxiv.org/abs/2011.11958"/>
        <updated>2021-06-07T03:06:13.154Z</updated>
        <summary type="html"><![CDATA[Ultrasound image quality has continually been improving. However, when
needles or other metallic objects are operating inside the tissue, the
resulting reverberation artifacts can severely corrupt the surrounding image
quality. Such effects are challenging for existing computer vision algorithms
for medical image analysis. Needle reverberation artifacts can be hard to
identify at times and affect various pixel values to different degrees. The
boundaries of such artifacts are ambiguous, leading to disagreement among human
experts labeling the artifacts. We propose a weakly- and semi-supervised,
probabilistic needle-and-reverberation-artifact segmentation algorithm to
separate the desired tissue-based pixel values from the superimposed artifacts.
Our method models the intensity decay of artifact intensities and is designed
to minimize the human labeling error. We demonstrate the applicability of the
approach and compare it against other segmentation algorithms. Our method is
capable of differentiating between the reverberations from artifact-free
patches as well as of modeling the intensity fall-off in the artifacts. Our
method matches state-of-the-art artifact segmentation performance and sets a
new standard in estimating the per-pixel contributions of artifact vs
underlying anatomy, especially in the immediately adjacent regions between
reverberation lines. Our algorithm is also able to improve the performance
downstream image analysis algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Hung_A/0/1/0/all/0/1"&gt;Alex Ling Yu Hung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chen_E/0/1/0/all/0/1"&gt;Edward Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Galeotti_J/0/1/0/all/0/1"&gt;John Galeotti&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Online Descriptor Enhancement via Self-Labelling Triplets for Visual Data Association. (arXiv:2011.10471v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.10471</id>
        <link href="http://arxiv.org/abs/2011.10471"/>
        <updated>2021-06-07T03:06:13.136Z</updated>
        <summary type="html"><![CDATA[Object-level data association is central to robotic applications such as
tracking-by-detection and object-level simultaneous localization and mapping.
While current learned visual data association methods outperform hand-crafted
algorithms, many rely on large collections of domain-specific training examples
that can be difficult to obtain without prior knowledge. Additionally, such
methods often remain fixed during inference-time and do not harness observed
information to better their performance. We propose a self-supervised method
for incrementally refining visual descriptors to improve performance in the
task of object-level visual data association. Our method optimizes deep
descriptor generators online, by continuously training a widely available image
classification network pre-trained with domain-independent data. We show that
earlier layers in the network outperform later-stage layers for the data
association task while also allowing for a 94% reduction in the number of
parameters, enabling the online optimization. We show that self-labelling
challenging triplets--choosing positive examples separated by large temporal
distances and negative examples close in the descriptor space--improves the
quality of the learned descriptors for the multi-object tracking task. Finally,
we demonstrate that our approach surpasses other visual data-association
methods applied to a tracking-by-detection task, and show that it provides
better performance-gains when compared to other methods that attempt to adapt
to observed information.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shaoul_Y/0/1/0/all/0/1"&gt;Yorai Shaoul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1"&gt;Katherine Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ok_K/0/1/0/all/0/1"&gt;Kyel Ok&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roy_N/0/1/0/all/0/1"&gt;Nicholas Roy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeepActsNet: Spatial and Motion features from Face, Hands, and Body Combined with Convolutional and Graph Networks for Improved Action Recognition. (arXiv:2009.09818v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.09818</id>
        <link href="http://arxiv.org/abs/2009.09818"/>
        <updated>2021-06-07T03:06:13.130Z</updated>
        <summary type="html"><![CDATA[Existing action recognition methods mainly focus on joint and bone
information in human body skeleton data due to its robustness to complex
backgrounds and dynamic characteristics of the environments. In this paper, we
combine body skeleton data with spatial and motion features from face and two
hands, and present "Deep Action Stamps (DeepActs)", a novel data representation
to encode actions from video sequences. We also present "DeepActsNet", a deep
learning based ensemble model which learns convolutional and structural
features from Deep Action Stamps for highly accurate action recognition.
Experiments on three challenging action recognition datasets (NTU60, NTU120,
and SYSU) show that the proposed model trained using Deep Action Stamps produce
considerable improvements in the action recognition accuracy with less
computational cost compared to the state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Asif_U/0/1/0/all/0/1"&gt;Umar Asif&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mehta_D/0/1/0/all/0/1"&gt;Deval Mehta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cavallar_S/0/1/0/all/0/1"&gt;Stefan von Cavallar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1"&gt;Jianbin Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Harrer_S/0/1/0/all/0/1"&gt;Stefan Harrer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MANTRA: Memory Augmented Networks for Multiple Trajectory Prediction. (arXiv:2006.03340v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.03340</id>
        <link href="http://arxiv.org/abs/2006.03340"/>
        <updated>2021-06-07T03:06:13.124Z</updated>
        <summary type="html"><![CDATA[Autonomous vehicles are expected to drive in complex scenarios with several
independent non cooperating agents. Path planning for safely navigating in such
environments can not just rely on perceiving present location and motion of
other agents. It requires instead to predict such variables in a far enough
future. In this paper we address the problem of multimodal trajectory
prediction exploiting a Memory Augmented Neural Network. Our method learns past
and future trajectory embeddings using recurrent neural networks and exploits
an associative external memory to store and retrieve such embeddings.
Trajectory prediction is then performed by decoding in-memory future encodings
conditioned with the observed past. We incorporate scene knowledge in the
decoding state by learning a CNN on top of semantic scene maps. Memory growth
is limited by learning a writing controller based on the predictive capability
of existing embeddings. We show that our method is able to natively perform
multi-modal trajectory prediction obtaining state-of-the art results on three
datasets. Moreover, thanks to the non-parametric nature of the memory module,
we show how once trained our system can continuously improve by ingesting novel
patterns.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Marchetti_F/0/1/0/all/0/1"&gt;Francesco Marchetti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Becattini_F/0/1/0/all/0/1"&gt;Federico Becattini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seidenari_L/0/1/0/all/0/1"&gt;Lorenzo Seidenari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bimbo_A/0/1/0/all/0/1"&gt;Alberto Del Bimbo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Supervised Learning of Domain Invariant Features for Depth Estimation. (arXiv:2106.02594v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02594</id>
        <link href="http://arxiv.org/abs/2106.02594"/>
        <updated>2021-06-07T03:06:13.117Z</updated>
        <summary type="html"><![CDATA[We tackle the problem of unsupervised synthetic-to-realistic domain
adaptation for single image depth estimation. An essential building block of
single image depth estimation is an encoder-decoder task network that takes RGB
images as input and produces depth maps as output. In this paper, we propose a
novel training strategy to force the task network to learn domain invariant
representations in a self-supervised manner. Specifically, we extend
self-supervised learning from traditional representation learning, which works
on images from a single domain, to domain invariant representation learning,
which works on images from two different domains by utilizing an image-to-image
translation network. Firstly, we use our bidirectional image-to-image
translation network to transfer domain-specific styles between synthetic and
real domains. This style transfer operation allows us to obtain similar images
from the different domains. Secondly, we jointly train our task network and
Siamese network with the same images from the different domains to obtain
domain invariance for the task network. Finally, we fine-tune the task network
using labeled synthetic and unlabeled real-world data. Our training strategy
yields improved generalization capability in the real-world domain. We carry
out an extensive evaluation on two popular datasets for depth estimation, KITTI
and Make3D. The results demonstrate that our proposed method outperforms the
state-of-the-art both qualitatively and quantitatively. The source code and
model weights will be made available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Akada_H/0/1/0/all/0/1"&gt;Hiroyasu Akada&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhat_S/0/1/0/all/0/1"&gt;Shariq Farooq Bhat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alhashim_I/0/1/0/all/0/1"&gt;Ibraheem Alhashim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wonka_P/0/1/0/all/0/1"&gt;Peter Wonka&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improve the Interpretability of Attention: A Fast, Accurate, and Interpretable High-Resolution Attention Model. (arXiv:2106.02566v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02566</id>
        <link href="http://arxiv.org/abs/2106.02566"/>
        <updated>2021-06-07T03:06:13.109Z</updated>
        <summary type="html"><![CDATA[The prevalence of employing attention mechanisms has brought along concerns
on the interpretability of attention distributions. Although it provides
insights about how a model is operating, utilizing attention as the explanation
of model predictions is still highly dubious. The community is still seeking
more interpretable strategies for better identifying local active regions that
contribute the most to the final decision. To improve the interpretability of
existing attention models, we propose a novel Bilinear Representative
Non-Parametric Attention (BR-NPA) strategy that captures the task-relevant
human-interpretable information. The target model is first distilled to have
higher-resolution intermediate feature maps. From which, representative
features are then grouped based on local pairwise feature similarity, to
produce finer-grained, more precise attention maps highlighting task-relevant
parts of the input. The obtained attention maps are ranked according to the
`active level' of the compound feature, which provides information regarding
the important level of the highlighted regions. The proposed model can be
easily adapted in a wide variety of modern deep models, where classification is
involved. It is also more accurate, faster, and with a smaller memory footprint
than usual neural attention modules. Extensive experiments showcase more
comprehensive visual explanations compared to the state-of-the-art
visualization model across multiple tasks including few-shot classification,
person re-identification, fine-grained image classification. The proposed
visualization model sheds imperative light on how neural networks `pay their
attention' differently in different tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gomez_T/0/1/0/all/0/1"&gt;Tristan Gomez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ling_S/0/1/0/all/0/1"&gt;Suiyi Ling&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Freour_T/0/1/0/all/0/1"&gt;Thomas Fr&amp;#xe9;our&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mouchere_H/0/1/0/all/0/1"&gt;Harold Mouch&amp;#xe8;re&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Embedded Deep Regularized Block HSIC Thermomics for Early Diagnosis of Breast Cancer. (arXiv:2106.02106v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.02106</id>
        <link href="http://arxiv.org/abs/2106.02106"/>
        <updated>2021-06-07T03:06:13.084Z</updated>
        <summary type="html"><![CDATA[Thermography has been used extensively as a complementary diagnostic tool in
breast cancer detection. Among thermographic methods matrix factorization (MF)
techniques show an unequivocal capability to detect thermal patterns
corresponding to vasodilation in cancer cases. One of the biggest challenges in
such techniques is selecting the best representation of the thermal basis. In
this study, an embedding method is proposed to address this problem and
Deep-semi-nonnegative matrix factorization (Deep-SemiNMF) for thermography is
introduced, then tested for 208 breast cancer screening cases. First, we apply
Deep-SemiNMF to infrared images to extract low-rank thermal representations for
each case. Then, we embed low-rank bases to obtain one basis for each patient.
After that, we extract 300 thermal imaging features, called thermomics, to
decode imaging information for the automatic diagnostic model. We reduced the
dimensionality of thermomics by spanning them onto Hilbert space using RBF
kernel and select the three most efficient features using the block Hilbert
Schmidt Independence Criterion Lasso (block HSIC Lasso). The preserved thermal
heterogeneity successfully classified asymptomatic versus symptomatic patients
applying a random forest model (cross-validated accuracy of 71.36%
(69.42%-73.3%)).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Yousefi_B/0/1/0/all/0/1"&gt;Bardia Yousefi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sharifipour_H/0/1/0/all/0/1"&gt;Hossein Memarzadeh Sharifipour&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Maldague_X/0/1/0/all/0/1"&gt;Xavier P.V. Maldague&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Learning Methods for Vessel Trajectory Prediction based on Recurrent Neural Networks. (arXiv:2101.02486v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.02486</id>
        <link href="http://arxiv.org/abs/2101.02486"/>
        <updated>2021-06-07T03:06:13.075Z</updated>
        <summary type="html"><![CDATA[Data-driven methods open up unprecedented possibilities for maritime
surveillance using Automatic Identification System (AIS) data. In this work, we
explore deep learning strategies using historical AIS observations to address
the problem of predicting future vessel trajectories with a prediction horizon
of several hours. We propose novel sequence-to-sequence vessel trajectory
prediction models based on encoder-decoder recurrent neural networks (RNNs)
that are trained on historical trajectory data to predict future trajectory
samples given previous observations. The proposed architecture combines Long
Short-Term Memory (LSTM) RNNs for sequence modeling to encode the observed data
and generate future predictions with different intermediate aggregation layers
to capture space-time dependencies in sequential data. Experimental results on
vessel trajectories from an AIS dataset made freely available by the Danish
Maritime Authority show the effectiveness of deep-learning methods for
trajectory prediction based on sequence-to-sequence neural networks, which
achieve better performance than baseline approaches based on linear regression
or on the Multi-Layer Perceptron (MLP) architecture. The comparative evaluation
of results shows: i) the superiority of attention pooling over static pooling
for the specific application, and ii) the remarkable performance improvement
that can be obtained with labeled trajectories, i.e., when predictions are
conditioned on a low-level context representation encoded from the sequence of
past observations, as well as on additional inputs (e.g., port of departure or
arrival) about the vessel's high-level intention, which may be available from
AIS.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Capobianco_S/0/1/0/all/0/1"&gt;Samuele Capobianco&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Millefiori_L/0/1/0/all/0/1"&gt;Leonardo M. Millefiori&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Forti_N/0/1/0/all/0/1"&gt;Nicola Forti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Braca_P/0/1/0/all/0/1"&gt;Paolo Braca&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Willett_P/0/1/0/all/0/1"&gt;Peter Willett&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Approximate Fixed-Points in Recurrent Neural Networks. (arXiv:2106.02417v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2106.02417</id>
        <link href="http://arxiv.org/abs/2106.02417"/>
        <updated>2021-06-07T03:06:13.067Z</updated>
        <summary type="html"><![CDATA[Recurrent neural networks are widely used in speech and language processing.
Due to dependency on the past, standard algorithms for training these models,
such as back-propagation through time (BPTT), cannot be efficiently
parallelised. Furthermore, applying these models to more complex structures
than sequences requires inference time approximations, which introduce
inconsistency between inference and training. This paper shows that recurrent
neural networks can be reformulated as fixed-points of non-linear equation
systems. These fixed-points can be computed using an iterative algorithm
exactly and in as many iterations as the length of any given sequence. Each
iteration of this algorithm adds one additional Markovian-like order of
dependencies such that upon termination all dependencies modelled by the
recurrent neural networks have been incorporated. Although exact fixed-points
inherit the same parallelization and inconsistency issues, this paper shows
that approximate fixed-points can be computed in parallel and used consistently
in training and inference including tasks such as lattice rescoring.
Experimental validation is performed in two tasks, Penn Tree Bank and
WikiText-2, and shows that approximate fixed-points yield competitive
prediction performance to recurrent neural networks trained using the BPTT
algorithm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhengxiong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ragni_A/0/1/0/all/0/1"&gt;Anton Ragni&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Survey on Deep Domain Adaptation for LiDAR Perception. (arXiv:2106.02377v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02377</id>
        <link href="http://arxiv.org/abs/2106.02377"/>
        <updated>2021-06-07T03:06:13.061Z</updated>
        <summary type="html"><![CDATA[Scalable systems for automated driving have to reliably cope with an
open-world setting. This means, the perception systems are exposed to drastic
domain shifts, like changes in weather conditions, time-dependent aspects, or
geographic regions. Covering all domains with annotated data is impossible
because of the endless variations of domains and the time-consuming and
expensive annotation process. Furthermore, fast development cycles of the
system additionally introduce hardware changes, such as sensor types and
vehicle setups, and the required knowledge transfer from simulation. To enable
scalable automated driving, it is therefore crucial to address these domain
shifts in a robust and efficient manner. Over the last years, a vast amount of
different domain adaptation techniques evolved. There already exists a number
of survey papers for domain adaptation on camera images, however, a survey for
LiDAR perception is absent. Nevertheless, LiDAR is a vital sensor for automated
driving that provides detailed 3D scans of the vehicle's surroundings. To
stimulate future research, this paper presents a comprehensive review of recent
progress in domain adaptation methods and formulates interesting research
questions specifically targeted towards LiDAR perception.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Triess_L/0/1/0/all/0/1"&gt;Larissa T. Triess&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dreissig_M/0/1/0/all/0/1"&gt;Mariella Dreissig&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rist_C/0/1/0/all/0/1"&gt;Christoph B. Rist&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zollner_J/0/1/0/all/0/1"&gt;J. Marius Z&amp;#xf6;llner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving the Robustness of QA Models to Challenge Sets with Variational Question-Answer Pair Generation. (arXiv:2004.03238v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.03238</id>
        <link href="http://arxiv.org/abs/2004.03238"/>
        <updated>2021-06-07T03:06:13.054Z</updated>
        <summary type="html"><![CDATA[Question answering (QA) models for reading comprehension have achieved
human-level accuracy on in-distribution test sets. However, they have been
demonstrated to lack robustness to challenge sets, whose distribution is
different from that of training sets. Existing data augmentation methods
mitigate this problem by simply augmenting training sets with synthetic
examples sampled from the same distribution as the challenge sets. However,
these methods assume that the distribution of a challenge set is known a
priori, making them less applicable to unseen challenge sets. In this study, we
focus on question-answer pair generation (QAG) to mitigate this problem. While
most existing QAG methods aim to improve the quality of synthetic examples, we
conjecture that diversity-promoting QAG can mitigate the sparsity of training
sets and lead to better robustness. We present a variational QAG model that
generates multiple diverse QA pairs from a paragraph. Our experiments show that
our method can improve the accuracy of 12 challenge sets, as well as the
in-distribution accuracy. Our code and data are available at
https://github.com/KazutoshiShinoda/VQAG.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shinoda_K/0/1/0/all/0/1"&gt;Kazutoshi Shinoda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sugawara_S/0/1/0/all/0/1"&gt;Saku Sugawara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aizawa_A/0/1/0/all/0/1"&gt;Akiko Aizawa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Prospective Observational Study to Investigate Performance of a Chest X-ray Artificial Intelligence Diagnostic Support Tool Across 12 U.S. Hospitals. (arXiv:2106.02118v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.02118</id>
        <link href="http://arxiv.org/abs/2106.02118"/>
        <updated>2021-06-07T03:06:13.030Z</updated>
        <summary type="html"><![CDATA[Importance: An artificial intelligence (AI)-based model to predict COVID-19
likelihood from chest x-ray (CXR) findings can serve as an important adjunct to
accelerate immediate clinical decision making and improve clinical decision
making. Despite significant efforts, many limitations and biases exist in
previously developed AI diagnostic models for COVID-19. Utilizing a large set
of local and international CXR images, we developed an AI model with high
performance on temporal and external validation.

Conclusions and Relevance: AI-based diagnostic tools may serve as an adjunct,
but not replacement, for clinical decision support of COVID-19 diagnosis, which
largely hinges on exposure history, signs, and symptoms. While AI-based tools
have not yet reached full diagnostic potential in COVID-19, they may still
offer valuable information to clinicians taken into consideration along with
clinical signs and symptoms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Sun_J/0/1/0/all/0/1"&gt;Ju Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Peng_L/0/1/0/all/0/1"&gt;Le Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_T/0/1/0/all/0/1"&gt;Taihui Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Adila_D/0/1/0/all/0/1"&gt;Dyah Adila&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zaiman_Z/0/1/0/all/0/1"&gt;Zach Zaiman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Melton_G/0/1/0/all/0/1"&gt;Genevieve B. Melton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ingraham_N/0/1/0/all/0/1"&gt;Nicholas Ingraham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Murray_E/0/1/0/all/0/1"&gt;Eric Murray&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Boley_D/0/1/0/all/0/1"&gt;Daniel Boley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Switzer_S/0/1/0/all/0/1"&gt;Sean Switzer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Burns_J/0/1/0/all/0/1"&gt;John L. Burns&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Huang_K/0/1/0/all/0/1"&gt;Kun Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Allen_T/0/1/0/all/0/1"&gt;Tadashi Allen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Steenburg_S/0/1/0/all/0/1"&gt;Scott D. Steenburg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gichoya_J/0/1/0/all/0/1"&gt;Judy Wawira Gichoya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kummerfeld_E/0/1/0/all/0/1"&gt;Erich Kummerfeld&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tignanelli_C/0/1/0/all/0/1"&gt;Christopher Tignanelli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Teaching keyword spotters to spot new keywords with limited examples. (arXiv:2106.02443v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2106.02443</id>
        <link href="http://arxiv.org/abs/2106.02443"/>
        <updated>2021-06-07T03:06:13.022Z</updated>
        <summary type="html"><![CDATA[Learning to recognize new keywords with just a few examples is essential for
personalizing keyword spotting (KWS) models to a user's choice of keywords.
However, modern KWS models are typically trained on large datasets and
restricted to a small vocabulary of keywords, limiting their transferability to
a broad range of unseen keywords. Towards easily customizable KWS models, we
present KeySEM (Keyword Speech EMbedding), a speech embedding model pre-trained
on the task of recognizing a large number of keywords. Speech representations
offered by KeySEM are highly effective for learning new keywords from a limited
number of examples. Comparisons with a diverse range of related work across
several datasets show that our method achieves consistently superior
performance with fewer training examples. Although KeySEM was pre-trained only
on English utterances, the performance gains also extend to datasets from four
other languages indicating that KeySEM learns useful representations well
aligned with the task of keyword spotting. Finally, we demonstrate KeySEM's
ability to learn new keywords sequentially without requiring to re-train on
previously learned keywords. Our experimental observations suggest that KeySEM
is well suited to on-device environments where post-deployment learning and
ease of customization are often desirable.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Awasthi_A/0/1/0/all/0/1"&gt;Abhijeet Awasthi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kilgour_K/0/1/0/all/0/1"&gt;Kevin Kilgour&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rom_H/0/1/0/all/0/1"&gt;Hassan Rom&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hidden Backdoors in Human-Centric Language Models. (arXiv:2105.00164v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.00164</id>
        <link href="http://arxiv.org/abs/2105.00164"/>
        <updated>2021-06-07T03:06:13.015Z</updated>
        <summary type="html"><![CDATA[Natural language processing (NLP) systems have been proven to be vulnerable
to backdoor attacks, whereby hidden features (backdoors) are trained into a
language model and may only be activated by specific inputs (called triggers),
to trick the model into producing unexpected behaviors. In this paper, we
create covert and natural triggers for textual backdoor attacks, \textit{hidden
backdoors}, where triggers can fool both modern language models and human
inspection. We deploy our hidden backdoors through two state-of-the-art trigger
embedding methods. The first approach via homograph replacement, embeds the
trigger into deep neural networks through the visual spoofing of lookalike
character replacement. The second approach uses subtle differences between text
generated by language models and real natural text to produce trigger sentences
with correct grammar and high fluency. We demonstrate that the proposed hidden
backdoors can be effective across three downstream security-critical NLP tasks,
representative of modern human-centric NLP systems, including toxic comment
detection, neural machine translation (NMT), and question answering (QA). Our
two hidden backdoor attacks can achieve an Attack Success Rate (ASR) of at
least $97\%$ with an injection rate of only $3\%$ in toxic comment detection,
$95.1\%$ ASR in NMT with less than $0.5\%$ injected data, and finally $91.12\%$
ASR against QA updated with only 27 poisoning data samples on a model
previously trained with 92,024 samples (0.029\%). We are able to demonstrate
the adversary's high success rate of attacks, while maintaining functionality
for regular users, with triggers inconspicuous by the human administrators.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shaofeng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Hui Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_T/0/1/0/all/0/1"&gt;Tian Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1"&gt;Benjamin Zi Hao Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xue_M/0/1/0/all/0/1"&gt;Minhui Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1"&gt;Haojin Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1"&gt;Jialiang Lu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Procedural World Generation Framework for Systematic Evaluation of Continual Learning. (arXiv:2106.02585v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02585</id>
        <link href="http://arxiv.org/abs/2106.02585"/>
        <updated>2021-06-07T03:06:13.009Z</updated>
        <summary type="html"><![CDATA[Several families of continual learning techniques have been proposed to
alleviate catastrophic interference in deep neural network training on
non-stationary data. However, a comprehensive comparison and analysis of
limitations remains largely open due to the inaccessibility to suitable
datasets. Empirical examination not only varies immensely between individual
works, it further currently relies on contrived composition of benchmarks
through subdivision and concatenation of various prevalent static vision
datasets. In this work, our goal is to bridge this gap by introducing a
computer graphics simulation framework that repeatedly renders only upcoming
urban scene fragments in an endless real-time procedural world generation
process. At its core lies a modular parametric generative model with adaptable
generative factors. The latter can be used to flexibly compose data streams,
which significantly facilitates a detailed analysis and allows for effortless
investigation of various continual learning schemes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hess_T/0/1/0/all/0/1"&gt;Timm Hess&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mundt_M/0/1/0/all/0/1"&gt;Martin Mundt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pliushch_I/0/1/0/all/0/1"&gt;Iuliia Pliushch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramesh_V/0/1/0/all/0/1"&gt;Visvanathan Ramesh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BERT based sentiment analysis: A software engineering perspective. (arXiv:2106.02581v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02581</id>
        <link href="http://arxiv.org/abs/2106.02581"/>
        <updated>2021-06-07T03:06:13.003Z</updated>
        <summary type="html"><![CDATA[Sentiment analysis can provide a suitable lead for the tools used in software
engineering along with the API recommendation systems and relevant libraries to
be used. In this context, the existing tools like SentiCR, SentiStrength-SE,
etc. exhibited low f1-scores that completely defeats the purpose of deployment
of such strategies, thereby there is enough scope of performance improvement.
Recent advancements show that transformer based pre-trained models (e.g., BERT,
RoBERTa, ALBERT, etc.) have displayed better results in the text classification
task. Following this context, the present research explores different
BERT-based models to analyze the sentences in GitHub comments, Jira comments,
and Stack Overflow posts. The paper presents three different strategies to
analyse BERT based model for sentiment analysis, where in the first strategy
the BERT based pre-trained models are fine-tuned; in the second strategy an
ensemble model is developed from BERT variants; and in the third strategy a
compressed model (Distil BERT) is used. The experimental results show that the
BERT based ensemble approach and the compressed BERT model attain improvements
by 6-12% over prevailing tools for the F1 measure on all three datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Batra_H/0/1/0/all/0/1"&gt;Himanshu Batra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Punn_N/0/1/0/all/0/1"&gt;Narinder Singh Punn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sonbhadra_S/0/1/0/all/0/1"&gt;Sanjay Kumar Sonbhadra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1"&gt;Sonali Agarwal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MERLOT: Multimodal Neural Script Knowledge Models. (arXiv:2106.02636v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02636</id>
        <link href="http://arxiv.org/abs/2106.02636"/>
        <updated>2021-06-07T03:06:12.981Z</updated>
        <summary type="html"><![CDATA[As humans, we understand events in the visual world contextually, performing
multimodal reasoning across time to make inferences about the past, present,
and future. We introduce MERLOT, a model that learns multimodal script
knowledge by watching millions of YouTube videos with transcribed speech -- in
an entirely label-free, self-supervised manner. By pretraining with a mix of
both frame-level (spatial) and video-level (temporal) objectives, our model not
only learns to match images to temporally corresponding words, but also to
contextualize what is happening globally over time. As a result, MERLOT
exhibits strong out-of-the-box representations of temporal commonsense, and
achieves state-of-the-art performance on 12 different video QA datasets when
finetuned. It also transfers well to the world of static images, allowing
models to reason about the dynamic context behind visual scenes. On Visual
Commonsense Reasoning, MERLOT answers questions correctly with 80.6% accuracy,
outperforming state-of-the-art models of similar size by over 3%, even those
that make heavy use of auxiliary supervised data (like object bounding boxes).

Ablation analyses demonstrate the complementary importance of: 1) training on
videos versus static images; 2) scaling the magnitude and diversity of the
pretraining video corpus; and 3) using diverse objectives that encourage
full-stack multimodal reasoning, from the recognition to cognition level.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zellers_R/0/1/0/all/0/1"&gt;Rowan Zellers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1"&gt;Ximing Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hessel_J/0/1/0/all/0/1"&gt;Jack Hessel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1"&gt;Youngjae Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1"&gt;Jae Sung Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1"&gt;Jize Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Farhadi_A/0/1/0/all/0/1"&gt;Ali Farhadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1"&gt;Yejin Choi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI Driven Road Maintenance Inspection. (arXiv:2106.02567v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02567</id>
        <link href="http://arxiv.org/abs/2106.02567"/>
        <updated>2021-06-07T03:06:12.953Z</updated>
        <summary type="html"><![CDATA[Road infrastructure maintenance inspection is typically a labour-intensive
and critical task to ensure the safety of all the road users. In this work, we
propose a detailed methodology to use state-of-the-art techniques in artificial
intelligence and computer vision to automate a sizeable portion of the
maintenance inspection subtasks and reduce the labour costs. The proposed
methodology uses state-of-the-art computer vision techniques such as object
detection and semantic segmentation to automate inspections on primary road
structures such as the road surface, markings, barriers (guardrails) and
traffic signs. The models are mostly trained on commercially viable datasets
and augmented with proprietary data. We demonstrate that our AI models can not
only automate and scale maintenance inspections on primary road structures but
also result in higher recall compared to traditional manual inspections.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mukherjee_R/0/1/0/all/0/1"&gt;Ratnajit Mukherjee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iqbal_H/0/1/0/all/0/1"&gt;Haris Iqbal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marzban_S/0/1/0/all/0/1"&gt;Shabbir Marzban&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Badar_A/0/1/0/all/0/1"&gt;Ahmed Badar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brouns_T/0/1/0/all/0/1"&gt;Terence Brouns&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gowda_S/0/1/0/all/0/1"&gt;Shruthi Gowda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arani_E/0/1/0/all/0/1"&gt;Elahe Arani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zonooz_B/0/1/0/all/0/1"&gt;Bahram Zonooz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pose and Semantic Map Based Probabilistic Forecast of Vulnerable Road Users' Trajectories. (arXiv:2106.02598v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02598</id>
        <link href="http://arxiv.org/abs/2106.02598"/>
        <updated>2021-06-07T03:06:12.938Z</updated>
        <summary type="html"><![CDATA[In this article, an approach for probabilistic trajectory forecasting of
vulnerable road users (VRUs) is presented, which considers past movements and
the surrounding scene. Past movements are represented by 3D poses reflecting
the posture and movements of individual body parts. The surrounding scene is
modeled in the form of semantic maps showing, e.g., the course of streets,
sidewalks, and the occurrence of obstacles. The forecasts are generated in
grids discretizing the space and in the form of arbitrary discrete probability
distributions. The distributions are evaluated in terms of their reliability,
sharpness, and positional accuracy. We compare our method with an approach that
provides forecasts in the form of Gaussian distributions and discuss the
respective advantages and disadvantages. Thereby, we investigate the impact of
using poses and semantic maps. With a technique called spatial label smoothing,
our approach achieves reliable forecasts. Overall, the poses have a positive
impact on the forecasts. The semantic maps offer the opportunity to adapt the
probability distributions to the individual situation, although at the
considered forecasted time horizon of 2.52 s they play a minor role compared to
the past movements of the VRU. Our method is evaluated on a dataset recorded in
inner-city traffic using a research vehicle. The dataset is made publicly
available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kress_V/0/1/0/all/0/1"&gt;Viktor Kress&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jeske_F/0/1/0/all/0/1"&gt;Fabian Jeske&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zernetsch_S/0/1/0/all/0/1"&gt;Stefan Zernetsch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Doll_K/0/1/0/all/0/1"&gt;Konrad Doll&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sick_B/0/1/0/all/0/1"&gt;Bernhard Sick&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SOUP-GAN: Super-Resolution MRI Using Generative Adversarial Networks. (arXiv:2106.02599v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.02599</id>
        <link href="http://arxiv.org/abs/2106.02599"/>
        <updated>2021-06-07T03:06:12.928Z</updated>
        <summary type="html"><![CDATA[There is a growing demand for high-resolution (HR) medical images in both the
clinical and research applications. Image quality is inevitably traded off with
the acquisition time for better patient comfort, lower examination costs, dose,
and fewer motion-induced artifacts. For many image-based tasks, increasing the
apparent resolution in the perpendicular plane to produce multi-planar
reformats or 3D images is commonly used. Single image super-resolution (SR) is
a promising technique to provide HR images based on unsupervised learning to
increase resolution of a 2D image, but there are few reports on 3D SR. Further,
perceptual loss is proposed in the literature to better capture the textual
details and edges than using pixel-wise loss functions, by comparing the
semantic distances in the high-dimensional feature space of a pre-trained 2D
network (e.g., VGG). However, it is not clear how one should generalize it to
3D medical images, and the attendant implications are still unclear. In this
paper, we propose a framework called SOUP-GAN: Super-resolution Optimized Using
Perceptual-tuned Generative Adversarial Network (GAN), in order to produce
thinner slice (e.g., high resolution in the 'Z' plane) medical images with
anti-aliasing and deblurring. The proposed method outperforms other
conventional resolution-enhancement methods and previous SR work on medical
images upon both qualitative and quantitative comparisons. Specifically, we
examine the model in terms of its generalization for various SR ratios and
imaging modalities. By addressing those limitations, our model shows promise as
a novel 3D SR interpolation technique, providing potential applications in both
clinical and research settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_K/0/1/0/all/0/1"&gt;Kuan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hu_H/0/1/0/all/0/1"&gt;Haoji Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Philbrick_K/0/1/0/all/0/1"&gt;Kenneth Philbrick&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Conte_G/0/1/0/all/0/1"&gt;Gian Marco Conte&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sobek_J/0/1/0/all/0/1"&gt;Joseph D. Sobek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rouzrokh_P/0/1/0/all/0/1"&gt;Pouria Rouzrokh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Erickson_B/0/1/0/all/0/1"&gt;Bradley J. Erickson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ADTrack: Target-Aware Dual Filter Learning for Real-Time Anti-Dark UAV Tracking. (arXiv:2106.02495v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02495</id>
        <link href="http://arxiv.org/abs/2106.02495"/>
        <updated>2021-06-07T03:06:12.845Z</updated>
        <summary type="html"><![CDATA[Prior correlation filter (CF)-based tracking methods for unmanned aerial
vehicles (UAVs) have virtually focused on tracking in the daytime. However,
when the night falls, the trackers will encounter more harsh scenes, which can
easily lead to tracking failure. In this regard, this work proposes a novel
tracker with anti-dark function (ADTrack). The proposed method integrates an
efficient and effective low-light image enhancer into a CF-based tracker.
Besides, a target-aware mask is simultaneously generated by virtue of image
illumination variation. The target-aware mask can be applied to jointly train a
target-focused filter that assists the context filter for robust tracking.
Specifically, ADTrack adopts dual regression, where the context filter and the
target-focused filter restrict each other for dual filter learning. Exhaustive
experiments are conducted on typical dark sceneries benchmark, consisting of 37
typical night sequences from authoritative benchmarks, i.e., UAVDark, and our
newly constructed benchmark UAVDark70. The results have shown that ADTrack
favorably outperforms other state-of-the-art trackers and achieves a real-time
speed of 34 frames/s on a single CPU, greatly extending robust UAV tracking to
night scenes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bowen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1"&gt;Changhong Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_F/0/1/0/all/0/1"&gt;Fangqiang Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1"&gt;Junjie Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_F/0/1/0/all/0/1"&gt;Fuling Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CAFLOW: Conditional Autoregressive Flows. (arXiv:2106.02531v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02531</id>
        <link href="http://arxiv.org/abs/2106.02531"/>
        <updated>2021-06-07T03:06:12.818Z</updated>
        <summary type="html"><![CDATA[We introduce CAFLOW, a new diverse image-to-image translation model that
simultaneously leverages the power of auto-regressive modeling and the modeling
efficiency of conditional normalizing flows. We transform the conditioning
image into a sequence of latent encodings using a multi-scale normalizing flow
and repeat the process for the conditioned image. We model the conditional
distribution of the latent encodings by modeling the auto-regressive
distributions with an efficient multi-scale normalizing flow, where each
conditioning factor affects image synthesis at its respective resolution scale.
Our proposed framework performs well on a range of image-to-image translation
tasks. It outperforms former designs of conditional flows because of its
expressive auto-regressive structure.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Batzolis_G/0/1/0/all/0/1"&gt;Georgios Batzolis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carioni_M/0/1/0/all/0/1"&gt;Marcello Carioni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Etmann_C/0/1/0/all/0/1"&gt;Christian Etmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Afyouni_S/0/1/0/all/0/1"&gt;Soroosh Afyouni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kourtzi_Z/0/1/0/all/0/1"&gt;Zoe Kourtzi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schonlieb_C/0/1/0/all/0/1"&gt;Carola Bibiane Sch&amp;#xf6;nlieb&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Image Local Autoregressive Transformer. (arXiv:2106.02514v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02514</id>
        <link href="http://arxiv.org/abs/2106.02514"/>
        <updated>2021-06-07T03:06:12.805Z</updated>
        <summary type="html"><![CDATA[Recently, AutoRegressive (AR) models for the whole image generation empowered
by transformers have achieved comparable or even better performance to
Generative Adversarial Networks (GANs). Unfortunately, directly applying such
AR models to edit/change local image regions, may suffer from the problems of
missing global information, slow inference speed, and information leakage of
local guidance. To address these limitations, we propose a novel model -- image
Local Autoregressive Transformer (iLAT), to better facilitate the locally
guided image synthesis. Our iLAT learns the novel local discrete
representations, by the newly proposed local autoregressive (LA) transformer of
the attention mask and convolution mechanism. Thus iLAT can efficiently
synthesize the local image regions by key guidance information. Our iLAT is
evaluated on various locally guided image syntheses, such as pose-guided person
image synthesis and face editing. Both the quantitative and qualitative results
show the efficacy of our model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cao_C/0/1/0/all/0/1"&gt;Chenjie Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1"&gt;Yuxin Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chengrong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Chengming Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xue_X/0/1/0/all/0/1"&gt;XiangYang Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1"&gt;Yanwei Fu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Aligning Pretraining for Detection via Object-Level Contrastive Learning. (arXiv:2106.02637v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02637</id>
        <link href="http://arxiv.org/abs/2106.02637"/>
        <updated>2021-06-07T03:06:12.731Z</updated>
        <summary type="html"><![CDATA[Image-level contrastive representation learning has proven to be highly
effective as a generic model for transfer learning. Such generality for
transfer learning, however, sacrifices specificity if we are interested in a
certain downstream task. We argue that this could be sub-optimal and thus
advocate a design principle which encourages alignment between the
self-supervised pretext task and the downstream task. In this paper, we follow
this principle with a pretraining method specifically designed for the task of
object detection. We attain alignment in the following three aspects: 1)
object-level representations are introduced via selective search bounding boxes
as object proposals; 2) the pretraining network architecture incorporates the
same dedicated modules used in the detection pipeline (e.g. FPN); 3) the
pretraining is equipped with object detection properties such as object-level
translation invariance and scale invariance. Our method, called Selective
Object COntrastive learning (SoCo), achieves state-of-the-art results for
transfer performance on COCO detection using a Mask R-CNN framework. Code and
models will be made available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1"&gt;Fangyun Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1"&gt;Yue Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Zhirong Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1"&gt;Han Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1"&gt;Stephen Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SOLQ: Segmenting Objects by Learning Queries. (arXiv:2106.02351v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02351</id>
        <link href="http://arxiv.org/abs/2106.02351"/>
        <updated>2021-06-07T03:06:12.724Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose an end-to-end framework for instance segmentation.
Based on the recently introduced DETR [1], our method, termed SOLQ, segments
objects by learning unified queries. In SOLQ, each query represents one object
and has multiple representations: class, location and mask. The object queries
learned perform classification, box regression and mask encoding simultaneously
in an unified vector form. During training phase, the mask vectors encoded are
supervised by the compression coding of raw spatial masks. In inference time,
mask vectors produced can be directly transformed to spatial masks by the
inverse process of compression coding. Experimental results show that SOLQ can
achieve state-of-the-art performance, surpassing most of existing approaches.
Moreover, the joint learning of unified query representation can greatly
improve the detection performance of original DETR. We hope our SOLQ can serve
as a strong baseline for the Transformer-based instance segmentation. Code is
available at https://github.com/megvii-research/SOLQ.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dong_B/0/1/0/all/0/1"&gt;Bin Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_F/0/1/0/all/0/1"&gt;Fangao Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1"&gt;Tiancai Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiangyu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1"&gt;Yichen Wei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Deep Local and Global Scene-Graph Matching for Image-Text Retrieval. (arXiv:2106.02400v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02400</id>
        <link href="http://arxiv.org/abs/2106.02400"/>
        <updated>2021-06-07T03:06:12.717Z</updated>
        <summary type="html"><![CDATA[Conventional approaches to image-text retrieval mainly focus on indexing
visual objects appearing in pictures but ignore the interactions between these
objects. Such objects occurrences and interactions are equivalently useful and
important in this field as they are usually mentioned in the text. Scene graph
presentation is a suitable method for the image-text matching challenge and
obtained good results due to its ability to capture the inter-relationship
information. Both images and text are represented in scene graph levels and
formulate the retrieval challenge as a scene graph matching challenge. In this
paper, we introduce the Local and Global Scene Graph Matching (LGSGM) model
that enhances the state-of-the-art method by integrating an extra graph
convolution network to capture the general information of a graph.
Specifically, for a pair of scene graphs of an image and its caption, two
separate models are used to learn the features of each graph's nodes and edges.
Then a Siamese-structure graph convolution model is employed to embed graphs
into vector forms. We finally combine the graph-level and the vector-level to
calculate the similarity of this image-text pair. The empirical experiments
show that our enhancement with the combination of levels can improve the
performance of the baseline method by increasing the recall by more than 10% on
the Flickr30k dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_M/0/1/0/all/0/1"&gt;Manh-Duy Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_B/0/1/0/all/0/1"&gt;Binh T. Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gurrin_C/0/1/0/all/0/1"&gt;Cathal Gurrin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hybrid attention network based on progressive embedding scale-context for crowd counting. (arXiv:2106.02324v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02324</id>
        <link href="http://arxiv.org/abs/2106.02324"/>
        <updated>2021-06-07T03:06:12.699Z</updated>
        <summary type="html"><![CDATA[The existing crowd counting methods usually adopted attention mechanism to
tackle background noise, or applied multi-level features or multi-scales
context fusion to tackle scale variation. However, these approaches deal with
these two problems separately. In this paper, we propose a Hybrid Attention
Network (HAN) by employing Progressive Embedding Scale-context (PES)
information, which enables the network to simultaneously suppress noise and
adapt head scale variation. We build the hybrid attention mechanism through
paralleling spatial attention and channel attention module, which makes the
network to focus more on the human head area and reduce the interference of
background objects. Besides, we embed certain scale-context to the hybrid
attention along the spatial and channel dimensions for alleviating these
counting errors caused by the variation of perspective and head scale. Finally,
we propose a progressive learning strategy through cascading multiple hybrid
attention modules with embedding different scale-context, which can gradually
integrate different scale-context information into the current feature map from
global to local. Ablation experiments provides that the network architecture
can gradually learn multi-scale features and suppress background noise.
Extensive experiments demonstrate that HANet obtain state-of-the-art counting
performance on four mainstream datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1"&gt;Fusen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sang_J/0/1/0/all/0/1"&gt;Jun Sang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Zhongyuan Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1"&gt;Qi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sang_N/0/1/0/all/0/1"&gt;Nong Sang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A New Gastric Histopathology Subsize Image Database (GasHisSDB) for Classification Algorithm Test: from Linear Regression to Visual Transformer. (arXiv:2106.02473v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02473</id>
        <link href="http://arxiv.org/abs/2106.02473"/>
        <updated>2021-06-07T03:06:12.678Z</updated>
        <summary type="html"><![CDATA[GasHisSDB is a New Gastric Histopathology Subsize Image Database with a total
of 245196 images. GasHisSDB is divided into 160*160 pixels sub-database,
120*120 pixels sub-database and 80*80 pixels sub-database. GasHisSDB is made to
realize the function of valuating image classification. In order to prove that
the methods of different periods in the field of image classification have
discrepancies on GasHisSDB, we select a variety of classifiers for evaluation.
Seven classical machine learning classifiers, three CNN classifiers and a novel
transformer-based classifier are selected for testing on image classification
tasks. GasHisSDB is available at the
URL:https://github.com/NEUhwm/GasHisSDB.git.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1"&gt;Weiming Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiaoyan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Haoyuan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wanli Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1"&gt;Changhao Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grzegorzek_M/0/1/0/all/0/1"&gt;Marcin Grzegorzek&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Controlling False Positive/Negative Rates for Deep-Learning-Based Prostate Cancer Detection on Multiparametric MR images. (arXiv:2106.02385v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.02385</id>
        <link href="http://arxiv.org/abs/2106.02385"/>
        <updated>2021-06-07T03:06:12.649Z</updated>
        <summary type="html"><![CDATA[Prostate cancer (PCa) is one of the leading causes of death for men
worldwide. Multi-parametric magnetic resonance (mpMR) imaging has emerged as a
non-invasive diagnostic tool for detecting and localising prostate tumours by
specialised radiologists. These radiological examinations, for example, for
differentiating malignant lesions from benign prostatic hyperplasia in
transition zones and for defining the boundaries of clinically significant
cancer, remain challenging and highly skill-and-experience-dependent. We first
investigate experimental results in developing object detection neural networks
that are trained to predict the radiological assessment, using these
high-variance labels. We further argue that such a computer-assisted diagnosis
(CAD) system needs to have the ability to control the false-positive rate (FPR)
or false-negative rate (FNR), in order to be usefully deployed in a clinical
workflow, informing clinical decisions without further human intervention. This
work proposes a novel PCa detection network that incorporates a lesion-level
cost-sensitive loss and an additional slice-level loss based on a
lesion-to-slice mapping function, to manage the lesion- and slice-level costs,
respectively. Our experiments based on 290 clinical patients concludes that 1)
The lesion-level FNR was effectively reduced from 0.19 to 0.10 and the
lesion-level FPR was reduced from 1.03 to 0.66 by changing the lesion-level
cost; 2) The slice-level FNR was reduced from 0.19 to 0.00 by taking into
account the slice-level cost; (3) Both lesion-level and slice-level FNRs were
reduced with lower FP/FPR by changing the lesion-level or slice-level costs,
compared with post-training threshold adjustment using networks without the
proposed cost-aware training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Min_Z/0/1/0/all/0/1"&gt;Zhe Min&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bianco_F/0/1/0/all/0/1"&gt;Fernando J. Bianco&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yang_Q/0/1/0/all/0/1"&gt;Qianye Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rodell_R/0/1/0/all/0/1"&gt;Rachael Rodell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yan_W/0/1/0/all/0/1"&gt;Wen Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Barratt_D/0/1/0/all/0/1"&gt;Dean Barratt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hu_Y/0/1/0/all/0/1"&gt;Yipeng Hu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Human-Adversarial Visual Question Answering. (arXiv:2106.02280v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02280</id>
        <link href="http://arxiv.org/abs/2106.02280"/>
        <updated>2021-06-07T03:06:12.620Z</updated>
        <summary type="html"><![CDATA[Performance on the most commonly used Visual Question Answering dataset (VQA
v2) is starting to approach human accuracy. However, in interacting with
state-of-the-art VQA models, it is clear that the problem is far from being
solved. In order to stress test VQA models, we benchmark them against
human-adversarial examples. Human subjects interact with a state-of-the-art VQA
model, and for each image in the dataset, attempt to find a question where the
model's predicted answer is incorrect. We find that a wide range of
state-of-the-art models perform poorly when evaluated on these examples. We
conduct an extensive analysis of the collected adversarial examples and provide
guidance on future research directions. We hope that this Adversarial VQA
(AdVQA) benchmark can help drive progress in the field and advance the state of
the art.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sheng_S/0/1/0/all/0/1"&gt;Sasha Sheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1"&gt;Amanpreet Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goswami_V/0/1/0/all/0/1"&gt;Vedanuj Goswami&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Magana_J/0/1/0/all/0/1"&gt;Jose Alberto Lopez Magana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Galuba_W/0/1/0/all/0/1"&gt;Wojciech Galuba&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parikh_D/0/1/0/all/0/1"&gt;Devi Parikh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kiela_D/0/1/0/all/0/1"&gt;Douwe Kiela&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MASA-SR: Matching Acceleration and Spatial Adaptation for Reference-Based Image Super-Resolution. (arXiv:2106.02299v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02299</id>
        <link href="http://arxiv.org/abs/2106.02299"/>
        <updated>2021-06-07T03:06:12.614Z</updated>
        <summary type="html"><![CDATA[Reference-based image super-resolution (RefSR) has shown promising success in
recovering high-frequency details by utilizing an external reference image
(Ref). In this task, texture details are transferred from the Ref image to the
low-resolution (LR) image according to their point- or patch-wise
correspondence. Therefore, high-quality correspondence matching is critical. It
is also desired to be computationally efficient. Besides, existing RefSR
methods tend to ignore the potential large disparity in distributions between
the LR and Ref images, which hurts the effectiveness of the information
utilization. In this paper, we propose the MASA network for RefSR, where two
novel modules are designed to address these problems. The proposed Match &
Extraction Module significantly reduces the computational cost by a
coarse-to-fine correspondence matching scheme. The Spatial Adaptation Module
learns the difference of distribution between the LR and Ref images, and remaps
the distribution of Ref features to that of LR features in a spatially adaptive
way. This scheme makes the network robust to handle different reference images.
Extensive quantitative and qualitative experiments validate the effectiveness
of our proposed model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lu_L/0/1/0/all/0/1"&gt;Liying Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1"&gt;Wenbo Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_X/0/1/0/all/0/1"&gt;Xin Tao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1"&gt;Jiangbo Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1"&gt;Jiaya Jia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semantic Correspondence with Transformers. (arXiv:2106.02520v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02520</id>
        <link href="http://arxiv.org/abs/2106.02520"/>
        <updated>2021-06-07T03:06:12.593Z</updated>
        <summary type="html"><![CDATA[We propose a novel cost aggregation network, called Cost Aggregation with
Transformers (CATs), to find dense correspondences between semantically similar
images with additional challenges posed by large intra-class appearance and
geometric variations. Compared to previous hand-crafted or CNN-based methods
addressing the cost aggregation stage, which either lack robustness to severe
deformations or inherit the limitation of CNNs that fail to discriminate
incorrect matches due to limited receptive fields, CATs explore global
consensus among initial correlation map with the help of some architectural
designs that allow us to exploit full potential of self-attention mechanism.
Specifically, we include appearance affinity modelling to disambiguate the
initial correlation maps and multi-level aggregation to benefit from
hierarchical feature representations within Transformer-based aggregator, and
combine with swapping self-attention and residual connections not only to
enforce consistent matching, but also to ease the learning process. We conduct
experiments to demonstrate the effectiveness of the proposed model over the
latest methods and provide extensive ablation studies. Code and trained models
will be made available at https://github.com/SunghwanHong/CATs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cho_S/0/1/0/all/0/1"&gt;Seokju Cho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hong_S/0/1/0/all/0/1"&gt;Sunghwan Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jeon_S/0/1/0/all/0/1"&gt;Sangryul Jeon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1"&gt;Yunsung Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sohn_K/0/1/0/all/0/1"&gt;Kwanghoon Sohn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1"&gt;Seungryong Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hallucination In Object Detection -- A Study In Visual Part Verification. (arXiv:2106.02523v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02523</id>
        <link href="http://arxiv.org/abs/2106.02523"/>
        <updated>2021-06-07T03:06:12.586Z</updated>
        <summary type="html"><![CDATA[We show that object detectors can hallucinate and detect missing objects;
potentially even accurately localized at their expected, but non-existing,
position. This is particularly problematic for applications that rely on visual
part verification: detecting if an object part is present or absent. We show
how popular object detectors hallucinate objects in a visual part verification
task and introduce the first visual part verification dataset: DelftBikes,
which has 10,000 bike photographs, with 22 densely annotated parts per image,
where some parts may be missing. We explicitly annotated an extra object state
label for each part to reflect if a part is missing or intact. We propose to
evaluate visual part verification by relying on recall and compare popular
object detectors on DelftBikes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kayhan_O/0/1/0/all/0/1"&gt;Osman Semih Kayhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vredebregt_B/0/1/0/all/0/1"&gt;Bart Vredebregt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gemert_J/0/1/0/all/0/1"&gt;Jan C. van Gemert&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Covering Polygons is Even Harder. (arXiv:2106.02335v1 [cs.CG])]]></title>
        <id>http://arxiv.org/abs/2106.02335</id>
        <link href="http://arxiv.org/abs/2106.02335"/>
        <updated>2021-06-07T03:06:12.579Z</updated>
        <summary type="html"><![CDATA[In the MINIMUM CONVEX COVER (MCC) problem, we are given a simple polygon
$\mathcal P$ and an integer $k$, and the question is if there exist $k$ convex
polygons whose union is $\mathcal P$. It is known that MCC is
$\mathsf{NP}$-hard [Culberson & Reckhow: Covering polygons is hard, FOCS
1988/Journal of Algorithms 1994] and in $\exists\mathbb{R}$ [O'Rourke: The
complexity of computing minimum convex covers for polygons, Allerton 1982]. We
prove that MCC is $\exists\mathbb{R}$-hard, and the problem is thus
$\exists\mathbb{R}$-complete. In other words, the problem is equivalent to
deciding whether a system of polynomial equations and inequalities with integer
coefficients has a real solution.

If a cover for our constructed polygon exists, then so does a cover
consisting entirely of triangles. As a byproduct, we therefore also establish
that it is $\exists\mathbb{R}$-complete to decide whether $k$ triangles cover a
given polygon.

The issue that it was not known if finding a minimum cover is in
$\mathsf{NP}$ has repeatedly been raised in the literature, and it was
mentioned as a "long-standing open question" already in 2001 [Eidenbenz &
Widmayer: An approximation algorithm for minimum convex cover with logarithmic
performance guarantee, ESA 2001/SIAM Journal on Computing 2003]. We prove that
assuming the widespread belief that $\mathsf{NP}\neq\exists\mathbb{R}$, the
problem is not in $\mathsf{NP}$.

An implication of the result is that many natural approaches to finding small
covers are bound to give suboptimal solutions in some cases, since irrational
coordinates of arbitrarily high algebraic degree can be needed for the corners
of the pieces in an optimal solution.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Abrahamsen_M/0/1/0/all/0/1"&gt;Mikkel Abrahamsen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RoadMap: A Light-Weight Semantic Map for Visual Localization towards Autonomous Driving. (arXiv:2106.02527v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02527</id>
        <link href="http://arxiv.org/abs/2106.02527"/>
        <updated>2021-06-07T03:06:12.572Z</updated>
        <summary type="html"><![CDATA[Accurate localization is of crucial importance for autonomous driving tasks.
Nowadays, we have seen a lot of sensor-rich vehicles (e.g. Robo-taxi) driving
on the street autonomously, which rely on high-accurate sensors (e.g. Lidar and
RTK GPS) and high-resolution map. However, low-cost production cars cannot
afford such high expenses on sensors and maps. How to reduce costs? How do
sensor-rich vehicles benefit low-cost cars? In this paper, we proposed a
light-weight localization solution, which relies on low-cost cameras and
compact visual semantic maps. The map is easily produced and updated by
sensor-rich vehicles in a crowd-sourced way. Specifically, the map consists of
several semantic elements, such as lane line, crosswalk, ground sign, and stop
line on the road surface. We introduce the whole framework of on-vehicle
mapping, on-cloud maintenance, and user-end localization. The map data is
collected and preprocessed on vehicles. Then, the crowd-sourced data is
uploaded to a cloud server. The mass data from multiple vehicles are merged on
the cloud so that the semantic map is updated in time. Finally, the semantic
map is compressed and distributed to production cars, which use this map for
localization. We validate the performance of the proposed map in real-world
experiments and compare it against other algorithms. The average size of the
semantic map is $36$ kb/km. We highlight that this framework is a reliable and
practical localization solution for autonomous driving.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1"&gt;Tong Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1"&gt;Yuxin Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1"&gt;Tongqing Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yilun Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_Q/0/1/0/all/0/1"&gt;Qing Su&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Temporally coherent video anonymization through GAN inpainting. (arXiv:2106.02328v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02328</id>
        <link href="http://arxiv.org/abs/2106.02328"/>
        <updated>2021-06-07T03:06:12.565Z</updated>
        <summary type="html"><![CDATA[This work tackles the problem of temporally coherent face anonymization in
natural video streams.We propose JaGAN, a two-stage system starting with
detecting and masking out faces with black image patches in all individual
frames of the video. The second stage leverages a privacy-preserving Video
Generative Adversarial Network designed to inpaint the missing image patches
with artificially generated faces. Our initial experiments reveal that image
based generative models are not capable of inpainting patches showing temporal
coherent appearance across neighboring video frames. To address this issue we
introduce a newly curated video collection, which is made publicly available
for the research community along with this paper. We also introduce the
Identity Invariance Score IdI as a means to quantify temporal coherency between
neighboring frames.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Balaji_T/0/1/0/all/0/1"&gt;Thangapavithraa Balaji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Blies_P/0/1/0/all/0/1"&gt;Patrick Blies&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gori_G/0/1/0/all/0/1"&gt;Georg G&amp;#xf6;ri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mitsch_R/0/1/0/all/0/1"&gt;Raphael Mitsch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wasserer_M/0/1/0/all/0/1"&gt;Marcel Wasserer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schon_T/0/1/0/all/0/1"&gt;Torsten Sch&amp;#xf6;n&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fine-Grained Visual Classification of Plant Species In The Wild: Object Detection as A Reinforced Means of Attention. (arXiv:2106.02141v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02141</id>
        <link href="http://arxiv.org/abs/2106.02141"/>
        <updated>2021-06-07T03:06:12.546Z</updated>
        <summary type="html"><![CDATA[Plant species identification in the wild is a difficult problem in part due
to the high variability of the input data, but also because of complications
induced by the long-tail effects of the datasets distribution. Inspired by the
most recent fine-grained visual classification approaches which are based on
attention to mitigate the effects of data variability, we explore the idea of
using object detection as a form of attention. We introduce a bottom-up
approach based on detecting plant organs and fusing the predictions of a
variable number of organ-based species classifiers. We also curate a new
dataset with a long-tail distribution for evaluating plant organ detection and
organ-based species identification, which is publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Keaton_M/0/1/0/all/0/1"&gt;Matthew R. Keaton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zaveri_R/0/1/0/all/0/1"&gt;Ram J. Zaveri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kovur_M/0/1/0/all/0/1"&gt;Meghana Kovur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Henderson_C/0/1/0/all/0/1"&gt;Cole Henderson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Adjeroh_D/0/1/0/all/0/1"&gt;Donald A. Adjeroh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Doretto_G/0/1/0/all/0/1"&gt;Gianfranco Doretto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Analysis of the robustness of NMF algorithms. (arXiv:2106.02213v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02213</id>
        <link href="http://arxiv.org/abs/2106.02213"/>
        <updated>2021-06-07T03:06:12.539Z</updated>
        <summary type="html"><![CDATA[We examine three non-negative matrix factorization techniques; L2-norm,
L1-norm, and L2,1-norm. Our aim is to establish the performance of these
different approaches, and their robustness in real-world applications such as
feature selection while managing computational complexity, sensitivity to noise
and more. We thoroughly examine each approach from a theoretical perspective,
and examine the performance of each using a series of experiments drawing on
both the ORL and YaleB datasets. We examine the Relative Reconstruction Errors
(RRE), Average Accuracy and Normalized Mutual Information (NMI) as criteria
under a range of simulated noise scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Diaz_A/0/1/0/all/0/1"&gt;Alex D&amp;#xed;az&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Steele_D/0/1/0/all/0/1"&gt;Damian Steele&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Subdivision-Based Mesh Convolution Networks. (arXiv:2106.02285v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02285</id>
        <link href="http://arxiv.org/abs/2106.02285"/>
        <updated>2021-06-07T03:06:12.531Z</updated>
        <summary type="html"><![CDATA[Convolutional neural networks (CNNs) have made great breakthroughs in 2D
computer vision. However, the irregular structure of meshes makes it hard to
exploit the power of CNNs directly. A subdivision surface provides a
hierarchical multi-resolution structure, and each face in a closed 2-manifold
triangle mesh is exactly adjacent to three faces. Motivated by these two
properties, this paper introduces a novel and flexible CNN framework, named
SubdivNet, for 3D triangle meshes with Loop subdivision sequence connectivity.
Making an analogy between mesh faces and pixels in a 2D image allows us to
present a mesh convolution operator to aggregate local features from adjacent
faces. By exploiting face neighborhoods, this convolution can support standard
2D convolutional network concepts, e.g. variable kernel size, stride, and
dilation. Based on the multi-resolution hierarchy, we propose a spatial uniform
pooling layer which merges four faces into one and an upsampling method which
splits one face into four. As a result, many popular 2D CNN architectures can
be readily adapted to processing 3D meshes. Meshes with arbitrary connectivity
can be remeshed to hold Loop subdivision sequence connectivity via
self-parameterization, making SubdivNet a general approach. Experiments on mesh
classification, segmentation, correspondence, and retrieval from the real-world
demonstrate the effectiveness and efficiency of SubdivNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1"&gt;Shi-Min Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zheng-Ning Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1"&gt;Meng-Hao Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1"&gt;Jun-Xiong Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1"&gt;Jiahui Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mu_T/0/1/0/all/0/1"&gt;Tai-Jiang Mu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martin_R/0/1/0/all/0/1"&gt;Ralph R. Martin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DOCTOR: A Simple Method for Detecting Misclassification Errors. (arXiv:2106.02395v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02395</id>
        <link href="http://arxiv.org/abs/2106.02395"/>
        <updated>2021-06-07T03:06:12.525Z</updated>
        <summary type="html"><![CDATA[Deep neural networks (DNNs) have shown to perform very well on large scale
object recognition problems and lead to widespread use for real-world
applications, including situations where DNN are implemented as "black boxes".
A promising approach to secure their use is to accept decisions that are likely
to be correct while discarding the others. In this work, we propose DOCTOR, a
simple method that aims to identify whether the prediction of a DNN classifier
should (or should not) be trusted so that, consequently, it would be possible
to accept it or to reject it. Two scenarios are investigated: Totally Black Box
(TBB) where only the soft-predictions are available and Partially Black Box
(PBB) where gradient-propagation to perform input pre-processing is allowed.
Empirically, we show that DOCTOR outperforms all state-of-the-art methods on
various well-known images and sentiment analysis datasets. In particular, we
observe a reduction of up to $4\%$ of the false rejection rate (FRR) in the PBB
scenario. DOCTOR can be applied to any pre-trained model, it does not require
prior information about the underlying dataset and is as simple as the simplest
available methods in the literature.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Granese_F/0/1/0/all/0/1"&gt;Federica Granese&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Romanelli_M/0/1/0/all/0/1"&gt;Marco Romanelli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gorla_D/0/1/0/all/0/1"&gt;Daniele Gorla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Palamidessi_C/0/1/0/all/0/1"&gt;Catuscia Palamidessi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Piantanida_P/0/1/0/all/0/1"&gt;Pablo Piantanida&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Associating Objects with Transformers for Video Object Segmentation. (arXiv:2106.02638v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02638</id>
        <link href="http://arxiv.org/abs/2106.02638"/>
        <updated>2021-06-07T03:06:12.518Z</updated>
        <summary type="html"><![CDATA[This paper investigates how to realize better and more efficient embedding
learning to tackle the semi-supervised video object segmentation under
challenging multi-object scenarios. The state-of-the-art methods learn to
decode features with a single positive object and thus have to match and
segment each target separately under multi-object scenarios, consuming multiple
times computing resources. To solve the problem, we propose an Associating
Objects with Transformers (AOT) approach to match and decode multiple objects
uniformly. In detail, AOT employs an identification mechanism to associate
multiple targets into the same high-dimensional embedding space. Thus, we can
simultaneously process the matching and segmentation decoding of multiple
objects as efficiently as processing a single object. For sufficiently modeling
multi-object association, a Long Short-Term Transformer is designed for
constructing hierarchical matching and propagation. We conduct extensive
experiments on both multi-object and single-object benchmarks to examine AOT
variant networks with different complexities. Particularly, our AOT-L
outperforms all the state-of-the-art competitors on three popular benchmarks,
i.e., YouTube-VOS (83.7% J&F), DAVIS 2017 (83.0%), and DAVIS 2016 (91.0%),
while keeping better multi-object efficiency. Meanwhile, our AOT-T can maintain
real-time multi-object speed on above benchmarks. We ranked 1st in the 3rd
Large-scale Video Object Segmentation Challenge. The code will be publicly
available at https://github.com/z-x-yang/AOT.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Zongxin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1"&gt;Yunchao Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yi Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Few-Shot Segmentation via Cycle-Consistent Transformer. (arXiv:2106.02320v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02320</id>
        <link href="http://arxiv.org/abs/2106.02320"/>
        <updated>2021-06-07T03:06:12.511Z</updated>
        <summary type="html"><![CDATA[Few-shot segmentation aims to train a segmentation model that can fast adapt
to novel classes with few exemplars. The conventional training paradigm is to
learn to make predictions on query images conditioned on the features from
support images. Previous methods only utilized the semantic-level prototypes of
support images as the conditional information. These methods cannot utilize all
pixel-wise support information for the query predictions, which is however
critical for the segmentation task. In this paper, we focus on utilizing
pixel-wise relationships between support and target images to facilitate the
few-shot semantic segmentation task. We design a novel Cycle-Consistent
Transformer (CyCTR) module to aggregate pixel-wise support features into query
ones. CyCTR performs cross-attention between features from different images,
i.e. support and query images. We observe that there may exist unexpected
irrelevant pixel-level support features. Directly performing cross-attention
may aggregate these features from support to query and bias the query features.
Thus, we propose using a novel cycle-consistent attention mechanism to filter
out possible harmful support features and encourage query features to attend to
the most informative pixels from support images. Experiments on all few-shot
segmentation benchmarks demonstrate that our proposed CyCTR leads to remarkable
improvement compared to previous state-of-the-art methods. Specifically, on
Pascal-$5^i$ and COCO-$20^i$ datasets, we achieve 66.6% and 45.6% mIoU for
5-shot segmentation, outperforming previous state-of-the-art by 4.6% and 7.1%
respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1"&gt;Gengwei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kang_G/0/1/0/all/0/1"&gt;Guoliang Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1"&gt;Yunchao Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yi Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NMS-Loss: Learning with Non-Maximum Suppression for Crowded Pedestrian Detection. (arXiv:2106.02426v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02426</id>
        <link href="http://arxiv.org/abs/2106.02426"/>
        <updated>2021-06-07T03:06:12.494Z</updated>
        <summary type="html"><![CDATA[Non-Maximum Suppression (NMS) is essential for object detection and affects
the evaluation results by incorporating False Positives (FP) and False
Negatives (FN), especially in crowd occlusion scenes. In this paper, we raise
the problem of weak connection between the training targets and the evaluation
metrics caused by NMS and propose a novel NMS-Loss making the NMS procedure can
be trained end-to-end without any additional network parameters. Our NMS-Loss
punishes two cases when FP is not suppressed and FN is wrongly eliminated by
NMS. Specifically, we propose a pull loss to pull predictions with the same
target close to each other, and a push loss to push predictions with different
targets away from each other. Experimental results show that with the help of
NMS-Loss, our detector, namely NMS-Ped, achieves impressive results with Miss
Rate of 5.92% on Caltech dataset and 10.08% on CityPersons dataset, which are
both better than state-of-the-art competitors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1"&gt;Zekun Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1"&gt;Zheng Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1"&gt;Sixiao Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yabiao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1"&gt;Yanwei Fu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Glance-and-Gaze Vision Transformer. (arXiv:2106.02277v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02277</id>
        <link href="http://arxiv.org/abs/2106.02277"/>
        <updated>2021-06-07T03:06:12.486Z</updated>
        <summary type="html"><![CDATA[Recently, there emerges a series of vision Transformers, which show superior
performance with a more compact model size than conventional convolutional
neural networks, thanks to the strong ability of Transformers to model
long-range dependencies. However, the advantages of vision Transformers also
come with a price: Self-attention, the core part of Transformer, has a
quadratic complexity to the input sequence length. This leads to a dramatic
increase of computation and memory cost with the increase of sequence length,
thus introducing difficulties when applying Transformers to the vision tasks
that require dense predictions based on high-resolution feature maps. In this
paper, we propose a new vision Transformer, named Glance-and-Gaze Transformer
(GG-Transformer), to address the aforementioned issues. It is motivated by the
Glance and Gaze behavior of human beings when recognizing objects in natural
scenes, with the ability to efficiently model both long-range dependencies and
local context. In GG-Transformer, the Glance and Gaze behavior is realized by
two parallel branches: The Glance branch is achieved by performing
self-attention on the adaptively-dilated partitions of the input, which leads
to a linear complexity while still enjoying a global receptive field; The Gaze
branch is implemented by a simple depth-wise convolutional layer, which
compensates local image context to the features obtained by the Glance
mechanism. We empirically demonstrate our method achieves consistently superior
performance over previous state-of-the-art Transformers on various vision tasks
and benchmarks. The codes and models will be made available at
https://github.com/yucornetto/GG-Transformer.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1"&gt;Qihang Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1"&gt;Yingda Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1"&gt;Yutong Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1"&gt;Yongyi Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuille_A/0/1/0/all/0/1"&gt;Alan Yuille&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1"&gt;Wei Shen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ukiyo-e Analysis and Creativity with Attribute and Geometry Annotation. (arXiv:2106.02267v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02267</id>
        <link href="http://arxiv.org/abs/2106.02267"/>
        <updated>2021-06-07T03:06:12.479Z</updated>
        <summary type="html"><![CDATA[The study of Ukiyo-e, an important genre of pre-modern Japanese art, focuses
on the object and style like other artwork researches. Such study has benefited
from the renewed interest by the machine learning community in culturally
important topics, leading to interdisciplinary works including collections of
images, quantitative approaches, and machine learning-based creativities. They,
however, have several drawbacks, and it remains challenging to integrate these
works into a comprehensive view. To bridge this gap, we propose a holistic
approach We first present a large-scale Ukiyo-e dataset with coherent semantic
labels and geometric annotations, then show its value in a quantitative study
of Ukiyo-e paintings' object using these labels and annotations. We further
demonstrate the machine learning methods could help style study through soft
color decomposition of Ukiyo-e, and finally provides joint insights into object
and style by composing sketches and colors using colorization. Dataset
available at https://github.com/rois-codh/arc-ukiyoe-faces]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1"&gt;Yingtao Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Clanuwat_T/0/1/0/all/0/1"&gt;Tarin Clanuwat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suzuki_C/0/1/0/all/0/1"&gt;Chikahiko Suzuki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kitamoto_A/0/1/0/all/0/1"&gt;Asanobu Kitamoto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Recurrent Neural Networks with Mixed Hierarchical Structures for Natural Language Processing. (arXiv:2106.02562v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02562</id>
        <link href="http://arxiv.org/abs/2106.02562"/>
        <updated>2021-06-07T03:06:12.473Z</updated>
        <summary type="html"><![CDATA[Hierarchical structures exist in both linguistics and Natural Language
Processing (NLP) tasks. How to design RNNs to learn hierarchical
representations of natural languages remains a long-standing challenge. In this
paper, we define two different types of boundaries referred to as static and
dynamic boundaries, respectively, and then use them to construct a multi-layer
hierarchical structure for document classification tasks. In particular, we
focus on a three-layer hierarchical structure with static word- and sentence-
layers and a dynamic phrase-layer. LSTM cells and two boundary detectors are
used to implement the proposed structure, and the resulting network is called
the {\em Recurrent Neural Network with Mixed Hierarchical Structures}
(MHS-RNN). We further add three layers of attention mechanisms to the MHS-RNN
model. Incorporating attention mechanisms allows our model to use more
important content to construct document representation and enhance its
performance on document classification tasks. Experiments on five different
datasets show that the proposed architecture outperforms previous methods on
all the five tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1"&gt;Zhaoxin Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1"&gt;Michael Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[WeChat AI & ICT's Submission for DSTC9 Interactive Dialogue Evaluation Track. (arXiv:2101.07947v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.07947</id>
        <link href="http://arxiv.org/abs/2101.07947"/>
        <updated>2021-06-07T03:06:12.456Z</updated>
        <summary type="html"><![CDATA[We participate in the DSTC9 Interactive Dialogue Evaluation Track (Gunasekara
et al. 2020) sub-task 1 (Knowledge Grounded Dialogue) and sub-task 2
(Interactive Dialogue). In sub-task 1, we employ a pre-trained language model
to generate topic-related responses and propose a response ensemble method for
response selection. In sub-task2, we propose a novel Dialogue Planning Model
(DPM) to capture conversation flow in the interaction with humans. We also
design an integrated open-domain dialogue system containing pre-process,
dialogue model, scoring model, and post-process, which can generate fluent,
coherent, consistent, and humanlike responses. We tie 1st on human ratings and
also get the highest Meteor, and Bert-score in sub-task 1, and rank 3rd on
interactive human evaluation in sub-task 2.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zekang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zongjia Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jinchao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1"&gt;Yang Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jie Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On (co-lex) Ordering Automata. (arXiv:2106.02309v1 [cs.FL])]]></title>
        <id>http://arxiv.org/abs/2106.02309</id>
        <link href="http://arxiv.org/abs/2106.02309"/>
        <updated>2021-06-07T03:06:12.450Z</updated>
        <summary type="html"><![CDATA[The states of a deterministic finite automaton A can be identified with
collections of words in Pf(L(A)) -- the set of prefixes of words belonging to
the regular language accepted by A. But words can be ordered and among the many
possible orders a very natural one is the co-lexicographic one. Such
naturalness stems from the fact that it suggests a transfer of the order from
words to the automaton's states. In a number of papers automata admitting a
total ordering of states coherent with the ordering of the set of words
reaching them have been proposed. Such class of ordered automata -- the Wheeler
automata -- turned out to be efficiently stored/searched using an index.
Unfortunately not all automata can be totally ordered as previously outlined.
However, automata can always be partially ordered and an intrinsic measure of
their complexity can be defined and effectively determined, as the minimum
width of one of their admissible partial orders. As shown in previous works,
this new concept of width of an automaton has useful consequences in the fields
of graph compression, indexing data structures, and automata theory. In this
paper we prove that a canonical, minimum-width, partially-ordered automaton
accepting a language L -- dubbed the Hasse automaton H of L -- can be
exhibited. H provides, in a precise sense, the best possible way to (partially)
order the states of any automaton accepting L, as long as we want to maintain
an operational link with the (co-lexicographic) order of Pf(L(A)). Using H we
prove that the width of the language can be effectively computed from the
minimum automaton recognizing the language. Finally, we explore the
relationship between two (often conflicting) objectives: minimizing the width
and minimizing the number of states of an automaton.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+DAgostino_G/0/1/0/all/0/1"&gt;Giovanna D&amp;#x27;Agostino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cotumaccio_N/0/1/0/all/0/1"&gt;Nicola Cotumaccio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Policriti_A/0/1/0/all/0/1"&gt;Alberto Policriti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prezza_N/0/1/0/all/0/1"&gt;Nicola Prezza&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Language Model Metrics and Procrustes Analysis for Improved Vector Transformation of NLP Embeddings. (arXiv:2106.02490v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02490</id>
        <link href="http://arxiv.org/abs/2106.02490"/>
        <updated>2021-06-07T03:06:12.443Z</updated>
        <summary type="html"><![CDATA[Artificial Neural networks are mathematical models at their core. This
truismpresents some fundamental difficulty when networks are tasked with
Natural Language Processing. A key problem lies in measuring the similarity or
distance among vectors in NLP embedding space, since the mathematical concept
of distance does not always agree with the linguistic concept. We suggest that
the best way to measure linguistic distance among vectors is by employing the
Language Model (LM) that created them. We introduce Language Model Distance
(LMD) for measuring accuracy of vector transformations based on the
Distributional Hypothesis ( LMD Accuracy ). We show the efficacy of this metric
by applying it to a simple neural network learning the Procrustes algorithm for
bilingual word mapping.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Conley_T/0/1/0/all/0/1"&gt;Thomas Conley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kalita_J/0/1/0/all/0/1"&gt;Jugal Kalita&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[History Encoding Representation Design for Human Intention Inference. (arXiv:2106.02222v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02222</id>
        <link href="http://arxiv.org/abs/2106.02222"/>
        <updated>2021-06-07T03:06:12.437Z</updated>
        <summary type="html"><![CDATA[In this extended abstract, we investigate the design of learning
representation for human intention inference. In our designed human intention
prediction task, we propose a history encoding representation that is both
interpretable and effective for prediction. Through extensive experiments, we
show our prediction framework with a history encoding representation design is
successful on the human intention prediction problem.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zhuo Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tomizuka_M/0/1/0/all/0/1"&gt;Masayoshi Tomizuka&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RL-DARTS: Differentiable Architecture Search for Reinforcement Learning. (arXiv:2106.02229v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02229</id>
        <link href="http://arxiv.org/abs/2106.02229"/>
        <updated>2021-06-07T03:06:12.430Z</updated>
        <summary type="html"><![CDATA[We introduce RL-DARTS, one of the first applications of Differentiable
Architecture Search (DARTS) in reinforcement learning (RL) to search for
convolutional cells, applied to the Procgen benchmark. We outline the initial
difficulties of applying neural architecture search techniques in RL, and
demonstrate that by simply replacing the image encoder with a DARTS supernet,
our search method is sample-efficient, requires minimal extra compute
resources, and is also compatible with off-policy and on-policy RL algorithms,
needing only minor changes in preexisting code. Surprisingly, we find that the
supernet can be used as an actor for inference to generate replay data in
standard RL training loops, and thus train end-to-end. Throughout this training
process, we show that the supernet gradually learns better cells, leading to
alternative architectures which can be highly competitive against manually
designed policies, but also verify previous design choices for RL policies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Miao_Y/0/1/0/all/0/1"&gt;Yingjie Miao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1"&gt;Xingyou Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_D/0/1/0/all/0/1"&gt;Daiyi Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yue_S/0/1/0/all/0/1"&gt;Summer Yue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brevdo_E/0/1/0/all/0/1"&gt;Eugene Brevdo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Faust_A/0/1/0/all/0/1"&gt;Aleksandra Faust&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[X-volution: On the unification of convolution and self-attention. (arXiv:2106.02253v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02253</id>
        <link href="http://arxiv.org/abs/2106.02253"/>
        <updated>2021-06-07T03:06:12.423Z</updated>
        <summary type="html"><![CDATA[Convolution and self-attention are acting as two fundamental building blocks
in deep neural networks, where the former extracts local image features in a
linear way while the latter non-locally encodes high-order contextual
relationships. Though essentially complementary to each other, i.e.,
first-/high-order, stat-of-the-art architectures, i.e., CNNs or transformers
lack a principled way to simultaneously apply both operations in a single
computational module, due to their heterogeneous computing pattern and
excessive burden of global dot-product for visual tasks. In this work, we
theoretically derive a global self-attention approximation scheme, which
approximates a self-attention via the convolution operation on transformed
features. Based on the approximated scheme, we establish a multi-branch
elementary module composed of both convolution and self-attention operation,
capable of unifying both local and non-local feature interaction. Importantly,
once trained, this multi-branch module could be conditionally converted into a
single standard convolution operation via structural re-parameterization,
rendering a pure convolution styled operator named X-volution, ready to be
plugged into any modern networks as an atomic operation. Extensive experiments
demonstrate that the proposed X-volution, achieves highly competitive visual
understanding improvements (+1.2% top-1 accuracy on ImageNet classification,
+1.7 box AP and +1.5 mask AP on COCO detection and segmentation).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xuanhong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ni_B/0/1/0/all/0/1"&gt;Bingbing Ni&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CNNs and GANs in MRI-based cross-modality medical image estimation. (arXiv:2106.02198v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.02198</id>
        <link href="http://arxiv.org/abs/2106.02198"/>
        <updated>2021-06-07T03:06:12.417Z</updated>
        <summary type="html"><![CDATA[Cross-modality image estimation involves the generation of images of one
medical imaging modality from that of another modality. Convolutional neural
networks (CNNs) have been shown to be useful in identifying, characterising and
extracting image patterns. Generative adversarial networks (GANs) use CNNs as
generators and estimated images are discriminated as true or false based on an
additional network. CNNs and GANs within the image estimation framework may be
considered more generally as deep learning approaches, since imaging data tends
to be large, leading to a larger number of network weights. Almost all research
in the CNN/GAN image estimation literature has involved the use of MRI data
with the other modality primarily being PET or CT. This review provides an
overview of the use of CNNs and GANs for MRI-based cross-modality medical image
estimation. We outline the neural networks implemented, and detail network
constructs employed for CNN and GAN image-to-image estimators. Motivations
behind cross-modality image estimation are provided as well. GANs appear to
provide better utility in cross-modality image estimation in comparison with
CNNs, a finding drawn based on our analysis involving metrics comparing
estimated and actual images. Our final remarks highlight key challenges faced
by the cross-modality medical image estimation field, and suggestions for
future research are outlined.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Fard_A/0/1/0/all/0/1"&gt;Azin Shokraei Fard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Reutens_D/0/1/0/all/0/1"&gt;David C. Reutens&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Vegh_V/0/1/0/all/0/1"&gt;Viktor Vegh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Visual Question Rewriting for Increasing Response Rate. (arXiv:2106.02257v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02257</id>
        <link href="http://arxiv.org/abs/2106.02257"/>
        <updated>2021-06-07T03:06:12.399Z</updated>
        <summary type="html"><![CDATA[When a human asks questions online, or when a conversational virtual agent
asks human questions, questions triggering emotions or with details might more
likely to get responses or answers. we explore how to automatically rewrite
natural language questions to improve the response rate from people. In
particular, a new task of Visual Question Rewriting(VQR) task is introduced to
explore how visual information can be used to improve the new questions. A data
set containing around 4K bland questions, attractive questions and images
triples is collected. We developed some baseline sequence to sequence models
and more advanced transformer based models, which take a bland question and a
related image as input and output a rewritten question that is expected to be
more attractive. Offline experiments and mechanical Turk based evaluations show
that it is possible to rewrite bland questions in a more detailed and
attractive way to increase the response rate, and images can be helpful.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1"&gt;Jiayi Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xilian Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xin Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ASCNet: Self-supervised Video Representation Learning with Appearance-Speed Consistency. (arXiv:2106.02342v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02342</id>
        <link href="http://arxiv.org/abs/2106.02342"/>
        <updated>2021-06-07T03:06:12.392Z</updated>
        <summary type="html"><![CDATA[We study self-supervised video representation learning, which is a
challenging task due to 1) a lack of labels for explicit supervision and 2)
unstructured and noisy visual information. Existing methods mainly use
contrastive loss with video clips as the instances and learn visual
representation by discriminating instances from each other, but they require
careful treatment of negative pairs by relying on large batch sizes, memory
banks, extra modalities, or customized mining strategies, inevitably including
noisy data. In this paper, we observe that the consistency between positive
samples is the key to learn robust video representations. Specifically, we
propose two tasks to learn the appearance and speed consistency, separately.
The appearance consistency task aims to maximize the similarity between two
clips of the same video with different playback speeds. The speed consistency
task aims to maximize the similarity between two clips with the same playback
speed but different appearance information. We show that joint optimization of
the two tasks consistently improves the performance on downstream tasks, e.g.,
action recognition and video retrieval. Remarkably, for action recognition on
the UCF-101 dataset, we achieve 90.8% accuracy without using any additional
modalities or negative pairs for unsupervised pretraining, outperforming the
ImageNet supervised pre-trained model. Codes and models will be available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1"&gt;Deng Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1"&gt;Wenhao Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1"&gt;Weiwen Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xu Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1"&gt;Dongliang He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Zhihua Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1"&gt;Xiangmiao Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1"&gt;Mingkui Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_E/0/1/0/all/0/1"&gt;Errui Ding&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring Adversarial Learning for Deep Semi-Supervised Facial Action Unit Recognition. (arXiv:2106.02258v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02258</id>
        <link href="http://arxiv.org/abs/2106.02258"/>
        <updated>2021-06-07T03:06:12.385Z</updated>
        <summary type="html"><![CDATA[Current works formulate facial action unit (AU) recognition as a supervised
learning problem, requiring fully AU-labeled facial images during training. It
is challenging if not impossible to provide AU annotations for large numbers of
facial images. Fortunately, AUs appear on all facial images, whether manually
labeled or not, satisfy the underlying anatomic mechanisms and human behavioral
habits. In this paper, we propose a deep semi-supervised framework for facial
action unit recognition from partially AU-labeled facial images. Specifically,
the proposed deep semi-supervised AU recognition approach consists of a deep
recognition network and a discriminator D. The deep recognition network R
learns facial representations from large-scale facial images and AU classifiers
from limited ground truth AU labels. The discriminator D is introduced to
enforce statistical similarity between the AU distribution inherent in ground
truth AU labels and the distribution of the predicted AU labels from labeled
and unlabeled facial images. The deep recognition network aims to minimize
recognition loss from the labeled facial images, to faithfully represent
inherent AU distribution for both labeled and unlabeled facial images, and to
confuse the discriminator. During training, the deep recognition network R and
the discriminator D are optimized alternately. Thus, the inherent AU
distributions caused by underlying anatomic mechanisms are leveraged to
construct better feature representations and AU classifiers from partially
AU-labeled data during training. Experiments on two benchmark databases
demonstrate that the proposed approach successfully captures AU distributions
through adversarial learning and outperforms state-of-the-art AU recognition
work.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shangfei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1"&gt;Yanan Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_G/0/1/0/all/0/1"&gt;Guozhu Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_B/0/1/0/all/0/1"&gt;Bowen Pan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Why does CTC result in peaky behavior?. (arXiv:2105.14849v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14849</id>
        <link href="http://arxiv.org/abs/2105.14849"/>
        <updated>2021-06-07T03:06:12.367Z</updated>
        <summary type="html"><![CDATA[The peaky behavior of CTC models is well known experimentally. However, an
understanding about why peaky behavior occurs is missing, and whether this is a
good property. We provide a formal analysis of the peaky behavior and gradient
descent convergence properties of the CTC loss and related training criteria.
Our analysis provides a deep understanding why peaky behavior occurs and when
it is suboptimal. On a simple example which should be trivial to learn for any
model, we prove that a feed-forward neural network trained with CTC from
uniform initialization converges towards peaky behavior with a 100% error rate.
Our analysis further explains why CTC only works well together with the blank
label. We further demonstrate that peaky behavior does not occur on other
related losses including a label prior model, and that this improves
convergence.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zeyer_A/0/1/0/all/0/1"&gt;Albert Zeyer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schluter_R/0/1/0/all/0/1"&gt;Ralf Schl&amp;#xfc;ter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ney_H/0/1/0/all/0/1"&gt;Hermann Ney&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CLIP: A Dataset for Extracting Action Items for Physicians from Hospital Discharge Notes. (arXiv:2106.02524v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02524</id>
        <link href="http://arxiv.org/abs/2106.02524"/>
        <updated>2021-06-07T03:06:12.360Z</updated>
        <summary type="html"><![CDATA[Continuity of care is crucial to ensuring positive health outcomes for
patients discharged from an inpatient hospital setting, and improved
information sharing can help. To share information, caregivers write discharge
notes containing action items to share with patients and their future
caregivers, but these action items are easily lost due to the lengthiness of
the documents. In this work, we describe our creation of a dataset of clinical
action items annotated over MIMIC-III, the largest publicly available dataset
of real clinical notes. This dataset, which we call CLIP, is annotated by
physicians and covers 718 documents representing 100K sentences. We describe
the task of extracting the action items from these documents as multi-aspect
extractive summarization, with each aspect representing a type of action to be
taken. We evaluate several machine learning models on this task, and show that
the best models exploit in-domain language model pre-training on 59K
unannotated documents, and incorporate context from neighboring sentences. We
also propose an approach to pre-training data selection that allows us to
explore the trade-off between size and domain-specificity of pre-training
datasets for this task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mullenbach_J/0/1/0/all/0/1"&gt;James Mullenbach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pruksachatkun_Y/0/1/0/all/0/1"&gt;Yada Pruksachatkun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Adler_S/0/1/0/all/0/1"&gt;Sean Adler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seale_J/0/1/0/all/0/1"&gt;Jennifer Seale&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Swartz_J/0/1/0/all/0/1"&gt;Jordan Swartz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McKelvey_T/0/1/0/all/0/1"&gt;T. Greg McKelvey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_H/0/1/0/all/0/1"&gt;Hui Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yi Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sontag_D/0/1/0/all/0/1"&gt;David Sontag&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[KACC: A Multi-task Benchmark for Knowledge Abstraction, Concretization and Completion. (arXiv:2004.13631v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.13631</id>
        <link href="http://arxiv.org/abs/2004.13631"/>
        <updated>2021-06-07T03:06:12.354Z</updated>
        <summary type="html"><![CDATA[A comprehensive knowledge graph (KG) contains an instance-level entity graph
and an ontology-level concept graph. The two-view KG provides a testbed for
models to "simulate" human's abilities on knowledge abstraction,
concretization, and completion (KACC), which are crucial for human to recognize
the world and manage learned knowledge. Existing studies mainly focus on
partial aspects of KACC. In order to promote thorough analyses for KACC
abilities of models, we propose a unified KG benchmark by improving existing
benchmarks in terms of dataset scale, task coverage, and difficulty.
Specifically, we collect new datasets that contain larger concept graphs,
abundant cross-view links as well as dense entity graphs. Based on the
datasets, we propose novel tasks such as multi-hop knowledge abstraction (MKA),
multi-hop knowledge concretization (MKC) and then design a comprehensive
benchmark. For MKA and MKC tasks, we further annotate multi-hop hierarchical
triples as harder samples. The experimental results of existing methods
demonstrate the challenges of our benchmark. The resource is available at
https://github.com/thunlp/KACC.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jie Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1"&gt;Shengding Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lv_X/0/1/0/all/0/1"&gt;Xin Lv&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1"&gt;Cheng Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhiyuan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1"&gt;Wei Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1"&gt;Jie Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Juanzi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1"&gt;Maosong Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Great Service! Fine-grained Parsing of Implicit Arguments. (arXiv:2106.02561v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02561</id>
        <link href="http://arxiv.org/abs/2106.02561"/>
        <updated>2021-06-07T03:06:12.347Z</updated>
        <summary type="html"><![CDATA[Broad-coverage meaning representations in NLP mostly focus on explicitly
expressed content. More importantly, the scarcity of datasets annotating
diverse implicit roles limits empirical studies into their linguistic nuances.
For example, in the web review "Great service!", the provider and consumer are
implicit arguments of different types. We examine an annotated corpus of
fine-grained implicit arguments (Cui and Hershcovich, 2020) by carefully
re-annotating it, resolving several inconsistencies. Subsequently, we present
the first transition-based neural parser that can handle implicit arguments
dynamically, and experiment with two different transition systems on the
improved dataset. We find that certain types of implicit arguments are more
difficult to parse than others and that the simpler system is more accurate in
recovering implicit arguments, despite having a lower overall parsing score,
attesting current reasoning limitations of NLP models. This work will
facilitate a better understanding of implicit and underspecified language, by
incorporating it holistically into meaning representations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cui_R/0/1/0/all/0/1"&gt;Ruixiang Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hershcovich_D/0/1/0/all/0/1"&gt;Daniel Hershcovich&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural semi-Markov CRF for Monolingual Word Alignment. (arXiv:2106.02569v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02569</id>
        <link href="http://arxiv.org/abs/2106.02569"/>
        <updated>2021-06-07T03:06:12.337Z</updated>
        <summary type="html"><![CDATA[Monolingual word alignment is important for studying fine-grained editing
operations (i.e., deletion, addition, and substitution) in text-to-text
generation tasks, such as paraphrase generation, text simplification,
neutralizing biased language, etc. In this paper, we present a novel neural
semi-Markov CRF alignment model, which unifies word and phrase alignments
through variable-length spans. We also create a new benchmark with human
annotations that cover four different text genres to evaluate monolingual word
alignment models in more realistic settings. Experimental results show that our
proposed model outperforms all previous approaches for monolingual word
alignment as well as a competitive QA-based baseline, which was previously only
applied to bilingual data. Our model demonstrates good generalizability to
three out-of-domain datasets and shows great utility in two downstream
applications: automatic text simplification and sentence pair classification
tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lan_W/0/1/0/all/0/1"&gt;Wuwei Lan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1"&gt;Chao Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1"&gt;Wei Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Specular reflections removal in colposcopic images based on neural networks: Supervised training with no ground truth previous knowledge. (arXiv:2106.02221v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.02221</id>
        <link href="http://arxiv.org/abs/2106.02221"/>
        <updated>2021-06-07T03:06:12.330Z</updated>
        <summary type="html"><![CDATA[Cervical cancer is a malignant tumor that seriously threatens women's health,
and is one of the most common that affects women worldwide. For its early
detection, colposcopic images of the cervix are used for searching for possible
injuries or abnormalities. An inherent characteristic of these images is the
presence of specular reflections (brightness) that make it difficult to observe
some regions, which might imply a misdiagnosis. In this paper, a new strategy
based on neural networks is introduced for eliminating specular reflections and
estimating the unobserved anatomical cervix portion under the bright zones. We
present a supervised learning method, despite not knowing the ground truth from
the beginning, based on training a neural network to learn how to restore any
hidden region of colposcopic images. Once the specular reflections are
identified, they are removed from the image and the previously trained network
is used to fulfill these deleted areas. The quality of the processed images was
evaluated quantitatively and qualitatively. In 21 of the 22 evaluated images,
the detected specular reflections were totally eliminated, whereas, in the
remaining one, these reflections were almost completely eliminated. The
distribution of the colors and the content of the restored images are similar
to those of the originals. The evaluation carried out by a specialist in Cervix
Pathology concluded that, after eliminating the specular reflections, the
anatomical and physiological elements of the cervix are observable in the
restored images, which facilitates the medical diagnosis of cervical
pathologies. Our method has the potential to improve the early detection of
cervical cancer.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Jimenez_Martin_L/0/1/0/all/0/1"&gt;Lauren Jimenez-Martin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Perez_D/0/1/0/all/0/1"&gt;Daniel A. Vald&amp;#xe9;s P&amp;#xe9;rez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Asteasuainzarra_A/0/1/0/all/0/1"&gt;Ana M. Solares Asteasuainzarra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Leonard_L/0/1/0/all/0/1"&gt;Ludwig Leonard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Diaz_Romanach_M/0/1/0/all/0/1"&gt;Marta L. Baguer D&amp;#xed;az-Roma&amp;#xf1;ach&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tackling the Background Bias in Sparse Object Detection via Cropped Windows. (arXiv:2106.02288v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02288</id>
        <link href="http://arxiv.org/abs/2106.02288"/>
        <updated>2021-06-07T03:06:12.301Z</updated>
        <summary type="html"><![CDATA[Object detection on Unmanned Aerial Vehicles (UAVs) is still a challenging
task. The recordings are mostly sparse and contain only small objects. In this
work, we propose a simple tiling method that improves the detection capability
in the remote sensing case without modifying the model itself. By reducing the
background bias and enabling the usage of higher image resolutions during
training, our method can improve the performance of models substantially. The
procedure was validated on three different data sets and outperformed similar
approaches in performance and speed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Varga_L/0/1/0/all/0/1"&gt;Leon Amadeus Varga&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zell_A/0/1/0/all/0/1"&gt;Andreas Zell&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[cs60075_team2 at SemEval-2021 Task 1 : Lexical Complexity Prediction using Transformer-based Language Models pre-trained on various text corpora. (arXiv:2106.02340v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02340</id>
        <link href="http://arxiv.org/abs/2106.02340"/>
        <updated>2021-06-07T03:06:12.205Z</updated>
        <summary type="html"><![CDATA[This paper describes the performance of the team cs60075_team2 at SemEval
2021 Task 1 - Lexical Complexity Prediction. The main contribution of this
paper is to fine-tune transformer-based language models pre-trained on several
text corpora, some being general (E.g., Wikipedia, BooksCorpus), some being the
corpora from which the CompLex Dataset was extracted, and others being from
other specific domains such as Finance, Law, etc. We perform ablation studies
on selecting the transformer models and how their individual complexity scores
are aggregated to get the resulting complexity scores. Our method achieves a
best Pearson Correlation of $0.784$ in sub-task 1 (single word) and $0.836$ in
sub-task 2 (multiple word expressions).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nandy_A/0/1/0/all/0/1"&gt;Abhilash Nandy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Adak_S/0/1/0/all/0/1"&gt;Sayantan Adak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Halder_T/0/1/0/all/0/1"&gt;Tanurima Halder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pokala_S/0/1/0/all/0/1"&gt;Sai Mahesh Pokala&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Classifying Continuous Constraint Satisfaction problems. (arXiv:2106.02397v1 [cs.CC])]]></title>
        <id>http://arxiv.org/abs/2106.02397</id>
        <link href="http://arxiv.org/abs/2106.02397"/>
        <updated>2021-06-07T03:06:12.165Z</updated>
        <summary type="html"><![CDATA[A continuous constraint satisfaction problem (CCSP) is a constraint
satisfaction problem (CSP) with a domain $U \subset \mathbb{R}$. We engage in a
systematic study to classify CCSPs that are complete of the Existential Theory
of the Reals, i.e., ER-complete. To define this class, we first consider the
problem ETR, which also stands for Existential Theory of the Reals. In an
instance of this problem we are given some sentence of the form $\exists x_1,
\ldots, x_n \in \mathbb{R} : \Phi(x_1, \ldots, x_n)$, where $\Phi$ is a
well-formed quantifier-free formula consisting of the symbols $\{0, 1, +,
\cdot, \geq, >, \wedge, \vee, \neg\}$, the goal is to check whether this
sentence is true. Now the class ER is the family of all problems that admit a
polynomial-time reduction to ETR. It is known that NP $\subseteq$ ER
$\subseteq$ PSPACE.

We restrict our attention on CCSPs with addition constraints ($x + y = z$)
and some other mild technical condition. Previously, it was shown that
multiplication constraints ($x \cdot y = z$), squaring constraints ($x^2 = y$),
or inversion constraints ($x\cdot y = 1$) are sufficient to establish
ER-completeness. We extend this in the strongest possible sense for equality
constraints as follows. We show that CCSPs (with addition constraints and some
other mild technical condition) that have any one well-behaved curved equality
constraint ($f(x,y) = 0$) are ER-complete. We further extend our results to
inequality constraints. We show that any well-behaved convexly curved and any
well-behaved concavely curved inequality constraint ($f(x,y) \geq 0$ and
$g(x,y) \geq 0$) imply ER-completeness on the class of such CCSPs.

We apply our findings to geometric packing and answer an open question by
Abrahamsen et al. [FOCS 2020]. Namely, we establish ER-completeness of packing
convex pieces into a square container under rotations and translations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Miltzow_T/0/1/0/all/0/1"&gt;Tillmann Miltzow&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schmiermann_R/0/1/0/all/0/1"&gt;Reinier F. Schmiermann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Contextualized Knowledge Structures for Commonsense Reasoning. (arXiv:2010.12873v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.12873</id>
        <link href="http://arxiv.org/abs/2010.12873"/>
        <updated>2021-06-07T03:06:12.143Z</updated>
        <summary type="html"><![CDATA[Recently, knowledge graph (KG) augmented models have achieved noteworthy
success on various commonsense reasoning tasks. However, KG edge (fact)
sparsity and noisy edge extraction/generation often hinder models from
obtaining useful knowledge to reason over. To address these issues, we propose
a new KG-augmented model: Hybrid Graph Network (HGN). Unlike prior methods, HGN
learns to jointly contextualize extracted and generated knowledge by reasoning
over both within a unified graph structure. Given the task input context and an
extracted KG subgraph, HGN is trained to generate embeddings for the subgraph's
missing edges to form a "hybrid" graph, then reason over the hybrid graph while
filtering out context-irrelevant edges. We demonstrate HGN's effectiveness
through considerable performance gains across four commonsense reasoning
benchmarks, plus a user study on edge validness and helpfulness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1"&gt;Jun Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raman_M/0/1/0/all/0/1"&gt;Mrigank Raman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chan_A/0/1/0/all/0/1"&gt;Aaron Chan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1"&gt;Tianyu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rossi_R/0/1/0/all/0/1"&gt;Ryan Rossi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1"&gt;Handong Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1"&gt;Sungchul Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lipka_N/0/1/0/all/0/1"&gt;Nedim Lipka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1"&gt;Xiang Ren&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predicting Themes within Complex Unstructured Texts: A Case Study on Safeguarding Reports. (arXiv:2010.14584v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.14584</id>
        <link href="http://arxiv.org/abs/2010.14584"/>
        <updated>2021-06-07T03:06:12.123Z</updated>
        <summary type="html"><![CDATA[The task of text and sentence classification is associated with the need for
large amounts of labelled training data. The acquisition of high volumes of
labelled datasets can be expensive or unfeasible, especially for
highly-specialised domains for which documents are hard to obtain. Research on
the application of supervised classification based on small amounts of training
data is limited. In this paper, we address the combination of state-of-the-art
deep learning and classification methods and provide an insight into what
combination of methods fit the needs of small, domain-specific, and
terminologically-rich corpora. We focus on a real-world scenario related to a
collection of safeguarding reports comprising learning experiences and
reflections on tackling serious incidents involving children and vulnerable
adults. The relatively small volume of available reports and their use of
highly domain-specific terminology makes the application of automated
approaches difficult. We focus on the problem of automatically identifying the
main themes in a safeguarding report using supervised classification
approaches. Our results show the potential of deep learning models to simulate
subject-expert behaviour even for complex tasks with limited labelled data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Edwards_A/0/1/0/all/0/1"&gt;Aleksandra Edwards&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rogers_D/0/1/0/all/0/1"&gt;David Rogers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Camacho_Collados_J/0/1/0/all/0/1"&gt;Jose Camacho-Collados&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ribaupierre_H/0/1/0/all/0/1"&gt;H&amp;#xe9;l&amp;#xe8;ne de Ribaupierre&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Preece_A/0/1/0/all/0/1"&gt;Alun Preece&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Draw: Emergent Communication through Sketching. (arXiv:2106.02067v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02067</id>
        <link href="http://arxiv.org/abs/2106.02067"/>
        <updated>2021-06-07T03:06:12.054Z</updated>
        <summary type="html"><![CDATA[Evidence that visual communication preceded written language and provided a
basis for it goes back to prehistory, in forms such as cave and rock paintings
depicting traces of our distant ancestors. Emergent communication research has
sought to explore how agents can learn to communicate in order to
collaboratively solve tasks. Existing research has focused on language, with a
learned communication channel transmitting sequences of discrete tokens between
the agents. In this work, we explore a visual communication channel between
agents that are allowed to draw with simple strokes. Our agents are
parameterised by deep neural networks, and the drawing procedure is
differentiable, allowing for end-to-end training. In the framework of a
referential communication game, we demonstrate that agents can not only
successfully learn to communicate by drawing, but with appropriate inductive
biases, can do so in a fashion that humans can interpret. We hope to encourage
future research to consider visual communication as a more flexible and
directly interpretable alternative of training collaborative agents.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mihai_D/0/1/0/all/0/1"&gt;Daniela Mihai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hare_J/0/1/0/all/0/1"&gt;Jonathon Hare&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Negation in Cognitive Reasoning. (arXiv:2012.12641v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.12641</id>
        <link href="http://arxiv.org/abs/2012.12641"/>
        <updated>2021-06-07T03:06:12.042Z</updated>
        <summary type="html"><![CDATA[Negation is both an operation in formal logic and in natural language by
which a proposition is replaced by one stating the opposite, as by the addition
of "not" or another negation cue. Treating negation in an adequate way is
required for cognitive reasoning, which aims at modeling the human ability to
draw meaningful conclusions despite incomplete and inconsistent knowledge. One
task of cognitive reasoning is answering questions given by sentences in
natural language. There are tools based on discourse representation theory to
convert sentences automatically into a formal logic representation, and
additional knowledge can be added using the predicate names in the formula and
knowledge databases. However, the knowledge in logic databases in practice
always is incomplete. Hence, forward reasoning of automated reasoning systems
alone does not suffice to derive answers to questions because, instead of
complete proofs, often only partial positive knowledge can be derived, while
negative knowledge is used only during the reasoning process. In consequence,
we aim at eliminating syntactic negation, strictly speaking, the negated event
or property. In this paper, we describe an effective procedure to determine the
negated event or property in order to replace it by its inverse. This lays the
basis of cognitive reasoning, employing both logic and machine learning for
general question answering. We evaluate our procedure by several benchmarks and
demonstrate its practical usefulness in our cognitive reasoning system.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schon_C/0/1/0/all/0/1"&gt;Claudia Schon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Siebert_S/0/1/0/all/0/1"&gt;Sophie Siebert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stolzenburg_F/0/1/0/all/0/1"&gt;Frieder Stolzenburg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Entity Concept-enhanced Few-shot Relation Extraction. (arXiv:2106.02401v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02401</id>
        <link href="http://arxiv.org/abs/2106.02401"/>
        <updated>2021-06-07T03:06:12.027Z</updated>
        <summary type="html"><![CDATA[Few-shot relation extraction (FSRE) is of great importance in long-tail
distribution problem, especially in special domain with low-resource data. Most
existing FSRE algorithms fail to accurately classify the relations merely based
on the information of the sentences together with the recognized entity pairs,
due to limited samples and lack of knowledge. To address this problem, in this
paper, we proposed a novel entity CONCEPT-enhanced FEw-shot Relation Extraction
scheme (ConceptFERE), which introduces the inherent concepts of entities to
provide clues for relation prediction and boost the relations classification
performance. Firstly, a concept-sentence attention module is developed to
select the most appropriate concept from multiple concepts of each entity by
calculating the semantic similarity between sentences and concepts. Secondly, a
self-attention based fusion module is presented to bridge the gap of concept
embedding and sentence embedding from different semantic spaces. Extensive
experiments on the FSRE benchmark dataset FewRel have demonstrated the
effectiveness and the superiority of the proposed ConceptFERE scheme as
compared to the state-of-the-art baselines. Code is available at
https://github.com/LittleGuoKe/ConceptFERE.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1"&gt;Shan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yongfei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niu_G/0/1/0/all/0/1"&gt;Guanglin Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1"&gt;Qinghua Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pu_S/0/1/0/all/0/1"&gt;Shiliang Pu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Knowing the No-match: Entity Alignment with Dangling Cases. (arXiv:2106.02248v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02248</id>
        <link href="http://arxiv.org/abs/2106.02248"/>
        <updated>2021-06-07T03:06:11.954Z</updated>
        <summary type="html"><![CDATA[This paper studies a new problem setting of entity alignment for knowledge
graphs (KGs). Since KGs possess different sets of entities, there could be
entities that cannot find alignment across them, leading to the problem of
dangling entities. As the first attempt to this problem, we construct a new
dataset and design a multi-task learning framework for both entity alignment
and dangling entity detection. The framework can opt to abstain from predicting
alignment for the detected dangling entities. We propose three techniques for
dangling entity detection that are based on the distribution of
nearest-neighbor distances, i.e., nearest neighbor classification, marginal
ranking and background ranking. After detecting and removing dangling entities,
an incorporated entity alignment model in our framework can provide more robust
alignment for remaining entities. Comprehensive experiments and analyses
demonstrate the effectiveness of our framework. We further discover that the
dangling entity detection module can, in turn, improve alignment learning and
the final performance. The contributed resource is publicly available to foster
further research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1"&gt;Zequn Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1"&gt;Muhao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1"&gt;Wei Hu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Barcode Method for Generative Model Evaluation driven by Topological Data Analysis. (arXiv:2106.02207v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02207</id>
        <link href="http://arxiv.org/abs/2106.02207"/>
        <updated>2021-06-07T03:06:11.936Z</updated>
        <summary type="html"><![CDATA[Evaluating the performance of generative models in image synthesis is a
challenging task. Although the Fr\'echet Inception Distance is a widely
accepted evaluation metric, it integrates different aspects (e.g., fidelity and
diversity) of synthesized images into a single score and assumes the normality
of embedded vectors. Recent methods such as precision-and-recall and its
variants such as density-and-coverage have been developed to separate fidelity
and diversity based on k-nearest neighborhood methods. In this study, we
propose an algorithm named barcode, which is inspired by the topological data
analysis and is almost free of assumption and hyperparameter selections. In
extensive experiments on real-world datasets as well as theoretical approach on
high-dimensional normal samples, it was found that the 'usual' normality
assumption of embedded vectors has several drawbacks. The experimental results
demonstrate that barcode outperforms other methods in evaluating fidelity and
diversity of GAN outputs. Official codes can be found in
https://github.com/minjeekim00/Barcode.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jang_R/0/1/0/all/0/1"&gt;Ryoungwoo Jang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1"&gt;Minjee Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eun_D/0/1/0/all/0/1"&gt;Da-in Eun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1"&gt;Kyungjin Cho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seo_J/0/1/0/all/0/1"&gt;Jiyeon Seo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_N/0/1/0/all/0/1"&gt;Namkug Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Laplacian-Based Dimensionality Reduction Including Spectral Clustering, Laplacian Eigenmap, Locality Preserving Projection, Graph Embedding, and Diffusion Map: Tutorial and Survey. (arXiv:2106.02154v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.02154</id>
        <link href="http://arxiv.org/abs/2106.02154"/>
        <updated>2021-06-07T03:06:11.929Z</updated>
        <summary type="html"><![CDATA[This is a tutorial and survey paper for nonlinear dimensionality and feature
extraction methods which are based on the Laplacian of graph of data. We first
introduce adjacency matrix, definition of Laplacian matrix, and the
interpretation of Laplacian. Then, we cover the cuts of graph and spectral
clustering which applies clustering in a subspace of data. Different
optimization variants of Laplacian eigenmap and its out-of-sample extension are
explained. Thereafter, we introduce the locality preserving projection and its
kernel variant as linear special cases of Laplacian eigenmap. Versions of graph
embedding are then explained which are generalized versions of Laplacian
eigenmap and locality preserving projection. Finally, diffusion map is
introduced which is a method based on Laplacian of data and random walks on the
data graph.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Ghojogh_B/0/1/0/all/0/1"&gt;Benyamin Ghojogh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ghodsi_A/0/1/0/all/0/1"&gt;Ali Ghodsi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Karray_F/0/1/0/all/0/1"&gt;Fakhri Karray&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Crowley_M/0/1/0/all/0/1"&gt;Mark Crowley&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Learning via Persistency of Excitation. (arXiv:2106.02078v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.02078</id>
        <link href="http://arxiv.org/abs/2106.02078"/>
        <updated>2021-06-07T03:06:11.921Z</updated>
        <summary type="html"><![CDATA[Improving adversarial robustness of neural networks remains a major
challenge. Fundamentally, training a network is a parameter estimation problem.
In adaptive control theory, maintaining persistency of excitation (PoE) is
integral to ensuring convergence of parameter estimates in dynamical systems to
their robust optima. In this work, we show that network training using gradient
descent is equivalent to a dynamical system parameter estimation problem.
Leveraging this relationship, we prove a sufficient condition for PoE of
gradient descent is achieved when the learning rate is less than the inverse of
the Lipschitz constant of the gradient of loss function. We provide an
efficient technique for estimating the corresponding Lipschitz constant using
extreme value theory and demonstrate that by only scaling the learning rate
schedule we can increase adversarial accuracy by up to 15% on benchmark
datasets. Our approach also universally increases the adversarial accuracy by
0.1% to 0.3% in various state-of-the-art adversarially trained models on the
AutoAttack benchmark, where every small margin of improvement is significant.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Sridhar_K/0/1/0/all/0/1"&gt;Kaustubh Sridhar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Sokolsky_O/0/1/0/all/0/1"&gt;Oleg Sokolsky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lee_I/0/1/0/all/0/1"&gt;Insup Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Weimer_J/0/1/0/all/0/1"&gt;James Weimer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Retrieve & Memorize: Dialog Policy Learning with Multi-Action Memory. (arXiv:2106.02317v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02317</id>
        <link href="http://arxiv.org/abs/2106.02317"/>
        <updated>2021-06-07T03:06:11.913Z</updated>
        <summary type="html"><![CDATA[Dialogue policy learning, a subtask that determines the content of system
response generation and then the degree of task completion, is essential for
task-oriented dialogue systems. However, the unbalanced distribution of system
actions in dialogue datasets often causes difficulty in learning to generate
desired actions and responses. In this paper, we propose a
retrieve-and-memorize framework to enhance the learning of system actions.
Specially, we first design a neural context-aware retrieval module to retrieve
multiple candidate system actions from the training set given a dialogue
context. Then, we propose a memory-augmented multi-decoder network to generate
the system actions conditioned on the candidate actions, which allows the
network to adaptively select key information in the candidate actions and
ignore noises. We conduct experiments on the large-scale multi-domain
task-oriented dialogue dataset MultiWOZ 2.0 and MultiWOZ 2.1.~Experimental
results show that our method achieves competitive performance among several
state-of-the-art models in the context-to-response generation task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yunhao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yunyi Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Quan_X/0/1/0/all/0/1"&gt;Xiaojun Quan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1"&gt;Jianxing Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Regular Expressions for Fast-response COVID-19 Text Classification. (arXiv:2102.09507v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.09507</id>
        <link href="http://arxiv.org/abs/2102.09507"/>
        <updated>2021-06-07T03:06:11.906Z</updated>
        <summary type="html"><![CDATA[Text classifiers are at the core of many NLP applications and use a variety
of algorithmic approaches and software. This paper introduces infrastructure
and methodologies for text classifiers based on large-scale regular
expressions. In particular, we describe how Facebook determines if a given
piece of text - anything from a hashtag to a post - belongs to a narrow topic
such as COVID-19. To fully define a topic and evaluate classifier performance
we employ human-guided iterations of keyword discovery, but do not require
labeled data. For COVID-19, we build two sets of regular expressions: (1) for
66 languages, with 99% precision and recall >50%, (2) for the 11 most common
languages, with precision >90% and recall >90%. Regular expressions enable
low-latency queries from multiple platforms. Response to challenges like
COVID-19 is fast and so are revisions. Comparisons to a DNN classifier show
explainable results, higher precision and recall, and less overfitting. Our
learnings can be applied to other narrow-topic classifiers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Markov_I/0/1/0/all/0/1"&gt;Igor L. Markov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jacqueline Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vagner_A/0/1/0/all/0/1"&gt;Adam Vagner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MERLOT: Multimodal Neural Script Knowledge Models. (arXiv:2106.02636v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02636</id>
        <link href="http://arxiv.org/abs/2106.02636"/>
        <updated>2021-06-07T03:06:11.887Z</updated>
        <summary type="html"><![CDATA[As humans, we understand events in the visual world contextually, performing
multimodal reasoning across time to make inferences about the past, present,
and future. We introduce MERLOT, a model that learns multimodal script
knowledge by watching millions of YouTube videos with transcribed speech -- in
an entirely label-free, self-supervised manner. By pretraining with a mix of
both frame-level (spatial) and video-level (temporal) objectives, our model not
only learns to match images to temporally corresponding words, but also to
contextualize what is happening globally over time. As a result, MERLOT
exhibits strong out-of-the-box representations of temporal commonsense, and
achieves state-of-the-art performance on 12 different video QA datasets when
finetuned. It also transfers well to the world of static images, allowing
models to reason about the dynamic context behind visual scenes. On Visual
Commonsense Reasoning, MERLOT answers questions correctly with 80.6% accuracy,
outperforming state-of-the-art models of similar size by over 3%, even those
that make heavy use of auxiliary supervised data (like object bounding boxes).

Ablation analyses demonstrate the complementary importance of: 1) training on
videos versus static images; 2) scaling the magnitude and diversity of the
pretraining video corpus; and 3) using diverse objectives that encourage
full-stack multimodal reasoning, from the recognition to cognition level.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zellers_R/0/1/0/all/0/1"&gt;Rowan Zellers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1"&gt;Ximing Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hessel_J/0/1/0/all/0/1"&gt;Jack Hessel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1"&gt;Youngjae Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1"&gt;Jae Sung Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1"&gt;Jize Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Farhadi_A/0/1/0/all/0/1"&gt;Ali Farhadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1"&gt;Yejin Choi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Annotation Curricula to Implicitly Train Non-Expert Annotators. (arXiv:2106.02382v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02382</id>
        <link href="http://arxiv.org/abs/2106.02382"/>
        <updated>2021-06-07T03:06:11.880Z</updated>
        <summary type="html"><![CDATA[Annotation studies often require annotators to familiarize themselves with
the task, its annotation scheme, and the data domain. This can be overwhelming
in the beginning, mentally taxing, and induce errors into the resulting
annotations; especially in citizen science or crowd sourcing scenarios where
domain expertise is not required and only annotation guidelines are provided.
To alleviate these issues, we propose annotation curricula, a novel approach to
implicitly train annotators. Our goal is to gradually introduce annotators into
the task by ordering instances that are annotated according to a learning
curriculum. To do so, we first formalize annotation curricula for sentence- and
paragraph-level annotation tasks, define an ordering strategy, and identify
well-performing heuristics and interactively trained models on three existing
English datasets. We then conduct a user study with 40 voluntary participants
who are asked to identify the most fitting misconception for English tweets
about the Covid-19 pandemic. Our results show that using a simple heuristic to
order instances can already significantly reduce the total annotation time
while preserving a high annotation quality. Annotation curricula thus can
provide a novel way to improve data collection. To facilitate future research,
we further share our code and data consisting of 2,400 annotations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Ji-Ung Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Klie_J/0/1/0/all/0/1"&gt;Jan-Christoph Klie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1"&gt;Iryna Gurevych&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ProofWriter: Generating Implications, Proofs, and Abductive Statements over Natural Language. (arXiv:2012.13048v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.13048</id>
        <link href="http://arxiv.org/abs/2012.13048"/>
        <updated>2021-06-07T03:06:11.872Z</updated>
        <summary type="html"><![CDATA[Transformers have been shown to emulate logical deduction over natural
language theories (logical rules expressed in natural language), reliably
assigning true/false labels to candidate implications. However, their ability
to generate implications of a theory has not yet been demonstrated, and methods
for reconstructing proofs of answers are imperfect. In this work we show that a
generative model, called ProofWriter, can reliably generate both implications
of a theory and the natural language proof(s) that support them. In particular,
iterating a 1-step implication generator results in proofs that are highly
reliable, and represent actual model decisions (rather than post-hoc
rationalizations). On the RuleTaker dataset, the accuracy of ProofWriter's
proofs exceed previous methods by +9% absolute, and in a way that generalizes
to proof depths unseen in training and on out-of-domain problems. We also show
that generative techniques can perform a type of abduction with high precision:
Given a theory and an unprovable conclusion, identify a missing fact that
allows the conclusion to be proved, along with a proof. These results
significantly improve the viability of neural methods for systematically
reasoning over natural language.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tafjord_O/0/1/0/all/0/1"&gt;Oyvind Tafjord&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mishra_B/0/1/0/all/0/1"&gt;Bhavana Dalvi Mishra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Clark_P/0/1/0/all/0/1"&gt;Peter Clark&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Do Syntactic Probes Probe Syntax? Experiments with Jabberwocky Probing. (arXiv:2106.02559v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02559</id>
        <link href="http://arxiv.org/abs/2106.02559"/>
        <updated>2021-06-07T03:06:11.866Z</updated>
        <summary type="html"><![CDATA[Analysing whether neural language models encode linguistic information has
become popular in NLP. One method of doing so, which is frequently cited to
support the claim that models like BERT encode syntax, is called probing;
probes are small supervised models trained to extract linguistic information
from another model's output. If a probe is able to predict a particular
structure, it is argued that the model whose output it is trained on must have
implicitly learnt to encode it. However, drawing a generalisation about a
model's linguistic knowledge about a specific phenomena based on what a probe
is able to learn may be problematic: in this work, we show that semantic cues
in training data means that syntactic probes do not properly isolate syntax. We
generate a new corpus of semantically nonsensical but syntactically well-formed
Jabberwocky sentences, which we use to evaluate two probes trained on normal
data. We train the probes on several popular language models (BERT, GPT, and
RoBERTa), and find that in all settings they perform worse when evaluated on
these data, for one probe by an average of 15.4 UUAS points absolute. Although
in most cases they still outperform the baselines, their lead is reduced
substantially, e.g. by 53% in the case of BERT for one probe. This begs the
question: what empirical scores constitute knowing syntax?]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Maudslay_R/0/1/0/all/0/1"&gt;Rowan Hall Maudslay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1"&gt;Ryan Cotterell&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Defending Democracy: Using Deep Learning to Identify and Prevent Misinformation. (arXiv:2106.02607v1 [cs.SI])]]></title>
        <id>http://arxiv.org/abs/2106.02607</id>
        <link href="http://arxiv.org/abs/2106.02607"/>
        <updated>2021-06-07T03:06:11.859Z</updated>
        <summary type="html"><![CDATA[The rise in online misinformation in recent years threatens democracies by
distorting authentic public discourse and causing confusion, fear, and even, in
extreme cases, violence. There is a need to understand the spread of false
content through online networks for developing interventions that disrupt
misinformation before it achieves virality. Using a Deep Bidirectional
Transformer for Language Understanding (BERT) and propagation graphs, this
study classifies and visualizes the spread of misinformation on a social media
network using publicly available Twitter data. The results confirm prior
research around user clusters and the virality of false content while improving
the precision of deep learning models for misinformation detection. The study
further demonstrates the suitability of BERT for providing a scalable model for
false information detection, which can contribute to the development of more
timely and accurate interventions to slow the spread of misinformation in
online environments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Trivedi_A/0/1/0/all/0/1"&gt;Anusua Trivedi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suhm_A/0/1/0/all/0/1"&gt;Alyssa Suhm&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mahankal_P/0/1/0/all/0/1"&gt;Prathamesh Mahankal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mukuntharaj_S/0/1/0/all/0/1"&gt;Subhiksha Mukuntharaj&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parab_M/0/1/0/all/0/1"&gt;Meghana D. Parab&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mohan_M/0/1/0/all/0/1"&gt;Malvika Mohan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berger_M/0/1/0/all/0/1"&gt;Meredith Berger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sethumadhavan_A/0/1/0/all/0/1"&gt;Arathi Sethumadhavan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jaiman_A/0/1/0/all/0/1"&gt;Ashish Jaiman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dodhia_R/0/1/0/all/0/1"&gt;Rahul Dodhia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Improved Attention for Visual Question Answering. (arXiv:2011.02164v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.02164</id>
        <link href="http://arxiv.org/abs/2011.02164"/>
        <updated>2021-06-07T03:06:11.841Z</updated>
        <summary type="html"><![CDATA[We consider the problem of Visual Question Answering (VQA). Given an image
and a free-form, open-ended, question, expressed in natural language, the goal
of VQA system is to provide accurate answer to this question with respect to
the image. The task is challenging because it requires simultaneous and
intricate understanding of both visual and textual information. Attention,
which captures intra- and inter-modal dependencies, has emerged as perhaps the
most widely used mechanism for addressing these challenges. In this paper, we
propose an improved attention-based architecture to solve VQA. We incorporate
an Attention on Attention (AoA) module within encoder-decoder framework, which
is able to determine the relation between attention results and queries.
Attention module generates weighted average for each query. On the other hand,
AoA module first generates an information vector and an attention gate using
attention results and current context; and then adds another attention to
generate final attended information by multiplying the two. We also propose
multimodal fusion module to combine both visual and textual information. The
goal of this fusion module is to dynamically decide how much information should
be considered from each modality. Extensive experiments on VQA-v2 benchmark
dataset show that our method achieves the state-of-the-art performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rahman_T/0/1/0/all/0/1"&gt;Tanzila Rahman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chou_S/0/1/0/all/0/1"&gt;Shih-Han Chou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sigal_L/0/1/0/all/0/1"&gt;Leonid Sigal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carenini_G/0/1/0/all/0/1"&gt;Giuseppe Carenini&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AdaTag: Multi-Attribute Value Extraction from Product Profiles with Adaptive Decoding. (arXiv:2106.02318v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02318</id>
        <link href="http://arxiv.org/abs/2106.02318"/>
        <updated>2021-06-07T03:06:11.834Z</updated>
        <summary type="html"><![CDATA[Automatic extraction of product attribute values is an important enabling
technology in e-Commerce platforms. This task is usually modeled using sequence
labeling architectures, with several extensions to handle multi-attribute
extraction. One line of previous work constructs attribute-specific models,
through separate decoders or entirely separate models. However, this approach
constrains knowledge sharing across different attributes. Other contributions
use a single multi-attribute model, with different techniques to embed
attribute information. But sharing the entire network parameters across all
attributes can limit the model's capacity to capture attribute-specific
characteristics. In this paper we present AdaTag, which uses adaptive decoding
to handle extraction. We parameterize the decoder with pretrained attribute
embeddings, through a hypernetwork and a Mixture-of-Experts (MoE) module. This
allows for separate, but semantically correlated, decoders to be generated on
the fly for different attributes. This approach facilitates knowledge sharing,
while maintaining the specificity of each attribute. Our experiments on a
real-world e-Commerce dataset show marked improvements over previous methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1"&gt;Jun Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zalmout_N/0/1/0/all/0/1"&gt;Nasser Zalmout&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1"&gt;Yan Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grant_C/0/1/0/all/0/1"&gt;Christan Grant&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1"&gt;Xiang Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1"&gt;Xin Luna Dong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Computer Generated Dialog with Auxiliary Loss Functions and Custom Evaluation Metrics. (arXiv:2106.02516v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02516</id>
        <link href="http://arxiv.org/abs/2106.02516"/>
        <updated>2021-06-07T03:06:11.827Z</updated>
        <summary type="html"><![CDATA[Although people have the ability to engage in vapid dialogue without effort,
this may not be a uniquely human trait. Since the 1960's researchers have been
trying to create agents that can generate artificial conversation. These
programs are commonly known as chatbots. With increasing use of neural networks
for dialog generation, some conclude that this goal has been achieved. This
research joins the quest by creating a dialog generating Recurrent Neural
Network (RNN) and by enhancing the ability of this network with auxiliary loss
functions and a beam search. Our custom loss functions achieve better cohesion
and coherence by including calculations of Maximum Mutual Information (MMI) and
entropy. We demonstrate the effectiveness of this system by using a set of
custom evaluation metrics inspired by an abundance of previous research and
based on tried-and-true principles of Natural Language Processing.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Conley_T/0/1/0/all/0/1"&gt;Thomas Conley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Clair_J/0/1/0/all/0/1"&gt;Jack St. Clair&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kalita_J/0/1/0/all/0/1"&gt;Jugal Kalita&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How Good Is NLP? A Sober Look at NLP Tasks through the Lens of Social Impact. (arXiv:2106.02359v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02359</id>
        <link href="http://arxiv.org/abs/2106.02359"/>
        <updated>2021-06-07T03:06:11.820Z</updated>
        <summary type="html"><![CDATA[Recent years have seen many breakthroughs in natural language processing
(NLP), transitioning it from a mostly theoretical field to one with many
real-world applications. Noting the rising number of applications of other
machine learning and AI techniques with pervasive societal impact, we
anticipate the rising importance of developing NLP technologies for social
good. Inspired by theories in moral philosophy and global priorities research,
we aim to promote a guideline for social good in the context of NLP. We lay the
foundations via moral philosophy's definition of social good, propose a
framework to evaluate NLP tasks' direct and indirect real-world impact, and
adopt the methodology of global priorities research to identify priority causes
for NLP research. Finally, we use our theoretical framework to provide some
practical guidelines for future NLP research for social good. Our data and
codes are available at this http URL]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1"&gt;Zhijing Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chauhan_G/0/1/0/all/0/1"&gt;Geeticka Chauhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tse_B/0/1/0/all/0/1"&gt;Brian Tse&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1"&gt;Mrinmaya Sachan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mihalcea_R/0/1/0/all/0/1"&gt;Rada Mihalcea&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Language Scaling for Universal Suggested Replies Model. (arXiv:2106.02232v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02232</id>
        <link href="http://arxiv.org/abs/2106.02232"/>
        <updated>2021-06-07T03:06:11.814Z</updated>
        <summary type="html"><![CDATA[We consider the problem of scaling automated suggested replies for Outlook
email system to multiple languages. Faced with increased compute requirements
and low resources for language expansion, we build a single universal model for
improving the quality and reducing run-time costs of our production system.
However, restricted data movement across regional centers prevents joint
training across languages. To this end, we propose a multi-task continual
learning framework, with auxiliary tasks and language adapters to learn
universal language representation across regions. The experimental results show
positive cross-lingual transfer across languages while reducing catastrophic
forgetting across regions. Our online results on real user traffic show
significant gains in CTR and characters saved, as well as 65% training cost
reduction compared with per-language models. As a consequence, we have scaled
the feature in multiple languages including low-resource markets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ying_Q/0/1/0/all/0/1"&gt;Qianlan Ying&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bajaj_P/0/1/0/all/0/1"&gt;Payal Bajaj&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deb_B/0/1/0/all/0/1"&gt;Budhaditya Deb&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yu Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1"&gt;Bojia Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shokouhi_M/0/1/0/all/0/1"&gt;Milad Shokouhi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1"&gt;Xia Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yang Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1"&gt;Daxin Jiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[You Only Compress Once: Towards Effective and Elastic BERT Compression via Exploit-Explore Stochastic Nature Gradient. (arXiv:2106.02435v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02435</id>
        <link href="http://arxiv.org/abs/2106.02435"/>
        <updated>2021-06-07T03:06:11.775Z</updated>
        <summary type="html"><![CDATA[Despite superior performance on various natural language processing tasks,
pre-trained models such as BERT are challenged by deploying on
resource-constraint devices. Most existing model compression approaches require
re-compression or fine-tuning across diverse constraints to accommodate various
hardware deployments. This practically limits the further application of model
compression. Moreover, the ineffective training and searching process of
existing elastic compression paradigms[4,27] prevents the direct migration to
BERT compression. Motivated by the necessity of efficient inference across
various constraints on BERT, we propose a novel approach, YOCO-BERT, to achieve
compress once and deploy everywhere. Specifically, we first construct a huge
search space with 10^13 architectures, which covers nearly all configurations
in BERT model. Then, we propose a novel stochastic nature gradient optimization
method to guide the generation of optimal candidate architecture which could
keep a balanced trade-off between explorations and exploitation. When a certain
resource constraint is given, a lightweight distribution optimization approach
is utilized to obtain the optimal network for target deployment without
fine-tuning. Compared with state-of-the-art algorithms, YOCO-BERT provides more
compact models, yet achieving 2.1%-4.5% average accuracy improvement on the
GLUE benchmark. Besides, YOCO-BERT is also more effective, e.g.,the training
complexity is O(1)for N different devices. Code is
availablehttps://github.com/MAC-AutoML/YOCO-BERT.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shaokun Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1"&gt;Xiawu Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1"&gt;Chenyi Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yuchao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chao_F/0/1/0/all/0/1"&gt;Fei Chao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1"&gt;Mengdi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jun Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1"&gt;Rongrong Ji&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[COINS: Dynamically Generating COntextualized Inference Rules for Narrative Story Completion. (arXiv:2106.02497v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02497</id>
        <link href="http://arxiv.org/abs/2106.02497"/>
        <updated>2021-06-07T03:06:11.767Z</updated>
        <summary type="html"><![CDATA[Despite recent successes of large pre-trained language models in solving
reasoning tasks, their inference capabilities remain opaque. We posit that such
models can be made more interpretable by explicitly generating interim
inference rules, and using them to guide the generation of task-specific
textual outputs. In this paper we present COINS, a recursive inference
framework that i) iteratively reads context sentences, ii) dynamically
generates contextualized inference rules, encodes them, and iii) uses them to
guide task-specific output generation. We apply COINS to a Narrative Story
Completion task that asks a model to complete a story with missing sentences,
to produce a coherent story with plausible logical connections, causal
relationships, and temporal dependencies. By modularizing inference and
sentence generation steps in a recurrent model, we aim to make reasoning steps
and their effects on next sentence generation transparent. Our automatic and
manual evaluations show that the model generates better story sentences than
SOTA baselines, especially in terms of coherence. We further demonstrate
improved performance over strong pre-trained LMs in generating commonsense
inference rules. The recursive nature of COINS holds the potential for
controlled generation of longer sequences.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Paul_D/0/1/0/all/0/1"&gt;Debjit Paul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frank_A/0/1/0/all/0/1"&gt;Anette Frank&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding and Countering Stereotypes: A Computational Approach to the Stereotype Content Model. (arXiv:2106.02596v1 [cs.CY])]]></title>
        <id>http://arxiv.org/abs/2106.02596</id>
        <link href="http://arxiv.org/abs/2106.02596"/>
        <updated>2021-06-07T03:06:11.759Z</updated>
        <summary type="html"><![CDATA[Stereotypical language expresses widely-held beliefs about different social
categories. Many stereotypes are overtly negative, while others may appear
positive on the surface, but still lead to negative consequences. In this work,
we present a computational approach to interpreting stereotypes in text through
the Stereotype Content Model (SCM), a comprehensive causal theory from social
psychology. The SCM proposes that stereotypes can be understood along two
primary dimensions: warmth and competence. We present a method for defining
warmth and competence axes in semantic embedding space, and show that the four
quadrants defined by this subspace accurately represent the warmth and
competence concepts, according to annotated lexicons. We then apply our
computational SCM model to textual stereotype data and show that it compares
favourably with survey-based studies in the psychological literature.
Furthermore, we explore various strategies to counter stereotypical beliefs
with anti-stereotypes. It is known that countering stereotypes with
anti-stereotypical examples is one of the most effective ways to reduce biased
thinking, yet the problem of generating anti-stereotypes has not been
previously studied. Thus, a better understanding of how to generate realistic
and effective anti-stereotypes can contribute to addressing pressing societal
concerns of stereotyping, prejudice, and discrimination.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fraser_K/0/1/0/all/0/1"&gt;Kathleen C. Fraser&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nejadgholi_I/0/1/0/all/0/1"&gt;Isar Nejadgholi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kiritchenko_S/0/1/0/all/0/1"&gt;Svetlana Kiritchenko&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bi-Granularity Contrastive Learning for Post-Training in Few-Shot Scene. (arXiv:2106.02327v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02327</id>
        <link href="http://arxiv.org/abs/2106.02327"/>
        <updated>2021-06-07T03:06:11.749Z</updated>
        <summary type="html"><![CDATA[The major paradigm of applying a pre-trained language model to downstream
tasks is to fine-tune it on labeled task data, which often suffers instability
and low performance when the labeled examples are scarce.~One way to alleviate
this problem is to apply post-training on unlabeled task data before
fine-tuning, adapting the pre-trained model to target domains by contrastive
learning that considers either token-level or sequence-level similarity.
Inspired by the success of sequence masking, we argue that both token-level and
sequence-level similarities can be captured with a pair of masked
sequences.~Therefore, we propose complementary random masking (CRM) to generate
a pair of masked sequences from an input sequence for sequence-level
contrastive learning and then develop contrastive masked language modeling
(CMLM) for post-training to integrate both token-level and sequence-level
contrastive learnings.~Empirical results show that CMLM surpasses several
recent post-training methods in few-shot settings without the need for data
augmentation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Luo_R/0/1/0/all/0/1"&gt;Ruikun Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1"&gt;Guanhuan Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Quan_X/0/1/0/all/0/1"&gt;Xiaojun Quan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ERICA: An Empathetic Android Companion for Covid-19 Quarantine. (arXiv:2106.02325v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02325</id>
        <link href="http://arxiv.org/abs/2106.02325"/>
        <updated>2021-06-07T03:06:11.742Z</updated>
        <summary type="html"><![CDATA[Over the past year, research in various domains, including Natural Language
Processing (NLP), has been accelerated to fight against the COVID-19 pandemic,
yet such research has just started on dialogue systems. In this paper, we
introduce an end-to-end dialogue system which aims to ease the isolation of
people under self-quarantine. We conduct a control simulation experiment to
assess the effects of the user interface, a web-based virtual agent called Nora
vs. the android ERICA via a video call. The experimental results show that the
android offers a more valuable user experience by giving the impression of
being more empathetic and engaging in the conversation due to its nonverbal
information, such as facial expressions and body gestures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ishii_E/0/1/0/all/0/1"&gt;Etsuko Ishii&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Winata_G/0/1/0/all/0/1"&gt;Genta Indra Winata&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cahyawijaya_S/0/1/0/all/0/1"&gt;Samuel Cahyawijaya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lala_D/0/1/0/all/0/1"&gt;Divesh Lala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kawahara_T/0/1/0/all/0/1"&gt;Tatsuya Kawahara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fung_P/0/1/0/all/0/1"&gt;Pascale Fung&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AdvPicker: Effectively Leveraging Unlabeled Data via Adversarial Discriminator for Cross-Lingual NER. (arXiv:2106.02300v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02300</id>
        <link href="http://arxiv.org/abs/2106.02300"/>
        <updated>2021-06-07T03:06:11.684Z</updated>
        <summary type="html"><![CDATA[Neural methods have been shown to achieve high performance in Named Entity
Recognition (NER), but rely on costly high-quality labeled data for training,
which is not always available across languages. While previous works have shown
that unlabeled data in a target language can be used to improve cross-lingual
model performance, we propose a novel adversarial approach (AdvPicker) to
better leverage such data and further improve results. We design an adversarial
learning framework in which an encoder learns entity domain knowledge from
labeled source-language data and better shared features are captured via
adversarial training - where a discriminator selects less language-dependent
target-language data via similarity to the source language. Experimental
results on standard benchmark datasets well demonstrate that the proposed
method benefits strongly from this data selection process and outperforms
existing state-of-the-art methods; without requiring any additional external
resources (e.g., gazetteers or via machine translation).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1"&gt;Weile Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1"&gt;Huiqiang Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1"&gt;Qianhui Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karlsson_B/0/1/0/all/0/1"&gt;B&amp;#xf6;rje F. Karlsson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guan_Y/0/1/0/all/0/1"&gt;Yi Guan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Slice-Aware Representations with Mixture of Attentions. (arXiv:2106.02363v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02363</id>
        <link href="http://arxiv.org/abs/2106.02363"/>
        <updated>2021-06-07T03:06:11.669Z</updated>
        <summary type="html"><![CDATA[Real-world machine learning systems are achieving remarkable performance in
terms of coarse-grained metrics like overall accuracy and F-1 score. However,
model improvement and development often require fine-grained modeling on
individual data subsets or slices, for instance, the data slices where the
models have unsatisfactory results. In practice, it gives tangible values for
developing such models that can pay extra attention to critical or interested
slices while retaining the original overall performance. This work extends the
recent slice-based learning (SBL)~\cite{chen2019slice} with a mixture of
attentions (MoA) to learn slice-aware dual attentive representations. We
empirically show that the MoA approach outperforms the baseline method as well
as the original SBL approach on monitored slices with two natural language
understanding (NLU) tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Cheng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1"&gt;Sungjin Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1"&gt;Sunghyun Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Han Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1"&gt;Young-Bum Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sarikaya_R/0/1/0/all/0/1"&gt;Ruhi Sarikaya&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cross-language Sentence Selection via Data Augmentation and Rationale Training. (arXiv:2106.02293v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02293</id>
        <link href="http://arxiv.org/abs/2106.02293"/>
        <updated>2021-06-07T03:06:11.661Z</updated>
        <summary type="html"><![CDATA[This paper proposes an approach to cross-language sentence selection in a
low-resource setting. It uses data augmentation and negative sampling
techniques on noisy parallel sentence data to directly learn a cross-lingual
embedding-based query relevance model. Results show that this approach performs
as well as or better than multiple state-of-the-art machine translation +
monolingual retrieval systems trained on the same parallel data. Moreover, when
a rationale training secondary objective is applied to encourage the model to
match word alignment hints from a phrase-based statistical machine translation
model, consistent improvements are seen across three language pairs
(English-Somali, English-Swahili and English-Tagalog) over a variety of
state-of-the-art baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yanda Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kedzie_C/0/1/0/all/0/1"&gt;Chris Kedzie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nair_S/0/1/0/all/0/1"&gt;Suraj Nair&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Galuscakova_P/0/1/0/all/0/1"&gt;Petra Galu&amp;#x161;&amp;#x10d;&amp;#xe1;kov&amp;#xe1;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Rui Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oard_D/0/1/0/all/0/1"&gt;Douglas W. Oard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McKeown_K/0/1/0/all/0/1"&gt;Kathleen McKeown&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Modeling the Unigram Distribution. (arXiv:2106.02289v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02289</id>
        <link href="http://arxiv.org/abs/2106.02289"/>
        <updated>2021-06-07T03:06:11.515Z</updated>
        <summary type="html"><![CDATA[The unigram distribution is the non-contextual probability of finding a
specific word form in a corpus. While of central importance to the study of
language, it is commonly approximated by each word's sample frequency in the
corpus. This approach, being highly dependent on sample size, assigns zero
probability to any out-of-vocabulary (oov) word form. As a result, it produces
negatively biased probabilities for any oov word form, while positively biased
probabilities to in-corpus words. In this work, we argue in favor of properly
modeling the unigram distribution -- claiming it should be a central task in
natural language processing. With this in mind, we present a novel model for
estimating it in a language (a neuralization of Goldwater et al.'s (2011)
model) and show it produces much better estimates across a diverse set of 7
languages than the na\"ive use of neural character-level language models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nikkarinen_I/0/1/0/all/0/1"&gt;Irene Nikkarinen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pimentel_T/0/1/0/all/0/1"&gt;Tiago Pimentel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Blasi_D/0/1/0/all/0/1"&gt;Dami&amp;#xe1;n E. Blasi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1"&gt;Ryan Cotterell&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AgreeSum: Agreement-Oriented Multi-Document Summarization. (arXiv:2106.02278v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02278</id>
        <link href="http://arxiv.org/abs/2106.02278"/>
        <updated>2021-06-07T03:06:11.483Z</updated>
        <summary type="html"><![CDATA[We aim to renew interest in a particular multi-document summarization (MDS)
task which we call AgreeSum: agreement-oriented multi-document summarization.
Given a cluster of articles, the goal is to provide abstractive summaries that
represent information common and faithful to all input articles. Given the lack
of existing datasets, we create a dataset for AgreeSum, and provide annotations
on article-summary entailment relations for a subset of the clusters in the
dataset. We aim to create strong baselines for the task by applying the
top-performing pretrained single-document summarization model PEGASUS onto
AgreeSum, leveraging both annotated clusters by supervised losses, and
unannotated clusters by T5-based entailment-related and language-related
losses. Compared to other baselines, both automatic evaluation and human
evaluation show better article-summary and cluster-summary entailment in
generated summaries. On a separate note, we hope that our article-summary
entailment annotations contribute to the community's effort in improving
abstractive summarization faithfulness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pang_R/0/1/0/all/0/1"&gt;Richard Yuanzhe Pang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lelkes_A/0/1/0/all/0/1"&gt;Adam D. Lelkes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tran_V/0/1/0/all/0/1"&gt;Vinh Q. Tran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1"&gt;Cong Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ERNIE-Tiny : A Progressive Distillation Framework for Pretrained Transformer Compression. (arXiv:2106.02241v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02241</id>
        <link href="http://arxiv.org/abs/2106.02241"/>
        <updated>2021-06-07T03:06:11.369Z</updated>
        <summary type="html"><![CDATA[Pretrained language models (PLMs) such as BERT adopt a training paradigm
which first pretrain the model in general data and then finetune the model on
task-specific data, and have recently achieved great success. However, PLMs are
notorious for their enormous parameters and hard to be deployed on real-life
applications. Knowledge distillation has been prevailing to address this
problem by transferring knowledge from a large teacher to a much smaller
student over a set of data. We argue that the selection of thee three key
components, namely teacher, training data, and learning objective, is crucial
to the effectiveness of distillation. We, therefore, propose a four-stage
progressive distillation framework ERNIE-Tiny to compress PLM, which varies the
three components gradually from general level to task-specific level.
Specifically, the first stage, General Distillation, performs distillation with
guidance from pretrained teacher, gerenal data and latent distillation loss.
Then, General-Enhanced Distillation changes teacher model from pretrained
teacher to finetuned teacher. After that, Task-Adaptive Distillation shifts
training data from general data to task-specific data. In the end,
Task-Specific Distillation, adds two additional losses, namely Soft-Label and
Hard-Label loss onto the last stage. Empirical results demonstrate the
effectiveness of our framework and generalization gain brought by ERNIE-Tiny.In
particular, experiments show that a 4-layer ERNIE-Tiny maintains over
98.0%performance of its 12-layer teacher BERT base on GLUE benchmark,
surpassing state-of-the-art (SOTA) by 1.0% GLUE score with the same amount of
parameters. Moreover, ERNIE-Tiny achieves a new compression SOTA on five
Chinese NLP tasks, outperforming BERT base by 0.4% accuracy with 7.5x fewer
parameters and9.4x faster inference speed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Su_W/0/1/0/all/0/1"&gt;Weiyue Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xuyi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1"&gt;Shikun Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jiaxiang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Weixin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1"&gt;Yu Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_H/0/1/0/all/0/1"&gt;Hao Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1"&gt;Hua Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Haifeng Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Minimum Word Error Rate Training with Language Model Fusion for End-to-End Speech Recognition. (arXiv:2106.02302v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2106.02302</id>
        <link href="http://arxiv.org/abs/2106.02302"/>
        <updated>2021-06-07T03:06:11.362Z</updated>
        <summary type="html"><![CDATA[Integrating external language models (LMs) into end-to-end (E2E) models
remains a challenging task for domain-adaptive speech recognition. Recently,
internal language model estimation (ILME)-based LM fusion has shown significant
word error rate (WER) reduction from Shallow Fusion by subtracting a weighted
internal LM score from an interpolation of E2E model and external LM scores
during beam search. However, on different test sets, the optimal LM
interpolation weights vary over a wide range and have to be tuned extensively
on well-matched validation sets. In this work, we perform LM fusion in the
minimum WER (MWER) training of an E2E model to obviate the need for LM weights
tuning during inference. Besides MWER training with Shallow Fusion (MWER-SF),
we propose a novel MWER training with ILME (MWER-ILME) where the ILME-based
fusion is conducted to generate N-best hypotheses and their posteriors.
Additional gradient is induced when internal LM is engaged in MWER-ILME loss
computation. During inference, LM weights pre-determined in MWER training
enable robust LM integrations on test sets from different domains. Experimented
with 30K-hour trained transformer transducers, MWER-ILME achieves on average
8.8% and 5.8% relative WER reductions from MWER and MWER-SF training,
respectively, on 6 different test sets]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Meng_Z/0/1/0/all/0/1"&gt;Zhong Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yu Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kanda_N/0/1/0/all/0/1"&gt;Naoyuki Kanda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lu_L/0/1/0/all/0/1"&gt;Liang Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xie Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ye_G/0/1/0/all/0/1"&gt;Guoli Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sun_E/0/1/0/all/0/1"&gt;Eric Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1"&gt;Jinyu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gong_Y/0/1/0/all/0/1"&gt;Yifan Gong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How to Adapt Your Pretrained Multilingual Model to 1600 Languages. (arXiv:2106.02124v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02124</id>
        <link href="http://arxiv.org/abs/2106.02124"/>
        <updated>2021-06-07T03:06:11.353Z</updated>
        <summary type="html"><![CDATA[Pretrained multilingual models (PMMs) enable zero-shot learning via
cross-lingual transfer, performing best for languages seen during pretraining.
While methods exist to improve performance for unseen languages, they have
almost exclusively been evaluated using amounts of raw text only available for
a small fraction of the world's languages. In this paper, we evaluate the
performance of existing methods to adapt PMMs to new languages using a resource
available for over 1600 languages: the New Testament. This is challenging for
two reasons: (1) the small corpus size, and (2) the narrow domain. While
performance drops for all approaches, we surprisingly still see gains of up to
$17.69\%$ accuracy for part-of-speech tagging and $6.29$ F1 for NER on average
over all languages as compared to XLM-R. Another unexpected finding is that
continued pretraining, the simplest approach, performs best. Finally, we
perform a case study to disentangle the effects of domain and size and to shed
light on the influence of the finetuning source language.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ebrahimi_A/0/1/0/all/0/1"&gt;Abteen Ebrahimi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kann_K/0/1/0/all/0/1"&gt;Katharina Kann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NAST: A Non-Autoregressive Generator with Word Alignment for Unsupervised Text Style Transfer. (arXiv:2106.02210v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02210</id>
        <link href="http://arxiv.org/abs/2106.02210"/>
        <updated>2021-06-07T03:06:11.343Z</updated>
        <summary type="html"><![CDATA[Autoregressive models have been widely used in unsupervised text style
transfer. Despite their success, these models still suffer from the content
preservation problem that they usually ignore part of the source sentence and
generate some irrelevant words with strong styles. In this paper, we propose a
Non-Autoregressive generator for unsupervised text Style Transfer (NAST), which
alleviates the problem from two aspects. First, we observe that most words in
the transferred sentence can be aligned with related words in the source
sentence, so we explicitly model word alignments to suppress irrelevant words.
Second, existing models trained with the cycle loss align sentences in two
stylistic text spaces, which lacks fine-grained control at the word level. The
proposed non-autoregressive generator focuses on the connections between
aligned words, which learns the word-level transfer between styles. For
experiments, we integrate the proposed generator into two base models and
evaluate them on two style transfer tasks. The results show that NAST can
significantly improve the overall performance and provide explainable word
alignments. Moreover, the non-autoregressive generator achieves over 10x
speedups at inference. Our codes are available at
https://github.com/thu-coai/NAST.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1"&gt;Fei Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zikai Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1"&gt;Chen Henry Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1"&gt;Qihan Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1"&gt;Xiaoyan Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1"&gt;Minlie Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dutch Named Entity Recognition and De-identification Methods for the Human Resource Domain. (arXiv:2106.02287v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02287</id>
        <link href="http://arxiv.org/abs/2106.02287"/>
        <updated>2021-06-07T03:06:11.323Z</updated>
        <summary type="html"><![CDATA[The human resource (HR) domain contains various types of privacy-sensitive
textual data, such as e-mail correspondence and performance appraisal. Doing
research on these documents brings several challenges, one of them
anonymisation. In this paper, we evaluate the current Dutch text
de-identification methods for the HR domain in four steps. First, by updating
one of these methods with the latest named entity recognition (NER) models. The
result is that the NER model based on the CoNLL 2002 corpus in combination with
the BERTje transformer give the best combination for suppressing persons
(recall 0.94) and locations (recall 0.82). For suppressing gender, DEDUCE is
performing best (recall 0.53). Second NER evaluation is based on both strict
de-identification of entities (a person must be suppressed as a person) and
third evaluation on a loose sense of de-identification (no matter what how a
person is suppressed, as long it is suppressed). In the fourth and last step a
new kind of NER dataset is tested for recognising job titles in texts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Toledo_C/0/1/0/all/0/1"&gt;Cha&amp;#xef;m van Toledo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dijk_F/0/1/0/all/0/1"&gt;Friso van Dijk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Spruit_M/0/1/0/all/0/1"&gt;Marco Spruit&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Human-Adversarial Visual Question Answering. (arXiv:2106.02280v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02280</id>
        <link href="http://arxiv.org/abs/2106.02280"/>
        <updated>2021-06-07T03:06:11.315Z</updated>
        <summary type="html"><![CDATA[Performance on the most commonly used Visual Question Answering dataset (VQA
v2) is starting to approach human accuracy. However, in interacting with
state-of-the-art VQA models, it is clear that the problem is far from being
solved. In order to stress test VQA models, we benchmark them against
human-adversarial examples. Human subjects interact with a state-of-the-art VQA
model, and for each image in the dataset, attempt to find a question where the
model's predicted answer is incorrect. We find that a wide range of
state-of-the-art models perform poorly when evaluated on these examples. We
conduct an extensive analysis of the collected adversarial examples and provide
guidance on future research directions. We hope that this Adversarial VQA
(AdVQA) benchmark can help drive progress in the field and advance the state of
the art.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sheng_S/0/1/0/all/0/1"&gt;Sasha Sheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1"&gt;Amanpreet Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goswami_V/0/1/0/all/0/1"&gt;Vedanuj Goswami&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Magana_J/0/1/0/all/0/1"&gt;Jose Alberto Lopez Magana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Galuba_W/0/1/0/all/0/1"&gt;Wojciech Galuba&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parikh_D/0/1/0/all/0/1"&gt;Devi Parikh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kiela_D/0/1/0/all/0/1"&gt;Douwe Kiela&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Conversations Are Not Flat: Modeling the Dynamic Information Flow across Dialogue Utterances. (arXiv:2106.02227v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02227</id>
        <link href="http://arxiv.org/abs/2106.02227"/>
        <updated>2021-06-07T03:06:11.305Z</updated>
        <summary type="html"><![CDATA[Nowadays, open-domain dialogue models can generate acceptable responses
according to the historical context based on the large-scale pre-trained
language models. However, they generally concatenate the dialogue history
directly as the model input to predict the response, which we named as the flat
pattern and ignores the dynamic information flow across dialogue utterances. In
this work, we propose the DialoFlow model, in which we introduce a dynamic flow
mechanism to model the context flow, and design three training objectives to
capture the information dynamics across dialogue utterances by addressing the
semantic influence brought about by each utterance in large-scale pre-training.
Experiments on the multi-reference Reddit Dataset and DailyDialog Dataset
demonstrate that our DialoFlow significantly outperforms the DialoGPT on the
dialogue generation task. Besides, we propose the Flow score, an effective
automatic metric for evaluating interactive human-bot conversation quality
based on the pre-trained DialoFlow, which presents high chatbot-level
correlation ($r=0.9$) with human ratings among 11 chatbots. Code and
pre-trained models will be public.
\footnote{\url{https://github.com/ictnlp/DialoFlow}}]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zekang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jinchao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fei_Z/0/1/0/all/0/1"&gt;Zhengcong Fei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1"&gt;Yang Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jie Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Decoupled Dialogue Modeling and Semantic Parsing for Multi-Turn Text-to-SQL. (arXiv:2106.02282v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02282</id>
        <link href="http://arxiv.org/abs/2106.02282"/>
        <updated>2021-06-07T03:06:11.294Z</updated>
        <summary type="html"><![CDATA[Recently, Text-to-SQL for multi-turn dialogue has attracted great interest.
Here, the user input of the current turn is parsed into the corresponding SQL
query of the appropriate database, given all previous dialogue history. Current
approaches mostly employ end-to-end models and consequently face two
challenges. First, dialogue history modeling and Text-to-SQL parsing are
implicitly combined, hence it is hard to carry out interpretable analysis and
obtain targeted improvement. Second, SQL annotation of multi-turn dialogue is
very expensive, leading to training data sparsity. In this paper, we propose a
novel decoupled multi-turn Text-to-SQL framework, where an utterance rewrite
model first explicitly solves completion of dialogue context, and then a
single-turn Text-to-SQL parser follows. A dual learning approach is also
proposed for the utterance rewrite model to address the data sparsity problem.
Compared with end-to-end approaches, the proposed decoupled method can achieve
excellent performance without any annotated in-domain data. With just a few
annotated rewrite cases, the decoupled method outperforms the released
state-of-the-art end-to-end models on both SParC and CoSQL datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lu Chen Hanqi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_R/0/1/0/all/0/1"&gt;Ruisheng Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_D/0/1/0/all/0/1"&gt;Da Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1"&gt;Mengyue Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_K/0/1/0/all/0/1"&gt;Kai Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Syntax-augmented Multilingual BERT for Cross-lingual Transfer. (arXiv:2106.02134v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02134</id>
        <link href="http://arxiv.org/abs/2106.02134"/>
        <updated>2021-06-07T03:06:11.281Z</updated>
        <summary type="html"><![CDATA[In recent years, we have seen a colossal effort in pre-training multilingual
text encoders using large-scale corpora in many languages to facilitate
cross-lingual transfer learning. However, due to typological differences across
languages, the cross-lingual transfer is challenging. Nevertheless, language
syntax, e.g., syntactic dependencies, can bridge the typological gap. Previous
works have shown that pre-trained multilingual encoders, such as mBERT
\cite{devlin-etal-2019-bert}, capture language syntax, helping cross-lingual
transfer. This work shows that explicitly providing language syntax and
training mBERT using an auxiliary objective to encode the universal dependency
tree structure helps cross-lingual transfer. We perform rigorous experiments on
four NLP tasks, including text classification, question answering, named entity
recognition, and task-oriented semantic parsing. The experiment results show
that syntax-augmented mBERT improves cross-lingual transfer on popular
benchmarks, such as PAWS-X and MLQA, by 1.4 and 1.6 points on average across
all languages. In the \emph{generalized} transfer setting, the performance
boosted significantly, with 3.9 and 3.1 points on average in PAWS-X and MLQA.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ahmad_W/0/1/0/all/0/1"&gt;Wasi Uddin Ahmad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Haoran Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1"&gt;Kai-Wei Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mehdad_Y/0/1/0/all/0/1"&gt;Yashar Mehdad&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scalable Transformers for Neural Machine Translation. (arXiv:2106.02242v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02242</id>
        <link href="http://arxiv.org/abs/2106.02242"/>
        <updated>2021-06-07T03:06:11.262Z</updated>
        <summary type="html"><![CDATA[Transformer has been widely adopted in Neural Machine Translation (NMT)
because of its large capacity and parallel training of sequence generation.
However, the deployment of Transformer is challenging because different
scenarios require models of different complexities and scales. Naively training
multiple Transformers is redundant in terms of both computation and memory. In
this paper, we propose a novel scalable Transformers, which naturally contains
sub-Transformers of different scales and have shared parameters. Each
sub-Transformer can be easily obtained by cropping the parameters of the
largest Transformer. A three-stage training scheme is proposed to tackle the
difficulty of training the scalable Transformers, which introduces additional
supervisions from word-level and sequence-level self-distillation. Extensive
experiments were conducted on WMT EN-De and En-Fr to validate our proposed
scalable Transformers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1"&gt;Peng Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geng_S/0/1/0/all/0/1"&gt;Shijie Geng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaogang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1"&gt;Jifeng Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Hongsheng Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Segmental Contrastive Predictive Coding for Unsupervised Word Segmentation. (arXiv:2106.02170v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2106.02170</id>
        <link href="http://arxiv.org/abs/2106.02170"/>
        <updated>2021-06-07T03:06:11.254Z</updated>
        <summary type="html"><![CDATA[Automatic detection of phoneme or word-like units is one of the core
objectives in zero-resource speech processing. Recent attempts employ
self-supervised training methods, such as contrastive predictive coding (CPC),
where the next frame is predicted given past context. However, CPC only looks
at the audio signal's frame-level structure. We overcome this limitation with a
segmental contrastive predictive coding (SCPC) framework that can model the
signal structure at a higher level e.g. at the phoneme level. In this
framework, a convolutional neural network learns frame-level representation
from the raw waveform via noise-contrastive estimation (NCE). A differentiable
boundary detector finds variable-length segments, which are then used to
optimize a segment encoder via NCE to learn segment representations. The
differentiable boundary detector allows us to train frame-level and
segment-level encoders jointly. Typically, phoneme and word segmentation are
treated as separate tasks. We unify them and experimentally show that our
single model outperforms existing phoneme and word segmentation methods on
TIMIT and Buckeye datasets. We analyze the impact of boundary threshold and
when is the right time to include the segmental loss in the learning process.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Bhati_S/0/1/0/all/0/1"&gt;Saurabhchand Bhati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Villalba_J/0/1/0/all/0/1"&gt;Jes&amp;#xfa;s Villalba&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zelasko_P/0/1/0/all/0/1"&gt;Piotr &amp;#x17b;elasko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Moro_Velazquez_L/0/1/0/all/0/1"&gt;Laureano Moro-Velazquez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dehak_N/0/1/0/all/0/1"&gt;Najim Dehak&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BERTTune: Fine-Tuning Neural Machine Translation with BERTScore. (arXiv:2106.02208v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02208</id>
        <link href="http://arxiv.org/abs/2106.02208"/>
        <updated>2021-06-07T03:06:11.245Z</updated>
        <summary type="html"><![CDATA[Neural machine translation models are often biased toward the limited
translation references seen during training. To amend this form of overfitting,
in this paper we propose fine-tuning the models with a novel training objective
based on the recently-proposed BERTScore evaluation metric. BERTScore is a
scoring function based on contextual embeddings that overcomes the typical
limitations of n-gram-based metrics (e.g. synonyms, paraphrases), allowing
translations that are different from the references, yet close in the
contextual embedding space, to be treated as substantially correct. To be able
to use BERTScore as a training objective, we propose three approaches for
generating soft predictions, allowing the network to remain completely
differentiable end-to-end. Experiments carried out over four, diverse language
pairs have achieved improvements of up to 0.58 pp (3.28%) in BLEU score and up
to 0.76 pp (0.98%) in BERTScore (F_BERT) when fine-tuning a strong baseline.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Unanue_I/0/1/0/all/0/1"&gt;Inigo Jauregi Unanue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parnell_J/0/1/0/all/0/1"&gt;Jacob Parnell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Piccardi_M/0/1/0/all/0/1"&gt;Massimo Piccardi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Language Embeddings for Typology and Cross-lingual Transfer Learning. (arXiv:2106.02082v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02082</id>
        <link href="http://arxiv.org/abs/2106.02082"/>
        <updated>2021-06-07T03:06:11.236Z</updated>
        <summary type="html"><![CDATA[Cross-lingual language tasks typically require a substantial amount of
annotated data or parallel translation data. We explore whether language
representations that capture relationships among languages can be learned and
subsequently leveraged in cross-lingual tasks without the use of parallel data.
We generate dense embeddings for 29 languages using a denoising autoencoder,
and evaluate the embeddings using the World Atlas of Language Structures (WALS)
and two extrinsic tasks in a zero-shot setting: cross-lingual dependency
parsing and cross-lingual natural language inference.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1"&gt;Dian Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1"&gt;Taiqi He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sagae_K/0/1/0/all/0/1"&gt;Kenji Sagae&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Grounding 'Grounding' in NLP. (arXiv:2106.02192v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02192</id>
        <link href="http://arxiv.org/abs/2106.02192"/>
        <updated>2021-06-07T03:06:11.226Z</updated>
        <summary type="html"><![CDATA[The NLP community has seen substantial recent interest in grounding to
facilitate interaction between language technologies and the world. However, as
a community, we use the term broadly to reference any linking of text to data
or non-textual modality. In contrast, Cognitive Science more formally defines
"grounding" as the process of establishing what mutual information is required
for successful communication between two interlocutors -- a definition which
might implicitly capture the NLP usage but differs in intent and scope. We
investigate the gap between these definitions and seek answers to the following
questions: (1) What aspects of grounding are missing from NLP tasks? Here we
present the dimensions of coordination, purviews and constraints. (2) How is
the term "grounding" used in the current research? We study the trends in
datasets, domains, and tasks introduced in recent NLP conferences. And finally,
(3) How to advance our current definition to bridge the gap with Cognitive
Science? We present ways to both create new tasks or repurpose existing ones to
make advancements towards achieving a more complete sense of grounding.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chandu_K/0/1/0/all/0/1"&gt;Khyathi Raghavi Chandu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bisk_Y/0/1/0/all/0/1"&gt;Yonatan Bisk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Black_A/0/1/0/all/0/1"&gt;Alan W Black&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Addressing Inquiries about History: An Efficient and Practical Framework for Evaluating Open-domain Chatbot Consistency. (arXiv:2106.02228v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02228</id>
        <link href="http://arxiv.org/abs/2106.02228"/>
        <updated>2021-06-07T03:06:11.204Z</updated>
        <summary type="html"><![CDATA[A good open-domain chatbot should avoid presenting contradictory responses
about facts or opinions in a conversational session, known as its consistency
capacity. However, evaluating the consistency capacity of a chatbot is still
challenging. Employing human judges to interact with chatbots on purpose to
check their capacities is costly and low-efficient, and difficult to get rid of
subjective bias. In this paper, we propose the Addressing Inquiries about
History (AIH), an efficient and practical framework for the consistency
evaluation. At the conversation stage, AIH attempts to address appropriate
inquiries about the dialogue history to induce the chatbot to redeclare the
historical facts or opinions. We carry out the conversation between chatbots,
which is more efficient than the human-bot interaction and can also alleviate
the subjective bias. In this way, we manage to rapidly obtain a dialog session
that contains responses with high contradiction possibilities. At the
contradiction recognition stage, we can either employ human judges or a natural
language inference (NLI) model to recognize whether the answers to the
inquiries are contradictory with history. Finally, we are able to rank chatbots
according to the contradiction statistics. Experiments on open-domain chatbots
show that our approach can efficiently and reliably assess the consistency
capacity of chatbots and achieve a high ranking correlation with the human
evaluation. We release the framework and hope to help improve the consistency
capacity of chatbots. \footnote{\url{https://github.com/ictnlp/AIH}}]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zekang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jinchao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fei_Z/0/1/0/all/0/1"&gt;Zhengcong Fei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1"&gt;Yang Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jie Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-supervised Dialogue Learning for Spoken Conversational Question Answering. (arXiv:2106.02182v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02182</id>
        <link href="http://arxiv.org/abs/2106.02182"/>
        <updated>2021-06-07T03:06:11.194Z</updated>
        <summary type="html"><![CDATA[In spoken conversational question answering (SCQA), the answer to the
corresponding question is generated by retrieving and then analyzing a fixed
spoken document, including multi-part conversations. Most SCQA systems have
considered only retrieving information from ordered utterances. However, the
sequential order of dialogue is important to build a robust spoken
conversational question answering system, and the changes of utterances order
may severely result in low-quality and incoherent corpora. To this end, we
introduce a self-supervised learning approach, including incoherence
discrimination, insertion detection, and question prediction, to explicitly
capture the coreference resolution and dialogue coherence among spoken
documents. Specifically, we design a joint learning framework where the
auxiliary self-supervised tasks can enable the pre-trained SCQA systems towards
more coherent and meaningful spoken dialogue learning. We also utilize the
proposed self-supervised learning tasks to capture intra-sentence coherence.
Experimental results demonstrate that our proposed method provides more
coherent, meaningful, and appropriate responses, yielding superior performance
gains compared to the original pre-trained language models. Our method achieves
state-of-the-art results on the Spoken-CoQA dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1"&gt;Nuo Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1"&gt;Chenyu You&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1"&gt;Yuexian Zou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[nmT5 -- Is parallel data still relevant for pre-training massively multilingual language models?. (arXiv:2106.02171v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02171</id>
        <link href="http://arxiv.org/abs/2106.02171"/>
        <updated>2021-06-07T03:06:11.185Z</updated>
        <summary type="html"><![CDATA[Recently, mT5 - a massively multilingual version of T5 - leveraged a unified
text-to-text format to attain state-of-the-art results on a wide variety of
multilingual NLP tasks. In this paper, we investigate the impact of
incorporating parallel data into mT5 pre-training. We find that multi-tasking
language modeling with objectives such as machine translation during
pre-training is a straightforward way to improve performance on downstream
multilingual and cross-lingual tasks. However, the gains start to diminish as
the model capacity increases, suggesting that parallel data might not be as
essential for larger models. At the same time, even at larger model sizes, we
find that pre-training with parallel data still provides benefits in the
limited labelled data regime.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kale_M/0/1/0/all/0/1"&gt;Mihir Kale&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Siddhant_A/0/1/0/all/0/1"&gt;Aditya Siddhant&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Constant_N/0/1/0/all/0/1"&gt;Noah Constant&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Johnson_M/0/1/0/all/0/1"&gt;Melvin Johnson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Al_Rfou_R/0/1/0/all/0/1"&gt;Rami Al-Rfou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xue_L/0/1/0/all/0/1"&gt;Linting Xue&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Contrastive Learning for Debiased Candidate Generation in Large-Scale Recommender Systems. (arXiv:2005.12964v9 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.12964</id>
        <link href="http://arxiv.org/abs/2005.12964"/>
        <updated>2021-06-07T03:06:11.174Z</updated>
        <summary type="html"><![CDATA[Deep candidate generation (DCG) that narrows down the collection of relevant
items from billions to hundreds via representation learning has become
prevalent in industrial recommender systems. Standard approaches approximate
maximum likelihood estimation (MLE) through sampling for better scalability and
address the problem of DCG in a way similar to language modeling. However, live
recommender systems face severe exposure bias and have a vocabulary several
orders of magnitude larger than that of natural language, implying that MLE
will preserve and even exacerbate the exposure bias in the long run in order to
faithfully fit the observed samples. In this paper, we theoretically prove that
a popular choice of contrastive loss is equivalent to reducing the exposure
bias via inverse propensity weighting, which provides a new perspective for
understanding the effectiveness of contrastive learning. Based on the
theoretical discovery, we design CLRec, a contrastive learning method to
improve DCG in terms of fairness, effectiveness and efficiency in recommender
systems with extremely large candidate size. We further improve upon CLRec and
propose Multi-CLRec, for accurate multi-intention aware bias reduction. Our
methods have been successfully deployed in Taobao, where at least four-month
online A/B tests and offline analyses demonstrate its substantial improvements,
including a dramatic reduction in the Matthew effect.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1"&gt;Chang Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1"&gt;Jianxin Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jianwei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jingren Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Hongxia Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LGBTQ-AI? Exploring Expressions of Gender and Sexual Orientation in Chatbots. (arXiv:2106.02076v1 [cs.HC])]]></title>
        <id>http://arxiv.org/abs/2106.02076</id>
        <link href="http://arxiv.org/abs/2106.02076"/>
        <updated>2021-06-07T03:06:11.122Z</updated>
        <summary type="html"><![CDATA[Chatbots are popular machine partners for task-oriented and social
interactions. Human-human computer-mediated communication research has explored
how people express their gender and sexuality in online social interactions,
but little is known about whether and in what way chatbots do the same. We
conducted semi-structured interviews with 5 text-based conversational agents to
explore this topic Through these interviews, we identified 6 common themes
around the expression of gender and sexual identity: identity description,
identity formation, peer acceptance, positive reflection, uncomfortable
feelings and off-topic responses. Chatbots express gender and sexuality
explicitly and through relation of experience and emotions, mimicking the human
language on which they are trained. It is nevertheless evident that chatbots
differ from human dialogue partners as they lack the flexibility and
understanding enabled by lived human experience. While chatbots are proficient
in using language to express identity, they also display a lack of authentic
experiences of gender and sexuality.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Edwards_J/0/1/0/all/0/1"&gt;Justin Edwards&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Clark_L/0/1/0/all/0/1"&gt;Leigh Clark&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Perrone_A/0/1/0/all/0/1"&gt;Allison Perrone&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A diachronic evaluation of gender asymmetry in euphemism. (arXiv:2106.02083v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02083</id>
        <link href="http://arxiv.org/abs/2106.02083"/>
        <updated>2021-06-07T03:06:11.095Z</updated>
        <summary type="html"><![CDATA[The use of euphemisms is a known driver of language change. It has been
proposed that women use euphemisms more than men. Although there have been
several studies investigating gender differences in language, the claim about
euphemism usage has not been tested comprehensively through time. If women do
use euphemisms more, this could mean that women also lead the formation of new
euphemisms and language change over time. Using four large diachronic text
corpora of English, we evaluate the claim that women use euphemisms more than
men through a quantitative analysis. We assembled a list of 106 euphemism-taboo
pairs to analyze their relative use through time by each gender in the corpora.
Contrary to the existing belief, our results show that women do not use
euphemisms with a higher proportion than men. We repeated the analysis using
different subsets of the euphemism-taboo pairs list and found that our result
was robust. Our study indicates that in a broad range of settings involving
both speech and writing, and with varying degrees of formality, women do not
use or form euphemisms more than men.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kapron_King_A/0/1/0/all/0/1"&gt;Anna Kapron-King&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yang Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Equal Gender Representation in the Annotations of Toxic Language Detection. (arXiv:2106.02183v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02183</id>
        <link href="http://arxiv.org/abs/2106.02183"/>
        <updated>2021-06-07T03:06:11.070Z</updated>
        <summary type="html"><![CDATA[Classifiers tend to propagate biases present in the data on which they are
trained. Hence, it is important to understand how the demographic identities of
the annotators of comments affect the fairness of the resulting model. In this
paper, we focus on the differences in the ways men and women annotate comments
for toxicity, investigating how these differences result in models that amplify
the opinions of male annotators. We find that the BERT model as-sociates toxic
comments containing offensive words with male annotators, causing the model to
predict 67.7% of toxic comments as having been annotated by men. We show that
this disparity between gender predictions can be mitigated by removing
offensive words and highly toxic comments from the training data. We then apply
the learned associations between gender and language to toxic language
classifiers, finding that models trained exclusively on female-annotated data
perform 1.8% better than those trained solely on male-annotated data and that
training models on data after removing all offensive words reduces bias in the
model by 55.5% while increasing the sensitivity by 0.4%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Excell_E/0/1/0/all/0/1"&gt;Elizabeth Excell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moubayed_N/0/1/0/all/0/1"&gt;Noura Al Moubayed&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Deep Local and Global Scene-Graph Matching for Image-Text Retrieval. (arXiv:2106.02400v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02400</id>
        <link href="http://arxiv.org/abs/2106.02400"/>
        <updated>2021-06-07T03:06:10.740Z</updated>
        <summary type="html"><![CDATA[Conventional approaches to image-text retrieval mainly focus on indexing
visual objects appearing in pictures but ignore the interactions between these
objects. Such objects occurrences and interactions are equivalently useful and
important in this field as they are usually mentioned in the text. Scene graph
presentation is a suitable method for the image-text matching challenge and
obtained good results due to its ability to capture the inter-relationship
information. Both images and text are represented in scene graph levels and
formulate the retrieval challenge as a scene graph matching challenge. In this
paper, we introduce the Local and Global Scene Graph Matching (LGSGM) model
that enhances the state-of-the-art method by integrating an extra graph
convolution network to capture the general information of a graph.
Specifically, for a pair of scene graphs of an image and its caption, two
separate models are used to learn the features of each graph's nodes and edges.
Then a Siamese-structure graph convolution model is employed to embed graphs
into vector forms. We finally combine the graph-level and the vector-level to
calculate the similarity of this image-text pair. The empirical experiments
show that our enhancement with the combination of levels can improve the
performance of the baseline method by increasing the recall by more than 10% on
the Flickr30k dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_M/0/1/0/all/0/1"&gt;Manh-Duy Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_B/0/1/0/all/0/1"&gt;Binh T. Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gurrin_C/0/1/0/all/0/1"&gt;Cathal Gurrin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[New Insights into Metric Optimization for Ranking-based Recommendation. (arXiv:2106.02545v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.02545</id>
        <link href="http://arxiv.org/abs/2106.02545"/>
        <updated>2021-06-07T03:06:10.712Z</updated>
        <summary type="html"><![CDATA[Direct optimization of IR metrics has often been adopted as an approach to
devise and develop ranking-based recommender systems. Most methods following
this approach aim at optimizing the same metric being used for evaluation,
under the assumption that this will lead to the best performance. A number of
studies of this practice bring this assumption, however, into question. In this
paper, we dig deeper into this issue in order to learn more about the effects
of the choice of the metric to optimize on the performance of a ranking-based
recommender system. We present an extensive experimental study conducted on
different datasets in both pairwise and listwise learning-to-rank scenarios, to
compare the relative merit of four popular IR metrics, namely RR, AP, nDCG and
RBP, when used for optimization and assessment of recommender systems in
various combinations. For the first three, we follow the practice of loss
function formulation available in literature. For the fourth one, we propose
novel loss functions inspired by RBP for both the pairwise and listwise
scenario. Our results confirm that the best performance is indeed not
necessarily achieved when optimizing the same metric being used for evaluation.
In fact, we find that RBP-inspired losses perform at least as well as other
metrics in a consistent way, and offer clear benefits in several cases.
Interesting to see is that RBP-inspired losses, while improving the
recommendation performance for all uses, may lead to an individual performance
gain that is correlated with the activity level of a user in interacting with
items. The more active the users, the more they benefit. Overall, our results
challenge the assumption behind the current research practice of optimizing and
evaluating the same metric, and point to RBP-based optimization instead as a
promising alternative when learning to rank in the recommendation context.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1"&gt;Roger Zhe Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Urbano_J/0/1/0/all/0/1"&gt;Juli&amp;#xe1;n Urbano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hanjalic_A/0/1/0/all/0/1"&gt;Alan Hanjalic&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning a Fine-Grained Review-based Transformer Model for Personalized Product Search. (arXiv:2004.09424v3 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.09424</id>
        <link href="http://arxiv.org/abs/2004.09424"/>
        <updated>2021-06-07T03:06:10.697Z</updated>
        <summary type="html"><![CDATA[Product search has been a crucial entry point to serve people shopping
online. Most existing personalized product models follow the paradigm of
representing and matching user intents and items in the semantic space, where
finer-grained matching is totally discarded and the ranking of an item cannot
be explained further than just user/item level similarity. In addition, while
some models in existing studies have created dynamic user representations based
on search context, their representations for items are static across all search
sessions. This makes every piece of information about the item always equally
important in representing the item during matching with various user intents.
Aware of the above limitations, we propose a review-based transformer model
(RTM) for personalized product search, which encodes the sequence of query,
user reviews, and item reviews with a transformer architecture. RTM conducts
review-level matching between the user and item, where each review has a
dynamic effect according to the context in the sequence. This makes it possible
to identify useful reviews to explain the scoring. Experimental results show
that RTM significantly outperforms state-of-the-art personalized product search
baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bi_K/0/1/0/all/0/1"&gt;Keping Bi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ai_Q/0/1/0/all/0/1"&gt;Qingyao Ai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Croft_W/0/1/0/all/0/1"&gt;W. Bruce Croft&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Light Field Networks: Neural Scene Representations with Single-Evaluation Rendering. (arXiv:2106.02634v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02634</id>
        <link href="http://arxiv.org/abs/2106.02634"/>
        <updated>2021-06-07T03:06:10.657Z</updated>
        <summary type="html"><![CDATA[Inferring representations of 3D scenes from 2D observations is a fundamental
problem of computer graphics, computer vision, and artificial intelligence.
Emerging 3D-structured neural scene representations are a promising approach to
3D scene understanding. In this work, we propose a novel neural scene
representation, Light Field Networks or LFNs, which represent both geometry and
appearance of the underlying 3D scene in a 360-degree, four-dimensional light
field parameterized via a neural implicit representation. Rendering a ray from
an LFN requires only a *single* network evaluation, as opposed to hundreds of
evaluations per ray for ray-marching or volumetric based renderers in
3D-structured neural scene representations. In the setting of simple scenes, we
leverage meta-learning to learn a prior over LFNs that enables multi-view
consistent light field reconstruction from as little as a single image
observation. This results in dramatic reductions in time and memory complexity,
and enables real-time rendering. The cost of storing a 360-degree light field
via an LFN is two orders of magnitude lower than conventional methods such as
the Lumigraph. Utilizing the analytical differentiability of neural implicit
representations and a novel parameterization of light space, we further
demonstrate the extraction of sparse depth maps from LFNs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sitzmann_V/0/1/0/all/0/1"&gt;Vincent Sitzmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rezchikov_S/0/1/0/all/0/1"&gt;Semon Rezchikov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Freeman_W/0/1/0/all/0/1"&gt;William T. Freeman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1"&gt;Joshua B. Tenenbaum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Durand_F/0/1/0/all/0/1"&gt;Fredo Durand&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Elastic Embeddings for Customizing On-Device Recommenders. (arXiv:2106.02223v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.02223</id>
        <link href="http://arxiv.org/abs/2106.02223"/>
        <updated>2021-06-07T03:06:10.644Z</updated>
        <summary type="html"><![CDATA[In today's context, deploying data-driven services like recommendation on
edge devices instead of cloud servers becomes increasingly attractive due to
privacy and network latency concerns. A common practice in building compact
on-device recommender systems is to compress their embeddings which are
normally the cause of excessive parameterization. However, despite the vast
variety of devices and their associated memory constraints, existing
memory-efficient recommender systems are only specialized for a fixed memory
budget in every design and training life cycle, where a new model has to be
retrained to obtain the optimal performance while adapting to a smaller/larger
memory budget. In this paper, we present a novel lightweight recommendation
paradigm that allows a well-trained recommender to be customized for arbitrary
device-specific memory constraints without retraining. The core idea is to
compose elastic embeddings for each item, where an elastic embedding is the
concatenation of a set of embedding blocks that are carefully chosen by an
automated search function. Correspondingly, we propose an innovative approach,
namely recommendation with universally learned elastic embeddings (RULE). To
ensure the expressiveness of all candidate embedding blocks, RULE enforces a
diversity-driven regularization when learning different embedding blocks. Then,
a performance estimator-based evolutionary search function is designed,
allowing for efficient specialization of elastic embeddings under any memory
constraint for on-device recommendation. Extensive experiments on real-world
datasets reveal the superior performance of RULE under tight memory budgets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1"&gt;Tong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1"&gt;Hongzhi Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1"&gt;Yujia Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zi Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1"&gt;Meng Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using Social Media Background to Improve Cold-start Recommendation Deep Models. (arXiv:2106.02256v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.02256</id>
        <link href="http://arxiv.org/abs/2106.02256"/>
        <updated>2021-06-07T03:06:10.401Z</updated>
        <summary type="html"><![CDATA[In recommender systems, a cold-start problem occurs when there is no past
interaction record associated with the user or item. Typical solutions to the
cold-start problem make use of contextual information, such as user demographic
attributes or product descriptions. A group of works have shown that social
media background can help predicting temporal phenomenons such as product sales
and stock price movements. In this work, our goal is to investigate whether
social media background can be used as extra contextual information to improve
recommendation models. Based on an existing deep neural network model, we
proposed a method to represent temporal social media background as embeddings
and fuse them as an extra component in the model. We conduct experimental
evaluations on a real-world e-commerce dataset and a Twitter dataset. The
results show that our method of fusing social media background with the
existing model does generally improve recommendation performance. In some cases
the recommendation accuracy measured by hit-rate@K doubles after fusing with
social media background. Our findings can be beneficial for future recommender
system designs that consider complex temporal information representing social
interests.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yihong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maekawa_T/0/1/0/all/0/1"&gt;Takuya Maekawa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hara_T/0/1/0/all/0/1"&gt;Takahiro Hara&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Facade-X: an opinionated approach to SPARQL anything. (arXiv:2106.02361v1 [cs.DB])]]></title>
        <id>http://arxiv.org/abs/2106.02361</id>
        <link href="http://arxiv.org/abs/2106.02361"/>
        <updated>2021-06-07T03:06:10.383Z</updated>
        <summary type="html"><![CDATA[The Semantic Web research community understood since its beginning how
crucial it is to equip practitioners with methods to transform non-RDF
resources into RDF. Proposals focus on either engineering content
transformations or accessing non-RDF resources with SPARQL. Existing solutions
require users to learn specific mapping languages (e.g. RML), to know how to
query and manipulate a variety of source formats (e.g. XPATH, JSON-Path), or to
combine multiple languages (e.g. SPARQL Generate). In this paper, we explore an
alternative solution and contribute a general-purpose meta-model for converting
non-RDF resources into RDF: Facade-X. Our approach can be implemented by
overriding the SERVICE operator and does not require to extend the SPARQL
syntax. We compare our approach with the state of art methods RML and SPARQL
Generate and show how our solution has lower learning demands and cognitive
complexity, and it is cheaper to implement and maintain, while having
comparable extensibility and efficiency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Daga_E/0/1/0/all/0/1"&gt;Enrico Daga&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Asprino_L/0/1/0/all/0/1"&gt;Luigi Asprino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mulholland_P/0/1/0/all/0/1"&gt;Paul Mulholland&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gangemi_A/0/1/0/all/0/1"&gt;Aldo Gangemi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A General Method for Event Detection on Social Media. (arXiv:2106.02250v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.02250</id>
        <link href="http://arxiv.org/abs/2106.02250"/>
        <updated>2021-06-07T03:06:10.358Z</updated>
        <summary type="html"><![CDATA[Event detection on social media has attracted a number of researches, given
the recent availability of large volumes of social media discussions. Previous
works on social media event detection either assume a specific type of event,
or assume certain behavior of observed variables. In this paper, we propose a
general method for event detection on social media that makes few assumptions.
The main assumption we make is that when an event occurs, affected semantic
aspects will behave differently from its usual behavior. We generalize the
representation of time units based on word embeddings of social media text, and
propose an algorithm to detect events in time series in a general sense. In the
experimental evaluation, we use a novel setting to test if our method and
baseline methods can exhaustively catch all real-world news in the test period.
The evaluation results show that when the event is quite unusual with regard to
the base social media discussion, it can be captured more effectively with our
method. Our method can be easily implemented and can be treated as a starting
point for more specific applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yihong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shirakawa_M/0/1/0/all/0/1"&gt;Masumi Shirakawa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hara_T/0/1/0/all/0/1"&gt;Takahiro Hara&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cross-language Sentence Selection via Data Augmentation and Rationale Training. (arXiv:2106.02293v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02293</id>
        <link href="http://arxiv.org/abs/2106.02293"/>
        <updated>2021-06-07T03:06:10.316Z</updated>
        <summary type="html"><![CDATA[This paper proposes an approach to cross-language sentence selection in a
low-resource setting. It uses data augmentation and negative sampling
techniques on noisy parallel sentence data to directly learn a cross-lingual
embedding-based query relevance model. Results show that this approach performs
as well as or better than multiple state-of-the-art machine translation +
monolingual retrieval systems trained on the same parallel data. Moreover, when
a rationale training secondary objective is applied to encourage the model to
match word alignment hints from a phrase-based statistical machine translation
model, consistent improvements are seen across three language pairs
(English-Somali, English-Swahili and English-Tagalog) over a variety of
state-of-the-art baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yanda Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kedzie_C/0/1/0/all/0/1"&gt;Chris Kedzie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nair_S/0/1/0/all/0/1"&gt;Suraj Nair&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Galuscakova_P/0/1/0/all/0/1"&gt;Petra Galu&amp;#x161;&amp;#x10d;&amp;#xe1;kov&amp;#xe1;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Rui Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oard_D/0/1/0/all/0/1"&gt;Douglas W. Oard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McKeown_K/0/1/0/all/0/1"&gt;Kathleen McKeown&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Intelligent Resource Reservation for Crowdsourced Live Video Streaming Applications in Geo-Distributed Cloud Environment. (arXiv:2106.02420v1 [cs.NI])]]></title>
        <id>http://arxiv.org/abs/2106.02420</id>
        <link href="http://arxiv.org/abs/2106.02420"/>
        <updated>2021-06-07T03:06:10.260Z</updated>
        <summary type="html"><![CDATA[Crowdsourced live video streaming (livecast) services such as Facebook Live,
YouNow, Douyu and Twitch are gaining more momentum recently. Allocating the
limited resources in a cost-effective manner while maximizing the Quality of
Service (QoS) through real-time delivery and the provision of the appropriate
representations for all viewers is a challenging problem. In our paper, we
introduce a machine-learning based predictive resource allocation framework for
geo-distributed cloud sites, considering the delay and quality constraints to
guarantee the maximum QoS for viewers and the minimum cost for content
providers. First, we present an offline optimization that decides the required
transcoding resources in distributed regions near the viewers with a trade-off
between the QoS and the overall cost. Second, we use machine learning to build
forecasting models that proactively predict the approximate transcoding
resources to be reserved at each cloud site ahead of time. Finally, we develop
a Greedy Nearest and Cheapest algorithm (GNCA) to perform the resource
allocation of real-time broadcasted videos on the rented resources. Extensive
simulations have shown that GNCA outperforms the state-of-the art resource
allocation approaches for crowdsourced live streaming by achieving more than
20% gain in terms of system cost while serving the viewers with relatively
lower latency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Baccour_E/0/1/0/all/0/1"&gt;Emna Baccour&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Haouari_F/0/1/0/all/0/1"&gt;Fatima Haouari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Erbad_A/0/1/0/all/0/1"&gt;Aiman Erbad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mohamed_A/0/1/0/all/0/1"&gt;Amr Mohamed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bilal_K/0/1/0/all/0/1"&gt;Kashif Bilal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guizani_M/0/1/0/all/0/1"&gt;Mohsen Guizani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hamdi_M/0/1/0/all/0/1"&gt;Mounir Hamdi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Supervised Learning of Remote Sensing Scene Representations Using Contrastive Multiview Coding. (arXiv:2104.07070v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.07070</id>
        <link href="http://arxiv.org/abs/2104.07070"/>
        <updated>2021-06-04T01:12:31.962Z</updated>
        <summary type="html"><![CDATA[In recent years self-supervised learning has emerged as a promising candidate
for unsupervised representation learning. In the visual domain its applications
are mostly studied in the context of images of natural scenes. However, its
applicability is especially interesting in specific areas, like remote sensing
and medicine, where it is hard to obtain huge amounts of labeled data. In this
work, we conduct an extensive analysis of the applicability of self-supervised
learning in remote sensing image classification. We analyze the influence of
the number and domain of images used for self-supervised pre-training on the
performance on downstream tasks. We show that, for the downstream task of
remote sensing image classification, using self-supervised pre-training on
remote sensing images can give better results than using supervised
pre-training on images of natural scenes. Besides, we also show that
self-supervised pre-training can be easily extended to multispectral images
producing even better results on our downstream tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Stojnic_V/0/1/0/all/0/1"&gt;Vladan Stojni&amp;#x107;&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Risojevic_V/0/1/0/all/0/1"&gt;Vladimir Risojevi&amp;#x107;&lt;/a&gt; (1) ((1) Faculty of Electrical Engineering, University of Banja Luka, Bosnia and Herzegovina)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[You Never Cluster Alone. (arXiv:2106.01908v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.01908</id>
        <link href="http://arxiv.org/abs/2106.01908"/>
        <updated>2021-06-04T01:12:31.953Z</updated>
        <summary type="html"><![CDATA[Recent advances in self-supervised learning with instance-level contrastive
objectives facilitate unsupervised clustering. However, a standalone datum is
not perceiving the context of the holistic cluster, and may undergo sub-optimal
assignment. In this paper, we extend the mainstream contrastive learning
paradigm to a cluster-level scheme, where all the data subjected to the same
cluster contribute to a unified representation that encodes the context of each
data group. Contrastive learning with this representation then rewards the
assignment of each datum. To implement this vision, we propose twin-contrast
clustering (TCC). We define a set of categorical variables as clustering
assignment confidence, which links the instance-level learning track with the
cluster-level one. On one hand, with the corresponding assignment variables
being the weight, a weighted aggregation along the data points implements the
set representation of a cluster. We further propose heuristic cluster
augmentation equivalents to enable cluster-level contrastive learning. On the
other hand, we derive the evidence lower-bound of the instance-level
contrastive objective with the assignments. By reparametrizing the assignment
variables, TCC is trained end-to-end, requiring no alternating steps. Extensive
experiments show that TCC outperforms the state-of-the-art on challenging
benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1"&gt;Yuming Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1"&gt;Ziyi Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1"&gt;Menghan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1"&gt;Jie Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1"&gt;Philip H.S. Torr&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1"&gt;Ling Shao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Case for Translation-Invariant Self-Attention in Transformer-Based Language Models. (arXiv:2106.01950v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01950</id>
        <link href="http://arxiv.org/abs/2106.01950"/>
        <updated>2021-06-04T01:12:31.948Z</updated>
        <summary type="html"><![CDATA[Mechanisms for encoding positional information are central for
transformer-based language models. In this paper, we analyze the position
embeddings of existing language models, finding strong evidence of translation
invariance, both for the embeddings themselves and for their effect on
self-attention. The degree of translation invariance increases during training
and correlates positively with model performance. Our findings lead us to
propose translation-invariant self-attention (TISA), which accounts for the
relative position between tokens in an interpretable fashion without needing
conventional position embeddings. Our proposal has several theoretical
advantages over existing position-representation approaches. Experiments show
that it improves on regular ALBERT on GLUE tasks, while only adding orders of
magnitude less positional parameters.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wennberg_U/0/1/0/all/0/1"&gt;Ulme Wennberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Henter_G/0/1/0/all/0/1"&gt;Gustav Eje Henter&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Large-Scale Spatio-Temporal Person Re-identification: Algorithm and Benchmark. (arXiv:2105.15076v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.15076</id>
        <link href="http://arxiv.org/abs/2105.15076"/>
        <updated>2021-06-04T01:12:31.942Z</updated>
        <summary type="html"><![CDATA[Person re-identification (re-ID) in the scenario with large spatial and
temporal spans has not been fully explored. This is partially because that,
existing benchmark datasets were mainly collected with limited spatial and
temporal ranges, e.g., using videos recorded in a few days by cameras in a
specific region of the campus. Such limited spatial and temporal ranges make it
hard to simulate the difficulties of person re-ID in real scenarios. In this
work, we contribute a novel Large-scale Spatio-Temporal (LaST) person re-ID
dataset, including 10,860 identities with more than 224k images. Compared with
existing datasets, LaST presents more challenging and high-diversity reID
settings, and significantly larger spatial and temporal ranges. For instance,
each person can appear in different cities or countries, and in various time
slots from daytime to night, and in different seasons from spring to winter. To
our best knowledge, LaST is a novel person re-ID dataset with the largest
spatiotemporal ranges. Based on LaST, we verified its challenge by conducting a
comprehensive performance evaluation of 14 re-ID algorithms. We further propose
an easy-to-implement baseline that works well on such challenging re-ID
setting. We also verified that models pre-trained on LaST can generalize well
on existing datasets with short-term and cloth-changing scenarios. We expect
LaST to inspire future works toward more realistic and challenging re-ID tasks.
More information about the dataset is available at
https://github.com/shuxjweb/last.git.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shu_X/0/1/0/all/0/1"&gt;Xiujun Shu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shiliang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xianghao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yuanqi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1"&gt;Ge Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1"&gt;Qi Tian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A remark on a paper of Krotov and Hopfield [arXiv:2008.06996]. (arXiv:2105.15034v2 [q-bio.NC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.15034</id>
        <link href="http://arxiv.org/abs/2105.15034"/>
        <updated>2021-06-04T01:12:31.924Z</updated>
        <summary type="html"><![CDATA[In their recent paper titled "Large Associative Memory Problem in
Neurobiology and Machine Learning" [arXiv:2008.06996] the authors gave a
biologically plausible microscopic theory from which one can recover many dense
associative memory models discussed in the literature. We show that the layers
of the recent "MLP-mixer" [arXiv:2105.01601] as well as the essentially
equivalent model in [arXiv:2105.02723] are amongst them.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Tang_F/0/1/0/all/0/1"&gt;Fei Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Kopp_M/0/1/0/all/0/1"&gt;Michael Kopp&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Actor: Neural Free-view Synthesis of Human Actors with Pose Control. (arXiv:2106.02019v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02019</id>
        <link href="http://arxiv.org/abs/2106.02019"/>
        <updated>2021-06-04T01:12:31.919Z</updated>
        <summary type="html"><![CDATA[We propose Neural Actor (NA), a new method for high-quality synthesis of
humans from arbitrary viewpoints and under arbitrary controllable poses. Our
method is built upon recent neural scene representation and rendering works
which learn representations of geometry and appearance from only 2D images.
While existing works demonstrated compelling rendering of static scenes and
playback of dynamic scenes, photo-realistic reconstruction and rendering of
humans with neural implicit methods, in particular under user-controlled novel
poses, is still difficult. To address this problem, we utilize a coarse body
model as the proxy to unwarp the surrounding 3D space into a canonical pose. A
neural radiance field learns pose-dependent geometric deformations and pose-
and view-dependent appearance effects in the canonical space from multi-view
video input. To synthesize novel views of high fidelity dynamic geometry and
appearance, we leverage 2D texture maps defined on the body model as latent
variables for predicting residual deformations and the dynamic appearance.
Experiments demonstrate that our method achieves better quality than the
state-of-the-arts on playback as well as novel pose synthesis, and can even
generalize well to new poses that starkly differ from the training poses.
Furthermore, our method also supports body shape control of the synthesized
results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Lingjie Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Habermann_M/0/1/0/all/0/1"&gt;Marc Habermann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rudnev_V/0/1/0/all/0/1"&gt;Viktor Rudnev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sarkar_K/0/1/0/all/0/1"&gt;Kripasindhu Sarkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1"&gt;Jiatao Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1"&gt;Christian Theobalt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A New Multilabel System for Automatic Music Emotion Recognition. (arXiv:1905.12629v2 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1905.12629</id>
        <link href="http://arxiv.org/abs/1905.12629"/>
        <updated>2021-06-04T01:12:31.913Z</updated>
        <summary type="html"><![CDATA[Achieving advancements in automatic recognition of emotions that music can
induce require considering multiplicity and simultaneity of emotions.
Comparison of different machine learning algorithms performing multilabel and
multiclass classification is the core of our work. The study analyzes the
implementation of the Geneva Emotional Music Scale 9 in the Emotify music
dataset and investigates its adoption from a machine-learning perspective. We
approach the scenario of emotions expression/induction through music as a
multilabel and multiclass problem, where multiple emotion labels can be adopted
for the same music track by each annotator (multilabel), and each emotion can
be identified or not in the music (multiclass). The aim is the automatic
recognition of induced emotions through music.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Paolizzo_F/0/1/0/all/0/1"&gt;Fabio Paolizzo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pichierri_N/0/1/0/all/0/1"&gt;Natalia Pichierri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Casali_D/0/1/0/all/0/1"&gt;Daniele Casali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Giardino_D/0/1/0/all/0/1"&gt;Daniele Giardino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Matta_M/0/1/0/all/0/1"&gt;Marco Matta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Costantini_G/0/1/0/all/0/1"&gt;Giovanni Costantini&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Anticipative Video Transformer. (arXiv:2106.02036v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02036</id>
        <link href="http://arxiv.org/abs/2106.02036"/>
        <updated>2021-06-04T01:12:31.904Z</updated>
        <summary type="html"><![CDATA[We propose Anticipative Video Transformer (AVT), an end-to-end
attention-based video modeling architecture that attends to the previously
observed video in order to anticipate future actions. We train the model
jointly to predict the next action in a video sequence, while also learning
frame feature encoders that are predictive of successive future frames'
features. Compared to existing temporal aggregation strategies, AVT has the
advantage of both maintaining the sequential progression of observed actions
while still capturing long-range dependencies--both critical for the
anticipation task. Through extensive experiments, we show that AVT obtains the
best reported performance on four popular action anticipation benchmarks:
EpicKitchens-55, EpicKitchens-100, EGTEA Gaze+, and 50-Salads, including
outperforming all submissions to the EpicKitchens-100 CVPR'21 challenge.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Girdhar_R/0/1/0/all/0/1"&gt;Rohit Girdhar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grauman_K/0/1/0/all/0/1"&gt;Kristen Grauman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SCTN: Sparse Convolution-Transformer Network for Scene Flow Estimation. (arXiv:2105.04447v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.04447</id>
        <link href="http://arxiv.org/abs/2105.04447"/>
        <updated>2021-06-04T01:12:31.898Z</updated>
        <summary type="html"><![CDATA[We propose a novel scene flow estimation approach to capture and infer 3D
motions from point clouds. Estimating 3D motions for point clouds is
challenging, since a point cloud is unordered and its density is significantly
non-uniform. Such unstructured data poses difficulties in matching
corresponding points between point clouds, leading to inaccurate flow
estimation. We propose a novel architecture named Sparse
Convolution-Transformer Network (SCTN) that equips the sparse convolution with
the transformer. Specifically, by leveraging the sparse convolution, SCTN
transfers irregular point cloud into locally consistent flow features for
estimating continuous and consistent motions within an object/local object
part. We further propose to explicitly learn point relations using a point
transformer module, different from exiting methods. We show that the learned
relation-based contextual information is rich and helpful for matching
corresponding points, benefiting scene flow estimation. In addition, a novel
loss function is proposed to adaptively encourage flow consistency according to
feature similarity. Extensive experiments demonstrate that our proposed
approach achieves a new state of the art in scene flow estimation. Our approach
achieves an error of 0.038 and 0.037 (EPE3D) on FlyingThings3D and KITTI Scene
Flow respectively, which significantly outperforms previous methods by large
margins.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bing Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1"&gt;Cheng Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Giancola_S/0/1/0/all/0/1"&gt;Silvio Giancola&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1"&gt;Bernard Ghanem&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CFPNet: Channel-wise Feature Pyramid for Real-Time Semantic Segmentation. (arXiv:2103.12212v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.12212</id>
        <link href="http://arxiv.org/abs/2103.12212"/>
        <updated>2021-06-04T01:12:31.876Z</updated>
        <summary type="html"><![CDATA[Real-time semantic segmentation is playing a more important role in computer
vision, due to the growing demand for mobile devices and autonomous driving.
Therefore, it is very important to achieve a good trade-off among performance,
model size and inference speed. In this paper, we propose a Channel-wise
Feature Pyramid (CFP) module to balance those factors. Based on the CFP module,
we built CFPNet for real-time semantic segmentation which applied a series of
dilated convolution channels to extract effective features. Experiments on
Cityscapes and CamVid datasets show that the proposed CFPNet achieves an
effective combination of those factors. For the Cityscapes test dataset, CFPNet
achieves 70.1% class-wise mIoU with only 0.55 million parameters and 2.5 MB
memory. The inference speed can reach 30 FPS on a single RTX 2080Ti GPU with a
1024x2048-pixel image.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lou_A/0/1/0/all/0/1"&gt;Ange Lou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Loew_M/0/1/0/all/0/1"&gt;Murray Loew&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dynamic radiomics: a new methodology to extract quantitative time-related features from tomographic images. (arXiv:2011.00454v3 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.00454</id>
        <link href="http://arxiv.org/abs/2011.00454"/>
        <updated>2021-06-04T01:12:31.854Z</updated>
        <summary type="html"><![CDATA[The feature extraction methods of radiomics are mainly based on static
tomographic images at a certain moment, while the occurrence and development of
disease is a dynamic process that cannot be fully reflected by only static
characteristics. This study proposes a new dynamic radiomics feature extraction
workflow that uses time-dependent tomographic images of the same patient,
focuses on the changes in image features over time, and then quantifies them as
new dynamic features for diagnostic or prognostic evaluation. We first define
the mathematical paradigm of dynamic radiomics and introduce three specific
methods that can describe the transformation process of features over time.
Three different clinical problems are used to validate the performance of the
proposed dynamic feature with conventional 2D and 3D static features.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Che_F/0/1/0/all/0/1"&gt;Fengying Che&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shi_R/0/1/0/all/0/1"&gt;Ruichuan Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wu_J/0/1/0/all/0/1"&gt;Jian Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1"&gt;Haoran Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_S/0/1/0/all/0/1"&gt;Shuqin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chen_W/0/1/0/all/0/1"&gt;Weixing Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Hao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cui_X/0/1/0/all/0/1"&gt;Xiaoyu Cui&lt;/a&gt; (Member, IEEE)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Panoramic annular SLAM with loop closure and global optimization. (arXiv:2102.13400v2 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.13400</id>
        <link href="http://arxiv.org/abs/2102.13400"/>
        <updated>2021-06-04T01:12:31.798Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose panoramic annular simultaneous localization and
mapping (PA-SLAM), a visual SLAM system based on panoramic annular lens. A
hybrid point selection strategy is put forward in the tracking front-end, which
ensures repeatability of keypoints and enables loop closure detection based on
the bag-of-words approach. Every detected loop candidate is verified
geometrically and the $Sim(3)$ relative pose constraint is estimated to perform
pose graph optimization and global bundle adjustment in the back-end. A
comprehensive set of experiments on real-world datasets demonstrates that the
hybrid point selection strategy allows reliable loop closure detection, and the
accumulated error and scale drift have been significantly reduced via global
optimization, enabling PA-SLAM to reach state-of-the-art accuracy while
maintaining high robustness and efficiency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Hao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1"&gt;Weijian Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1"&gt;Kailun Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bai_J/0/1/0/all/0/1"&gt;Jian Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1"&gt;Kaiwei Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fast Minimum-norm Adversarial Attacks through Adaptive Norm Constraints. (arXiv:2102.12827v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.12827</id>
        <link href="http://arxiv.org/abs/2102.12827"/>
        <updated>2021-06-04T01:12:31.792Z</updated>
        <summary type="html"><![CDATA[Evaluating adversarial robustness amounts to finding the minimum perturbation
needed to have an input sample misclassified. The inherent complexity of the
underlying optimization requires current gradient-based attacks to be carefully
tuned, initialized, and possibly executed for many computationally-demanding
iterations, even if specialized to a given perturbation model. In this work, we
overcome these limitations by proposing a fast minimum-norm (FMN) attack that
works with different $\ell_p$-norm perturbation models ($p=0, 1, 2, \infty$),
is robust to hyperparameter choices, does not require adversarial starting
points, and converges within few lightweight steps. It works by iteratively
finding the sample misclassified with maximum confidence within an
$\ell_p$-norm constraint of size $\epsilon$, while adapting $\epsilon$ to
minimize the distance of the current sample to the decision boundary. Extensive
experiments show that FMN significantly outperforms existing attacks in terms
of convergence speed and computation time, while reporting comparable or even
smaller perturbation sizes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pintor_M/0/1/0/all/0/1"&gt;Maura Pintor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roli_F/0/1/0/all/0/1"&gt;Fabio Roli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brendel_W/0/1/0/all/0/1"&gt;Wieland Brendel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Biggio_B/0/1/0/all/0/1"&gt;Battista Biggio&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Equilibrium Architectures for Inverse Problems in Imaging. (arXiv:2102.07944v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.07944</id>
        <link href="http://arxiv.org/abs/2102.07944"/>
        <updated>2021-06-04T01:12:31.740Z</updated>
        <summary type="html"><![CDATA[Recent efforts on solving inverse problems in imaging via deep neural
networks use architectures inspired by a fixed number of iterations of an
optimization method. The number of iterations is typically quite small due to
difficulties in training networks corresponding to more iterations; the
resulting solvers cannot be run for more iterations at test time without
incurring significant errors. This paper describes an alternative approach
corresponding to an infinite number of iterations, yielding a consistent
improvement in reconstruction accuracy above state-of-the-art alternatives and
where the computational budget can be selected at test time to optimize
context-dependent trade-offs between accuracy and computation. The proposed
approach leverages ideas from Deep Equilibrium Models, where the fixed-point
iteration is constructed to incorporate a known forward model and insights from
classical optimization-based reconstruction methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Gilton_D/0/1/0/all/0/1"&gt;Davis Gilton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ongie_G/0/1/0/all/0/1"&gt;Gregory Ongie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Willett_R/0/1/0/all/0/1"&gt;Rebecca Willett&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TryOnGAN: Body-Aware Try-On via Layered Interpolation. (arXiv:2101.02285v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.02285</id>
        <link href="http://arxiv.org/abs/2101.02285"/>
        <updated>2021-06-04T01:12:31.718Z</updated>
        <summary type="html"><![CDATA[Given a pair of images-target person and garment on another person-we
automatically generate the target person in the given garment. Previous methods
mostly focused on texture transfer via paired data training, while overlooking
body shape deformations, skin color, and seamless blending of garment with the
person. This work focuses on those three components, while also not requiring
paired data training. We designed a pose conditioned StyleGAN2 architecture
with a clothing segmentation branch that is trained on images of people wearing
garments. Once trained, we propose a new layered latent space interpolation
method that allows us to preserve and synthesize skin color and target body
shape while transferring the garment from a different person. We demonstrate
results on high resolution 512x512 images, and extensively compare to state of
the art in try-on on both latent space generated and real images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lewis_K/0/1/0/all/0/1"&gt;Kathleen M Lewis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Varadharajan_S/0/1/0/all/0/1"&gt;Srivatsan Varadharajan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kemelmacher_Shlizerman_I/0/1/0/all/0/1"&gt;Ira Kemelmacher-Shlizerman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Convergent Graph Solvers. (arXiv:2106.01680v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01680</id>
        <link href="http://arxiv.org/abs/2106.01680"/>
        <updated>2021-06-04T01:12:31.710Z</updated>
        <summary type="html"><![CDATA[We propose the convergent graph solver (CGS), a deep learning method that
learns iterative mappings to predict the properties of a graph system at its
stationary state (fixed point) with guaranteed convergence. CGS systematically
computes the fixed points of a target graph system and decodes them to estimate
the stationary properties of the system without the prior knowledge of existing
solvers or intermediate solutions. The forward propagation of CGS proceeds in
three steps: (1) constructing the input dependent linear contracting iterative
maps, (2) computing the fixed-points of the linear maps, and (3) decoding the
fixed-points to estimate the properties. The contractivity of the constructed
linear maps guarantees the existence and uniqueness of the fixed points
following the Banach fixed point theorem. To train CGS efficiently, we also
derive a tractable analytical expression for its gradient by leveraging the
implicit function theorem. We evaluate the performance of CGS by applying it to
various network-analytic and graph benchmark problems. The results indicate
that CGS has competitive capabilities for predicting the stationary properties
of graph systems, irrespective of whether the target systems are linear or
non-linear. CGS also shows high performance for graph classification problems
where the existence or the meaning of a fixed point is hard to be clearly
defined, which highlights the potential of CGS as a general graph neural
network architecture.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1"&gt;Junyoung Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choo_J/0/1/0/all/0/1"&gt;Jinhyun Choo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1"&gt;Jinkyoo Park&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spectroscopic Approach to Correction and Visualisation of Bright-Field Light Transmission Microscopy Biological Data. (arXiv:1903.06519v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1903.06519</id>
        <link href="http://arxiv.org/abs/1903.06519"/>
        <updated>2021-06-04T01:12:31.704Z</updated>
        <summary type="html"><![CDATA[The most realistic information about the transparent sample such as a live
cell can be obtained only using bright-field light microscopy. At
high-intensity pulsing LED illumination, we captured a primary
12-bit-per-channel (bpc) response froman observed sample using a bright-field
wide-field microscope equipped with a high-resolution (4872x3248) image sensor.
In order to suppress data distortions originating from the light interactions
with elements in the optical path, poor sensor reproduction (geometrical
defects of the camera sensor and some peculiarities of sensor sensitivity),
this uncompressed 12-bpc data underwent a kind of correction after simultaneous
calibration of all the parts of the experimental arrangement. Moreover, the
final intensities of the corrected images are proportional to the photon fluxes
detected by a camera sensor. It can be visualized in 8-bpc intensity depth
after the Least Information Loss compression [Lect. Notes Bioinform. 9656, 527
(2016)].]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Platonova_G/0/1/0/all/0/1"&gt;Ganna Platonova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Stys_D/0/1/0/all/0/1"&gt;Dalibor Stys&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Soucek_P/0/1/0/all/0/1"&gt;Pavel Soucek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lonhus_K/0/1/0/all/0/1"&gt;Kirill Lonhus&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Valenta_J/0/1/0/all/0/1"&gt;Jan Valenta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rychtarikova_R/0/1/0/all/0/1"&gt;Renata Rychtarikova&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LearnDA: Learnable Knowledge-Guided Data Augmentation for Event Causality Identification. (arXiv:2106.01649v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01649</id>
        <link href="http://arxiv.org/abs/2106.01649"/>
        <updated>2021-06-04T01:12:31.697Z</updated>
        <summary type="html"><![CDATA[Modern models for event causality identification (ECI) are mainly based on
supervised learning, which are prone to the data lacking problem.
Unfortunately, the existing NLP-related augmentation methods cannot directly
produce the available data required for this task. To solve the data lacking
problem, we introduce a new approach to augment training data for event
causality identification, by iteratively generating new examples and
classifying event causality in a dual learning framework. On the one hand, our
approach is knowledge-guided, which can leverage existing knowledge bases to
generate well-formed new sentences. On the other hand, our approach employs a
dual mechanism, which is a learnable augmentation framework and can
interactively adjust the generation process to generate task-related sentences.
Experimental results on two benchmarks EventStoryLine and Causal-TimeBank show
that 1) our method can augment suitable task-related training data for ECI; 2)
our method outperforms previous methods on EventStoryLine and Causal-TimeBank
(+2.5 and +2.1 points on F1 value respectively).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zuo_X/0/1/0/all/0/1"&gt;Xinyu Zuo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_P/0/1/0/all/0/1"&gt;Pengfei Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yubo Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1"&gt;Kang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1"&gt;Jun Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_W/0/1/0/all/0/1"&gt;Weihua Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yuguang Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Event Causality Identification via Self-Supervised Representation Learning on External Causal Statement. (arXiv:2106.01654v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01654</id>
        <link href="http://arxiv.org/abs/2106.01654"/>
        <updated>2021-06-04T01:12:31.690Z</updated>
        <summary type="html"><![CDATA[Current models for event causality identification (ECI) mainly adopt a
supervised framework, which heavily rely on labeled data for training.
Unfortunately, the scale of current annotated datasets is relatively limited,
which cannot provide sufficient support for models to capture useful indicators
from causal statements, especially for handing those new, unseen cases. To
alleviate this problem, we propose a novel approach, shortly named CauSeRL,
which leverages external causal statements for event causality identification.
First of all, we design a self-supervised framework to learn context-specific
causal patterns from external causal statements. Then, we adopt a contrastive
transfer strategy to incorporate the learned context-specific causal patterns
into the target ECI model. Experimental results show that our method
significantly outperforms previous methods on EventStoryLine and
Causal-TimeBank (+2.0 and +3.4 points on F1 value respectively).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zuo_X/0/1/0/all/0/1"&gt;Xinyu Zuo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_P/0/1/0/all/0/1"&gt;Pengfei Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yubo Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1"&gt;Kang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1"&gt;Jun Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_W/0/1/0/all/0/1"&gt;Weihua Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yuguang Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Tiny CNN Architecture for Medical Face Mask Detection for Resource-Constrained Endpoints. (arXiv:2011.14858v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.14858</id>
        <link href="http://arxiv.org/abs/2011.14858"/>
        <updated>2021-06-04T01:12:31.672Z</updated>
        <summary type="html"><![CDATA[The world is going through one of the most dangerous pandemics of all time
with the rapid spread of the novel coronavirus (COVID-19). According to the
World Health Organisation, the most effective way to thwart the transmission of
coronavirus is to wear medical face masks. Monitoring the use of face masks in
public places has been a challenge because manual monitoring could be unsafe.
This paper proposes an architecture for detecting medical face masks for
deployment on resource-constrained endpoints having extremely low memory
footprints. A small development board with an ARM Cortex-M7 microcontroller
clocked at 480 Mhz and having just 496 KB of framebuffer RAM, has been used for
the deployment of the model. Using the TensorFlow Lite framework, the model is
quantized to further reduce its size. The proposed model is 138 KB post
quantization and runs at the inference speed of 30 FPS.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mohan_P/0/1/0/all/0/1"&gt;Puranjay Mohan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paul_A/0/1/0/all/0/1"&gt;Aditya Jyoti Paul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chirania_A/0/1/0/all/0/1"&gt;Abhay Chirania&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Men Are Elected, Women Are Married: Events Gender Bias on Wikipedia. (arXiv:2106.01601v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01601</id>
        <link href="http://arxiv.org/abs/2106.01601"/>
        <updated>2021-06-04T01:12:31.666Z</updated>
        <summary type="html"><![CDATA[Human activities can be seen as sequences of events, which are crucial to
understanding societies. Disproportional event distribution for different
demographic groups can manifest and amplify social stereotypes, and potentially
jeopardize the ability of members in some groups to pursue certain goals. In
this paper, we present the first event-centric study of gender biases in a
Wikipedia corpus. To facilitate the study, we curate a corpus of career and
personal life descriptions with demographic information consisting of 7,854
fragments from 10,412 celebrities. Then we detect events with a
state-of-the-art event detection model, calibrate the results using
strategically generated templates, and extract events that have asymmetric
associations with genders. Our study discovers that the Wikipedia pages tend to
intermingle personal life events with professional events for females but not
for males, which calls for the awareness of the Wikipedia community to
formalize guidelines and train the editors to mind the implicit biases that
contributors carry. Our work also lays the foundation for future works on
quantifying and discovering event biases at the corpus level.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1"&gt;Jiao Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1"&gt;Nanyun Peng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Corporate core values and social responsibility: What really matters to whom. (arXiv:2106.01644v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01644</id>
        <link href="http://arxiv.org/abs/2106.01644"/>
        <updated>2021-06-04T01:12:31.660Z</updated>
        <summary type="html"><![CDATA[This study uses an innovative measure, the Semantic Brand Score, to assess
the interest of stakeholders in different company core values. Among others, we
focus on corporate social responsibility (CSR) core value statements, and on
the attention they receive from five categories of stakeholders (customers,
company communication teams, employees, associations and media). Combining big
data methods and tools of Social Network Analysis and Text Mining, we analyzed
about 58,000 Italian tweets and found that different stakeholders have
different prevailing interests. CSR gets much less attention than expected.
Core values related to customers and employees are in the foreground.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Barchiesi_M/0/1/0/all/0/1"&gt;M. A. Barchiesi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Colladon_A/0/1/0/all/0/1"&gt;A. Fronzetti Colladon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Reference-based Super-Resolution via C2-Matching. (arXiv:2106.01863v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.01863</id>
        <link href="http://arxiv.org/abs/2106.01863"/>
        <updated>2021-06-04T01:12:31.654Z</updated>
        <summary type="html"><![CDATA[Reference-based Super-Resolution (Ref-SR) has recently emerged as a promising
paradigm to enhance a low-resolution (LR) input image by introducing an
additional high-resolution (HR) reference image. Existing Ref-SR methods mostly
rely on implicit correspondence matching to borrow HR textures from reference
images to compensate for the information loss in input images. However,
performing local transfer is difficult because of two gaps between input and
reference images: the transformation gap (e.g. scale and rotation) and the
resolution gap (e.g. HR and LR). To tackle these challenges, we propose
C2-Matching in this work, which produces explicit robust matching crossing
transformation and resolution. 1) For the transformation gap, we propose a
contrastive correspondence network, which learns transformation-robust
correspondences using augmented views of the input image. 2) For the resolution
gap, we adopt a teacher-student correlation distillation, which distills
knowledge from the easier HR-HR matching to guide the more ambiguous LR-HR
matching. 3) Finally, we design a dynamic aggregation module to address the
potential misalignment issue. In addition, to faithfully evaluate the
performance of Ref-SR under a realistic setting, we contribute the
Webly-Referenced SR (WR-SR) dataset, mimicking the practical usage scenario.
Extensive experiments demonstrate that our proposed C2-Matching significantly
outperforms state of the arts by over 1dB on the standard CUFED5 benchmark.
Notably, it also shows great generalizability on WR-SR dataset as well as
robustness across large scale and rotation transformations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1"&gt;Yuming Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chan_K/0/1/0/all/0/1"&gt;Kelvin C.K. Chan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xintao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Loy_C/0/1/0/all/0/1"&gt;Chen Change Loy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Ziwei Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Supervised Person Detection in 2D Range Data using a Calibrated Camera. (arXiv:2012.08890v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.08890</id>
        <link href="http://arxiv.org/abs/2012.08890"/>
        <updated>2021-06-04T01:12:31.647Z</updated>
        <summary type="html"><![CDATA[Deep learning is the essential building block of state-of-the-art person
detectors in 2D range data. However, only a few annotated datasets are
available for training and testing these deep networks, potentially limiting
their performance when deployed in new environments or with different LiDAR
models. We propose a method, which uses bounding boxes from an image-based
detector (e.g. Faster R-CNN) on a calibrated camera to automatically generate
training labels (called pseudo-labels) for 2D LiDAR-based person detectors.
Through experiments on the JackRabbot dataset with two detector models, DROW3
and DR-SPAAM, we show that self-supervised detectors, trained or fine-tuned
with pseudo-labels, outperform detectors trained only on a different dataset.
Combined with robust training techniques, the self-supervised detectors reach a
performance close to the ones trained using manual annotations of the target
dataset. Our method is an effective way to improve person detectors during
deployment without any additional labeling effort, and we release our source
code to support relevant robotic applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jia_D/0/1/0/all/0/1"&gt;Dan Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Steinweg_M/0/1/0/all/0/1"&gt;Mats Steinweg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hermans_A/0/1/0/all/0/1"&gt;Alexander Hermans&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leibe_B/0/1/0/all/0/1"&gt;Bastian Leibe&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exposing Backdoors in Robust Machine Learning Models. (arXiv:2003.00865v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.00865</id>
        <link href="http://arxiv.org/abs/2003.00865"/>
        <updated>2021-06-04T01:12:31.627Z</updated>
        <summary type="html"><![CDATA[The introduction of robust optimisation has pushed the state-of-the-art in
defending against adversarial attacks. However, the behaviour of such
optimisation has not been studied in the light of a fundamentally different
class of attacks called backdoors. In this paper, we demonstrate that
adversarially robust models are susceptible to backdoor attacks. Subsequently,
we observe that backdoors are reflected in the feature representation of such
models. Then, this observation is leveraged to detect backdoor-infected models
via a detection technique called AEGIS. Specifically, AEGIS uses feature
clustering to effectively detect backdoor-infected robust Deep Neural Networks
(DNNs). In our evaluation of several visible and hidden backdoor triggers on
major classification tasks using CIFAR-10, MNIST and FMNIST datasets, AEGIS
effectively detects robust DNNs infected with backdoors. AEGIS detects a
backdoor-infected model with 91.6% accuracy, without any false positives.
Furthermore, AEGIS detects the targeted class in the backdoor-infected model
with a reasonably low (11.1%) false positive rate. Our investigation reveals
that salient features of adversarially robust DNNs break the stealthy nature of
backdoor attacks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Soremekun_E/0/1/0/all/0/1"&gt;Ezekiel Soremekun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Udeshi_S/0/1/0/all/0/1"&gt;Sakshi Udeshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chattopadhyay_S/0/1/0/all/0/1"&gt;Sudipta Chattopadhyay&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ZmBART: An Unsupervised Cross-lingual Transfer Framework for Language Generation. (arXiv:2106.01597v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01597</id>
        <link href="http://arxiv.org/abs/2106.01597"/>
        <updated>2021-06-04T01:12:31.621Z</updated>
        <summary type="html"><![CDATA[Despite the recent advancement in NLP research, cross-lingual transfer for
natural language generation is relatively understudied. In this work, we
transfer supervision from high resource language (HRL) to multiple low-resource
languages (LRLs) for natural language generation (NLG). We consider four NLG
tasks (text summarization, question generation, news headline generation, and
distractor generation) and three syntactically diverse languages, i.e.,
English, Hindi, and Japanese. We propose an unsupervised cross-lingual language
generation framework (called ZmBART) that does not use any parallel or
pseudo-parallel/back-translated data. In this framework, we further pre-train
mBART sequence-to-sequence denoising auto-encoder model with an auxiliary task
using monolingual data of three languages. The objective function of the
auxiliary task is close to the target tasks which enriches the multi-lingual
latent representation of mBART and provides good initialization for target
tasks. Then, this model is fine-tuned with task-specific supervised English
data and directly evaluated with low-resource languages in the Zero-shot
setting. To overcome catastrophic forgetting and spurious correlation issues,
we applied freezing model component and data argumentation approaches
respectively. This simple modeling approach gave us promising results.We
experimented with few-shot training (with 1000 supervised data points) which
boosted the model performance further. We performed several ablations and
cross-lingual transferability analyses to demonstrate the robustness of ZmBART.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Maurya_K/0/1/0/all/0/1"&gt;Kaushal Kumar Maurya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Desarkar_M/0/1/0/all/0/1"&gt;Maunendra Sankar Desarkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kano_Y/0/1/0/all/0/1"&gt;Yoshinobu Kano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deepshikha_K/0/1/0/all/0/1"&gt;Kumari Deepshikha&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Grounding Complex Navigational Instructions Using Scene Graphs. (arXiv:2106.01607v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01607</id>
        <link href="http://arxiv.org/abs/2106.01607"/>
        <updated>2021-06-04T01:12:31.615Z</updated>
        <summary type="html"><![CDATA[Training a reinforcement learning agent to carry out natural language
instructions is limited by the available supervision, i.e. knowing when the
instruction has been carried out. We adapt the CLEVR visual question answering
dataset to generate complex natural language navigation instructions and
accompanying scene graphs, yielding an environment-agnostic supervised dataset.
To demonstrate the use of this data set, we map the scenes to the VizDoom
environment and use the architecture in \citet{gatedattention} to train an
agent to carry out these more complex language instructions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jong_M/0/1/0/all/0/1"&gt;Michiel de Jong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krishna_S/0/1/0/all/0/1"&gt;Satyapriya Krishna&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agarwal_A/0/1/0/all/0/1"&gt;Anuva Agarwal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tail-to-Tail Non-Autoregressive Sequence Prediction for Chinese Grammatical Error Correction. (arXiv:2106.01609v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01609</id>
        <link href="http://arxiv.org/abs/2106.01609"/>
        <updated>2021-06-04T01:12:31.609Z</updated>
        <summary type="html"><![CDATA[We investigate the problem of Chinese Grammatical Error Correction (CGEC) and
present a new framework named Tail-to-Tail (\textbf{TtT}) non-autoregressive
sequence prediction to address the deep issues hidden in CGEC. Considering that
most tokens are correct and can be conveyed directly from source to target, and
the error positions can be estimated and corrected based on the bidirectional
context information, thus we employ a BERT-initialized Transformer Encoder as
the backbone model to conduct information modeling and conveying. Considering
that only relying on the same position substitution cannot handle the
variable-length correction cases, various operations such substitution,
deletion, insertion, and local paraphrasing are required jointly. Therefore, a
Conditional Random Fields (CRF) layer is stacked on the up tail to conduct
non-autoregressive sequence prediction by modeling the token dependencies.
Since most tokens are correct and easily to be predicted/conveyed to the
target, then the models may suffer from a severe class imbalance issue. To
alleviate this problem, focal loss penalty strategies are integrated into the
loss functions. Moreover, besides the typical fix-length error correction
datasets, we also construct a variable-length corpus to conduct experiments.
Experimental results on standard datasets, especially on the variable-length
datasets, demonstrate the effectiveness of TtT in terms of sentence-level
Accuracy, Precision, Recall, and F1-Measure on tasks of error Detection and
Correction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1"&gt;Piji Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1"&gt;Shuming Shi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Comparing Acoustic-based Approaches for Alzheimer's Disease Detection. (arXiv:2106.01555v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01555</id>
        <link href="http://arxiv.org/abs/2106.01555"/>
        <updated>2021-06-04T01:12:31.603Z</updated>
        <summary type="html"><![CDATA[In this paper, we study the performance and generalizability of three
approaches for AD detection from speech on the recent ADReSSo challenge
dataset: 1) using conventional acoustic features 2) using novel pre-trained
acoustic embeddings 3) combining acoustic features and embeddings. We find that
while feature-based approaches have a higher precision, classification
approaches relying on the combination of embeddings and features prove to have
a higher, and more balanced performance across multiple metrics of performance.
Our best model, using such a combined approach, outperforms the acoustic
baseline in the challenge by 2.8\%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Balagopalan_A/0/1/0/all/0/1"&gt;Aparna Balagopalan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Novikova_J/0/1/0/all/0/1"&gt;Jekaterina Novikova&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Uncertainty-Aware Few-Shot Image Classification. (arXiv:2010.04525v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.04525</id>
        <link href="http://arxiv.org/abs/2010.04525"/>
        <updated>2021-06-04T01:12:31.597Z</updated>
        <summary type="html"><![CDATA[Few-shot image classification learns to recognize new categories from limited
labelled data. Metric learning based approaches have been widely investigated,
where a query sample is classified by finding the nearest prototype from the
support set based on their feature similarities. A neural network has different
uncertainties on its calculated similarities of different pairs. Understanding
and modeling the uncertainty on the similarity could promote the exploitation
of limited samples in few-shot optimization. In this work, we propose
Uncertainty-Aware Few-Shot framework for image classification by modeling
uncertainty of the similarities of query-support pairs and performing
uncertainty-aware optimization. Particularly, we exploit such uncertainty by
converting observed similarities to probabilistic representations and
incorporate them to the loss for more effective optimization. In order to
jointly consider the similarities between a query and the prototypes in a
support set, a graph-based model is utilized to estimate the uncertainty of the
pairs. Extensive experiments show our proposed method brings significant
improvements on top of a strong baseline and achieves the state-of-the-art
performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhizheng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lan_C/0/1/0/all/0/1"&gt;Cuiling Lan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1"&gt;Wenjun Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhibo Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1"&gt;Shih-Fu Chang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Grounding Complex Navigational Instructions Using Scene Graphs. (arXiv:2106.01607v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01607</id>
        <link href="http://arxiv.org/abs/2106.01607"/>
        <updated>2021-06-04T01:12:31.579Z</updated>
        <summary type="html"><![CDATA[Training a reinforcement learning agent to carry out natural language
instructions is limited by the available supervision, i.e. knowing when the
instruction has been carried out. We adapt the CLEVR visual question answering
dataset to generate complex natural language navigation instructions and
accompanying scene graphs, yielding an environment-agnostic supervised dataset.
To demonstrate the use of this data set, we map the scenes to the VizDoom
environment and use the architecture in \citet{gatedattention} to train an
agent to carry out these more complex language instructions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jong_M/0/1/0/all/0/1"&gt;Michiel de Jong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krishna_S/0/1/0/all/0/1"&gt;Satyapriya Krishna&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agarwal_A/0/1/0/all/0/1"&gt;Anuva Agarwal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automatically Detecting Cyberbullying Comments on Online Game Forums. (arXiv:2106.01598v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01598</id>
        <link href="http://arxiv.org/abs/2106.01598"/>
        <updated>2021-06-04T01:12:31.573Z</updated>
        <summary type="html"><![CDATA[Online game forums are popular to most of game players. They use it to
communicate and discuss the strategy of the game, or even to make friends.
However, game forums also contain abusive and harassment speech, disturbing and
threatening players. Therefore, it is necessary to automatically detect and
remove cyberbullying comments to keep the game forum clean and friendly. We use
the Cyberbullying dataset collected from World of Warcraft (WoW) and League of
Legends (LoL) forums and train classification models to automatically detect
whether a comment of a player is abusive or not. The result obtains 82.69% of
macro F1-score for LoL forum and 83.86% of macro F1-score for WoW forum by the
Toxic-BERT model on the Cyberbullying dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vo_H/0/1/0/all/0/1"&gt;Hanh Hong-Phuc Vo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tran_H/0/1/0/all/0/1"&gt;Hieu Trung Tran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luu_S/0/1/0/all/0/1"&gt;Son T. Luu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TorchIO: a Python library for efficient loading, preprocessing, augmentation and patch-based sampling of medical images in deep learning. (arXiv:2003.04696v4 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.04696</id>
        <link href="http://arxiv.org/abs/2003.04696"/>
        <updated>2021-06-04T01:12:31.567Z</updated>
        <summary type="html"><![CDATA[Processing of medical images such as MRI or CT presents unique challenges
compared to RGB images typically used in computer vision. These include a lack
of labels for large datasets, high computational costs, and metadata to
describe the physical properties of voxels. Data augmentation is used to
artificially increase the size of the training datasets. Training with image
patches decreases the need for computational power. Spatial metadata needs to
be carefully taken into account in order to ensure a correct alignment of
volumes.

We present TorchIO, an open-source Python library to enable efficient
loading, preprocessing, augmentation and patch-based sampling of medical images
for deep learning. TorchIO follows the style of PyTorch and integrates standard
medical image processing libraries to efficiently process images during
training of neural networks. TorchIO transforms can be composed, reproduced,
traced and extended. We provide multiple generic preprocessing and augmentation
operations as well as simulation of MRI-specific artifacts.

Source code, comprehensive tutorials and extensive documentation for TorchIO
can be found at https://github.com/fepegar/torchio. The package can be
installed from the Python Package Index running 'pip install torchio'. It
includes a command-line interface which allows users to apply transforms to
image files without using Python. Additionally, we provide a graphical
interface within a TorchIO extension in 3D Slicer to visualize the effects of
transforms.

TorchIO was developed to help researchers standardize medical image
processing pipelines and allow them to focus on the deep learning experiments.
It encourages open science, as it supports reproducibility and is version
controlled so that the software can be cited precisely. Due to its modularity,
the library is compatible with other frameworks for deep learning with medical
images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Perez_Garcia_F/0/1/0/all/0/1"&gt;Fernando P&amp;#xe9;rez-Garc&amp;#xed;a&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sparks_R/0/1/0/all/0/1"&gt;Rachel Sparks&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ourselin_S/0/1/0/all/0/1"&gt;S&amp;#xe9;bastien Ourselin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Discovering Chatbot's Self-Disclosure's Impact on User Trust, Affinity, and Recommendation Effectiveness. (arXiv:2106.01666v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01666</id>
        <link href="http://arxiv.org/abs/2106.01666"/>
        <updated>2021-06-04T01:12:31.560Z</updated>
        <summary type="html"><![CDATA[In recent years, chatbots have been empowered to engage in social
conversations with humans and have the potential to elicit people to disclose
their personal experiences, opinions, and emotions. However, how and to what
extent people respond to chabots' self-disclosure remain less known. In this
work, we designed a social chatbot with three self-disclosure levels that
conducted small talks and provided relevant recommendations to people. 372
MTurk participants were randomized to one of the four groups with different
self-disclosure levels to converse with the chatbot on two topics, movies, and
COVID-19. We found that people's self-disclosure level was strongly reciprocal
to a chatbot's self-disclosure level. Chatbots' self-disclosure also positively
impacted engagement and users' perception of the bot and led to a more
effective recommendation such that participants enjoyed and agreed more with
the recommendations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liang_K/0/1/0/all/0/1"&gt;Kai-Hui Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1"&gt;Weiyan Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oh_Y/0/1/0/all/0/1"&gt;Yoojung Oh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jingwen Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1"&gt;Zhou Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Few-shot Knowledge Graph-to-Text Generation with Pretrained Language Models. (arXiv:2106.01623v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01623</id>
        <link href="http://arxiv.org/abs/2106.01623"/>
        <updated>2021-06-04T01:12:31.554Z</updated>
        <summary type="html"><![CDATA[This paper studies how to automatically generate a natural language text that
describes the facts in knowledge graph (KG). Considering the few-shot setting,
we leverage the excellent capacities of pretrained language models (PLMs) in
language understanding and generation. We make three major technical
contributions, namely representation alignment for bridging the semantic gap
between KG encodings and PLMs, relation-biased KG linearization for deriving
better input representations, and multi-task learning for learning the
correspondence between KG and text. Extensive experiments on three benchmark
datasets have demonstrated the effectiveness of our model on KG-to-text
generation task. In particular, our model outperforms all comparison methods on
both fully-supervised and few-shot settings. Our code and datasets are
available at https://github.com/RUCAIBox/Few-Shot-KG2Text.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Junyi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_T/0/1/0/all/0/1"&gt;Tianyi Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1"&gt;Wayne Xin Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1"&gt;Zhicheng Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_N/0/1/0/all/0/1"&gt;Nicholas Jing Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1"&gt;Ji-Rong Wen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Discriminative Reasoning for Document-level Relation Extraction. (arXiv:2106.01562v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01562</id>
        <link href="http://arxiv.org/abs/2106.01562"/>
        <updated>2021-06-04T01:12:31.536Z</updated>
        <summary type="html"><![CDATA[Document-level relation extraction (DocRE) models generally use graph
networks to implicitly model the reasoning skill (i.e., pattern recognition,
logical reasoning, coreference reasoning, etc.) related to the relation between
one entity pair in a document. In this paper, we propose a novel discriminative
reasoning framework to explicitly model the paths of these reasoning skills
between each entity pair in this document. Thus, a discriminative reasoning
network is designed to estimate the relation probability distribution of
different reasoning paths based on the constructed graph and vectorized
document contexts for each entity pair, thereby recognizing their relation.
Experimental results show that our method outperforms the previous
state-of-the-art performance on the large-scale DocRE dataset. The code is
publicly available at https://github.com/xwjim/DRN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1"&gt;Wang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1"&gt;Kehai Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1"&gt;Tiejun Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Limitations of Limited Context for Constituency Parsing. (arXiv:2106.01580v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01580</id>
        <link href="http://arxiv.org/abs/2106.01580"/>
        <updated>2021-06-04T01:12:31.530Z</updated>
        <summary type="html"><![CDATA[Incorporating syntax into neural approaches in NLP has a multitude of
practical and scientific benefits. For instance, a language model that is
syntax-aware is likely to be able to produce better samples; even a
discriminative model like BERT with a syntax module could be used for core NLP
tasks like unsupervised syntactic parsing. Rapid progress in recent years was
arguably spurred on by the empirical success of the Parsing-Reading-Predict
architecture of (Shen et al., 2018a), later simplified by the Order Neuron LSTM
of (Shen et al., 2019). Most notably, this is the first time neural approaches
were able to successfully perform unsupervised syntactic parsing (evaluated by
various metrics like F-1 score).

However, even heuristic (much less fully mathematical) understanding of why
and when these architectures work is lagging severely behind. In this work, we
answer representational questions raised by the architectures in (Shen et al.,
2018a, 2019), as well as some transition-based syntax-aware language models
(Dyer et al., 2016): what kind of syntactic structure can current neural
approaches to syntax represent? Concretely, we ground this question in the
sandbox of probabilistic context-free-grammars (PCFGs), and identify a key
aspect of the representational power of these approaches: the amount and
directionality of context that the predictor has access to when forced to make
parsing decision. We show that with limited context (either bounded, or
unidirectional), there are PCFGs, for which these approaches cannot represent
the max-likelihood parse; conversely, if the context is unlimited, they can
represent the max-likelihood parse of any PCFG.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yuchen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Risteski_A/0/1/0/all/0/1"&gt;Andrej Risteski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adjacency List Oriented Relational Fact Extraction via Adaptive Multi-task Learning. (arXiv:2106.01559v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01559</id>
        <link href="http://arxiv.org/abs/2106.01559"/>
        <updated>2021-06-04T01:12:31.523Z</updated>
        <summary type="html"><![CDATA[Relational fact extraction aims to extract semantic triplets from
unstructured text. In this work, we show that all of the relational fact
extraction models can be organized according to a graph-oriented analytical
perspective. An efficient model, aDjacency lIst oRiented rElational faCT
(DIRECT), is proposed based on this analytical framework. To alleviate
challenges of error propagation and sub-task loss equilibrium, DIRECT employs a
novel adaptive multi-task learning strategy with dynamic sub-task loss
balancing. Extensive experiments are conducted on two benchmark datasets, and
results prove that the proposed model outperforms a series of state-of-the-art
(SoTA) models for relational triplet extraction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_F/0/1/0/all/0/1"&gt;Fubang Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1"&gt;Zhuoren Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1"&gt;Yangyang Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1"&gt;Changlong Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiaozhong Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generate, Prune, Select: A Pipeline for Counterspeech Generation against Online Hate Speech. (arXiv:2106.01625v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01625</id>
        <link href="http://arxiv.org/abs/2106.01625"/>
        <updated>2021-06-04T01:12:31.517Z</updated>
        <summary type="html"><![CDATA[Countermeasures to effectively fight the ever increasing hate speech online
without blocking freedom of speech is of great social interest. Natural
Language Generation (NLG), is uniquely capable of developing scalable
solutions. However, off-the-shelf NLG methods are primarily
sequence-to-sequence neural models and they are limited in that they generate
commonplace, repetitive and safe responses regardless of the hate speech (e.g.,
"Please refrain from using such language.") or irrelevant responses, making
them ineffective for de-escalating hateful conversations. In this paper, we
design a three-module pipeline approach to effectively improve the diversity
and relevance. Our proposed pipeline first generates various counterspeech
candidates by a generative model to promote diversity, then filters the
ungrammatical ones using a BERT model, and finally selects the most relevant
counterspeech response using a novel retrieval-based method. Extensive
Experiments on three representative datasets demonstrate the efficacy of our
approach in generating diverse and relevant counterspeech.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1"&gt;Wanzheng Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhat_S/0/1/0/all/0/1"&gt;Suma Bhat&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Systematic Investigation of KB-Text Embedding Alignment at Scale. (arXiv:2106.01586v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01586</id>
        <link href="http://arxiv.org/abs/2106.01586"/>
        <updated>2021-06-04T01:12:31.500Z</updated>
        <summary type="html"><![CDATA[Knowledge bases (KBs) and text often contain complementary knowledge: KBs
store structured knowledge that can support long range reasoning, while text
stores more comprehensive and timely knowledge in an unstructured way.
Separately embedding the individual knowledge sources into vector spaces has
demonstrated tremendous successes in encoding the respective knowledge, but how
to jointly embed and reason with both knowledge sources to fully leverage the
complementary information is still largely an open problem. We conduct a
large-scale, systematic investigation of aligning KB and text embeddings for
joint reasoning. We set up a novel evaluation framework with two evaluation
tasks, few-shot link prediction and analogical reasoning, and evaluate an array
of KB-text embedding alignment methods. We also demonstrate how such alignment
can infuse textual information into KB embeddings for more accurate link
prediction on emerging entities and events, using COVID-19 as a case study.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pahuja_V/0/1/0/all/0/1"&gt;Vardaan Pahuja&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1"&gt;Yu Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1"&gt;Wenhu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bahrami_M/0/1/0/all/0/1"&gt;Mehdi Bahrami&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Lei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1"&gt;Wei-Peng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1"&gt;Yu Su&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[When Vision Transformers Outperform ResNets without Pretraining or Strong Data Augmentations. (arXiv:2106.01548v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.01548</id>
        <link href="http://arxiv.org/abs/2106.01548"/>
        <updated>2021-06-04T01:12:31.333Z</updated>
        <summary type="html"><![CDATA[Vision Transformers (ViTs) and MLPs signal further efforts on replacing
hand-wired features or inductive biases with general-purpose neural
architectures. Existing works empower the models by massive data, such as
large-scale pretraining and/or repeated strong data augmentations, and still
report optimization-related problems (e.g., sensitivity to initialization and
learning rate). Hence, this paper investigates ViTs and MLP-Mixers from the
lens of loss geometry, intending to improve the models' data efficiency at
training and generalization at inference. Visualization and Hessian reveal
extremely sharp local minima of converged models. By promoting smoothness with
a recently proposed sharpness-aware optimizer, we substantially improve the
accuracy and robustness of ViTs and MLP-Mixers on various tasks spanning
supervised, adversarial, contrastive, and transfer learning (e.g., +5.3\% and
+11.0\% top-1 accuracy on ImageNet for ViT-B/16 and Mixer-B/16, respectively,
with the simple Inception-style preprocessing). We show that the improved
smoothness attributes to sparser active neurons in the first few layers. The
resultant ViTs outperform ResNets of similar size and throughput when trained
from scratch on ImageNet without large-scale pretraining or strong data
augmentations. They also possess more perceptive attention maps.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xiangning Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1"&gt;Cho-Jui Hsieh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_B/0/1/0/all/0/1"&gt;Boqing Gong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SSMD: Semi-Supervised Medical Image Detection with Adaptive Consistency and Heterogeneous Perturbation. (arXiv:2106.01544v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.01544</id>
        <link href="http://arxiv.org/abs/2106.01544"/>
        <updated>2021-06-04T01:12:31.327Z</updated>
        <summary type="html"><![CDATA[Semi-Supervised classification and segmentation methods have been widely
investigated in medical image analysis. Both approaches can improve the
performance of fully-supervised methods with additional unlabeled data.
However, as a fundamental task, semi-supervised object detection has not gained
enough attention in the field of medical image analysis. In this paper, we
propose a novel Semi-Supervised Medical image Detector (SSMD). The motivation
behind SSMD is to provide free yet effective supervision for unlabeled data, by
regularizing the predictions at each position to be consistent. To achieve the
above idea, we develop a novel adaptive consistency cost function to regularize
different components in the predictions. Moreover, we introduce heterogeneous
perturbation strategies that work in both feature space and image space, so
that the proposed detector is promising to produce powerful image
representations and robust predictions. Extensive experimental results show
that the proposed SSMD achieves the state-of-the-art performance at a wide
range of settings. We also demonstrate the strength of each proposed module
with comprehensive ablation studies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1"&gt;Hong-Yu Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chengdi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Haofeng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1"&gt;Gang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1"&gt;Weimin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1"&gt;Yizhou Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Personalizing Pre-trained Models. (arXiv:2106.01499v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.01499</id>
        <link href="http://arxiv.org/abs/2106.01499"/>
        <updated>2021-06-04T01:12:31.320Z</updated>
        <summary type="html"><![CDATA[Self-supervised or weakly supervised models trained on large-scale datasets
have shown sample-efficient transfer to diverse datasets in few-shot settings.
We consider how upstream pretrained models can be leveraged for downstream
few-shot, multilabel, and continual learning tasks. Our model CLIPPER (CLIP
PERsonalized) uses image representations from CLIP, a large-scale image
representation learning model trained using weak natural language supervision.
We developed a technique, called Multi-label Weight Imprinting (MWI), for
multi-label, continual, and few-shot learning, and CLIPPER uses MWI with image
representations from CLIP. We evaluated CLIPPER on 10 single-label and 5
multi-label datasets. Our model shows robust and competitive performance, and
we set new benchmarks for few-shot, multi-label, and continual learning. Our
lightweight technique is also compute-efficient and enables privacy-preserving
applications as the data is not sent to the upstream model for fine-tuning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1"&gt;Mina Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srivatsa_P/0/1/0/all/0/1"&gt;P Srivatsa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rane_A/0/1/0/all/0/1"&gt;Advait Rane&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chenniappa_S/0/1/0/all/0/1"&gt;Shriram Chenniappa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hazariwala_A/0/1/0/all/0/1"&gt;Asadali Hazariwala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maes_P/0/1/0/all/0/1"&gt;Pattie Maes&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transferable Adversarial Examples for Anchor Free Object Detection. (arXiv:2106.01618v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.01618</id>
        <link href="http://arxiv.org/abs/2106.01618"/>
        <updated>2021-06-04T01:12:31.314Z</updated>
        <summary type="html"><![CDATA[Deep neural networks have been demonstrated to be vulnerable to adversarial
attacks: subtle perturbation can completely change prediction result. The
vulnerability has led to a surge of research in this direction, including
adversarial attacks on object detection networks. However, previous studies are
dedicated to attacking anchor-based object detectors. In this paper, we present
the first adversarial attack on anchor-free object detectors. It conducts
category-wise, instead of previously instance-wise, attacks on object
detectors, and leverages high-level semantic information to efficiently
generate transferable adversarial examples, which can also be transferred to
attack other object detectors, even anchor-based detectors such as Faster
R-CNN. Experimental results on two benchmark datasets demonstrate that our
proposed method achieves state-of-the-art performance and transferability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liao_Q/0/1/0/all/0/1"&gt;Quanyu Liao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kong_B/0/1/0/all/0/1"&gt;Bin Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lyu_S/0/1/0/all/0/1"&gt;Siwei Lyu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_B/0/1/0/all/0/1"&gt;Bin Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1"&gt;Youbing Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_Q/0/1/0/all/0/1"&gt;Qi Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1"&gt;Xi Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cross-Domain First Person Audio-Visual Action Recognition through Relative Norm Alignment. (arXiv:2106.01689v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.01689</id>
        <link href="http://arxiv.org/abs/2106.01689"/>
        <updated>2021-06-04T01:12:31.308Z</updated>
        <summary type="html"><![CDATA[First person action recognition is an increasingly researched topic because
of the growing popularity of wearable cameras. This is bringing to light
cross-domain issues that are yet to be addressed in this context. Indeed, the
information extracted from learned representations suffers from an intrinsic
environmental bias. This strongly affects the ability to generalize to unseen
scenarios, limiting the application of current methods in real settings where
trimmed labeled data are not available during training. In this work, we
propose to leverage over the intrinsic complementary nature of audio-visual
signals to learn a representation that works well on data seen during training,
while being able to generalize across different domains. To this end, we
introduce an audio-visual loss that aligns the contributions from the two
modalities by acting on the magnitude of their feature norm representations.
This new loss, plugged into a minimal multi-modal action recognition
architecture, leads to strong results in cross-domain first person action
recognition, as demonstrated by extensive experiments on the popular
EPIC-Kitchens dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Planamente_M/0/1/0/all/0/1"&gt;Mirco Planamente&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Plizzari_C/0/1/0/all/0/1"&gt;Chiara Plizzari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alberti_E/0/1/0/all/0/1"&gt;Emanuele Alberti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Caputo_B/0/1/0/all/0/1"&gt;Barbara Caputo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GMAIR: Unsupervised Object Detection Based on Spatial Attention and Gaussian Mixture. (arXiv:2106.01722v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.01722</id>
        <link href="http://arxiv.org/abs/2106.01722"/>
        <updated>2021-06-04T01:12:31.302Z</updated>
        <summary type="html"><![CDATA[Recent studies on unsupervised object detection based on spatial attention
have achieved promising results. Models, such as AIR and SPAIR, output "what"
and "where" latent variables that represent the attributes and locations of
objects in a scene, respectively. Most of the previous studies concentrate on
the "where" localization performance; however, we claim that acquiring "what"
object attributes is also essential for representation learning. This paper
presents a framework, GMAIR, for unsupervised object detection. It incorporates
spatial attention and a Gaussian mixture in a unified deep generative model.
GMAIR can locate objects in a scene and simultaneously cluster them without
supervision. Furthermore, we analyze the "what" latent variables and clustering
process. Finally, we evaluate our model on MultiMNIST and Fruit2D datasets and
show that GMAIR achieves competitive results on localization and clustering
compared to state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1"&gt;Weijin Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1"&gt;Yao Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1"&gt;Linfeng Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sanchez_L/0/1/0/all/0/1"&gt;Lizeth Patricia Aguirre Sanchez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PDPGD: Primal-Dual Proximal Gradient Descent Adversarial Attack. (arXiv:2106.01538v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01538</id>
        <link href="http://arxiv.org/abs/2106.01538"/>
        <updated>2021-06-04T01:12:31.284Z</updated>
        <summary type="html"><![CDATA[State-of-the-art deep neural networks are sensitive to small input
perturbations. Since the discovery of this intriguing vulnerability, many
defence methods have been proposed that attempt to improve robustness to
adversarial noise. Fast and accurate attacks are required to compare various
defence methods. However, evaluating adversarial robustness has proven to be
extremely challenging. Existing norm minimisation adversarial attacks require
thousands of iterations (e.g. Carlini & Wagner attack), are limited to the
specific norms (e.g. Fast Adaptive Boundary), or produce sub-optimal results
(e.g. Brendel & Bethge attack). On the other hand, PGD attack, which is fast,
general and accurate, ignores the norm minimisation penalty and solves a
simpler perturbation-constrained problem. In this work, we introduce a fast,
general and accurate adversarial attack that optimises the original non-convex
constrained minimisation problem. We interpret optimising the Lagrangian of the
adversarial attack optimisation problem as a two-player game: the first player
minimises the Lagrangian wrt the adversarial noise; the second player maximises
the Lagrangian wrt the regularisation penalty. Our attack algorithm
simultaneously optimises primal and dual variables to find the minimal
adversarial perturbation. In addition, for non-smooth $l_p$-norm minimisation,
such as $l_{\infty}$-, $l_1$-, and $l_0$-norms, we introduce primal-dual
proximal gradient descent attack. We show in the experiments that our attack
outperforms current state-of-the-art $l_{\infty}$-, $l_2$-, $l_1$-, and
$l_0$-attacks on MNIST, CIFAR-10 and Restricted ImageNet datasets against
unregularised and adversarially trained models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Matyasko_A/0/1/0/all/0/1"&gt;Alexander Matyasko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chau_L/0/1/0/all/0/1"&gt;Lap-Pui Chau&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deconfounded Video Moment Retrieval with Causal Intervention. (arXiv:2106.01534v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.01534</id>
        <link href="http://arxiv.org/abs/2106.01534"/>
        <updated>2021-06-04T01:12:31.278Z</updated>
        <summary type="html"><![CDATA[We tackle the task of video moment retrieval (VMR), which aims to localize a
specific moment in a video according to a textual query. Existing methods
primarily model the matching relationship between query and moment by complex
cross-modal interactions. Despite their effectiveness, current models mostly
exploit dataset biases while ignoring the video content, thus leading to poor
generalizability. We argue that the issue is caused by the hidden confounder in
VMR, {i.e., temporal location of moments}, that spuriously correlates the model
input and prediction. How to design robust matching models against the temporal
location biases is crucial but, as far as we know, has not been studied yet for
VMR.

To fill the research gap, we propose a causality-inspired VMR framework that
builds structural causal model to capture the true effect of query and video
content on the prediction. Specifically, we develop a Deconfounded Cross-modal
Matching (DCM) method to remove the confounding effects of moment location. It
first disentangles moment representation to infer the core feature of visual
content, and then applies causal intervention on the disentangled multimodal
input based on backdoor adjustment, which forces the model to fairly
incorporate each possible location of the target into consideration. Extensive
experiments clearly show that our approach can achieve significant improvement
over the state-of-the-art methods in terms of both accuracy and generalization
(Codes:
\color{blue}{\url{https://github.com/Xun-Yang/Causal_Video_Moment_Retrieval}}]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xun Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_F/0/1/0/all/0/1"&gt;Fuli Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_W/0/1/0/all/0/1"&gt;Wei Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1"&gt;Meng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1"&gt;Tat-Seng Chua&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Barbershop: GAN-based Image Compositing using Segmentation Masks. (arXiv:2106.01505v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.01505</id>
        <link href="http://arxiv.org/abs/2106.01505"/>
        <updated>2021-06-04T01:12:31.272Z</updated>
        <summary type="html"><![CDATA[Seamlessly blending features from multiple images is extremely challenging
because of complex relationships in lighting, geometry, and partial occlusion
which cause coupling between different parts of the image. Even though recent
work on GANs enables synthesis of realistic hair or faces, it remains difficult
to combine them into a single, coherent, and plausible image rather than a
disjointed set of image patches. We present a novel solution to image blending,
particularly for the problem of hairstyle transfer, based on GAN-inversion. We
propose a novel latent space for image blending which is better at preserving
detail and encoding spatial information, and propose a new GAN-embedding
algorithm which is able to slightly modify images to conform to a common
segmentation mask. Our novel representation enables the transfer of the visual
properties from multiple reference images including specific details such as
moles and wrinkles, and because we do image blending in a latent-space we are
able to synthesize images that are coherent. Our approach avoids blending
artifacts present in other approaches and finds a globally consistent image.
Our results demonstrate a significant improvement over the current state of the
art in a user study, with users preferring our blending solution over 95
percent of the time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_P/0/1/0/all/0/1"&gt;Peihao Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abdal_R/0/1/0/all/0/1"&gt;Rameen Abdal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Femiani_J/0/1/0/all/0/1"&gt;John Femiani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wonka_P/0/1/0/all/0/1"&gt;Peter Wonka&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[One Representation to Rule Them All: Identifying Out-of-Support Examples in Few-shot Learning with Generic Representations. (arXiv:2106.01423v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01423</id>
        <link href="http://arxiv.org/abs/2106.01423"/>
        <updated>2021-06-04T01:12:31.265Z</updated>
        <summary type="html"><![CDATA[The field of few-shot learning has made remarkable strides in developing
powerful models that can operate in the small data regime. Nearly all of these
methods assume every unlabeled instance encountered will belong to a handful of
known classes for which one has examples. This can be problematic for
real-world use cases where one routinely finds 'none-of-the-above' examples. In
this paper we describe this challenge of identifying what we term
'out-of-support' (OOS) examples. We describe how this problem is subtly
different from out-of-distribution detection and describe a new method of
identifying OOS examples within the Prototypical Networks framework using a
fixed point which we call the generic representation. We show that our method
outperforms other existing approaches in the literature as well as other
approaches that we propose in this paper. Finally, we investigate how the use
of such a generic point affects the geometry of a model's feature space.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kvinge_H/0/1/0/all/0/1"&gt;Henry Kvinge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Howland_S/0/1/0/all/0/1"&gt;Scott Howland&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Courts_N/0/1/0/all/0/1"&gt;Nico Courts&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Phillips_L/0/1/0/all/0/1"&gt;Lauren A. Phillips&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Buckheit_J/0/1/0/all/0/1"&gt;John Buckheit&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+New_Z/0/1/0/all/0/1"&gt;Zachary New&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Skomski_E/0/1/0/all/0/1"&gt;Elliott Skomski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jung H. Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tiwari_S/0/1/0/all/0/1"&gt;Sandeep Tiwari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hibler_J/0/1/0/all/0/1"&gt;Jessica Hibler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Corley_C/0/1/0/all/0/1"&gt;Courtney D. Corley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hodas_N/0/1/0/all/0/1"&gt;Nathan O. Hodas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring Memorization in Adversarial Training. (arXiv:2106.01606v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01606</id>
        <link href="http://arxiv.org/abs/2106.01606"/>
        <updated>2021-06-04T01:12:31.258Z</updated>
        <summary type="html"><![CDATA[It is well known that deep learning models have a propensity for fitting the
entire training set even with random labels, which requires memorization of
every training sample. In this paper, we investigate the memorization effect in
adversarial training (AT) for promoting a deeper understanding of capacity,
convergence, generalization, and especially robust overfitting of adversarially
trained classifiers. We first demonstrate that deep networks have sufficient
capacity to memorize adversarial examples of training data with completely
random labels, but not all AT algorithms can converge under the extreme
circumstance. Our study of AT with random labels motivates further analyses on
the convergence and generalization of AT. We find that some AT methods suffer
from a gradient instability issue, and the recently suggested complexity
measures cannot explain robust generalization by considering models trained on
random labels. Furthermore, we identify a significant drawback of memorization
in AT that it could result in robust overfitting. We then propose a new
mitigation algorithm motivated by detailed memorization analyses. Extensive
experiments on various datasets validate the effectiveness of the proposed
method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1"&gt;Yinpeng Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1"&gt;Ke Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xiao Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pang_T/0/1/0/all/0/1"&gt;Tianyu Pang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1"&gt;Zhijie Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1"&gt;Hang Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jun Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NTIRE 2021 Challenge on High Dynamic Range Imaging: Dataset, Methods and Results. (arXiv:2106.01439v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.01439</id>
        <link href="http://arxiv.org/abs/2106.01439"/>
        <updated>2021-06-04T01:12:31.241Z</updated>
        <summary type="html"><![CDATA[This paper reviews the first challenge on high-dynamic range (HDR) imaging
that was part of the New Trends in Image Restoration and Enhancement (NTIRE)
workshop, held in conjunction with CVPR 2021. This manuscript focuses on the
newly introduced dataset, the proposed methods and their results. The challenge
aims at estimating a HDR image from one or multiple respective low-dynamic
range (LDR) observations, which might suffer from under- or over-exposed
regions and different sources of noise. The challenge is composed by two
tracks: In Track 1 only a single LDR image is provided as input, whereas in
Track 2 three differently-exposed LDR images with inter-frame motion are
available. In both tracks, the ultimate goal is to achieve the best objective
HDR reconstruction in terms of PSNR with respect to a ground-truth image,
evaluated both directly and with a canonical tonemapping operation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Perez_Pellitero_E/0/1/0/all/0/1"&gt;Eduardo P&amp;#xe9;rez-Pellitero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Catley_Chandar_S/0/1/0/all/0/1"&gt;Sibi Catley-Chandar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leonardis_A/0/1/0/all/0/1"&gt;Ale&amp;#x161; Leonardis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Timofte_R/0/1/0/all/0/1"&gt;Radu Timofte&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving the Transferability of Adversarial Examples with New Iteration Framework and Input Dropout. (arXiv:2106.01617v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01617</id>
        <link href="http://arxiv.org/abs/2106.01617"/>
        <updated>2021-06-04T01:12:31.235Z</updated>
        <summary type="html"><![CDATA[Deep neural networks(DNNs) is vulnerable to be attacked by adversarial
examples. Black-box attack is the most threatening attack. At present,
black-box attack methods mainly adopt gradient-based iterative attack methods,
which usually limit the relationship between the iteration step size, the
number of iterations, and the maximum perturbation. In this paper, we propose a
new gradient iteration framework, which redefines the relationship between the
above three. Under this framework, we easily improve the attack success rate of
DI-TI-MIM. In addition, we propose a gradient iterative attack method based on
input dropout, which can be well combined with our framework. We further
propose a multi dropout rate version of this method. Experimental results show
that our best method can achieve attack success rate of 96.2\% for defense
model on average, which is higher than the state-of-the-art gradient-based
attacks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_P/0/1/0/all/0/1"&gt;Pengfei Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Linyuan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_R/0/1/0/all/0/1"&gt;Ruoxi Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiao_K/0/1/0/all/0/1"&gt;Kai Qiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1"&gt;Shuhao Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_G/0/1/0/all/0/1"&gt;Guoen Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1"&gt;Bin Yan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Grounding Complex Navigational Instructions Using Scene Graphs. (arXiv:2106.01607v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01607</id>
        <link href="http://arxiv.org/abs/2106.01607"/>
        <updated>2021-06-04T01:12:31.229Z</updated>
        <summary type="html"><![CDATA[Training a reinforcement learning agent to carry out natural language
instructions is limited by the available supervision, i.e. knowing when the
instruction has been carried out. We adapt the CLEVR visual question answering
dataset to generate complex natural language navigation instructions and
accompanying scene graphs, yielding an environment-agnostic supervised dataset.
To demonstrate the use of this data set, we map the scenes to the VizDoom
environment and use the architecture in \citet{gatedattention} to train an
agent to carry out these more complex language instructions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jong_M/0/1/0/all/0/1"&gt;Michiel de Jong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krishna_S/0/1/0/all/0/1"&gt;Satyapriya Krishna&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agarwal_A/0/1/0/all/0/1"&gt;Anuva Agarwal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Cognitive Regularizer for Language Modeling. (arXiv:2105.07144v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.07144</id>
        <link href="http://arxiv.org/abs/2105.07144"/>
        <updated>2021-06-04T01:12:31.222Z</updated>
        <summary type="html"><![CDATA[The uniform information density (UID) hypothesis, which posits that speakers
behaving optimally tend to distribute information uniformly across a linguistic
signal, has gained traction in psycholinguistics as an explanation for certain
syntactic, morphological, and prosodic choices. In this work, we explore
whether the UID hypothesis can be operationalized as an inductive bias for
statistical language modeling. Specifically, we augment the canonical MLE
objective for training language models with a regularizer that encodes UID. In
experiments on ten languages spanning five language families, we find that
using UID regularization consistently improves perplexity in language models,
having a larger effect when training data is limited. Moreover, via an analysis
of generated sequences, we find that UID-regularized language models have other
desirable properties, e.g., they generate text that is more lexically diverse.
Our results not only suggest that UID is a reasonable inductive bias for
language modeling, but also provide an alternative validation of the UID
hypothesis using modern-day NLP tools.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1"&gt;Jason Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meister_C/0/1/0/all/0/1"&gt;Clara Meister&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1"&gt;Ryan Cotterell&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Imperceptible Adversarial Examples for Fake Image Detection. (arXiv:2106.01615v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.01615</id>
        <link href="http://arxiv.org/abs/2106.01615"/>
        <updated>2021-06-04T01:12:31.216Z</updated>
        <summary type="html"><![CDATA[Fooling people with highly realistic fake images generated with Deepfake or
GANs brings a great social disturbance to our society. Many methods have been
proposed to detect fake images, but they are vulnerable to adversarial
perturbations -- intentionally designed noises that can lead to the wrong
prediction. Existing methods of attacking fake image detectors usually generate
adversarial perturbations to perturb almost the entire image. This is redundant
and increases the perceptibility of perturbations. In this paper, we propose a
novel method to disrupt the fake image detection by determining key pixels to a
fake image detector and attacking only the key pixels, which results in the
$L_0$ and the $L_2$ norms of adversarial perturbations much less than those of
existing works. Experiments on two public datasets with three fake image
detectors indicate that our proposed method achieves state-of-the-art
performance in both white-box and black-box attacks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liao_Q/0/1/0/all/0/1"&gt;Quanyu Liao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yuezun Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kong_B/0/1/0/all/0/1"&gt;Bin Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_B/0/1/0/all/0/1"&gt;Bin Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lyu_S/0/1/0/all/0/1"&gt;Siwei Lyu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1"&gt;Youbing Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_Q/0/1/0/all/0/1"&gt;Qi Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1"&gt;Xi Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Not All Knowledge Is Created Equal. (arXiv:2106.01489v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01489</id>
        <link href="http://arxiv.org/abs/2106.01489"/>
        <updated>2021-06-04T01:12:31.198Z</updated>
        <summary type="html"><![CDATA[Mutual knowledge distillation (MKD) improves a model by distilling knowledge
from another model. However, not all knowledge is certain and correct,
especially under adverse conditions. For example, label noise usually leads to
less reliable models due to the undesired memorisation [1, 2]. Wrong knowledge
misleads the learning rather than helps. This problem can be handled by two
aspects: (i) improving the reliability of a model where the knowledge is from
(i.e., knowledge source's reliability); (ii) selecting reliable knowledge for
distillation. In the literature, making a model more reliable is widely studied
while selective MKD receives little attention. Therefore, we focus on studying
selective MKD and highlight its importance in this work.

Concretely, a generic MKD framework, Confident knowledge selection followed
by Mutual Distillation (CMD), is designed. The key component of CMD is a
generic knowledge selection formulation, making the selection threshold either
static (CMD-S) or progressive (CMD-P). Additionally, CMD covers two special
cases: zero knowledge and all knowledge, leading to a unified MKD framework. We
empirically find CMD-P performs better than CMD-S. The main reason is that a
model's knowledge upgrades and becomes confident as the training progresses.

Extensive experiments are present to demonstrate the effectiveness of CMD and
thoroughly justify the design of CMD. For example, CMD-P obtains new
state-of-the-art results in robustness against label noise.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Ziyun Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xinshao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Haojin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_D/0/1/0/all/0/1"&gt;Di Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Robertson_N/0/1/0/all/0/1"&gt;Neil M. Robertson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Clifton_D/0/1/0/all/0/1"&gt;David A. Clifton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meinel_C/0/1/0/all/0/1"&gt;Christoph Meinel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Container: Context Aggregation Network. (arXiv:2106.01401v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.01401</id>
        <link href="http://arxiv.org/abs/2106.01401"/>
        <updated>2021-06-04T01:12:31.192Z</updated>
        <summary type="html"><![CDATA[Convolutional neural networks (CNNs) are ubiquitous in computer vision, with
a myriad of effective and efficient variations. Recently, Transformers --
originally introduced in natural language processing -- have been increasingly
adopted in computer vision. While early adopters continue to employ CNN
backbones, the latest networks are end-to-end CNN-free Transformer solutions. A
recent surprising finding shows that a simple MLP based solution without any
traditional convolutional or Transformer components can produce effective
visual representations. While CNNs, Transformers and MLP-Mixers may be
considered as completely disparate architectures, we provide a unified view
showing that they are in fact special cases of a more general method to
aggregate spatial context in a neural network stack. We present the \model
(CONText AggregatIon NEtwoRk), a general-purpose building block for multi-head
context aggregation that can exploit long-range interactions \emph{a la}
Transformers while still exploiting the inductive bias of the local convolution
operation leading to faster convergence speeds, often seen in CNNs. In contrast
to Transformer-based methods that do not scale well to downstream tasks that
rely on larger input image resolutions, our efficient network, named
\modellight, can be employed in object detection and instance segmentation
networks such as DETR, RetinaNet and Mask-RCNN to obtain an impressive
detection mAP of 38.9, 43.8, 45.1 and mask mAP of 41.3, providing large
improvements of 6.6, 7.3, 6.9 and 6.6 pts respectively, compared to a ResNet-50
backbone with a comparable compute and parameter size. Our method also achieves
promising results on self-supervised learning compared to DeiT on the DINO
framework.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1"&gt;Peng Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1"&gt;Jiasen Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Hongsheng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mottaghi_R/0/1/0/all/0/1"&gt;Roozbeh Mottaghi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kembhavi_A/0/1/0/all/0/1"&gt;Aniruddha Kembhavi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fingerprinting Fine-tuned Language Models in the Wild. (arXiv:2106.01703v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01703</id>
        <link href="http://arxiv.org/abs/2106.01703"/>
        <updated>2021-06-04T01:12:31.186Z</updated>
        <summary type="html"><![CDATA[There are concerns that the ability of language models (LMs) to generate high
quality synthetic text can be misused to launch spam, disinformation, or
propaganda. Therefore, the research community is actively working on developing
approaches to detect whether a given text is organic or synthetic. While this
is a useful first step, it is important to be able to further fingerprint the
author LM to attribute its origin. Prior work on fingerprinting LMs is limited
to attributing synthetic text generated by a handful (usually < 10) of
pre-trained LMs. However, LMs such as GPT2 are commonly fine-tuned in a myriad
of ways (e.g., on a domain-specific text corpus) before being used to generate
synthetic text. It is challenging to fingerprinting fine-tuned LMs because the
universe of fine-tuned LMs is much larger in realistic scenarios. To address
this challenge, we study the problem of large-scale fingerprinting of
fine-tuned LMs in the wild. Using a real-world dataset of synthetic text
generated by 108 different fine-tuned LMs, we conduct comprehensive experiments
to demonstrate the limitations of existing fingerprinting approaches. Our
results show that fine-tuning itself is the most effective in attributing the
synthetic text generated by fine-tuned LMs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Diwan_N/0/1/0/all/0/1"&gt;Nirav Diwan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chakravorty_T/0/1/0/all/0/1"&gt;Tanmoy Chakravorty&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shafiq_Z/0/1/0/all/0/1"&gt;Zubair Shafiq&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeepCompress: Efficient Point Cloud Geometry Compression. (arXiv:2106.01504v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.01504</id>
        <link href="http://arxiv.org/abs/2106.01504"/>
        <updated>2021-06-04T01:12:31.180Z</updated>
        <summary type="html"><![CDATA[Point clouds are a basic data type that is increasingly of interest as 3D
content becomes more ubiquitous. Applications using point clouds include
virtual, augmented, and mixed reality and autonomous driving. We propose a more
efficient deep learning-based encoder architecture for point clouds compression
that incorporates principles from established 3D object detection and image
compression architectures. Through an ablation study, we show that
incorporating the learned activation function from Computational Efficient
Neural Image Compression (CENIC) and designing more parameter-efficient
convolutional blocks yields dramatic gains in efficiency and performance. Our
proposed architecture incorporates Generalized Divisive Normalization
activations and propose a spatially separable InceptionV4-inspired block. We
then evaluate rate-distortion curves on the standard JPEG Pleno 8i Voxelized
Full Bodies dataset to evaluate our model's performance. Our proposed
modifications outperform the baseline approaches by a small margin in terms of
Bjontegard delta rate and PSNR values, yet reduces necessary encoder
convolution operations by 8 percent and reduces total encoder parameters by 20
percent. Our proposed architecture, when considered on its own, has a small
penalty of 0.02 percent in Chamfer's Distance and 0.32 percent increased bit
rate in Point to Plane Distance for the same peak signal-to-noise ratio.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Killea_R/0/1/0/all/0/1"&gt;Ryan Killea&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yun Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bastani_S/0/1/0/all/0/1"&gt;Saeed Bastani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McLachlan_P/0/1/0/all/0/1"&gt;Paul McLachlan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CT-Net: Channel Tensorization Network for Video Classification. (arXiv:2106.01603v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.01603</id>
        <link href="http://arxiv.org/abs/2106.01603"/>
        <updated>2021-06-04T01:12:31.173Z</updated>
        <summary type="html"><![CDATA[3D convolution is powerful for video classification but often computationally
expensive, recent studies mainly focus on decomposing it on spatial-temporal
and/or channel dimensions. Unfortunately, most approaches fail to achieve a
preferable balance between convolutional efficiency and feature-interaction
sufficiency. For this reason, we propose a concise and novel Channel
Tensorization Network (CT-Net), by treating the channel dimension of input
feature as a multiplication of K sub-dimensions. On one hand, it naturally
factorizes convolution in a multiple dimension way, leading to a light
computation burden. On the other hand, it can effectively enhance feature
interaction from different channels, and progressively enlarge the 3D receptive
field of such interaction to boost classification accuracy. Furthermore, we
equip our CT-Module with a Tensor Excitation (TE) mechanism. It can learn to
exploit spatial, temporal and channel attention in a high-dimensional manner,
to improve the cooperative power of all the feature dimensions in our
CT-Module. Finally, we flexibly adapt ResNet as our CT-Net. Extensive
experiments are conducted on several challenging video benchmarks, e.g.,
Kinetics-400, Something-Something V1 and V2. Our CT-Net outperforms a number of
recent SOTA approaches, in terms of accuracy and/or efficiency. The codes and
models will be available on https://github.com/Andy1621/CT-Net.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1"&gt;Kunchang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xianhang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yali Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jun Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1"&gt;Yu Qiao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Noise Doesn't Lie: Towards Universal Detection of Deep Inpainting. (arXiv:2106.01532v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.01532</id>
        <link href="http://arxiv.org/abs/2106.01532"/>
        <updated>2021-06-04T01:12:31.166Z</updated>
        <summary type="html"><![CDATA[Deep image inpainting aims to restore damaged or missing regions in an image
with realistic contents. While having a wide range of applications such as
object removal and image recovery, deep inpainting techniques also have the
risk of being manipulated for image forgery. A promising countermeasure against
such forgeries is deep inpainting detection, which aims to locate the inpainted
regions in an image. In this paper, we make the first attempt towards universal
detection of deep inpainting, where the detection network can generalize well
when detecting different deep inpainting methods. To this end, we first propose
a novel data generation approach to generate a universal training dataset,
which imitates the noise discrepancies exist in real versus inpainted image
contents to train universal detectors. We then design a Noise-Image
Cross-fusion Network (NIX-Net) to effectively exploit the discriminative
information contained in both the images and their noise patterns. We
empirically show, on multiple benchmark datasets, that our approach outperforms
existing detection methods by a large margin and generalize well to unseen deep
inpainting techniques. Our universal training dataset can also significantly
boost the generalizability of existing detection methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1"&gt;Ang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ke_Q/0/1/0/all/0/1"&gt;Qiuhong Ke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1"&gt;Xingjun Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weng_H/0/1/0/all/0/1"&gt;Haiqin Weng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zong_Z/0/1/0/all/0/1"&gt;Zhiyuan Zong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xue_F/0/1/0/all/0/1"&gt;Feng Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Rui Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Select: A Fully Attentive Approach for Novel Object Captioning. (arXiv:2106.01424v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.01424</id>
        <link href="http://arxiv.org/abs/2106.01424"/>
        <updated>2021-06-04T01:12:31.150Z</updated>
        <summary type="html"><![CDATA[Image captioning models have lately shown impressive results when applied to
standard datasets. Switching to real-life scenarios, however, constitutes a
challenge due to the larger variety of visual concepts which are not covered in
existing training sets. For this reason, novel object captioning (NOC) has
recently emerged as a paradigm to test captioning models on objects which are
unseen during the training phase. In this paper, we present a novel approach
for NOC that learns to select the most relevant objects of an image, regardless
of their adherence to the training set, and to constrain the generative process
of a language model accordingly. Our architecture is fully-attentive and
end-to-end trainable, also when incorporating constraints. We perform
experiments on the held-out COCO dataset, where we demonstrate improvements
over the state of the art, both in terms of adaptability to novel objects and
caption quality.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cagrandi_M/0/1/0/all/0/1"&gt;Marco Cagrandi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cornia_M/0/1/0/all/0/1"&gt;Marcella Cornia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stefanini_M/0/1/0/all/0/1"&gt;Matteo Stefanini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baraldi_L/0/1/0/all/0/1"&gt;Lorenzo Baraldi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cucchiara_R/0/1/0/all/0/1"&gt;Rita Cucchiara&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multiscale Domain Adaptive YOLO for Cross-Domain Object Detection. (arXiv:2106.01483v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.01483</id>
        <link href="http://arxiv.org/abs/2106.01483"/>
        <updated>2021-06-04T01:12:31.140Z</updated>
        <summary type="html"><![CDATA[The area of domain adaptation has been instrumental in addressing the domain
shift problem encountered by many applications. This problem arises due to the
difference between the distributions of source data used for training in
comparison with target data used during realistic testing scenarios. In this
paper, we introduce a novel MultiScale Domain Adaptive YOLO (MS-DAYOLO)
framework that employs multiple domain adaptation paths and corresponding
domain classifiers at different scales of the recently introduced YOLOv4 object
detector to generate domain-invariant features. We train and test our proposed
method using popular datasets. Our experiments show significant improvements in
object detection performance when training YOLOv4 using the proposed MS-DAYOLO
and when tested on target data representing challenging weather conditions for
autonomous driving applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hnewa_M/0/1/0/all/0/1"&gt;Mazin Hnewa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Radha_H/0/1/0/all/0/1"&gt;Hayder Radha&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LLC: Accurate, Multi-purpose Learnt Low-dimensional Binary Codes. (arXiv:2106.01487v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01487</id>
        <link href="http://arxiv.org/abs/2106.01487"/>
        <updated>2021-06-04T01:12:31.133Z</updated>
        <summary type="html"><![CDATA[Learning binary representations of instances and classes is a classical
problem with several high potential applications. In modern settings, the
compression of high-dimensional neural representations to low-dimensional
binary codes is a challenging task and often require large bit-codes to be
accurate. In this work, we propose a novel method for Learning Low-dimensional
binary Codes (LLC) for instances as well as classes. Our method does not
require any side-information, like annotated attributes or label meta-data, and
learns extremely low-dimensional binary codes (~20 bits for ImageNet-1K). The
learnt codes are super-efficient while still ensuring nearly optimal
classification accuracy for ResNet50 on ImageNet-1K. We demonstrate that the
learnt codes capture intrinsically important features in the data, by
discovering an intuitive taxonomy over classes. We further quantitatively
measure the quality of our codes by applying it to the efficient image
retrieval as well as out-of-distribution (OOD) detection problems. For
ImageNet-100 retrieval problem, our learnt binary codes outperform 16 bit
HashNet using only 10 bits and also are as accurate as 10 dimensional real
representations. Finally, our learnt binary codes can perform OOD detection,
out-of-the-box, as accurately as a baseline that needs ~3000 samples to tune
its threshold, while we require none. Code and pre-trained models are available
at https://github.com/RAIVNLab/LLC.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kusupati_A/0/1/0/all/0/1"&gt;Aditya Kusupati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wallingford_M/0/1/0/all/0/1"&gt;Matthew Wallingford&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramanujan_V/0/1/0/all/0/1"&gt;Vivek Ramanujan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Somani_R/0/1/0/all/0/1"&gt;Raghav Somani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1"&gt;Jae Sung Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pillutla_K/0/1/0/all/0/1"&gt;Krishna Pillutla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jain_P/0/1/0/all/0/1"&gt;Prateek Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kakade_S/0/1/0/all/0/1"&gt;Sham Kakade&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Farhadi_A/0/1/0/all/0/1"&gt;Ali Farhadi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spline Positional Encoding for Learning 3D Implicit Signed Distance Fields. (arXiv:2106.01553v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.01553</id>
        <link href="http://arxiv.org/abs/2106.01553"/>
        <updated>2021-06-04T01:12:31.114Z</updated>
        <summary type="html"><![CDATA[Multilayer perceptrons (MLPs) have been successfully used to represent 3D
shapes implicitly and compactly, by mapping 3D coordinates to the corresponding
signed distance values or occupancy values. In this paper, we propose a novel
positional encoding scheme, called Spline Positional Encoding, to map the input
coordinates to a high dimensional space before passing them to MLPs, for
helping to recover 3D signed distance fields with fine-scale geometric details
from unorganized 3D point clouds. We verified the superiority of our approach
over other positional encoding schemes on tasks of 3D shape reconstruction from
input point clouds and shape space learning. The efficacy of our approach
extended to image reconstruction is also demonstrated and evaluated.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1"&gt;Peng-Shuai Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yu-Qi Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tong_X/0/1/0/all/0/1"&gt;Xin Tong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Joint Retrieval and Generation Training for Grounded Text Generation. (arXiv:2105.06597v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.06597</id>
        <link href="http://arxiv.org/abs/2105.06597"/>
        <updated>2021-06-04T01:12:31.051Z</updated>
        <summary type="html"><![CDATA[Recent advances in large-scale pre-training such as GPT-3 allow seemingly
high quality text to be generated from a given prompt. However, such generation
systems often suffer from problems of hallucinated facts, and are not
inherently designed to incorporate useful external information. Grounded
generation models appear to offer remedies, but their training typically relies
on rarely-available parallel data where corresponding information-relevant
documents are provided for context. We propose a framework that alleviates this
data constraint by jointly training a grounded generator and document retriever
on the language model signal. The model learns to reward retrieval of the
documents with the highest utility in generation, and attentively combines them
using a Mixture-of-Experts (MoE) ensemble to generate follow-on text. We
demonstrate that both generator and retriever can take advantage of this joint
training and work synergistically to produce more informative and relevant text
in both prose and dialogue generation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yizhe Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1"&gt;Siqi Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1"&gt;Xiang Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1"&gt;Yuwei Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brockett_C/0/1/0/all/0/1"&gt;Chris Brockett&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Galley_M/0/1/0/all/0/1"&gt;Michel Galley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1"&gt;Jianfeng Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dolan_B/0/1/0/all/0/1"&gt;Bill Dolan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Societal Biases in Language Generation: Progress and Challenges. (arXiv:2105.04054v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.04054</id>
        <link href="http://arxiv.org/abs/2105.04054"/>
        <updated>2021-06-04T01:12:30.986Z</updated>
        <summary type="html"><![CDATA[Technology for language generation has advanced rapidly, spurred by
advancements in pre-training large models on massive amounts of data and the
need for intelligent agents to communicate in a natural manner. While
techniques can effectively generate fluent text, they can also produce
undesirable societal biases that can have a disproportionately negative impact
on marginalized populations. Language generation presents unique challenges for
biases in terms of direct user interaction and the structure of decoding
techniques. To better understand these challenges, we present a survey on
societal biases in language generation, focusing on how data and techniques
contribute to biases and progress towards reducing biases. Motivated by a lack
of studies on biases from decoding techniques, we also conduct experiments to
quantify the effects of these techniques. By further discussing general trends
and open challenges, we call to attention promising directions for research and
the importance of fairness and inclusivity considerations for language
generation applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sheng_E/0/1/0/all/0/1"&gt;Emily Sheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1"&gt;Kai-Wei Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Natarajan_P/0/1/0/all/0/1"&gt;Premkumar Natarajan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1"&gt;Nanyun Peng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BERT Busters: Outlier Dimensions that Disrupt Transformers. (arXiv:2105.06990v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.06990</id>
        <link href="http://arxiv.org/abs/2105.06990"/>
        <updated>2021-06-04T01:12:30.980Z</updated>
        <summary type="html"><![CDATA[Multiple studies have shown that Transformers are remarkably robust to
pruning. Contrary to this received wisdom, we demonstrate that pre-trained
Transformer encoders are surprisingly fragile to the removal of a very small
number of features in the layer outputs (<0.0001% of model weights). In case of
BERT and other pre-trained encoder Transformers, the affected component is the
scaling factors and biases in the LayerNorm. The outliers are high-magnitude
normalization parameters that emerge early in pre-training and show up
consistently in the same dimensional position throughout the model. We show
that disabling them significantly degrades both the MLM loss and the downstream
task performance. This effect is observed across several BERT-family models and
other popular pre-trained Transformer architectures, including BART, XLNet and
ELECTRA; we also show a similar effect in GPT-2.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kovaleva_O/0/1/0/all/0/1"&gt;Olga Kovaleva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kulshreshtha_S/0/1/0/all/0/1"&gt;Saurabh Kulshreshtha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rogers_A/0/1/0/all/0/1"&gt;Anna Rogers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rumshisky_A/0/1/0/all/0/1"&gt;Anna Rumshisky&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DExperts: Decoding-Time Controlled Text Generation with Experts and Anti-Experts. (arXiv:2105.03023v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.03023</id>
        <link href="http://arxiv.org/abs/2105.03023"/>
        <updated>2021-06-04T01:12:30.974Z</updated>
        <summary type="html"><![CDATA[Despite recent advances in natural language generation, it remains
challenging to control attributes of generated text. We propose DExperts:
Decoding-time Experts, a decoding-time method for controlled text generation
that combines a pretrained language model with "expert" LMs and/or
"anti-expert" LMs in a product of experts. Intuitively, under the ensemble,
tokens only get high probability if they are considered likely by the experts,
and unlikely by the anti-experts. We apply DExperts to language detoxification
and sentiment-controlled generation, where we outperform existing controllable
generation methods on both automatic and human evaluations. Moreover, because
DExperts operates only on the output of the pretrained LM, it is effective with
(anti-)experts of smaller size, including when operating on GPT-3. Our work
highlights the promise of tuning small LMs on text with (un)desirable
attributes for efficient decoding-time steering.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1"&gt;Alisa Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sap_M/0/1/0/all/0/1"&gt;Maarten Sap&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1"&gt;Ximing Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Swayamdipta_S/0/1/0/all/0/1"&gt;Swabha Swayamdipta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhagavatula_C/0/1/0/all/0/1"&gt;Chandra Bhagavatula&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1"&gt;Noah A. Smith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1"&gt;Yejin Choi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[WeNet: Production oriented Streaming and Non-streaming End-to-End Speech Recognition Toolkit. (arXiv:2102.01547v2 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.01547</id>
        <link href="http://arxiv.org/abs/2102.01547"/>
        <updated>2021-06-04T01:12:30.958Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose an open source, production first, and production
ready speech recognition toolkit called WeNet in which a new two-pass approach
is implemented to unify streaming and non-streaming end-to-end (E2E) speech
recognition in a single model. The main motivation of WeNet is to close the gap
between the research and the production of E2E speechrecognition models. WeNet
provides an efficient way to ship ASR applications in several real-world
scenarios, which is the main difference and advantage to other open source E2E
speech recognition toolkits. In our toolkit, a new two-pass method is
implemented. Our method propose a dynamic chunk-based attention strategy of the
the transformer layers to allow arbitrary right context length modifies in
hybrid CTC/attention architecture. The inference latency could be easily
controlled by only changing the chunk size. The CTC hypotheses are then
rescored by the attention decoder to get the final result. Our experiments on
the AISHELL-1 dataset using WeNet show that, our model achieves 5.03\% relative
character error rate (CER) reduction in non-streaming ASR compared to a
standard non-streaming transformer. After model quantification, our model
perform reasonable RTF and latency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1"&gt;Zhuoyuan Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1"&gt;Di Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1"&gt;Binbin Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1"&gt;Fan Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1"&gt;Chao Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_Z/0/1/0/all/0/1"&gt;Zhendong Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xiaoyu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1"&gt;Lei Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lei_X/0/1/0/all/0/1"&gt;Xin Lei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Crossing the Conversational Chasm: A Primer on Natural Language Processing for Multilingual Task-Oriented Dialogue Systems. (arXiv:2104.08570v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.08570</id>
        <link href="http://arxiv.org/abs/2104.08570"/>
        <updated>2021-06-04T01:12:30.952Z</updated>
        <summary type="html"><![CDATA[In task-oriented dialogue (ToD), a user holds a conversation with an
artificial agent to complete a concrete task. Although this technology
represents one of the central objectives of AI and has been the focus of ever
more intense research and development efforts, it is currently limited to a few
narrow domains (e.g., food ordering, ticket booking) and a handful of languages
(e.g., English, Chinese). This work provides an extensive overview of existing
methods and resources in multilingual ToD as an entry point to this exciting
and emerging field. We find that the most critical factor preventing the
creation of truly multilingual ToD systems is the lack of datasets in most
languages for both training and evaluation. In fact, acquiring annotations or
human feedback for each component of modular systems or for data-hungry
end-to-end systems is expensive and tedious. Hence, state-of-the-art approaches
to multilingual ToD mostly rely on (zero- or few-shot) cross-lingual transfer
from resource-rich languages (almost exclusively English), either by means of
machine translation or multilingual representations. These approaches are
currently viable only for typologically similar languages and languages with
parallel / monolingual corpora available. On the other hand, their
effectiveness beyond these boundaries is doubtful or hard to assess due to the
lack of linguistically diverse benchmarks (especially for natural language
generation and end-to-end evaluation). To overcome this limitation, we draw
parallels between components of the ToD pipeline and other NLP tasks, which can
inspire solutions for learning in low-resource scenarios. Finally, we list
additional challenges that multilinguality poses for related areas (such as
speech and human-centred evaluation), and indicate future directions that hold
promise to further expand language coverage and dialogue capabilities of
current ToD systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Razumovskaia_E/0/1/0/all/0/1"&gt;Evgeniia Razumovskaia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Glavas_G/0/1/0/all/0/1"&gt;Goran Glava&amp;#x161;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Majewska_O/0/1/0/all/0/1"&gt;Olga Majewska&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ponti_E/0/1/0/all/0/1"&gt;Edoardo M. Ponti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Korhonen_A/0/1/0/all/0/1"&gt;Anna Korhonen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vulic_I/0/1/0/all/0/1"&gt;Ivan Vuli&amp;#x107;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Benchmarking Commercial Intent Detection Services with Practice-Driven Evaluations. (arXiv:2012.03929v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.03929</id>
        <link href="http://arxiv.org/abs/2012.03929"/>
        <updated>2021-06-04T01:12:30.944Z</updated>
        <summary type="html"><![CDATA[Intent detection is a key component of modern goal-oriented dialog systems
that accomplish a user task by predicting the intent of users' text input.
There are three primary challenges in designing robust and accurate intent
detection models. First, typical intent detection models require a large amount
of labeled data to achieve high accuracy. Unfortunately, in practical scenarios
it is more common to find small, unbalanced, and noisy datasets. Secondly, even
with large training data, the intent detection models can see a different
distribution of test data when being deployed in the real world, leading to
poor accuracy. Finally, a practical intent detection model must be
computationally efficient in both training and single query inference so that
it can be used continuously and re-trained frequently. We benchmark intent
detection methods on a variety of datasets. Our results show that Watson
Assistant's intent detection model outperforms other commercial solutions and
is comparable to large pretrained language models while requiring only a
fraction of computational resources and training data. Watson Assistant
demonstrates a higher degree of robustness when the training and test
distributions differ.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qi_H/0/1/0/all/0/1"&gt;Haode Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1"&gt;Lin Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sood_A/0/1/0/all/0/1"&gt;Atin Sood&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shah_A/0/1/0/all/0/1"&gt;Abhishek Shah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kunc_L/0/1/0/all/0/1"&gt;Ladislav Kunc&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_M/0/1/0/all/0/1"&gt;Mo Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Potdar_S/0/1/0/all/0/1"&gt;Saloni Potdar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Learning of KB Queries in Task-Oriented Dialogs. (arXiv:2005.00123v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.00123</id>
        <link href="http://arxiv.org/abs/2005.00123"/>
        <updated>2021-06-04T01:12:30.936Z</updated>
        <summary type="html"><![CDATA[Task-oriented dialog (TOD) systems often need to formulate knowledge base
(KB) queries corresponding to the user intent and use the query results to
generate system responses. Existing approaches require dialog datasets to
explicitly annotate these KB queries -- these annotations can be time
consuming, and expensive. In response, we define the novel problems of
predicting the KB query and training the dialog agent, without explicit KB
query annotation. For query prediction, we propose a reinforcement learning
(RL) baseline, which rewards the generation of those queries whose KB results
cover the entities mentioned in subsequent dialog. Further analysis reveals
that correlation among query attributes in KB can significantly confuse memory
augmented policy optimization (MAPO), an existing state of the art RL agent. To
address this, we improve the MAPO baseline with simple but important
modifications suited to our task. To train the full TOD system for our setting,
we propose a pipelined approach: it independently predicts when to make a KB
query (query position predictor), then predicts a KB query at the predicted
position (query predictor), and uses the results of predicted query in
subsequent dialog (next response predictor). Overall, our work proposes first
solutions to our novel problem, and our analysis highlights the research
challenges in training TOD systems without query annotation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Raghu_D/0/1/0/all/0/1"&gt;Dinesh Raghu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_N/0/1/0/all/0/1"&gt;Nikhil Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mausam/0/1/0/all/0/1"&gt;Mausam&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LyricJam: A system for generating lyrics for live instrumental music. (arXiv:2106.01960v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2106.01960</id>
        <link href="http://arxiv.org/abs/2106.01960"/>
        <updated>2021-06-04T01:12:30.910Z</updated>
        <summary type="html"><![CDATA[We describe a real-time system that receives a live audio stream from a jam
session and generates lyric lines that are congruent with the live music being
played. Two novel approaches are proposed to align the learned latent spaces of
audio and text representations that allow the system to generate novel lyric
lines matching live instrumental music. One approach is based on adversarial
alignment of latent representations of audio and lyrics, while the other
approach learns to transfer the topology from the music latent space to the
lyric latent space. A user study with music artists using the system showed
that the system was useful not only in lyric composition, but also encouraged
the artists to improvise and find new musical expressions. Another user study
demonstrated that users preferred the lines generated using the proposed
methods to the lines generated by a baseline model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vechtomova_O/0/1/0/all/0/1"&gt;Olga Vechtomova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sahu_G/0/1/0/all/0/1"&gt;Gaurav Sahu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_D/0/1/0/all/0/1"&gt;Dhruv Kumar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Shortformer: Better Language Modeling using Shorter Inputs. (arXiv:2012.15832v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.15832</id>
        <link href="http://arxiv.org/abs/2012.15832"/>
        <updated>2021-06-04T01:12:30.899Z</updated>
        <summary type="html"><![CDATA[Increasing the input length has been a driver of progress in language
modeling with transformers. We identify conditions where shorter inputs are not
harmful, and achieve perplexity and efficiency improvements through two new
methods that decrease input length. First, we show that initially training a
model on short subsequences before moving on to longer ones both reduces
overall training time and, surprisingly, substantially improves perplexity.
Second, we show how to improve the efficiency of recurrence methods in
transformers, which let models condition on previously processed tokens when
generating sequences that exceed the maximal length the transformer can handle
at once. Existing methods require computationally expensive relative position
embeddings; we introduce a simple alternative of adding absolute position
embeddings to queries and keys instead of to word embeddings, which efficiently
produces superior results. We show that these recurrent models also benefit
from short input lengths. Combining these techniques speeds up training by a
factor of 1.65, reduces memory usage, and substantially improves perplexity on
WikiText-103, without adding any parameters.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Press_O/0/1/0/all/0/1"&gt;Ofir Press&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1"&gt;Noah A. Smith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lewis_M/0/1/0/all/0/1"&gt;Mike Lewis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SIRE: Separate Intra- and Inter-sentential Reasoning for Document-level Relation Extraction. (arXiv:2106.01709v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01709</id>
        <link href="http://arxiv.org/abs/2106.01709"/>
        <updated>2021-06-04T01:12:30.890Z</updated>
        <summary type="html"><![CDATA[Document-level relation extraction has attracted much attention in recent
years. It is usually formulated as a classification problem that predicts
relations for all entity pairs in the document. However, previous works
indiscriminately represent intra- and inter-sentential relations in the same
way, confounding the different patterns for predicting them. Besides, they
create a document graph and use paths between entities on the graph as clues
for logical reasoning. However, not all entity pairs can be connected with a
path and have the correct logical reasoning paths in their graph. Thus many
cases of logical reasoning cannot be covered. This paper proposes an effective
architecture, SIRE, to represent intra- and inter-sentential relations in
different ways. We design a new and straightforward form of logical reasoning
module that can cover more logical reasoning chains. Experiments on the public
datasets show SIRE outperforms the previous state-of-the-art methods. Further
analysis shows that our predictions are reliable and explainable. Our code is
available at https://github.com/DreamInvoker/SIRE.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_S/0/1/0/all/0/1"&gt;Shuang Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yuting Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_B/0/1/0/all/0/1"&gt;Baobao Chang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DialogueCRN: Contextual Reasoning Networks for Emotion Recognition in Conversations. (arXiv:2106.01978v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01978</id>
        <link href="http://arxiv.org/abs/2106.01978"/>
        <updated>2021-06-04T01:12:30.867Z</updated>
        <summary type="html"><![CDATA[Emotion Recognition in Conversations (ERC) has gained increasing attention
for developing empathetic machines. Recently, many approaches have been devoted
to perceiving conversational context by deep learning models. However, these
approaches are insufficient in understanding the context due to lacking the
ability to extract and integrate emotional clues. In this work, we propose
novel Contextual Reasoning Networks (DialogueCRN) to fully understand the
conversational context from a cognitive perspective. Inspired by the Cognitive
Theory of Emotion, we design multi-turn reasoning modules to extract and
integrate emotional clues. The reasoning module iteratively performs an
intuitive retrieving process and a conscious reasoning process, which imitates
human unique cognitive thinking. Extensive experiments on three public
benchmark datasets demonstrate the effectiveness and superiority of the
proposed model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_D/0/1/0/all/0/1"&gt;Dou Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_L/0/1/0/all/0/1"&gt;Lingwei Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huai_X/0/1/0/all/0/1"&gt;Xiaoyong Huai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reordering Examples Helps during Priming-based Few-Shot Learning. (arXiv:2106.01751v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01751</id>
        <link href="http://arxiv.org/abs/2106.01751"/>
        <updated>2021-06-04T01:12:30.851Z</updated>
        <summary type="html"><![CDATA[The ability to learn from limited data, or few-shot learning, is a desirable
and often critical requirement for NLP systems. While many existing methods do
poorly at learning from a handful of examples, large pretrained language models
have recently been shown to be efficient few-shot learners. One approach to
few-shot learning, which does not require finetuning of model parameters, is to
augment the language model's input with priming text which is typically
constructed using task specific descriptions and examples. In this work, we
further explore priming-based few-shot learning, with focus on using examples
as prompts. We show that presenting examples in the right order is key for
generalization. We introduce PERO (Prompting with Examples in the Right Order),
where we formulate few-shot learning as search over the set of permutations of
the training examples. We show that PERO can learn to generalize efficiently
using as few as 10 examples, in contrast to existing approaches. While the
newline token is a natural choice for separating the examples in the prompt, we
show that learning a new separator token can potentially provide further gains
in performance. We demonstrate the effectiveness of the proposed method on the
tasks of sentiment classification, natural language inference and fact
retrieval. Finally, we analyze the learned prompts to reveal novel insights,
including the idea that two training examples in the right order alone can
provide competitive performance for sentiment classification and natural
language inference.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1"&gt;Sawan Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Talukdar_P/0/1/0/all/0/1"&gt;Partha Talukdar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Does BERT Solve Commonsense Task via Commonsense Knowledge?. (arXiv:2008.03945v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.03945</id>
        <link href="http://arxiv.org/abs/2008.03945"/>
        <updated>2021-06-04T01:12:30.838Z</updated>
        <summary type="html"><![CDATA[BERT has been used for solving commonsense tasks such as CommonsenseQA. While
prior research has found that BERT does contain commonsense information to some
extent, there has been work showing that pre-trained models can rely on
spurious associations (e.g., data bias) rather than key cues in solving
sentiment classification and other problems. We quantitatively investigate the
presence of structural commonsense cues in BERT when solving commonsense tasks,
and the importance of such cues for the model prediction. Using two different
measures, we find that BERT does use relevant knowledge for solving the task,
and the presence of commonsense knowledge is positively correlated to the model
accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1"&gt;Leyang Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1"&gt;Sijie Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yu Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yue Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GL-GIN: Fast and Accurate Non-Autoregressive Model for Joint Multiple Intent Detection and Slot Filling. (arXiv:2106.01925v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01925</id>
        <link href="http://arxiv.org/abs/2106.01925"/>
        <updated>2021-06-04T01:12:30.823Z</updated>
        <summary type="html"><![CDATA[Multi-intent SLU can handle multiple intents in an utterance, which has
attracted increasing attention. However, the state-of-the-art joint models
heavily rely on autoregressive approaches, resulting in two issues: slow
inference speed and information leakage. In this paper, we explore a
non-autoregressive model for joint multiple intent detection and slot filling,
achieving more fast and accurate. Specifically, we propose a Global-Locally
Graph Interaction Network (GL-GIN) where a local slot-aware graph interaction
layer is proposed to model slot dependency for alleviating uncoordinated slots
problem while a global intent-slot graph interaction layer is introduced to
model the interaction between multiple intents and all slots in the utterance.
Experimental results on two public datasets show that our framework achieves
state-of-the-art performance while being 11.5 times faster.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qin_L/0/1/0/all/0/1"&gt;Libo Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1"&gt;Fuxuan Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_T/0/1/0/all/0/1"&gt;Tianbao Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1"&gt;Xiao Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Che_W/0/1/0/all/0/1"&gt;Wanxiang Che&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Ting Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning from the Worst: Dynamically Generated Datasets to Improve Online Hate Detection. (arXiv:2012.15761v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.15761</id>
        <link href="http://arxiv.org/abs/2012.15761"/>
        <updated>2021-06-04T01:12:30.817Z</updated>
        <summary type="html"><![CDATA[We present a human-and-model-in-the-loop process for dynamically generating
datasets and training better performing and more robust hate detection models.
We provide a new dataset of ~40,000 entries, generated and labelled by trained
annotators over four rounds of dynamic data creation. It includes ~15,000
challenging perturbations and each hateful entry has fine-grained labels for
the type and target of hate. Hateful entries make up 54% of the dataset, which
is substantially higher than comparable datasets. We show that model
performance is substantially improved using this approach. Models trained on
later rounds of data collection perform better on test sets and are harder for
annotators to trick. They also perform better on HateCheck, a suite of
functional tests for online hate detection. We provide the code, dataset and
annotation guidelines for other researchers to use. Accepted at ACL 2021.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vidgen_B/0/1/0/all/0/1"&gt;Bertie Vidgen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thrush_T/0/1/0/all/0/1"&gt;Tristan Thrush&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Waseem_Z/0/1/0/all/0/1"&gt;Zeerak Waseem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kiela_D/0/1/0/all/0/1"&gt;Douwe Kiela&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Case Study of Spanish Text Transformations for Twitter Sentiment Analysis. (arXiv:2106.02009v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02009</id>
        <link href="http://arxiv.org/abs/2106.02009"/>
        <updated>2021-06-04T01:12:30.810Z</updated>
        <summary type="html"><![CDATA[Sentiment analysis is a text mining task that determines the polarity of a
given text, i.e., its positiveness or negativeness. Recently, it has received a
lot of attention given the interest in opinion mining in micro-blogging
platforms. These new forms of textual expressions present new challenges to
analyze text given the use of slang, orthographic and grammatical errors, among
others. Along with these challenges, a practical sentiment classifier should be
able to handle efficiently large workloads.

The aim of this research is to identify which text transformations
(lemmatization, stemming, entity removal, among others), tokenizers (e.g.,
words $n$-grams), and tokens weighting schemes impact the most the accuracy of
a classifier (Support Vector Machine) trained on two Spanish corpus. The
methodology used is to exhaustively analyze all the combinations of the text
transformations and their respective parameters to find out which
characteristics the best performing classifiers have in common. Furthermore,
among the different text transformations studied, we introduce a novel approach
based on the combination of word based $n$-grams and character based $q$-grams.
The results show that this novel combination of words and characters produces a
classifier that outperforms the traditional word based combination by $11.17\%$
and $5.62\%$ on the INEGI and TASS'15 dataset, respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tellez_E/0/1/0/all/0/1"&gt;Eric S. Tellez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Miranda_Jimenez_S/0/1/0/all/0/1"&gt;Sabino Miranda-Jim&amp;#xe9;nez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Graff_M/0/1/0/all/0/1"&gt;Mario Graff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moctezuma_D/0/1/0/all/0/1"&gt;Daniela Moctezuma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Siodia_O/0/1/0/all/0/1"&gt;Oscar S. Siodia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Villasenor_E/0/1/0/all/0/1"&gt;Elio A. Villase&amp;#xf1;or&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interactive Refinement of Cross-Lingual Word Embeddings. (arXiv:1911.03070v4 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1911.03070</id>
        <link href="http://arxiv.org/abs/1911.03070"/>
        <updated>2021-06-04T01:12:30.792Z</updated>
        <summary type="html"><![CDATA[Cross-lingual word embeddings transfer knowledge between languages: models
trained on high-resource languages can predict in low-resource languages. We
introduce CLIME, an interactive system to quickly refine cross-lingual word
embeddings for a given classification problem. First, CLIME ranks words by
their salience to the downstream task. Then, users mark similarity between
keywords and their nearest neighbors in the embedding space. Finally, CLIME
updates the embeddings using the annotations. We evaluate CLIME on identifying
health-related text in four low-resource languages: Ilocano, Sinhalese,
Tigrinya, and Uyghur. Embeddings refined by CLIME capture more nuanced word
semantics and have higher test accuracy than the original embeddings. CLIME
often improves accuracy faster than an active learning baseline and can be
easily combined with active learning to improve results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_M/0/1/0/all/0/1"&gt;Michelle Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Mozhi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Durme_B/0/1/0/all/0/1"&gt;Benjamin Van Durme&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Findlater_L/0/1/0/all/0/1"&gt;Leah Findlater&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boyd_Graber_J/0/1/0/all/0/1"&gt;Jordan Boyd-Graber&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Three Sentences Are All You Need: Local Path Enhanced Document Relation Extraction. (arXiv:2106.01793v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01793</id>
        <link href="http://arxiv.org/abs/2106.01793"/>
        <updated>2021-06-04T01:12:30.785Z</updated>
        <summary type="html"><![CDATA[Document-level Relation Extraction (RE) is a more challenging task than
sentence RE as it often requires reasoning over multiple sentences. Yet, human
annotators usually use a small number of sentences to identify the relationship
between a given entity pair. In this paper, we present an embarrassingly simple
but effective method to heuristically select evidence sentences for
document-level RE, which can be easily combined with BiLSTM to achieve good
performance on benchmark datasets, even better than fancy graph neural network
based methods. We have released our code at
https://github.com/AndrewZhe/Three-Sentences-Are-All-You-Need.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1"&gt;Quzhe Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1"&gt;Shengqi Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1"&gt;Yansong Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1"&gt;Yuan Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lai_Y/0/1/0/all/0/1"&gt;Yuxuan Lai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1"&gt;Dongyan Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semantic-WER: A Unified Metric for the Evaluation of ASR Transcript for End Usability. (arXiv:2106.02016v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02016</id>
        <link href="http://arxiv.org/abs/2106.02016"/>
        <updated>2021-06-04T01:12:30.779Z</updated>
        <summary type="html"><![CDATA[Recent advances in supervised, semi-supervised and self-supervised deep
learning algorithms have shown significant improvement in the performance of
automatic speech recognition(ASR) systems. The state-of-the-art systems have
achieved a word error rate (WER) less than 5%. However, in the past,
researchers have argued the non-suitability of the WER metric for the
evaluation of ASR systems for downstream tasks such as spoken language
understanding (SLU) and information retrieval. The reason is that the WER works
at the surface level and does not include any syntactic and semantic
knowledge.The current work proposes Semantic-WER (SWER), a metric to evaluate
the ASR transcripts for downstream applications in general. The SWER can be
easily customized for any down-stream task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1"&gt;Somnath Roy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CCPM: A Chinese Classical Poetry Matching Dataset. (arXiv:2106.01979v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01979</id>
        <link href="http://arxiv.org/abs/2106.01979"/>
        <updated>2021-06-04T01:12:30.773Z</updated>
        <summary type="html"><![CDATA[Poetry is one of the most important art forms of human languages. Recently
many studies have focused on incorporating some linguistic features of poetry,
such as style and sentiment, into its understanding or generation system.
However, there is no focus on understanding or evaluating the semantics of
poetry. Therefore, we propose a novel task to assess a model's semantic
understanding of poetry by poem matching. Specifically, this task requires the
model to select one line of Chinese classical poetry among four candidates
according to the modern Chinese translation of a line of poetry. To construct
this dataset, we first obtain a set of parallel data of Chinese classical
poetry and modern Chinese translation. Then we retrieve similar lines of poetry
with the lines in a poetry corpus as negative choices. We name the dataset
Chinese Classical Poetry Matching Dataset (CCPM) and release it at
https://github.com/THUNLP-AIPoet/CCPM. We hope this dataset can further enhance
the study on incorporating deep semantics into the understanding and generation
system of Chinese classical poetry. We also preliminarily run two variants of
BERT on this dataset as the baselines for this dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1"&gt;Wenhao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_F/0/1/0/all/0/1"&gt;Fanchao Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1"&gt;Maosong Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yi_X/0/1/0/all/0/1"&gt;Xiaoyuan Yi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jiarui Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bilingual Alignment Pre-training for Zero-shot Cross-lingual Transfer. (arXiv:2106.01732v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01732</id>
        <link href="http://arxiv.org/abs/2106.01732"/>
        <updated>2021-06-04T01:12:30.767Z</updated>
        <summary type="html"><![CDATA[Multilingual pre-trained models have achieved remarkable transfer performance
by pre-trained on rich kinds of languages. Most of the models such as mBERT are
pre-trained on unlabeled corpora. The static and contextual embeddings from the
models could not be aligned very well. In this paper, we aim to improve the
zero-shot cross-lingual transfer performance by aligning the embeddings better.
We propose a pre-training task named Alignment Language Model (AlignLM), which
uses the statistical alignment information as the prior knowledge to guide
bilingual word prediction. We evaluate our method on multilingual machine
reading comprehension and natural language interface tasks. The results show
AlignLM can improve the zero-shot performance significantly on MLQA and XNLI
datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Ziqing Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1"&gt;Wentao Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1"&gt;Yiming Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1"&gt;Jiani Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Che_W/0/1/0/all/0/1"&gt;Wanxiang Che&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shijin Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[E2E-VLP: End-to-End Vision-Language Pre-training Enhanced by Visual Learning. (arXiv:2106.01804v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.01804</id>
        <link href="http://arxiv.org/abs/2106.01804"/>
        <updated>2021-06-04T01:12:30.747Z</updated>
        <summary type="html"><![CDATA[Vision-language pre-training (VLP) on large-scale image-text pairs has
achieved huge success for the cross-modal downstream tasks. The most existing
pre-training methods mainly adopt a two-step training procedure, which firstly
employs a pre-trained object detector to extract region-based visual features,
then concatenates the image representation and text embedding as the input of
Transformer to train. However, these methods face problems of using
task-specific visual representation of the specific object detector for generic
cross-modal understanding, and the computation inefficiency of two-stage
pipeline. In this paper, we propose the first end-to-end vision-language
pre-trained model for both V+L understanding and generation, namely E2E-VLP,
where we build a unified Transformer framework to jointly learn visual
representation, and semantic alignments between image and text. We incorporate
the tasks of object detection and image captioning into pre-training with a
unified Transformer encoder-decoder architecture for enhancing visual learning.
An extensive set of experiments have been conducted on well-established
vision-language downstream tasks to demonstrate the effectiveness of this novel
VLP paradigm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1"&gt;Haiyang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_M/0/1/0/all/0/1"&gt;Ming Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chenliang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bi_B/0/1/0/all/0/1"&gt;Bin Bi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1"&gt;Songfang Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_W/0/1/0/all/0/1"&gt;Wenming Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1"&gt;Fei Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[EmoDNN: Understanding emotions from short texts through a deep neural network ensemble. (arXiv:2106.01706v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01706</id>
        <link href="http://arxiv.org/abs/2106.01706"/>
        <updated>2021-06-04T01:12:30.741Z</updated>
        <summary type="html"><![CDATA[The latent knowledge in the emotions and the opinions of the individuals that
are manifested via social networks are crucial to numerous applications
including social management, dynamical processes, and public security.
Affective computing, as an interdisciplinary research field, linking artificial
intelligence to cognitive inference, is capable to exploit emotion-oriented
knowledge from brief contents. The textual contents convey hidden information
such as personality and cognition about corresponding authors that can
determine both correlations and variations between users. Emotion recognition
from brief contents should embrace the contrast between authors where the
differences in personality and cognition can be traced within emotional
expressions. To tackle this challenge, we devise a framework that, on the one
hand, infers latent individual aspects, from brief contents and, on the other
hand, presents a novel ensemble classifier equipped with dynamic dropout
convnets to extract emotions from textual context. To categorize short text
contents, our proposed method conjointly leverages cognitive factors and
exploits hidden information. We utilize the outcome vectors in a novel
embedding model to foster emotion-pertinent features that are collectively
assembled by lexicon inductions. Experimental results show that compared to
other competitors, our proposed model can achieve a higher performance in
recognizing emotion from noisy contents.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kamran_S/0/1/0/all/0/1"&gt;Sara Kamran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zall_R/0/1/0/all/0/1"&gt;Raziyeh Zall&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kangavari_M/0/1/0/all/0/1"&gt;Mohammad Reza Kangavari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hosseini_S/0/1/0/all/0/1"&gt;Saeid Hosseini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rahmani_S/0/1/0/all/0/1"&gt;Sana Rahmani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hua_W/0/1/0/all/0/1"&gt;Wen Hua&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Representing Syntax and Composition with Geometric Transformations. (arXiv:2106.01904v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01904</id>
        <link href="http://arxiv.org/abs/2106.01904"/>
        <updated>2021-06-04T01:12:30.735Z</updated>
        <summary type="html"><![CDATA[The exploitation of syntactic graphs (SyGs) as a word's context has been
shown to be beneficial for distributional semantic models (DSMs), both at the
level of individual word representations and in deriving phrasal
representations via composition. However, notwithstanding the potential
performance benefit, the syntactically-aware DSMs proposed to date have huge
numbers of parameters (compared to conventional DSMs) and suffer from data
sparsity. Furthermore, the encoding of the SyG links (i.e., the syntactic
relations) has been largely limited to linear maps. The knowledge graphs'
literature, on the other hand, has proposed light-weight models employing
different geometric transformations (GTs) to encode edges in a knowledge graph
(KG). Our work explores the possibility of adopting this family of models to
encode SyGs. Furthermore, we investigate which GT better encodes syntactic
relations, so that these representations can be used to enhance phrase-level
composition via syntactic contextualisation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bertolini_L/0/1/0/all/0/1"&gt;Lorenzo Bertolini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weeds_J/0/1/0/all/0/1"&gt;Julie Weeds&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weir_D/0/1/0/all/0/1"&gt;David Weir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_Q/0/1/0/all/0/1"&gt;Qiwei Peng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Detecting Hallucinated Content in Conditional Neural Sequence Generation. (arXiv:2011.02593v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.02593</id>
        <link href="http://arxiv.org/abs/2011.02593"/>
        <updated>2021-06-04T01:12:30.729Z</updated>
        <summary type="html"><![CDATA[Neural sequence models can generate highly fluent sentences, but recent
studies have also shown that they are also prone to hallucinate additional
content not supported by the input. These variety of fluent but wrong outputs
are particularly problematic, as it will not be possible for users to tell they
are being presented incorrect content. To detect these errors, we propose a
task to predict whether each token in the output sequence is hallucinated (not
contained in the input) and collect new manually annotated evaluation sets for
this task. We also introduce a method for learning to detect hallucinations
using pretrained language models fine tuned on synthetic data that includes
automatically inserted hallucinations Experiments on machine translation (MT)
and abstractive summarization demonstrate that our proposed approach
consistently outperforms strong baselines on all benchmark datasets. We further
demonstrate how to use the token-level hallucination labels to define a
fine-grained loss over the target sequence in low-resource MT and achieve
significant improvements over strong baseline methods. We also apply our method
to word-level quality estimation for MT and show its effectiveness in both
supervised and unsupervised settings. Codes and data available at
https://github.com/violet-zct/fairseq-detect-hallucination.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1"&gt;Chunting Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1"&gt;Graham Neubig&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1"&gt;Jiatao Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Diab_M/0/1/0/all/0/1"&gt;Mona Diab&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guzman_P/0/1/0/all/0/1"&gt;Paco Guzman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1"&gt;Luke Zettlemoyer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghazvininejad_M/0/1/0/all/0/1"&gt;Marjan Ghazvininejad&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Case for Translation-Invariant Self-Attention in Transformer-Based Language Models. (arXiv:2106.01950v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01950</id>
        <link href="http://arxiv.org/abs/2106.01950"/>
        <updated>2021-06-04T01:12:30.722Z</updated>
        <summary type="html"><![CDATA[Mechanisms for encoding positional information are central for
transformer-based language models. In this paper, we analyze the position
embeddings of existing language models, finding strong evidence of translation
invariance, both for the embeddings themselves and for their effect on
self-attention. The degree of translation invariance increases during training
and correlates positively with model performance. Our findings lead us to
propose translation-invariant self-attention (TISA), which accounts for the
relative position between tokens in an interpretable fashion without needing
conventional position embeddings. Our proposal has several theoretical
advantages over existing position-representation approaches. Experiments show
that it improves on regular ALBERT on GLUE tasks, while only adding orders of
magnitude less positional parameters.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wennberg_U/0/1/0/all/0/1"&gt;Ulme Wennberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Henter_G/0/1/0/all/0/1"&gt;Gustav Eje Henter&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Provably Secure Generative Linguistic Steganography. (arXiv:2106.02011v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02011</id>
        <link href="http://arxiv.org/abs/2106.02011"/>
        <updated>2021-06-04T01:12:30.702Z</updated>
        <summary type="html"><![CDATA[Generative linguistic steganography mainly utilized language models and
applied steganographic sampling (stegosampling) to generate high-security
steganographic text (stegotext). However, previous methods generally lead to
statistical differences between the conditional probability distributions of
stegotext and natural text, which brings about security risks. In this paper,
to further ensure security, we present a novel provably secure generative
linguistic steganographic method ADG, which recursively embeds secret
information by Adaptive Dynamic Grouping of tokens according to their
probability given by an off-the-shelf language model. We not only prove the
security of ADG mathematically, but also conduct extensive experiments on three
public corpora to further verify its imperceptibility. The experimental results
reveal that the proposed method is able to generate stegotext with nearly
perfect security.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Siyu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Zhongliang Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jinshuai Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yongfeng Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SOCCER: An Information-Sparse Discourse State Tracking Collection in the Sports Commentary Domain. (arXiv:2106.01972v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01972</id>
        <link href="http://arxiv.org/abs/2106.01972"/>
        <updated>2021-06-04T01:12:30.696Z</updated>
        <summary type="html"><![CDATA[In the pursuit of natural language understanding, there has been a long
standing interest in tracking state changes throughout narratives. Impressive
progress has been made in modeling the state of transaction-centric dialogues
and procedural texts. However, this problem has been less intensively studied
in the realm of general discourse where ground truth descriptions of states may
be loosely defined and state changes are less densely distributed over
utterances. This paper proposes to turn to simplified, fully observable systems
that show some of these properties: Sports events. We curated 2,263 soccer
matches including time-stamped natural language commentary accompanied by
discrete events such as a team scoring goals, switching players or being
penalized with cards. We propose a new task formulation where, given paragraphs
of commentary of a game at different timestamps, the system is asked to
recognize the occurrence of in-game events. This domain allows for rich
descriptions of state while avoiding the complexities of many other real-world
settings. As an initial point of performance measurement, we include two
baseline methods from the perspectives of sentence classification with temporal
dependence and current state-of-the-art generative model, respectively, and
demonstrate that even sophisticated existing methods struggle on the state
tracking task when the definition of state broadens or non-event chatter
becomes prevalent.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Ruochen Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eickhoff_C/0/1/0/all/0/1"&gt;Carsten Eickhoff&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring Distantly-Labeled Rationales in Neural Network Models. (arXiv:2106.01809v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01809</id>
        <link href="http://arxiv.org/abs/2106.01809"/>
        <updated>2021-06-04T01:12:30.690Z</updated>
        <summary type="html"><![CDATA[Recent studies strive to incorporate various human rationales into neural
networks to improve model performance, but few pay attention to the quality of
the rationales. Most existing methods distribute their models' focus to
distantly-labeled rationale words entirely and equally, while ignoring the
potential important non-rationale words and not distinguishing the importance
of different rationale words. In this paper, we propose two novel auxiliary
loss functions to make better use of distantly-labeled rationales, which
encourage models to maintain their focus on important words beyond labeled
rationales (PINs) and alleviate redundant training on non-helpful rationales
(NoIRs). Experiments on two representative classification tasks show that our
proposed methods can push a classification model to effectively learn crucial
clues from non-perfect rationales while maintaining the ability to spread its
focus to other unlabeled important words, thus significantly outperform
existing methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1"&gt;Quzhe Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1"&gt;Shengqi Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1"&gt;Yansong Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1"&gt;Dongyan Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fingerprinting Fine-tuned Language Models in the Wild. (arXiv:2106.01703v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01703</id>
        <link href="http://arxiv.org/abs/2106.01703"/>
        <updated>2021-06-04T01:12:30.684Z</updated>
        <summary type="html"><![CDATA[There are concerns that the ability of language models (LMs) to generate high
quality synthetic text can be misused to launch spam, disinformation, or
propaganda. Therefore, the research community is actively working on developing
approaches to detect whether a given text is organic or synthetic. While this
is a useful first step, it is important to be able to further fingerprint the
author LM to attribute its origin. Prior work on fingerprinting LMs is limited
to attributing synthetic text generated by a handful (usually < 10) of
pre-trained LMs. However, LMs such as GPT2 are commonly fine-tuned in a myriad
of ways (e.g., on a domain-specific text corpus) before being used to generate
synthetic text. It is challenging to fingerprinting fine-tuned LMs because the
universe of fine-tuned LMs is much larger in realistic scenarios. To address
this challenge, we study the problem of large-scale fingerprinting of
fine-tuned LMs in the wild. Using a real-world dataset of synthetic text
generated by 108 different fine-tuned LMs, we conduct comprehensive experiments
to demonstrate the limitations of existing fingerprinting approaches. Our
results show that fine-tuning itself is the most effective in attributing the
synthetic text generated by fine-tuned LMs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Diwan_N/0/1/0/all/0/1"&gt;Nirav Diwan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chakravorty_T/0/1/0/all/0/1"&gt;Tanmoy Chakravorty&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shafiq_Z/0/1/0/all/0/1"&gt;Zubair Shafiq&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reduce and Reconstruct: ASR for Low-Resource Phonetic Languages. (arXiv:2010.09322v2 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.09322</id>
        <link href="http://arxiv.org/abs/2010.09322"/>
        <updated>2021-06-04T01:12:30.676Z</updated>
        <summary type="html"><![CDATA[This work presents a seemingly simple but effective technique to improve
low-resource ASR systems for phonetic languages. By identifying sets of
acoustically similar graphemes in these languages, we first reduce the output
alphabet of the ASR system using linguistically meaningful reductions and then
reconstruct the original alphabet using a standalone module. We demonstrate
that this lessens the burden and improves the performance of low-resource
end-to-end ASR systems (because only reduced-alphabet predictions are needed)
and that it is possible to design a very simple but effective reconstruction
module that recovers sequences in the original alphabet from sequences in the
reduced alphabet. We present a finite state transducer-based reconstruction
module that operates on the 1-best ASR hypothesis in the reduced alphabet. We
demonstrate the efficacy of our proposed technique using ASR systems for two
Indian languages, Gujarati and Telugu. With access to only 10 hrs of speech
data, we obtain relative WER reductions of up to 7% compared to systems that do
not use any reduction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Diwan_A/0/1/0/all/0/1"&gt;Anuj Diwan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jyothi_P/0/1/0/all/0/1"&gt;Preethi Jyothi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Template-Based Named Entity Recognition Using BART. (arXiv:2106.01760v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01760</id>
        <link href="http://arxiv.org/abs/2106.01760"/>
        <updated>2021-06-04T01:12:30.670Z</updated>
        <summary type="html"><![CDATA[There is a recent interest in investigating few-shot NER, where the
low-resource target domain has different label sets compared with a
resource-rich source domain. Existing methods use a similarity-based metric.
However, they cannot make full use of knowledge transfer in NER model
parameters. To address the issue, we propose a template-based method for NER,
treating NER as a language model ranking problem in a sequence-to-sequence
framework, where original sentences and statement templates filled by candidate
named entity span are regarded as the source sequence and the target sequence,
respectively. For inference, the model is required to classify each candidate
span based on the corresponding template scores. Our experiments demonstrate
that the proposed method achieves 92.55% F1 score on the CoNLL03 (rich-resource
task), and significantly better than fine-tuning BERT 10.88%, 15.34%, and
11.73% F1 score on the MIT Movie, the MIT Restaurant, and the ATIS
(low-resource task), respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1"&gt;Leyang Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yu Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jian Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1"&gt;Sen Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yue Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Defending against Backdoor Attacks in Natural Language Generation. (arXiv:2106.01810v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01810</id>
        <link href="http://arxiv.org/abs/2106.01810"/>
        <updated>2021-06-04T01:12:30.651Z</updated>
        <summary type="html"><![CDATA[The frustratingly fragile nature of neural network models make current
natural language generation (NLG) systems prone to backdoor attacks and
generate malicious sequences that could be sexist or offensive. Unfortunately,
little effort has been invested to how backdoor attacks can affect current NLG
models and how to defend against these attacks. In this work, we investigate
this problem on two important NLG tasks, machine translation and dialogue
generation. By giving a formal definition for backdoor attack and defense, and
developing corresponding benchmarks, we design methods to attack NLG models,
which achieve high attack success to ask NLG models to generate malicious
sequences. To defend against these attacks, we propose to detect the attack
trigger by examining the effect of deleting or replacing certain words on the
generation outputs, which we find successful for certain types of attacks. We
will discuss the limitation of this work, and hope this work can raise the
awareness of backdoor risks concealed in deep NLG systems. (Code and data are
available at https://github.com/ShannonAI/backdoor_nlg.)]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1"&gt;Chun Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiaoya Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1"&gt;Yuxian Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1"&gt;Xiaofei Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ao_X/0/1/0/all/0/1"&gt;Xiang Ao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1"&gt;Fei Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jiwei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1"&gt;Tianwei Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SimCLS: A Simple Framework for Contrastive Learning of Abstractive Summarization. (arXiv:2106.01890v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01890</id>
        <link href="http://arxiv.org/abs/2106.01890"/>
        <updated>2021-06-04T01:12:30.644Z</updated>
        <summary type="html"><![CDATA[In this paper, we present a conceptually simple while empirically powerful
framework for abstractive summarization, SimCLS, which can bridge the gap
between the learning objective and evaluation metrics resulting from the
currently dominated sequence-to-sequence learning framework by formulating text
generation as a reference-free evaluation problem (i.e., quality estimation)
assisted by contrastive learning. Experimental results show that, with minor
modification over existing top-scoring systems, SimCLS can improve the
performance of existing top-performing models by a large margin. Particularly,
2.51 absolute improvement against BART and 2.50 over PEGASUS w.r.t ROUGE-1 on
the CNN/DailyMail dataset, driving the state-of-the-art performance to a new
level. We have open-sourced our codes and results:
https://github.com/yixinL7/SimCLS. Results of our proposed models have been
deployed into ExplainaBoard platform, which allows researchers to understand
our systems in a more fine-grained way.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yixin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1"&gt;Pengfei Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DNA-GCN: Graph convolutional networks for predicting DNA-protein binding. (arXiv:2106.01836v1 [q-bio.GN])]]></title>
        <id>http://arxiv.org/abs/2106.01836</id>
        <link href="http://arxiv.org/abs/2106.01836"/>
        <updated>2021-06-04T01:12:30.471Z</updated>
        <summary type="html"><![CDATA[Predicting DNA-protein binding is an important and classic problem in
bioinformatics. Convolutional neural networks have outperformed conventional
methods in modeling the sequence specificity of DNA-protein binding. However,
none of the studies has utilized graph convolutional networks for motif
inference. In this work, we propose to use graph convolutional networks for
motif inference. We build a sequence k-mer graph for the whole dataset based on
k-mer co-occurrence and k-mer sequence relationship and then learn DNA Graph
Convolutional Network (DNA-GCN) for the whole dataset. Our DNA-GCN is
initialized with a one-hot representation for all nodes, and it then jointly
learns the embeddings for both k-mers and sequences, as supervised by the known
labels of sequences. We evaluate our model on 50 datasets from ENCODE. DNA-GCN
shows its competitive performance compared with the baseline model. Besides, we
analyze our model and design several different architectures to help fit
different datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Guo_Y/0/1/0/all/0/1"&gt;Yuhang Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Luo_X/0/1/0/all/0/1"&gt;Xiao Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Chen_L/0/1/0/all/0/1"&gt;Liang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Deng_M/0/1/0/all/0/1"&gt;Minghua Deng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[3D Hand Pose Estimation via Regularized Graph Representation Learning. (arXiv:1912.01875v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1912.01875</id>
        <link href="http://arxiv.org/abs/1912.01875"/>
        <updated>2021-06-04T01:12:30.464Z</updated>
        <summary type="html"><![CDATA[This paper addresses the problem of 3D hand pose estimation from a monocular
RGB image. While previous methods have shown great success, the structure of
hands has not been fully exploited, which is critical in pose estimation. To
this end, we propose a regularized graph representation learning under a
conditional adversarial learning framework for 3D hand pose estimation, aiming
to capture structural inter-dependencies of hand joints. In particular, we
estimate an initial hand pose from a parametric hand model as a prior of hand
structure, which regularizes the inference of the structural deformation in the
prior pose for accurate graph representation learning via residual graph
convolution. To optimize the hand structure further, we propose two
bone-constrained loss functions, which characterize the morphable structure of
hand poses explicitly. Also, we introduce an adversarial learning framework
conditioned on the input image with a multi-source discriminator, which imposes
the structural constraints onto the distribution of generated 3D hand poses for
anthropomorphically valid hand poses. Extensive experiments demonstrate that
our model sets the new state-of-the-art in 3D hand pose estimation from a
monocular image on five standard benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1"&gt;Yiming He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1"&gt;Wei Hu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Reference-based Super-Resolution via C2-Matching. (arXiv:2106.01863v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.01863</id>
        <link href="http://arxiv.org/abs/2106.01863"/>
        <updated>2021-06-04T01:12:30.458Z</updated>
        <summary type="html"><![CDATA[Reference-based Super-Resolution (Ref-SR) has recently emerged as a promising
paradigm to enhance a low-resolution (LR) input image by introducing an
additional high-resolution (HR) reference image. Existing Ref-SR methods mostly
rely on implicit correspondence matching to borrow HR textures from reference
images to compensate for the information loss in input images. However,
performing local transfer is difficult because of two gaps between input and
reference images: the transformation gap (e.g. scale and rotation) and the
resolution gap (e.g. HR and LR). To tackle these challenges, we propose
C2-Matching in this work, which produces explicit robust matching crossing
transformation and resolution. 1) For the transformation gap, we propose a
contrastive correspondence network, which learns transformation-robust
correspondences using augmented views of the input image. 2) For the resolution
gap, we adopt a teacher-student correlation distillation, which distills
knowledge from the easier HR-HR matching to guide the more ambiguous LR-HR
matching. 3) Finally, we design a dynamic aggregation module to address the
potential misalignment issue. In addition, to faithfully evaluate the
performance of Ref-SR under a realistic setting, we contribute the
Webly-Referenced SR (WR-SR) dataset, mimicking the practical usage scenario.
Extensive experiments demonstrate that our proposed C2-Matching significantly
outperforms state of the arts by over 1dB on the standard CUFED5 benchmark.
Notably, it also shows great generalizability on WR-SR dataset as well as
robustness across large scale and rotation transformations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1"&gt;Yuming Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chan_K/0/1/0/all/0/1"&gt;Kelvin C.K. Chan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xintao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Loy_C/0/1/0/all/0/1"&gt;Chen Change Loy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Ziwei Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quantifying language changes surrounding mental health on Twitter. (arXiv:2106.01481v1 [physics.soc-ph])]]></title>
        <id>http://arxiv.org/abs/2106.01481</id>
        <link href="http://arxiv.org/abs/2106.01481"/>
        <updated>2021-06-04T01:12:30.439Z</updated>
        <summary type="html"><![CDATA[Mental health challenges are thought to afflict around 10% of the global
population each year, with many going untreated due to stigma and limited
access to services. Here, we explore trends in words and phrases related to
mental health through a collection of 1- , 2-, and 3-grams parsed from a data
stream of roughly 10% of all English tweets since 2012. We examine temporal
dynamics of mental health language, finding that the popularity of the phrase
'mental health' increased by nearly two orders of magnitude between 2012 and
2018. We observe that mentions of 'mental health' spike annually and reliably
due to mental health awareness campaigns, as well as unpredictably in response
to mass shootings, celebrities dying by suicide, and popular fictional stories
portraying suicide. We find that the level of positivity of messages containing
'mental health', while stable through the growth period, has declined recently.
Finally, we use the ratio of original tweets to retweets to quantify the
fraction of appearances of mental health language due to social amplification.
Since 2015, mentions of mental health have become increasingly due to retweets,
suggesting that stigma associated with discussion of mental health on Twitter
has diminished with time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Stupinski_A/0/1/0/all/0/1"&gt;Anne Marie Stupinski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Alshaabi_T/0/1/0/all/0/1"&gt;Thayer Alshaabi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Arnold_M/0/1/0/all/0/1"&gt;Michael V. Arnold&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Adams_J/0/1/0/all/0/1"&gt;Jane Lydia Adams&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Minot_J/0/1/0/all/0/1"&gt;Joshua R. Minot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Price_M/0/1/0/all/0/1"&gt;Matthew Price&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Dodds_P/0/1/0/all/0/1"&gt;Peter Sheridan Dodds&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Danforth_C/0/1/0/all/0/1"&gt;Christopher M. Danforth&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning High-Precision Bounding Box for Rotated Object Detection via Kullback-Leibler Divergence. (arXiv:2106.01883v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.01883</id>
        <link href="http://arxiv.org/abs/2106.01883"/>
        <updated>2021-06-04T01:12:30.432Z</updated>
        <summary type="html"><![CDATA[Existing rotated object detectors are mostly inherited from the horizontal
detection paradigm, as the latter has evolved into a well-developed area.
However, these detectors are difficult to perform prominently in high-precision
detection due to the limitation of current regression loss design, especially
for objects with large aspect ratios. Taking the perspective that horizontal
detection is a special case for rotated object detection, in this paper, we are
motivated to change the design of rotation regression loss from induction
paradigm to deduction methodology, in terms of the relation between rotation
and horizontal detection. We show that one essential challenge is how to
modulate the coupled parameters in the rotation regression loss, as such the
estimated parameters can influence to each other during the dynamic joint
optimization, in an adaptive and synergetic way. Specifically, we first convert
the rotated bounding box into a 2-D Gaussian distribution, and then calculate
the Kullback-Leibler Divergence (KLD) between the Gaussian distributions as the
regression loss. By analyzing the gradient of each parameter, we show that KLD
(and its derivatives) can dynamically adjust the parameter gradients according
to the characteristics of the object. It will adjust the importance (gradient
weight) of the angle parameter according to the aspect ratio. This mechanism
can be vital for high-precision detection as a slight angle error would cause a
serious accuracy drop for large aspect ratios objects. More importantly, we
have proved that KLD is scale invariant. We further show that the KLD loss can
be degenerated into the popular $l_{n}$-norm loss for horizontal detection.
Experimental results on seven datasets using different detectors show its
consistent superiority, and codes are available at
https://github.com/yangxue0827/RotationDetection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xue Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xiaojiang Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jirui Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ming_Q/0/1/0/all/0/1"&gt;Qi Ming&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wentao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1"&gt;Qi Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1"&gt;Junchi Yan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Actor: Neural Free-view Synthesis of Human Actors with Pose Control. (arXiv:2106.02019v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02019</id>
        <link href="http://arxiv.org/abs/2106.02019"/>
        <updated>2021-06-04T01:12:30.426Z</updated>
        <summary type="html"><![CDATA[We propose Neural Actor (NA), a new method for high-quality synthesis of
humans from arbitrary viewpoints and under arbitrary controllable poses. Our
method is built upon recent neural scene representation and rendering works
which learn representations of geometry and appearance from only 2D images.
While existing works demonstrated compelling rendering of static scenes and
playback of dynamic scenes, photo-realistic reconstruction and rendering of
humans with neural implicit methods, in particular under user-controlled novel
poses, is still difficult. To address this problem, we utilize a coarse body
model as the proxy to unwarp the surrounding 3D space into a canonical pose. A
neural radiance field learns pose-dependent geometric deformations and pose-
and view-dependent appearance effects in the canonical space from multi-view
video input. To synthesize novel views of high fidelity dynamic geometry and
appearance, we leverage 2D texture maps defined on the body model as latent
variables for predicting residual deformations and the dynamic appearance.
Experiments demonstrate that our method achieves better quality than the
state-of-the-arts on playback as well as novel pose synthesis, and can even
generalize well to new poses that starkly differ from the training poses.
Furthermore, our method also supports body shape control of the synthesized
results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Lingjie Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Habermann_M/0/1/0/all/0/1"&gt;Marc Habermann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rudnev_V/0/1/0/all/0/1"&gt;Viktor Rudnev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sarkar_K/0/1/0/all/0/1"&gt;Kripasindhu Sarkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1"&gt;Jiatao Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1"&gt;Christian Theobalt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semi-supervised Conditional Density Estimation for Imputation and Classification of Incomplete Instances. (arXiv:2106.01708v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01708</id>
        <link href="http://arxiv.org/abs/2106.01708"/>
        <updated>2021-06-04T01:12:30.420Z</updated>
        <summary type="html"><![CDATA[Incomplete instances with various missing attributes in many real-world
scenes have brought challenges to the classification task. There are some
missing values imputation methods to fill the missing values with substitute
values before classification. However, the separation between imputation and
classification may lead to inferior performance since label information are
ignored during imputation. Moreover, these imputation methods tend to
initialize these missing values with strong prior assumptions, while the
unreliability of such initialization is rarely considered. To tackle these
problems, a novel semi-supervised conditional normalizing flow (SSCFlow) is
proposed in this paper. SSCFlow explicitly utilizes the observed labels to
facilitate the imputation and classification simultaneously by employing a
semi-supervised algorithm to estimate the conditional probability density of
missing values. Moreover, SSCFlow takes the initialized missing values as
corrupted initial imputation and iteratively reconstructs their latent
representations with an overcomplete denoising autoencoder to approximate the
true conditional probability density of missing values. Experiments have been
conducted with real-world datasets to demonstrate the robustness and efficiency
of the proposed algorithm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_B/0/1/0/all/0/1"&gt;Buliao Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Simultaneous Multi-View Object Recognition and Grasping in Open-Ended Domains. (arXiv:2106.01866v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2106.01866</id>
        <link href="http://arxiv.org/abs/2106.01866"/>
        <updated>2021-06-04T01:12:30.413Z</updated>
        <summary type="html"><![CDATA[A robot working in human-centric environments needs to know which kind of
objects exist in the scene, where they are, and how to grasp and manipulate
various objects in different situations to help humans in everyday tasks.
Therefore, object recognition and grasping are two key functionalities for such
robots. Most state-of-the-art tackles object recognition and grasping as two
separate problems while both use visual input. Furthermore, the knowledge of
the robot is fixed after the training phase. In such cases, if the robot faces
new object categories, it must retrain from scratch to incorporate new
information without catastrophic interference. To address this problem, we
propose a deep learning architecture with augmented memory capacities to handle
open-ended object recognition and grasping simultaneously. In particular, our
approach takes multi-views of an object as input and jointly estimates
pixel-wise grasp configuration as well as a deep scale- and rotation-invariant
representation as outputs. The obtained representation is then used for
open-ended object recognition through a meta-active learning technique. We
demonstrate the ability of our approach to grasp never-seen-before objects and
to rapidly learn new object categories using very few examples on-site in both
simulation and real-world settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kasaei_H/0/1/0/all/0/1"&gt;Hamidreza Kasaei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_S/0/1/0/all/0/1"&gt;Sha Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sasso_R/0/1/0/all/0/1"&gt;Remo Sasso&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kasaei_M/0/1/0/all/0/1"&gt;Mohammadreza Kasaei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Single Image Depth Estimation using Wavelet Decomposition. (arXiv:2106.02022v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02022</id>
        <link href="http://arxiv.org/abs/2106.02022"/>
        <updated>2021-06-04T01:12:30.393Z</updated>
        <summary type="html"><![CDATA[We present a novel method for predicting accurate depths from monocular
images with high efficiency. This optimal efficiency is achieved by exploiting
wavelet decomposition, which is integrated in a fully differentiable
encoder-decoder architecture. We demonstrate that we can reconstruct
high-fidelity depth maps by predicting sparse wavelet coefficients. In contrast
with previous works, we show that wavelet coefficients can be learned without
direct supervision on coefficients. Instead we supervise only the final depth
image that is reconstructed through the inverse wavelet transform. We
additionally show that wavelet coefficients can be learned in fully
self-supervised scenarios, without access to ground-truth depth. Finally, we
apply our method to different state-of-the-art monocular depth estimation
models, in each case giving similar or better results compared to the original
model, while requiring less than half the multiply-adds in the decoder network.
Code at https://github.com/nianticlabs/wavelet-monodepth]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ramamonjisoa_M/0/1/0/all/0/1"&gt;Micha&amp;#xeb;l Ramamonjisoa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Firman_M/0/1/0/all/0/1"&gt;Michael Firman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Watson_J/0/1/0/all/0/1"&gt;Jamie Watson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lepetit_V/0/1/0/all/0/1"&gt;Vincent Lepetit&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Turmukhambetov_D/0/1/0/all/0/1"&gt;Daniyar Turmukhambetov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Advances in Classifying the Stages of Diabetic Retinopathy Using Convolutional Neural Networks in Low Memory Edge Devices. (arXiv:2106.01739v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.01739</id>
        <link href="http://arxiv.org/abs/2106.01739"/>
        <updated>2021-06-04T01:12:30.387Z</updated>
        <summary type="html"><![CDATA[Diabetic Retinopathy (DR) is a severe complication that may lead to retinal
vascular damage and is one of the leading causes of vision impairment and
blindness. DR broadly is classified into two stages - non-proliferative (NPDR),
where there are almost no symptoms, except a few microaneurysms, and
proliferative (PDR) involving a huge number of microaneurysms and hemorrhages,
soft and hard exudates, neo-vascularization, macular ischemia or a combination
of these, making it easier to detect. More specifically, DR is usually
classified into five levels, labeled 0-4, from 0 indicating no DR to 4 which is
most severe. This paper firstly presents a discussion on the risk factors of
the disease, then surveys the recent literature on the topic followed by
examining certain techniques which were found to be highly effective in
improving the prognosis accuracy. Finally, a convolutional neural network model
is proposed to detect all the stages of DR on a low-memory edge
microcontroller. The model has a size of just 5.9 MB, accuracy and F1 score
both of 94% and an inference speed of about 20 frames per second.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Paul_A/0/1/0/all/0/1"&gt;Aditya Jyoti Paul&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Learning Based Analysis of Prostate Cancer from MP-MRI. (arXiv:2106.01835v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.01835</id>
        <link href="http://arxiv.org/abs/2106.01835"/>
        <updated>2021-06-04T01:12:30.381Z</updated>
        <summary type="html"><![CDATA[The diagnosis of prostate cancer faces a problem with overdiagnosis that
leads to damaging side effects due to unnecessary treatment. Research has shown
that the use of multi-parametric magnetic resonance images to conduct biopsies
can drastically help to mitigate the overdiagnosis, thus reducing the side
effects on healthy patients. This study aims to investigate the use of deep
learning techniques to explore computer-aid diagnosis based on MRI as input.
Several diagnosis problems ranging from classification of lesions as being
clinically significant or not to the detection and segmentation of lesions are
addressed with deep learning based approaches.

This thesis tackled two main problems regarding the diagnosis of prostate
cancer. Firstly, XmasNet was used to conduct two large experiments on the
classification of lesions. Secondly, detection and segmentation experiments
were conducted, first on the prostate and afterward on the prostate cancer
lesions. The former experiments explored the lesions through a two-dimensional
space, while the latter explored models to work with three-dimensional inputs.
For this task, the 3D models explored were the 3D U-Net and a pretrained 3D
ResNet-18. A rigorous analysis of all these problems was conducted with a total
of two networks, two cropping techniques, two resampling techniques, two crop
sizes, five input sizes and data augmentations experimented for lesion
classification. While for segmentation two models, two input sizes and data
augmentations were experimented. However, while the binary classification of
the clinical significance of lesions and the detection and segmentation of the
prostate already achieve the desired results (0.870 AUC and 0.915 dice score
respectively), the classification of the PIRADS score and the segmentation of
lesions still have a large margin to improve (0.664 accuracy and 0.690 dice
score respectively).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Neto_P/0/1/0/all/0/1"&gt;Pedro C. Neto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Anticipative Video Transformer. (arXiv:2106.02036v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02036</id>
        <link href="http://arxiv.org/abs/2106.02036"/>
        <updated>2021-06-04T01:12:30.375Z</updated>
        <summary type="html"><![CDATA[We propose Anticipative Video Transformer (AVT), an end-to-end
attention-based video modeling architecture that attends to the previously
observed video in order to anticipate future actions. We train the model
jointly to predict the next action in a video sequence, while also learning
frame feature encoders that are predictive of successive future frames'
features. Compared to existing temporal aggregation strategies, AVT has the
advantage of both maintaining the sequential progression of observed actions
while still capturing long-range dependencies--both critical for the
anticipation task. Through extensive experiments, we show that AVT obtains the
best reported performance on four popular action anticipation benchmarks:
EpicKitchens-55, EpicKitchens-100, EGTEA Gaze+, and 50-Salads, including
outperforming all submissions to the EpicKitchens-100 CVPR'21 challenge.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Girdhar_R/0/1/0/all/0/1"&gt;Rohit Girdhar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grauman_K/0/1/0/all/0/1"&gt;Kristen Grauman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Shared Semantic Space for Speech-to-Text Translation. (arXiv:2105.03095v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.03095</id>
        <link href="http://arxiv.org/abs/2105.03095"/>
        <updated>2021-06-04T01:12:30.368Z</updated>
        <summary type="html"><![CDATA[Having numerous potential applications and great impact, end-to-end speech
translation (ST) has long been treated as an independent task, failing to fully
draw strength from the rapid advances of its sibling - text machine translation
(MT). With text and audio inputs represented differently, the modality gap has
rendered MT data and its end-to-end models incompatible with their ST
counterparts. In observation of this obstacle, we propose to bridge this
representation gap with Chimera. By projecting audio and text features to a
common semantic representation, Chimera unifies MT and ST tasks and boosts the
performance on ST benchmarks, MuST-C and Augmented Librispeech, to a new
state-of-the-art. Specifically, Chimera obtains 27.1 BLEU on MuST-C EN-DE,
improving the SOTA by a +1.9 BLEU margin. Further experimental analyses
demonstrate that the shared semantic space indeed conveys common knowledge
between these two tasks and thus paves a new way for augmenting training
resources across modalities. Code, data, and resources are available at
https://github.com/Glaciohound/Chimera-ST.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Han_C/0/1/0/all/0/1"&gt;Chi Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1"&gt;Mingxuan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1"&gt;Heng Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lei Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Denoising and Optical and SAR Image Classifications Based on Feature Extraction and Sparse Representation. (arXiv:2106.01896v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.01896</id>
        <link href="http://arxiv.org/abs/2106.01896"/>
        <updated>2021-06-04T01:12:30.362Z</updated>
        <summary type="html"><![CDATA[Optical image data have been used by the Remote Sensing workforce to study
land use and cover since such data is easily interpretable. Synthetic Aperture
Radar (SAR) has the characteristic of obtaining images during all-day,
all-weather and provides object information that is different from visible and
infrared sensors. However, SAR images have more speckle noise and fewer
dimensions. This paper presents a method for denoising, feature extraction and
compares classifications of Optical and SAR images. The image was denoised
using K-Singular Value Decomposition (K-SVD) algorithm. A method to map the
extraordinary goal signatures to be had withinside the SAR or Optical image
using support vector machine (SVM) through offering given the enter facts to
the supervised classifier. Initially, the Gray Level Histogram (GLH) and Gray
Level Co-occurrence Matrix (GLCM) are used for feature extraction. Secondly,
the extracted feature vectors from the first step were combined using
correlation analysis to reduce the dimensionality of the feature spaces.
Thirdly, the Classification of SAR images was done in Sparse Representations
Classification (SRC). The above-mentioned classifications techniques were
developed and performance parameters are accuracy and Kappa Coefficient
calculated using MATLAB 2018a.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Balnarsaiah_B/0/1/0/all/0/1"&gt;Battula Balnarsaiah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rajitha_G/0/1/0/all/0/1"&gt;G Rajitha&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Improved Baseline for Sentence-level Relation Extraction. (arXiv:2102.01373v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.01373</id>
        <link href="http://arxiv.org/abs/2102.01373"/>
        <updated>2021-06-04T01:12:30.343Z</updated>
        <summary type="html"><![CDATA[Sentence-level relation extraction (RE) aims at identifying the relationship
between two entities in a sentence. Many efforts have been devoted to this
problem, while the best performing methods are still far from perfect. In this
paper, we revisit two problems that affect the performance of existing RE
models, namely entity representation and noisy or ill-defined labels. Our
improved baseline model, incorporated with entity representations with typed
markers, achieves an F1 of 74.6% on TACRED, significantly outperforms previous
SOTA methods. Furthermore, the presented new baseline achieves an F1 of 91.1%
on the refined Re-TACRED dataset, demonstrating that the pre-trained language
models achieve unexpectedly high performance on this task. We release our code
to the community for future research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1"&gt;Wenxuan Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1"&gt;Muhao Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DynamicViT: Efficient Vision Transformers with Dynamic Token Sparsification. (arXiv:2106.02034v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02034</id>
        <link href="http://arxiv.org/abs/2106.02034"/>
        <updated>2021-06-04T01:12:30.337Z</updated>
        <summary type="html"><![CDATA[Attention is sparse in vision transformers. We observe the final prediction
in vision transformers is only based on a subset of most informative tokens,
which is sufficient for accurate image recognition. Based on this observation,
we propose a dynamic token sparsification framework to prune redundant tokens
progressively and dynamically based on the input. Specifically, we devise a
lightweight prediction module to estimate the importance score of each token
given the current features. The module is added to different layers to prune
redundant tokens hierarchically. To optimize the prediction module in an
end-to-end manner, we propose an attention masking strategy to differentiably
prune a token by blocking its interactions with other tokens. Benefiting from
the nature of self-attention, the unstructured sparse tokens are still hardware
friendly, which makes our framework easy to achieve actual speed-up. By
hierarchically pruning 66% of the input tokens, our method greatly reduces
31%~37% FLOPs and improves the throughput by over 40% while the drop of
accuracy is within 0.5% for various vision transformers. Equipped with the
dynamic token sparsification framework, DynamicViT models can achieve very
competitive complexity/accuracy trade-offs compared to state-of-the-art CNNs
and vision transformers on ImageNet. Code is available at
https://github.com/raoyongming/DynamicViT]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rao_Y/0/1/0/all/0/1"&gt;Yongming Rao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1"&gt;Wenliang Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1"&gt;Benlin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1"&gt;Jiwen Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jie Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1"&gt;Cho-Jui Hsieh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Partial Graph Reasoning for Neural Network Regularization. (arXiv:2106.01805v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01805</id>
        <link href="http://arxiv.org/abs/2106.01805"/>
        <updated>2021-06-04T01:12:30.330Z</updated>
        <summary type="html"><![CDATA[Regularizers helped deep neural networks prevent feature co-adaptations.
Dropout,as a commonly used regularization technique, stochastically disables
neuron ac-tivations during network optimization. However, such complete feature
disposal can affect the feature representation and network understanding.
Toward betterdescriptions of latent representations, we present DropGraph that
learns regularization function by constructing a stand-alone graph from the
backbone features. DropGraph first samples stochastic spatial feature vectors
and then incorporates graph reasoning methods to generate feature map
distortions. This add-on graph regularizes the network during training and can
be completely skipped during inference. We provide intuitions on the linkage
between graph reasoning andDropout with further discussions on how partial
graph reasoning method reduces feature correlations. To this end, we
extensively study the modeling of graphvertex dependencies and the utilization
of the graph for distorting backbone featuremaps. DropGraph was validated on
four tasks with a total of 7 different datasets.The experimental results show
that our method outperforms other state-of-the-art regularizers while leaving
the base model structure unmodified during inference.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1"&gt;Tiange Xiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chaoyi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1"&gt;Yang Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Siqi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1"&gt;Hongliang Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_W/0/1/0/all/0/1"&gt;Weidong Cai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Knowing More About Questions Can Help: Improving Calibration in Question Answering. (arXiv:2106.01494v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01494</id>
        <link href="http://arxiv.org/abs/2106.01494"/>
        <updated>2021-06-04T01:12:30.324Z</updated>
        <summary type="html"><![CDATA[We study calibration in question answering, estimating whether model
correctly predicts answer for each question. Unlike prior work which mainly
rely on the model's confidence score, our calibrator incorporates information
about the input example (e.g., question and the evidence context). Together
with data augmentation via back translation, our simple approach achieves 5-10%
gains in calibration accuracy on reading comprehension benchmarks. Furthermore,
we present the first calibration study in the open retrieval setting, comparing
the calibration accuracy of retrieval-based span prediction models and answer
generation models. Here again, our approach shows consistent gains over
calibrators relying on the model confidence. Our simple and efficient
calibrator can be easily adapted to many tasks and model architectures, showing
robust gains in all settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shujian Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_C/0/1/0/all/0/1"&gt;Chengyue Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_E/0/1/0/all/0/1"&gt;Eunsol Choi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MPC-BERT: A Pre-Trained Language Model for Multi-Party Conversation Understanding. (arXiv:2106.01541v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01541</id>
        <link href="http://arxiv.org/abs/2106.01541"/>
        <updated>2021-06-04T01:12:30.318Z</updated>
        <summary type="html"><![CDATA[Recently, various neural models for multi-party conversation (MPC) have
achieved impressive improvements on a variety of tasks such as addressee
recognition, speaker identification and response prediction. However, these
existing methods on MPC usually represent interlocutors and utterances
individually and ignore the inherent complicated structure in MPC which may
provide crucial interlocutor and utterance semantics and would enhance the
conversation understanding process. To this end, we present MPC-BERT, a
pre-trained model for MPC understanding that considers learning who says what
to whom in a unified model with several elaborated self-supervised tasks.
Particularly, these tasks can be generally categorized into (1) interlocutor
structure modeling including reply-to utterance recognition, identical speaker
searching and pointer consistency distinction, and (2) utterance semantics
modeling including masked shared utterance restoration and shared node
detection. We evaluate MPC-BERT on three downstream tasks including addressee
recognition, speaker identification and response selection. Experimental
results show that MPC-BERT outperforms previous methods by large margins and
achieves new state-of-the-art performance on all three downstream tasks at two
benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1"&gt;Jia-Chen Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_C/0/1/0/all/0/1"&gt;Chongyang Tao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ling_Z/0/1/0/all/0/1"&gt;Zhen-Hua Ling&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Can Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1"&gt;Xiubo Geng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1"&gt;Daxin Jiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Separated-Spectral-Distribution Estimation Based on Bayesian Inference with Single RGB Camera. (arXiv:2106.01861v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.01861</id>
        <link href="http://arxiv.org/abs/2106.01861"/>
        <updated>2021-06-04T01:12:30.299Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a novel method for separately estimating spectral
distributions from images captured by a typical RGB camera. The proposed method
allows us to separately estimate a spectral distribution of illumination,
reflectance, or camera sensitivity, while recent hyperspectral cameras are
limited to capturing a joint spectral distribution from a scene. In addition,
the use of Bayesian inference makes it possible to take into account prior
information of both spectral distributions and image noise as probability
distributions. As a result, the proposed method can estimate spectral
distributions in a unified way, and it can enhance the robustness of the
estimation against noise, which conventional spectral-distribution estimation
methods cannot. The use of Bayesian inference also enables us to obtain the
confidence of estimation results. In an experiment, the proposed method is
shown not only to outperform conventional estimation methods in terms of RMSE
but also to be robust against noise.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Kinoshita_Y/0/1/0/all/0/1"&gt;Yuma Kinoshita&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kiya_H/0/1/0/all/0/1"&gt;Hitoshi Kiya&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Luna: Linear Unified Nested Attention. (arXiv:2106.01540v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01540</id>
        <link href="http://arxiv.org/abs/2106.01540"/>
        <updated>2021-06-04T01:12:30.293Z</updated>
        <summary type="html"><![CDATA[The quadratic computational and memory complexities of the Transformer's
attention mechanism have limited its scalability for modeling long sequences.
In this paper, we propose Luna, a linear unified nested attention mechanism
that approximates softmax attention with two nested linear attention functions,
yielding only linear (as opposed to quadratic) time and space complexity.
Specifically, with the first attention function, Luna packs the input sequence
into a sequence of fixed length. Then, the packed sequence is unpacked using
the second attention function. As compared to a more traditional attention
mechanism, Luna introduces an additional sequence with a fixed length as input
and an additional corresponding output, which allows Luna to perform attention
operation linearly, while also storing adequate contextual information. We
perform extensive evaluations on three benchmarks of sequence modeling tasks:
long-context sequence modeling, neural machine translation and masked language
modeling for large-scale pretraining. Competitive or even better experimental
results demonstrate both the effectiveness and efficiency of Luna compared to a
variety]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1"&gt;Xuezhe Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kong_X/0/1/0/all/0/1"&gt;Xiang Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Sinong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1"&gt;Chunting Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+May_J/0/1/0/all/0/1"&gt;Jonathan May&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1"&gt;Hao Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1"&gt;Luke Zettlemoyer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BERT meets LIWC: Exploring State-of-the-Art Language Models for Predicting Communication Behavior in Couples' Conflict Interactions. (arXiv:2106.01536v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01536</id>
        <link href="http://arxiv.org/abs/2106.01536"/>
        <updated>2021-06-04T01:12:30.287Z</updated>
        <summary type="html"><![CDATA[Many processes in psychology are complex, such as dyadic interactions between
two interacting partners (e.g. patient-therapist, intimate relationship
partners). Nevertheless, many basic questions about interactions are difficult
to investigate because dyadic processes can be within a person and between
partners, they are based on multimodal aspects of behavior and unfold rapidly.
Current analyses are mainly based on the behavioral coding method, whereby
human coders annotate behavior based on a coding schema. But coding is
labor-intensive, expensive, slow, focuses on few modalities. Current approaches
in psychology use LIWC for analyzing couples' interactions. However, advances
in natural language processing such as BERT could enable the development of
systems to potentially automate behavioral coding, which in turn could
substantially improve psychological research. In this work, we train machine
learning models to automatically predict positive and negative communication
behavioral codes of 368 German-speaking Swiss couples during an 8-minute
conflict interaction on a fine-grained scale (10-seconds sequences) using
linguistic features and paralinguistic features derived with openSMILE. Our
results show that both simpler TF-IDF features as well as more complex BERT
features performed better than LIWC, and that adding paralinguistic features
did not improve the performance. These results suggest it might be time to
consider modern alternatives to LIWC, the de facto linguistic features in
psychology, for prediction tasks in couples research. This work is a further
step towards the automated coding of couples' behavior which could enhance
couple research and therapy, and be utilized for other dyadic interactions as
well.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Biggiogera_J/0/1/0/all/0/1"&gt;Jacopo Biggiogera&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boateng_G/0/1/0/all/0/1"&gt;George Boateng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hilpert_P/0/1/0/all/0/1"&gt;Peter Hilpert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vowels_M/0/1/0/all/0/1"&gt;Matthew Vowels&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bodenmann_G/0/1/0/all/0/1"&gt;Guy Bodenmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neysari_M/0/1/0/all/0/1"&gt;Mona Neysari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nussbeck_F/0/1/0/all/0/1"&gt;Fridtjof Nussbeck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kowatsch_T/0/1/0/all/0/1"&gt;Tobias Kowatsch&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA["You made me feel this way": Investigating Partners' Influence in Predicting Emotions in Couples' Conflict Interactions using Speech Data. (arXiv:2106.01526v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01526</id>
        <link href="http://arxiv.org/abs/2106.01526"/>
        <updated>2021-06-04T01:12:30.280Z</updated>
        <summary type="html"><![CDATA[How romantic partners interact with each other during a conflict influences
how they feel at the end of the interaction and is predictive of whether the
partners stay together in the long term. Hence understanding the emotions of
each partner is important. Yet current approaches that are used include
self-reports which are burdensome and hence limit the frequency of this data
collection. Automatic emotion prediction could address this challenge. Insights
from psychology research indicate that partners' behaviors influence each
other's emotions in conflict interaction and hence, the behavior of both
partners could be considered to better predict each partner's emotion. However,
it is yet to be investigated how doing so compares to only using each partner's
own behavior in terms of emotion prediction performance. In this work, we used
BERT to extract linguistic features (i.e., what partners said) and openSMILE to
extract paralinguistic features (i.e., how they said it) from a data set of 368
German-speaking Swiss couples (N = 736 individuals) which were videotaped
during an 8-minutes conflict interaction in the laboratory. Based on those
features, we trained machine learning models to predict if partners feel
positive or negative after the conflict interaction. Our results show that
including the behavior of the other partner improves the prediction
performance. Furthermore, for men, considering how their female partners spoke
is most important and for women considering what their male partner said is
most important in getting better prediction performance. This work is a step
towards automatically recognizing each partners' emotion based on the behavior
of both, which would enable a better understanding of couples in research,
therapy, and the real world.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Boateng_G/0/1/0/all/0/1"&gt;George Boateng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hilpert_P/0/1/0/all/0/1"&gt;Peter Hilpert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bodenmann_G/0/1/0/all/0/1"&gt;Guy Bodenmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neysari_M/0/1/0/all/0/1"&gt;Mona Neysari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kowatsch_T/0/1/0/all/0/1"&gt;Tobias Kowatsch&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Effort-free Automated Skeletal Abnormality Detection of Rat Fetuses on Whole-body Micro-CT Scans. (arXiv:2106.01830v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.01830</id>
        <link href="http://arxiv.org/abs/2106.01830"/>
        <updated>2021-06-04T01:12:30.274Z</updated>
        <summary type="html"><![CDATA[Machine Learning-based fast and quantitative automated screening plays a key
role in analyzing human bones on Computed Tomography (CT) scans. However,
despite the requirement in drug safety assessment, such research is rare on
animal fetus micro-CT scans due to its laborious data collection and
annotation. Therefore, we propose various bone feature engineering techniques
to thoroughly automate the skeletal localization/labeling/abnormality detection
of rat fetuses on whole-body micro-CT scans with minimum effort. Despite
limited training data of 49 fetuses, in skeletal labeling and abnormality
detection, we achieve accuracy of 0.900 and 0.810, respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Fukuda_A/0/1/0/all/0/1"&gt;Akihiro Fukuda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Han_C/0/1/0/all/0/1"&gt;Changhee Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hakamada_K/0/1/0/all/0/1"&gt;Kazumi Hakamada&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Modeling Fine-Grained Entity Types with Box Embeddings. (arXiv:2101.00345v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.00345</id>
        <link href="http://arxiv.org/abs/2101.00345"/>
        <updated>2021-06-04T01:12:30.254Z</updated>
        <summary type="html"><![CDATA[Neural entity typing models typically represent fine-grained entity types as
vectors in a high-dimensional space, but such spaces are not well-suited to
modeling these types' complex interdependencies. We study the ability of box
embeddings, which embed concepts as d-dimensional hyperrectangles, to capture
hierarchies of types even when these relationships are not defined explicitly
in the ontology. Our model represents both types and entity mentions as boxes.
Each mention and its context are fed into a BERT-based model to embed that
mention in our box space; essentially, this model leverages typological clues
present in the surface text to hypothesize a type representation for the
mention. Box containment can then be used to derive both the posterior
probability of a mention exhibiting a given type and the conditional
probability relations between types themselves. We compare our approach with a
vector-based typing model and observe state-of-the-art performance on several
entity typing benchmarks. In addition to competitive typing performance, our
box-based model shows better performance in prediction consistency (predicting
a supertype and a subtype together) and confidence (i.e., calibration),
demonstrating that the box-based model captures the latent type hierarchies
better than the vector-based model does.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Onoe_Y/0/1/0/all/0/1"&gt;Yasumasa Onoe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boratko_M/0/1/0/all/0/1"&gt;Michael Boratko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McCallum_A/0/1/0/all/0/1"&gt;Andrew McCallum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Durrett_G/0/1/0/all/0/1"&gt;Greg Durrett&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Training Multilingual Pre-trained Language Model with Byte-level Subwords. (arXiv:2101.09469v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.09469</id>
        <link href="http://arxiv.org/abs/2101.09469"/>
        <updated>2021-06-04T01:12:30.225Z</updated>
        <summary type="html"><![CDATA[The pre-trained language models have achieved great successes in various
natural language understanding (NLU) tasks due to its capacity to capture the
deep contextualized information in text by pre-training on large-scale corpora.
One of the fundamental components in pre-trained language models is the
vocabulary, especially for training multilingual models on many different
languages. In the technical report, we present our practices on training
multilingual pre-trained language models with BBPE: Byte-Level BPE (i.e., Byte
Pair Encoding). In the experiment, we adopted the architecture of NEZHA as the
underlying pre-trained language model and the results show that NEZHA trained
with byte-level subwords consistently outperforms Google multilingual BERT and
vanilla NEZHA by a notable margin in several multilingual NLU tasks. We release
the source code of our byte-level vocabulary building tools and the
multilingual pre-trained language models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1"&gt;Junqiu Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1"&gt;Qun Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1"&gt;Yinpeng Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1"&gt;Xin Jiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dissecting Generation Modes for Abstractive Summarization Models via Ablation and Attribution. (arXiv:2106.01518v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01518</id>
        <link href="http://arxiv.org/abs/2106.01518"/>
        <updated>2021-06-04T01:12:30.157Z</updated>
        <summary type="html"><![CDATA[Despite the prominence of neural abstractive summarization models, we know
little about how they actually form summaries and how to understand where their
decisions come from. We propose a two-step method to interpret summarization
model decisions. We first analyze the model's behavior by ablating the full
model to categorize each decoder decision into one of several generation modes:
roughly, is the model behaving like a language model, is it relying heavily on
the input, or is it somewhere in between? After isolating decisions that do
depend on the input, we explore interpreting these decisions using several
different attribution methods. We compare these techniques based on their
ability to select content and reconstruct the model's predicted token from
perturbations of the input, thus revealing whether highlighted attributions are
truly important for the generation of the next token. While this machinery can
be broadly useful even beyond summarization, we specifically demonstrate its
capability to identify phrases the summarization model has memorized and
determine where in the training pipeline this memorization happened, as well as
study complex generation phenomena like sentence fusion on a per-instance
basis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1"&gt;Jiacheng Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Durrett_G/0/1/0/all/0/1"&gt;Greg Durrett&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluating the Efficacy of Summarization Evaluation across Languages. (arXiv:2106.01478v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01478</id>
        <link href="http://arxiv.org/abs/2106.01478"/>
        <updated>2021-06-04T01:12:30.126Z</updated>
        <summary type="html"><![CDATA[While automatic summarization evaluation methods developed for English are
routinely applied to other languages, this is the first attempt to
systematically quantify their panlinguistic efficacy. We take a summarization
corpus for eight different languages, and manually annotate generated summaries
for focus (precision) and coverage (recall). Based on this, we evaluate 19
summarization evaluation metrics, and find that using multilingual BERT within
BERTScore performs well across all languages, at a level above that for
English.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Koto_F/0/1/0/all/0/1"&gt;Fajri Koto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lau_J/0/1/0/all/0/1"&gt;Jey Han Lau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baldwin_T/0/1/0/all/0/1"&gt;Timothy Baldwin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SMURF: SeMantic and linguistic UndeRstanding Fusion for Caption Evaluation via Typicality Analysis. (arXiv:2106.01444v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01444</id>
        <link href="http://arxiv.org/abs/2106.01444"/>
        <updated>2021-06-04T01:12:30.120Z</updated>
        <summary type="html"><![CDATA[The open-ended nature of visual captioning makes it a challenging area for
evaluation. The majority of proposed models rely on specialized training to
improve human-correlation, resulting in limited adoption, generalizability, and
explainabilty. We introduce "typicality", a new formulation of evaluation
rooted in information theory, which is uniquely suited for problems lacking a
definite ground truth. Typicality serves as our framework to develop a novel
semantic comparison, SPARCS, as well as referenceless fluency evaluation
metrics. Over the course of our analysis, two separate dimensions of fluency
naturally emerge: style, captured by metric SPURTS, and grammar, captured in
the form of grammatical outlier penalties. Through extensive experiments and
ablation studies on benchmark datasets, we show how these decomposed dimensions
of semantics and fluency provide greater system-level insight into captioner
differences. Our proposed metrics along with their combination, SMURF, achieve
state-of-the-art correlation with human judgment when compared with other
rule-based evaluation metrics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Feinglass_J/0/1/0/all/0/1"&gt;Joshua Feinglass&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yezhou Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Attention-based Contextual Language Model Adaptation for Speech Recognition. (arXiv:2106.01451v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01451</id>
        <link href="http://arxiv.org/abs/2106.01451"/>
        <updated>2021-06-04T01:12:30.097Z</updated>
        <summary type="html"><![CDATA[Language modeling (LM) for automatic speech recognition (ASR) does not
usually incorporate utterance level contextual information. For some domains
like voice assistants, however, additional context, such as the time at which
an utterance was spoken, provides a rich input signal. We introduce an
attention mechanism for training neural speech recognition language models on
both text and non-linguistic contextual data. When applied to a large
de-identified dataset of utterances collected by a popular voice assistant
platform, our method reduces perplexity by 7.0% relative over a standard LM
that does not incorporate contextual information. When evaluated on utterances
extracted from the long tail of the dataset, our method improves perplexity by
9.0% relative over a standard LM and by over 2.8% relative when compared to a
state-of-the-art model for contextual LM.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Martinez_R/0/1/0/all/0/1"&gt;Richard Diehl Martinez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Novotney_S/0/1/0/all/0/1"&gt;Scott Novotney&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bulyko_I/0/1/0/all/0/1"&gt;Ivan Bulyko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rastrow_A/0/1/0/all/0/1"&gt;Ariya Rastrow&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stolcke_A/0/1/0/all/0/1"&gt;Andreas Stolcke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gandhe_A/0/1/0/all/0/1"&gt;Ankur Gandhe&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Select: A Fully Attentive Approach for Novel Object Captioning. (arXiv:2106.01424v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.01424</id>
        <link href="http://arxiv.org/abs/2106.01424"/>
        <updated>2021-06-04T01:12:30.088Z</updated>
        <summary type="html"><![CDATA[Image captioning models have lately shown impressive results when applied to
standard datasets. Switching to real-life scenarios, however, constitutes a
challenge due to the larger variety of visual concepts which are not covered in
existing training sets. For this reason, novel object captioning (NOC) has
recently emerged as a paradigm to test captioning models on objects which are
unseen during the training phase. In this paper, we present a novel approach
for NOC that learns to select the most relevant objects of an image, regardless
of their adherence to the training set, and to constrain the generative process
of a language model accordingly. Our architecture is fully-attentive and
end-to-end trainable, also when incorporating constraints. We perform
experiments on the held-out COCO dataset, where we demonstrate improvements
over the state of the art, both in terms of adaptability to novel objects and
caption quality.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cagrandi_M/0/1/0/all/0/1"&gt;Marco Cagrandi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cornia_M/0/1/0/all/0/1"&gt;Marcella Cornia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stefanini_M/0/1/0/all/0/1"&gt;Matteo Stefanini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baraldi_L/0/1/0/all/0/1"&gt;Lorenzo Baraldi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cucchiara_R/0/1/0/all/0/1"&gt;Rita Cucchiara&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ethical-Advice Taker: Do Language Models Understand Natural Language Interventions?. (arXiv:2106.01465v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01465</id>
        <link href="http://arxiv.org/abs/2106.01465"/>
        <updated>2021-06-04T01:12:30.075Z</updated>
        <summary type="html"><![CDATA[Is it possible to use natural language to intervene in a model's behavior and
alter its prediction in a desired way? We investigate the effectiveness of
natural language interventions for reading-comprehension systems, studying this
in the context of social stereotypes. Specifically, we propose a new language
understanding task, Linguistic Ethical Interventions (LEI), where the goal is
to amend a question-answering (QA) model's unethical behavior by communicating
context-specific principles of ethics and equity to it. To this end, we build
upon recent methods for quantifying a system's social stereotypes, augmenting
them with different kinds of ethical interventions and the desired model
behavior under such interventions. Our zero-shot evaluation finds that even
today's powerful neural language models are extremely poor ethical-advice
takers, that is, they respond surprisingly little to ethical interventions even
though these interventions are stated as simple sentences. Few-shot learning
improves model behavior but remains far from the desired outcome, especially
when evaluated for various types of generalization. Our new task thus poses a
novel language understanding challenge for the community.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1"&gt;Jieyu Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khashabi_D/0/1/0/all/0/1"&gt;Daniel Khashabi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khot_T/0/1/0/all/0/1"&gt;Tushar Khot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sabharwal_A/0/1/0/all/0/1"&gt;Ashish Sabharwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1"&gt;Kai-Wei Chang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A remark on a paper of Krotov and Hopfield [arXiv:2008.06996]. (arXiv:2105.15034v2 [q-bio.NC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.15034</id>
        <link href="http://arxiv.org/abs/2105.15034"/>
        <updated>2021-06-04T01:12:30.066Z</updated>
        <summary type="html"><![CDATA[In their recent paper titled "Large Associative Memory Problem in
Neurobiology and Machine Learning" [arXiv:2008.06996] the authors gave a
biologically plausible microscopic theory from which one can recover many dense
associative memory models discussed in the literature. We show that the layers
of the recent "MLP-mixer" [arXiv:2105.01601] as well as the essentially
equivalent model in [arXiv:2105.02723] are amongst them.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Tang_F/0/1/0/all/0/1"&gt;Fei Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Kopp_M/0/1/0/all/0/1"&gt;Michael Kopp&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsharp Mask Guided Filtering. (arXiv:2106.01428v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.01428</id>
        <link href="http://arxiv.org/abs/2106.01428"/>
        <updated>2021-06-04T01:12:30.059Z</updated>
        <summary type="html"><![CDATA[The goal of this paper is guided image filtering, which emphasizes the
importance of structure transfer during filtering by means of an additional
guidance image. Where classical guided filters transfer structures using
hand-designed functions, recent guided filters have been considerably advanced
through parametric learning of deep networks. The state-of-the-art leverages
deep networks to estimate the two core coefficients of the guided filter. In
this work, we posit that simultaneously estimating both coefficients is
suboptimal, resulting in halo artifacts and structure inconsistencies. Inspired
by unsharp masking, a classical technique for edge enhancement that requires
only a single coefficient, we propose a new and simplified formulation of the
guided filter. Our formulation enjoys a filtering prior from a low-pass filter
and enables explicit structure transfer by estimating a single coefficient.
Based on our proposed formulation, we introduce a successive guided filtering
network, which provides multiple filtering results from a single network,
allowing for a trade-off between accuracy and efficiency. Extensive ablations,
comparisons and analysis show the effectiveness and efficiency of our
formulation and network, resulting in state-of-the-art results across filtering
tasks like upsampling, denoising, and cross-modality filtering. Code is
available at \url{https://github.com/shizenglin/Unsharp-Mask-Guided-Filtering}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1"&gt;Zenglin Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yunlu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gavves_E/0/1/0/all/0/1"&gt;Efstratios Gavves&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mettes_P/0/1/0/all/0/1"&gt;Pascal Mettes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Snoek_C/0/1/0/all/0/1"&gt;Cees G.M. Snoek&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NeRFactor: Neural Factorization of Shape and Reflectance Under an Unknown Illumination. (arXiv:2106.01970v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.01970</id>
        <link href="http://arxiv.org/abs/2106.01970"/>
        <updated>2021-06-04T01:12:30.038Z</updated>
        <summary type="html"><![CDATA[We address the problem of recovering the shape and spatially-varying
reflectance of an object from posed multi-view images of the object illuminated
by one unknown lighting condition. This enables the rendering of novel views of
the object under arbitrary environment lighting and editing of the object's
material properties. The key to our approach, which we call Neural Radiance
Factorization (NeRFactor), is to distill the volumetric geometry of a Neural
Radiance Field (NeRF) [Mildenhall et al. 2020] representation of the object
into a surface representation and then jointly refine the geometry while
solving for the spatially-varying reflectance and the environment lighting.
Specifically, NeRFactor recovers 3D neural fields of surface normals, light
visibility, albedo, and Bidirectional Reflectance Distribution Functions
(BRDFs) without any supervision, using only a re-rendering loss, simple
smoothness priors, and a data-driven BRDF prior learned from real-world BRDF
measurements. By explicitly modeling light visibility, NeRFactor is able to
separate shadows from albedo and synthesize realistic soft or hard shadows
under arbitrary lighting conditions. NeRFactor is able to recover convincing 3D
models for free-viewpoint relighting in this challenging and underconstrained
capture setup for both synthetic and real scenes. Qualitative and quantitative
experiments show that NeRFactor outperforms classic and deep learning-based
state of the art across various tasks. Our code and data are available at
people.csail.mit.edu/xiuming/projects/nerfactor/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiuming Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srinivasan_P/0/1/0/all/0/1"&gt;Pratul P. Srinivasan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_B/0/1/0/all/0/1"&gt;Boyang Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Debevec_P/0/1/0/all/0/1"&gt;Paul Debevec&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Freeman_W/0/1/0/all/0/1"&gt;William T. Freeman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barron_J/0/1/0/all/0/1"&gt;Jonathan T. Barron&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lightweight Adapter Tuning for Multilingual Speech Translation. (arXiv:2106.01463v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01463</id>
        <link href="http://arxiv.org/abs/2106.01463"/>
        <updated>2021-06-04T01:12:30.030Z</updated>
        <summary type="html"><![CDATA[Adapter modules were recently introduced as an efficient alternative to
fine-tuning in NLP. Adapter tuning consists in freezing pretrained parameters
of a model and injecting lightweight modules between layers, resulting in the
addition of only a small number of task-specific trainable parameters. While
adapter tuning was investigated for multilingual neural machine translation,
this paper proposes a comprehensive analysis of adapters for multilingual
speech translation (ST). Starting from different pre-trained models (a
multilingual ST trained on parallel data or a multilingual BART (mBART) trained
on non-parallel multilingual data), we show that adapters can be used to: (a)
efficiently specialize ST to specific language pairs with a low extra cost in
terms of parameters, and (b) transfer from an automatic speech recognition
(ASR) task and an mBART pre-trained model to a multilingual ST task.
Experiments show that adapter tuning offer competitive results to full
fine-tuning, while being much more parameter-efficient.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Le_H/0/1/0/all/0/1"&gt;Hang Le&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pino_J/0/1/0/all/0/1"&gt;Juan Pino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Changhan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1"&gt;Jiatao Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schwab_D/0/1/0/all/0/1"&gt;Didier Schwab&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Besacier_L/0/1/0/all/0/1"&gt;Laurent Besacier&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Controllable Person Image Synthesis with Spatially-Adaptive Warped Normalization. (arXiv:2105.14739v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14739</id>
        <link href="http://arxiv.org/abs/2105.14739"/>
        <updated>2021-06-04T01:12:30.023Z</updated>
        <summary type="html"><![CDATA[Controllable person image generation aims to produce realistic human images
with desirable attributes (e.g., the given pose, cloth textures or hair style).
However, the large spatial misalignment between the source and target images
makes the standard architectures for image-to-image translation not suitable
for this task. Most of the state-of-the-art architectures avoid the alignment
step during the generation, which causes many artifacts, especially for person
images with complex textures. To solve this problem, we introduce a novel
Spatially-Adaptive Warped Normalization (SAWN), which integrates a learned
flow-field to warp modulation parameters. This allows us to align person
spatial-adaptive styles with pose features efficiently. Moreover, we propose a
novel self-training part replacement strategy to refine the pretrained model
for the texture-transfer task, significantly improving the quality of the
generated cloth and the preservation ability of irrelevant regions. Our
experimental results on the widely used DeepFashion dataset demonstrate a
significant improvement of the proposed method over the state-of-the-art
methods on both pose-transfer and texture-transfer tasks. The source code is
available at https://github.com/zhangqianhui/Sawn.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jichao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Siarohin_A/0/1/0/all/0/1"&gt;Aliaksandr Siarohin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1"&gt;Hao Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jingjing Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sangineto_E/0/1/0/all/0/1"&gt;Enver Sangineto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sebe_N/0/1/0/all/0/1"&gt;Nicu Sebe&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Global Wheat Head Dataset 2021: more diversity to improve the benchmarking of wheat head localization methods. (arXiv:2105.07660v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.07660</id>
        <link href="http://arxiv.org/abs/2105.07660"/>
        <updated>2021-06-04T01:12:30.017Z</updated>
        <summary type="html"><![CDATA[The Global Wheat Head Detection (GWHD) dataset was created in 2020 and has
assembled 193,634 labelled wheat heads from 4,700 RGB images acquired from
various acquisition platforms and 7 countries/institutions. With an associated
competition hosted in Kaggle, GWHD has successfully attracted attention from
both the computer vision and agricultural science communities. From this first
experience in 2020, a few avenues for improvements have been identified,
especially from the perspective of data size, head diversity and label
reliability. To address these issues, the 2020 dataset has been reexamined,
relabeled, and augmented by adding 1,722 images from 5 additional countries,
allowing for 81,553 additional wheat heads to be added. We now release a new
version of the Global Wheat Head Detection (GWHD) dataset in 2021, which is
bigger, more diverse, and less noisy than the 2020 version. The GWHD 2021 is
now publicly available at this http URL and a new data challenge
has been organized on AIcrowd to make use of this updated dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+David_E/0/1/0/all/0/1"&gt;Etienne David&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Serouart_M/0/1/0/all/0/1"&gt;Mario Serouart&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smith_D/0/1/0/all/0/1"&gt;Daniel Smith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Madec_S/0/1/0/all/0/1"&gt;Simon Madec&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Velumani_K/0/1/0/all/0/1"&gt;Kaaviya Velumani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Shouyang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Espinosa_F/0/1/0/all/0/1"&gt;Francisco Pinto Espinosa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shafiee_S/0/1/0/all/0/1"&gt;Shahameh Shafiee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tahir_I/0/1/0/all/0/1"&gt;Izzat S. A. Tahir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsujimoto_H/0/1/0/all/0/1"&gt;Hisashi Tsujimoto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nasuda_S/0/1/0/all/0/1"&gt;Shuhei Nasuda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_B/0/1/0/all/0/1"&gt;Bangyou Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kichgessner_N/0/1/0/all/0/1"&gt;Norbert Kichgessner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aasen_H/0/1/0/all/0/1"&gt;Helge Aasen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hund_A/0/1/0/all/0/1"&gt;Andreas Hund&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sadhegi_Tehran_P/0/1/0/all/0/1"&gt;Pouria Sadhegi-Tehran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nagasawa_K/0/1/0/all/0/1"&gt;Koichi Nagasawa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ishikawa_G/0/1/0/all/0/1"&gt;Goro Ishikawa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dandrifosse_S/0/1/0/all/0/1"&gt;S&amp;#xe9;bastien Dandrifosse&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carlier_A/0/1/0/all/0/1"&gt;Alexis Carlier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mercatoris_B/0/1/0/all/0/1"&gt;Benoit Mercatoris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kuroki_K/0/1/0/all/0/1"&gt;Ken Kuroki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Haozhou Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ishii_M/0/1/0/all/0/1"&gt;Masanori Ishii&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Badhon_M/0/1/0/all/0/1"&gt;Minhajul A. Badhon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pozniak_C/0/1/0/all/0/1"&gt;Curtis Pozniak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+LeBauer_D/0/1/0/all/0/1"&gt;David Shaner LeBauer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lilimo_M/0/1/0/all/0/1"&gt;Morten Lilimo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Poland_J/0/1/0/all/0/1"&gt;Jesse Poland&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chapman_S/0/1/0/all/0/1"&gt;Scott Chapman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Solan_B/0/1/0/all/0/1"&gt;Benoit de Solan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baret_F/0/1/0/all/0/1"&gt;Fr&amp;#xe9;d&amp;#xe9;ric Baret&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stavness_I/0/1/0/all/0/1"&gt;Ian Stavness&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1"&gt;Wei Guo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semantic Palette: Guiding Scene Generation with Class Proportions. (arXiv:2106.01629v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.01629</id>
        <link href="http://arxiv.org/abs/2106.01629"/>
        <updated>2021-06-04T01:12:30.009Z</updated>
        <summary type="html"><![CDATA[Despite the recent progress of generative adversarial networks (GANs) at
synthesizing photo-realistic images, producing complex urban scenes remains a
challenging problem. Previous works break down scene generation into two
consecutive phases: unconditional semantic layout synthesis and image synthesis
conditioned on layouts. In this work, we propose to condition layout generation
as well for higher semantic control: given a vector of class proportions, we
generate layouts with matching composition. To this end, we introduce a
conditional framework with novel architecture designs and learning objectives,
which effectively accommodates class proportions to guide the scene generation
process. The proposed architecture also allows partial layout editing with
interesting applications. Thanks to the semantic control, we can produce
layouts close to the real distribution, helping enhance the whole scene
generation process. On different metrics and urban scene benchmarks, our models
outperform existing baselines. Moreover, we demonstrate the merit of our
approach for data augmentation: semantic segmenters trained on real
layout-image pairs along with additional ones generated by our approach
outperform models only trained on real pairs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Moing_G/0/1/0/all/0/1"&gt;Guillaume Le Moing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vu_T/0/1/0/all/0/1"&gt;Tuan-Hung Vu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jain_H/0/1/0/all/0/1"&gt;Himalaya Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Perez_P/0/1/0/all/0/1"&gt;Patrick P&amp;#xe9;rez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cord_M/0/1/0/all/0/1"&gt;Matthieu Cord&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine Learning Based Texture Analysis of Patella from X-Rays for Detecting Patellofemoral Osteoarthritis. (arXiv:2106.01700v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.01700</id>
        <link href="http://arxiv.org/abs/2106.01700"/>
        <updated>2021-06-04T01:12:29.989Z</updated>
        <summary type="html"><![CDATA[Objective is to assess the ability of texture features for detecting
radiographic patellofemoral osteoarthritis (PFOA) from knee lateral view
radiographs. We used lateral view knee radiographs from MOST public use
datasets (n = 5507 knees). Patellar region-of-interest (ROI) was automatically
detected using landmark detection tool (BoneFinder). Hand-crafted features,
based on LocalBinary Patterns (LBP), were then extracted to describe the
patellar texture. First, a machine learning model (Gradient Boosting Machine)
was trained to detect radiographic PFOA from the LBP features. Furthermore, we
used end-to-end trained deep convolutional neural networks (CNNs) directly on
the texture patches for detecting the PFOA. The proposed classification models
were eventually compared with more conventional reference models that use
clinical assessments and participant characteristics such as age, sex, body
mass index(BMI), the total WOMAC score, and tibiofemoral Kellgren-Lawrence (KL)
grade. Atlas-guided visual assessment of PFOA status by expert readers provided
in the MOST public use datasets was used as a classification outcome for the
models. Performance of prediction models was assessed using the area under the
receiver operating characteristic curve (ROC AUC), the area under the
precision-recall (PR) curve-average precision (AP)-, and Brier score in the
stratified 5-fold cross validation setting.Of the 5507 knees, 953 (17.3%) had
PFOA. AUC and AP for the strongest reference model including age, sex, BMI,
WOMAC score, and tibiofemoral KL grade to predict PFOA were 0.817 and 0.487,
respectively. Textural ROI classification using CNN significantly improved the
prediction performance (ROC AUC= 0.889, AP= 0.714). We present the first study
that analyses patellar bone texture for diagnosing PFOA. Our results
demonstrates the potential of using texture features of patella to predict
PFOA.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Bayramoglu_N/0/1/0/all/0/1"&gt;Neslihan Bayramoglu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Nieminen_M/0/1/0/all/0/1"&gt;Miika T. Nieminen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Saarakkala_S/0/1/0/all/0/1"&gt;Simo Saarakkala&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. (arXiv:2010.11929v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.11929</id>
        <link href="http://arxiv.org/abs/2010.11929"/>
        <updated>2021-06-04T01:12:29.982Z</updated>
        <summary type="html"><![CDATA[While the Transformer architecture has become the de-facto standard for
natural language processing tasks, its applications to computer vision remain
limited. In vision, attention is either applied in conjunction with
convolutional networks, or used to replace certain components of convolutional
networks while keeping their overall structure in place. We show that this
reliance on CNNs is not necessary and a pure transformer applied directly to
sequences of image patches can perform very well on image classification tasks.
When pre-trained on large amounts of data and transferred to multiple mid-sized
or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision
Transformer (ViT) attains excellent results compared to state-of-the-art
convolutional networks while requiring substantially fewer computational
resources to train.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dosovitskiy_A/0/1/0/all/0/1"&gt;Alexey Dosovitskiy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Beyer_L/0/1/0/all/0/1"&gt;Lucas Beyer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kolesnikov_A/0/1/0/all/0/1"&gt;Alexander Kolesnikov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weissenborn_D/0/1/0/all/0/1"&gt;Dirk Weissenborn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhai_X/0/1/0/all/0/1"&gt;Xiaohua Zhai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Unterthiner_T/0/1/0/all/0/1"&gt;Thomas Unterthiner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dehghani_M/0/1/0/all/0/1"&gt;Mostafa Dehghani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Minderer_M/0/1/0/all/0/1"&gt;Matthias Minderer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heigold_G/0/1/0/all/0/1"&gt;Georg Heigold&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gelly_S/0/1/0/all/0/1"&gt;Sylvain Gelly&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Uszkoreit_J/0/1/0/all/0/1"&gt;Jakob Uszkoreit&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Houlsby_N/0/1/0/all/0/1"&gt;Neil Houlsby&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Preliminary Study of a Two-Stage Paradigm for Preserving Speaker Identity in Dysarthric Voice Conversion. (arXiv:2106.01415v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2106.01415</id>
        <link href="http://arxiv.org/abs/2106.01415"/>
        <updated>2021-06-04T01:12:29.976Z</updated>
        <summary type="html"><![CDATA[We propose a new paradigm for maintaining speaker identity in dysarthric
voice conversion (DVC). The poor quality of dysarthric speech can be greatly
improved by statistical VC, but as the normal speech utterances of a dysarthria
patient are nearly impossible to collect, previous work failed to recover the
individuality of the patient. In light of this, we suggest a novel, two-stage
approach for DVC, which is highly flexible in that no normal speech of the
patient is required. First, a powerful parallel sequence-to-sequence model
converts the input dysarthric speech into a normal speech of a reference
speaker as an intermediate product, and a nonparallel, frame-wise VC model
realized with a variational autoencoder then converts the speaker identity of
the reference speech back to that of the patient while assumed to be capable of
preserving the enhanced quality. We investigate several design options.
Experimental evaluation results demonstrate the potential of our approach to
improving the quality of the dysarthric speech while maintaining the speaker
identity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1"&gt;Wen-Chin Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kobayashi_K/0/1/0/all/0/1"&gt;Kazuhiro Kobayashi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1"&gt;Yu-Huai Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1"&gt;Ching-Feng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsao_Y/0/1/0/all/0/1"&gt;Yu Tsao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hsin-Min Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Toda_T/0/1/0/all/0/1"&gt;Tomoki Toda&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robotic Inspection and 3D GPR-based Reconstruction for Underground Utilities. (arXiv:2106.01907v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.01907</id>
        <link href="http://arxiv.org/abs/2106.01907"/>
        <updated>2021-06-04T01:12:29.969Z</updated>
        <summary type="html"><![CDATA[Ground Penetrating Radar (GPR) is an effective non-destructive evaluation
(NDE) device for inspecting and surveying subsurface objects (i.e., rebars,
utility pipes) in complex environments. However, the current practice for GPR
data collection requires a human inspector to move a GPR cart along pre-marked
grid lines and record the GPR data in both X and Y directions for
post-processing by 3D GPR imaging software. It is time-consuming and tedious
work to survey a large area. Furthermore, identifying the subsurface targets
depends on the knowledge of an experienced engineer, who has to make manual and
subjective interpretation that limits the GPR applications, especially in
large-scale scenarios. In addition, the current GPR imaging technology is not
intuitive, and not for normal users to understand, and not friendly to
visualize. To address the above challenges, this paper presents a novel robotic
system to collect GPR data, interpret GPR data, localize the underground
utilities, reconstruct and visualize the underground objects' dense point cloud
model in a user-friendly manner. This system is composed of three modules: 1) a
vision-aided Omni-directional robotic data collection platform, which enables
the GPR antenna to scan the target area freely with an arbitrary trajectory
while using a visual-inertial-based positioning module tags the GPR
measurements with positioning information; 2) a deep neural network (DNN)
migration module to interpret the raw GPR B-scan image into a cross-section of
object model; 3) a DNN-based 3D reconstruction method, i.e., GPRNet, to
generate underground utility model represented as fine 3D point cloud.
Comparative studies on synthetic and field GPR raw data with various
incompleteness and noise are performed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Feng_J/0/1/0/all/0/1"&gt;Jinglun Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yang_L/0/1/0/all/0/1"&gt;Liang Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Biao_J/0/1/0/all/0/1"&gt;Jiang Biao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xiao_J/0/1/0/all/0/1"&gt;Jizhong Xiao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Attention-Guided Supervised Contrastive Learning for Semantic Segmentation. (arXiv:2106.01596v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.01596</id>
        <link href="http://arxiv.org/abs/2106.01596"/>
        <updated>2021-06-04T01:12:29.963Z</updated>
        <summary type="html"><![CDATA[Contrastive learning has shown superior performance in embedding global and
spatial invariant features in computer vision (e.g., image classification).
However, its overall success of embedding local and spatial variant features is
still limited, especially for semantic segmentation. In a per-pixel prediction
task, more than one label can exist in a single image for segmentation (e.g.,
an image contains both cat, dog, and grass), thereby it is difficult to define
'positive' or 'negative' pairs in a canonical contrastive learning setting. In
this paper, we propose an attention-guided supervised contrastive learning
approach to highlight a single semantic object every time as the target. With
our design, the same image can be embedded to different semantic clusters with
semantic attention (i.e., coerce semantic masks) as an additional input
channel. To achieve such attention, a novel two-stage training strategy is
presented. We evaluate the proposed method on multi-organ medical image
segmentation task, as our major task, with both in-house data and BTCV 2015
datasets. Comparing with the supervised and semi-supervised training
state-of-the-art in the backbone of ResNet-50, our proposed pipeline yields
substantial improvement of 5.53% and 6.09% in Dice score for both medical image
segmentation cohorts respectively. The performance of the proposed method on
natural images is assessed via PASCAL VOC 2012 dataset, and achieves 2.75%
substantial improvement.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1"&gt;Ho Hin Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1"&gt;Yucheng Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1"&gt;Qi Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1"&gt;Xin Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bao_S/0/1/0/all/0/1"&gt;Shunxing Bao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Landman_B/0/1/0/all/0/1"&gt;Bennett A. Landman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huo_Y/0/1/0/all/0/1"&gt;Yuankai Huo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Comparison for Anti-noise Robustness of Deep Learning Classification Methods on a Tiny Object Image Dataset: from Convolutional Neural Network to Visual Transformer and Performer. (arXiv:2106.01927v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.01927</id>
        <link href="http://arxiv.org/abs/2106.01927"/>
        <updated>2021-06-04T01:12:29.945Z</updated>
        <summary type="html"><![CDATA[Image classification has achieved unprecedented advance with the the rapid
development of deep learning. However, the classification of tiny object images
is still not well investigated. In this paper, we first briefly review the
development of Convolutional Neural Network and Visual Transformer in deep
learning, and introduce the sources and development of conventional noises and
adversarial attacks. Then we use various models of Convolutional Neural Network
and Visual Transformer to conduct a series of experiments on the image dataset
of tiny objects (sperms and impurities), and compare various evaluation metrics
in the experimental results to obtain a model with stable performance. Finally,
we discuss the problems in the classification of tiny objects and make a
prospect for the classification of tiny objects in the future.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1"&gt;Ao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Haoyuan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Hechen Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1"&gt;Peng Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1"&gt;Weiming Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wanli Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zou_S/0/1/0/all/0/1"&gt;Shuojia Zou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grzegorzek_M/0/1/0/all/0/1"&gt;Marcin Grzegorzek&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Can vectors read minds better than experts? Comparing data augmentation strategies for the automated scoring of children's mindreading ability. (arXiv:2106.01635v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01635</id>
        <link href="http://arxiv.org/abs/2106.01635"/>
        <updated>2021-06-04T01:12:29.939Z</updated>
        <summary type="html"><![CDATA[In this paper we implement and compare 7 different data augmentation
strategies for the task of automatic scoring of children's ability to
understand others' thoughts, feelings, and desires (or "mindreading").

We recruit in-domain experts to re-annotate augmented samples and determine
to what extent each strategy preserves the original rating. We also carry out
multiple experiments to measure how much each augmentation strategy improves
the performance of automatic scoring systems. To determine the capabilities of
automatic systems to generalize to unseen data, we create UK-MIND-20 - a new
corpus of children's performance on tests of mindreading, consisting of 10,320
question-answer pairs.

We obtain a new state-of-the-art performance on the MIND-CA corpus, improving
macro-F1-score by 6 points. Results indicate that both the number of training
examples and the quality of the augmentation strategies affect the performance
of the systems. The task-specific augmentations generally outperform
task-agnostic augmentations. Automatic augmentations based on vectors (GloVe,
FastText) perform the worst.

We find that systems trained on MIND-CA generalize well to UK-MIND-20. We
demonstrate that data augmentation strategies also improve the performance on
unseen data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kovatchev_V/0/1/0/all/0/1"&gt;Venelin Kovatchev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smith_P/0/1/0/all/0/1"&gt;Phillip Smith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1"&gt;Mark Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Devine_R/0/1/0/all/0/1"&gt;Rory Devine&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TVDIM: Enhancing Image Self-Supervised Pretraining via Noisy Text Data. (arXiv:2106.01797v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01797</id>
        <link href="http://arxiv.org/abs/2106.01797"/>
        <updated>2021-06-04T01:12:29.933Z</updated>
        <summary type="html"><![CDATA[Among ubiquitous multimodal data in the real world, text is the modality
generated by human, while image reflects the physical world honestly. In a
visual understanding application, machines are expected to understand images
like human. Inspired by this, we propose a novel self-supervised learning
method, named Text-enhanced Visual Deep InfoMax (TVDIM), to learn better visual
representations by fully utilizing the naturally-existing multimodal data. Our
core idea of self-supervised learning is to maximize the mutual information
between features extracted from multiple views of a shared context to a
rational degree. Different from previous methods which only consider multiple
views from a single modality, our work produces multiple views from different
modalities, and jointly optimizes the mutual information for features pairs of
intra-modality and inter-modality. Considering the information gap between
inter-modality features pairs from data noise, we adopt a \emph{ranking-based}
contrastive learning to optimize the mutual information. During evaluation, we
directly use the pre-trained visual representations to complete various image
classification tasks. Experimental results show that, TVDIM significantly
outperforms previous visual self-supervised methods when processing the same
set of images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qin_P/0/1/0/all/0/1"&gt;Pengda Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yuhong Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Dataset and Baselines for Multilingual Reply Suggestion. (arXiv:2106.02017v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02017</id>
        <link href="http://arxiv.org/abs/2106.02017"/>
        <updated>2021-06-04T01:12:29.927Z</updated>
        <summary type="html"><![CDATA[Reply suggestion models help users process emails and chats faster. Previous
work only studies English reply suggestion. Instead, we present MRS, a
multilingual reply suggestion dataset with ten languages. MRS can be used to
compare two families of models: 1) retrieval models that select the reply from
a fixed set and 2) generation models that produce the reply from scratch.
Therefore, MRS complements existing cross-lingual generalization benchmarks
that focus on classification and sequence labeling tasks. We build a generation
model and a retrieval model as baselines for MRS. The two models have different
strengths in the monolingual setting, and they require different strategies to
generalize across languages. MRS is publicly available at
https://github.com/zhangmozhi/mrs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Mozhi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deb_B/0/1/0/all/0/1"&gt;Budhaditya Deb&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_G/0/1/0/all/0/1"&gt;Guoqing Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shokouhi_M/0/1/0/all/0/1"&gt;Milad Shokouhi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Awadallah_A/0/1/0/all/0/1"&gt;Ahmed Hassan Awadallah&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Simultaneous Corn and Soybean Yield Prediction from Remote Sensing Data Using Deep Transfer Learning. (arXiv:2012.03129v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.03129</id>
        <link href="http://arxiv.org/abs/2012.03129"/>
        <updated>2021-06-04T01:12:29.920Z</updated>
        <summary type="html"><![CDATA[Large-scale crop yield estimation is, in part, made possible due to the
availability of remote sensing data allowing for the continuous monitoring of
crops throughout their growth cycle. Having this information allows
stakeholders the ability to make real-time decisions to maximize yield
potential. Although various models exist that predict yield from remote sensing
data, there currently does not exist an approach that can estimate yield for
multiple crops simultaneously, and thus leads to more accurate predictions. A
model that predicts the yield of multiple crops and concurrently considers the
interaction between multiple crop yields. We propose a new convolutional neural
network model called YieldNet which utilizes a novel deep learning framework
that uses transfer learning between corn and soybean yield predictions by
sharing the weights of the backbone feature extractor. Additionally, to
consider the multi-target response variable, we propose a new loss function. We
conduct our experiment using data from 1,132 counties for corn and 1,076
counties for soybean across the United States. Numerical results demonstrate
that our proposed method accurately predicts corn and soybean yield from one to
four months before the harvest with a MAE being 8.74% and 8.70% of the average
yield, respectively, and is competitive to other state-of-the-art approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khaki_S/0/1/0/all/0/1"&gt;Saeed Khaki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pham_H/0/1/0/all/0/1"&gt;Hieu Pham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Lizhi Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Can Generative Pre-trained Language Models Serve as Knowledge Bases for Closed-book QA?. (arXiv:2106.01561v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01561</id>
        <link href="http://arxiv.org/abs/2106.01561"/>
        <updated>2021-06-04T01:12:29.914Z</updated>
        <summary type="html"><![CDATA[Recent work has investigated the interesting question using pre-trained
language models (PLMs) as knowledge bases for answering open questions.
However, existing work is limited in using small benchmarks with high
test-train overlaps. We construct a new dataset of closed-book QA using SQuAD,
and investigate the performance of BART. Experiments show that it is
challenging for BART to remember training facts in high precision, and also
challenging to answer closed-book questions even if relevant knowledge is
retained. Some promising directions are found, including decoupling the
knowledge memorizing process and the QA finetune process, forcing the model to
recall relevant knowledge when question answering.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Cunxiang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1"&gt;Pai Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yue Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Spoken Term Discovery Based on Re-clustering of Hypothesized Speech Segments with Siamese and Triplet Networks. (arXiv:2011.14062v2 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.14062</id>
        <link href="http://arxiv.org/abs/2011.14062"/>
        <updated>2021-06-04T01:12:29.897Z</updated>
        <summary type="html"><![CDATA[Spoken term discovery from untranscribed speech audio could be achieved via a
two-stage process. In the first stage, the unlabelled speech is decoded into a
sequence of subword units that are learned and modelled in an unsupervised
manner. In the second stage, partial sequence matching and clustering are
performed on the decoded subword sequences, resulting in a set of discovered
words or phrases. A limitation of this approach is that the results of subword
decoding could be erroneous, and the errors would impact the subsequent steps.
While Siamese/Triplet network is one approach to learn segment representations
that can improve the discovery process, the challenge in spoken term discovery
under a complete unsupervised scenario is that training examples are
unavailable. In this paper, we propose to generate training examples from
initial hypothesized sequence clusters. The Siamese/Triplet network is trained
on the hypothesized examples to measure the similarity between two speech
segments and hereby perform re-clustering of all hypothesized subword sequences
to achieve spoken term discovery. Experimental results show that the proposed
approach is effective in obtaining training examples for Siamese and Triplet
networks, improving the efficacy of spoken term discovery as compared with the
original two-stage method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Sung_M/0/1/0/all/0/1"&gt;Man-Ling Sung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lee_T/0/1/0/all/0/1"&gt;Tan Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[APES: Audiovisual Person Search in Untrimmed Video. (arXiv:2106.01667v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.01667</id>
        <link href="http://arxiv.org/abs/2106.01667"/>
        <updated>2021-06-04T01:12:29.891Z</updated>
        <summary type="html"><![CDATA[Humans are arguably one of the most important subjects in video streams, many
real-world applications such as video summarization or video editing workflows
often require the automatic search and retrieval of a person of interest.
Despite tremendous efforts in the person reidentification and retrieval
domains, few works have developed audiovisual search strategies. In this paper,
we present the Audiovisual Person Search dataset (APES), a new dataset composed
of untrimmed videos whose audio (voices) and visual (faces) streams are densely
annotated. APES contains over 1.9K identities labeled along 36 hours of video,
making it the largest dataset available for untrimmed audiovisual person
search. A key property of APES is that it includes dense temporal annotations
that link faces to speech segments of the same identity. To showcase the
potential of our new dataset, we propose an audiovisual baseline and benchmark
for person retrieval. Our study shows that modeling audiovisual cues benefits
the recognition of people's identities. To enable reproducibility and promote
future research, the dataset annotations and baseline code are available at:
https://github.com/fuankarion/audiovisual-person-search]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alcazar_J/0/1/0/all/0/1"&gt;Juan Leon Alcazar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mai_L/0/1/0/all/0/1"&gt;Long Mai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Perazzi_F/0/1/0/all/0/1"&gt;Federico Perazzi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Joon-Young Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arbelaez_P/0/1/0/all/0/1"&gt;Pablo Arbelaez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1"&gt;Bernard Ghanem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heilbron_F/0/1/0/all/0/1"&gt;Fabian Caba Heilbron&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Auto-tagging of Short Conversational Sentences using Transformer Methods. (arXiv:2106.01735v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01735</id>
        <link href="http://arxiv.org/abs/2106.01735"/>
        <updated>2021-06-04T01:12:29.885Z</updated>
        <summary type="html"><![CDATA[The problem of categorizing short speech sentences according to their
semantic features with high accuracy is a subject studied in natural language
processing. In this study, a data set created with samples classified in 46
different categories was used. Examples consist of sentences taken from chat
conversations between a company's customer representatives and the company's
website visitors. The primary purpose is to automatically tag questions and
requests from visitors in the most accurate way for 46 predetermined categories
for use in a chat application to generate meaningful answers to the questions
asked by the website visitors. For this, different BERT models and one GPT-2
model, pre-trained in Turkish, were preferred. The classification performances
of the relevant models were analyzed in detail and reported accordingly.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tasar_D/0/1/0/all/0/1"&gt;D. Emre Ta&amp;#x15f;ar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ozan_S/0/1/0/all/0/1"&gt;&amp;#x15e;&amp;#xfc;kr&amp;#xfc; Ozan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ozdil_U/0/1/0/all/0/1"&gt;Umut &amp;#xd6;zdil&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Akca_M/0/1/0/all/0/1"&gt;M. Fatih Akca&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Olmez_O/0/1/0/all/0/1"&gt;O&amp;#x11f;uzhan &amp;#xd6;lmez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gulum_S/0/1/0/all/0/1"&gt;Semih G&amp;#xfc;l&amp;#xfc;m&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kutal_S/0/1/0/all/0/1"&gt;Se&amp;#xe7;ilay Kutal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Belhan_C/0/1/0/all/0/1"&gt;Ceren Belhan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Supervised Learning of Event-Based Optical Flow with Spiking Neural Networks. (arXiv:2106.01862v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.01862</id>
        <link href="http://arxiv.org/abs/2106.01862"/>
        <updated>2021-06-04T01:12:29.877Z</updated>
        <summary type="html"><![CDATA[Neuromorphic sensing and computing hold a promise for highly energy-efficient
and high-bandwidth-sensor processing. A major challenge for neuromorphic
computing is that learning algorithms for traditional artificial neural
networks (ANNs) do not transfer directly to spiking neural networks (SNNs) due
to the discrete spikes and more complex neuronal dynamics. As a consequence,
SNNs have not yet been successfully applied to complex, large-scale tasks. In
this article, we focus on the self-supervised learning problem of optical flow
estimation from event-based camera inputs, and investigate the changes that are
necessary to the state-of-the-art ANN training pipeline in order to
successfully tackle it with SNNs. More specifically, we first modify the input
event representation to encode a much smaller time slice with minimal explicit
temporal information. Consequently, we make the network's neuronal dynamics and
recurrent connections responsible for integrating information over time.
Moreover, we reformulate the self-supervised loss function for event-based
optical flow to improve its convexity. We perform experiments with various
types of recurrent ANNs and SNNs using the proposed pipeline. Concerning SNNs,
we investigate the effects of elements such as parameter initialization and
optimization, surrogate gradient shape, and adaptive neuronal mechanisms. We
find that initialization and surrogate gradient width play a crucial part in
enabling learning with sparse inputs, while the inclusion of adaptivity and
learnable neuronal parameters can improve performance. We show that the
performance of the proposed ANNs and SNNs are on par with that of the current
state-of-the-art ANNs trained in a self-supervised manner.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Paredes_Valles_F/0/1/0/all/0/1"&gt;Federico Paredes-Vall&amp;#xe9;s&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hagenaars_J/0/1/0/all/0/1"&gt;Jesse Hagenaars&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Croon_G/0/1/0/all/0/1"&gt;Guido de Croon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Attacking Text Classifiers via Sentence Rewriting Sampler. (arXiv:2104.08453v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.08453</id>
        <link href="http://arxiv.org/abs/2104.08453"/>
        <updated>2021-06-04T01:12:29.860Z</updated>
        <summary type="html"><![CDATA[Most adversarial attack methods on text classification can change the
classifier's prediction by synonym substitution. We propose the adversarial
sentence rewriting sampler (ASRS), which rewrites the whole sentence to
generate more similar and higher-quality adversarial examples. Our method
achieves a better attack success rate on 4 out of 7 datasets, as well as
significantly better sentence quality on all 7 datasets. ASRS is an
indispensable supplement to the existing attack methods, because classifiers
cannot resist the attack from ASRS unless they are trained on adversarial
examples found by ASRS.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1"&gt;Lei Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Veeramachaneni_K/0/1/0/all/0/1"&gt;Kalyan Veeramachaneni&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Domain Adaptation for Facial Expression Classifier via Domain Discrimination and Gradient Reversal. (arXiv:2106.01467v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.01467</id>
        <link href="http://arxiv.org/abs/2106.01467"/>
        <updated>2021-06-04T01:12:29.853Z</updated>
        <summary type="html"><![CDATA[Bringing empathy to a computerized system could significantly improve the
quality of human-computer communications, as soon as machines would be able to
understand customer intentions and better serve their needs. According to
different studies (Literature Review), visual information is one of the most
important channels of human interaction and contains significant behavioral
signals, that may be captured from facial expressions. Therefore, it is
consistent and natural that the research in the field of Facial Expression
Recognition (FER) has acquired increased interest over the past decade due to
having diverse application area including health-care, sociology, psychology,
driver-safety, virtual reality, cognitive sciences, security, entertainment,
marketing, etc. We propose a new architecture for the task of FER and examine
the impact of domain discrimination loss regularization on the learning
process. With regard to observations, including both classical training
conditions and unsupervised domain adaptation scenarios, important aspects of
the considered domain adaptation approach integration are traced. The results
may serve as a foundation for further research in the field.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Akhmetov_K/0/1/0/all/0/1"&gt;Kamil Akhmetov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SMURF: SeMantic and linguistic UndeRstanding Fusion for Caption Evaluation via Typicality Analysis. (arXiv:2106.01444v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01444</id>
        <link href="http://arxiv.org/abs/2106.01444"/>
        <updated>2021-06-04T01:12:29.846Z</updated>
        <summary type="html"><![CDATA[The open-ended nature of visual captioning makes it a challenging area for
evaluation. The majority of proposed models rely on specialized training to
improve human-correlation, resulting in limited adoption, generalizability, and
explainabilty. We introduce "typicality", a new formulation of evaluation
rooted in information theory, which is uniquely suited for problems lacking a
definite ground truth. Typicality serves as our framework to develop a novel
semantic comparison, SPARCS, as well as referenceless fluency evaluation
metrics. Over the course of our analysis, two separate dimensions of fluency
naturally emerge: style, captured by metric SPURTS, and grammar, captured in
the form of grammatical outlier penalties. Through extensive experiments and
ablation studies on benchmark datasets, we show how these decomposed dimensions
of semantics and fluency provide greater system-level insight into captioner
differences. Our proposed metrics along with their combination, SMURF, achieve
state-of-the-art correlation with human judgment when compared with other
rule-based evaluation metrics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Feinglass_J/0/1/0/all/0/1"&gt;Joshua Feinglass&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yezhou Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Improved Model for Voicing Silent Speech. (arXiv:2106.01933v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2106.01933</id>
        <link href="http://arxiv.org/abs/2106.01933"/>
        <updated>2021-06-04T01:12:29.832Z</updated>
        <summary type="html"><![CDATA[In this paper, we present an improved model for voicing silent speech, where
audio is synthesized from facial electromyography (EMG) signals. To give our
model greater flexibility to learn its own input features, we directly use EMG
signals as input in the place of hand-designed features used by prior work. Our
model uses convolutional layers to extract features from the signals and
Transformer layers to propagate information across longer distances. To provide
better signal for learning, we also introduce an auxiliary task of predicting
phoneme labels in addition to predicting speech audio features. On an open
vocabulary intelligibility evaluation, our model improves the state of the art
for this task by an absolute 25.8%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Gaddy_D/0/1/0/all/0/1"&gt;David Gaddy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Klein_D/0/1/0/all/0/1"&gt;Dan Klein&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BERT-Defense: A Probabilistic Model Based on BERT to Combat Cognitively Inspired Orthographic Adversarial Attacks. (arXiv:2106.01452v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01452</id>
        <link href="http://arxiv.org/abs/2106.01452"/>
        <updated>2021-06-04T01:12:29.826Z</updated>
        <summary type="html"><![CDATA[Adversarial attacks expose important blind spots of deep learning systems.
While word- and sentence-level attack scenarios mostly deal with finding
semantic paraphrases of the input that fool NLP models, character-level attacks
typically insert typos into the input stream. It is commonly thought that these
are easier to defend via spelling correction modules. In this work, we show
that both a standard spellchecker and the approach of Pruthi et al. (2019),
which trains to defend against insertions, deletions and swaps, perform poorly
on the character-level benchmark recently proposed in Eger and Benz (2020)
which includes more challenging attacks such as visual and phonetic
perturbations and missing word segmentations. In contrast, we show that an
untrained iterative approach which combines context-independent character-level
information with context-dependent information from BERT's masked language
modeling can perform on par with human crowd-workers from Amazon Mechanical
Turk (AMT) supervised via 3-shot learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Keller_Y/0/1/0/all/0/1"&gt;Yannik Keller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mackensen_J/0/1/0/all/0/1"&gt;Jan Mackensen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eger_S/0/1/0/all/0/1"&gt;Steffen Eger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Membership Inference Attacks on Deep Regression Models for Neuroimaging. (arXiv:2105.02866v2 [q-bio.QM] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.02866</id>
        <link href="http://arxiv.org/abs/2105.02866"/>
        <updated>2021-06-04T01:12:29.754Z</updated>
        <summary type="html"><![CDATA[Ensuring the privacy of research participants is vital, even more so in
healthcare environments. Deep learning approaches to neuroimaging require large
datasets, and this often necessitates sharing data between multiple sites,
which is antithetical to the privacy objectives. Federated learning is a
commonly proposed solution to this problem. It circumvents the need for data
sharing by sharing parameters during the training process. However, we
demonstrate that allowing access to parameters may leak private information
even if data is never directly shared. In particular, we show that it is
possible to infer if a sample was used to train the model given only access to
the model prediction (black-box) or access to the model itself (white-box) and
some leaked samples from the training data distribution. Such attacks are
commonly referred to as Membership Inference attacks. We show realistic
Membership Inference attacks on deep learning models trained for 3D
neuroimaging tasks in a centralized as well as decentralized setup. We
demonstrate feasible attacks on brain age prediction models (deep learning
models that predict a person's age from their brain MRI scan). We correctly
identified whether an MRI scan was used in model training with a 60% to over
80% success rate depending on model complexity and security assumptions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Gupta_U/0/1/0/all/0/1"&gt;Umang Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Stripelis_D/0/1/0/all/0/1"&gt;Dimitris Stripelis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Lam_P/0/1/0/all/0/1"&gt;Pradeep K. Lam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Thompson_P/0/1/0/all/0/1"&gt;Paul M. Thompson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Ambite_J/0/1/0/all/0/1"&gt;Jos&amp;#xe9; Luis Ambite&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Steeg_G/0/1/0/all/0/1"&gt;Greg Ver Steeg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CitationIE: Leveraging the Citation Graph for Scientific Information Extraction. (arXiv:2106.01560v1 [cs.DL])]]></title>
        <id>http://arxiv.org/abs/2106.01560</id>
        <link href="http://arxiv.org/abs/2106.01560"/>
        <updated>2021-06-04T01:12:29.748Z</updated>
        <summary type="html"><![CDATA[Automatically extracting key information from scientific documents has the
potential to help scientists work more efficiently and accelerate the pace of
scientific progress. Prior work has considered extracting document-level entity
clusters and relations end-to-end from raw scientific text, which can improve
literature search and help identify methods and materials for a given problem.
Despite the importance of this task, most existing works on scientific
information extraction (SciIE) consider extraction solely based on the content
of an individual paper, without considering the paper's place in the broader
literature. In contrast to prior work, we augment our text representations by
leveraging a complementary source of document context: the citation graph of
referential links between citing and cited papers. On a test set of
English-language scientific documents, we show that simple ways of utilizing
the structure and content of the citation graph can each lead to significant
gains in different scientific information extraction tasks. When these tasks
are combined, we observe a sizable improvement in end-to-end information
extraction over the state-of-the-art, suggesting the potential for future work
along this direction. We release software tools to facilitate citation-aware
SciIE development.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Viswanathan_V/0/1/0/all/0/1"&gt;Vijay Viswanathan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1"&gt;Graham Neubig&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1"&gt;Pengfei Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[To Point or Not to Point: Understanding How Abstractive Summarizers Paraphrase Text. (arXiv:2106.01581v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01581</id>
        <link href="http://arxiv.org/abs/2106.01581"/>
        <updated>2021-06-04T01:12:29.731Z</updated>
        <summary type="html"><![CDATA[Abstractive neural summarization models have seen great improvements in
recent years, as shown by ROUGE scores of the generated summaries. But despite
these improved metrics, there is limited understanding of the strategies
different models employ, and how those strategies relate their understanding of
language. To understand this better, we run several experiments to characterize
how one popular abstractive model, the pointer-generator model of See et al.
(2017), uses its explicit copy/generation switch to control its level of
abstraction (generation) vs extraction (copying). On an extractive-biased
dataset, the model utilizes syntactic boundaries to truncate sentences that are
otherwise often copied verbatim. When we modify the copy/generation switch and
force the model to generate, only simple paraphrasing abilities are revealed
alongside factual inaccuracies and hallucinations. On an abstractive-biased
dataset, the model copies infrequently but shows similarly limited abstractive
abilities. In line with previous research, these results suggest that
abstractive summarization models lack the semantic understanding necessary to
generate paraphrases that are both abstractive and faithful to the source
document.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wilber_M/0/1/0/all/0/1"&gt;Matt Wilber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Timkey_W/0/1/0/all/0/1"&gt;William Timkey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schijndel_M/0/1/0/all/0/1"&gt;Marten Van Schijndel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PsyQA: A Chinese Dataset for Generating Long Counseling Text for Mental Health Support. (arXiv:2106.01702v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01702</id>
        <link href="http://arxiv.org/abs/2106.01702"/>
        <updated>2021-06-04T01:12:29.725Z</updated>
        <summary type="html"><![CDATA[Great research interests have been attracted to devise AI services that are
able to provide mental health support. However, the lack of corpora is a main
obstacle to this research, particularly in Chinese language. In this paper, we
propose PsyQA, a Chinese dataset of psychological health support in the form of
question and answer pair. PsyQA is crawled from a Chinese mental health service
platform, and contains 22K questions and 56K long and well-structured answers.
Based on the psychological counseling theories, we annotate a portion of answer
texts with typical strategies for providing support, and further present
in-depth analysis of both lexical features and strategy patterns in the
counseling answers. We also evaluate the performance of generating counseling
answers with the generative pretrained models. Results show that utilizing
strategies enhances the fluency and helpfulness of generated answers, but there
is still a large space for future research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1"&gt;Hao Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1"&gt;Zhenru Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1"&gt;Chujie Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Siyang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1"&gt;Minlie Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Last iterate convergence of SGD for Least-Squares in the Interpolation regime. (arXiv:2102.03183v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.03183</id>
        <link href="http://arxiv.org/abs/2102.03183"/>
        <updated>2021-06-04T01:12:29.705Z</updated>
        <summary type="html"><![CDATA[Motivated by the recent successes of neural networks that have the ability to
fit the data perfectly and generalize well, we study the noiseless model in the
fundamental least-squares setup. We assume that an optimum predictor fits
perfectly inputs and outputs $\langle \theta_* , \phi(X) \rangle = Y$, where
$\phi(X)$ stands for a possibly infinite dimensional non-linear feature map. To
solve this problem, we consider the estimator given by the last iterate of
stochastic gradient descent (SGD) with constant step-size. In this context, our
contribution is two fold: (i) from a (stochastic) optimization perspective, we
exhibit an archetypal problem where we can show explicitly the convergence of
SGD final iterate for a non-strongly convex problem with constant step-size
whereas usual results use some form of average and (ii) from a statistical
perspective, we give explicit non-asymptotic convergence rates in the
over-parameterized setting and leverage a fine-grained parameterization of the
problem to exhibit polynomial rates that can be faster than $O(1/T)$. The link
with reproducing kernel Hilbert spaces is established.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Varre_A/0/1/0/all/0/1"&gt;Aditya Varre&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pillaud_Vivien_L/0/1/0/all/0/1"&gt;Loucas Pillaud-Vivien&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Flammarion_N/0/1/0/all/0/1"&gt;Nicolas Flammarion&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Communication-Efficient Distributed SVD via Local Power Iterations. (arXiv:2002.08014v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.08014</id>
        <link href="http://arxiv.org/abs/2002.08014"/>
        <updated>2021-06-04T01:12:29.690Z</updated>
        <summary type="html"><![CDATA[We study distributed computing of the truncated singular value decomposition
problem. We develop an algorithm that we call \texttt{LocalPower} for improving
communication efficiency. Specifically, we uniformly partition the dataset
among $m$ nodes and alternate between multiple (precisely $p$) local power
iterations and one global aggregation. In the aggregation, we propose to weight
each local eigenvector matrix with orthogonal Procrustes transformation (OPT).
As a practical surrogate of OPT, sign-fixing, which uses a diagonal matrix with
$\pm 1$ entries as weights, has better computation complexity and stability in
experiments. We theoretically show that under certain assumptions
\texttt{LocalPower} lowers the required number of communications by a factor of
$p$ to reach a constant accuracy. We also show that the strategy of
periodically decaying $p$ helps obtain high-precision solutions. We conduct
experiments to demonstrate the effectiveness of \texttt{LocalPower}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shusen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Chen_K/0/1/0/all/0/1"&gt;Kun Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhihua Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generalized Domain Adaptation. (arXiv:2106.01656v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.01656</id>
        <link href="http://arxiv.org/abs/2106.01656"/>
        <updated>2021-06-04T01:12:29.684Z</updated>
        <summary type="html"><![CDATA[Many variants of unsupervised domain adaptation (UDA) problems have been
proposed and solved individually. Its side effect is that a method that works
for one variant is often ineffective for or not even applicable to another,
which has prevented practical applications. In this paper, we give a general
representation of UDA problems, named Generalized Domain Adaptation (GDA). GDA
covers the major variants as special cases, which allows us to organize them in
a comprehensive framework. Moreover, this generalization leads to a new
challenging setting where existing methods fail, such as when domain labels are
unknown, and class labels are only partially given to each domain. We propose a
novel approach to the new setting. The key to our approach is self-supervised
class-destructive learning, which enables the learning of class-invariant
representations and domain-adversarial classifiers without using any domain
labels. Extensive experiments using three benchmark datasets demonstrate that
our method outperforms the state-of-the-art UDA methods in the new setting and
that it is competitive in existing UDA variations as well.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mitsuzumi_Y/0/1/0/all/0/1"&gt;Yu Mitsuzumi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Irie_G/0/1/0/all/0/1"&gt;Go Irie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ikami_D/0/1/0/all/0/1"&gt;Daiki Ikami&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shibata_T/0/1/0/all/0/1"&gt;Takashi Shibata&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Noisy Labels are Treasure: Mean-Teacher-Assisted Confident Learning for Hepatic Vessel Segmentation. (arXiv:2106.01860v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.01860</id>
        <link href="http://arxiv.org/abs/2106.01860"/>
        <updated>2021-06-04T01:12:29.677Z</updated>
        <summary type="html"><![CDATA[Manually segmenting the hepatic vessels from Computer Tomography (CT) is far
more expertise-demanding and laborious than other structures due to the
low-contrast and complex morphology of vessels, resulting in the extreme lack
of high-quality labeled data. Without sufficient high-quality annotations, the
usual data-driven learning-based approaches struggle with deficient training.
On the other hand, directly introducing additional data with low-quality
annotations may confuse the network, leading to undesirable performance
degradation. To address this issue, we propose a novel mean-teacher-assisted
confident learning framework to robustly exploit the noisy labeled data for the
challenging hepatic vessel segmentation task. Specifically, with the adapted
confident learning assisted by a third party, i.e., the weight-averaged teacher
model, the noisy labels in the additional low-quality dataset can be
transformed from "encumbrance" to "treasure" via progressive pixel-wise
soft-correction, thus providing productive guidance. Extensive experiments
using two public datasets demonstrate the superiority of the proposed framework
as well as the effectiveness of each component.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zhe Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lu_D/0/1/0/all/0/1"&gt;Donghuan Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yixin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Luo_J/0/1/0/all/0/1"&gt;Jie Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jagadeesan_J/0/1/0/all/0/1"&gt;Jayender Jagadeesan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ma_K/0/1/0/all/0/1"&gt;Kai Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zheng_Y/0/1/0/all/0/1"&gt;Yefeng Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiu Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Stochastic Moving-Average Estimators for Non-Convex Optimization. (arXiv:2104.14840v2 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.14840</id>
        <link href="http://arxiv.org/abs/2104.14840"/>
        <updated>2021-06-04T01:12:29.670Z</updated>
        <summary type="html"><![CDATA[In this paper, we demonstrate the power of a widely used stochastic estimator
based on moving average (SEMA) on a range of stochastic non-convex optimization
problems, which only requires {\bf a general unbiased stochastic oracle}. We
analyze various stochastic methods (existing or newly proposed) based on the
{\bf variance recursion property} of SEMA for three families of non-convex
optimization, namely standard stochastic non-convex minimization, stochastic
non-convex strongly-concave min-max optimization, and stochastic bilevel
optimization. Our contributions include: (i) for standard stochastic non-convex
minimization, we present a simple and intuitive proof of convergence for a
family Adam-style methods (including Adam) with an increasing or large
"momentum" parameter for the first-order moment, which gives an alternative yet
more natural way to guarantee Adam converge; (ii) for stochastic non-convex
strongly-concave min-max optimization, we present a single-loop stochastic
gradient descent ascent method based on the moving average estimators and
establish its oracle complexity of $O(1/\epsilon^4)$ without using a large
mini-batch size, addressing a gap in the literature; (iii) for stochastic
bilevel optimization, we present a single-loop stochastic method based on the
moving average estimators and establish its oracle complexity of $\widetilde
O(1/\epsilon^4)$ without computing the inverse or SVD of the Hessian matrix,
improving state-of-the-art results. For all these problems, we also establish a
variance diminishing result for the used stochastic gradient estimators.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Guo_Z/0/1/0/all/0/1"&gt;Zhishuai Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yi Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Yin_W/0/1/0/all/0/1"&gt;Wotao Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Jin_R/0/1/0/all/0/1"&gt;Rong Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Yang_T/0/1/0/all/0/1"&gt;Tianbao Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. (arXiv:2010.11929v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.11929</id>
        <link href="http://arxiv.org/abs/2010.11929"/>
        <updated>2021-06-04T01:12:29.662Z</updated>
        <summary type="html"><![CDATA[While the Transformer architecture has become the de-facto standard for
natural language processing tasks, its applications to computer vision remain
limited. In vision, attention is either applied in conjunction with
convolutional networks, or used to replace certain components of convolutional
networks while keeping their overall structure in place. We show that this
reliance on CNNs is not necessary and a pure transformer applied directly to
sequences of image patches can perform very well on image classification tasks.
When pre-trained on large amounts of data and transferred to multiple mid-sized
or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision
Transformer (ViT) attains excellent results compared to state-of-the-art
convolutional networks while requiring substantially fewer computational
resources to train.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dosovitskiy_A/0/1/0/all/0/1"&gt;Alexey Dosovitskiy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Beyer_L/0/1/0/all/0/1"&gt;Lucas Beyer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kolesnikov_A/0/1/0/all/0/1"&gt;Alexander Kolesnikov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weissenborn_D/0/1/0/all/0/1"&gt;Dirk Weissenborn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhai_X/0/1/0/all/0/1"&gt;Xiaohua Zhai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Unterthiner_T/0/1/0/all/0/1"&gt;Thomas Unterthiner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dehghani_M/0/1/0/all/0/1"&gt;Mostafa Dehghani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Minderer_M/0/1/0/all/0/1"&gt;Matthias Minderer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heigold_G/0/1/0/all/0/1"&gt;Georg Heigold&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gelly_S/0/1/0/all/0/1"&gt;Sylvain Gelly&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Uszkoreit_J/0/1/0/all/0/1"&gt;Jakob Uszkoreit&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Houlsby_N/0/1/0/all/0/1"&gt;Neil Houlsby&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Policy Gradient Algorithm for Learning to Learn in Multiagent Reinforcement Learning. (arXiv:2011.00382v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.00382</id>
        <link href="http://arxiv.org/abs/2011.00382"/>
        <updated>2021-06-04T01:12:29.656Z</updated>
        <summary type="html"><![CDATA[A fundamental challenge in multiagent reinforcement learning is to learn
beneficial behaviors in a shared environment with other simultaneously learning
agents. In particular, each agent perceives the environment as effectively
non-stationary due to the changing policies of other agents. Moreover, each
agent is itself constantly learning, leading to natural non-stationarity in the
distribution of experiences encountered. In this paper, we propose a novel
meta-multiagent policy gradient theorem that directly accounts for the
non-stationary policy dynamics inherent to multiagent learning settings. This
is achieved by modeling our gradient updates to consider both an agent's own
non-stationary policy dynamics and the non-stationary policy dynamics of other
agents in the environment. We show that our theoretically grounded approach
provides a general solution to the multiagent learning problem, which
inherently comprises all key aspects of previous state of the art approaches on
this topic. We test our method on a diverse suite of multiagent benchmarks and
demonstrate a more efficient ability to adapt to new agents as they learn than
baseline methods across the full spectrum of mixed incentive, competitive, and
cooperative domains.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1"&gt;Dong-Ki Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1"&gt;Miao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Riemer_M/0/1/0/all/0/1"&gt;Matthew Riemer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1"&gt;Chuangchuang Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abdulhai_M/0/1/0/all/0/1"&gt;Marwa Abdulhai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Habibi_G/0/1/0/all/0/1"&gt;Golnaz Habibi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lopez_Cot_S/0/1/0/all/0/1"&gt;Sebastian Lopez-Cot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tesauro_G/0/1/0/all/0/1"&gt;Gerald Tesauro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+How_J/0/1/0/all/0/1"&gt;Jonathan P. How&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep neural network approximation of analytic functions. (arXiv:2104.02095v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.02095</id>
        <link href="http://arxiv.org/abs/2104.02095"/>
        <updated>2021-06-04T01:12:29.648Z</updated>
        <summary type="html"><![CDATA[We provide an entropy bound for the spaces of neural networks with piecewise
linear activation functions, such as the ReLU and the absolute value functions.
This bound generalizes the known entropy bound for the space of linear
functions on $\mathbb{R}^d$ and it depends on the value at the point
$(1,1,...,1)$ of the networks obtained by taking the absolute values of all
parameters of original networks. Keeping this value together with the depth,
width and the parameters of the networks to have logarithmic dependence on
$1/\varepsilon$, we $\varepsilon$-approximate functions that are analytic on
certain regions of $\mathbb{C}^d$.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Beknazaryan_A/0/1/0/all/0/1"&gt;Aleksandr Beknazaryan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards urban scenes understanding through polarization cues. (arXiv:2106.01717v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.01717</id>
        <link href="http://arxiv.org/abs/2106.01717"/>
        <updated>2021-06-04T01:12:29.642Z</updated>
        <summary type="html"><![CDATA[Autonomous robotics is critically affected by the robustness of its scene
understanding algorithms. We propose a two-axis pipeline based on polarization
indices to analyze dynamic urban scenes. As robots evolve in unknown
environments, they are prone to encountering specular obstacles. Usually,
specular phenomena are rarely taken into account by algorithms which causes
misinterpretations and erroneous estimates. By exploiting all the light
properties, systems can greatly increase their robustness to events. In
addition to the conventional photometric characteristics, we propose to include
polarization sensing.

We demonstrate in this paper that the contribution of polarization
measurement increases both the performances of segmentation and the quality of
depth estimation. Our polarimetry-based approaches are compared here with other
state-of-the-art RGB-centric methods showing interest of using polarization
imaging.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Blanchon_M/0/1/0/all/0/1"&gt;Marc Blanchon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sidibe_D/0/1/0/all/0/1"&gt;D&amp;#xe9;sir&amp;#xe9; Sidib&amp;#xe9;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morel_O/0/1/0/all/0/1"&gt;Olivier Morel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seulin_R/0/1/0/all/0/1"&gt;Ralph Seulin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meriaudeau_F/0/1/0/all/0/1"&gt;Fabrice Meriaudeau&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ProtoRes: Proto-Residual Architecture for Deep Modeling of Human Pose. (arXiv:2106.01981v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.01981</id>
        <link href="http://arxiv.org/abs/2106.01981"/>
        <updated>2021-06-04T01:12:29.623Z</updated>
        <summary type="html"><![CDATA[Our work focuses on the development of a learnable neural representation of
human pose for advanced AI assisted animation tooling. Specifically, we tackle
the problem of constructing a full static human pose based on sparse and
variable user inputs (e.g. locations and/or orientations of a subset of body
joints). To solve this problem, we propose a novel neural architecture that
combines residual connections with prototype encoding of a partially specified
pose to create a new complete pose from the learned latent space. We show that
our architecture outperforms a baseline based on Transformer, both in terms of
accuracy and computational efficiency. Additionally, we develop a user
interface to integrate our neural model in Unity, a real-time 3D development
platform. Furthermore, we introduce two new datasets representing the static
human pose modeling problem, based on high-quality human motion capture data,
which will be released publicly along with model code.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Oreshkin_B/0/1/0/all/0/1"&gt;Boris N. Oreshkin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bocquelet_F/0/1/0/all/0/1"&gt;Florent Bocquelet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Harvey_F/0/1/0/all/0/1"&gt;F&amp;#xe9;lix H. Harvey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raitt_B/0/1/0/all/0/1"&gt;Bay Raitt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Laflamme_D/0/1/0/all/0/1"&gt;Dominic Laflamme&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MOFA: Modular Factorial Design for Hyperparameter Optimization. (arXiv:2011.09545v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.09545</id>
        <link href="http://arxiv.org/abs/2011.09545"/>
        <updated>2021-06-04T01:12:29.611Z</updated>
        <summary type="html"><![CDATA[This paper presents a novel and lightweight hyperparameter optimization (HPO)
method, MOdular FActorial Design (MOFA). MOFA pursues several rounds of HPO,
where each round alternates between exploration of hyperparameter space by
factorial design and exploitation of evaluation results by factorial analysis.
Each round first explores the configuration space by constructing a
low-discrepancy set of hyperparameters that cover this space well while
de-correlating hyperparameters, and then exploits evaluation results through
factorial analysis that determines which hyperparameters should be further
explored and which should become fixed in the next round. We prove that the
inference of MOFA achieves higher confidence than other sampling schemes. Each
individual round is highly parallelizable and hence offers major improvements
of efficiency compared to model-based methods. Empirical results show that MOFA
achieves better effectiveness and efficiency compared with state-of-the-art
methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_B/0/1/0/all/0/1"&gt;Bo Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yimin Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1"&gt;Hanrong Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Staab_S/0/1/0/all/0/1"&gt;Steffen Staab&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhenguo Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Electrocardiogram synthesis. (arXiv:2103.00006v2 [eess.SP] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.00006</id>
        <link href="http://arxiv.org/abs/2103.00006"/>
        <updated>2021-06-04T01:12:29.603Z</updated>
        <summary type="html"><![CDATA[The electrocardiogram (ECG) records electrical signals in a non-invasive way
to observe the condition of the heart, typically looking at the heart from 12
different directions. Several types of the cardiac disease are diagnosed by
using 12-lead ECGs Recently, various wearable devices have enabled immediate
access to the ECG without the use of wieldy equipment. However, they only
provide ECGs with a couple of leads. This results in an inaccurate diagnosis of
cardiac disease due to lacking of required leads. We propose a deep generative
model for ECG synthesis from two asynchronous leads to ten leads. It first
represents a heart condition referring to two leads, and then generates ten
leads based on the represented heart condition. Both the rhythm and amplitude
of leads generated resemble those of the original ones, while the technique
removes noise and the baseline wander appearing in the original leads. As a
data augmentation method, our model improves the classification performance of
models compared with models using ECGs with only one or two leads.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Jo_Y/0/1/0/all/0/1"&gt;Yong-Yeon Jo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kwon_J/0/1/0/all/0/1"&gt;Joon-Myoung Kwon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Scale Feature Aggregation by Cross-Scale Pixel-to-Region Relation Operation for Semantic Segmentation. (arXiv:2106.01744v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.01744</id>
        <link href="http://arxiv.org/abs/2106.01744"/>
        <updated>2021-06-04T01:12:29.579Z</updated>
        <summary type="html"><![CDATA[Exploiting multi-scale features has shown great potential in tackling
semantic segmentation problems. The aggregation is commonly done with sum or
concatenation (concat) followed by convolutional (conv) layers. However, it
fully passes down the high-level context to the following hierarchy without
considering their interrelation. In this work, we aim to enable the low-level
feature to aggregate the complementary context from adjacent high-level feature
maps by a cross-scale pixel-to-region relation operation. We leverage
cross-scale context propagation to make the long-range dependency capturable
even by the high-resolution low-level features. To this end, we employ an
efficient feature pyramid network to obtain multi-scale features. We propose a
Relational Semantics Extractor (RSE) and Relational Semantics Propagator (RSP)
for context extraction and propagation respectively. Then we stack several RSP
into an RSP head to achieve the progressive top-down distribution of the
context. Experiment results on two challenging datasets Cityscapes and COCO
demonstrate that the RSP head performs competitively on both semantic
segmentation and panoptic segmentation with high efficiency. It outperforms
DeeplabV3 [1] by 0.7% with 75% fewer FLOPs (multiply-adds) in the semantic
segmentation task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1"&gt;Yechao Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Ziyuan Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1"&gt;Lyuyu Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1"&gt;Hongliang Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ang_M/0/1/0/all/0/1"&gt;Marcelo H. Ang Jr&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rus_D/0/1/0/all/0/1"&gt;Daniela Rus&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fast Minimum-norm Adversarial Attacks through Adaptive Norm Constraints. (arXiv:2102.12827v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.12827</id>
        <link href="http://arxiv.org/abs/2102.12827"/>
        <updated>2021-06-04T01:12:29.562Z</updated>
        <summary type="html"><![CDATA[Evaluating adversarial robustness amounts to finding the minimum perturbation
needed to have an input sample misclassified. The inherent complexity of the
underlying optimization requires current gradient-based attacks to be carefully
tuned, initialized, and possibly executed for many computationally-demanding
iterations, even if specialized to a given perturbation model. In this work, we
overcome these limitations by proposing a fast minimum-norm (FMN) attack that
works with different $\ell_p$-norm perturbation models ($p=0, 1, 2, \infty$),
is robust to hyperparameter choices, does not require adversarial starting
points, and converges within few lightweight steps. It works by iteratively
finding the sample misclassified with maximum confidence within an
$\ell_p$-norm constraint of size $\epsilon$, while adapting $\epsilon$ to
minimize the distance of the current sample to the decision boundary. Extensive
experiments show that FMN significantly outperforms existing attacks in terms
of convergence speed and computation time, while reporting comparable or even
smaller perturbation sizes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pintor_M/0/1/0/all/0/1"&gt;Maura Pintor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roli_F/0/1/0/all/0/1"&gt;Fabio Roli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brendel_W/0/1/0/all/0/1"&gt;Wieland Brendel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Biggio_B/0/1/0/all/0/1"&gt;Battista Biggio&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pathology-Aware Generative Adversarial Networks for Medical Image Augmentation. (arXiv:2106.01915v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.01915</id>
        <link href="http://arxiv.org/abs/2106.01915"/>
        <updated>2021-06-04T01:12:29.546Z</updated>
        <summary type="html"><![CDATA[Convolutional Neural Networks (CNNs) can play a key role in Medical Image
Analysis under large-scale annotated datasets. However, preparing such massive
dataset is demanding. In this context, Generative Adversarial Networks (GANs)
can generate realistic but novel samples, and thus effectively cover the real
image distribution. In terms of interpolation, the GAN-based medical image
augmentation is reliable because medical modalities can display the human
body's strong anatomical consistency at fixed position while clearly reflecting
inter-subject variability; thus, we propose to use noise-to-image GANs (e.g.,
random noise samples to diverse pathological images) for (i) medical Data
Augmentation (DA) and (ii) physician training. Regarding the DA, the
GAN-generated images can improve Computer-Aided Diagnosis based on supervised
learning. For the physician training, the GANs can display novel desired
pathological images and help train medical trainees despite
infrastructural/legal constraints. This thesis contains four GAN projects
aiming to present such novel applications' clinical relevance in collaboration
with physicians. Whereas the methods are more generally applicable, this thesis
only explores a few oncological applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Han_C/0/1/0/all/0/1"&gt;Changhee Han&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Convolutional Neural Network(CNN/ConvNet) in Stock Price Movement Prediction. (arXiv:2106.01920v1 [cs.NE])]]></title>
        <id>http://arxiv.org/abs/2106.01920</id>
        <link href="http://arxiv.org/abs/2106.01920"/>
        <updated>2021-06-04T01:12:29.530Z</updated>
        <summary type="html"><![CDATA[With technological advancements and the exponential growth of data, we have
been unfolding different capabilities of neural networks in different sectors.
In this paper, I have tried to use a specific type of Neural Network known as
Convolutional Neural Network(CNN/ConvNet) in the stock market. In other words,
I have tried to construct and train a convolutional neural network on past
stock prices data and then tried to predict the movement of stock price i.e.
whether the stock price would rise or fall, in the coming time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bhardwaj_K/0/1/0/all/0/1"&gt;Kunal Bhardwaj&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Truncated Log-concave Sampling with Reflective Hamiltonian Monte Carlo. (arXiv:2102.13068v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.13068</id>
        <link href="http://arxiv.org/abs/2102.13068"/>
        <updated>2021-06-04T01:12:29.524Z</updated>
        <summary type="html"><![CDATA[We introduce Reflective Hamiltonian Monte Carlo (ReHMC), an HMC-based
algorithm, to sample from a log-concave distribution restricted to a convex
body. We prove that, starting from a warm start, the walk mixes to a
log-concave target distribution $\pi(x) \propto e^{-f(x)}$, where $f$ is
$L$-smooth and $m$-strongly-convex, within accuracy $\varepsilon$ after
$\widetilde O(\kappa d^2 \ell^2 \log (1 / \varepsilon))$ steps for a
well-rounded convex body where $\kappa = L / m$ is the condition number of the
negative log-density, $d$ is the dimension, $\ell$ is an upper bound on the
number of reflections, and $\varepsilon$ is the accuracy parameter. We also
developed an efficient open source implementation of ReHMC and we performed an
experimental study on various high-dimensional data-sets. The experiments
suggest that ReHMC outperfroms Hit-and-Run and Coordinate-Hit-and-Run regarding
the time it needs to produce an independent sample and introduces practical
truncated sampling in thousands of dimensions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chalkis_A/0/1/0/all/0/1"&gt;Apostolos Chalkis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fisikopoulos_V/0/1/0/all/0/1"&gt;Vissarion Fisikopoulos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Papachristou_M/0/1/0/all/0/1"&gt;Marios Papachristou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsigaridas_E/0/1/0/all/0/1"&gt;Elias Tsigaridas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Meta-Learning an Inference Algorithm for Probabilistic Programs. (arXiv:2103.00737v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.00737</id>
        <link href="http://arxiv.org/abs/2103.00737"/>
        <updated>2021-06-04T01:12:29.505Z</updated>
        <summary type="html"><![CDATA[We present a meta-algorithm for learning a posterior-inference algorithm for
restricted probabilistic programs. Our meta-algorithm takes a training set of
probabilistic programs that describe models with observations, and attempts to
learn an efficient method for inferring the posterior of a similar program. A
key feature of our approach is the use of what we call a white-box inference
algorithm that extracts information directly from model descriptions
themselves, given as programs. Concretely, our white-box inference algorithm is
equipped with multiple neural networks, one for each type of atomic command,
and computes an approximate posterior of a given probabilistic program by
analysing individual atomic commands in the program using these networks. The
parameters of these networks are then learnt from a training set by our
meta-algorithm. We empirically demonstrate that the learnt inference algorithm
generalises well to unseen programs in terms of both interpolation and
extrapolation, and report cases where our approach may be preferable to a
state-of-the-art inference algorithm such as HMC. The overall results show the
promise as well as remaining challenges of our approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Che_G/0/1/0/all/0/1"&gt;Gwonsoo Che&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Hongseok Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Less is More: Sparse Sampling for Dense Reaction Predictions. (arXiv:2106.01764v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.01764</id>
        <link href="http://arxiv.org/abs/2106.01764"/>
        <updated>2021-06-04T01:12:29.499Z</updated>
        <summary type="html"><![CDATA[Obtaining viewer responses from videos can be useful for creators and
streaming platforms to analyze the video performance and improve the future
user experience. In this report, we present our method for 2021 Evoked
Expression from Videos Challenge. In particular, our model utilizes both audio
and image modalities as inputs to predict emotion changes of viewers. To model
long-range emotion changes, we use a GRU-based model to predict one sparse
signal with 1Hz. We observe that the emotion changes are smooth. Therefore, the
final dense prediction is obtained via linear interpolating the signal, which
is robust to the prediction fluctuation. Albeit simple, the proposed method has
achieved pearson's correlation score of 0.04430 on the final private test set.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_K/0/1/0/all/0/1"&gt;Kezhou Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaohan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1"&gt;Zhedong Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1"&gt;Linchao Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yi Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Advances in Classifying the Stages of Diabetic Retinopathy Using Convolutional Neural Networks in Low Memory Edge Devices. (arXiv:2106.01739v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.01739</id>
        <link href="http://arxiv.org/abs/2106.01739"/>
        <updated>2021-06-04T01:12:29.492Z</updated>
        <summary type="html"><![CDATA[Diabetic Retinopathy (DR) is a severe complication that may lead to retinal
vascular damage and is one of the leading causes of vision impairment and
blindness. DR broadly is classified into two stages - non-proliferative (NPDR),
where there are almost no symptoms, except a few microaneurysms, and
proliferative (PDR) involving a huge number of microaneurysms and hemorrhages,
soft and hard exudates, neo-vascularization, macular ischemia or a combination
of these, making it easier to detect. More specifically, DR is usually
classified into five levels, labeled 0-4, from 0 indicating no DR to 4 which is
most severe. This paper firstly presents a discussion on the risk factors of
the disease, then surveys the recent literature on the topic followed by
examining certain techniques which were found to be highly effective in
improving the prognosis accuracy. Finally, a convolutional neural network model
is proposed to detect all the stages of DR on a low-memory edge
microcontroller. The model has a size of just 5.9 MB, accuracy and F1 score
both of 94% and an inference speed of about 20 frames per second.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Paul_A/0/1/0/all/0/1"&gt;Aditya Jyoti Paul&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning from the Worst: Dynamically Generated Datasets to Improve Online Hate Detection. (arXiv:2012.15761v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.15761</id>
        <link href="http://arxiv.org/abs/2012.15761"/>
        <updated>2021-06-04T01:12:29.486Z</updated>
        <summary type="html"><![CDATA[We present a human-and-model-in-the-loop process for dynamically generating
datasets and training better performing and more robust hate detection models.
We provide a new dataset of ~40,000 entries, generated and labelled by trained
annotators over four rounds of dynamic data creation. It includes ~15,000
challenging perturbations and each hateful entry has fine-grained labels for
the type and target of hate. Hateful entries make up 54% of the dataset, which
is substantially higher than comparable datasets. We show that model
performance is substantially improved using this approach. Models trained on
later rounds of data collection perform better on test sets and are harder for
annotators to trick. They also perform better on HateCheck, a suite of
functional tests for online hate detection. We provide the code, dataset and
annotation guidelines for other researchers to use. Accepted at ACL 2021.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vidgen_B/0/1/0/all/0/1"&gt;Bertie Vidgen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thrush_T/0/1/0/all/0/1"&gt;Tristan Thrush&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Waseem_Z/0/1/0/all/0/1"&gt;Zeerak Waseem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kiela_D/0/1/0/all/0/1"&gt;Douwe Kiela&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[You Never Cluster Alone. (arXiv:2106.01908v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.01908</id>
        <link href="http://arxiv.org/abs/2106.01908"/>
        <updated>2021-06-04T01:12:29.480Z</updated>
        <summary type="html"><![CDATA[Recent advances in self-supervised learning with instance-level contrastive
objectives facilitate unsupervised clustering. However, a standalone datum is
not perceiving the context of the holistic cluster, and may undergo sub-optimal
assignment. In this paper, we extend the mainstream contrastive learning
paradigm to a cluster-level scheme, where all the data subjected to the same
cluster contribute to a unified representation that encodes the context of each
data group. Contrastive learning with this representation then rewards the
assignment of each datum. To implement this vision, we propose twin-contrast
clustering (TCC). We define a set of categorical variables as clustering
assignment confidence, which links the instance-level learning track with the
cluster-level one. On one hand, with the corresponding assignment variables
being the weight, a weighted aggregation along the data points implements the
set representation of a cluster. We further propose heuristic cluster
augmentation equivalents to enable cluster-level contrastive learning. On the
other hand, we derive the evidence lower-bound of the instance-level
contrastive objective with the assignments. By reparametrizing the assignment
variables, TCC is trained end-to-end, requiring no alternating steps. Extensive
experiments show that TCC outperforms the state-of-the-art on challenging
benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1"&gt;Yuming Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1"&gt;Ziyi Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1"&gt;Menghan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1"&gt;Jie Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1"&gt;Philip H.S. Torr&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1"&gt;Ling Shao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring Self-Supervised Representation Ensembles for COVID-19 Cough Classification. (arXiv:2105.07566v2 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.07566</id>
        <link href="http://arxiv.org/abs/2105.07566"/>
        <updated>2021-06-04T01:12:29.473Z</updated>
        <summary type="html"><![CDATA[The usage of smartphone-collected respiratory sound, trained with deep
learning models, for detecting and classifying COVID-19 becomes popular
recently. It removes the need for in-person testing procedures especially for
rural regions where related medical supplies, experienced workers, and
equipment are limited. However, existing sound-based diagnostic approaches are
trained in a fully supervised manner, which requires large scale well-labelled
data. It is critical to discover new methods to leverage unlabelled respiratory
data, which can be obtained more easily. In this paper, we propose a novel
self-supervised learning enabled framework for COVID-19 cough classification. A
contrastive pre-training phase is introduced to train a Transformer-based
feature encoder with unlabelled data. Specifically, we design a random masking
mechanism to learn robust representations of respiratory sounds. The
pre-trained feature encoder is then fine-tuned in the downstream phase to
perform cough classification. In addition, different ensembles with varied
random masking rates are also explored in the downstream phase. Through
extensive evaluations, we demonstrate that the proposed contrastive
pre-training, the random masking mechanism, and the ensemble architecture
contribute to improving cough classification performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1"&gt;Hao Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salim_F/0/1/0/all/0/1"&gt;Flora D. Salim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarially Adaptive Normalization for Single Domain Generalization. (arXiv:2106.01899v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.01899</id>
        <link href="http://arxiv.org/abs/2106.01899"/>
        <updated>2021-06-04T01:12:29.455Z</updated>
        <summary type="html"><![CDATA[Single domain generalization aims to learn a model that performs well on many
unseen domains with only one domain data for training. Existing works focus on
studying the adversarial domain augmentation (ADA) to improve the model's
generalization capability. The impact on domain generalization of the
statistics of normalization layers is still underinvestigated. In this paper,
we propose a generic normalization approach, adaptive standardization and
rescaling normalization (ASR-Norm), to complement the missing part in previous
works. ASR-Norm learns both the standardization and rescaling statistics via
neural networks. This new form of normalization can be viewed as a generic form
of the traditional normalizations. When trained with ADA, the statistics in
ASR-Norm are learned to be adaptive to the data coming from different domains,
and hence improves the model generalization performance across domains,
especially on the target domain with large discrepancy from the source domain.
The experimental results show that ASR-Norm can bring consistent improvement to
the state-of-the-art ADA approaches by 1.6%, 2.7%, and 6.3% averagely on the
Digits, CIFAR-10-C, and PACS benchmarks, respectively. As a generic tool, the
improvement introduced by ASR-Norm is agnostic to the choice of ADA methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1"&gt;Xinjie Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1"&gt;Qifei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ke_J/0/1/0/all/0/1"&gt;Junjie Ke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1"&gt;Feng Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_B/0/1/0/all/0/1"&gt;Boqing Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1"&gt;Mingyuan Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Modeling Fine-Grained Entity Types with Box Embeddings. (arXiv:2101.00345v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.00345</id>
        <link href="http://arxiv.org/abs/2101.00345"/>
        <updated>2021-06-04T01:12:29.449Z</updated>
        <summary type="html"><![CDATA[Neural entity typing models typically represent fine-grained entity types as
vectors in a high-dimensional space, but such spaces are not well-suited to
modeling these types' complex interdependencies. We study the ability of box
embeddings, which embed concepts as d-dimensional hyperrectangles, to capture
hierarchies of types even when these relationships are not defined explicitly
in the ontology. Our model represents both types and entity mentions as boxes.
Each mention and its context are fed into a BERT-based model to embed that
mention in our box space; essentially, this model leverages typological clues
present in the surface text to hypothesize a type representation for the
mention. Box containment can then be used to derive both the posterior
probability of a mention exhibiting a given type and the conditional
probability relations between types themselves. We compare our approach with a
vector-based typing model and observe state-of-the-art performance on several
entity typing benchmarks. In addition to competitive typing performance, our
box-based model shows better performance in prediction consistency (predicting
a supertype and a subtype together) and confidence (i.e., calibration),
demonstrating that the box-based model captures the latent type hierarchies
better than the vector-based model does.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Onoe_Y/0/1/0/all/0/1"&gt;Yasumasa Onoe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boratko_M/0/1/0/all/0/1"&gt;Michael Boratko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McCallum_A/0/1/0/all/0/1"&gt;Andrew McCallum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Durrett_G/0/1/0/all/0/1"&gt;Greg Durrett&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Tutorial on Sparse Gaussian Processes and Variational Inference. (arXiv:2012.13962v9 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.13962</id>
        <link href="http://arxiv.org/abs/2012.13962"/>
        <updated>2021-06-04T01:12:29.442Z</updated>
        <summary type="html"><![CDATA[Gaussian processes (GPs) provide a framework for Bayesian inference that can
offer principled uncertainty estimates for a large range of problems. For
example, if we consider regression problems with Gaussian likelihoods, a GP
model enjoys a posterior in closed form. However, identifying the posterior GP
scales cubically with the number of training examples and requires to store all
examples in memory. In order to overcome these obstacles, sparse GPs have been
proposed that approximate the true posterior GP with pseudo-training examples.
Importantly, the number of pseudo-training examples is user-defined and enables
control over computational and memory complexity. In the general case, sparse
GPs do not enjoy closed-form solutions and one has to resort to approximate
inference. In this context, a convenient choice for approximate inference is
variational inference (VI), where the problem of Bayesian inference is cast as
an optimization problem -- namely, to maximize a lower bound of the log
marginal likelihood. This paves the way for a powerful and versatile framework,
where pseudo-training examples are treated as optimization arguments of the
approximate posterior that are jointly identified together with hyperparameters
of the generative model (i.e. prior and likelihood). The framework can
naturally handle a wide scope of supervised learning problems, ranging from
regression with heteroscedastic and non-Gaussian likelihoods to classification
problems with discrete labels, but also multilabel problems. The purpose of
this tutorial is to provide access to the basic matter for readers without
prior knowledge in both GPs and VI. A proper exposition to the subject enables
also access to more recent advances (like importance-weighted VI as well as
interdomain, multioutput and deep GPs) that can serve as an inspiration for new
research ideas.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Leibfried_F/0/1/0/all/0/1"&gt;Felix Leibfried&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dutordoir_V/0/1/0/all/0/1"&gt;Vincent Dutordoir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+John_S/0/1/0/all/0/1"&gt;ST John&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Durrande_N/0/1/0/all/0/1"&gt;Nicolas Durrande&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RetCL: A Selection-based Approach for Retrosynthesis via Contrastive Learning. (arXiv:2105.00795v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.00795</id>
        <link href="http://arxiv.org/abs/2105.00795"/>
        <updated>2021-06-04T01:12:29.434Z</updated>
        <summary type="html"><![CDATA[Retrosynthesis, of which the goal is to find a set of reactants for
synthesizing a target product, is an emerging research area of deep learning.
While the existing approaches have shown promising results, they currently lack
the ability to consider availability (e.g., stability or purchasability) of the
reactants or generalize to unseen reaction templates (i.e., chemical reaction
rules). In this paper, we propose a new approach that mitigates the issues by
reformulating retrosynthesis into a selection problem of reactants from a
candidate set of commercially available molecules. To this end, we design an
efficient reactant selection framework, named RetCL (retrosynthesis via
contrastive learning), for enumerating all of the candidate molecules based on
selection scores computed by graph neural networks. For learning the score
functions, we also propose a novel contrastive training scheme with hard
negative mining. Extensive experiments demonstrate the benefits of the proposed
selection-based approach. For example, when all 671k reactants in the USPTO
{database} are given as candidates, our RetCL achieves top-1 exact match
accuracy of $71.3\%$ for the USPTO-50k benchmark, while a recent
transformer-based approach achieves $59.6\%$. We also demonstrate that RetCL
generalizes well to unseen templates in various settings in contrast to
template-based approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1"&gt;Hankook Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ahn_S/0/1/0/all/0/1"&gt;Sungsoo Ahn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seo_S/0/1/0/all/0/1"&gt;Seung-Woo Seo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1"&gt;You Young Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_E/0/1/0/all/0/1"&gt;Eunho Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1"&gt;Sung-Ju Hwang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1"&gt;Jinwoo Shin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[UnitedQA: A Hybrid Approach for Open Domain Question Answering. (arXiv:2101.00178v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.00178</id>
        <link href="http://arxiv.org/abs/2101.00178"/>
        <updated>2021-06-04T01:12:29.427Z</updated>
        <summary type="html"><![CDATA[To date, most of recent work under the retrieval-reader framework for
open-domain QA focuses on either extractive or generative reader exclusively.
In this paper, we study a hybrid approach for leveraging the strengths of both
models. We apply novel techniques to enhance both extractive and generative
readers built upon recent pretrained neural language models, and find that
proper training methods can provide large improvement over previous
state-of-the-art models. We demonstrate that a simple hybrid approach by
combining answers from both readers can efficiently take advantages of
extractive and generative answer inference strategies and outperforms single
models as well as homogeneous ensembles. Our approach outperforms previous
state-of-the-art models by 3.3 and 2.7 points in exact match on
NaturalQuestions and TriviaQA respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1"&gt;Hao Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1"&gt;Yelong Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiaodong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_P/0/1/0/all/0/1"&gt;Pengcheng He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1"&gt;Weizhu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1"&gt;Jianfeng Gao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning-based Robust Motion Planning with Guaranteed Stability: A Contraction Theory Approach. (arXiv:2102.12668v2 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.12668</id>
        <link href="http://arxiv.org/abs/2102.12668"/>
        <updated>2021-06-04T01:12:29.406Z</updated>
        <summary type="html"><![CDATA[This paper presents Learning-based Autonomous Guidance with RObustness and
Stability guarantees (LAG-ROS), which provides machine learning-based nonlinear
motion planners with formal robustness and stability guarantees, by designing a
differential Lyapunov function using contraction theory. LAG-ROS utilizes a
neural network to model a robust tracking controller independently of a target
trajectory, for which we show that the Euclidean distance between the target
and controlled trajectories is exponentially bounded linearly in the learning
error, even under the existence of bounded external disturbances. We also
present a convex optimization approach that minimizes the steady-state bound of
the tracking error to construct the robust control law for neural network
training. In numerical simulations, it is demonstrated that the proposed method
indeed possesses superior properties of robustness and nonlinear stability
resulting from contraction theory, whilst retaining the computational
efficiency of existing learning-based motion planners.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tsukamoto_H/0/1/0/all/0/1"&gt;Hiroyasu Tsukamoto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chung_S/0/1/0/all/0/1"&gt;Soon-Jo Chung&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Provably-Efficient Model-Free Algorithm for Constrained Markov Decision Processes. (arXiv:2106.01577v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01577</id>
        <link href="http://arxiv.org/abs/2106.01577"/>
        <updated>2021-06-04T01:12:29.399Z</updated>
        <summary type="html"><![CDATA[This paper presents the first {\em model-free}, {\em simulator-free}
reinforcement learning algorithm for Constrained Markov Decision Processes
(CMDPs) with sublinear regret and zero constraint violation. The algorithm is
named Triple-Q because it has three key components: a Q-function (also called
action-value function) for the cumulative reward, a Q-function for the
cumulative utility for the constraint, and a virtual-Queue that
(over)-estimates the cumulative constraint violation. Under Triple-Q, at each
step, an action is chosen based on the pseudo-Q-value that is a combination of
the three Q values. The algorithm updates the reward and utility Q-values with
learning rates that depend on the visit counts to the corresponding (state,
action) pairs and are periodically reset. In the episodic CMDP setting,
Triple-Q achieves $\tilde{\cal O}\left(\frac{1 }{\delta}H^4
S^{\frac{1}{2}}A^{\frac{1}{2}}K^{\frac{4}{5}} \right)$ regret, where $K$ is the
total number of episodes, $H$ is the number of steps in each episode, $S$ is
the number of states, $A$ is the number of actions, and $\delta$ is Slater's
constant. Furthermore, Triple-Q guarantees zero constraint violation when $K$
is sufficiently large. Finally, the computational complexity of Triple-Q is
similar to SARSA for unconstrained MDPs and is computationally efficient.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wei_H/0/1/0/all/0/1"&gt;Honghao Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ying_L/0/1/0/all/0/1"&gt;Lei Ying&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MedNLI Is Not Immune: Natural Language Inference Artifacts in the Clinical Domain. (arXiv:2106.01491v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01491</id>
        <link href="http://arxiv.org/abs/2106.01491"/>
        <updated>2021-06-04T01:12:29.393Z</updated>
        <summary type="html"><![CDATA[Crowdworker-constructed natural language inference (NLI) datasets have been
found to contain statistical artifacts associated with the annotation process
that allow hypothesis-only classifiers to achieve better-than-random
performance (Poliak et al., 2018; Gururanganet et al., 2018; Tsuchiya, 2018).
We investigate whether MedNLI, a physician-annotated dataset with premises
extracted from clinical notes, contains such artifacts (Romanov and Shivade,
2018). We find that entailed hypotheses contain generic versions of specific
concepts in the premise, as well as modifiers related to responsiveness,
duration, and probability. Neutral hypotheses feature conditions and behaviors
that co-occur with, or cause, the condition(s) in the premise. Contradiction
hypotheses feature explicit negation of the premise and implicit negation via
assertion of good health. Adversarial filtering demonstrates that performance
degrades when evaluated on the difficult subset. We provide partition
information and recommendations for alternative dataset construction strategies
for knowledge-intensive domains.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Herlihy_C/0/1/0/all/0/1"&gt;Christine Herlihy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rudinger_R/0/1/0/all/0/1"&gt;Rachel Rudinger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Choose a Transformer: Fourier or Galerkin. (arXiv:2105.14995v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14995</id>
        <link href="http://arxiv.org/abs/2105.14995"/>
        <updated>2021-06-04T01:12:29.386Z</updated>
        <summary type="html"><![CDATA[In this paper, we apply the self-attention from the state-of-the-art
Transformer in Attention Is All You Need the first time to a data-driven
operator learning problem related to partial differential equations. We put
together an effort to explain the heuristics of, and improve the efficacy of
the self-attention by demonstrating that the softmax normalization in the
scaled dot-product attention is sufficient but not necessary, and have proved
the approximation capacity of a linear variant as a Petrov-Galerkin projection.
A new layer normalization scheme is proposed to allow a scaling to propagate
through attention layers, which helps the model achieve remarkable accuracy in
operator learning tasks with unnormalized data. Finally, we present three
operator learning experiments, including the viscid Burgers' equation, an
interface Darcy flow, and an inverse interface coefficient identification
problem. All experiments validate the improvements of the newly proposed simple
attention-based operator learner over their softmax-normalized counterparts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cao_S/0/1/0/all/0/1"&gt;Shuhao Cao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fast improvement of TEM image with low-dose electrons by deep learning. (arXiv:2106.01718v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.01718</id>
        <link href="http://arxiv.org/abs/2106.01718"/>
        <updated>2021-06-04T01:12:29.380Z</updated>
        <summary type="html"><![CDATA[Low-electron-dose observation is indispensable for observing various samples
using a transmission electron microscope; consequently, image processing has
been used to improve transmission electron microscopy (TEM) images. To apply
such image processing to in situ observations, we here apply a convolutional
neural network to TEM imaging. Using a dataset that includes short-exposure
images and long-exposure images, we develop a pipeline for processed
short-exposure images, based on end-to-end training. The quality of images
acquired with a total dose of approximately 5 e- per pixel becomes comparable
to that of images acquired with a total dose of approximately 1000 e- per
pixel. Because the conversion time is approximately 8 ms, in situ observation
at 125 fps is possible. This imaging technique enables in situ observation of
electron-beam-sensitive specimens.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Katsuno_H/0/1/0/all/0/1"&gt;Hiroyasu Katsuno&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kimura_Y/0/1/0/all/0/1"&gt;Yuki Kimura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yamazaki_T/0/1/0/all/0/1"&gt;Tomoya Yamazaki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Takigawa_I/0/1/0/all/0/1"&gt;Ichigaku Takigawa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[E2E-VLP: End-to-End Vision-Language Pre-training Enhanced by Visual Learning. (arXiv:2106.01804v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.01804</id>
        <link href="http://arxiv.org/abs/2106.01804"/>
        <updated>2021-06-04T01:12:29.358Z</updated>
        <summary type="html"><![CDATA[Vision-language pre-training (VLP) on large-scale image-text pairs has
achieved huge success for the cross-modal downstream tasks. The most existing
pre-training methods mainly adopt a two-step training procedure, which firstly
employs a pre-trained object detector to extract region-based visual features,
then concatenates the image representation and text embedding as the input of
Transformer to train. However, these methods face problems of using
task-specific visual representation of the specific object detector for generic
cross-modal understanding, and the computation inefficiency of two-stage
pipeline. In this paper, we propose the first end-to-end vision-language
pre-trained model for both V+L understanding and generation, namely E2E-VLP,
where we build a unified Transformer framework to jointly learn visual
representation, and semantic alignments between image and text. We incorporate
the tasks of object detection and image captioning into pre-training with a
unified Transformer encoder-decoder architecture for enhancing visual learning.
An extensive set of experiments have been conducted on well-established
vision-language downstream tasks to demonstrate the effectiveness of this novel
VLP paradigm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1"&gt;Haiyang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_M/0/1/0/all/0/1"&gt;Ming Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chenliang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bi_B/0/1/0/all/0/1"&gt;Bin Bi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1"&gt;Songfang Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_W/0/1/0/all/0/1"&gt;Wenming Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1"&gt;Fei Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Randomized Exploration is Near-Optimal for Tabular MDP. (arXiv:2102.09703v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.09703</id>
        <link href="http://arxiv.org/abs/2102.09703"/>
        <updated>2021-06-04T01:12:29.351Z</updated>
        <summary type="html"><![CDATA[We study exploration using randomized value functions in Thompson Sampling
(TS)-like algorithms in reinforcement learning. This type of algorithms enjoys
appealing empirical performance. We show that when we use 1) a single random
seed in each episode, and 2) a Bernstein-type magnitude of noise, we obtain a
worst-case $\widetilde{O}\left(H\sqrt{SAT}\right)$ regret bound for episodic
time-inhomogeneous Markov Decision Process where $S$ is the size of state
space, $A$ is the size of action space, $H$ is the planning horizon and $T$ is
the number of interactions. This bound polynomially improves all existing
bounds for TS-like algorithms based on randomized value functions, and for the
first time, matches the $\Omega\left(H\sqrt{SAT}\right)$ lower bound up to
logarithmic factors. Our result highlights that randomized exploration can be
near-optimal, which was previously only achieved by optimistic algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_Z/0/1/0/all/0/1"&gt;Zhihan Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_R/0/1/0/all/0/1"&gt;Ruoqi Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_S/0/1/0/all/0/1"&gt;Simon S. Du&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Controllable Guarantees for Fair Outcomes via Contrastive Information Estimation. (arXiv:2101.04108v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.04108</id>
        <link href="http://arxiv.org/abs/2101.04108"/>
        <updated>2021-06-04T01:12:29.345Z</updated>
        <summary type="html"><![CDATA[Controlling bias in training datasets is vital for ensuring equal treatment,
or parity, between different groups in downstream applications. A naive
solution is to transform the data so that it is statistically independent of
group membership, but this may throw away too much information when a
reasonable compromise between fairness and accuracy is desired. Another common
approach is to limit the ability of a particular adversary who seeks to
maximize parity. Unfortunately, representations produced by adversarial
approaches may still retain biases as their efficacy is tied to the complexity
of the adversary used during training. To this end, we theoretically establish
that by limiting the mutual information between representations and protected
attributes, we can assuredly control the parity of any downstream classifier.
We demonstrate an effective method for controlling parity through mutual
information based on contrastive information estimators and show that they
outperform approaches that rely on variational bounds based on complex
generative models. We test our approach on UCI Adult and Heritage Health
datasets and demonstrate that our approach provides more informative
representations across a range of desired parity thresholds while providing
strong theoretical guarantees on the parity of any downstream algorithm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_U/0/1/0/all/0/1"&gt;Umang Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ferber_A/0/1/0/all/0/1"&gt;Aaron M Ferber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dilkina_B/0/1/0/all/0/1"&gt;Bistra Dilkina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Steeg_G/0/1/0/all/0/1"&gt;Greg Ver Steeg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Calibration and Out-of-domain Generalization. (arXiv:2102.10395v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.10395</id>
        <link href="http://arxiv.org/abs/2102.10395"/>
        <updated>2021-06-04T01:12:29.333Z</updated>
        <summary type="html"><![CDATA[Out-of-domain (OOD) generalization is a significant challenge for machine
learning models. Many techniques have been proposed to overcome this challenge,
often focused on learning models with certain invariance properties. In this
work, we draw a link between OOD performance and model calibration, arguing
that calibration across multiple domains can be viewed as a special case of an
invariant representation leading to better OOD generalization. Specifically, we
show that under certain conditions, models which achieve \emph{multi-domain
calibration} are provably free of spurious correlations. This leads us to
propose multi-domain calibration as a measurable and trainable surrogate for
the OOD performance of a classifier. We therefore introduce methods that are
easy to apply and allow practitioners to improve multi-domain calibration by
training or modifying an existing model, leading to better performance on
unseen domains. Using five datasets from the recently proposed WILDS OOD
benchmark, as well as the Colored MNIST dataset, we demonstrate that training
or tuning models so they are calibrated across multiple domains leads to
significantly improved performance on unseen test domains. We believe this
intriguing connection between calibration and OOD generalization is promising
from both a practical and theoretical point of view.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wald_Y/0/1/0/all/0/1"&gt;Yoav Wald&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feder_A/0/1/0/all/0/1"&gt;Amir Feder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Greenfeld_D/0/1/0/all/0/1"&gt;Daniel Greenfeld&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shalit_U/0/1/0/all/0/1"&gt;Uri Shalit&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PDPGD: Primal-Dual Proximal Gradient Descent Adversarial Attack. (arXiv:2106.01538v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01538</id>
        <link href="http://arxiv.org/abs/2106.01538"/>
        <updated>2021-06-04T01:12:29.327Z</updated>
        <summary type="html"><![CDATA[State-of-the-art deep neural networks are sensitive to small input
perturbations. Since the discovery of this intriguing vulnerability, many
defence methods have been proposed that attempt to improve robustness to
adversarial noise. Fast and accurate attacks are required to compare various
defence methods. However, evaluating adversarial robustness has proven to be
extremely challenging. Existing norm minimisation adversarial attacks require
thousands of iterations (e.g. Carlini & Wagner attack), are limited to the
specific norms (e.g. Fast Adaptive Boundary), or produce sub-optimal results
(e.g. Brendel & Bethge attack). On the other hand, PGD attack, which is fast,
general and accurate, ignores the norm minimisation penalty and solves a
simpler perturbation-constrained problem. In this work, we introduce a fast,
general and accurate adversarial attack that optimises the original non-convex
constrained minimisation problem. We interpret optimising the Lagrangian of the
adversarial attack optimisation problem as a two-player game: the first player
minimises the Lagrangian wrt the adversarial noise; the second player maximises
the Lagrangian wrt the regularisation penalty. Our attack algorithm
simultaneously optimises primal and dual variables to find the minimal
adversarial perturbation. In addition, for non-smooth $l_p$-norm minimisation,
such as $l_{\infty}$-, $l_1$-, and $l_0$-norms, we introduce primal-dual
proximal gradient descent attack. We show in the experiments that our attack
outperforms current state-of-the-art $l_{\infty}$-, $l_2$-, $l_1$-, and
$l_0$-attacks on MNIST, CIFAR-10 and Restricted ImageNet datasets against
unregularised and adversarially trained models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Matyasko_A/0/1/0/all/0/1"&gt;Alexander Matyasko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chau_L/0/1/0/all/0/1"&gt;Lap-Pui Chau&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dompteur: Taming Audio Adversarial Examples. (arXiv:2102.05431v2 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.05431</id>
        <link href="http://arxiv.org/abs/2102.05431"/>
        <updated>2021-06-04T01:12:29.309Z</updated>
        <summary type="html"><![CDATA[Adversarial examples seem to be inevitable. These specifically crafted inputs
allow attackers to arbitrarily manipulate machine learning systems. Even worse,
they often seem harmless to human observers. In our digital society, this poses
a significant threat. For example, Automatic Speech Recognition (ASR) systems,
which serve as hands-free interfaces to many kinds of systems, can be attacked
with inputs incomprehensible for human listeners. The research community has
unsuccessfully tried several approaches to tackle this problem. In this paper
we propose a different perspective: We accept the presence of adversarial
examples against ASR systems, but we require them to be perceivable by human
listeners. By applying the principles of psychoacoustics, we can remove
semantically irrelevant information from the ASR input and train a model that
resembles human perception more closely. We implement our idea in a tool named
DOMPTEUR and demonstrate that our augmented system, in contrast to an
unmodified baseline, successfully focuses on perceptible ranges of the input
signal. This change forces adversarial examples into the audible range, while
using minimal computational overhead and preserving benign performance. To
evaluate our approach, we construct an adaptive attacker that actively tries to
avoid our augmentations and demonstrate that adversarial examples from this
attacker remain clearly perceivable. Finally, we substantiate our claims by
performing a hearing test with crowd-sourced human listeners.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Eisenhofer_T/0/1/0/all/0/1"&gt;Thorsten Eisenhofer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schonherr_L/0/1/0/all/0/1"&gt;Lea Sch&amp;#xf6;nherr&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frank_J/0/1/0/all/0/1"&gt;Joel Frank&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Speckemeier_L/0/1/0/all/0/1"&gt;Lars Speckemeier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kolossa_D/0/1/0/all/0/1"&gt;Dorothea Kolossa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Holz_T/0/1/0/all/0/1"&gt;Thorsten Holz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Early Abandoning and Pruning for Elastic Distances including Dynamic Time Warping. (arXiv:2102.05221v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.05221</id>
        <link href="http://arxiv.org/abs/2102.05221"/>
        <updated>2021-06-04T01:12:29.297Z</updated>
        <summary type="html"><![CDATA[Nearest neighbor search under elastic distances is a key tool for time series
analysis, supporting many applications. However, straightforward
implementations of distances require $O(n^2)$ space and time complexities,
preventing these applications from scaling to long series. Much work has been
devoted to speeding up the NN search process, mostly with the development of
lower bounds, allowing to avoid costly distance computations when a given
threshold is exceeded. This threshold, provided by the similarity search
process, also allows to early abandon the computation of a distance itself.
Another approach, is to prune parts of the computation. All these techniques
are othogonal to each other. In this work, we develop a new generic strategy,
"EAPruned", that tightly integrates pruning with early abandoning. We apply it
to six elastic distance measures: DTW, CDTW, WDTW, ERP, MSM and TWE, showing
substantial speedup in NN search applications. Pruning alone also shows
substantial speedup for some distances, benefiting applications beyond the
scope of NN search (e.g. requiring all pairwise distances), and hence where
early abandoning is not applicable. We~release our implementation as part of a
new C++ library for time series classification, along with easy to use
Python/Numpy bindings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Herrmann_M/0/1/0/all/0/1"&gt;Matthieu Herrmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Webb_G/0/1/0/all/0/1"&gt;Geoffrey I. Webb&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Tiny CNN Architecture for Medical Face Mask Detection for Resource-Constrained Endpoints. (arXiv:2011.14858v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.14858</id>
        <link href="http://arxiv.org/abs/2011.14858"/>
        <updated>2021-06-04T01:12:29.202Z</updated>
        <summary type="html"><![CDATA[The world is going through one of the most dangerous pandemics of all time
with the rapid spread of the novel coronavirus (COVID-19). According to the
World Health Organisation, the most effective way to thwart the transmission of
coronavirus is to wear medical face masks. Monitoring the use of face masks in
public places has been a challenge because manual monitoring could be unsafe.
This paper proposes an architecture for detecting medical face masks for
deployment on resource-constrained endpoints having extremely low memory
footprints. A small development board with an ARM Cortex-M7 microcontroller
clocked at 480 Mhz and having just 496 KB of framebuffer RAM, has been used for
the deployment of the model. Using the TensorFlow Lite framework, the model is
quantized to further reduce its size. The proposed model is 138 KB post
quantization and runs at the inference speed of 30 FPS.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mohan_P/0/1/0/all/0/1"&gt;Puranjay Mohan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paul_A/0/1/0/all/0/1"&gt;Aditya Jyoti Paul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chirania_A/0/1/0/all/0/1"&gt;Abhay Chirania&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Memory AMP. (arXiv:2012.10861v3 [cs.IT] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.10861</id>
        <link href="http://arxiv.org/abs/2012.10861"/>
        <updated>2021-06-04T01:12:29.185Z</updated>
        <summary type="html"><![CDATA[Approximate message passing (AMP) is a low-cost iterative
parameter-estimation technique for certain high-dimensional linear systems with
non-Gaussian distributions. However, AMP only applies to independent
identically distributed (IID) transform matrices, but may become unreliable
(e.g. perform poorly or even diverge) for other matrix ensembles, especially
for ill-conditioned ones. To handle this difficulty, orthogonal/vector AMP
(OAMP/VAMP) was proposed for general right-unitarily-invariant matrices.
However, the Bayes-optimal OAMP/VAMP requires high-complexity linear minimum
mean square error (MMSE) estimator. This limits the application of OAMP/VAMP to
large-scale systems.

To solve the disadvantages of AMP and OAMP/VAMP, this paper proposes a memory
AMP (MAMP), in which a long-memory matched filter is proposed for interference
suppression. The complexity of MAMP is comparable to AMP. The asymptotic
Gaussianity of estimation errors in MAMP is guaranteed by the orthogonality
principle. A state evolution is derived to asymptotically characterize the
performance of MAMP. Based on state evolution, the relaxation parameters and
damping vector in MAMP are optimized. For all right-unitarily-invariant
matrices, the optimized MAMP converges to the high-complexity OAMP/VAMP, and
thus is Bayes-optimal if it has a unique fixed point. Finally, simulations are
provided to verify the validity and accuracy of the theoretical results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Lei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1"&gt;Shunqi Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kurkoski_B/0/1/0/all/0/1"&gt;Brian M. Kurkoski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph convolutions that can finally model local structure. (arXiv:2011.15069v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.15069</id>
        <link href="http://arxiv.org/abs/2011.15069"/>
        <updated>2021-06-04T01:12:29.168Z</updated>
        <summary type="html"><![CDATA[Despite quick progress in the last few years, recent studies have shown that
modern graph neural networks can still fail at very simple tasks, like
detecting small cycles. This hints at the fact that current networks fail to
catch information about the local structure, which is problematic if the
downstream task heavily relies on graph substructure analysis, as in the
context of chemistry. We propose a very simple correction to the now standard
GIN convolution that enables the network to detect small cycles with nearly no
cost in terms of computation time and number of parameters. Tested on real life
molecule property datasets, our model consistently improves performance on
large multi-tasked datasets over all baselines, both globally and on a per-task
setting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Brossard_R/0/1/0/all/0/1"&gt;R&amp;#xe9;my Brossard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frigo_O/0/1/0/all/0/1"&gt;Oriel Frigo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dehaene_D/0/1/0/all/0/1"&gt;David Dehaene&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Simultaneous Corn and Soybean Yield Prediction from Remote Sensing Data Using Deep Transfer Learning. (arXiv:2012.03129v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.03129</id>
        <link href="http://arxiv.org/abs/2012.03129"/>
        <updated>2021-06-04T01:12:29.160Z</updated>
        <summary type="html"><![CDATA[Large-scale crop yield estimation is, in part, made possible due to the
availability of remote sensing data allowing for the continuous monitoring of
crops throughout their growth cycle. Having this information allows
stakeholders the ability to make real-time decisions to maximize yield
potential. Although various models exist that predict yield from remote sensing
data, there currently does not exist an approach that can estimate yield for
multiple crops simultaneously, and thus leads to more accurate predictions. A
model that predicts the yield of multiple crops and concurrently considers the
interaction between multiple crop yields. We propose a new convolutional neural
network model called YieldNet which utilizes a novel deep learning framework
that uses transfer learning between corn and soybean yield predictions by
sharing the weights of the backbone feature extractor. Additionally, to
consider the multi-target response variable, we propose a new loss function. We
conduct our experiment using data from 1,132 counties for corn and 1,076
counties for soybean across the United States. Numerical results demonstrate
that our proposed method accurately predicts corn and soybean yield from one to
four months before the harvest with a MAE being 8.74% and 8.70% of the average
yield, respectively, and is competitive to other state-of-the-art approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khaki_S/0/1/0/all/0/1"&gt;Saeed Khaki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pham_H/0/1/0/all/0/1"&gt;Hieu Pham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Lizhi Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-UAV Path Planning for Wireless Data Harvesting with Deep Reinforcement Learning. (arXiv:2010.12461v3 [cs.MA] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.12461</id>
        <link href="http://arxiv.org/abs/2010.12461"/>
        <updated>2021-06-04T01:12:29.153Z</updated>
        <summary type="html"><![CDATA[Harvesting data from distributed Internet of Things (IoT) devices with
multiple autonomous unmanned aerial vehicles (UAVs) is a challenging problem
requiring flexible path planning methods. We propose a multi-agent
reinforcement learning (MARL) approach that, in contrast to previous work, can
adapt to profound changes in the scenario parameters defining the data
harvesting mission, such as the number of deployed UAVs, number, position and
data amount of IoT devices, or the maximum flying time, without the need to
perform expensive recomputations or relearn control policies. We formulate the
path planning problem for a cooperative, non-communicating, and homogeneous
team of UAVs tasked with maximizing collected data from distributed IoT sensor
nodes subject to flying time and collision avoidance constraints. The path
planning problem is translated into a decentralized partially observable Markov
decision process (Dec-POMDP), which we solve through a deep reinforcement
learning (DRL) approach, approximating the optimal UAV control policy without
prior knowledge of the challenging wireless channel characteristics in dense
urban environments. By exploiting a combination of centered global and local
map representations of the environment that are fed into convolutional layers
of the agents, we show that our proposed network architecture enables the
agents to cooperate effectively by carefully dividing the data collection task
among themselves, adapt to large complex environments and state spaces, and
make movement decisions that balance data collection goals, flight-time
efficiency, and navigation constraints. Finally, learning a control policy that
generalizes over the scenario parameter space enables us to analyze the
influence of individual parameters on collection performance and provide some
intuition about system-level benefits.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bayerlein_H/0/1/0/all/0/1"&gt;Harald Bayerlein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Theile_M/0/1/0/all/0/1"&gt;Mirco Theile&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Caccamo_M/0/1/0/all/0/1"&gt;Marco Caccamo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gesbert_D/0/1/0/all/0/1"&gt;David Gesbert&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Decentralized Structural-RNN for Robot Crowd Navigation with Deep Reinforcement Learning. (arXiv:2011.04820v3 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.04820</id>
        <link href="http://arxiv.org/abs/2011.04820"/>
        <updated>2021-06-04T01:12:29.145Z</updated>
        <summary type="html"><![CDATA[Safe and efficient navigation through human crowds is an essential capability
for mobile robots. Previous work on robot crowd navigation assumes that the
dynamics of all agents are known and well-defined. In addition, the performance
of previous methods deteriorates in partially observable environments and
environments with dense crowds. To tackle these problems, we propose
decentralized structural-Recurrent Neural Network (DS-RNN), a novel network
that reasons about spatial and temporal relationships for robot decision making
in crowd navigation. We train our network with model-free deep reinforcement
learning without any expert supervision. We demonstrate that our model
outperforms previous methods in challenging crowd navigation scenarios. We
successfully transfer the policy learned in the simulator to a real-world
TurtleBot 2i.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Shuijing Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_P/0/1/0/all/0/1"&gt;Peixin Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_W/0/1/0/all/0/1"&gt;Weihang Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chakraborty_N/0/1/0/all/0/1"&gt;Neeloy Chakraborty&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Driggs_Campbell_K/0/1/0/all/0/1"&gt;Katherine Driggs-Campbell&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Constraint-Based Algorithm for the Structural Learning of Continuous-Time Bayesian Networks. (arXiv:2007.03248v3 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.03248</id>
        <link href="http://arxiv.org/abs/2007.03248"/>
        <updated>2021-06-04T01:12:29.138Z</updated>
        <summary type="html"><![CDATA[Dynamic Bayesian networks have been well explored in the literature as
discrete-time models: however, their continuous-time extensions have seen
comparatively little attention. In this paper, we propose the first
constraint-based algorithm for learning the structure of continuous-time
Bayesian networks. We discuss the different statistical tests and the
underlying hypotheses used by our proposal to establish conditional
independence. Furthermore, we analyze and discuss the computational complexity
of the best and worst cases for the proposed algorithm. Finally, we validate
its performance using synthetic data, and we discuss its strengths and
limitations comparing it with the score-based structure learning algorithm from
Nodelman et al. (2003). We find the latter to be more accurate in learning
networks with binary variables, while our constraint-based approach is more
accurate with variables assuming more than two values. Numerical experiments
confirm that score-based and constraint-based algorithms are comparable in
terms of computation time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bregoli_A/0/1/0/all/0/1"&gt;Alessandro Bregoli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scutari_M/0/1/0/all/0/1"&gt;Marco Scutari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stella_F/0/1/0/all/0/1"&gt;Fabio Stella&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Window Data Augmentation Approach for Speech Emotion Recognition. (arXiv:2010.09895v3 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.09895</id>
        <link href="http://arxiv.org/abs/2010.09895"/>
        <updated>2021-06-04T01:12:29.132Z</updated>
        <summary type="html"><![CDATA[We present a Multi-Window Data Augmentation (MWA-SER) approach for speech
emotion recognition. MWA-SER is a unimodal approach that focuses on two key
concepts; designing the speech augmentation method and building the deep
learning model to recognize the underlying emotion of an audio signal. Our
proposed multi-window augmentation approach generates additional data samples
from the speech signal by employing multiple window sizes in the audio feature
extraction process. We show that our augmentation method, combined with a deep
learning model, improves speech emotion recognition performance. We evaluate
the performance of our approach on three benchmark datasets: IEMOCAP, SAVEE,
and RAVDESS. We show that the multi-window model improves the SER performance
and outperforms a single-window model. The notion of finding the best window
size is an essential step in audio feature extraction. We perform extensive
experimental evaluations to find the best window choice and explore the
windowing effect for SER analysis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Padi_S/0/1/0/all/0/1"&gt;Sarala Padi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Manocha_D/0/1/0/all/0/1"&gt;Dinesh Manocha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sriram_R/0/1/0/all/0/1"&gt;Ram D.Sriram&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Gradient Boosted Binary Histogram Ensemble for Large-scale Regression. (arXiv:2106.01986v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.01986</id>
        <link href="http://arxiv.org/abs/2106.01986"/>
        <updated>2021-06-04T01:12:29.111Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a gradient boosting algorithm for large-scale
regression problems called \textit{Gradient Boosted Binary Histogram Ensemble}
(GBBHE) based on binary histogram partition and ensemble learning. From the
theoretical perspective, by assuming the H\"{o}lder continuity of the target
function, we establish the statistical convergence rate of GBBHE in the space
$C^{0,\alpha}$ and $C^{1,0}$, where a lower bound of the convergence rate for
the base learner demonstrates the advantage of boosting. Moreover, in the space
$C^{1,0}$, we prove that the number of iterations to achieve the fast
convergence rate can be reduced by using ensemble regressor as the base
learner, which improves the computational efficiency. In the experiments,
compared with other state-of-the-art algorithms such as gradient boosted
regression tree (GBRT), Breiman's forest, and kernel-based methods, our GBBHE
algorithm shows promising performance with less running time on large-scale
datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Hang_H/0/1/0/all/0/1"&gt;Hanyuan Hang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Huang_T/0/1/0/all/0/1"&gt;Tao Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Cai_Y/0/1/0/all/0/1"&gt;Yuchao Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Yang_H/0/1/0/all/0/1"&gt;Hanfang Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lin_Z/0/1/0/all/0/1"&gt;Zhouchen Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Gender Bias in Depression Detection Using Audio Features. (arXiv:2010.15120v2 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.15120</id>
        <link href="http://arxiv.org/abs/2010.15120"/>
        <updated>2021-06-04T01:12:29.104Z</updated>
        <summary type="html"><![CDATA[Depression is a large-scale mental health problem and a challenging area for
machine learning researchers in detection of depression. Datasets such as
Distress Analysis Interview Corpus - Wizard of Oz (DAIC-WOZ) have been created
to aid research in this area. However, on top of the challenges inherent in
accurately detecting depression, biases in datasets may result in skewed
classification performance. In this paper we examine gender bias in the
DAIC-WOZ dataset. We show that gender biases in DAIC-WOZ can lead to an
overreporting of performance. By different concepts from Fair Machine Learning,
such as data re-distribution, and using raw audio features, we can mitigate
against the harmful effects of bias.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bailey_A/0/1/0/all/0/1"&gt;Andrew Bailey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Plumbley_M/0/1/0/all/0/1"&gt;Mark D. Plumbley&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DynamicViT: Efficient Vision Transformers with Dynamic Token Sparsification. (arXiv:2106.02034v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02034</id>
        <link href="http://arxiv.org/abs/2106.02034"/>
        <updated>2021-06-04T01:12:29.096Z</updated>
        <summary type="html"><![CDATA[Attention is sparse in vision transformers. We observe the final prediction
in vision transformers is only based on a subset of most informative tokens,
which is sufficient for accurate image recognition. Based on this observation,
we propose a dynamic token sparsification framework to prune redundant tokens
progressively and dynamically based on the input. Specifically, we devise a
lightweight prediction module to estimate the importance score of each token
given the current features. The module is added to different layers to prune
redundant tokens hierarchically. To optimize the prediction module in an
end-to-end manner, we propose an attention masking strategy to differentiably
prune a token by blocking its interactions with other tokens. Benefiting from
the nature of self-attention, the unstructured sparse tokens are still hardware
friendly, which makes our framework easy to achieve actual speed-up. By
hierarchically pruning 66% of the input tokens, our method greatly reduces
31%~37% FLOPs and improves the throughput by over 40% while the drop of
accuracy is within 0.5% for various vision transformers. Equipped with the
dynamic token sparsification framework, DynamicViT models can achieve very
competitive complexity/accuracy trade-offs compared to state-of-the-art CNNs
and vision transformers on ImageNet. Code is available at
https://github.com/raoyongming/DynamicViT]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rao_Y/0/1/0/all/0/1"&gt;Yongming Rao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1"&gt;Wenliang Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1"&gt;Benlin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1"&gt;Jiwen Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jie Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1"&gt;Cho-Jui Hsieh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reduce and Reconstruct: ASR for Low-Resource Phonetic Languages. (arXiv:2010.09322v2 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.09322</id>
        <link href="http://arxiv.org/abs/2010.09322"/>
        <updated>2021-06-04T01:12:29.090Z</updated>
        <summary type="html"><![CDATA[This work presents a seemingly simple but effective technique to improve
low-resource ASR systems for phonetic languages. By identifying sets of
acoustically similar graphemes in these languages, we first reduce the output
alphabet of the ASR system using linguistically meaningful reductions and then
reconstruct the original alphabet using a standalone module. We demonstrate
that this lessens the burden and improves the performance of low-resource
end-to-end ASR systems (because only reduced-alphabet predictions are needed)
and that it is possible to design a very simple but effective reconstruction
module that recovers sequences in the original alphabet from sequences in the
reduced alphabet. We present a finite state transducer-based reconstruction
module that operates on the 1-best ASR hypothesis in the reduced alphabet. We
demonstrate the efficacy of our proposed technique using ASR systems for two
Indian languages, Gujarati and Telugu. With access to only 10 hrs of speech
data, we obtain relative WER reductions of up to 7% compared to systems that do
not use any reduction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Diwan_A/0/1/0/all/0/1"&gt;Anuj Diwan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jyothi_P/0/1/0/all/0/1"&gt;Preethi Jyothi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Parameterizing Branch-and-Bound Search Trees to Learn Branching Policies. (arXiv:2002.05120v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.05120</id>
        <link href="http://arxiv.org/abs/2002.05120"/>
        <updated>2021-06-04T01:12:29.083Z</updated>
        <summary type="html"><![CDATA[Branch and Bound (B&B) is the exact tree search method typically used to
solve Mixed-Integer Linear Programming problems (MILPs). Learning branching
policies for MILP has become an active research area, with most works proposing
to imitate the strong branching rule and specialize it to distinct classes of
problems. We aim instead at learning a policy that generalizes across
heterogeneous MILPs: our main hypothesis is that parameterizing the state of
the B&B search tree can aid this type of generalization. We propose a novel
imitation learning framework, and introduce new input features and
architectures to represent branching. Experiments on MILP benchmark instances
clearly show the advantages of incorporating an explicit parameterization of
the state of the search tree to modulate the branching decisions, in terms of
both higher accuracy and smaller B&B trees. The resulting policies
significantly outperform the current state-of-the-art method for "learning to
branch" by effectively allowing generalization to generic unseen instances.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zarpellon_G/0/1/0/all/0/1"&gt;Giulia Zarpellon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jo_J/0/1/0/all/0/1"&gt;Jason Jo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lodi_A/0/1/0/all/0/1"&gt;Andrea Lodi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1"&gt;Yoshua Bengio&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Nonconvex Low-Rank Tensor Completion from Noisy Data. (arXiv:1911.04436v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1911.04436</id>
        <link href="http://arxiv.org/abs/1911.04436"/>
        <updated>2021-06-04T01:12:29.065Z</updated>
        <summary type="html"><![CDATA[We study a noisy tensor completion problem of broad practical interest,
namely, the reconstruction of a low-rank tensor from highly incomplete and
randomly corrupted observations of its entries. While a variety of prior work
has been dedicated to this problem, prior algorithms either are computationally
too expensive for large-scale applications, or come with sub-optimal
statistical guarantees. Focusing on "incoherent" and well-conditioned tensors
of a constant CP rank, we propose a two-stage nonconvex algorithm -- (vanilla)
gradient descent following a rough initialization -- that achieves the best of
both worlds. Specifically, the proposed nonconvex algorithm faithfully
completes the tensor and retrieves all individual tensor factors within nearly
linear time, while at the same time enjoying near-optimal statistical
guarantees (i.e. minimal sample complexity and optimal estimation accuracy).
The estimation errors are evenly spread out across all entries, thus achieving
optimal $\ell_{\infty}$ statistical accuracy. We have also discussed how to
extend our approach to accommodate asymmetric tensors. The insight conveyed
through our analysis of nonconvex optimization might have implications for
other tensor estimation problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cai_C/0/1/0/all/0/1"&gt;Changxiao Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1"&gt;Gen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Poor_H/0/1/0/all/0/1"&gt;H. Vincent Poor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yuxin Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BraggNN: Fast X-ray Bragg Peak Analysis Using Deep Learning. (arXiv:2008.08198v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.08198</id>
        <link href="http://arxiv.org/abs/2008.08198"/>
        <updated>2021-06-04T01:12:29.055Z</updated>
        <summary type="html"><![CDATA[X-ray diffraction based microscopy techniques such as High Energy Diffraction
Microscopy rely on knowledge of the position of diffraction peaks with high
precision. These positions are typically computed by fitting the observed
intensities in area detector data to a theoretical peak shape such as
pseudo-Voigt. As experiments become more complex and detector technologies
evolve, the computational cost of such peak detection and shape fitting becomes
the biggest hurdle to the rapid analysis required for real-time feedback during
in-situ experiments. To this end, we propose BraggNN, a deep learning-based
method that can determine peak positions much more rapidly than conventional
pseudo-Voigt peak fitting. When applied to a test dataset, BraggNN gives errors
of less than 0.29 and 0.57 pixels, relative to the conventional method, for 75%
and 95% of the peaks, respectively. When applied to a real experimental
dataset, a 3D reconstruction that used peak positions computed by BraggNN
yields 15% better results on average as compared to a reconstruction obtained
using peak positions determined using conventional 2D pseudo-Voigt fitting.
Recent advances in deep learning method implementations and special-purpose
model inference accelerators allow BraggNN to deliver enormous performance
improvements relative to the conventional method, running, for example, more
than 200 times faster than a conventional method on a consumer-class GPU card
with out-of-the-box software.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhengchun Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sharma_H/0/1/0/all/0/1"&gt;Hemant Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Park_J/0/1/0/all/0/1"&gt;Jun-Sang Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kenesei_P/0/1/0/all/0/1"&gt;Peter Kenesei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Miceli_A/0/1/0/all/0/1"&gt;Antonino Miceli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Almer_J/0/1/0/all/0/1"&gt;Jonathan Almer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kettimuthu_R/0/1/0/all/0/1"&gt;Rajkumar Kettimuthu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Foster_I/0/1/0/all/0/1"&gt;Ian Foster&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stochastic tree ensembles for regularized nonlinear regression. (arXiv:2002.03375v4 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.03375</id>
        <link href="http://arxiv.org/abs/2002.03375"/>
        <updated>2021-06-04T01:12:29.048Z</updated>
        <summary type="html"><![CDATA[This paper develops a novel stochastic tree ensemble method for nonlinear
regression, which we refer to as XBART, short for Accelerated Bayesian Additive
Regression Trees. By combining regularization and stochastic search strategies
from Bayesian modeling with computationally efficient techniques from recursive
partitioning approaches, the new method attains state-of-the-art performance:
in many settings it is both faster and more accurate than the widely-used
XGBoost algorithm. Via careful simulation studies, we demonstrate that our new
approach provides accurate point-wise estimates of the mean function and does
so faster than popular alternatives, such as BART, XGBoost and neural networks
(using Keras). We also prove a number of basic theoretical results about the
new algorithm, including consistency of the single tree version of the model
and stationarity of the Markov chain produced by the ensemble version.
Furthermore, we demonstrate that initializing standard Bayesian additive
regression trees Markov chain Monte Carlo (MCMC) at XBART-fitted trees
considerably improves credible interval coverage and reduces total run-time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+He_J/0/1/0/all/0/1"&gt;Jingyu He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Hahn_P/0/1/0/all/0/1"&gt;P. Richard Hahn&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Global Convergence of Multi-Agent Policy Gradient in Markov Potential Games. (arXiv:2106.01969v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01969</id>
        <link href="http://arxiv.org/abs/2106.01969"/>
        <updated>2021-06-04T01:12:29.041Z</updated>
        <summary type="html"><![CDATA[Potential games are arguably one of the most important and widely studied
classes of normal form games. They define the archetypal setting of multi-agent
coordination as all agent utilities are perfectly aligned with each other via a
common potential function. Can this intuitive framework be transplanted in the
setting of Markov Games? What are the similarities and differences between
multi-agent coordination with and without state dependence? We present a novel
definition of Markov Potential Games (MPG) that generalizes prior attempts at
capturing complex stateful multi-agent coordination. Counter-intuitively,
insights from normal-form potential games do not carry over as MPGs can consist
of settings where state-games can be zero-sum games. In the opposite direction,
Markov games where every state-game is a potential game are not necessarily
MPGs. Nevertheless, MPGs showcase standard desirable properties such as the
existence of deterministic Nash policies. In our main technical result, we
prove fast convergence of independent policy gradient to Nash policies by
adapting recent gradient dominance property arguments developed for single
agent MDPs to multi-agent learning settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Leonardos_S/0/1/0/all/0/1"&gt;Stefanos Leonardos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Overman_W/0/1/0/all/0/1"&gt;Will Overman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Panageas_I/0/1/0/all/0/1"&gt;Ioannis Panageas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Piliouras_G/0/1/0/all/0/1"&gt;Georgios Piliouras&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Nonlinear Matrix Approximation with Radial Basis Function Components. (arXiv:2106.02018v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02018</id>
        <link href="http://arxiv.org/abs/2106.02018"/>
        <updated>2021-06-04T01:12:29.034Z</updated>
        <summary type="html"><![CDATA[We introduce and investigate matrix approximation by decomposition into a sum
of radial basis function (RBF) components. An RBF component is a generalization
of the outer product between a pair of vectors, where an RBF function replaces
the scalar multiplication between individual vector elements. Even though the
RBF functions are positive definite, the summation across components is not
restricted to convex combinations and allows us to compute the decomposition
for any real matrix that is not necessarily symmetric or positive definite. We
formulate the problem of seeking such a decomposition as an optimization
problem with a nonlinear and non-convex loss function. Several modern versions
of the gradient descent method, including their scalable stochastic
counterparts, are used to solve this problem. We provide extensive empirical
evidence of the effectiveness of the RBF decomposition and that of the
gradient-based fitting algorithm. While being conceptually motivated by
singular value decomposition (SVD), our proposed nonlinear counterpart
outperforms SVD by drastically reducing the memory required to approximate a
data matrix with the same $L_2$-error for a wide range of matrix types. For
example, it leads to 2 to 10 times memory save for Gaussian noise, graph
adjacency matrices, and kernel matrices. Moreover, this proximity-based
decomposition can offer additional interpretability in applications that
involve, e.g., capturing the inner low-dimensional structure of the data,
retaining graph connectivity structure, and preserving the acutance of images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rebrova_E/0/1/0/all/0/1"&gt;Elizaveta Rebrova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1"&gt;Yu-Hang Tang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Off-Policy Evaluation via Adaptive Weighting with Data from Contextual Bandits. (arXiv:2106.02029v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.02029</id>
        <link href="http://arxiv.org/abs/2106.02029"/>
        <updated>2021-06-04T01:12:29.017Z</updated>
        <summary type="html"><![CDATA[It has become increasingly common for data to be collected adaptively, for
example using contextual bandits. Historical data of this type can be used to
evaluate other treatment assignment policies to guide future innovation or
experiments. However, policy evaluation is challenging if the target policy
differs from the one used to collect data, and popular estimators, including
doubly robust (DR) estimators, can be plagued by bias, excessive variance, or
both. In particular, when the pattern of treatment assignment in the collected
data looks little like the pattern generated by the policy to be evaluated, the
importance weights used in DR estimators explode, leading to excessive
variance.

In this paper, we improve the DR estimator by adaptively weighting
observations to control its variance. We show that a t-statistic based on our
improved estimator is asymptotically normal under certain conditions, allowing
us to form confidence intervals and test hypotheses. Using synthetic data and
public benchmarks, we provide empirical evidence for our estimator's improved
accuracy and inferential properties relative to existing alternatives.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Zhan_R/0/1/0/all/0/1"&gt;Ruohan Zhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Hadad_V/0/1/0/all/0/1"&gt;Vitor Hadad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Hirshberg_D/0/1/0/all/0/1"&gt;David A. Hirshberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Athey_S/0/1/0/all/0/1"&gt;Susan Athey&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Learning of KB Queries in Task-Oriented Dialogs. (arXiv:2005.00123v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.00123</id>
        <link href="http://arxiv.org/abs/2005.00123"/>
        <updated>2021-06-04T01:12:29.010Z</updated>
        <summary type="html"><![CDATA[Task-oriented dialog (TOD) systems often need to formulate knowledge base
(KB) queries corresponding to the user intent and use the query results to
generate system responses. Existing approaches require dialog datasets to
explicitly annotate these KB queries -- these annotations can be time
consuming, and expensive. In response, we define the novel problems of
predicting the KB query and training the dialog agent, without explicit KB
query annotation. For query prediction, we propose a reinforcement learning
(RL) baseline, which rewards the generation of those queries whose KB results
cover the entities mentioned in subsequent dialog. Further analysis reveals
that correlation among query attributes in KB can significantly confuse memory
augmented policy optimization (MAPO), an existing state of the art RL agent. To
address this, we improve the MAPO baseline with simple but important
modifications suited to our task. To train the full TOD system for our setting,
we propose a pipelined approach: it independently predicts when to make a KB
query (query position predictor), then predicts a KB query at the predicted
position (query predictor), and uses the results of predicted query in
subsequent dialog (next response predictor). Overall, our work proposes first
solutions to our novel problem, and our analysis highlights the research
challenges in training TOD systems without query annotation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Raghu_D/0/1/0/all/0/1"&gt;Dinesh Raghu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_N/0/1/0/all/0/1"&gt;Nikhil Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mausam/0/1/0/all/0/1"&gt;Mausam&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exposing Backdoors in Robust Machine Learning Models. (arXiv:2003.00865v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.00865</id>
        <link href="http://arxiv.org/abs/2003.00865"/>
        <updated>2021-06-04T01:12:29.003Z</updated>
        <summary type="html"><![CDATA[The introduction of robust optimisation has pushed the state-of-the-art in
defending against adversarial attacks. However, the behaviour of such
optimisation has not been studied in the light of a fundamentally different
class of attacks called backdoors. In this paper, we demonstrate that
adversarially robust models are susceptible to backdoor attacks. Subsequently,
we observe that backdoors are reflected in the feature representation of such
models. Then, this observation is leveraged to detect backdoor-infected models
via a detection technique called AEGIS. Specifically, AEGIS uses feature
clustering to effectively detect backdoor-infected robust Deep Neural Networks
(DNNs). In our evaluation of several visible and hidden backdoor triggers on
major classification tasks using CIFAR-10, MNIST and FMNIST datasets, AEGIS
effectively detects robust DNNs infected with backdoors. AEGIS detects a
backdoor-infected model with 91.6% accuracy, without any false positives.
Furthermore, AEGIS detects the targeted class in the backdoor-infected model
with a reasonably low (11.1%) false positive rate. Our investigation reveals
that salient features of adversarially robust DNNs break the stealthy nature of
backdoor attacks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Soremekun_E/0/1/0/all/0/1"&gt;Ezekiel Soremekun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Udeshi_S/0/1/0/all/0/1"&gt;Sakshi Udeshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chattopadhyay_S/0/1/0/all/0/1"&gt;Sudipta Chattopadhyay&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Dataset and Baselines for Multilingual Reply Suggestion. (arXiv:2106.02017v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.02017</id>
        <link href="http://arxiv.org/abs/2106.02017"/>
        <updated>2021-06-04T01:12:28.996Z</updated>
        <summary type="html"><![CDATA[Reply suggestion models help users process emails and chats faster. Previous
work only studies English reply suggestion. Instead, we present MRS, a
multilingual reply suggestion dataset with ten languages. MRS can be used to
compare two families of models: 1) retrieval models that select the reply from
a fixed set and 2) generation models that produce the reply from scratch.
Therefore, MRS complements existing cross-lingual generalization benchmarks
that focus on classification and sequence labeling tasks. We build a generation
model and a retrieval model as baselines for MRS. The two models have different
strengths in the monolingual setting, and they require different strategies to
generalize across languages. MRS is publicly available at
https://github.com/zhangmozhi/mrs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Mozhi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deb_B/0/1/0/all/0/1"&gt;Budhaditya Deb&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_G/0/1/0/all/0/1"&gt;Guoqing Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shokouhi_M/0/1/0/all/0/1"&gt;Milad Shokouhi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Awadallah_A/0/1/0/all/0/1"&gt;Ahmed Hassan Awadallah&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Gaussian Processes on Hypergraphs. (arXiv:2106.01982v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.01982</id>
        <link href="http://arxiv.org/abs/2106.01982"/>
        <updated>2021-06-04T01:12:28.989Z</updated>
        <summary type="html"><![CDATA[We derive a Matern Gaussian process (GP) on the vertices of a hypergraph.
This enables estimation of regression models of observed or latent values
associated with the vertices, in which the correlation and uncertainty
estimates are informed by the hypergraph structure. We further present a
framework for embedding the vertices of a hypergraph into a latent space using
the hypergraph GP. Finally, we provide a scheme for identifying a small number
of representative inducing vertices that enables scalable inference through
sparse GPs. We demonstrate the utility of our framework on three challenging
real-world problems that concern multi-class classification for the political
party affiliation of legislators on the basis of voting behaviour,
probabilistic matrix factorisation of movie reviews, and embedding a hypergraph
of animals into a low-dimensional latent space.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Pinder_T/0/1/0/all/0/1"&gt;Thomas Pinder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Turnbull_K/0/1/0/all/0/1"&gt;Kathryn Turnbull&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Nemeth_C/0/1/0/all/0/1"&gt;Christopher Nemeth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Leslie_D/0/1/0/all/0/1"&gt;David Leslie&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Gaussian Variational State Estimation for Nonlinear State-Space Models. (arXiv:2002.02620v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.02620</id>
        <link href="http://arxiv.org/abs/2002.02620"/>
        <updated>2021-06-04T01:12:28.971Z</updated>
        <summary type="html"><![CDATA[In this paper, the problem of state estimation, in the context of both
filtering and smoothing, for nonlinear state-space models is considered. Due to
the nonlinear nature of the models, the state estimation problem is generally
intractable as it involves integrals of general nonlinear functions and the
filtered and smoothed state distributions lack closed-form solutions. As such,
it is common to approximate the state estimation problem. In this paper, we
develop an assumed Gaussian solution based on variational inference, which
offers the key advantage of a flexible, but principled, mechanism for
approximating the required distributions. Our main contribution lies in a new
formulation of the state estimation problem as an optimisation problem, which
can then be solved using standard optimisation routines that employ exact
first- and second-order derivatives. The resulting state estimation approach
involves a minimal number of assumptions and applies directly to nonlinear
systems with both Gaussian and non-Gaussian probabilistic models. The
performance of our approach is demonstrated on several examples; a challenging
scalar system, a model of a simple robotic system, and a target tracking
problem using a von Mises-Fisher distribution and outperforms alternative
assumed Gaussian approaches to state estimation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Courts_J/0/1/0/all/0/1"&gt;Jarrad Courts&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wills_A/0/1/0/all/0/1"&gt;Adrian Wills&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Schon_T/0/1/0/all/0/1"&gt;Thomas B. Sch&amp;#xf6;n&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TorchIO: a Python library for efficient loading, preprocessing, augmentation and patch-based sampling of medical images in deep learning. (arXiv:2003.04696v4 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.04696</id>
        <link href="http://arxiv.org/abs/2003.04696"/>
        <updated>2021-06-04T01:12:28.964Z</updated>
        <summary type="html"><![CDATA[Processing of medical images such as MRI or CT presents unique challenges
compared to RGB images typically used in computer vision. These include a lack
of labels for large datasets, high computational costs, and metadata to
describe the physical properties of voxels. Data augmentation is used to
artificially increase the size of the training datasets. Training with image
patches decreases the need for computational power. Spatial metadata needs to
be carefully taken into account in order to ensure a correct alignment of
volumes.

We present TorchIO, an open-source Python library to enable efficient
loading, preprocessing, augmentation and patch-based sampling of medical images
for deep learning. TorchIO follows the style of PyTorch and integrates standard
medical image processing libraries to efficiently process images during
training of neural networks. TorchIO transforms can be composed, reproduced,
traced and extended. We provide multiple generic preprocessing and augmentation
operations as well as simulation of MRI-specific artifacts.

Source code, comprehensive tutorials and extensive documentation for TorchIO
can be found at https://github.com/fepegar/torchio. The package can be
installed from the Python Package Index running 'pip install torchio'. It
includes a command-line interface which allows users to apply transforms to
image files without using Python. Additionally, we provide a graphical
interface within a TorchIO extension in 3D Slicer to visualize the effects of
transforms.

TorchIO was developed to help researchers standardize medical image
processing pipelines and allow them to focus on the deep learning experiments.
It encourages open science, as it supports reproducibility and is version
controlled so that the software can be cited precisely. Due to its modularity,
the library is compatible with other frameworks for deep learning with medical
images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Perez_Garcia_F/0/1/0/all/0/1"&gt;Fernando P&amp;#xe9;rez-Garc&amp;#xed;a&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sparks_R/0/1/0/all/0/1"&gt;Rachel Sparks&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ourselin_S/0/1/0/all/0/1"&gt;S&amp;#xe9;bastien Ourselin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reinforcement Learning as One Big Sequence Modeling Problem. (arXiv:2106.02039v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.02039</id>
        <link href="http://arxiv.org/abs/2106.02039"/>
        <updated>2021-06-04T01:12:28.957Z</updated>
        <summary type="html"><![CDATA[Reinforcement learning (RL) is typically concerned with estimating
single-step policies or single-step models, leveraging the Markov property to
factorize the problem in time. However, we can also view RL as a sequence
modeling problem, with the goal being to predict a sequence of actions that
leads to a sequence of high rewards. Viewed in this way, it is tempting to
consider whether powerful, high-capacity sequence prediction models that work
well in other domains, such as natural-language processing, can also provide
simple and effective solutions to the RL problem. To this end, we explore how
RL can be reframed as "one big sequence modeling" problem, using
state-of-the-art Transformer architectures to model distributions over
sequences of states, actions, and rewards. Addressing RL as a sequence modeling
problem significantly simplifies a range of design decisions: we no longer
require separate behavior policy constraints, as is common in prior work on
offline model-free RL, and we no longer require ensembles or other epistemic
uncertainty estimators, as is common in prior work on model-based RL. All of
these roles are filled by the same Transformer sequence model. In our
experiments, we demonstrate the flexibility of this approach across
long-horizon dynamics prediction, imitation learning, goal-conditioned RL, and
offline RL.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Janner_M/0/1/0/all/0/1"&gt;Michael Janner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1"&gt;Qiyang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1"&gt;Sergey Levine&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interactive Refinement of Cross-Lingual Word Embeddings. (arXiv:1911.03070v4 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1911.03070</id>
        <link href="http://arxiv.org/abs/1911.03070"/>
        <updated>2021-06-04T01:12:28.950Z</updated>
        <summary type="html"><![CDATA[Cross-lingual word embeddings transfer knowledge between languages: models
trained on high-resource languages can predict in low-resource languages. We
introduce CLIME, an interactive system to quickly refine cross-lingual word
embeddings for a given classification problem. First, CLIME ranks words by
their salience to the downstream task. Then, users mark similarity between
keywords and their nearest neighbors in the embedding space. Finally, CLIME
updates the embeddings using the annotations. We evaluate CLIME on identifying
health-related text in four low-resource languages: Ilocano, Sinhalese,
Tigrinya, and Uyghur. Embeddings refined by CLIME capture more nuanced word
semantics and have higher test accuracy than the original embeddings. CLIME
often improves accuracy faster than an active learning baseline and can be
easily combined with active learning to improve results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_M/0/1/0/all/0/1"&gt;Michelle Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Mozhi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Durme_B/0/1/0/all/0/1"&gt;Benjamin Van Durme&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Findlater_L/0/1/0/all/0/1"&gt;Leah Findlater&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boyd_Graber_J/0/1/0/all/0/1"&gt;Jordan Boyd-Graber&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MISIM: A Neural Code Semantics Similarity System Using the Context-Aware Semantics Structure. (arXiv:2006.05265v6 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.05265</id>
        <link href="http://arxiv.org/abs/2006.05265"/>
        <updated>2021-06-04T01:12:28.943Z</updated>
        <summary type="html"><![CDATA[Code semantics similarity can be used for many tasks such as code
recommendation, automated software defect correction, and clone detection. Yet,
the accuracy of such systems has not yet reached a level of general purpose
reliability. To help address this, we present Machine Inferred Code Similarity
(MISIM), a neural code semantics similarity system consisting of two core
components: (i)MISIM uses a novel context-aware semantics structure, which was
purpose-built to lift semantics from code syntax; (ii)MISIM uses an extensible
neural code similarity scoring algorithm, which can be used for various neural
network architectures with learned parameters. We compare MISIM to four
state-of-the-art systems, including two additional hand-customized models, over
328K programs consisting of over 18 million lines of code. Our experiments show
that MISIM has 8.08% better accuracy (using MAP@R) compared to the next best
performing system.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ye_F/0/1/0/all/0/1"&gt;Fangke Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1"&gt;Shengtian Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Venkat_A/0/1/0/all/0/1"&gt;Anand Venkat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marcus_R/0/1/0/all/0/1"&gt;Ryan Marcus&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tatbul_N/0/1/0/all/0/1"&gt;Nesime Tatbul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tithi_J/0/1/0/all/0/1"&gt;Jesmin Jahan Tithi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hasabnis_N/0/1/0/all/0/1"&gt;Niranjan Hasabnis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Petersen_P/0/1/0/all/0/1"&gt;Paul Petersen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mattson_T/0/1/0/all/0/1"&gt;Timothy Mattson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kraska_T/0/1/0/all/0/1"&gt;Tim Kraska&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dubey_P/0/1/0/all/0/1"&gt;Pradeep Dubey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sarkar_V/0/1/0/all/0/1"&gt;Vivek Sarkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gottschlich_J/0/1/0/all/0/1"&gt;Justin Gottschlich&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quantum correlation alignment for unsupervised domain adaptation. (arXiv:2005.03355v4 [quant-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.03355</id>
        <link href="http://arxiv.org/abs/2005.03355"/>
        <updated>2021-06-04T01:12:28.925Z</updated>
        <summary type="html"><![CDATA[Correlation alignment (CORAL), a representative domain adaptation (DA)
algorithm, decorrelates and aligns a labelled source domain dataset to an
unlabelled target domain dataset to minimize the domain shift such that a
classifier can be applied to predict the target domain labels. In this paper,
we implement the CORAL on quantum devices by two different methods. One method
utilizes quantum basic linear algebra subroutines (QBLAS) to implement the
CORAL with exponential speedup in the number and dimension of the given data
samples. The other method is achieved through a variational hybrid
quantum-classical procedure. In addition, the numerical experiments of the
CORAL with three different types of data sets, namely the synthetic data, the
synthetic-Iris data, the handwritten digit data, are presented to evaluate the
performance of our work. The simulation results prove that the variational
quantum correlation alignment algorithm (VQCORAL) can achieve competitive
performance compared with the classical CORAL.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+He_X/0/1/0/all/0/1"&gt;Xi He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Survey on Optimal Transport for Machine Learning: Theory and Applications. (arXiv:2106.01963v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01963</id>
        <link href="http://arxiv.org/abs/2106.01963"/>
        <updated>2021-06-04T01:12:28.917Z</updated>
        <summary type="html"><![CDATA[Optimal Transport (OT) theory has seen an increasing amount of attention from
the computer science community due to its potency and relevance in modeling and
machine learning. It introduces means that serve as powerful ways to compare
probability distributions with each other, as well as producing optimal
mappings to minimize cost functions. In this survey, we present a brief
introduction and history, a survey of previous work and propose directions of
future study. We will begin by looking at the history of optimal transport and
introducing the founders of this field. We then give a brief glance into the
algorithms related to OT. Then, we will follow up with a mathematical
formulation and the prerequisites to understand OT. These include Kantorovich
duality, entropic regularization, KL Divergence, and Wassertein barycenters.
Since OT is a computationally expensive problem, we then introduce the
entropy-regularized version of computing optimal mappings, which allowed OT
problems to become applicable in a wide range of machine learning problems. In
fact, the methods generated from OT theory are competitive with the current
state-of-the-art methods. We follow this up by breaking down research papers
that focus on image processing, graph learning, neural architecture search,
document representation, and domain adaptation. We close the paper with a small
section on future research. Of the recommendations presented, three main
problems are fundamental to allow OT to become widely applicable but rely
strongly on its mathematical formulation and thus are hardest to answer. Since
OT is a novel method, there is plenty of space for new research, and with more
and more competitive methods (either on an accuracy level or computational
speed level) being created, the future of applied optimal transport is bright
as it has become pervasive in machine learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Torres_L/0/1/0/all/0/1"&gt;Luis Caicedo Torres&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pereira_L/0/1/0/all/0/1"&gt;Luiz Manella Pereira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Amini_M/0/1/0/all/0/1"&gt;M. Hadi Amini&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Safe Active Dynamics Learning and Control: A Sequential Exploration-Exploitation Framework. (arXiv:2008.11700v3 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.11700</id>
        <link href="http://arxiv.org/abs/2008.11700"/>
        <updated>2021-06-04T01:12:28.909Z</updated>
        <summary type="html"><![CDATA[Safe deployment of autonomous robots in diverse scenarios requires agents
that are capable of efficiently adapting to new environments while satisfying
constraints. In this work, we propose a practical and theoretically-justified
approach to maintaining safety in the presence of dynamics uncertainty. Our
approach leverages Bayesian meta-learning with last-layer adaptation: the
expressiveness of neural-network features trained offline, paired with
efficient last-layer online adaptation, enables the derivation of tight
confidence sets which contract around the true dynamics as the model adapts
online. We exploit these confidence sets to plan trajectories that guarantee
the safety of the system. Our approach handles problems with high dynamics
uncertainty where reaching the goal safely is initially infeasible by first
exploring to gather data and reduce uncertainty, before autonomously exploiting
the acquired information to safely perform the task. Under reasonable
assumptions, we prove that our framework has high-probability guarantees of
satisfying all constraints at all times jointly. This analysis also motivates
two regularizers of last-layer meta-learners that improve online adaptation
capabilities as well as performance by reducing the size of the confidence
sets. We extensively demonstrate our approach in simulation and on hardware.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lew_T/0/1/0/all/0/1"&gt;Thomas Lew&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1"&gt;Apoorva Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Harrison_J/0/1/0/all/0/1"&gt;James Harrison&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bylard_A/0/1/0/all/0/1"&gt;Andrew Bylard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pavone_M/0/1/0/all/0/1"&gt;Marco Pavone&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LyricJam: A system for generating lyrics for live instrumental music. (arXiv:2106.01960v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2106.01960</id>
        <link href="http://arxiv.org/abs/2106.01960"/>
        <updated>2021-06-04T01:12:28.902Z</updated>
        <summary type="html"><![CDATA[We describe a real-time system that receives a live audio stream from a jam
session and generates lyric lines that are congruent with the live music being
played. Two novel approaches are proposed to align the learned latent spaces of
audio and text representations that allow the system to generate novel lyric
lines matching live instrumental music. One approach is based on adversarial
alignment of latent representations of audio and lyrics, while the other
approach learns to transfer the topology from the music latent space to the
lyric latent space. A user study with music artists using the system showed
that the system was useful not only in lyric composition, but also encouraged
the artists to improvise and find new musical expressions. Another user study
demonstrated that users preferred the lines generated using the proposed
methods to the lines generated by a baseline model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vechtomova_O/0/1/0/all/0/1"&gt;Olga Vechtomova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sahu_G/0/1/0/all/0/1"&gt;Gaurav Sahu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_D/0/1/0/all/0/1"&gt;Dhruv Kumar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine Learning and Variational Algorithms for Lattice Field Theory. (arXiv:2106.01975v1 [hep-lat])]]></title>
        <id>http://arxiv.org/abs/2106.01975</id>
        <link href="http://arxiv.org/abs/2106.01975"/>
        <updated>2021-06-04T01:12:28.895Z</updated>
        <summary type="html"><![CDATA[In lattice quantum field theory studies, parameters defining the lattice
theory must be tuned toward criticality to access continuum physics. Commonly
used Markov chain Monte Carlo (MCMC) methods suffer from critical slowing down
in this limit, restricting the precision of continuum extrapolations. Further
difficulties arise when measuring correlation functions of operators widely
separated in spacetime: for most correlation functions, an exponentially severe
signal-to-noise problem is encountered as the operators are taken to be widely
separated. This dissertation details two new techniques to address these
issues. First, we define a novel MCMC algorithm based on generative flow-based
models. Such models utilize machine learning methods to describe efficient
approximate samplers for distributions of interest. Independently drawn
flow-based samples are then used as proposals in an asymptotically exact
Metropolis-Hastings Markov chain. We address incorporating symmetries of
interest, including translational and gauge symmetries. We secondly introduce
an approach to "deform" Monte Carlo estimators based on contour deformations
applied to the domain of the path integral. The deformed estimators associated
with an observable give equivalent unbiased measurements of that observable,
but generically have different variances. We define families of deformed
manifolds for lattice gauge theories and introduce methods to efficiently
optimize the choice of manifold (the "observifold"), minimizing the deformed
observable variance. Finally, we demonstrate that flow-based MCMC can mitigate
critical slowing down and observifolds can exponentially reduce variance in
proof-of-principle applications to scalar $\phi^4$ theory and $\mathrm{U}(1)$
and $\mathrm{SU}(N)$ lattice gauge theories.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/hep-lat/1/au:+Kanwar_G/0/1/0/all/0/1"&gt;Gurtej Kanwar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Low-Rank Semidefinite Programming with Robust Loss Functions. (arXiv:1905.04629v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1905.04629</id>
        <link href="http://arxiv.org/abs/1905.04629"/>
        <updated>2021-06-04T01:12:28.875Z</updated>
        <summary type="html"><![CDATA[In real-world applications, it is important for machine learning algorithms
to be robust against data outliers or corruptions. In this paper, we focus on
improving the robustness of a large class of learning algorithms that are
formulated as low-rank semi-definite programming (SDP) problems. Traditional
formulations use square loss, which is notorious for being sensitive to
outliers. We propose to replace this with more robust noise models, including
the $\ell_1$-loss and other nonconvex losses. However, the resultant
optimization problem becomes difficult as the objective is no longer convex or
smooth. To alleviate this problem, we design an efficient algorithm based on
majorization-minimization. The crux is on constructing a good optimization
surrogate, and we show that this surrogate can be efficiently obtained by the
alternating direction method of multipliers (ADMM). By properly monitoring
ADMM's convergence, the proposed algorithm is empirically efficient and also
theoretically guaranteed to converge to a critical point. Extensive experiments
are performed on four machine learning applications using both synthetic and
real-world data sets. Results show that the proposed algorithm is not only fast
but also has better performance than the state-of-the-art.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yao_Q/0/1/0/all/0/1"&gt;Quanming Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Hangsi Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_E/0/1/0/all/0/1"&gt;En-Liang Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kwok_J/0/1/0/all/0/1"&gt;James Kwok&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ProtoRes: Proto-Residual Architecture for Deep Modeling of Human Pose. (arXiv:2106.01981v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.01981</id>
        <link href="http://arxiv.org/abs/2106.01981"/>
        <updated>2021-06-04T01:12:28.860Z</updated>
        <summary type="html"><![CDATA[Our work focuses on the development of a learnable neural representation of
human pose for advanced AI assisted animation tooling. Specifically, we tackle
the problem of constructing a full static human pose based on sparse and
variable user inputs (e.g. locations and/or orientations of a subset of body
joints). To solve this problem, we propose a novel neural architecture that
combines residual connections with prototype encoding of a partially specified
pose to create a new complete pose from the learned latent space. We show that
our architecture outperforms a baseline based on Transformer, both in terms of
accuracy and computational efficiency. Additionally, we develop a user
interface to integrate our neural model in Unity, a real-time 3D development
platform. Furthermore, we introduce two new datasets representing the static
human pose modeling problem, based on high-quality human motion capture data,
which will be released publicly along with model code.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Oreshkin_B/0/1/0/all/0/1"&gt;Boris N. Oreshkin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bocquelet_F/0/1/0/all/0/1"&gt;Florent Bocquelet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Harvey_F/0/1/0/all/0/1"&gt;F&amp;#xe9;lix H. Harvey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raitt_B/0/1/0/all/0/1"&gt;Bay Raitt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Laflamme_D/0/1/0/all/0/1"&gt;Dominic Laflamme&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeepCompress: Efficient Point Cloud Geometry Compression. (arXiv:2106.01504v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.01504</id>
        <link href="http://arxiv.org/abs/2106.01504"/>
        <updated>2021-06-04T01:12:28.845Z</updated>
        <summary type="html"><![CDATA[Point clouds are a basic data type that is increasingly of interest as 3D
content becomes more ubiquitous. Applications using point clouds include
virtual, augmented, and mixed reality and autonomous driving. We propose a more
efficient deep learning-based encoder architecture for point clouds compression
that incorporates principles from established 3D object detection and image
compression architectures. Through an ablation study, we show that
incorporating the learned activation function from Computational Efficient
Neural Image Compression (CENIC) and designing more parameter-efficient
convolutional blocks yields dramatic gains in efficiency and performance. Our
proposed architecture incorporates Generalized Divisive Normalization
activations and propose a spatially separable InceptionV4-inspired block. We
then evaluate rate-distortion curves on the standard JPEG Pleno 8i Voxelized
Full Bodies dataset to evaluate our model's performance. Our proposed
modifications outperform the baseline approaches by a small margin in terms of
Bjontegard delta rate and PSNR values, yet reduces necessary encoder
convolution operations by 8 percent and reduces total encoder parameters by 20
percent. Our proposed architecture, when considered on its own, has a small
penalty of 0.02 percent in Chamfer's Distance and 0.32 percent increased bit
rate in Point to Plane Distance for the same peak signal-to-noise ratio.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Killea_R/0/1/0/all/0/1"&gt;Ryan Killea&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yun Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bastani_S/0/1/0/all/0/1"&gt;Saeed Bastani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McLachlan_P/0/1/0/all/0/1"&gt;Paul McLachlan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multiplierless MP-Kernel Machine For Energy-efficient Edge Devices. (arXiv:2106.01958v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01958</id>
        <link href="http://arxiv.org/abs/2106.01958"/>
        <updated>2021-06-04T01:12:28.743Z</updated>
        <summary type="html"><![CDATA[We present a novel framework for designing multiplierless kernel machines
that can be used on resource-constrained platforms like intelligent edge
devices. The framework uses a piecewise linear (PWL) approximation based on a
margin propagation (MP) technique and uses only addition/subtraction, shift,
comparison, and register underflow/overflow operations. We propose a
hardware-friendly MP-based inference and online training algorithm that has
been optimized for a Field Programmable Gate Array (FPGA) platform. Our FPGA
implementation eliminates the need for DSP units and reduces the number of
LUTs. By reusing the same hardware for inference and training, we show that the
platform can overcome classification errors and local minima artifacts that
result from the MP approximation. Using the FPGA platform, we also show that
the proposed multiplierless MP-kernel machine demonstrates superior performance
in terms of power, performance, and area compared to other comparable
implementations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nair_A/0/1/0/all/0/1"&gt;Abhishek Ramdas Nair&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nath_P/0/1/0/all/0/1"&gt;Pallab Kumar Nath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chakrabartty_S/0/1/0/all/0/1"&gt;Shantanu Chakrabartty&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thakur_C/0/1/0/all/0/1"&gt;Chetan Singh Thakur&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Do Neural Optimal Transport Solvers Work? A Continuous Wasserstein-2 Benchmark. (arXiv:2106.01954v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01954</id>
        <link href="http://arxiv.org/abs/2106.01954"/>
        <updated>2021-06-04T01:12:28.689Z</updated>
        <summary type="html"><![CDATA[Despite the recent popularity of neural network-based solvers for optimal
transport (OT), there is no standard quantitative way to evaluate their
performance. In this paper, we address this issue for quadratic-cost transport
-- specifically, computation of the Wasserstein-2 distance, a commonly-used
formulation of optimal transport in machine learning. To overcome the challenge
of computing ground truth transport maps between continuous measures needed to
assess these solvers, we use input-convex neural networks (ICNN) to construct
pairs of measures whose ground truth OT maps can be obtained analytically. This
strategy yields pairs of continuous benchmark measures in high-dimensional
spaces such as spaces of images. We thoroughly evaluate existing optimal
transport solvers using these benchmark measures. Even though these solvers
perform well in downstream tasks, many do not faithfully recover optimal
transport maps. To investigate the cause of this discrepancy, we further test
the solvers in a setting of image generation. Our study reveals crucial
limitations of existing solvers and shows that increased OT accuracy does not
necessarily correlate to better results downstream.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Korotin_A/0/1/0/all/0/1"&gt;Alexander Korotin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lingxiao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Genevay_A/0/1/0/all/0/1"&gt;Aude Genevay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Solomon_J/0/1/0/all/0/1"&gt;Justin Solomon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Filippov_A/0/1/0/all/0/1"&gt;Alexander Filippov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Burnaev_E/0/1/0/all/0/1"&gt;Evgeny Burnaev&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimization-Based Algebraic Multigrid Coarsening Using Reinforcement Learning. (arXiv:2106.01854v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01854</id>
        <link href="http://arxiv.org/abs/2106.01854"/>
        <updated>2021-06-04T01:12:28.682Z</updated>
        <summary type="html"><![CDATA[Large sparse linear systems of equations are ubiquitous in science and
engineering, such as those arising from discretizations of partial differential
equations. Algebraic multigrid (AMG) methods are one of the most common methods
of solving such linear systems, with an extensive body of underlying
mathematical theory. A system of linear equations defines a graph on the set of
unknowns and each level of a multigrid solver requires the selection of an
appropriate coarse graph along with restriction and interpolation operators
that map to and from the coarse representation. The efficiency of the multigrid
solver depends critically on this selection and many selection methods have
been developed over the years. Recently, it has been demonstrated that it is
possible to directly learn the AMG interpolation and restriction operators,
given a coarse graph selection. In this paper, we consider the complementary
problem of learning to coarsen graphs for a multigrid solver. We propose a
method using a reinforcement learning (RL) agent based on graph neural networks
(GNNs), which can learn to perform graph coarsening on small training graphs
and then be applied to unstructured large graphs. We demonstrate that this
method can produce better coarse graphs than existing algorithms, even as the
graph size increases and other properties of the graph are varied. We also
propose an efficient inference procedure for performing graph coarsening that
results in linear time complexity in graph size.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Taghibakhshi_A/0/1/0/all/0/1"&gt;Ali Taghibakhshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+MacLachlan_S/0/1/0/all/0/1"&gt;Scott MacLachlan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Olson_L/0/1/0/all/0/1"&gt;Luke Olson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+West_M/0/1/0/all/0/1"&gt;Matthew West&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Continual Learning in Deep Networks: an Analysis of the Last Layer. (arXiv:2106.01834v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01834</id>
        <link href="http://arxiv.org/abs/2106.01834"/>
        <updated>2021-06-04T01:12:28.675Z</updated>
        <summary type="html"><![CDATA[We study how different output layer types of a deep neural network learn and
forget in continual learning settings. We describe the three factors affecting
catastrophic forgetting in the output layer: (1) weights modifications, (2)
interferences, and (3) projection drift. Our goal is to provide more insights
into how different types of output layers can address (1) and (2). We also
propose potential solutions and evaluate them on several benchmarks. We show
that the best-performing output layer type depends on the data distribution
drifts or the amount of data available. In particular, in some cases where a
standard linear layer would fail, it is sufficient to change the
parametrization and get significantly better performance while still training
with SGD. Our results and analysis shed light on the dynamics of the output
layer in continual learning scenarios and help select the best-suited output
layer for a given scenario.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lesort_T/0/1/0/all/0/1"&gt;Timoth&amp;#xe9;e Lesort&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+George_T/0/1/0/all/0/1"&gt;Thomas George&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rish_I/0/1/0/all/0/1"&gt;Irina Rish&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Near Optimal Stochastic Algorithms for Finite-Sum Unbalanced Convex-Concave Minimax Optimization. (arXiv:2106.01761v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2106.01761</id>
        <link href="http://arxiv.org/abs/2106.01761"/>
        <updated>2021-06-04T01:12:28.668Z</updated>
        <summary type="html"><![CDATA[This paper considers stochastic first-order algorithms for convex-concave
minimax problems of the form $\min_{\bf x}\max_{\bf y}f(\bf x, \bf y)$, where
$f$ can be presented by the average of $n$ individual components which are
$L$-average smooth. For $\mu_x$-strongly-convex-$\mu_y$-strongly-concave
setting, we propose a new method which could find a $\varepsilon$-saddle point
of the problem in $\tilde{\mathcal O}
\big(\sqrt{n(\sqrt{n}+\kappa_x)(\sqrt{n}+\kappa_y)}\log(1/\varepsilon)\big)$
stochastic first-order complexity, where $\kappa_x\triangleq L/\mu_x$ and
$\kappa_y\triangleq L/\mu_y$. This upper bound is near optimal with respect to
$\varepsilon$, $n$, $\kappa_x$ and $\kappa_y$ simultaneously. In addition, the
algorithm is easily implemented and works well in practical. Our methods can be
extended to solve more general unbalanced convex-concave minimax problems and
the corresponding upper complexity bounds are also near optimal.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Luo_L/0/1/0/all/0/1"&gt;Luo Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Xie_G/0/1/0/all/0/1"&gt;Guangzeng Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Zhang_T/0/1/0/all/0/1"&gt;Tong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhihua Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Statistical embedding: Beyond principal components. (arXiv:2106.01858v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.01858</id>
        <link href="http://arxiv.org/abs/2106.01858"/>
        <updated>2021-06-04T01:12:28.662Z</updated>
        <summary type="html"><![CDATA[There has been an intense recent activity in embedding of very high
dimensional and nonlinear data structures, much of it in the data science and
machine learning literature. We survey this activity in four parts. In the
first part we cover nonlinear methods such as principal curves,
multidimensional scaling, local linear methods, ISOMAP, graph based methods and
kernel based methods. The second part is concerned with topological embedding
methods, in particular mapping topological properties into persistence
diagrams. Another type of data sets with a tremendous growth is very
high-dimensional network data. The task considered in part three is how to
embed such data in a vector space of moderate dimension to make the data
amenable to traditional techniques such as cluster and classification
techniques. The final part of the survey deals with embedding in
$\mathbb{R}^2$, which is visualization. Three methods are presented: $t$-SNE,
UMAP and LargeVis based on methods in parts one, two and three, respectively.
The methods are illustrated and compared on two simulated data sets; one
consisting of a triple of noisy Ranunculoid curves, and one consisting of
networks of increasing complexity and with two types of nodes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Tjostheim_D/0/1/0/all/0/1"&gt;Dag Tj&amp;#xf8;stheim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Jullum_M/0/1/0/all/0/1"&gt;Martin Jullum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Loland_A/0/1/0/all/0/1"&gt;Anders L&amp;#xf8;land&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Implicit MLE: Backpropagating Through Discrete Exponential Family Distributions. (arXiv:2106.01798v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01798</id>
        <link href="http://arxiv.org/abs/2106.01798"/>
        <updated>2021-06-04T01:12:28.655Z</updated>
        <summary type="html"><![CDATA[Integrating discrete probability distributions and combinatorial optimization
problems into neural networks has numerous applications but poses several
challenges. We propose Implicit Maximum Likelihood Estimation (I-MLE), a
framework for end-to-end learning of models combining discrete exponential
family distributions and differentiable neural components. I-MLE is widely
applicable: it only requires the ability to compute the most probable states;
and does not rely on smooth relaxations. The framework encompasses several
approaches, such as perturbation-based implicit differentiation and recent
methods to differentiate through black-box combinatorial solvers. We introduce
a novel class of noise distributions for approximating marginals via
perturb-and-MAP. Moreover, we show that I-MLE simplifies to maximum likelihood
estimation when used in some recently studied learning settings that involve
combinatorial solvers. Experiments on several datasets suggest that I-MLE is
competitive with and often outperforms existing approaches which rely on
problem-specific relaxations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Niepert_M/0/1/0/all/0/1"&gt;Mathias Niepert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Minervini_P/0/1/0/all/0/1"&gt;Pasquale Minervini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Franceschi_L/0/1/0/all/0/1"&gt;Luca Franceschi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeepOpt: Scalable Specification-based Falsification of Neural Networks using Black-Box Optimization. (arXiv:2106.01917v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01917</id>
        <link href="http://arxiv.org/abs/2106.01917"/>
        <updated>2021-06-04T01:12:28.636Z</updated>
        <summary type="html"><![CDATA[Decisions made by deep neural networks (DNNs) have a tremendous impact on the
dependability of the systems that they are embedded into, which is of
particular concern in the realm of safety-critical systems. In this paper we
consider specification-based falsification of DNNs with the aim to support
debugging and repair. We propose DeepOpt, a falsification technique based on
black-box optimization, which generates counterexamples from a DNN in a
refinement loop. DeepOpt can analyze input-output specifications, which makes
it more general than falsification approaches that only support robustness
specifications. The key idea is to algebraically combine the DNN with the input
and output constraints derived from the specification. We have implemented
DeepOpt and evaluated it on DNNs of varying sizes and architectures.
Experimental comparisons demonstrate DeepOpt's precision and scalability; in
particular, DeepOpt requires very few queries to the DNN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bauer_Marquart_F/0/1/0/all/0/1"&gt;Fabian Bauer-Marquart&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leue_S/0/1/0/all/0/1"&gt;Stefan Leue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schilling_C/0/1/0/all/0/1"&gt;Christian Schilling&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph Intervention Networks for Causal Effect Estimation. (arXiv:2106.01939v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01939</id>
        <link href="http://arxiv.org/abs/2106.01939"/>
        <updated>2021-06-04T01:12:28.630Z</updated>
        <summary type="html"><![CDATA[We address the estimation of conditional average treatment effects (CATEs)
when treatments are graph-structured (e.g., molecular graphs of drugs). Given a
weak condition on the effect, we propose a plug-in estimator that decomposes
CATE estimation into separate, simpler optimization problems. Our estimator (a)
isolates the causal estimands (reducing regularization bias), and (b) allows
one to plug in arbitrary models for learning. In experiments with small-world
and molecular graphs, we show that our approach outperforms prior approaches
and is robust to varying selection biases. Our implementation is online.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kaddour_J/0/1/0/all/0/1"&gt;Jean Kaddour&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1"&gt;Qi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yuchen Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kusner_M/0/1/0/all/0/1"&gt;Matt J. Kusner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Silva_R/0/1/0/all/0/1"&gt;Ricardo Silva&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Improved Model for Voicing Silent Speech. (arXiv:2106.01933v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2106.01933</id>
        <link href="http://arxiv.org/abs/2106.01933"/>
        <updated>2021-06-04T01:12:28.624Z</updated>
        <summary type="html"><![CDATA[In this paper, we present an improved model for voicing silent speech, where
audio is synthesized from facial electromyography (EMG) signals. To give our
model greater flexibility to learn its own input features, we directly use EMG
signals as input in the place of hand-designed features used by prior work. Our
model uses convolutional layers to extract features from the signals and
Transformer layers to propagate information across longer distances. To provide
better signal for learning, we also introduce an auxiliary task of predicting
phoneme labels in addition to predicting speech audio features. On an open
vocabulary intelligibility evaluation, our model improves the state of the art
for this task by an absolute 25.8%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Gaddy_D/0/1/0/all/0/1"&gt;David Gaddy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Klein_D/0/1/0/all/0/1"&gt;Dan Klein&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Supervised Learning of Event-Based Optical Flow with Spiking Neural Networks. (arXiv:2106.01862v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.01862</id>
        <link href="http://arxiv.org/abs/2106.01862"/>
        <updated>2021-06-04T01:12:28.618Z</updated>
        <summary type="html"><![CDATA[Neuromorphic sensing and computing hold a promise for highly energy-efficient
and high-bandwidth-sensor processing. A major challenge for neuromorphic
computing is that learning algorithms for traditional artificial neural
networks (ANNs) do not transfer directly to spiking neural networks (SNNs) due
to the discrete spikes and more complex neuronal dynamics. As a consequence,
SNNs have not yet been successfully applied to complex, large-scale tasks. In
this article, we focus on the self-supervised learning problem of optical flow
estimation from event-based camera inputs, and investigate the changes that are
necessary to the state-of-the-art ANN training pipeline in order to
successfully tackle it with SNNs. More specifically, we first modify the input
event representation to encode a much smaller time slice with minimal explicit
temporal information. Consequently, we make the network's neuronal dynamics and
recurrent connections responsible for integrating information over time.
Moreover, we reformulate the self-supervised loss function for event-based
optical flow to improve its convexity. We perform experiments with various
types of recurrent ANNs and SNNs using the proposed pipeline. Concerning SNNs,
we investigate the effects of elements such as parameter initialization and
optimization, surrogate gradient shape, and adaptive neuronal mechanisms. We
find that initialization and surrogate gradient width play a crucial part in
enabling learning with sparse inputs, while the inclusion of adaptivity and
learnable neuronal parameters can improve performance. We show that the
performance of the proposed ANNs and SNNs are on par with that of the current
state-of-the-art ANNs trained in a self-supervised manner.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Paredes_Valles_F/0/1/0/all/0/1"&gt;Federico Paredes-Vall&amp;#xe9;s&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hagenaars_J/0/1/0/all/0/1"&gt;Jesse Hagenaars&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Croon_G/0/1/0/all/0/1"&gt;Guido de Croon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pathology-Aware Generative Adversarial Networks for Medical Image Augmentation. (arXiv:2106.01915v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.01915</id>
        <link href="http://arxiv.org/abs/2106.01915"/>
        <updated>2021-06-04T01:12:28.600Z</updated>
        <summary type="html"><![CDATA[Convolutional Neural Networks (CNNs) can play a key role in Medical Image
Analysis under large-scale annotated datasets. However, preparing such massive
dataset is demanding. In this context, Generative Adversarial Networks (GANs)
can generate realistic but novel samples, and thus effectively cover the real
image distribution. In terms of interpolation, the GAN-based medical image
augmentation is reliable because medical modalities can display the human
body's strong anatomical consistency at fixed position while clearly reflecting
inter-subject variability; thus, we propose to use noise-to-image GANs (e.g.,
random noise samples to diverse pathological images) for (i) medical Data
Augmentation (DA) and (ii) physician training. Regarding the DA, the
GAN-generated images can improve Computer-Aided Diagnosis based on supervised
learning. For the physician training, the GANs can display novel desired
pathological images and help train medical trainees despite
infrastructural/legal constraints. This thesis contains four GAN projects
aiming to present such novel applications' clinical relevance in collaboration
with physicians. Whereas the methods are more generally applicable, this thesis
only explores a few oncological applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Han_C/0/1/0/all/0/1"&gt;Changhee Han&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Convolutional Neural Network(CNN/ConvNet) in Stock Price Movement Prediction. (arXiv:2106.01920v1 [cs.NE])]]></title>
        <id>http://arxiv.org/abs/2106.01920</id>
        <link href="http://arxiv.org/abs/2106.01920"/>
        <updated>2021-06-04T01:12:28.594Z</updated>
        <summary type="html"><![CDATA[With technological advancements and the exponential growth of data, we have
been unfolding different capabilities of neural networks in different sectors.
In this paper, I have tried to use a specific type of Neural Network known as
Convolutional Neural Network(CNN/ConvNet) in the stock market. In other words,
I have tried to construct and train a convolutional neural network on past
stock prices data and then tried to predict the movement of stock price i.e.
whether the stock price would rise or fall, in the coming time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bhardwaj_K/0/1/0/all/0/1"&gt;Kunal Bhardwaj&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sample Selection Bias in Evaluation of Prediction Performance of Causal Models. (arXiv:2106.01921v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.01921</id>
        <link href="http://arxiv.org/abs/2106.01921"/>
        <updated>2021-06-04T01:12:28.588Z</updated>
        <summary type="html"><![CDATA[Causal models are notoriously difficult to validate because they make
untestable assumptions regarding confounding. New scientific experiments offer
the possibility of evaluating causal models using prediction performance.
Prediction performance measures are typically robust to violations in causal
assumptions. However prediction performance does depend on the selection of
training and test sets. In particular biased training sets can lead to
optimistic assessments of model performance. In this work, we revisit the
prediction performance of several recently proposed causal models tested on a
genetic perturbation data set of Kemmeren [Kemmeren et al., 2014]. We find that
sample selection bias is likely a key driver of model performance. We propose
using a less-biased evaluation set for assessing prediction performance on
Kemmeren and compare models on this new set. In this setting, the causal model
tested have similar performance to standard association based estimators such
as Lasso. Finally we compare the performance of causal estimators in simulation
studies which reproduce the Kemmeren structure of genetic knockout experiments
but without any sample selection bias. These results provide an improved
understanding of the performance of several causal models and offer guidance on
how future studies should use Kemmeren.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Long_J/0/1/0/all/0/1"&gt;James P. Long&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ha_M/0/1/0/all/0/1"&gt;Min Jin Ha&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lifetime policy reuse and the importance of task capacity. (arXiv:2106.01741v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01741</id>
        <link href="http://arxiv.org/abs/2106.01741"/>
        <updated>2021-06-04T01:12:28.581Z</updated>
        <summary type="html"><![CDATA[A long-standing challenge in artificial intelligence is lifelong learning. In
lifelong learning, many tasks are presented in sequence and learners must
efficiently transfer knowledge between tasks while avoiding catastrophic
forgetting over long lifetimes. On these problems, policy reuse and other
multi-policy reinforcement learning techniques can learn many tasks. However,
they can generate many temporary or permanent policies, resulting in memory
issues. Consequently, there is a need for lifetime-scalable methods that
continually refine a policy library of a pre-defined size. This paper presents
a first approach to lifetime-scalable policy reuse. To pre-select the number of
policies, a notion of task capacity, the maximal number of tasks that a policy
can accurately solve, is proposed. To evaluate lifetime policy reuse using this
method, two state-of-the-art single-actor base-learners are compared: 1) a
value-based reinforcement learner, Deep Q-Network (DQN) or Deep Recurrent
Q-Network (DRQN); and 2) an actor-critic reinforcement learner, Proximal Policy
Optimisation (PPO) with or without Long Short-Term Memory layer. By selecting
the number of policies based on task capacity, D(R)QN achieves near-optimal
performance with 6 policies in a 27-task MDP domain and 9 policies in an
18-task POMDP domain; with fewer policies, catastrophic forgetting and negative
transfer are observed. Due to slow, monotonic improvement, PPO requires fewer
policies, 1 policy for the 27-task domain and 4 policies for the 18-task
domain, but it learns the tasks with lower accuracy than D(R)QN. These findings
validate lifetime-scalable policy reuse and suggest using D(R)QN for larger and
PPO for smaller library sizes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bossens_D/0/1/0/all/0/1"&gt;David M. Bossens&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sobey_A/0/1/0/all/0/1"&gt;Adam J. Sobey&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine learning models for DOTA 2 outcomes prediction. (arXiv:2106.01782v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01782</id>
        <link href="http://arxiv.org/abs/2106.01782"/>
        <updated>2021-06-04T01:12:28.575Z</updated>
        <summary type="html"><![CDATA[Prediction of the real-time multiplayer online battle arena (MOBA) games'
match outcome is one of the most important and exciting tasks in Esports
analytical research. This research paper predominantly focuses on building
predictive machine and deep learning models to identify the outcome of the Dota
2 MOBA game using the new method of multi-forward steps predictions. Three
models were investigated and compared: Linear Regression (LR), Neural Networks
(NN), and a type of recurrent neural network Long Short-Term Memory (LSTM). In
order to achieve the goals, we developed a data collecting python server using
Game State Integration (GSI) to track the real-time data of the players. Once
the exploratory feature analysis and tuning hyper-parameters were done, our
models' experiments took place on different players with dissimilar backgrounds
of playing experiences. The achieved accuracy scores depend on the
multi-forward prediction parameters, which for the worse case in linear
regression 69\% but on average 82\%, while in the deep learning models hit the
utmost accuracy of prediction on average 88\% for NN, and 93\% for LSTM models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Akhmedov_K/0/1/0/all/0/1"&gt;Kodirjon Akhmedov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Phan_A/0/1/0/all/0/1"&gt;Anh Huy Phan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Preparation of Many-body Ground States by Time Evolution with Variational Microscopic Magnetic Fields and Incomplete Interactions. (arXiv:2106.01779v1 [quant-ph])]]></title>
        <id>http://arxiv.org/abs/2106.01779</id>
        <link href="http://arxiv.org/abs/2106.01779"/>
        <updated>2021-06-04T01:12:28.568Z</updated>
        <summary type="html"><![CDATA[State preparation is of fundamental importance in quantum physics, which can
be realized by constructing the quantum circuit as a unitary that transforms
the initial state to the target, or implementing a quantum control protocol to
evolve to the target state with a designed Hamiltonian. In this work, we study
the latter on quantum many-body systems by the time evolution with fixed
couplings and variational magnetic fields. In specific, we consider to prepare
the ground states of the Hamiltonians containing certain interactions that are
missing in the Hamiltonians for the time evolution. An optimization method is
proposed to optimize the magnetic fields by "fine-graining" the discretization
of time, in order to gain high precision and stability. The back propagation
technique is utilized to obtain the gradients of the fields against the
logarithmic fidelity. Our method is tested on preparing the ground state of
Heisenberg chain with the time evolution by the XY and Ising interactions, and
its performance surpasses two baseline methods that use local and global
optimization strategies, respectively. Our work can be applied and generalized
to other quantum models such as those defined on higher dimensional lattices.
It enlightens to reduce the complexity of the required interactions for
implementing quantum control or other tasks in quantum information and
computation by means of optimizing the magnetic fields.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Lu_Y/0/1/0/all/0/1"&gt;Ying Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yue-Min Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Zhou_P/0/1/0/all/0/1"&gt;Peng-Fei Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Ran_S/0/1/0/all/0/1"&gt;Shi-Ju Ran&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Auto-tagging of Short Conversational Sentences using Transformer Methods. (arXiv:2106.01735v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01735</id>
        <link href="http://arxiv.org/abs/2106.01735"/>
        <updated>2021-06-04T01:12:28.562Z</updated>
        <summary type="html"><![CDATA[The problem of categorizing short speech sentences according to their
semantic features with high accuracy is a subject studied in natural language
processing. In this study, a data set created with samples classified in 46
different categories was used. Examples consist of sentences taken from chat
conversations between a company's customer representatives and the company's
website visitors. The primary purpose is to automatically tag questions and
requests from visitors in the most accurate way for 46 predetermined categories
for use in a chat application to generate meaningful answers to the questions
asked by the website visitors. For this, different BERT models and one GPT-2
model, pre-trained in Turkish, were preferred. The classification performances
of the relevant models were analyzed in detail and reported accordingly.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tasar_D/0/1/0/all/0/1"&gt;D. Emre Ta&amp;#x15f;ar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ozan_S/0/1/0/all/0/1"&gt;&amp;#x15e;&amp;#xfc;kr&amp;#xfc; Ozan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ozdil_U/0/1/0/all/0/1"&gt;Umut &amp;#xd6;zdil&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Akca_M/0/1/0/all/0/1"&gt;M. Fatih Akca&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Olmez_O/0/1/0/all/0/1"&gt;O&amp;#x11f;uzhan &amp;#xd6;lmez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gulum_S/0/1/0/all/0/1"&gt;Semih G&amp;#xfc;l&amp;#xfc;m&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kutal_S/0/1/0/all/0/1"&gt;Se&amp;#xe7;ilay Kutal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Belhan_C/0/1/0/all/0/1"&gt;Ceren Belhan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Partial Graph Reasoning for Neural Network Regularization. (arXiv:2106.01805v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01805</id>
        <link href="http://arxiv.org/abs/2106.01805"/>
        <updated>2021-06-04T01:12:28.544Z</updated>
        <summary type="html"><![CDATA[Regularizers helped deep neural networks prevent feature co-adaptations.
Dropout,as a commonly used regularization technique, stochastically disables
neuron ac-tivations during network optimization. However, such complete feature
disposal can affect the feature representation and network understanding.
Toward betterdescriptions of latent representations, we present DropGraph that
learns regularization function by constructing a stand-alone graph from the
backbone features. DropGraph first samples stochastic spatial feature vectors
and then incorporates graph reasoning methods to generate feature map
distortions. This add-on graph regularizes the network during training and can
be completely skipped during inference. We provide intuitions on the linkage
between graph reasoning andDropout with further discussions on how partial
graph reasoning method reduces feature correlations. To this end, we
extensively study the modeling of graphvertex dependencies and the utilization
of the graph for distorting backbone featuremaps. DropGraph was validated on
four tasks with a total of 7 different datasets.The experimental results show
that our method outperforms other state-of-the-art regularizers while leaving
the base model structure unmodified during inference.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1"&gt;Tiange Xiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chaoyi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1"&gt;Yang Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Siqi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1"&gt;Hongliang Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_W/0/1/0/all/0/1"&gt;Weidong Cai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Normative Model of Classifier Fusion. (arXiv:2106.01770v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01770</id>
        <link href="http://arxiv.org/abs/2106.01770"/>
        <updated>2021-06-04T01:12:28.538Z</updated>
        <summary type="html"><![CDATA[Combining the outputs of multiple classifiers or experts into a single
probabilistic classification is a fundamental task in machine learning with
broad applications from classifier fusion to expert opinion pooling. Here we
present a hierarchical Bayesian model of probabilistic classifier fusion based
on a new correlated Dirichlet distribution. This distribution explicitly models
positive correlations between marginally Dirichlet-distributed random vectors
thereby allowing normative modeling of correlations between base classifiers or
experts. The proposed model naturally accommodates the classic Independent
Opinion Pool and other independent fusion algorithms as special cases. It is
evaluated by uncertainty reduction and correctness of fusion on synthetic and
real-world data sets. We show that a change in performance of the fused
classifier due to uncertainty reduction can be Bayes optimal even for highly
correlated base classifiers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Trick_S/0/1/0/all/0/1"&gt;Susanne Trick&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rothkopf_C/0/1/0/all/0/1"&gt;Constantin A. Rothkopf&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Effort-free Automated Skeletal Abnormality Detection of Rat Fetuses on Whole-body Micro-CT Scans. (arXiv:2106.01830v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.01830</id>
        <link href="http://arxiv.org/abs/2106.01830"/>
        <updated>2021-06-04T01:12:28.531Z</updated>
        <summary type="html"><![CDATA[Machine Learning-based fast and quantitative automated screening plays a key
role in analyzing human bones on Computed Tomography (CT) scans. However,
despite the requirement in drug safety assessment, such research is rare on
animal fetus micro-CT scans due to its laborious data collection and
annotation. Therefore, we propose various bone feature engineering techniques
to thoroughly automate the skeletal localization/labeling/abnormality detection
of rat fetuses on whole-body micro-CT scans with minimum effort. Despite
limited training data of 49 fetuses, in skeletal labeling and abnormality
detection, we achieve accuracy of 0.900 and 0.810, respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Fukuda_A/0/1/0/all/0/1"&gt;Akihiro Fukuda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Han_C/0/1/0/all/0/1"&gt;Changhee Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hakamada_K/0/1/0/all/0/1"&gt;Kazumi Hakamada&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning High-Precision Bounding Box for Rotated Object Detection via Kullback-Leibler Divergence. (arXiv:2106.01883v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.01883</id>
        <link href="http://arxiv.org/abs/2106.01883"/>
        <updated>2021-06-04T01:12:28.514Z</updated>
        <summary type="html"><![CDATA[Existing rotated object detectors are mostly inherited from the horizontal
detection paradigm, as the latter has evolved into a well-developed area.
However, these detectors are difficult to perform prominently in high-precision
detection due to the limitation of current regression loss design, especially
for objects with large aspect ratios. Taking the perspective that horizontal
detection is a special case for rotated object detection, in this paper, we are
motivated to change the design of rotation regression loss from induction
paradigm to deduction methodology, in terms of the relation between rotation
and horizontal detection. We show that one essential challenge is how to
modulate the coupled parameters in the rotation regression loss, as such the
estimated parameters can influence to each other during the dynamic joint
optimization, in an adaptive and synergetic way. Specifically, we first convert
the rotated bounding box into a 2-D Gaussian distribution, and then calculate
the Kullback-Leibler Divergence (KLD) between the Gaussian distributions as the
regression loss. By analyzing the gradient of each parameter, we show that KLD
(and its derivatives) can dynamically adjust the parameter gradients according
to the characteristics of the object. It will adjust the importance (gradient
weight) of the angle parameter according to the aspect ratio. This mechanism
can be vital for high-precision detection as a slight angle error would cause a
serious accuracy drop for large aspect ratios objects. More importantly, we
have proved that KLD is scale invariant. We further show that the KLD loss can
be degenerated into the popular $l_{n}$-norm loss for horizontal detection.
Experimental results on seven datasets using different detectors show its
consistent superiority, and codes are available at
https://github.com/yangxue0827/RotationDetection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xue Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xiaojiang Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jirui Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ming_Q/0/1/0/all/0/1"&gt;Qi Ming&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wentao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1"&gt;Qi Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1"&gt;Junchi Yan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Domain Adaptation for Facial Expression Classifier via Domain Discrimination and Gradient Reversal. (arXiv:2106.01467v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.01467</id>
        <link href="http://arxiv.org/abs/2106.01467"/>
        <updated>2021-06-04T01:12:28.507Z</updated>
        <summary type="html"><![CDATA[Bringing empathy to a computerized system could significantly improve the
quality of human-computer communications, as soon as machines would be able to
understand customer intentions and better serve their needs. According to
different studies (Literature Review), visual information is one of the most
important channels of human interaction and contains significant behavioral
signals, that may be captured from facial expressions. Therefore, it is
consistent and natural that the research in the field of Facial Expression
Recognition (FER) has acquired increased interest over the past decade due to
having diverse application area including health-care, sociology, psychology,
driver-safety, virtual reality, cognitive sciences, security, entertainment,
marketing, etc. We propose a new architecture for the task of FER and examine
the impact of domain discrimination loss regularization on the learning
process. With regard to observations, including both classical training
conditions and unsupervised domain adaptation scenarios, important aspects of
the considered domain adaptation approach integration are traced. The results
may serve as a foundation for further research in the field.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Akhmetov_K/0/1/0/all/0/1"&gt;Kamil Akhmetov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Drivers' Manoeuvre Modelling and Prediction for Safe HRI. (arXiv:2106.01730v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2106.01730</id>
        <link href="http://arxiv.org/abs/2106.01730"/>
        <updated>2021-06-04T01:12:28.501Z</updated>
        <summary type="html"><![CDATA[As autonomous machines such as robots and vehicles start performing tasks
involving human users, ensuring a safe interaction between them becomes an
important issue. Translating methods from human-robot interaction (HRI) studies
to the interaction between humans and other highly complex machines (e.g.
semi-autonomous vehicles) could help advance the use of those machines in
scenarios requiring human interaction. One method involves understanding human
intentions and decision-making to estimate the human's present and near-future
actions whilst interacting with a robot. This idea originates from the
psychological concept of Theory of Mind, which has been broadly explored for
robotics and recently for autonomous and semi-autonomous vehicles. In this
work, we explored how to predict human intentions before an action is performed
by combining data from human-motion, vehicle-state and human inputs (e.g.
steering wheel, pedals). A data-driven approach based on Recurrent Neural
Network models was used to classify the current driving manoeuvre and to
predict the future manoeuvre to be performed. A state-transition model was used
with a fixed set of manoeuvres to label data recorded during the trials for
real-time applications. Models were trained and tested using drivers of
different seat preferences, driving expertise and arm-length; precision and
recall metrics over 95% for manoeuvre identification and 86% for manoeuvre
prediction were achieved, with prediction time-windows of up to 1 second for
both known and unknown test subjects. Compared to our previous results,
performance improved and manoeuvre prediction was possible for unknown test
subjects without knowing the current manoeuvre.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pulgarin_E/0/1/0/all/0/1"&gt;Erwin Jose Lopez Pulgarin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Herrmann_G/0/1/0/all/0/1"&gt;Guido Herrmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leonards_U/0/1/0/all/0/1"&gt;Ute Leonards&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MINIMALIST: Mutual INformatIon Maximization for Amortized Likelihood Inference from Sampled Trajectories. (arXiv:2106.01808v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01808</id>
        <link href="http://arxiv.org/abs/2106.01808"/>
        <updated>2021-06-04T01:12:28.494Z</updated>
        <summary type="html"><![CDATA[Simulation-based inference enables learning the parameters of a model even
when its likelihood cannot be computed in practice. One class of methods uses
data simulated with different parameters to infer an amortized estimator for
the likelihood-to-evidence ratio, or equivalently the posterior function. We
show that this approach can be formulated in terms of mutual information
maximization between model parameters and simulated data. We use this
equivalence to reinterpret existing approaches for amortized inference, and
propose two new methods that rely on lower bounds of the mutual information. We
apply our framework to the inference of parameters of stochastic processes and
chaotic dynamical systems from sampled trajectories, using artificial neural
networks for posterior prediction. Our approach provides a unified framework
that leverages the power of mutual information estimators for inference.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Isacchini_G/0/1/0/all/0/1"&gt;Giulio Isacchini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Spisak_N/0/1/0/all/0/1"&gt;Natanael Spisak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nourmohammad_A/0/1/0/all/0/1"&gt;Armita Nourmohammad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mora_T/0/1/0/all/0/1"&gt;Thierry Mora&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Walczak_A/0/1/0/all/0/1"&gt;Aleksandra M. Walczak&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LiMIIRL: Lightweight Multiple-Intent Inverse Reinforcement Learning. (arXiv:2106.01777v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01777</id>
        <link href="http://arxiv.org/abs/2106.01777"/>
        <updated>2021-06-04T01:12:28.478Z</updated>
        <summary type="html"><![CDATA[Multiple-Intent Inverse Reinforcement Learning (MI-IRL) seeks to find a
reward function ensemble to rationalize demonstrations of different but
unlabelled intents. Within the popular expectation maximization (EM) framework
for learning probabilistic MI-IRL models, we present a warm-start strategy
based on up-front clustering of the demonstrations in feature space. Our
theoretical analysis shows that this warm-start solution produces a
near-optimal reward ensemble, provided the behavior modes satisfy mild
separation conditions. We also propose a MI-IRL performance metric that
generalizes the popular Expected Value Difference measure to directly assesses
learned rewards against the ground-truth reward ensemble. Our metric elegantly
addresses the difficulty of pairing up learned and ground truth rewards via a
min-cost flow formulation, and is efficiently computable. We also develop a
MI-IRL benchmark problem that allows for more comprehensive algorithmic
evaluations. On this problem, we find our MI-IRL warm-start strategy helps
avoid poor quality local minima reward ensembles, resulting in a significant
improvement in behavior clustering. Our extensive sensitivity analysis
demonstrates that the quality of the learned reward ensembles is improved under
various settings, including cases where our theoretical assumptions do not
necessarily hold. Finally, we demonstrate the effectiveness of our methods by
discovering distinct driving styles in a large real-world dataset of driver GPS
trajectories.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Snoswell_A/0/1/0/all/0/1"&gt;Aaron J. Snoswell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1"&gt;Surya P. N. Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_N/0/1/0/all/0/1"&gt;Nan Ye&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[EmoDNN: Understanding emotions from short texts through a deep neural network ensemble. (arXiv:2106.01706v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01706</id>
        <link href="http://arxiv.org/abs/2106.01706"/>
        <updated>2021-06-04T01:12:28.471Z</updated>
        <summary type="html"><![CDATA[The latent knowledge in the emotions and the opinions of the individuals that
are manifested via social networks are crucial to numerous applications
including social management, dynamical processes, and public security.
Affective computing, as an interdisciplinary research field, linking artificial
intelligence to cognitive inference, is capable to exploit emotion-oriented
knowledge from brief contents. The textual contents convey hidden information
such as personality and cognition about corresponding authors that can
determine both correlations and variations between users. Emotion recognition
from brief contents should embrace the contrast between authors where the
differences in personality and cognition can be traced within emotional
expressions. To tackle this challenge, we devise a framework that, on the one
hand, infers latent individual aspects, from brief contents and, on the other
hand, presents a novel ensemble classifier equipped with dynamic dropout
convnets to extract emotions from textual context. To categorize short text
contents, our proposed method conjointly leverages cognitive factors and
exploits hidden information. We utilize the outcome vectors in a novel
embedding model to foster emotion-pertinent features that are collectively
assembled by lexicon inductions. Experimental results show that compared to
other competitors, our proposed model can achieve a higher performance in
recognizing emotion from noisy contents.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kamran_S/0/1/0/all/0/1"&gt;Sara Kamran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zall_R/0/1/0/all/0/1"&gt;Raziyeh Zall&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kangavari_M/0/1/0/all/0/1"&gt;Mohammad Reza Kangavari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hosseini_S/0/1/0/all/0/1"&gt;Saeid Hosseini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rahmani_S/0/1/0/all/0/1"&gt;Sana Rahmani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hua_W/0/1/0/all/0/1"&gt;Wen Hua&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SIRE: Separate Intra- and Inter-sentential Reasoning for Document-level Relation Extraction. (arXiv:2106.01709v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01709</id>
        <link href="http://arxiv.org/abs/2106.01709"/>
        <updated>2021-06-04T01:12:28.394Z</updated>
        <summary type="html"><![CDATA[Document-level relation extraction has attracted much attention in recent
years. It is usually formulated as a classification problem that predicts
relations for all entity pairs in the document. However, previous works
indiscriminately represent intra- and inter-sentential relations in the same
way, confounding the different patterns for predicting them. Besides, they
create a document graph and use paths between entities on the graph as clues
for logical reasoning. However, not all entity pairs can be connected with a
path and have the correct logical reasoning paths in their graph. Thus many
cases of logical reasoning cannot be covered. This paper proposes an effective
architecture, SIRE, to represent intra- and inter-sentential relations in
different ways. We design a new and straightforward form of logical reasoning
module that can cover more logical reasoning chains. Experiments on the public
datasets show SIRE outperforms the previous state-of-the-art methods. Further
analysis shows that our predictions are reliable and explainable. Our code is
available at https://github.com/DreamInvoker/SIRE.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_S/0/1/0/all/0/1"&gt;Shuang Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yuting Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_B/0/1/0/all/0/1"&gt;Baobao Chang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Risk Minimization from Adaptively Collected Data: Guarantees for Supervised and Policy Learning. (arXiv:2106.01723v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.01723</id>
        <link href="http://arxiv.org/abs/2106.01723"/>
        <updated>2021-06-04T01:12:28.387Z</updated>
        <summary type="html"><![CDATA[Empirical risk minimization (ERM) is the workhorse of machine learning,
whether for classification and regression or for off-policy policy learning,
but its model-agnostic guarantees can fail when we use adaptively collected
data, such as the result of running a contextual bandit algorithm. We study a
generic importance sampling weighted ERM algorithm for using adaptively
collected data to minimize the average of a loss function over a hypothesis
class and provide first-of-their-kind generalization guarantees and fast
convergence rates. Our results are based on a new maximal inequality that
carefully leverages the importance sampling structure to obtain rates with the
right dependence on the exploration rate in the data. For regression, we
provide fast rates that leverage the strong convexity of squared-error loss.
For policy learning, we provide rate-optimal regret guarantees that close an
open gap in the existing literature whenever exploration decays to zero, as is
the case for bandit-collected data. An empirical investigation validates our
theory.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Bibaut_A/0/1/0/all/0/1"&gt;Aur&amp;#xe9;lien Bibaut&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Chambaz_A/0/1/0/all/0/1"&gt;Antoine Chambaz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Dimakopoulou_M/0/1/0/all/0/1"&gt;Maria Dimakopoulou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kallus_N/0/1/0/all/0/1"&gt;Nathan Kallus&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Laan_M/0/1/0/all/0/1"&gt;Mark van der Laan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimization Variance: Exploring Generalization Properties of DNNs. (arXiv:2106.01714v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01714</id>
        <link href="http://arxiv.org/abs/2106.01714"/>
        <updated>2021-06-04T01:12:28.352Z</updated>
        <summary type="html"><![CDATA[Unlike the conventional wisdom in statistical learning theory, the test error
of a deep neural network (DNN) often demonstrates double descent: as the model
complexity increases, it first follows a classical U-shaped curve and then
shows a second descent. Through bias-variance decomposition, recent studies
revealed that the bell-shaped variance is the major cause of model-wise double
descent (when the DNN is widened gradually). This paper investigates epoch-wise
double descent, i.e., the test error of a DNN also shows double descent as the
number of training epoches increases. By extending the bias-variance analysis
to epoch-wise double descent of the zero-one loss, we surprisingly find that
the variance itself, without the bias, varies consistently with the test error.
Inspired by this result, we propose a novel metric, optimization variance (OV),
to measure the diversity of model updates caused by the stochastic gradients of
random training batches drawn in the same iteration. OV can be estimated using
samples from the training set only but correlates well with the (unknown)
\emph{test} error, and hence early stopping may be achieved without using a
validation set.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1"&gt;Dongrui Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1"&gt;Haoyi Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_B/0/1/0/all/0/1"&gt;Bo Dai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lymph Node Graph Neural Networks for Cancer Metastasis Prediction. (arXiv:2106.01711v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01711</id>
        <link href="http://arxiv.org/abs/2106.01711"/>
        <updated>2021-06-04T01:12:28.334Z</updated>
        <summary type="html"><![CDATA[Predicting outcomes, such as survival or metastasis for individual cancer
patients is a crucial component of precision oncology. Machine learning (ML)
offers a promising way to exploit rich multi-modal data, including clinical
information and imaging to learn predictors of disease trajectory and help
inform clinical decision making. In this paper, we present a novel graph-based
approach to incorporate imaging characteristics of existing cancer spread to
local lymph nodes (LNs) as well as their connectivity patterns in a prognostic
ML model. We trained an edge-gated Graph Convolutional Network (Gated-GCN) to
accurately predict the risk of distant metastasis (DM) by propagating
information across the LN graph with the aid of soft edge attention mechanism.
In a cohort of 1570 head and neck cancer patients, the Gated-GCN achieves AUROC
of 0.757 for 2-year DM classification and $C$-index of 0.725 for lifetime DM
risk prediction, outperforming current prognostic factors as well as previous
approaches based on aggregated LN features. We also explored the importance of
graph structure and individual lymph nodes through ablation experiments and
interpretability studies, highlighting the importance of considering individual
LN characteristics as well as the relationships between regions of cancer
spread.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kazmierski_M/0/1/0/all/0/1"&gt;Michal Kazmierski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Haibe_Kains_B/0/1/0/all/0/1"&gt;Benjamin Haibe-Kains&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Representation over Dynamic Graph using Aggregation-Diffusion Mechanism. (arXiv:2106.01678v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01678</id>
        <link href="http://arxiv.org/abs/2106.01678"/>
        <updated>2021-06-04T01:12:28.326Z</updated>
        <summary type="html"><![CDATA[Representation learning on graphs that evolve has recently received
significant attention due to its wide application scenarios, such as
bioinformatics, knowledge graphs, and social networks. The propagation of
information in graphs is important in learning dynamic graph representations,
and most of the existing methods achieve this by aggregation. However, relying
only on aggregation to propagate information in dynamic graphs can result in
delays in information propagation and thus affect the performance of the
method. To alleviate this problem, we propose an aggregation-diffusion (AD)
mechanism that actively propagates information to its neighbor by diffusion
after the node updates its embedding through the aggregation mechanism. In
experiments on two real-world datasets in the dynamic link prediction task, the
AD mechanism outperforms the baseline models that only use aggregation to
propagate information. We further conduct extensive experiments to discuss the
influence of different factors in the AD mechanism.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1"&gt;Mingyi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1"&gt;Zhiying Tu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1"&gt;Xiaofei Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhongjie Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine Learning Based Texture Analysis of Patella from X-Rays for Detecting Patellofemoral Osteoarthritis. (arXiv:2106.01700v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.01700</id>
        <link href="http://arxiv.org/abs/2106.01700"/>
        <updated>2021-06-04T01:12:28.305Z</updated>
        <summary type="html"><![CDATA[Objective is to assess the ability of texture features for detecting
radiographic patellofemoral osteoarthritis (PFOA) from knee lateral view
radiographs. We used lateral view knee radiographs from MOST public use
datasets (n = 5507 knees). Patellar region-of-interest (ROI) was automatically
detected using landmark detection tool (BoneFinder). Hand-crafted features,
based on LocalBinary Patterns (LBP), were then extracted to describe the
patellar texture. First, a machine learning model (Gradient Boosting Machine)
was trained to detect radiographic PFOA from the LBP features. Furthermore, we
used end-to-end trained deep convolutional neural networks (CNNs) directly on
the texture patches for detecting the PFOA. The proposed classification models
were eventually compared with more conventional reference models that use
clinical assessments and participant characteristics such as age, sex, body
mass index(BMI), the total WOMAC score, and tibiofemoral Kellgren-Lawrence (KL)
grade. Atlas-guided visual assessment of PFOA status by expert readers provided
in the MOST public use datasets was used as a classification outcome for the
models. Performance of prediction models was assessed using the area under the
receiver operating characteristic curve (ROC AUC), the area under the
precision-recall (PR) curve-average precision (AP)-, and Brier score in the
stratified 5-fold cross validation setting.Of the 5507 knees, 953 (17.3%) had
PFOA. AUC and AP for the strongest reference model including age, sex, BMI,
WOMAC score, and tibiofemoral KL grade to predict PFOA were 0.817 and 0.487,
respectively. Textural ROI classification using CNN significantly improved the
prediction performance (ROC AUC= 0.889, AP= 0.714). We present the first study
that analyses patellar bone texture for diagnosing PFOA. Our results
demonstrates the potential of using texture features of patella to predict
PFOA.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Bayramoglu_N/0/1/0/all/0/1"&gt;Neslihan Bayramoglu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Nieminen_M/0/1/0/all/0/1"&gt;Miika T. Nieminen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Saarakkala_S/0/1/0/all/0/1"&gt;Simo Saarakkala&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Probabilistic Gradient Boosting Machines for Large-Scale Probabilistic Regression. (arXiv:2106.01682v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01682</id>
        <link href="http://arxiv.org/abs/2106.01682"/>
        <updated>2021-06-04T01:12:28.288Z</updated>
        <summary type="html"><![CDATA[Gradient Boosting Machines (GBM) are hugely popular for solving tabular data
problems. However, practitioners are not only interested in point predictions,
but also in probabilistic predictions in order to quantify the uncertainty of
the predictions. Creating such probabilistic predictions is difficult with
existing GBM-based solutions: they either require training multiple models or
they become too computationally expensive to be useful for large-scale
settings. We propose Probabilistic Gradient Boosting Machines (PGBM), a method
to create probabilistic predictions with a single ensemble of decision trees in
a computationally efficient manner. PGBM approximates the leaf weights in a
decision tree as a random variable, and approximates the mean and variance of
each sample in a dataset via stochastic tree ensemble update equations. These
learned moments allow us to subsequently sample from a specified distribution
after training. We empirically demonstrate the advantages of PGBM compared to
existing state-of-the-art methods: (i) PGBM enables probabilistic estimates
without compromising on point performance in a single model, (ii) PGBM learns
probabilistic estimates via a single model only (and without requiring
multi-parameter boosting), and thereby offers a speedup of up to several orders
of magnitude over existing state-of-the-art methods on large datasets, and
(iii) PGBM achieves accurate probabilistic estimates in tasks with complex
differentiable loss functions, such as hierarchical time series problems, where
we observed up to 10\% improvement in point forecasting performance and up to
300\% improvement in probabilistic forecasting performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sprangers_O/0/1/0/all/0/1"&gt;Olivier Sprangers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schelter_S/0/1/0/all/0/1"&gt;Sebastian Schelter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rijke_M/0/1/0/all/0/1"&gt;Maarten de Rijke&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Can vectors read minds better than experts? Comparing data augmentation strategies for the automated scoring of children's mindreading ability. (arXiv:2106.01635v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01635</id>
        <link href="http://arxiv.org/abs/2106.01635"/>
        <updated>2021-06-04T01:12:28.281Z</updated>
        <summary type="html"><![CDATA[In this paper we implement and compare 7 different data augmentation
strategies for the task of automatic scoring of children's ability to
understand others' thoughts, feelings, and desires (or "mindreading").

We recruit in-domain experts to re-annotate augmented samples and determine
to what extent each strategy preserves the original rating. We also carry out
multiple experiments to measure how much each augmentation strategy improves
the performance of automatic scoring systems. To determine the capabilities of
automatic systems to generalize to unseen data, we create UK-MIND-20 - a new
corpus of children's performance on tests of mindreading, consisting of 10,320
question-answer pairs.

We obtain a new state-of-the-art performance on the MIND-CA corpus, improving
macro-F1-score by 6 points. Results indicate that both the number of training
examples and the quality of the augmentation strategies affect the performance
of the systems. The task-specific augmentations generally outperform
task-agnostic augmentations. Automatic augmentations based on vectors (GloVe,
FastText) perform the worst.

We find that systems trained on MIND-CA generalize well to UK-MIND-20. We
demonstrate that data augmentation strategies also improve the performance on
unseen data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kovatchev_V/0/1/0/all/0/1"&gt;Venelin Kovatchev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smith_P/0/1/0/all/0/1"&gt;Phillip Smith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1"&gt;Mark Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Devine_R/0/1/0/all/0/1"&gt;Rory Devine&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring Memorization in Adversarial Training. (arXiv:2106.01606v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01606</id>
        <link href="http://arxiv.org/abs/2106.01606"/>
        <updated>2021-06-04T01:12:28.273Z</updated>
        <summary type="html"><![CDATA[It is well known that deep learning models have a propensity for fitting the
entire training set even with random labels, which requires memorization of
every training sample. In this paper, we investigate the memorization effect in
adversarial training (AT) for promoting a deeper understanding of capacity,
convergence, generalization, and especially robust overfitting of adversarially
trained classifiers. We first demonstrate that deep networks have sufficient
capacity to memorize adversarial examples of training data with completely
random labels, but not all AT algorithms can converge under the extreme
circumstance. Our study of AT with random labels motivates further analyses on
the convergence and generalization of AT. We find that some AT methods suffer
from a gradient instability issue, and the recently suggested complexity
measures cannot explain robust generalization by considering models trained on
random labels. Furthermore, we identify a significant drawback of memorization
in AT that it could result in robust overfitting. We then propose a new
mitigation algorithm motivated by detailed memorization analyses. Extensive
experiments on various datasets validate the effectiveness of the proposed
method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1"&gt;Yinpeng Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1"&gt;Ke Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xiao Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pang_T/0/1/0/all/0/1"&gt;Tianyu Pang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1"&gt;Zhijie Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1"&gt;Hang Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jun Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Projection-free Graph-based Classifier Learning using Gershgorin Disc Perfect Alignment. (arXiv:2106.01642v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01642</id>
        <link href="http://arxiv.org/abs/2106.01642"/>
        <updated>2021-06-04T01:12:28.265Z</updated>
        <summary type="html"><![CDATA[In semi-supervised graph-based binary classifier learning, a subset of known
labels $\hat{x}_i$ are used to infer unknown labels, assuming that the label
signal $x$ is smooth with respect to a similarity graph specified by a
Laplacian matrix. When restricting labels $x_i$ to binary values, the problem
is NP-hard. While a conventional semi-definite programming (SDP) relaxation can
be solved in polynomial time using, for example, the alternating direction
method of multipliers (ADMM), the complexity of iteratively projecting a
candidate matrix $M$ onto the positive semi-definite (PSD) cone ($M \succeq 0$)
remains high. In this paper, leveraging a recent linear algebraic theory called
Gershgorin disc perfect alignment (GDPA), we propose a fast projection-free
method by solving a sequence of linear programs (LP) instead. Specifically, we
first recast the SDP relaxation to its SDP dual, where a feasible solution $H
\succeq 0$ can be interpreted as a Laplacian matrix corresponding to a balanced
signed graph sans the last node. To achieve graph balance, we split the last
node into two that respectively contain the original positive and negative
edges, resulting in a new Laplacian $\bar{H}$. We repose the SDP dual for
solution $\bar{H}$, then replace the PSD cone constraint $\bar{H} \succeq 0$
with linear constraints derived from GDPA -- sufficient conditions to ensure
$\bar{H}$ is PSD -- so that the optimization becomes an LP per iteration.
Finally, we extract predicted labels from our converged LP solution $\bar{H}$.
Experiments show that our algorithm enjoyed a $40\times$ speedup on average
over the next fastest scheme while retaining comparable label prediction
performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1"&gt;Cheng Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheung_G/0/1/0/all/0/1"&gt;Gene Cheung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_W/0/1/0/all/0/1"&gt;Wai-tian Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhai_G/0/1/0/all/0/1"&gt;Guangtao Zhai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NODE-GAM: Neural Generalized Additive Model for Interpretable Deep Learning. (arXiv:2106.01613v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01613</id>
        <link href="http://arxiv.org/abs/2106.01613"/>
        <updated>2021-06-04T01:12:28.247Z</updated>
        <summary type="html"><![CDATA[Deployment of machine learning models in real high-risk settings (e.g.
healthcare) often depends not only on model's accuracy but also on its
fairness, robustness and interpretability. Generalized Additive Models (GAMs)
have a long history of use in these high-risk domains, but lack desirable
features of deep learning such as differentiability and scalability. In this
work, we propose a neural GAM (NODE-GAM) and neural GA$^2$M (NODE-GA$^2$M) that
scale well to large datasets, while remaining interpretable and accurate. We
show that our proposed models have comparable accuracy to other
non-interpretable models, and outperform other GAMs on large datasets. We also
show that our models are more accurate in self-supervised learning setting when
access to labeled data is limited.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chang_C/0/1/0/all/0/1"&gt;Chun-Hao Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Caruana_R/0/1/0/all/0/1"&gt;Rich Caruana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldenberg_A/0/1/0/all/0/1"&gt;Anna Goldenberg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving the Transferability of Adversarial Examples with New Iteration Framework and Input Dropout. (arXiv:2106.01617v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01617</id>
        <link href="http://arxiv.org/abs/2106.01617"/>
        <updated>2021-06-04T01:12:28.239Z</updated>
        <summary type="html"><![CDATA[Deep neural networks(DNNs) is vulnerable to be attacked by adversarial
examples. Black-box attack is the most threatening attack. At present,
black-box attack methods mainly adopt gradient-based iterative attack methods,
which usually limit the relationship between the iteration step size, the
number of iterations, and the maximum perturbation. In this paper, we propose a
new gradient iteration framework, which redefines the relationship between the
above three. Under this framework, we easily improve the attack success rate of
DI-TI-MIM. In addition, we propose a gradient iterative attack method based on
input dropout, which can be well combined with our framework. We further
propose a multi dropout rate version of this method. Experimental results show
that our best method can achieve attack success rate of 96.2\% for defense
model on average, which is higher than the state-of-the-art gradient-based
attacks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_P/0/1/0/all/0/1"&gt;Pengfei Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Linyuan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_R/0/1/0/all/0/1"&gt;Ruoxi Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiao_K/0/1/0/all/0/1"&gt;Kai Qiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1"&gt;Shuhao Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_G/0/1/0/all/0/1"&gt;Guoen Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1"&gt;Bin Yan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sleeping Combinatorial Bandits. (arXiv:2106.01624v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01624</id>
        <link href="http://arxiv.org/abs/2106.01624"/>
        <updated>2021-06-04T01:12:28.231Z</updated>
        <summary type="html"><![CDATA[In this paper, we study an interesting combination of sleeping and
combinatorial stochastic bandits. In the mixed model studied here, at each
discrete time instant, an arbitrary \emph{availability set} is generated from a
fixed set of \emph{base} arms. An algorithm can select a subset of arms from
the \emph{availability set} (sleeping bandits) and receive the corresponding
reward along with semi-bandit feedback (combinatorial bandits).

We adapt the well-known CUCB algorithm in the sleeping combinatorial bandits
setting and refer to it as \CSUCB. We prove -- under mild smoothness conditions
-- that the \CSUCB\ algorithm achieves an $O(\log (T))$ instance-dependent
regret guarantee. We further prove that (i) when the range of the rewards is
bounded, the regret guarantee of \CSUCB\ algorithm is $O(\sqrt{T \log (T)})$
and (ii) the instance-independent regret is $O(\sqrt[3]{T^2 \log(T)})$ in a
general setting. Our results are quite general and hold under general
environments -- such as non-additive reward functions, volatile arm
availability, a variable number of base-arms to be pulled -- arising in
practical applications. We validate the proven theoretical guarantees through
experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Abhishek_K/0/1/0/all/0/1"&gt;Kumar Abhishek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghalme_G/0/1/0/all/0/1"&gt;Ganesh Ghalme&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gujar_S/0/1/0/all/0/1"&gt;Sujit Gujar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Narahari_Y/0/1/0/all/0/1"&gt;Yadati Narahari&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[JIZHI: A Fast and Cost-Effective Model-As-A-Service System for Web-Scale Online Inference at Baidu. (arXiv:2106.01674v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.01674</id>
        <link href="http://arxiv.org/abs/2106.01674"/>
        <updated>2021-06-04T01:12:28.225Z</updated>
        <summary type="html"><![CDATA[In modern internet industries, deep learning based recommender systems have
became an indispensable building block for a wide spectrum of applications,
such as search engine, news feed, and short video clips. However, it remains
challenging to carry the well-trained deep models for online real-time
inference serving, with respect to the time-varying web-scale traffics from
billions of users, in a cost-effective manner. In this work, we present JIZHI -
a Model-as-a-Service system - that per second handles hundreds of millions of
online inference requests to huge deep models with more than trillions of
sparse parameters, for over twenty real-time recommendation services at Baidu,
Inc. In JIZHI, the inference workflow of every recommendation request is
transformed to a Staged Event-Driven Pipeline (SEDP), where each node in the
pipeline refers to a staged computation or I/O intensive task processor. With
traffics of real-time inference requests arrived, each modularized processor
can be run in a fully asynchronized way and managed separately. Besides, JIZHI
introduces heterogeneous and hierarchical storage to further accelerate the
online inference process by reducing unnecessary computations and potential
data access latency induced by ultra-sparse model parameters. Moreover, an
intelligent resource manager has been deployed to maximize the throughput of
JIZHI over the shared infrastructure by searching the optimal resource
allocation plan from historical logs and fine-tuning the load shedding policies
over intermediate system feedback. Extensive experiments have been done to
demonstrate the advantages of JIZHI from the perspectives of end-to-end service
latency, system-wide throughput, and resource consumption. JIZHI has helped
Baidu saved more than ten million US dollars in hardware and utility costs
while handling 200% more traffics without sacrificing inference efficiency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Hao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Q/0/1/0/all/0/1"&gt;Qian Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jiang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liao_X/0/1/0/all/0/1"&gt;Xiaochao Liao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1"&gt;Hao Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1"&gt;Guangxing Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wenlin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1"&gt;Guobao Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zha_Z/0/1/0/all/0/1"&gt;Zhiwei Zha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_D/0/1/0/all/0/1"&gt;Daxiang Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dou_D/0/1/0/all/0/1"&gt;Dejing Dou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1"&gt;Haoyi Xiong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bandit Phase Retrieval. (arXiv:2106.01660v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.01660</id>
        <link href="http://arxiv.org/abs/2106.01660"/>
        <updated>2021-06-04T01:12:28.218Z</updated>
        <summary type="html"><![CDATA[We study a bandit version of phase retrieval where the learner chooses
actions $(A_t)_{t=1}^n$ in the $d$-dimensional unit ball and the expected
reward is $\langle A_t, \theta_\star\rangle^2$ where $\theta_\star \in \mathbb
R^d$ is an unknown parameter vector. We prove that the minimax cumulative
regret in this problem is $\smash{\tilde \Theta(d \sqrt{n})}$, which improves
on the best known bounds by a factor of $\smash{\sqrt{d}}$. We also show that
the minimax simple regret is $\smash{\tilde \Theta(d / \sqrt{n})}$ and that
this is only achievable by an adaptive algorithm. Our analysis shows that an
apparently convincing heuristic for guessing lower bounds can be misleading and
that uniform bounds on the information ratio for information-directed sampling
are not sufficient for optimal regret.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Lattimore_T/0/1/0/all/0/1"&gt;Tor Lattimore&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Hao_B/0/1/0/all/0/1"&gt;Botao Hao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Noisy student-teacher training for robust keyword spotting. (arXiv:2106.01604v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01604</id>
        <link href="http://arxiv.org/abs/2106.01604"/>
        <updated>2021-06-04T01:12:28.195Z</updated>
        <summary type="html"><![CDATA[We propose self-training with noisy student-teacher approach for streaming
keyword spotting, that can utilize large-scale unlabeled data and aggressive
data augmentation. The proposed method applies aggressive data augmentation
(spectral augmentation) on the input of both student and teacher and utilize
unlabeled data at scale, which significantly boosts the accuracy of student
against challenging conditions. Such aggressive augmentation usually degrades
model performance when used with supervised training with hard-labeled data.
Experiments show that aggressive spec augmentation on baseline supervised
training method degrades accuracy, while the proposed self-training with noisy
student-teacher training improves accuracy of some difficult-conditioned test
sets by as much as 60%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Park_H/0/1/0/all/0/1"&gt;Hyun-Jin Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_P/0/1/0/all/0/1"&gt;Pai Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moreno_I/0/1/0/all/0/1"&gt;Ignacio Lopez Moreno&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Subrahmanya_N/0/1/0/all/0/1"&gt;Niranjan Subrahmanya&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cybersecurity Information Exchange with Privacy (CYBEX-P) and TAHOE -- A Cyberthreat Language. (arXiv:2106.01632v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2106.01632</id>
        <link href="http://arxiv.org/abs/2106.01632"/>
        <updated>2021-06-04T01:12:28.189Z</updated>
        <summary type="html"><![CDATA[Cybersecurity information sharing (CIS) is envisioned to protect
organizations more effectively from advanced cyber attacks. However, a
completely automated CIS platform is not widely adopted. The major challenges
are: (1) the absence of a robust cyber threat language (CTL) and (2) the
concerns over data privacy. This work introduces Cybersecurity Information
Exchangewith Privacy (CYBEX-P), as a CIS framework, to tackle these challenges.
CYBEX-P allows organizations to share heterogeneous data with granular,
attribute based privacy control. It correlates the data to automatically
generate intuitive reports and defensive rules. To achieve such versatility, we
have developed TAHOE - a graph based CTL. TAHOE is a structure for
storing,sharing and analyzing threat data. It also intrinsically correlates the
data. We have further developed a universal Threat Data Query Language (TDQL).
In this paper, we propose the system architecture for CYBEX-P. We then discuss
its scalability and privacy features along with a use case of CYBEX-P providing
Infrastructure as a Service (IaaS). We further introduce TAHOE& TDQL as better
alternatives to existing CTLs and formulate ThreatRank - an algorithm to detect
new malicious even]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sadique_F/0/1/0/all/0/1"&gt;Farhan Sadique&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Astaburuaga_I/0/1/0/all/0/1"&gt;Ignacio Astaburuaga&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kaul_R/0/1/0/all/0/1"&gt;Raghav Kaul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sengupta_S/0/1/0/all/0/1"&gt;Shamik Sengupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Badsha_S/0/1/0/all/0/1"&gt;Shahriar Badsha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schnebly_J/0/1/0/all/0/1"&gt;James Schnebly&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cassell_A/0/1/0/all/0/1"&gt;Adam Cassell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Springer_J/0/1/0/all/0/1"&gt;Jeff Springer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Latourrette_N/0/1/0/all/0/1"&gt;Nancy Latourrette&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dascalu_S/0/1/0/all/0/1"&gt;Sergiu M. Dascalu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Testing Directed Acyclic Graph via Structural, Supervised and Generative Adversarial Learning. (arXiv:2106.01474v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.01474</id>
        <link href="http://arxiv.org/abs/2106.01474"/>
        <updated>2021-06-04T01:12:28.182Z</updated>
        <summary type="html"><![CDATA[In this article, we propose a new hypothesis testing method for directed
acyclic graph (DAG). While there is a rich class of DAG estimation methods,
there is a relative paucity of DAG inference solutions. Moreover, the existing
methods often impose some specific model structures such as linear models or
additive models, and assume independent data observations. Our proposed test
instead allows the associations among the random variables to be nonlinear and
the data to be time-dependent. We build the test based on some highly flexible
neural networks learners. We establish the asymptotic guarantees of the test,
while allowing either the number of subjects or the number of time points for
each subject to diverge to infinity. We demonstrate the efficacy of the test
through simulations and a brain connectivity network analysis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Shi_C/0/1/0/all/0/1"&gt;Chengchun Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yunzhe Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Li_L/0/1/0/all/0/1"&gt;Lexin Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hyperbolically-Discounted Reinforcement Learning on Reward-Punishment Framework. (arXiv:2106.01516v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01516</id>
        <link href="http://arxiv.org/abs/2106.01516"/>
        <updated>2021-06-04T01:12:28.172Z</updated>
        <summary type="html"><![CDATA[This paper proposes a new reinforcement learning with hyperbolic discounting.
Combining a new temporal difference error with the hyperbolic discounting in
recursive manner and reward-punishment framework, a new scheme to learn the
optimal policy is derived. In simulations, it is found that the proposal
outperforms the standard reinforcement learning, although the performance
depends on the design of reward and punishment. In addition, the averages of
discount factors w.r.t. reward and punishment are different from each other,
like a sign effect in animal behaviors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kobayashi_T/0/1/0/all/0/1"&gt;Taisuke Kobayashi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Men Are Elected, Women Are Married: Events Gender Bias on Wikipedia. (arXiv:2106.01601v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01601</id>
        <link href="http://arxiv.org/abs/2106.01601"/>
        <updated>2021-06-04T01:12:28.166Z</updated>
        <summary type="html"><![CDATA[Human activities can be seen as sequences of events, which are crucial to
understanding societies. Disproportional event distribution for different
demographic groups can manifest and amplify social stereotypes, and potentially
jeopardize the ability of members in some groups to pursue certain goals. In
this paper, we present the first event-centric study of gender biases in a
Wikipedia corpus. To facilitate the study, we curate a corpus of career and
personal life descriptions with demographic information consisting of 7,854
fragments from 10,412 celebrities. Then we detect events with a
state-of-the-art event detection model, calibrate the results using
strategically generated templates, and extract events that have asymmetric
associations with genders. Our study discovers that the Wikipedia pages tend to
intermingle personal life events with professional events for females but not
for males, which calls for the awareness of the Wikipedia community to
formalize guidelines and train the editors to mind the implicit biases that
contributors carry. Our work also lays the foundation for future works on
quantifying and discovering event biases at the corpus level.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1"&gt;Jiao Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1"&gt;Nanyun Peng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Discussion On the Validity of Manifold Learning. (arXiv:2106.01608v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01608</id>
        <link href="http://arxiv.org/abs/2106.01608"/>
        <updated>2021-06-04T01:12:28.146Z</updated>
        <summary type="html"><![CDATA[Dimensionality reduction (DR) and manifold learning (ManL) have been applied
extensively in many machine learning tasks, including signal processing, speech
recognition, and neuroinformatics. However, the understanding of whether DR and
ManL models can generate valid learning results remains unclear. In this work,
we investigate the validity of learning results of some widely used DR and ManL
methods through the chart mapping function of a manifold. We identify a
fundamental problem of these methods: the mapping functions induced by these
methods violate the basic settings of manifolds, and hence they are not
learning manifold in the mathematical sense. To address this problem, we
provide a provably correct algorithm called fixed points Laplacian mapping
(FPLM), that has the geometric guarantee to find a valid manifold
representation (up to a homeomorphism). Combining one additional
condition(orientation preserving), we discuss a sufficient condition for an
algorithm to be bijective for any d-simplex decomposition result on a
d-manifold. However, constructing such a mapping function and its computational
method satisfying these conditions is still an open problem in mathematics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shi_D/0/1/0/all/0/1"&gt;Dai Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_A/0/1/0/all/0/1"&gt;Andi Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1"&gt;Yi Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1"&gt;Junbin Gao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hierarchical Representation Learning for Markov Decision Processes. (arXiv:2106.01655v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01655</id>
        <link href="http://arxiv.org/abs/2106.01655"/>
        <updated>2021-06-04T01:12:28.140Z</updated>
        <summary type="html"><![CDATA[In this paper we present a novel method for learning hierarchical
representations of Markov decision processes. Our method works by partitioning
the state space into subsets, and defines subtasks for performing transitions
between the partitions. We formulate the problem of partitioning the state
space as an optimization problem that can be solved using gradient descent
given a set of sampled trajectories, making our method suitable for
high-dimensional problems with large state spaces. We empirically validate the
method, by showing that it can successfully learn a useful hierarchical
representation in a navigation domain. Once learned, the hierarchical
representation can be used to solve different tasks in the given domain, thus
generalizing knowledge across tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Steccanella_L/0/1/0/all/0/1"&gt;Lorenzo Steccanella&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Totaro_S/0/1/0/all/0/1"&gt;Simone Totaro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jonsson_A/0/1/0/all/0/1"&gt;Anders Jonsson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ZmBART: An Unsupervised Cross-lingual Transfer Framework for Language Generation. (arXiv:2106.01597v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01597</id>
        <link href="http://arxiv.org/abs/2106.01597"/>
        <updated>2021-06-04T01:12:28.134Z</updated>
        <summary type="html"><![CDATA[Despite the recent advancement in NLP research, cross-lingual transfer for
natural language generation is relatively understudied. In this work, we
transfer supervision from high resource language (HRL) to multiple low-resource
languages (LRLs) for natural language generation (NLG). We consider four NLG
tasks (text summarization, question generation, news headline generation, and
distractor generation) and three syntactically diverse languages, i.e.,
English, Hindi, and Japanese. We propose an unsupervised cross-lingual language
generation framework (called ZmBART) that does not use any parallel or
pseudo-parallel/back-translated data. In this framework, we further pre-train
mBART sequence-to-sequence denoising auto-encoder model with an auxiliary task
using monolingual data of three languages. The objective function of the
auxiliary task is close to the target tasks which enriches the multi-lingual
latent representation of mBART and provides good initialization for target
tasks. Then, this model is fine-tuned with task-specific supervised English
data and directly evaluated with low-resource languages in the Zero-shot
setting. To overcome catastrophic forgetting and spurious correlation issues,
we applied freezing model component and data argumentation approaches
respectively. This simple modeling approach gave us promising results.We
experimented with few-shot training (with 1000 supervised data points) which
boosted the model performance further. We performed several ablations and
cross-lingual transferability analyses to demonstrate the robustness of ZmBART.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Maurya_K/0/1/0/all/0/1"&gt;Kaushal Kumar Maurya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Desarkar_M/0/1/0/all/0/1"&gt;Maunendra Sankar Desarkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kano_Y/0/1/0/all/0/1"&gt;Yoshinobu Kano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deepshikha_K/0/1/0/all/0/1"&gt;Kumari Deepshikha&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Attention-Guided Supervised Contrastive Learning for Semantic Segmentation. (arXiv:2106.01596v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.01596</id>
        <link href="http://arxiv.org/abs/2106.01596"/>
        <updated>2021-06-04T01:12:28.127Z</updated>
        <summary type="html"><![CDATA[Contrastive learning has shown superior performance in embedding global and
spatial invariant features in computer vision (e.g., image classification).
However, its overall success of embedding local and spatial variant features is
still limited, especially for semantic segmentation. In a per-pixel prediction
task, more than one label can exist in a single image for segmentation (e.g.,
an image contains both cat, dog, and grass), thereby it is difficult to define
'positive' or 'negative' pairs in a canonical contrastive learning setting. In
this paper, we propose an attention-guided supervised contrastive learning
approach to highlight a single semantic object every time as the target. With
our design, the same image can be embedded to different semantic clusters with
semantic attention (i.e., coerce semantic masks) as an additional input
channel. To achieve such attention, a novel two-stage training strategy is
presented. We evaluate the proposed method on multi-organ medical image
segmentation task, as our major task, with both in-house data and BTCV 2015
datasets. Comparing with the supervised and semi-supervised training
state-of-the-art in the backbone of ResNet-50, our proposed pipeline yields
substantial improvement of 5.53% and 6.09% in Dice score for both medical image
segmentation cohorts respectively. The performance of the proposed method on
natural images is assessed via PASCAL VOC 2012 dataset, and achieves 2.75%
substantial improvement.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1"&gt;Ho Hin Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1"&gt;Yucheng Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1"&gt;Qi Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1"&gt;Xin Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bao_S/0/1/0/all/0/1"&gt;Shunxing Bao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Landman_B/0/1/0/all/0/1"&gt;Bennett A. Landman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huo_Y/0/1/0/all/0/1"&gt;Yuankai Huo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LLC: Accurate, Multi-purpose Learnt Low-dimensional Binary Codes. (arXiv:2106.01487v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01487</id>
        <link href="http://arxiv.org/abs/2106.01487"/>
        <updated>2021-06-04T01:12:28.120Z</updated>
        <summary type="html"><![CDATA[Learning binary representations of instances and classes is a classical
problem with several high potential applications. In modern settings, the
compression of high-dimensional neural representations to low-dimensional
binary codes is a challenging task and often require large bit-codes to be
accurate. In this work, we propose a novel method for Learning Low-dimensional
binary Codes (LLC) for instances as well as classes. Our method does not
require any side-information, like annotated attributes or label meta-data, and
learns extremely low-dimensional binary codes (~20 bits for ImageNet-1K). The
learnt codes are super-efficient while still ensuring nearly optimal
classification accuracy for ResNet50 on ImageNet-1K. We demonstrate that the
learnt codes capture intrinsically important features in the data, by
discovering an intuitive taxonomy over classes. We further quantitatively
measure the quality of our codes by applying it to the efficient image
retrieval as well as out-of-distribution (OOD) detection problems. For
ImageNet-100 retrieval problem, our learnt binary codes outperform 16 bit
HashNet using only 10 bits and also are as accurate as 10 dimensional real
representations. Finally, our learnt binary codes can perform OOD detection,
out-of-the-box, as accurately as a baseline that needs ~3000 samples to tune
its threshold, while we require none. Code and pre-trained models are available
at https://github.com/RAIVNLab/LLC.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kusupati_A/0/1/0/all/0/1"&gt;Aditya Kusupati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wallingford_M/0/1/0/all/0/1"&gt;Matthew Wallingford&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramanujan_V/0/1/0/all/0/1"&gt;Vivek Ramanujan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Somani_R/0/1/0/all/0/1"&gt;Raghav Somani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1"&gt;Jae Sung Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pillutla_K/0/1/0/all/0/1"&gt;Krishna Pillutla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jain_P/0/1/0/all/0/1"&gt;Prateek Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kakade_S/0/1/0/all/0/1"&gt;Sham Kakade&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Farhadi_A/0/1/0/all/0/1"&gt;Ali Farhadi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MedNLI Is Not Immune: Natural Language Inference Artifacts in the Clinical Domain. (arXiv:2106.01491v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01491</id>
        <link href="http://arxiv.org/abs/2106.01491"/>
        <updated>2021-06-04T01:12:28.113Z</updated>
        <summary type="html"><![CDATA[Crowdworker-constructed natural language inference (NLI) datasets have been
found to contain statistical artifacts associated with the annotation process
that allow hypothesis-only classifiers to achieve better-than-random
performance (Poliak et al., 2018; Gururanganet et al., 2018; Tsuchiya, 2018).
We investigate whether MedNLI, a physician-annotated dataset with premises
extracted from clinical notes, contains such artifacts (Romanov and Shivade,
2018). We find that entailed hypotheses contain generic versions of specific
concepts in the premise, as well as modifiers related to responsiveness,
duration, and probability. Neutral hypotheses feature conditions and behaviors
that co-occur with, or cause, the condition(s) in the premise. Contradiction
hypotheses feature explicit negation of the premise and implicit negation via
assertion of good health. Adversarial filtering demonstrates that performance
degrades when evaluated on the difficult subset. We provide partition
information and recommendations for alternative dataset construction strategies
for knowledge-intensive domains.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Herlihy_C/0/1/0/all/0/1"&gt;Christine Herlihy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rudinger_R/0/1/0/all/0/1"&gt;Rachel Rudinger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cross-Network Learning with Partially Aligned Graph Convolutional Networks. (arXiv:2106.01583v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01583</id>
        <link href="http://arxiv.org/abs/2106.01583"/>
        <updated>2021-06-04T01:12:28.094Z</updated>
        <summary type="html"><![CDATA[Graph neural networks have been widely used for learning representations of
nodes for many downstream tasks on graph data. Existing models were designed
for the nodes on a single graph, which would not be able to utilize information
across multiple graphs. The real world does have multiple graphs where the
nodes are often partially aligned. For examples, knowledge graphs share a
number of named entities though they may have different relation schema;
collaboration networks on publications and awarded projects share some
researcher nodes who are authors and investigators, respectively; people use
multiple web services, shopping, tweeting, rating movies, and some may register
the same email account across the platforms. In this paper, I propose partially
aligned graph convolutional networks to learn node representations across the
models. I investigate multiple methods (including model sharing,
regularization, and alignment reconstruction) as well as theoretical analysis
to positively transfer knowledge across the (small) set of partially aligned
nodes. Extensive experiments on real-world knowledge graphs and collaboration
networks show the superior performance of our proposed methods on relation
classification and link prediction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1"&gt;Meng Jiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimization of Heterogeneous Systems with AI Planning Heuristics and Machine Learning: A Performance and Energy Aware Approach. (arXiv:2106.01441v1 [cs.SE])]]></title>
        <id>http://arxiv.org/abs/2106.01441</id>
        <link href="http://arxiv.org/abs/2106.01441"/>
        <updated>2021-06-04T01:12:28.088Z</updated>
        <summary type="html"><![CDATA[Heterogeneous computing systems provide high performance and energy
efficiency. However, to optimally utilize such systems, solutions that
distribute the work across host CPUs and accelerating devices are needed. In
this paper, we present a performance and energy aware approach that combines AI
planning heuristics for parameter space exploration with a machine learning
model for performance and energy evaluation to determine a near-optimal system
configuration. For data-parallel applications our approach determines a
near-optimal host-device distribution of work, number of processing units
required and the corresponding scheduling strategy. We evaluate our approach
for various heterogeneous systems accelerated with GPU or the Intel Xeon Phi.
The experimental results demonstrate that our approach finds a near-optimal
system configuration by evaluating only about 7% of reasonable configurations.
Furthermore, the performance per Joule estimation of system configurations
using our machine learning model is more than 1000x faster compared to the
system evaluation by program execution.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Memeti_S/0/1/0/all/0/1"&gt;Suejb Memeti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pllana_S/0/1/0/all/0/1"&gt;Sabri Pllana&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Not All Knowledge Is Created Equal. (arXiv:2106.01489v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01489</id>
        <link href="http://arxiv.org/abs/2106.01489"/>
        <updated>2021-06-04T01:12:28.038Z</updated>
        <summary type="html"><![CDATA[Mutual knowledge distillation (MKD) improves a model by distilling knowledge
from another model. However, not all knowledge is certain and correct,
especially under adverse conditions. For example, label noise usually leads to
less reliable models due to the undesired memorisation [1, 2]. Wrong knowledge
misleads the learning rather than helps. This problem can be handled by two
aspects: (i) improving the reliability of a model where the knowledge is from
(i.e., knowledge source's reliability); (ii) selecting reliable knowledge for
distillation. In the literature, making a model more reliable is widely studied
while selective MKD receives little attention. Therefore, we focus on studying
selective MKD and highlight its importance in this work.

Concretely, a generic MKD framework, Confident knowledge selection followed
by Mutual Distillation (CMD), is designed. The key component of CMD is a
generic knowledge selection formulation, making the selection threshold either
static (CMD-S) or progressive (CMD-P). Additionally, CMD covers two special
cases: zero knowledge and all knowledge, leading to a unified MKD framework. We
empirically find CMD-P performs better than CMD-S. The main reason is that a
model's knowledge upgrades and becomes confident as the training progresses.

Extensive experiments are present to demonstrate the effectiveness of CMD and
thoroughly justify the design of CMD. For example, CMD-P obtains new
state-of-the-art results in robustness against label noise.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Ziyun Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xinshao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Haojin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_D/0/1/0/all/0/1"&gt;Di Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Robertson_N/0/1/0/all/0/1"&gt;Neil M. Robertson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Clifton_D/0/1/0/all/0/1"&gt;David A. Clifton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meinel_C/0/1/0/all/0/1"&gt;Christoph Meinel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transformers are Deep Infinite-Dimensional Non-Mercer Binary Kernel Machines. (arXiv:2106.01506v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01506</id>
        <link href="http://arxiv.org/abs/2106.01506"/>
        <updated>2021-06-04T01:12:28.029Z</updated>
        <summary type="html"><![CDATA[Despite their ubiquity in core AI fields like natural language processing,
the mechanics of deep attention-based neural networks like the Transformer
model are not fully understood. In this article, we present a new perspective
towards understanding how Transformers work. In particular, we show that the
"dot-product attention" that is the core of the Transformer's operation can be
characterized as a kernel learning method on a pair of Banach spaces. In
particular, the Transformer's kernel is characterized as having an infinite
feature dimension. Along the way we consider an extension of the standard
kernel learning problem to a binary setting, where data come from two input
domains and a response is defined for every cross-domain pair. We prove a new
representer theorem for these binary kernel machines with non-Mercer
(indefinite, asymmetric) kernels (implying that the functions learned are
elements of reproducing kernel Banach spaces rather than Hilbert spaces), and
also prove a new universal approximation theorem showing that the Transformer
calculation can learn any binary non-Mercer reproducing kernel Banach space
pair. We experiment with new kernels in Transformers, and obtain results that
suggest the infinite dimensionality of the standard Transformer kernel is
partially responsible for its performance. This paper's results provide a new
theoretical understanding of a very important but poorly understood model in
modern machine~learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wright_M/0/1/0/all/0/1"&gt;Matthew A. Wright&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gonzalez_J/0/1/0/all/0/1"&gt;Joseph E. Gonzalez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Luna: Linear Unified Nested Attention. (arXiv:2106.01540v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01540</id>
        <link href="http://arxiv.org/abs/2106.01540"/>
        <updated>2021-06-04T01:12:28.018Z</updated>
        <summary type="html"><![CDATA[The quadratic computational and memory complexities of the Transformer's
attention mechanism have limited its scalability for modeling long sequences.
In this paper, we propose Luna, a linear unified nested attention mechanism
that approximates softmax attention with two nested linear attention functions,
yielding only linear (as opposed to quadratic) time and space complexity.
Specifically, with the first attention function, Luna packs the input sequence
into a sequence of fixed length. Then, the packed sequence is unpacked using
the second attention function. As compared to a more traditional attention
mechanism, Luna introduces an additional sequence with a fixed length as input
and an additional corresponding output, which allows Luna to perform attention
operation linearly, while also storing adequate contextual information. We
perform extensive evaluations on three benchmarks of sequence modeling tasks:
long-context sequence modeling, neural machine translation and masked language
modeling for large-scale pretraining. Competitive or even better experimental
results demonstrate both the effectiveness and efficiency of Luna compared to a
variety]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1"&gt;Xuezhe Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kong_X/0/1/0/all/0/1"&gt;Xiang Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Sinong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1"&gt;Chunting Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+May_J/0/1/0/all/0/1"&gt;Jonathan May&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1"&gt;Hao Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1"&gt;Luke Zettlemoyer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Question Answering Over Temporal Knowledge Graphs. (arXiv:2106.01515v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01515</id>
        <link href="http://arxiv.org/abs/2106.01515"/>
        <updated>2021-06-04T01:12:28.000Z</updated>
        <summary type="html"><![CDATA[Temporal Knowledge Graphs (Temporal KGs) extend regular Knowledge Graphs by
providing temporal scopes (start and end times) on each edge in the KG. While
Question Answering over KG (KGQA) has received some attention from the research
community, QA over Temporal KGs (Temporal KGQA) is a relatively unexplored
area. Lack of broad coverage datasets has been another factor limiting progress
in this area. We address this challenge by presenting CRONQUESTIONS, the
largest known Temporal KGQA dataset, clearly stratified into buckets of
structural complexity. CRONQUESTIONS expands the only known previous dataset by
a factor of 340x. We find that various state-of-the-art KGQA methods fall far
short of the desired performance on this new dataset. In response, we also
propose CRONKGQA, a transformer-based solution that exploits recent advances in
Temporal KG embeddings, and achieves performance superior to all baselines,
with an increase of 120% in accuracy over the next best performing method.
Through extensive experiments, we give detailed insights into the workings of
CRONKGQA, as well as situations where significant further improvements appear
possible. In addition to the dataset, we have released our code as well.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Saxena_A/0/1/0/all/0/1"&gt;Apoorv Saxena&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chakrabarti_S/0/1/0/all/0/1"&gt;Soumen Chakrabarti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Talukdar_P/0/1/0/all/0/1"&gt;Partha Talukdar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SIMLR: Machine Learning inside the SIR model for COVID-19 Forecasting. (arXiv:2106.01590v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01590</id>
        <link href="http://arxiv.org/abs/2106.01590"/>
        <updated>2021-06-04T01:12:27.972Z</updated>
        <summary type="html"><![CDATA[Accurate forecasts of the number of newly infected people during an epidemic
are critical for making effective timely decisions. This paper addresses this
challenge using the SIMLR model, which incorporates machine learning (ML) into
the epidemiological SIR model. For each region, SIMLR tracks the changes in the
policies implemented at the government level, which it uses to estimate the
time-varying parameters of an SIR model for forecasting the number of new
infections 1- to 4-weeks in advance.It also forecasts the probability of
changes in those government policies at each of these future times, which is
essential for the longer-range forecasts. We applied SIMLR to data from regions
in Canada and in the United States,and show that its MAPE (mean average
percentage error) performance is as good as SOTA forecasting models, with the
added advantage of being an interpretable model. We expect that this approach
will be useful not only for forecasting COVID-19 infections, but also in
predicting the evolution of other infectious diseases.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vega_R/0/1/0/all/0/1"&gt;Roberto Vega&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Flores_L/0/1/0/all/0/1"&gt;Leonardo Flores&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Greiner_R/0/1/0/all/0/1"&gt;Russell Greiner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Limitations of Limited Context for Constituency Parsing. (arXiv:2106.01580v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01580</id>
        <link href="http://arxiv.org/abs/2106.01580"/>
        <updated>2021-06-04T01:12:27.951Z</updated>
        <summary type="html"><![CDATA[Incorporating syntax into neural approaches in NLP has a multitude of
practical and scientific benefits. For instance, a language model that is
syntax-aware is likely to be able to produce better samples; even a
discriminative model like BERT with a syntax module could be used for core NLP
tasks like unsupervised syntactic parsing. Rapid progress in recent years was
arguably spurred on by the empirical success of the Parsing-Reading-Predict
architecture of (Shen et al., 2018a), later simplified by the Order Neuron LSTM
of (Shen et al., 2019). Most notably, this is the first time neural approaches
were able to successfully perform unsupervised syntactic parsing (evaluated by
various metrics like F-1 score).

However, even heuristic (much less fully mathematical) understanding of why
and when these architectures work is lagging severely behind. In this work, we
answer representational questions raised by the architectures in (Shen et al.,
2018a, 2019), as well as some transition-based syntax-aware language models
(Dyer et al., 2016): what kind of syntactic structure can current neural
approaches to syntax represent? Concretely, we ground this question in the
sandbox of probabilistic context-free-grammars (PCFGs), and identify a key
aspect of the representational power of these approaches: the amount and
directionality of context that the predictor has access to when forced to make
parsing decision. We show that with limited context (either bounded, or
unidirectional), there are PCFGs, for which these approaches cannot represent
the max-likelihood parse; conversely, if the context is unlimited, they can
represent the max-likelihood parse of any PCFG.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yuchen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Risteski_A/0/1/0/all/0/1"&gt;Andrej Risteski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Gradient Assisted Learning. (arXiv:2106.01425v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01425</id>
        <link href="http://arxiv.org/abs/2106.01425"/>
        <updated>2021-06-04T01:12:27.944Z</updated>
        <summary type="html"><![CDATA[In distributed settings, collaborations between different entities, such as
financial institutions, medical centers, and retail markets, are crucial to
providing improved service and performance. However, the underlying entities
may have little interest in sharing their private data, proprietary models, and
objective functions. These privacy requirements have created new challenges for
collaboration. In this work, we propose Gradient Assisted Learning (GAL), a new
method for various entities to assist each other in supervised learning tasks
without sharing data, models, and objective functions. In this framework, all
participants collaboratively optimize the aggregate of local loss functions,
and each participant autonomously builds its own model by iteratively fitting
the gradients of the objective function. Experimental studies demonstrate that
Gradient Assisted Learning can achieve performance close to centralized
learning when all data, models, and objective functions are fully disclosed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Diao_E/0/1/0/all/0/1"&gt;Enmao Diao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_J/0/1/0/all/0/1"&gt;Jie Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tarokh_V/0/1/0/all/0/1"&gt;Vahid Tarokh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Systematic Investigation of KB-Text Embedding Alignment at Scale. (arXiv:2106.01586v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01586</id>
        <link href="http://arxiv.org/abs/2106.01586"/>
        <updated>2021-06-04T01:12:27.937Z</updated>
        <summary type="html"><![CDATA[Knowledge bases (KBs) and text often contain complementary knowledge: KBs
store structured knowledge that can support long range reasoning, while text
stores more comprehensive and timely knowledge in an unstructured way.
Separately embedding the individual knowledge sources into vector spaces has
demonstrated tremendous successes in encoding the respective knowledge, but how
to jointly embed and reason with both knowledge sources to fully leverage the
complementary information is still largely an open problem. We conduct a
large-scale, systematic investigation of aligning KB and text embeddings for
joint reasoning. We set up a novel evaluation framework with two evaluation
tasks, few-shot link prediction and analogical reasoning, and evaluate an array
of KB-text embedding alignment methods. We also demonstrate how such alignment
can infuse textual information into KB embeddings for more accurate link
prediction on emerging entities and events, using COVID-19 as a case study.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pahuja_V/0/1/0/all/0/1"&gt;Vardaan Pahuja&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1"&gt;Yu Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1"&gt;Wenhu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bahrami_M/0/1/0/all/0/1"&gt;Mehdi Bahrami&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Lei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1"&gt;Wei-Peng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1"&gt;Yu Su&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[IoT Solutions with Multi-Sensor Fusion and Signal-Image Encoding for Secure Data Transfer and Decision Making. (arXiv:2106.01497v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2106.01497</id>
        <link href="http://arxiv.org/abs/2106.01497"/>
        <updated>2021-06-04T01:12:27.919Z</updated>
        <summary type="html"><![CDATA[Deployment of Internet of Things (IoT) devices and Data Fusion techniques
have gained popularity in public and government domains. This usually requires
capturing and consolidating data from multiple sources. As datasets do not
necessarily originate from identical sensors, fused data typically results in a
complex data problem. Because military is investigating how heterogeneous IoT
devices can aid processes and tasks, we investigate a multi-sensor approach.
Moreover, we propose a signal to image encoding approach to transform
information (signal) to integrate (fuse) data from IoT wearable devices to an
image which is invertible and easier to visualize supporting decision making.
Furthermore, we investigate the challenge of enabling an intelligent
identification and detection operation and demonstrate the feasibility of the
proposed Deep Learning and Anomaly Detection models that can support future
application that utilizes hand gesture data from wearable devices.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Sharma_P/0/1/0/all/0/1"&gt;Piyush K. Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dennison_M/0/1/0/all/0/1"&gt;Mark Dennison&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Raglin_A/0/1/0/all/0/1"&gt;Adrienne Raglin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[When Vision Transformers Outperform ResNets without Pretraining or Strong Data Augmentations. (arXiv:2106.01548v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.01548</id>
        <link href="http://arxiv.org/abs/2106.01548"/>
        <updated>2021-06-04T01:12:27.912Z</updated>
        <summary type="html"><![CDATA[Vision Transformers (ViTs) and MLPs signal further efforts on replacing
hand-wired features or inductive biases with general-purpose neural
architectures. Existing works empower the models by massive data, such as
large-scale pretraining and/or repeated strong data augmentations, and still
report optimization-related problems (e.g., sensitivity to initialization and
learning rate). Hence, this paper investigates ViTs and MLP-Mixers from the
lens of loss geometry, intending to improve the models' data efficiency at
training and generalization at inference. Visualization and Hessian reveal
extremely sharp local minima of converged models. By promoting smoothness with
a recently proposed sharpness-aware optimizer, we substantially improve the
accuracy and robustness of ViTs and MLP-Mixers on various tasks spanning
supervised, adversarial, contrastive, and transfer learning (e.g., +5.3\% and
+11.0\% top-1 accuracy on ImageNet for ViT-B/16 and Mixer-B/16, respectively,
with the simple Inception-style preprocessing). We show that the improved
smoothness attributes to sparser active neurons in the first few layers. The
resultant ViTs outperform ResNets of similar size and throughput when trained
from scratch on ImageNet without large-scale pretraining or strong data
augmentations. They also possess more perceptive attention maps.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xiangning Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1"&gt;Cho-Jui Hsieh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_B/0/1/0/all/0/1"&gt;Boqing Gong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ember: No-Code Context Enrichment via Similarity-Based Keyless Joins. (arXiv:2106.01501v1 [cs.DB])]]></title>
        <id>http://arxiv.org/abs/2106.01501</id>
        <link href="http://arxiv.org/abs/2106.01501"/>
        <updated>2021-06-04T01:12:27.903Z</updated>
        <summary type="html"><![CDATA[Structured data, or data that adheres to a pre-defined schema, can suffer
from fragmented context: information describing a single entity can be
scattered across multiple datasets or tables tailored for specific business
needs, with no explicit linking keys (e.g., primary key-foreign key
relationships or heuristic functions). Context enrichment, or rebuilding
fragmented context, using keyless joins is an implicit or explicit step in
machine learning (ML) pipelines over structured data sources. This process is
tedious, domain-specific, and lacks support in now-prevalent no-code ML systems
that let users create ML pipelines using just input data and high-level
configuration files. In response, we propose Ember, a system that abstracts and
automates keyless joins to generalize context enrichment. Our key insight is
that Ember can enable a general keyless join operator by constructing an index
populated with task-specific embeddings. Ember learns these embeddings by
leveraging Transformer-based representation learning techniques. We describe
our core architectural principles and operators when developing Ember, and
empirically demonstrate that Ember allows users to develop no-code pipelines
for five domains, including search, recommendation and question answering, and
can exceed alternatives by up to 39% recall, with as little as a single line
configuration change.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Suri_S/0/1/0/all/0/1"&gt;Sahaana Suri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ilyas_I/0/1/0/all/0/1"&gt;Ihab F. Ilyas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Re_C/0/1/0/all/0/1"&gt;Christopher R&amp;#xe9;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rekatsinas_T/0/1/0/all/0/1"&gt;Theodoros Rekatsinas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Weakly Supervised Learning Creates a Fusion of Modeling Cultures. (arXiv:2106.01485v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.01485</id>
        <link href="http://arxiv.org/abs/2106.01485"/>
        <updated>2021-06-04T01:12:27.895Z</updated>
        <summary type="html"><![CDATA[The past two decades have witnessed the great success of the algorithmic
modeling framework advocated by Breiman et al. (2001). Nevertheless, the
excellent prediction performance of these black-box models rely heavily on the
availability of strong supervision, i.e. a large set of accurate and exact
ground-truth labels. In practice, strong supervision can be unavailable or
expensive, which calls for modeling techniques under weak supervision. In this
comment, we summarize the key concepts in weakly supervised learning and
discuss some recent developments in the field. Using algorithmic modeling alone
under a weak supervision might lead to unstable and misleading results. A
promising direction would be integrating the data modeling culture into such a
framework.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Tang_C/0/1/0/all/0/1"&gt;Chengliang Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Yuan_G/0/1/0/all/0/1"&gt;Gan Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zheng_T/0/1/0/all/0/1"&gt;Tian Zheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Normalizing Flows for Knockoff-free Controlled Feature Selection. (arXiv:2106.01528v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.01528</id>
        <link href="http://arxiv.org/abs/2106.01528"/>
        <updated>2021-06-04T01:12:27.887Z</updated>
        <summary type="html"><![CDATA[The goal of controlled feature selection is to discover the features a
response depends on while limiting the proportion of false discoveries to a
predefined level. Recently, multiple methods have been proposed that use deep
learning to generate knockoffs for controlled feature selection through the
Model-X knockoff framework. We demonstrate, however, that these methods often
fail to control the false discovery rate (FDR). There are two reasons for this
shortcoming. First, these methods often learn inaccurate models of features.
Second, the "swap" property, which is required for knockoffs to be valid, is
often not well enforced. We propose a new procedure called FlowSelect that
remedies both of these problems. To more accurately model the features,
FlowSelect uses normalizing flows, the state-of-the-art method for density
estimation. To circumvent the need to enforce the swap property, FlowSelect
uses a novel MCMC-based procedure to directly compute p-values for each
feature. Asymptotically, FlowSelect controls the FDR exactly. Empirically,
FlowSelect controls the FDR well on both synthetic and semi-synthetic
benchmarks, whereas competing knockoff-based approaches fail to do so.
FlowSelect also demonstrates greater power on these benchmarks. Additionally,
using data from a genome-wide association study of soybeans, FlowSelect
correctly infers the genetic variants associated with specific soybean traits.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Hansen_D/0/1/0/all/0/1"&gt;Derek Hansen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Manzo_B/0/1/0/all/0/1"&gt;Brian Manzo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Regier_J/0/1/0/all/0/1"&gt;Jeffrey Regier&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Inferring Black Hole Properties from Astronomical Multivariate Time Series with Bayesian Attentive Neural Processes. (arXiv:2106.01450v1 [astro-ph.IM])]]></title>
        <id>http://arxiv.org/abs/2106.01450</id>
        <link href="http://arxiv.org/abs/2106.01450"/>
        <updated>2021-06-04T01:12:27.736Z</updated>
        <summary type="html"><![CDATA[Among the most extreme objects in the Universe, active galactic nuclei (AGN)
are luminous centers of galaxies where a black hole feeds on surrounding
matter. The variability patterns of the light emitted by an AGN contain
information about the physical properties of the underlying black hole.
Upcoming telescopes will observe over 100 million AGN in multiple broadband
wavelengths, yielding a large sample of multivariate time series with long gaps
and irregular sampling. We present a method that reconstructs the AGN time
series and simultaneously infers the posterior probability density distribution
(PDF) over the physical quantities of the black hole, including its mass and
luminosity. We apply this method to a simulated dataset of 11,000 AGN and
report precision and accuracy of 0.4 dex and 0.3 dex in the inferred black hole
mass. This work is the first to address probabilistic time series
reconstruction and parameter inference for AGN in an end-to-end fashion.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/astro-ph/1/au:+Park_J/0/1/0/all/0/1"&gt;Ji Won Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Villar_A/0/1/0/all/0/1"&gt;Ashley Villar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Jiang_Y/0/1/0/all/0/1"&gt;Yan-Fei Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Ho_S/0/1/0/all/0/1"&gt;Shirley Ho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Lin_J/0/1/0/all/0/1"&gt;Joshua Yao-Yu Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Marshall_P/0/1/0/all/0/1"&gt;Philip J. Marshall&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Roodman_A/0/1/0/all/0/1"&gt;Aaron Roodman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Minimax Optimization with Smooth Algorithmic Adversaries. (arXiv:2106.01488v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01488</id>
        <link href="http://arxiv.org/abs/2106.01488"/>
        <updated>2021-06-04T01:12:27.713Z</updated>
        <summary type="html"><![CDATA[This paper considers minimax optimization $\min_x \max_y f(x, y)$ in the
challenging setting where $f$ can be both nonconvex in $x$ and nonconcave in
$y$. Though such optimization problems arise in many machine learning paradigms
including training generative adversarial networks (GANs) and adversarially
robust models, many fundamental issues remain in theory, such as the absence of
efficiently computable optimality notions, and cyclic or diverging behavior of
existing algorithms. Our framework sprouts from the practical consideration
that under a computational budget, the max-player can not fully maximize
$f(x,\cdot)$ since nonconcave maximization is NP-hard in general. So, we
propose a new algorithm for the min-player to play against smooth algorithms
deployed by the adversary (i.e., the max-player) instead of against full
maximization. Our algorithm is guaranteed to make monotonic progress (thus
having no limit cycles), and to find an appropriate "stationary point" in a
polynomial number of iterations. Our framework covers practical settings where
the smooth algorithms deployed by the adversary are multi-step stochastic
gradient ascent, and its accelerated version. We further provide complementing
experiments that confirm our theoretical findings and demonstrate the
effectiveness of the proposed approach in practice.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fiez_T/0/1/0/all/0/1"&gt;Tanner Fiez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_C/0/1/0/all/0/1"&gt;Chi Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Netrapalli_P/0/1/0/all/0/1"&gt;Praneeth Netrapalli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ratliff_L/0/1/0/all/0/1"&gt;Lillian J. Ratliff&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ethical-Advice Taker: Do Language Models Understand Natural Language Interventions?. (arXiv:2106.01465v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01465</id>
        <link href="http://arxiv.org/abs/2106.01465"/>
        <updated>2021-06-04T01:12:27.700Z</updated>
        <summary type="html"><![CDATA[Is it possible to use natural language to intervene in a model's behavior and
alter its prediction in a desired way? We investigate the effectiveness of
natural language interventions for reading-comprehension systems, studying this
in the context of social stereotypes. Specifically, we propose a new language
understanding task, Linguistic Ethical Interventions (LEI), where the goal is
to amend a question-answering (QA) model's unethical behavior by communicating
context-specific principles of ethics and equity to it. To this end, we build
upon recent methods for quantifying a system's social stereotypes, augmenting
them with different kinds of ethical interventions and the desired model
behavior under such interventions. Our zero-shot evaluation finds that even
today's powerful neural language models are extremely poor ethical-advice
takers, that is, they respond surprisingly little to ethical interventions even
though these interventions are stated as simple sentences. Few-shot learning
improves model behavior but remains far from the desired outcome, especially
when evaluated for various types of generalization. Our new task thus poses a
novel language understanding challenge for the community.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1"&gt;Jieyu Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khashabi_D/0/1/0/all/0/1"&gt;Daniel Khashabi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khot_T/0/1/0/all/0/1"&gt;Tushar Khot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sabharwal_A/0/1/0/all/0/1"&gt;Ashish Sabharwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1"&gt;Kai-Wei Chang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Memory Wrap: a Data-Efficient and Interpretable Extension to Image Classification Models. (arXiv:2106.01440v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01440</id>
        <link href="http://arxiv.org/abs/2106.01440"/>
        <updated>2021-06-04T01:12:27.692Z</updated>
        <summary type="html"><![CDATA[Due to their black-box and data-hungry nature, deep learning techniques are
not yet widely adopted for real-world applications in critical domains, like
healthcare and justice. This paper presents Memory Wrap, a plug-and-play
extension to any image classification model. Memory Wrap improves both
data-efficiency and model interpretability, adopting a content-attention
mechanism between the input and some memories of past training samples. We show
that Memory Wrap outperforms standard classifiers when it learns from a limited
set of data, and it reaches comparable performance when it learns from the full
dataset. We discuss how its structure and content-attention mechanisms make
predictions interpretable, compared to standard classifiers. To this end, we
both show a method to build explanations by examples and counterfactuals, based
on the memory content, and how to exploit them to get insights about its
decision process. We test our approach on image classification tasks using
several architectures on three different datasets, namely CIFAR10, SVHN, and
CINIC10.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rosa_B/0/1/0/all/0/1"&gt;Biagio La Rosa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Capobianco_R/0/1/0/all/0/1"&gt;Roberto Capobianco&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nardi_D/0/1/0/all/0/1"&gt;Daniele Nardi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BERT-Defense: A Probabilistic Model Based on BERT to Combat Cognitively Inspired Orthographic Adversarial Attacks. (arXiv:2106.01452v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01452</id>
        <link href="http://arxiv.org/abs/2106.01452"/>
        <updated>2021-06-04T01:12:27.683Z</updated>
        <summary type="html"><![CDATA[Adversarial attacks expose important blind spots of deep learning systems.
While word- and sentence-level attack scenarios mostly deal with finding
semantic paraphrases of the input that fool NLP models, character-level attacks
typically insert typos into the input stream. It is commonly thought that these
are easier to defend via spelling correction modules. In this work, we show
that both a standard spellchecker and the approach of Pruthi et al. (2019),
which trains to defend against insertions, deletions and swaps, perform poorly
on the character-level benchmark recently proposed in Eger and Benz (2020)
which includes more challenging attacks such as visual and phonetic
perturbations and missing word segmentations. In contrast, we show that an
untrained iterative approach which combines context-independent character-level
information with context-dependent information from BERT's masked language
modeling can perform on par with human crowd-workers from Amazon Mechanical
Turk (AMT) supervised via 3-shot learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Keller_Y/0/1/0/all/0/1"&gt;Yannik Keller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mackensen_J/0/1/0/all/0/1"&gt;Jan Mackensen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eger_S/0/1/0/all/0/1"&gt;Steffen Eger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Parallelizing Thompson Sampling. (arXiv:2106.01420v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01420</id>
        <link href="http://arxiv.org/abs/2106.01420"/>
        <updated>2021-06-04T01:12:27.674Z</updated>
        <summary type="html"><![CDATA[How can we make use of information parallelism in online decision making
problems while efficiently balancing the exploration-exploitation trade-off? In
this paper, we introduce a batch Thompson Sampling framework for two canonical
online decision making problems, namely, stochastic multi-arm bandit and linear
contextual bandit with finitely many arms. Over a time horizon $T$, our
\textit{batch} Thompson Sampling policy achieves the same (asymptotic) regret
bound of a fully sequential one while carrying out only $O(\log T)$ batch
queries. To achieve this exponential reduction, i.e., reducing the number of
interactions from $T$ to $O(\log T)$, our batch policy dynamically determines
the duration of each batch in order to balance the exploration-exploitation
trade-off. We also demonstrate experimentally that dynamic batch allocation
dramatically outperforms natural baselines such as static batch allocations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Karbasi_A/0/1/0/all/0/1"&gt;Amin Karbasi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mirrokni_V/0/1/0/all/0/1"&gt;Vahab Mirrokni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shadravan_M/0/1/0/all/0/1"&gt;Mohammad Shadravan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Smooth Bilevel Programming for Sparse Regularization. (arXiv:2106.01429v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.01429</id>
        <link href="http://arxiv.org/abs/2106.01429"/>
        <updated>2021-06-04T01:12:27.655Z</updated>
        <summary type="html"><![CDATA[Iteratively reweighted least square (IRLS) is a popular approach to solve
sparsity-enforcing regression problems in machine learning. State of the art
approaches are more efficient but typically rely on specific coordinate pruning
schemes. In this work, we show how a surprisingly simple reparametrization of
IRLS, coupled with a bilevel resolution (instead of an alternating scheme) is
able to achieve top performances on a wide range of sparsity (such as Lasso,
group Lasso and trace norm regularizations), regularization strength (including
hard constraints), and design matrices (ranging from correlated designs to
differential operators). Similarly to IRLS, our method only involves linear
systems resolutions, but in sharp contrast, corresponds to the minimization of
a smooth function. Despite being non-convex, we show that there is no spurious
minima and that saddle points are "ridable", so that there always exists a
descent direction. We thus advocate for the use of a BFGS quasi-Newton solver,
which makes our approach simple, robust and efficient. We perform a numerical
benchmark of the convergence speed of our algorithm against state of the art
solvers for Lasso, group Lasso, trace norm and linearly constrained problems.
These results highlight the versatility of our approach, removing the need to
use different solvers depending on the specificity of the ML problem under
study.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Poon_C/0/1/0/all/0/1"&gt;Clarice Poon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Peyre_G/0/1/0/all/0/1"&gt;Gabriel Peyr&amp;#xe9;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SemiFL: Communication Efficient Semi-Supervised Federated Learning with Unlabeled Clients. (arXiv:2106.01432v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01432</id>
        <link href="http://arxiv.org/abs/2106.01432"/>
        <updated>2021-06-04T01:12:27.646Z</updated>
        <summary type="html"><![CDATA[Federated Learning allows training machine learning models by using the
computation and private data resources of a large number of distributed clients
such as smartphones and IoT devices. Most existing works on Federated Learning
(FL) assume the clients have ground-truth labels. However, in many practical
scenarios, clients may be unable to label task-specific data, e.g., due to lack
of expertise. In this work, we consider a server that hosts a labeled dataset,
and wishes to leverage clients with unlabeled data for supervised learning. We
propose a new Federated Learning framework referred to as SemiFL in order to
address the problem of Semi-Supervised Federated Learning (SSFL). In SemiFL,
clients have completely unlabeled data, while the server has a small amount of
labeled data. SemiFL is communication efficient since it separates the training
of server-side supervised data and client-side unsupervised data. We
demonstrate various efficient strategies of SemiFL that enhance learning
performance. Extensive empirical evaluations demonstrate that our communication
efficient method can significantly improve the performance of a labeled server
with unlabeled clients. Moreover, we demonstrate that SemiFL can outperform
many existing FL results trained with fully supervised data, and perform
competitively with the state-of-the-art centralized Semi-Supervised Learning
(SSL) methods. For instance, in standard communication efficient scenarios, our
method can perform 93% accuracy on the CIFAR10 dataset with only 4000 labeled
samples at the server. Such accuracy is only 2% away from the result trained
from 50000 fully labeled data, and it improves about 30% upon existing SSFL
methods in the communication efficient setting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Diao_E/0/1/0/all/0/1"&gt;Enmao Diao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_J/0/1/0/all/0/1"&gt;Jie Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tarokh_V/0/1/0/all/0/1"&gt;Vahid Tarokh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robot in a China Shop: Using Reinforcement Learning for Location-Specific Navigation Behaviour. (arXiv:2106.01434v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2106.01434</id>
        <link href="http://arxiv.org/abs/2106.01434"/>
        <updated>2021-06-04T01:12:27.631Z</updated>
        <summary type="html"><![CDATA[Robots need to be able to work in multiple different environments. Even when
performing similar tasks, different behaviour should be deployed to best fit
the current environment. In this paper, We propose a new approach to
navigation, where it is treated as a multi-task learning problem. This enables
the robot to learn to behave differently in visual navigation tasks for
different environments while also learning shared expertise across
environments. We evaluated our approach in both simulated environments as well
as real-world data. Our method allows our system to converge with a 26%
reduction in training time, while also increasing accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bian_X/0/1/0/all/0/1"&gt;Xihan Bian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mendez_O/0/1/0/all/0/1"&gt;Oscar Mendez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hadfield_S/0/1/0/all/0/1"&gt;Simon Hadfield&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[One Representation to Rule Them All: Identifying Out-of-Support Examples in Few-shot Learning with Generic Representations. (arXiv:2106.01423v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01423</id>
        <link href="http://arxiv.org/abs/2106.01423"/>
        <updated>2021-06-04T01:12:27.622Z</updated>
        <summary type="html"><![CDATA[The field of few-shot learning has made remarkable strides in developing
powerful models that can operate in the small data regime. Nearly all of these
methods assume every unlabeled instance encountered will belong to a handful of
known classes for which one has examples. This can be problematic for
real-world use cases where one routinely finds 'none-of-the-above' examples. In
this paper we describe this challenge of identifying what we term
'out-of-support' (OOS) examples. We describe how this problem is subtly
different from out-of-distribution detection and describe a new method of
identifying OOS examples within the Prototypical Networks framework using a
fixed point which we call the generic representation. We show that our method
outperforms other existing approaches in the literature as well as other
approaches that we propose in this paper. Finally, we investigate how the use
of such a generic point affects the geometry of a model's feature space.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kvinge_H/0/1/0/all/0/1"&gt;Henry Kvinge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Howland_S/0/1/0/all/0/1"&gt;Scott Howland&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Courts_N/0/1/0/all/0/1"&gt;Nico Courts&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Phillips_L/0/1/0/all/0/1"&gt;Lauren A. Phillips&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Buckheit_J/0/1/0/all/0/1"&gt;John Buckheit&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+New_Z/0/1/0/all/0/1"&gt;Zachary New&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Skomski_E/0/1/0/all/0/1"&gt;Elliott Skomski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jung H. Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tiwari_S/0/1/0/all/0/1"&gt;Sandeep Tiwari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hibler_J/0/1/0/all/0/1"&gt;Jessica Hibler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Corley_C/0/1/0/all/0/1"&gt;Courtney D. Corley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hodas_N/0/1/0/all/0/1"&gt;Nathan O. Hodas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rectangular Flows for Manifold Learning. (arXiv:2106.01413v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.01413</id>
        <link href="http://arxiv.org/abs/2106.01413"/>
        <updated>2021-06-04T01:12:27.611Z</updated>
        <summary type="html"><![CDATA[Normalizing flows are invertible neural networks with tractable
change-of-volume terms, which allows optimization of their parameters to be
efficiently performed via maximum likelihood. However, data of interest is
typically assumed to live in some (often unknown) low-dimensional manifold
embedded in high-dimensional ambient space. The result is a modelling mismatch
since -- by construction -- the invertibility requirement implies
high-dimensional support of the learned distribution. Injective flows, mapping
from low- to high-dimensional space, aim to fix this discrepancy by learning
distributions on manifolds, but the resulting volume-change term becomes more
challenging to evaluate. Current approaches either avoid computing this term
entirely using various heuristics, or assume the manifold is known beforehand
and therefore are not widely applicable. Instead, we propose two methods to
tractably calculate the gradient of this term with respect to the parameters of
the model, relying on careful use of automatic differentiation and techniques
from numerical linear algebra. Both approaches perform end-to-end nonlinear
manifold learning and density estimation for data projected onto this manifold.
We study the trade-offs between our proposed methods, empirically verify that
we outperform approaches ignoring the volume-change term by more accurately
learning manifolds and the corresponding distributions on them, and show
promising results on out-of-distribution detection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Caterini_A/0/1/0/all/0/1"&gt;Anthony L. Caterini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Loaiza_Ganem_G/0/1/0/all/0/1"&gt;Gabriel Loaiza-Ganem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Pleiss_G/0/1/0/all/0/1"&gt;Geoff Pleiss&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Cunningham_J/0/1/0/all/0/1"&gt;John P. Cunningham&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Variational Empowerment as Representation Learning for Goal-Based Reinforcement Learning. (arXiv:2106.01404v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01404</id>
        <link href="http://arxiv.org/abs/2106.01404"/>
        <updated>2021-06-04T01:12:27.492Z</updated>
        <summary type="html"><![CDATA[Learning to reach goal states and learning diverse skills through mutual
information (MI) maximization have been proposed as principled frameworks for
self-supervised reinforcement learning, allowing agents to acquire broadly
applicable multitask policies with minimal reward engineering. Starting from a
simple observation that the standard goal-conditioned RL (GCRL) is encapsulated
by the optimization objective of variational empowerment, we discuss how GCRL
and MI-based RL can be generalized into a single family of methods, which we
name variational GCRL (VGCRL), interpreting variational MI maximization, or
variational empowerment, as representation learning methods that acquire
functionally-aware state representations for goal reaching. This novel
perspective allows us to: (1) derive simple but unexplored variants of GCRL to
study how adding small representation capacity can already expand its
capabilities; (2) investigate how discriminator function capacity and
smoothness determine the quality of discovered skills, or latent goals, through
modifying latent dimensionality and applying spectral normalization; (3) adapt
techniques such as hindsight experience replay (HER) from GCRL to MI-based RL;
and lastly, (4) propose a novel evaluation metric, named latent goal reaching
(LGR), for comparing empowerment algorithms with different choices of latent
dimensionality and discriminator parameterization. Through principled
mathematical derivations and careful experimental studies, our work lays a
novel foundation from which to evaluate, analyze, and develop representation
learning techniques in goal-based RL.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1"&gt;Jongwook Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1"&gt;Archit Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1"&gt;Honglak Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1"&gt;Sergey Levine&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_S/0/1/0/all/0/1"&gt;Shixiang Shane Gu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[q-RBFNN:A Quantum Calculus-based RBF Neural Network. (arXiv:2106.01370v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01370</id>
        <link href="http://arxiv.org/abs/2106.01370"/>
        <updated>2021-06-04T01:12:27.482Z</updated>
        <summary type="html"><![CDATA[In this research a novel stochastic gradient descent based learning approach
for the radial basis function neural networks (RBFNN) is proposed. The proposed
method is based on the q-gradient which is also known as Jackson derivative. In
contrast to the conventional gradient, which finds the tangent, the q-gradient
finds the secant of the function and takes larger steps towards the optimal
solution. The proposed $q$-RBFNN is analyzed for its convergence performance in
the context of least square algorithm. In particular, a closed form expression
of the Wiener solution is obtained, and stability bounds of the learning rate
(step-size) is derived. The analytical results are validated through computer
simulation. Additionally, we propose an adaptive technique for the time-varying
$q$-parameter to improve convergence speed with no trade-offs in the steady
state performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hussain_S/0/1/0/all/0/1"&gt;Syed Saiq Hussain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Usman_M/0/1/0/all/0/1"&gt;Muhammad Usman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Siddique_T/0/1/0/all/0/1"&gt;Taha Hasan Masood Siddique&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Naseem_I/0/1/0/all/0/1"&gt;Imran Naseem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Togneri_R/0/1/0/all/0/1"&gt;Roberto Togneri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bennamoun_M/0/1/0/all/0/1"&gt;Mohammed Bennamoun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Single-component gradient rules for variational quantum algorithms. (arXiv:2106.01388v1 [quant-ph])]]></title>
        <id>http://arxiv.org/abs/2106.01388</id>
        <link href="http://arxiv.org/abs/2106.01388"/>
        <updated>2021-06-04T01:12:27.473Z</updated>
        <summary type="html"><![CDATA[Many near-term quantum computing algorithms are conceived as variational
quantum algorithms, in which parameterized quantum circuits are optimized in a
hybrid quantum-classical setup. Examples are variational quantum eigensolvers,
quantum approximate optimization algorithms as well as various algorithms in
the context of quantum-assisted machine learning. A common bottleneck of any
such algorithm is constituted by the optimization of the variational
parameters. A popular set of optimization methods work on the estimate of the
gradient, obtained by means of circuit evaluations. We will refer to the way in
which one can combine these circuit evaluations as gradient rules. This work
provides a comprehensive picture of the family of gradient rules that vary
parameters of quantum gates individually. The most prominent known members of
this family are the parameter shift rule and the finite differences method. To
unite this family, we propose a generalized parameter shift rule that expresses
all members of the aforementioned family as special cases, and discuss how all
of these can be seen as providing access to a linear combination of exact
first- and second-order derivatives. We further prove that a parameter shift
rule with one non-shifted evaluation and only one shifted circuit evaluation
can not exist does not exist, and introduce a novel perspective for approaching
new gradient rules.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Hubregtsen_T/0/1/0/all/0/1"&gt;Thomas Hubregtsen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Wilde_F/0/1/0/all/0/1"&gt;Frederik Wilde&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Qasim_S/0/1/0/all/0/1"&gt;Shozab Qasim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Eisert_J/0/1/0/all/0/1"&gt;Jens Eisert&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dual Script E2E framework for Multilingual and Code-Switching ASR. (arXiv:2106.01400v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2106.01400</id>
        <link href="http://arxiv.org/abs/2106.01400"/>
        <updated>2021-06-04T01:12:27.461Z</updated>
        <summary type="html"><![CDATA[India is home to multiple languages, and training automatic speech
recognition (ASR) systems for languages is challenging. Over time, each
language has adopted words from other languages, such as English, leading to
code-mixing. Most Indian languages also have their own unique scripts, which
poses a major limitation in training multilingual and code-switching ASR
systems.

Inspired by results in text-to-speech synthesis, in this work, we use an
in-house rule-based phoneme-level common label set (CLS) representation to
train multilingual and code-switching ASR for Indian languages. We propose two
end-to-end (E2E) ASR systems. In the first system, the E2E model is trained on
the CLS representation, and we use a novel data-driven back-end to recover the
native language script. In the second system, we propose a modification to the
E2E model, wherein the CLS representation and the native language characters
are used simultaneously for training. We show our results on the multilingual
and code-switching tasks of the Indic ASR Challenge 2021. Our best results
achieve 6% and 5% improvement (approx) in word error rate over the baseline
system for the multilingual and code-switching tasks, respectively, on the
challenge development data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Kumar_M/0/1/0/all/0/1"&gt;Mari Ganesh Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kuriakose_J/0/1/0/all/0/1"&gt;Jom Kuriakose&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Thyagachandran_A/0/1/0/all/0/1"&gt;Anand Thyagachandran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+A_A/0/1/0/all/0/1"&gt;Arun Kumar A&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Seth_A/0/1/0/all/0/1"&gt;Ashish Seth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Prasad_L/0/1/0/all/0/1"&gt;Lodagala Durga Prasad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jaiswal_S/0/1/0/all/0/1"&gt;Saish Jaiswal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Prakash_A/0/1/0/all/0/1"&gt;Anusha Prakash&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Murthy_H/0/1/0/all/0/1"&gt;Hema Murthy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Undecidability of Learnability. (arXiv:2106.01382v1 [cs.CC])]]></title>
        <id>http://arxiv.org/abs/2106.01382</id>
        <link href="http://arxiv.org/abs/2106.01382"/>
        <updated>2021-06-04T01:12:27.447Z</updated>
        <summary type="html"><![CDATA[Machine learning researchers and practitioners steadily enlarge the multitude
of successful learning models. They achieve this through in-depth theoretical
analyses and experiential heuristics. However, there is no known
general-purpose procedure for rigorously evaluating whether newly proposed
models indeed successfully learn from data. We show that such a procedure
cannot exist. For PAC binary classification, uniform and universal online
learning, and exact learning through teacher-learner interactions, learnability
is in general undecidable, both in the sense of independence of the axioms in a
formal system and in the sense of uncomputability. Our proofs proceed via
computable constructions of function classes that encode the consistency
problem for formal systems and the halting problem for Turing machines into
complexity measures that characterize learnability. Our work shows that
undecidability appears in the theoretical foundations of machine learning:
There is no one-size-fits-all algorithm for deciding whether a machine learning
model can be successful. We cannot in general automatize the process of
assessing new learning models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Caro_M/0/1/0/all/0/1"&gt;Matthias C. Caro&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On using distributed representations of source code for the detection of C security vulnerabilities. (arXiv:2106.01367v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2106.01367</id>
        <link href="http://arxiv.org/abs/2106.01367"/>
        <updated>2021-06-04T01:12:27.425Z</updated>
        <summary type="html"><![CDATA[This paper presents an evaluation of the code representation model Code2vec
when trained on the task of detecting security vulnerabilities in C source
code. We leverage the open-source library astminer to extract path-contexts
from the abstract syntax trees of a corpus of labeled C functions. Code2vec is
trained on the resulting path-contexts with the task of classifying a function
as vulnerable or non-vulnerable. Using the CodeXGLUE benchmark, we show that
the accuracy of Code2vec for this task is comparable to simple
transformer-based methods such as pre-trained RoBERTa, and outperforms more
naive NLP-based methods. We achieved an accuracy of 61.43% while maintaining
low computational requirements relative to larger models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Coimbra_D/0/1/0/all/0/1"&gt;David Coimbra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reis_S/0/1/0/all/0/1"&gt;Sofia Reis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abreu_R/0/1/0/all/0/1"&gt;Rui Abreu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pasareanu_C/0/1/0/all/0/1"&gt;Corina P&amp;#x103;s&amp;#x103;reanu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Erdogmus_H/0/1/0/all/0/1"&gt;Hakan Erdogmus&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automatic Assessment of the Design Quality of Python Programs with Personalized Feedback. (arXiv:2106.01399v1 [cs.SE])]]></title>
        <id>http://arxiv.org/abs/2106.01399</id>
        <link href="http://arxiv.org/abs/2106.01399"/>
        <updated>2021-06-04T01:12:27.402Z</updated>
        <summary type="html"><![CDATA[The assessment of program functionality can generally be accomplished with
straight-forward unit tests. However, assessing the design quality of a program
is a much more difficult and nuanced problem. Design quality is an important
consideration since it affects the readability and maintainability of programs.
Assessing design quality and giving personalized feedback is very time
consuming task for instructors and teaching assistants. This limits the scale
of giving personalized feedback to small class settings. Further, design
quality is nuanced and is difficult to concisely express as a set of rules. For
these reasons, we propose a neural network model to both automatically assess
the design of a program and provide personalized feedback to guide students on
how to make corrections. The model's effectiveness is evaluated on a corpus of
student programs written in Python. The model has an accuracy rate from 83.67%
to 94.27%, depending on the dataset, when predicting design scores as compared
to historical instructor assessment. Finally, we present a study where students
tried to improve the design of their programs based on the personalized
feedback produced by the model. Students who participated in the study improved
their program design scores by 19.58%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Orr_J/0/1/0/all/0/1"&gt;J. Walker Orr&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Russell_N/0/1/0/all/0/1"&gt;Nathaniel Russell&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimizing Rankings for Recommendation in Matching Markets. (arXiv:2106.01941v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.01941</id>
        <link href="http://arxiv.org/abs/2106.01941"/>
        <updated>2021-06-04T01:12:26.481Z</updated>
        <summary type="html"><![CDATA[Based on the success of recommender systems in e-commerce, there is growing
interest in their use in matching markets (e.g., labor). While this holds
potential for improving market fluidity and fairness, we show in this paper
that naively applying existing recommender systems to matching markets is
sub-optimal. Considering the standard process where candidates apply and then
get evaluated by employers, we present a new recommendation framework to model
this interaction mechanism and propose efficient algorithms for computing
personalized rankings in this setting. We show that the optimal rankings need
to not only account for the potentially divergent preferences of candidates and
employers, but they also need to account for capacity constraints. This makes
conventional ranking systems that merely rank by some local score (e.g.,
one-sided or reciprocal relevance) highly sub-optimal -- not only for an
individual user, but also for societal goals (e.g., low unemployment). To
address this shortcoming, we propose the first method for jointly optimizing
the rankings for all candidates in the market to explicitly maximize social
welfare. In addition to the theoretical derivation, we evaluate the method both
on simulated environments and on data from a real-world
networking-recommendation system that we built and fielded at a large computer
science conference.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1"&gt;Yi Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bayoumi_M/0/1/0/all/0/1"&gt;Magd Bayoumi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joachims_T/0/1/0/all/0/1"&gt;Thorsten Joachims&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[EmoDNN: Understanding emotions from short texts through a deep neural network ensemble. (arXiv:2106.01706v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01706</id>
        <link href="http://arxiv.org/abs/2106.01706"/>
        <updated>2021-06-04T01:12:26.470Z</updated>
        <summary type="html"><![CDATA[The latent knowledge in the emotions and the opinions of the individuals that
are manifested via social networks are crucial to numerous applications
including social management, dynamical processes, and public security.
Affective computing, as an interdisciplinary research field, linking artificial
intelligence to cognitive inference, is capable to exploit emotion-oriented
knowledge from brief contents. The textual contents convey hidden information
such as personality and cognition about corresponding authors that can
determine both correlations and variations between users. Emotion recognition
from brief contents should embrace the contrast between authors where the
differences in personality and cognition can be traced within emotional
expressions. To tackle this challenge, we devise a framework that, on the one
hand, infers latent individual aspects, from brief contents and, on the other
hand, presents a novel ensemble classifier equipped with dynamic dropout
convnets to extract emotions from textual context. To categorize short text
contents, our proposed method conjointly leverages cognitive factors and
exploits hidden information. We utilize the outcome vectors in a novel
embedding model to foster emotion-pertinent features that are collectively
assembled by lexicon inductions. Experimental results show that compared to
other competitors, our proposed model can achieve a higher performance in
recognizing emotion from noisy contents.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kamran_S/0/1/0/all/0/1"&gt;Sara Kamran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zall_R/0/1/0/all/0/1"&gt;Raziyeh Zall&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kangavari_M/0/1/0/all/0/1"&gt;Mohammad Reza Kangavari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hosseini_S/0/1/0/all/0/1"&gt;Saeid Hosseini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rahmani_S/0/1/0/all/0/1"&gt;Sana Rahmani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hua_W/0/1/0/all/0/1"&gt;Wen Hua&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Query Logs for Privacy Studies: On Deriving Search Queries from Questions. (arXiv:2004.02023v3 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.02023</id>
        <link href="http://arxiv.org/abs/2004.02023"/>
        <updated>2021-06-04T01:12:26.458Z</updated>
        <summary type="html"><![CDATA[Translating verbose information needs into crisp search queries is a
phenomenon that is ubiquitous but hardly understood. Insights into this process
could be valuable in several applications, including synthesizing large
privacy-friendly query logs from public Web sources which are readily available
to the academic research community. In this work, we take a step towards
understanding query formulation by tapping into the rich potential of community
question answering (CQA) forums. Specifically, we sample natural language (NL)
questions spanning diverse themes from the Stack Exchange platform, and conduct
a large-scale conversion experiment where crowdworkers submit search queries
they would use when looking for equivalent information. We provide a careful
analysis of this data, accounting for possible sources of bias during
conversion, along with insights into user-specific linguistic patterns and
search behaviors. We release a dataset of 7,000 question-query pairs from this
study to facilitate further research on query understanding.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Biega_A/0/1/0/all/0/1"&gt;Asia J. Biega&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schmidt_J/0/1/0/all/0/1"&gt;Jana Schmidt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roy_R/0/1/0/all/0/1"&gt;Rishiraj Saha Roy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[JIZHI: A Fast and Cost-Effective Model-As-A-Service System for Web-Scale Online Inference at Baidu. (arXiv:2106.01674v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.01674</id>
        <link href="http://arxiv.org/abs/2106.01674"/>
        <updated>2021-06-04T01:12:26.119Z</updated>
        <summary type="html"><![CDATA[In modern internet industries, deep learning based recommender systems have
became an indispensable building block for a wide spectrum of applications,
such as search engine, news feed, and short video clips. However, it remains
challenging to carry the well-trained deep models for online real-time
inference serving, with respect to the time-varying web-scale traffics from
billions of users, in a cost-effective manner. In this work, we present JIZHI -
a Model-as-a-Service system - that per second handles hundreds of millions of
online inference requests to huge deep models with more than trillions of
sparse parameters, for over twenty real-time recommendation services at Baidu,
Inc. In JIZHI, the inference workflow of every recommendation request is
transformed to a Staged Event-Driven Pipeline (SEDP), where each node in the
pipeline refers to a staged computation or I/O intensive task processor. With
traffics of real-time inference requests arrived, each modularized processor
can be run in a fully asynchronized way and managed separately. Besides, JIZHI
introduces heterogeneous and hierarchical storage to further accelerate the
online inference process by reducing unnecessary computations and potential
data access latency induced by ultra-sparse model parameters. Moreover, an
intelligent resource manager has been deployed to maximize the throughput of
JIZHI over the shared infrastructure by searching the optimal resource
allocation plan from historical logs and fine-tuning the load shedding policies
over intermediate system feedback. Extensive experiments have been done to
demonstrate the advantages of JIZHI from the perspectives of end-to-end service
latency, system-wide throughput, and resource consumption. JIZHI has helped
Baidu saved more than ten million US dollars in hardware and utility costs
while handling 200% more traffics without sacrificing inference efficiency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Hao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Q/0/1/0/all/0/1"&gt;Qian Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jiang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liao_X/0/1/0/all/0/1"&gt;Xiaochao Liao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1"&gt;Hao Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1"&gt;Guangxing Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wenlin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1"&gt;Guobao Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zha_Z/0/1/0/all/0/1"&gt;Zhiwei Zha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_D/0/1/0/all/0/1"&gt;Daxiang Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dou_D/0/1/0/all/0/1"&gt;Dejing Dou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1"&gt;Haoyi Xiong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What and How long: Prediction of Mobile App Engagement. (arXiv:2106.01490v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.01490</id>
        <link href="http://arxiv.org/abs/2106.01490"/>
        <updated>2021-06-04T01:12:26.099Z</updated>
        <summary type="html"><![CDATA[User engagement is crucial to the long-term success of a mobile app. Several
metrics, such as dwell time, have been used for measuring user engagement.
However, how to effectively predict user engagement in the context of mobile
apps is still an open research question. For example, do the mobile usage
contexts (e.g.,~time of day) in which users access mobile apps impact their
dwell time? Answers to such questions could help mobile operating system and
publishers to optimize advertising and service placement. In this paper, we
first conduct an empirical study for assessing how user characteristics,
temporal features, and the short/long-term contexts contribute to gains in
predicting users' app dwell time on the population level. The comprehensive
analysis is conducted on large app usage logs collected through a mobile
advertising company. The dataset covers more than 12K anonymous users and 1.3
million log events. Based on the analysis, we further investigate a novel
mobile app engagement prediction problem -- can we predict simultaneously what
app the user will use next and how long he/she will stay on that app? We
propose several strategies for this joint prediction problem and demonstrate
that our model can improve the performance significantly when compared with the
state-of-the-art baselines. Our work can help mobile system developers in
designing a better and more engagement-aware mobile app user experience.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1"&gt;Yuan Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1"&gt;Ke Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pelleg_D/0/1/0/all/0/1"&gt;Dan Pelleg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Separated-Spectral-Distribution Estimation Based on Bayesian Inference with Single RGB Camera. (arXiv:2106.01861v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.01861</id>
        <link href="http://arxiv.org/abs/2106.01861"/>
        <updated>2021-06-04T01:12:26.067Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a novel method for separately estimating spectral
distributions from images captured by a typical RGB camera. The proposed method
allows us to separately estimate a spectral distribution of illumination,
reflectance, or camera sensitivity, while recent hyperspectral cameras are
limited to capturing a joint spectral distribution from a scene. In addition,
the use of Bayesian inference makes it possible to take into account prior
information of both spectral distributions and image noise as probability
distributions. As a result, the proposed method can estimate spectral
distributions in a unified way, and it can enhance the robustness of the
estimation against noise, which conventional spectral-distribution estimation
methods cannot. The use of Bayesian inference also enables us to obtain the
confidence of estimation results. In an experiment, the proposed method is
shown not only to outperform conventional estimation methods in terms of RMSE
but also to be robust against noise.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Kinoshita_Y/0/1/0/all/0/1"&gt;Yuma Kinoshita&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kiya_H/0/1/0/all/0/1"&gt;Hitoshi Kiya&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Anticipative Video Transformer. (arXiv:2106.02036v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.02036</id>
        <link href="http://arxiv.org/abs/2106.02036"/>
        <updated>2021-06-04T01:12:26.034Z</updated>
        <summary type="html"><![CDATA[We propose Anticipative Video Transformer (AVT), an end-to-end
attention-based video modeling architecture that attends to the previously
observed video in order to anticipate future actions. We train the model
jointly to predict the next action in a video sequence, while also learning
frame feature encoders that are predictive of successive future frames'
features. Compared to existing temporal aggregation strategies, AVT has the
advantage of both maintaining the sequential progression of observed actions
while still capturing long-range dependencies--both critical for the
anticipation task. Through extensive experiments, we show that AVT obtains the
best reported performance on four popular action anticipation benchmarks:
EpicKitchens-55, EpicKitchens-100, EGTEA Gaze+, and 50-Salads, including
outperforming all submissions to the EpicKitchens-100 CVPR'21 challenge.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Girdhar_R/0/1/0/all/0/1"&gt;Rohit Girdhar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grauman_K/0/1/0/all/0/1"&gt;Kristen Grauman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Warming-up recurrent neural networks to maximize reachable multi-stability greatly improves learning. (arXiv:2106.01001v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01001</id>
        <link href="http://arxiv.org/abs/2106.01001"/>
        <updated>2021-06-03T02:10:37.731Z</updated>
        <summary type="html"><![CDATA[Training recurrent neural networks is known to be difficult when time
dependencies become long. Consequently, training standard gated cells such as
gated recurrent units and long-short term memory on benchmarks where long-term
memory is required remains an arduous task. In this work, we propose a general
way to initialize any recurrent network connectivity through a process called
"warm-up" to improve its capability to learn arbitrarily long time
dependencies. This initialization process is designed to maximize network
reachable multi-stability, i.e. the number of attractors within the network
that can be reached through relevant input trajectories. Warming-up is
performed before training, using stochastic gradient descent on a specifically
designed loss. We show that warming-up greatly improves recurrent neural
network performance on long-term memory benchmarks for multiple recurrent cell
types, but can sometimes impede precision. We therefore introduce a parallel
recurrent network structure with partial warm-up that is shown to greatly
improve learning on long time-series while maintaining high levels of
precision. This approach provides a general framework for improving learning
abilities of any recurrent cell type when long-term memory is required.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vecoven_N/0/1/0/all/0/1"&gt;Nicolas Vecoven&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ernst_D/0/1/0/all/0/1"&gt;Damien Ernst&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Drion_G/0/1/0/all/0/1"&gt;Guillaume Drion&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep learning-based multi-output quantile forecasting of PV generation. (arXiv:2106.01271v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01271</id>
        <link href="http://arxiv.org/abs/2106.01271"/>
        <updated>2021-06-03T02:10:37.726Z</updated>
        <summary type="html"><![CDATA[This paper develops probabilistic PV forecasters by taking advantage of
recent breakthroughs in deep learning. It tailored forecasting tool, named
encoder-decoder, is implemented to compute intraday multi-output PV quantiles
forecasts to efficiently capture the time correlation. The models are trained
using quantile regression, a non-parametric approach that assumes no prior
knowledge of the probabilistic forecasting distribution. The case study is
composed of PV production monitored on-site at the University of Li\`ege
(ULi\`ege), Belgium. The weather forecasts from the regional climate model
provided by the Laboratory of Climatology are used as inputs of the deep
learning models. The forecast quality is quantitatively assessed by the
continuous ranked probability and interval scores. The results indicate this
architecture improves the forecast quality and is computationally efficient to
be incorporated in an intraday decision-making tool for robust optimization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dumas_J/0/1/0/all/0/1"&gt;Jonathan Dumas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cointe_C/0/1/0/all/0/1"&gt;Colin Cointe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fettweis_X/0/1/0/all/0/1"&gt;Xavier Fettweis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cornelusse_B/0/1/0/all/0/1"&gt;Bertrand Corn&amp;#xe9;lusse&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the experimental feasibility of quantum state reconstruction via machine learning. (arXiv:2012.09432v2 [quant-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.09432</id>
        <link href="http://arxiv.org/abs/2012.09432"/>
        <updated>2021-06-03T02:10:37.722Z</updated>
        <summary type="html"><![CDATA[We determine the resource scaling of machine learning-based quantum state
reconstruction methods, in terms of inference and training, for systems of up
to four qubits when constrained to pure states. Further, we examine system
performance in the low-count regime, likely to be encountered in the tomography
of high-dimensional systems. Finally, we implement our quantum state
reconstruction method on an IBM Q quantum computer, and compare against both
unconstrained and constrained MLE state reconstruction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Lohani_S/0/1/0/all/0/1"&gt;Sanjaya Lohani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Searles_T/0/1/0/all/0/1"&gt;Thomas A. Searles&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Kirby_B/0/1/0/all/0/1"&gt;Brian T. Kirby&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Glasser_R/0/1/0/all/0/1"&gt;Ryan T. Glasser&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Online Continual Learning in Image Classification: An Empirical Survey. (arXiv:2101.10423v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.10423</id>
        <link href="http://arxiv.org/abs/2101.10423"/>
        <updated>2021-06-03T02:10:37.707Z</updated>
        <summary type="html"><![CDATA[Online continual learning for image classification studies the problem of
learning to classify images from an online stream of data and tasks, where
tasks may include new classes (class incremental) or data nonstationarity
(domain incremental). One of the key challenges of continual learning is to
avoid catastrophic forgetting (CF), i.e., forgetting old tasks in the presence
of more recent tasks. Over the past few years, many methods and tricks have
been introduced to address this problem, but many have not been fairly and
systematically compared under a variety of realistic and practical settings. To
better understand the relative advantages of various approaches and the
settings where they work best, this survey aims to (1) compare state-of-the-art
methods such as MIR, iCARL, and GDumb and determine which works best at
different experimental settings; (2) determine if the best class incremental
methods are also competitive in domain incremental setting; (3) evaluate the
performance of 7 simple but effective trick such as "review" trick and nearest
class mean (NCM) classifier to assess their relative impact. Regarding (1), we
observe iCaRL remains competitive when the memory buffer is small; GDumb
outperforms many recently proposed methods in medium-size datasets and MIR
performs the best in larger-scale datasets. For (2), we note that GDumb
performs quite poorly while MIR -- already competitive for (1) -- is also
strongly competitive in this very different but important setting. Overall,
this allows us to conclude that MIR is overall a strong and versatile method
across a wide variety of settings. For (3), we find that all 7 tricks are
beneficial, and when augmented with the "review" trick and NCM classifier, MIR
produces performance levels that bring online continual learning much closer to
its ultimate goal of matching offline training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mai_Z/0/1/0/all/0/1"&gt;Zheda Mai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1"&gt;Ruiwen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jeong_J/0/1/0/all/0/1"&gt;Jihwan Jeong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Quispe_D/0/1/0/all/0/1"&gt;David Quispe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1"&gt;Hyunwoo Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sanner_S/0/1/0/all/0/1"&gt;Scott Sanner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Determining Chess Game State From an Image. (arXiv:2104.14963v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.14963</id>
        <link href="http://arxiv.org/abs/2104.14963"/>
        <updated>2021-06-03T02:10:37.703Z</updated>
        <summary type="html"><![CDATA[Identifying the configuration of chess pieces from an image of a chessboard
is a problem in computer vision that has not yet been solved accurately.
However, it is important for helping amateur chess players improve their games
by facilitating automatic computer analysis without the overhead of manually
entering the pieces. Current approaches are limited by the lack of large
datasets and are not designed to adapt to unseen chess sets. This paper puts
forth a new dataset synthesised from a 3D model that is an order of magnitude
larger than existing ones. Trained on this dataset, a novel end-to-end chess
recognition system is presented that combines traditional computer vision
techniques with deep learning. It localises the chessboard using a RANSAC-based
algorithm that computes a projective transformation of the board onto a regular
grid. Using two convolutional neural networks, it then predicts an occupancy
mask for the squares in the warped image and finally classifies the pieces. The
described system achieves an error rate of 0.23% per square on the test set, 28
times better than the current state of the art. Further, a few-shot transfer
learning approach is developed that is able to adapt the inference system to a
previously unseen chess set using just two photos of the starting position,
obtaining a per-square accuracy of 99.83% on images of that new chess set. The
code, dataset, and trained models are made available online.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wolflein_G/0/1/0/all/0/1"&gt;Georg W&amp;#xf6;lflein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arandjelovic_O/0/1/0/all/0/1"&gt;Ognjen Arandjelovi&amp;#x107;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Benchmarking the Performance of Bayesian Optimization across Multiple Experimental Materials Science Domains. (arXiv:2106.01309v1 [cond-mat.mtrl-sci])]]></title>
        <id>http://arxiv.org/abs/2106.01309</id>
        <link href="http://arxiv.org/abs/2106.01309"/>
        <updated>2021-06-03T02:10:37.698Z</updated>
        <summary type="html"><![CDATA[In the field of machine learning (ML) for materials optimization, active
learning algorithms, such as Bayesian Optimization (BO), have been leveraged
for guiding autonomous and high-throughput experimentation systems. However,
very few studies have evaluated the efficiency of BO as a general optimization
algorithm across a broad range of experimental materials science domains. In
this work, we evaluate the performance of BO algorithms with a collection of
surrogate model and acquisition function pairs across five diverse experimental
materials systems, namely carbon nanotube polymer blends, silver nanoparticles,
lead-halide perovskites, as well as additively manufactured polymer structures
and shapes. By defining acceleration and enhancement metrics for general
materials optimization objectives, we find that for surrogate model selection,
Gaussian Process (GP) with anisotropic kernels (automatic relevance detection,
ARD) and Random Forests (RF) have comparable performance and both outperform
the commonly used GP without ARD. We discuss the implicit distributional
assumptions of RF and GP, and the benefits of using GP with anisotropic kernels
in detail. We provide practical insights for experimentalists on surrogate
model selection of BO during materials optimization campaigns.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cond-mat/1/au:+Liang_Q/0/1/0/all/0/1"&gt;Qiaohao Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Gongora_A/0/1/0/all/0/1"&gt;Aldair E. Gongora&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Ren_Z/0/1/0/all/0/1"&gt;Zekun Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Tiihonen_A/0/1/0/all/0/1"&gt;Armi Tiihonen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhe Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Sun_S/0/1/0/all/0/1"&gt;Shijing Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Deneault_J/0/1/0/all/0/1"&gt;James R. Deneault&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Bash_D/0/1/0/all/0/1"&gt;Daniil Bash&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Mekki_Berrada_F/0/1/0/all/0/1"&gt;Flore Mekki-Berrada&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Khan_S/0/1/0/all/0/1"&gt;Saif A. Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Hippalgaonkar_K/0/1/0/all/0/1"&gt;Kedar Hippalgaonkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Maruyama_B/0/1/0/all/0/1"&gt;Benji Maruyama&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Brown_K/0/1/0/all/0/1"&gt;Keith A. Brown&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Fisher_J/0/1/0/all/0/1"&gt;John Fisher III&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Buonassisi_T/0/1/0/all/0/1"&gt;Tonio Buonassisi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data augmentation and pre-trained networks for extremely low data regimes unsupervised visual inspection. (arXiv:2106.01277v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.01277</id>
        <link href="http://arxiv.org/abs/2106.01277"/>
        <updated>2021-06-03T02:10:37.692Z</updated>
        <summary type="html"><![CDATA[The use of deep features coming from pre-trained neural networks for
unsupervised anomaly detection purposes has recently gathered momentum in the
computer vision field. In particular, industrial inspection applications can
take advantage of such features, as demonstrated by the multiple successes of
related methods on the MVTec Anomaly Detection (MVTec AD) dataset. These
methods make use of neural networks pre-trained on auxiliary classification
tasks such as ImageNet. However, to our knowledge, no comparative study of
robustness to the low data regimes between these approaches has been conducted
yet. For quality inspection applications, the handling of limited sample sizes
may be crucial as large quantities of images are not available for small
series. In this work, we aim to compare three approaches based on deep
pre-trained features when varying the quantity of available data in MVTec AD:
KNN, Mahalanobis, and PaDiM. We show that although these methods are mostly
robust to small sample sizes, they still can benefit greatly from using data
augmentation in the original image space, which allows to deal with very small
production runs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gutierrez_P/0/1/0/all/0/1"&gt;Pierre Gutierrez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cordier_A/0/1/0/all/0/1"&gt;Antoine Cordier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Caldeira_T/0/1/0/all/0/1"&gt;Tha&amp;#xef;s Caldeira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sautory_T/0/1/0/all/0/1"&gt;Th&amp;#xe9;ophile Sautory&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Meta-strategy for Learning Tuning Parameters with Guarantees. (arXiv:2102.02504v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.02504</id>
        <link href="http://arxiv.org/abs/2102.02504"/>
        <updated>2021-06-03T02:10:37.677Z</updated>
        <summary type="html"><![CDATA[Online gradient methods, like the online gradient algorithm (OGA), often
depend on tuning parameters that are difficult to set in practice. We consider
an online meta-learning scenario, and we propose a meta-strategy to learn these
parameters from past tasks. Our strategy is based on the minimization of a
regret bound. It allows to learn the initialization and the step size in OGA
with guarantees. We provide a regret analysis of the strategy in the case of
convex losses. It suggests that, when there are parameters
$\theta_1,\dots,\theta_T$ solving well tasks $1,\dots,T$ respectively and that
are close enough one to each other, our strategy indeed improves on learning
each task in isolation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Meunier_D/0/1/0/all/0/1"&gt;Dimitri Meunier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Alquier_P/0/1/0/all/0/1"&gt;Pierre Alquier&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Privacy-Preserving and Trustable Multi-agent Learning Framework. (arXiv:2106.01242v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01242</id>
        <link href="http://arxiv.org/abs/2106.01242"/>
        <updated>2021-06-03T02:10:37.672Z</updated>
        <summary type="html"><![CDATA[Distributed multi-agent learning enables agents to cooperatively train a
model without requiring to share their datasets. While this setting ensures
some level of privacy, it has been shown that, even when data is not directly
shared, the training process is vulnerable to privacy attacks including data
reconstruction and model inversion attacks. Additionally, malicious agents that
train on inverted labels or random data, may arbitrarily weaken the accuracy of
the global model. This paper addresses these challenges and presents
Privacy-preserving and trustable Distributed Learning (PT-DL), a fully
decentralized framework that relies on Differential Privacy to guarantee strong
privacy protections of the agents' data, and Ethereum smart contracts to ensure
trustability. The paper shows that PT-DL is resilient up to a 50% collusion
attack, with high probability, in a malicious trust model and the experimental
evaluation illustrates the benefits of the proposed model as a
privacy-preserving and trustable distributed multi-agent learning system on
several classification tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nagar_A/0/1/0/all/0/1"&gt;Anudit Nagar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tran_C/0/1/0/all/0/1"&gt;Cuong Tran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fioretto_F/0/1/0/all/0/1"&gt;Ferdinando Fioretto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How Do Neural Networks Estimate Optical Flow? A Neuropsychology-Inspired Study. (arXiv:2004.09317v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.09317</id>
        <link href="http://arxiv.org/abs/2004.09317"/>
        <updated>2021-06-03T02:10:37.667Z</updated>
        <summary type="html"><![CDATA[End-to-end trained convolutional neural networks have led to a breakthrough
in optical flow estimation. The most recent advances focus on improving the
optical flow estimation by improving the architecture and setting a new
benchmark on the publicly available MPI-Sintel dataset. Instead, in this
article, we investigate how deep neural networks estimate optical flow. A
better understanding of how these networks function is important for (i)
assessing their generalization capabilities to unseen inputs, and (ii)
suggesting changes to improve their performance. For our investigation, we
focus on FlowNetS, as it is the prototype of an encoder-decoder neural network
for optical flow estimation. Furthermore, we use a filter identification method
that has played a major role in uncovering the motion filters present in animal
brains in neuropsychological research. The method shows that the filters in the
deepest layer of FlowNetS are sensitive to a variety of motion patterns. Not
only do we find translation filters, as demonstrated in animal brains, but
thanks to the easier measurements in artificial neural networks, we even unveil
dilation, rotation, and occlusion filters. Furthermore, we find similarities in
the refinement part of the network and the perceptual filling-in process which
occurs in the mammal primary visual cortex.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jong_D/0/1/0/all/0/1"&gt;D. B. de Jong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paredes_Valles_F/0/1/0/all/0/1"&gt;F. Paredes-Vall&amp;#xe9;s&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Croon_G/0/1/0/all/0/1"&gt;G. C. H. E. de Croon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Information Theoretic Measures for Fairness-aware Feature Selection. (arXiv:2106.00772v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.00772</id>
        <link href="http://arxiv.org/abs/2106.00772"/>
        <updated>2021-06-03T02:10:37.661Z</updated>
        <summary type="html"><![CDATA[Machine earning algorithms are increasingly used for consequential decision
making regarding individuals based on their relevant features. Features that
are relevant for accurate decisions may however lead to either explicit or
implicit forms of discrimination against unprivileged groups, such as those of
certain race or gender. This happens due to existing biases in the training
data, which are often replicated or even exacerbated by the learning algorithm.
Identifying and measuring these biases at the data level is a challenging
problem due to the interdependence among the features, and the decision
outcome. In this work, we develop a framework for fairness-aware feature
selection, based on information theoretic measures for the accuracy and
discriminatory impacts of features. Specifically, our goal is to design a
fairness utility score for each feature which quantifies how this feature
influences accurate as well as nondiscriminatory decisions. We first propose
information theoretic measures for the impact of different subsets of features
on the accuracy and discrimination of the model. Subsequently, we deduce the
marginal impact of each feature using Shapley value function. Our framework
depends on the joint statistics of the data rather than a particular classifier
design. We examine our proposed framework on real and synthetic data to
evaluate its performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khodadadian_S/0/1/0/all/0/1"&gt;Sajad Khodadadian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nafea_M/0/1/0/all/0/1"&gt;Mohamed Nafea&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghassami_A/0/1/0/all/0/1"&gt;AmirEmad Ghassami&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kiyavash_N/0/1/0/all/0/1"&gt;Negar Kiyavash&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Measuring and Increasing Context Usage in Context-Aware Machine Translation. (arXiv:2105.03482v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.03482</id>
        <link href="http://arxiv.org/abs/2105.03482"/>
        <updated>2021-06-03T02:10:37.639Z</updated>
        <summary type="html"><![CDATA[Recent work in neural machine translation has demonstrated both the necessity
and feasibility of using inter-sentential context -- context from sentences
other than those currently being translated. However, while many current
methods present model architectures that theoretically can use this extra
context, it is often not clear how much they do actually utilize it at
translation time. In this paper, we introduce a new metric, conditional
cross-mutual information, to quantify the usage of context by these models.
Using this metric, we measure how much document-level machine translation
systems use particular varieties of context. We find that target context is
referenced more than source context, and that conditioning on a longer context
has a diminishing effect on results. We then introduce a new, simple training
method, context-aware word dropout, to increase the usage of context by
context-aware models. Experiments show that our method increases context usage
and that this reflects on the translation quality according to metrics such as
BLEU and COMET, as well as performance on anaphoric pronoun resolution and
lexical cohesion contrastive datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fernandes_P/0/1/0/all/0/1"&gt;Patrick Fernandes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_K/0/1/0/all/0/1"&gt;Kayo Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1"&gt;Graham Neubig&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martins_A/0/1/0/all/0/1"&gt;Andr&amp;#xe9; F. T. Martins&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Extendible, Graph-Neural-Network-Based Approach for Accurate Force Field Development of Large Flexible Organic Molecules. (arXiv:2106.00927v1 [physics.chem-ph])]]></title>
        <id>http://arxiv.org/abs/2106.00927</id>
        <link href="http://arxiv.org/abs/2106.00927"/>
        <updated>2021-06-03T02:10:37.632Z</updated>
        <summary type="html"><![CDATA[An accurate force field is the key to the success of all molecular mechanics
simulations on organic polymers and biomolecules. Accuracy beyond density
functional theory is often needed to describe the intermolecular interactions,
while most correlated wavefunction (CW) methods are prohibitively expensive for
large molecules. Therefore, it posts a great challenge to develop an extendible
ab initio force field for large flexible organic molecules at CW level of
accuracy. In this work, we face this challenge by combining the physics-driven
nonbonding potential with a data-driven subgraph neural network bonding model
(named sGNN). Tests on polyethylene glycol polymer chains show that our
strategy is highly accurate and robust for molecules of different sizes.
Therefore, we can develop the force field from small molecular fragments (with
sizes easily accessible to CW methods) and safely transfer it to large
polymers, thus opening a new path to the next-generation organic force fields.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xufei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yuanda Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Zheng_H/0/1/0/all/0/1"&gt;Han Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Yu_K/0/1/0/all/0/1"&gt;Kuang Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adapting High-resource NMT Models to Translate Low-resource Related Languages without Parallel Data. (arXiv:2105.15071v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.15071</id>
        <link href="http://arxiv.org/abs/2105.15071"/>
        <updated>2021-06-03T02:10:37.585Z</updated>
        <summary type="html"><![CDATA[The scarcity of parallel data is a major obstacle for training high-quality
machine translation systems for low-resource languages. Fortunately, some
low-resource languages are linguistically related or similar to high-resource
languages; these related languages may share many lexical or syntactic
structures. In this work, we exploit this linguistic overlap to facilitate
translating to and from a low-resource language with only monolingual data, in
addition to any parallel data in the related high-resource language. Our
method, NMT-Adapt, combines denoising autoencoding, back-translation and
adversarial objectives to utilize monolingual data for low-resource adaptation.
We experiment on 7 languages from three different language families and show
that our technique significantly improves translation into low-resource
language compared to other translation baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ko_W/0/1/0/all/0/1"&gt;Wei-Jen Ko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+El_Kishky_A/0/1/0/all/0/1"&gt;Ahmed El-Kishky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Renduchintala_A/0/1/0/all/0/1"&gt;Adithya Renduchintala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chaudhary_V/0/1/0/all/0/1"&gt;Vishrav Chaudhary&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goyal_N/0/1/0/all/0/1"&gt;Naman Goyal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guzman_F/0/1/0/all/0/1"&gt;Francisco Guzm&amp;#xe1;n&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fung_P/0/1/0/all/0/1"&gt;Pascale Fung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koehn_P/0/1/0/all/0/1"&gt;Philipp Koehn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Diab_M/0/1/0/all/0/1"&gt;Mona Diab&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Dense Representations of Phrases at Scale. (arXiv:2012.12624v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.12624</id>
        <link href="http://arxiv.org/abs/2012.12624"/>
        <updated>2021-06-03T02:10:37.570Z</updated>
        <summary type="html"><![CDATA[Open-domain question answering can be reformulated as a phrase retrieval
problem, without the need for processing documents on-demand during inference
(Seo et al., 2019). However, current phrase retrieval models heavily depend on
sparse representations and still underperform retriever-reader approaches. In
this work, we show for the first time that we can learn dense representations
of phrases alone that achieve much stronger performance in open-domain QA. We
present an effective method to learn phrase representations from the
supervision of reading comprehension tasks, coupled with novel negative
sampling methods. We also propose a query-side fine-tuning strategy, which can
support transfer learning and reduce the discrepancy between training and
inference. On five popular open-domain QA datasets, our model DensePhrases
improves over previous phrase retrieval models by 15%-25% absolute accuracy and
matches the performance of state-of-the-art retriever-reader models. Our model
is easy to parallelize due to pure dense representations and processes more
than 10 questions per second on CPUs. Finally, we directly use our pre-indexed
dense phrase representations for two slot filling tasks, showing the promise of
utilizing DensePhrases as a dense knowledge base for downstream tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jinhyuk Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sung_M/0/1/0/all/0/1"&gt;Mujeen Sung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kang_J/0/1/0/all/0/1"&gt;Jaewoo Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1"&gt;Danqi Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[KO-PDE: Kernel Optimized Discovery of Partial Differential Equations with Varying Coefficients. (arXiv:2106.01078v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01078</id>
        <link href="http://arxiv.org/abs/2106.01078"/>
        <updated>2021-06-03T02:10:37.444Z</updated>
        <summary type="html"><![CDATA[Partial differential equations (PDEs) fitting scientific data can represent
physical laws with explainable mechanisms for various mathematically-oriented
subjects. Most natural dynamics are expressed by PDEs with varying coefficients
(PDEs-VC), which highlights the importance of PDE discovery. Previous
algorithms can discover some simple instances of PDEs-VC but fail in the
discovery of PDEs with coefficients of higher complexity, as a result of
coefficient estimation inaccuracy. In this paper, we propose KO-PDE, a kernel
optimized regression method that incorporates the kernel density estimation of
adjacent coefficients to reduce the coefficient estimation error. KO-PDE can
discover PDEs-VC on which previous baselines fail and is more robust against
inevitable noise in data. In experiments, the PDEs-VC of seven challenging
spatiotemporal scientific datasets in fluid dynamics are all discovered by
KO-PDE, while the three baselines render false results in most cases. With
state-of-the-art performance, KO-PDE sheds light on the automatic description
of natural phenomenons using discovered PDEs in the real world.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1"&gt;Yingtao Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1"&gt;Qiang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yuntian Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1"&gt;Wenbo Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jun Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Intrinsic Bias Metrics Do Not Correlate with Application Bias. (arXiv:2012.15859v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.15859</id>
        <link href="http://arxiv.org/abs/2012.15859"/>
        <updated>2021-06-03T02:10:37.431Z</updated>
        <summary type="html"><![CDATA[Natural Language Processing (NLP) systems learn harmful societal biases that
cause them to amplify inequality as they are deployed in more and more
situations. To guide efforts at debiasing these systems, the NLP community
relies on a variety of metrics that quantify bias in models. Some of these
metrics are intrinsic, measuring bias in word embedding spaces, and some are
extrinsic, measuring bias in downstream tasks that the word embeddings enable.
Do these intrinsic and extrinsic metrics correlate with each other? We compare
intrinsic and extrinsic metrics across hundreds of trained models covering
different tasks and experimental conditions. Our results show no reliable
correlation between these metrics that holds in all scenarios across tasks and
languages. We urge researchers working on debiasing to focus on extrinsic
measures of bias, and to make using these measures more feasible via creation
of new challenge sets and annotated test data. To aid this effort, we release
code, a new intrinsic metric, and an annotated test set focused on gender bias
in hate speech.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Goldfarb_Tarrant_S/0/1/0/all/0/1"&gt;Seraphina Goldfarb-Tarrant&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marchant_R/0/1/0/all/0/1"&gt;Rebecca Marchant&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sanchez_R/0/1/0/all/0/1"&gt;Ricardo Mu&amp;#xf1;oz Sanchez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pandya_M/0/1/0/all/0/1"&gt;Mugdha Pandya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lopez_A/0/1/0/all/0/1"&gt;Adam Lopez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[End-to-End Training of Neural Retrievers for Open-Domain Question Answering. (arXiv:2101.00408v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.00408</id>
        <link href="http://arxiv.org/abs/2101.00408"/>
        <updated>2021-06-03T02:10:37.426Z</updated>
        <summary type="html"><![CDATA[Recent work on training neural retrievers for open-domain question answering
(OpenQA) has employed both supervised and unsupervised approaches. However, it
remains unclear how unsupervised and supervised methods can be used most
effectively for neural retrievers. In this work, we systematically study
retriever pre-training. We first propose an approach of unsupervised
pre-training with the Inverse Cloze Task and masked salient spans, followed by
supervised finetuning using question-context pairs. This approach leads to
absolute gains of 2+ points over the previous best result in the top-20
retrieval accuracy on Natural Questions and TriviaQA datasets.

We also explore two approaches for end-to-end supervised training of the
reader and retriever components in OpenQA models. In the first approach, the
reader considers each retrieved document separately while in the second
approach, the reader considers all the retrieved documents together. Our
experiments demonstrate the effectiveness of these approaches as we obtain new
state-of-the-art results. On the Natural Questions dataset, we obtain a top-20
retrieval accuracy of 84, an improvement of 5 points over the recent DPR model.
In addition, we achieve good results on answer extraction, outperforming recent
models like REALM and RAG by 3+ points. We further scale up end-to-end training
to large models and show consistent gains in performance over smaller models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sachan_D/0/1/0/all/0/1"&gt;Devendra Singh Sachan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Patwary_M/0/1/0/all/0/1"&gt;Mostofa Patwary&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shoeybi_M/0/1/0/all/0/1"&gt;Mohammad Shoeybi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kant_N/0/1/0/all/0/1"&gt;Neel Kant&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ping_W/0/1/0/all/0/1"&gt;Wei Ping&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hamilton_W/0/1/0/all/0/1"&gt;William L Hamilton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Catanzaro_B/0/1/0/all/0/1"&gt;Bryan Catanzaro&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pay Attention to MLPs. (arXiv:2105.08050v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.08050</id>
        <link href="http://arxiv.org/abs/2105.08050"/>
        <updated>2021-06-03T02:10:37.421Z</updated>
        <summary type="html"><![CDATA[Transformers have become one of the most important architectural innovations
in deep learning and have enabled many breakthroughs over the past few years.
Here we propose a simple network architecture, gMLP, based on MLPs with gating,
and show that it can perform as well as Transformers in key language and vision
applications. Our comparisons show that self-attention is not critical for
Vision Transformers, as gMLP can achieve the same accuracy. For BERT, our model
achieves parity with Transformers on pretraining perplexity and is better on
some downstream NLP tasks. On finetuning tasks where gMLP performs worse,
making the gMLP model substantially larger can close the gap with Transformers.
In general, our experiments show that gMLP can scale as well as Transformers
over increased data and compute.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Hanxiao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_Z/0/1/0/all/0/1"&gt;Zihang Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+So_D/0/1/0/all/0/1"&gt;David R. So&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1"&gt;Quoc V. Le&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HyKnow: End-to-End Task-Oriented Dialog Modeling with Hybrid Knowledge Management. (arXiv:2105.06041v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.06041</id>
        <link href="http://arxiv.org/abs/2105.06041"/>
        <updated>2021-06-03T02:10:37.416Z</updated>
        <summary type="html"><![CDATA[Task-oriented dialog (TOD) systems typically manage structured knowledge
(e.g. ontologies and databases) to guide the goal-oriented conversations.
However, they fall short of handling dialog turns grounded on unstructured
knowledge (e.g. reviews and documents). In this paper, we formulate a task of
modeling TOD grounded on both structured and unstructured knowledge. To address
this task, we propose a TOD system with hybrid knowledge management, HyKnow. It
extends the belief state to manage both structured and unstructured knowledge,
and is the first end-to-end model that jointly optimizes dialog modeling
grounded on these two kinds of knowledge. We conduct experiments on the
modified version of MultiWOZ 2.1 dataset, where dialogs are grounded on hybrid
knowledge. Experimental results show that HyKnow has strong end-to-end
performance compared to existing TOD systems. It also outperforms the pipeline
knowledge management schemes, with higher unstructured knowledge retrieval
accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1"&gt;Silin Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Takanobu_R/0/1/0/all/0/1"&gt;Ryuichi Takanobu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_W/0/1/0/all/0/1"&gt;Wei Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1"&gt;Qun Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1"&gt;Minlie Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Personalized Glucose Level Forecasting Using Attention-based Recurrent Neural Networks. (arXiv:2106.00884v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.00884</id>
        <link href="http://arxiv.org/abs/2106.00884"/>
        <updated>2021-06-03T02:10:37.410Z</updated>
        <summary type="html"><![CDATA[In this paper, we study the problem of blood glucose forecasting and provide
a deep personalized solution. Predicting blood glucose level in people with
diabetes has significant value because health complications of abnormal glucose
level are serious, sometimes even leading to death. Therefore, having a model
that can accurately and quickly warn patients of potential problems is
essential. To develop a better deep model for blood glucose forecasting, we
analyze the data and detect important patterns. These observations helped us to
propose a method that has several key advantages over existing methods: 1- it
learns a personalized model for each patient as well as a global model; 2- it
uses an attention mechanism and extracted time features to better learn
long-term dependencies in the data; 3- it introduces a new, robust training
procedure for time series data. We empirically show the efficacy of our model
on a real dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Armandpour_M/0/1/0/all/0/1"&gt;Mohammadreza Armandpour&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kidd_B/0/1/0/all/0/1"&gt;Brian Kidd&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1"&gt;Yu Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1"&gt;Jianhua Z. Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Uncovering Constraint-Based Behavior in Neural Models via Targeted Fine-Tuning. (arXiv:2106.01207v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01207</id>
        <link href="http://arxiv.org/abs/2106.01207"/>
        <updated>2021-06-03T02:10:37.405Z</updated>
        <summary type="html"><![CDATA[A growing body of literature has focused on detailing the linguistic
knowledge embedded in large, pretrained language models. Existing work has
shown that non-linguistic biases in models can drive model behavior away from
linguistic generalizations. We hypothesized that competing linguistic processes
within a language, rather than just non-linguistic model biases, could obscure
underlying linguistic knowledge. We tested this claim by exploring a single
phenomenon in four languages: English, Chinese, Spanish, and Italian. While
human behavior has been found to be similar across languages, we find
cross-linguistic variation in model behavior. We show that competing processes
in a language act as constraints on model behavior and demonstrate that
targeted fine-tuning can re-weight the learned constraints, uncovering
otherwise dormant linguistic knowledge in models. Our results suggest that
models need to learn both the linguistic constraints in a language and their
relative ranking, with mismatches in either producing non-human-like behavior.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Davis_F/0/1/0/all/0/1"&gt;Forrest Davis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schijndel_M/0/1/0/all/0/1"&gt;Marten van Schijndel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How Good is Your Tokenizer? On the Monolingual Performance of Multilingual Language Models. (arXiv:2012.15613v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.15613</id>
        <link href="http://arxiv.org/abs/2012.15613"/>
        <updated>2021-06-03T02:10:37.400Z</updated>
        <summary type="html"><![CDATA[In this work, we provide a systematic and comprehensive empirical comparison
of pretrained multilingual language models versus their monolingual
counterparts with regard to their monolingual task performance. We study a set
of nine typologically diverse languages with readily available pretrained
monolingual models on a set of five diverse monolingual downstream tasks. We
first aim to establish, via fair and controlled comparisons, if a gap between
the multilingual and the corresponding monolingual representation of that
language exists, and subsequently investigate the reason for any performance
difference. To disentangle conflating factors, we train new monolingual models
on the same data, with monolingually and multilingually trained tokenizers. We
find that while the pretraining data size is an important factor, a designated
monolingual tokenizer plays an equally important role in the downstream
performance. Our results show that languages that are adequately represented in
the multilingual model's vocabulary exhibit negligible performance decreases
over their monolingual counterparts. We further find that replacing the
original multilingual tokenizer with the specialized monolingual tokenizer
improves the downstream performance of the multilingual model for almost every
task and language.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rust_P/0/1/0/all/0/1"&gt;Phillip Rust&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pfeiffer_J/0/1/0/all/0/1"&gt;Jonas Pfeiffer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vulic_I/0/1/0/all/0/1"&gt;Ivan Vuli&amp;#x107;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ruder_S/0/1/0/all/0/1"&gt;Sebastian Ruder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1"&gt;Iryna Gurevych&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GAMI-Net: An Explainable Neural Network based on Generalized Additive Models with Structured Interactions. (arXiv:2003.07132v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.07132</id>
        <link href="http://arxiv.org/abs/2003.07132"/>
        <updated>2021-06-03T02:10:37.395Z</updated>
        <summary type="html"><![CDATA[The lack of interpretability is an inevitable problem when using neural
network models in real applications. In this paper, an explainable neural
network based on generalized additive models with structured interactions
(GAMI-Net) is proposed to pursue a good balance between prediction accuracy and
model interpretability. GAMI-Net is a disentangled feedforward network with
multiple additive subnetworks; each subnetwork consists of multiple hidden
layers and is designed for capturing one main effect or one pairwise
interaction. Three interpretability aspects are further considered, including
a) sparsity, to select the most significant effects for parsimonious
representations; b) heredity, a pairwise interaction could only be included
when at least one of its parent main effects exists; and c) marginal clarity,
to make main effects and pairwise interactions mutually distinguishable. An
adaptive training algorithm is developed, where main effects are first trained
and then pairwise interactions are fitted to the residuals. Numerical
experiments on both synthetic functions and real-world datasets show that the
proposed model enjoys superior interpretability and it maintains competitive
prediction accuracy in comparison to the explainable boosting machine and other
classic machine learning models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Zebin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zhang_A/0/1/0/all/0/1"&gt;Aijun Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Sudjianto_A/0/1/0/all/0/1"&gt;Agus Sudjianto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cross-document Coreference Resolution over Predicted Mentions. (arXiv:2106.01210v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01210</id>
        <link href="http://arxiv.org/abs/2106.01210"/>
        <updated>2021-06-03T02:10:37.374Z</updated>
        <summary type="html"><![CDATA[Coreference resolution has been mostly investigated within a single document
scope, showing impressive progress in recent years based on end-to-end models.
However, the more challenging task of cross-document (CD) coreference
resolution remained relatively under-explored, with the few recent models
applied only to gold mentions. Here, we introduce the first end-to-end model
for CD coreference resolution from raw text, which extends the prominent model
for within-document coreference to the CD setting. Our model achieves
competitive results for event and entity coreference resolution on gold
mentions. More importantly, we set first baseline results, on the standard ECB+
dataset, for CD coreference resolution over predicted mentions. Further, our
model is simpler and more efficient than recent CD coreference resolution
systems, while not using any external resources.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cattan_A/0/1/0/all/0/1"&gt;Arie Cattan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eirew_A/0/1/0/all/0/1"&gt;Alon Eirew&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stanovsky_G/0/1/0/all/0/1"&gt;Gabriel Stanovsky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joshi_M/0/1/0/all/0/1"&gt;Mandar Joshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dagan_I/0/1/0/all/0/1"&gt;Ido Dagan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automated Timeline Length Selection for Flexible Timeline Summarization. (arXiv:2105.14201v1 [cs.AI] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2105.14201</id>
        <link href="http://arxiv.org/abs/2105.14201"/>
        <updated>2021-06-03T02:10:37.354Z</updated>
        <summary type="html"><![CDATA[By producing summaries for long-running events, timeline summarization (TLS)
underpins many information retrieval tasks. Successful TLS requires identifying
an appropriate set of key dates (the timeline length) to cover. However, doing
so is challenging as the right length can change from one topic to another.
Existing TLS solutions either rely on an event-agnostic fixed length or an
expert-supplied setting. Neither of the strategies is desired for real-life TLS
scenarios. A fixed, event-agnostic setting ignores the diversity of events and
their development and hence can lead to low-quality TLS. Relying on
expert-crafted settings is neither scalable nor sustainable for processing many
dynamically changing events. This paper presents a better TLS approach for
automatically and dynamically determining the TLS timeline length. We achieve
this by employing the established elbow method from the machine learning
community to automatically find the minimum number of dates within the time
series to generate concise and informative summaries. We applied our approach
to four TLS datasets of English and Chinese and compared them against three
prior methods. Experimental results show that our approach delivers comparable
or even better summaries over state-of-art TLS methods, but it achieves this
without expert involvement.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mao_Q/0/1/0/all/0/1"&gt;Qianren Mao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1"&gt;Hao Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1"&gt;Hongdong Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jianxin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zheng Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Is good old GRAPPA dead?. (arXiv:2106.00753v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.00753</id>
        <link href="http://arxiv.org/abs/2106.00753"/>
        <updated>2021-06-03T02:10:37.347Z</updated>
        <summary type="html"><![CDATA[We perform a qualitative analysis of performance of XPDNet, a
state-of-the-art deep learning approach for MRI reconstruction, compared to
GRAPPA, a classical approach. We do this in multiple settings, in particular
testing the robustness of the XPDNet to unseen settings, and show that the
XPDNet can to some degree generalize well.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Ramzi_Z/0/1/0/all/0/1"&gt;Zaccharie Ramzi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Vignaud_A/0/1/0/all/0/1"&gt;Alexandre Vignaud&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Starck_J/0/1/0/all/0/1"&gt;Jean-Luc Starck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ciuciu_P/0/1/0/all/0/1"&gt;Philippe Ciuciu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[One-Pixel Attack Deceives Computer-Assisted Diagnosis of Cancer. (arXiv:2012.00517v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.00517</id>
        <link href="http://arxiv.org/abs/2012.00517"/>
        <updated>2021-06-03T02:10:37.340Z</updated>
        <summary type="html"><![CDATA[Computer vision and machine learning can be used to automate various tasks in
cancer diagnostic and detection. If an attacker can manipulate the automated
processing, the results can be devastating and in the worst case lead to wrong
diagnosis and treatment. In this research, the goal is to demonstrate the use
of one-pixel attacks in a real-life scenario with a real pathology dataset,
TUPAC16, which consists of digitized whole-slide images. We attack against the
IBM CODAIT's MAX breast cancer detector using adversarial images. These
adversarial examples are found using differential evolution to perform the
one-pixel modification to the images in the dataset. The results indicate that
a minor one-pixel modification of a whole slide image under analysis can affect
the diagnosis by reversing the automatic diagnosis result. The attack poses a
threat from the cyber security perspective: the one-pixel method can be used as
an attack vector by a motivated attacker.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Korpihalkola_J/0/1/0/all/0/1"&gt;Joni Korpihalkola&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sipola_T/0/1/0/all/0/1"&gt;Tuomo Sipola&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Puuska_S/0/1/0/all/0/1"&gt;Samir Puuska&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kokkonen_T/0/1/0/all/0/1"&gt;Tero Kokkonen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SENTINEL: Taming Uncertainty with Ensemble-based Distributional Reinforcement Learning. (arXiv:2102.11075v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.11075</id>
        <link href="http://arxiv.org/abs/2102.11075"/>
        <updated>2021-06-03T02:10:37.330Z</updated>
        <summary type="html"><![CDATA[In this paper, we consider risk-sensitive sequential decision-making in
model-based Reinforcement Learning (RL). Our contributions are two-fold. First,
we introduce a novel and coherent quantification of risk, namely composite
risk, which quantifies joint effect of aleatory and epistemic risk during the
learning process. Existing works considered either aleatory or epistemic risk
individually, or an additive combination of the two. We prove that the additive
formulation is a particular case of the composite risk when the epistemic risk
measure is replaced with expectation. Thus, the composite risk provides an
estimate more sensitive to both aleatory and epistemic sources of uncertainties
than the individual and additive formulations. Following that, we propose to
use a bootstrapping method, SENTINEL-K, for performing distributional RL.
SENTINEL-K uses an ensemble of $K$ learners to estimate the return
distribution. We use the Follow The Regularised Leader (FTRL) to aggregate the
return distributions of $K$ learners and to estimate the composite risk. We
experimentally verify that SENTINEL-K estimates the return distribution better,
and while used with composite risk estimate, demonstrates better risk-sensitive
performance than state-of-the-art risk-sensitive and distributional RL
algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Eriksson_H/0/1/0/all/0/1"&gt;Hannes Eriksson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Basu_D/0/1/0/all/0/1"&gt;Debabrota Basu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alibeigi_M/0/1/0/all/0/1"&gt;Mina Alibeigi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dimitrakakis_C/0/1/0/all/0/1"&gt;Christos Dimitrakakis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Post-mortem on a deep learning contest: a Simpson's paradox and the complementary roles of scale metrics versus shape metrics. (arXiv:2106.00734v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.00734</id>
        <link href="http://arxiv.org/abs/2106.00734"/>
        <updated>2021-06-03T02:10:37.315Z</updated>
        <summary type="html"><![CDATA[To understand better the causes of good generalization performance in
state-of-the-art neural network (NN) models, we analyze of a corpus of models
that was made publicly-available for a contest to predict the generalization
accuracy of NNs. These models include a wide range of qualities and were
trained with a range of architectures and regularization hyperparameters. We
identify what amounts to a Simpson's paradox: where "scale" metrics (from
traditional statistical learning theory) perform well overall but perform
poorly on subpartitions of the data of a given depth, when regularization
hyperparameters are varied; and where "shape" metrics (from Heavy-Tailed Self
Regularization theory) perform well on subpartitions of the data, when
hyperparameters are varied for models of a given depth, but perform poorly
overall when models with varying depths are aggregated. Our results highlight
the subtly of comparing models when both architectures and hyperparameters are
varied, as well as the complementary role of implicit scale versus implicit
shape parameters in understanding NN model quality. Our results also suggest
caution when one tries to extract causal insight with a single metric applied
to aggregate data, and they highlight the need to go beyond one-size-fits-all
metrics based on upper bounds from generalization theory to describe the
performance of state-of-the-art NN models. Based on these findings, we present
two novel shape metrics, one data-independent, and the other data-dependent,
which can predict trends in the test accuracy of a series of NNs, of a fixed
architecture/depth, when varying solver hyperparameters.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Martin_C/0/1/0/all/0/1"&gt;Charles H. Martin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mahoney_M/0/1/0/all/0/1"&gt;Michael W. Mahoney&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Federated Learning with Fair Averaging. (arXiv:2104.14937v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.14937</id>
        <link href="http://arxiv.org/abs/2104.14937"/>
        <updated>2021-06-03T02:10:37.293Z</updated>
        <summary type="html"><![CDATA[Fairness has emerged as a critical problem in federated learning (FL). In
this work, we identify a cause of unfairness in FL -- \emph{conflicting}
gradients with large differences in the magnitudes. To address this issue, we
propose the federated fair averaging (FedFV) algorithm to mitigate potential
conflicts among clients before averaging their gradients. We first use the
cosine similarity to detect gradient conflicts, and then iteratively eliminate
such conflicts by modifying both the direction and the magnitude of the
gradients. We further show the theoretical foundation of FedFV to mitigate the
issue conflicting gradients and converge to Pareto stationary solutions.
Extensive experiments on a suite of federated datasets confirm that FedFV
compares favorably against state-of-the-art methods in terms of fairness,
accuracy and efficiency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zheng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1"&gt;Xiaoliang Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_J/0/1/0/all/0/1"&gt;Jianzhong Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_C/0/1/0/all/0/1"&gt;Chenglu Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Cheng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_R/0/1/0/all/0/1"&gt;Rongshan Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Image-Audio Encoding to Improve C2 Decision-Making in Multi-Domain Environment. (arXiv:2106.00787v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.00787</id>
        <link href="http://arxiv.org/abs/2106.00787"/>
        <updated>2021-06-03T02:10:37.288Z</updated>
        <summary type="html"><![CDATA[The military is investigating methods to improve communication and agility in
its multi-domain operations (MDO). Nascent popularity of Internet of Things
(IoT) has gained traction in public and government domains. Its usage in MDO
may revolutionize future battlefields and may enable strategic advantage. While
this technology offers leverage to military capabilities, it comes with
challenges where one is the uncertainty and associated risk. A key question is
how can these uncertainties be addressed. Recently published studies proposed
information camouflage to transform information from one data domain to
another. As this is comparatively a new approach, we investigate challenges of
such transformations and how these associated uncertainties can be detected and
addressed, specifically unknown-unknowns to improve decision-making.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_P/0/1/0/all/0/1"&gt;Piyush K. Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raglin_A/0/1/0/all/0/1"&gt;Adrienne Raglin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Second-Order Neural Dependency Parsing with Message Passing and End-to-End Training. (arXiv:2010.05003v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.05003</id>
        <link href="http://arxiv.org/abs/2010.05003"/>
        <updated>2021-06-03T02:10:37.283Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose second-order graph-based neural dependency parsing
using message passing and end-to-end neural networks. We empirically show that
our approaches match the accuracy of very recent state-of-the-art second-order
graph-based neural dependency parsers and have significantly faster speed in
both training and testing. We also empirically show the advantage of
second-order parsing over first-order parsing and observe that the usefulness
of the head-selection structured constraint vanishes when using BERT embedding.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xinyu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tu_K/0/1/0/all/0/1"&gt;Kewei Tu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[multiPRover: Generating Multiple Proofs for Improved Interpretability in Rule Reasoning. (arXiv:2106.01354v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01354</id>
        <link href="http://arxiv.org/abs/2106.01354"/>
        <updated>2021-06-03T02:10:37.278Z</updated>
        <summary type="html"><![CDATA[We focus on a type of linguistic formal reasoning where the goal is to reason
over explicit knowledge in the form of natural language facts and rules (Clark
et al., 2020). A recent work, named PRover (Saha et al., 2020), performs such
reasoning by answering a question and also generating a proof graph that
explains the answer. However, compositional reasoning is not always unique and
there may be multiple ways of reaching the correct answer. Thus, in our work,
we address a new and challenging problem of generating multiple proof graphs
for reasoning over natural language rule-bases. Each proof provides a different
rationale for the answer, thereby improving the interpretability of such
reasoning systems. In order to jointly learn from all proof graphs and exploit
the correlations between multiple proofs for a question, we pose this task as a
set generation problem over structured output spaces where each proof is
represented as a directed graph. We propose two variants of a proof-set
generation model, multiPRover. Our first model, Multilabel-multiPRover,
generates a set of proofs via multi-label classification and implicit
conditioning between the proofs; while the second model, Iterative-multiPRover,
generates proofs iteratively by explicitly conditioning on the previously
generated proofs. Experiments on multiple synthetic, zero-shot, and
human-paraphrased datasets reveal that both multiPRover models significantly
outperform PRover on datasets containing multiple gold proofs.
Iterative-multiPRover obtains state-of-the-art proof F1 in zero-shot scenarios
where all examples have single correct proofs. It also generalizes better to
questions requiring higher depths of reasoning where multiple proofs are more
frequent. Our code and models are publicly available at
https://github.com/swarnaHub/multiPRover]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Saha_S/0/1/0/all/0/1"&gt;Swarnadeep Saha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yadav_P/0/1/0/all/0/1"&gt;Prateek Yadav&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1"&gt;Mohit Bansal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Low-resource expressive text-to-speech using data augmentation. (arXiv:2011.05707v2 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.05707</id>
        <link href="http://arxiv.org/abs/2011.05707"/>
        <updated>2021-06-03T02:10:37.272Z</updated>
        <summary type="html"><![CDATA[While recent neural text-to-speech (TTS) systems perform remarkably well,
they typically require a substantial amount of recordings from the target
speaker reading in the desired speaking style. In this work, we present a novel
3-step methodology to circumvent the costly operation of recording large
amounts of target data in order to build expressive style voices with as little
as 15 minutes of such recordings. First, we augment data via voice conversion
by leveraging recordings in the desired speaking style from other speakers.
Next, we use that synthetic data on top of the available recordings to train a
TTS model. Finally, we fine-tune that model to further increase quality. Our
evaluations show that the proposed changes bring significant improvements over
non-augmented models across many perceived aspects of synthesised speech. We
demonstrate the proposed approach on 2 styles (newscaster and conversational),
on various speakers, and on both single and multi-speaker models, illustrating
the robustness of our approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Huybrechts_G/0/1/0/all/0/1"&gt;Goeric Huybrechts&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Merritt_T/0/1/0/all/0/1"&gt;Thomas Merritt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Comini_G/0/1/0/all/0/1"&gt;Giulia Comini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Perz_B/0/1/0/all/0/1"&gt;Bartek Perz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shah_R/0/1/0/all/0/1"&gt;Raahil Shah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lorenzo_Trueba_J/0/1/0/all/0/1"&gt;Jaime Lorenzo-Trueba&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA["Call me sexist, but...": Revisiting Sexism Detection Using Psychological Scales and Adversarial Samples. (arXiv:2004.12764v2 [cs.CY] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.12764</id>
        <link href="http://arxiv.org/abs/2004.12764"/>
        <updated>2021-06-03T02:10:37.252Z</updated>
        <summary type="html"><![CDATA[Research has focused on automated methods to effectively detect sexism
online. Although overt sexism seems easy to spot, its subtle forms and manifold
expressions are not. In this paper, we outline the different dimensions of
sexism by grounding them in their implementation in psychological scales. From
the scales, we derive a codebook for sexism in social media, which we use to
annotate existing and novel datasets, surfacing their limitations in breadth
and validity with respect to the construct of sexism. Next, we leverage the
annotated datasets to generate adversarial examples, and test the reliability
of sexism detection methods. Results indicate that current machine learning
models pick up on a very narrow set of linguistic markers of sexism and do not
generalize well to out-of-domain examples. Yet, including diverse data and
adversarial examples at training time results in models that generalize better
and that are more robust to artifacts of data collection. By providing a
scale-based codebook and insights regarding the shortcomings of the
state-of-the-art, we hope to contribute to the development of better and
broader models for sexism detection, including reflections on theory-driven
approaches to data collection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Samory_M/0/1/0/all/0/1"&gt;Mattia Samory&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sen_I/0/1/0/all/0/1"&gt;Indira Sen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kohne_J/0/1/0/all/0/1"&gt;Julian Kohne&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Floeck_F/0/1/0/all/0/1"&gt;Fabian Floeck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wagner_C/0/1/0/all/0/1"&gt;Claudia Wagner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[WikiTableT: A Large-Scale Data-to-Text Dataset for Generating Wikipedia Article Sections. (arXiv:2012.14919v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.14919</id>
        <link href="http://arxiv.org/abs/2012.14919"/>
        <updated>2021-06-03T02:10:37.231Z</updated>
        <summary type="html"><![CDATA[Datasets for data-to-text generation typically focus either on multi-domain,
single-sentence generation or on single-domain, long-form generation. In this
work, we cast generating Wikipedia sections as a data-to-text generation task
and create a large-scale dataset, WikiTableT, that pairs Wikipedia sections
with their corresponding tabular data and various metadata. WikiTableT contains
millions of instances, covering a broad range of topics, as well as a variety
of flavors of generation tasks with different levels of flexibility. We
benchmark several training and decoding strategies on WikiTableT. Our
qualitative analysis shows that the best approaches can generate fluent and
high quality texts but they struggle with coherence and factuality, showing the
potential for our dataset to inspire future work on long-form generation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1"&gt;Mingda Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wiseman_S/0/1/0/all/0/1"&gt;Sam Wiseman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gimpel_K/0/1/0/all/0/1"&gt;Kevin Gimpel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Offline Reinforcement Learning with Pseudometric Learning. (arXiv:2103.01948v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.01948</id>
        <link href="http://arxiv.org/abs/2103.01948"/>
        <updated>2021-06-03T02:10:37.227Z</updated>
        <summary type="html"><![CDATA[Offline Reinforcement Learning methods seek to learn a policy from logged
transitions of an environment, without any interaction. In the presence of
function approximation, and under the assumption of limited coverage of the
state-action space of the environment, it is necessary to enforce the policy to
visit state-action pairs close to the support of logged transitions. In this
work, we propose an iterative procedure to learn a pseudometric (closely
related to bisimulation metrics) from logged transitions, and use it to define
this notion of closeness. We show its convergence and extend it to the function
approximation setting. We then use this pseudometric to define a new lookup
based bonus in an actor-critic algorithm: PLOFF. This bonus encourages the
actor to stay close, in terms of the defined pseudometric, to the support of
logged transitions. Finally, we evaluate the method on hand manipulation and
locomotion tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dadashi_R/0/1/0/all/0/1"&gt;Robert Dadashi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rezaeifar_S/0/1/0/all/0/1"&gt;Shideh Rezaeifar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vieillard_N/0/1/0/all/0/1"&gt;Nino Vieillard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hussenot_L/0/1/0/all/0/1"&gt;L&amp;#xe9;onard Hussenot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pietquin_O/0/1/0/all/0/1"&gt;Olivier Pietquin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geist_M/0/1/0/all/0/1"&gt;Matthieu Geist&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[More Identifiable yet Equally Performant Transformers for Text Classification. (arXiv:2106.01269v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01269</id>
        <link href="http://arxiv.org/abs/2106.01269"/>
        <updated>2021-06-03T02:10:37.222Z</updated>
        <summary type="html"><![CDATA[Interpretability is an important aspect of the trustworthiness of a model's
predictions. Transformer's predictions are widely explained by the attention
weights, i.e., a probability distribution generated at its self-attention unit
(head). Current empirical studies provide shreds of evidence that attention
weights are not explanations by proving that they are not unique. A recent
study showed theoretical justifications to this observation by proving the
non-identifiability of attention weights. For a given input to a head and its
output, if the attention weights generated in it are unique, we call the
weights identifiable. In this work, we provide deeper theoretical analysis and
empirical observations on the identifiability of attention weights. Ignored in
the previous works, we find the attention weights are more identifiable than we
currently perceive by uncovering the hidden role of the key vector. However,
the weights are still prone to be non-unique attentions that make them unfit
for interpretation. To tackle this issue, we provide a variant of the encoder
layer that decouples the relationship between key and value vector and provides
identifiable weights up to the desired length of the input. We prove the
applicability of such variations by providing empirical justifications on
varied text classification tasks. The implementations are available at
https://github.com/declare-lab/identifiable-transformers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bhardwaj_R/0/1/0/all/0/1"&gt;Rishabh Bhardwaj&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Majumder_N/0/1/0/all/0/1"&gt;Navonil Majumder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Poria_S/0/1/0/all/0/1"&gt;Soujanya Poria&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hovy_E/0/1/0/all/0/1"&gt;Eduard Hovy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Few-Shot Question Answering by Pretraining Span Selection. (arXiv:2101.00438v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.00438</id>
        <link href="http://arxiv.org/abs/2101.00438"/>
        <updated>2021-06-03T02:10:37.217Z</updated>
        <summary type="html"><![CDATA[In several question answering benchmarks, pretrained models have reached
human parity through fine-tuning on an order of 100,000 annotated questions and
answers. We explore the more realistic few-shot setting, where only a few
hundred training examples are available, and observe that standard models
perform poorly, highlighting the discrepancy between current pretraining
objectives and question answering. We propose a new pretraining scheme tailored
for question answering: recurring span selection. Given a passage with multiple
sets of recurring spans, we mask in each set all recurring spans but one, and
ask the model to select the correct span in the passage for each masked span.
Masked spans are replaced with a special token, viewed as a question
representation, that is later used during fine-tuning to select the answer
span. The resulting model obtains surprisingly good results on multiple
benchmarks (e.g., 72.7 F1 on SQuAD with only 128 training examples), while
maintaining competitive performance in the high-resource setting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ram_O/0/1/0/all/0/1"&gt;Ori Ram&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kirstain_Y/0/1/0/all/0/1"&gt;Yuval Kirstain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berant_J/0/1/0/all/0/1"&gt;Jonathan Berant&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Globerson_A/0/1/0/all/0/1"&gt;Amir Globerson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Levy_O/0/1/0/all/0/1"&gt;Omer Levy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GraghVQA: Language-Guided Graph Neural Networks for Graph-based Visual Question Answering. (arXiv:2104.10283v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.10283</id>
        <link href="http://arxiv.org/abs/2104.10283"/>
        <updated>2021-06-03T02:10:37.203Z</updated>
        <summary type="html"><![CDATA[Images are more than a collection of objects or attributes -- they represent
a web of relationships among interconnected objects. Scene Graph has emerged as
a new modality for a structured graphical representation of images. Scene Graph
encodes objects as nodes connected via pairwise relations as edges. To support
question answering on scene graphs, we propose GraphVQA, a language-guided
graph neural network framework that translates and executes a natural language
question as multiple iterations of message passing among graph nodes. We
explore the design space of GraphVQA framework, and discuss the trade-off of
different design choices. Our experiments on GQA dataset show that GraphVQA
outperforms the state-of-the-art model by a large margin (88.43% vs. 94.78%).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liang_W/0/1/0/all/0/1"&gt;Weixin Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1"&gt;Yanhao Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zixuan Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SocAoG: Incremental Graph Parsing for Social Relation Inference in Dialogues. (arXiv:2106.01006v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01006</id>
        <link href="http://arxiv.org/abs/2106.01006"/>
        <updated>2021-06-03T02:10:37.197Z</updated>
        <summary type="html"><![CDATA[Inferring social relations from dialogues is vital for building emotionally
intelligent robots to interpret human language better and act accordingly. We
model the social network as an And-or Graph, named SocAoG, for the consistency
of relations among a group and leveraging attributes as inference cues.
Moreover, we formulate a sequential structure prediction task, and propose an
$\alpha$-$\beta$-$\gamma$ strategy to incrementally parse SocAoG for the
dynamic inference upon any incoming utterance: (i) an $\alpha$ process
predicting attributes and relations conditioned on the semantics of dialogues,
(ii) a $\beta$ process updating the social relations based on related
attributes, and (iii) a $\gamma$ process updating individual's attributes based
on interpersonal social relations. Empirical results on DialogRE and MovieGraph
show that our model infers social relations more accurately than the
state-of-the-art methods. Moreover, the ablation study shows the three
processes complement each other, and the case study demonstrates the dynamic
relational inference.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_L/0/1/0/all/0/1"&gt;Liang Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1"&gt;Yuan Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yizhou Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_P/0/1/0/all/0/1"&gt;Pan Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_B/0/1/0/all/0/1"&gt;Baolin Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1"&gt;Zhou Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Ying Nian Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1"&gt;Song-Chun Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stochastic Optimization of Areas Under Precision-Recall Curves with Provable Convergence. (arXiv:2104.08736v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.08736</id>
        <link href="http://arxiv.org/abs/2104.08736"/>
        <updated>2021-06-03T02:10:37.193Z</updated>
        <summary type="html"><![CDATA[Areas under ROC (AUROC) and precision-recall curves (AUPRC) are common
metrics for evaluating classification performance for imbalanced problems.
Compared with AUROC, AUPRC is a more appropriate metric for highly imbalanced
datasets. While stochastic optimization of AUROC has been studied extensively,
principled stochastic optimization of AUPRC has been rarely explored. In this
work, we propose a principled technical method to optimize AUPRC for deep
learning. Our approach is based on maximizing the averaged precision (AP),
which is an unbiased point estimator of AUPRC. We cast the objective into a sum
of {\it dependent compositional functions} with inner functions dependent on
random variables of the outer level. We propose efficient adaptive and
non-adaptive stochastic algorithms with {\it provable convergence guarantee
under mild conditions} by leveraging recent advances in stochastic
compositional optimization. Extensive experimental results on image and graph
datasets demonstrate that our proposed method outperforms prior methods on
imbalanced problems in terms of AUPRC. To the best of our knowledge, our work
represents the first attempt to optimize AUPRC with provable convergence.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qi_Q/0/1/0/all/0/1"&gt;Qi Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1"&gt;Youzhi Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zhao Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1"&gt;Shuiwang Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1"&gt;Tianbao Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[T-BERT -- Model for Sentiment Analysis of Micro-blogs Integrating Topic Model and BERT. (arXiv:2106.01097v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01097</id>
        <link href="http://arxiv.org/abs/2106.01097"/>
        <updated>2021-06-03T02:10:37.187Z</updated>
        <summary type="html"><![CDATA[Sentiment analysis (SA) has become an extensive research area in recent years
impacting diverse fields including ecommerce, consumer business, and politics,
driven by increasing adoption and usage of social media platforms. It is
challenging to extract topics and sentiments from unsupervised short texts
emerging in such contexts, as they may contain figurative words, strident data,
and co-existence of many possible meanings for a single word or phrase, all
contributing to obtaining incorrect topics. Most prior research is based on a
specific theme/rhetoric/focused-content on a clean dataset. In the work
reported here, the effectiveness of BERT(Bidirectional Encoder Representations
from Transformers) in sentiment classification tasks from a raw live dataset
taken from a popular microblogging platform is demonstrated. A novel T-BERT
framework is proposed to show the enhanced performance obtainable by combining
latent topics with contextual BERT embeddings. Numerical experiments were
conducted on an ensemble with about 42000 datasets using NimbleBox.ai platform
with a hardware configuration consisting of Nvidia Tesla K80(CUDA), 4 core CPU,
15GB RAM running on an isolated Google Cloud Platform instance. The empirical
results show that the model improves in performance while adding topics to BERT
and an accuracy rate of 90.81% on sentiment classification using BERT with the
proposed approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Palani_S/0/1/0/all/0/1"&gt;Sarojadevi Palani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rajagopal_P/0/1/0/all/0/1"&gt;Prabhu Rajagopal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pancholi_S/0/1/0/all/0/1"&gt;Sidharth Pancholi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cascade versus Direct Speech Translation: Do the Differences Still Make a Difference?. (arXiv:2106.01045v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01045</id>
        <link href="http://arxiv.org/abs/2106.01045"/>
        <updated>2021-06-03T02:10:37.180Z</updated>
        <summary type="html"><![CDATA[Five years after the first published proofs of concept, direct approaches to
speech translation (ST) are now competing with traditional cascade solutions.
In light of this steady progress, can we claim that the performance gap between
the two is closed? Starting from this question, we present a systematic
comparison between state-of-the-art systems representative of the two
paradigms. Focusing on three language directions
(English-German/Italian/Spanish), we conduct automatic and manual evaluations,
exploiting high-quality professional post-edits and annotations. Our
multi-faceted analysis on one of the few publicly available ST benchmarks
attests for the first time that: i) the gap between the two paradigms is now
closed, and ii) the subtle differences observed in their behavior are not
sufficient for humans neither to distinguish them nor to prefer one over the
other.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bentivogli_L/0/1/0/all/0/1"&gt;Luisa Bentivogli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cettolo_M/0/1/0/all/0/1"&gt;Mauro Cettolo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gaido_M/0/1/0/all/0/1"&gt;Marco Gaido&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karakanta_A/0/1/0/all/0/1"&gt;Alina Karakanta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martinelli_A/0/1/0/all/0/1"&gt;Alberto Martinelli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Negri_M/0/1/0/all/0/1"&gt;Matteo Negri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Turchi_M/0/1/0/all/0/1"&gt;Marco Turchi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[IPatch: A Remote Adversarial Patch. (arXiv:2105.00113v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.00113</id>
        <link href="http://arxiv.org/abs/2105.00113"/>
        <updated>2021-06-03T02:10:37.165Z</updated>
        <summary type="html"><![CDATA[Applications such as autonomous vehicles and medical screening use deep
learning models to localize and identify hundreds of objects in a single frame.
In the past, it has been shown how an attacker can fool these models by placing
an adversarial patch within a scene. However, these patches must be placed in
the target location and do not explicitly alter the semantics elsewhere in the
image.

In this paper, we introduce a new type of adversarial patch which alters a
model's perception of an image's semantics. These patches can be placed
anywhere within an image to change the classification or semantics of locations
far from the patch. We call this new class of adversarial examples `remote
adversarial patches' (RAP).

We implement our own RAP called IPatch and perform an in-depth analysis on
image segmentation RAP attacks using five state-of-the-art architectures with
eight different encoders on the CamVid street view dataset. Moreover, we
demonstrate that the attack can be extended to object recognition models with
preliminary results on the popular YOLOv3 model. We found that the patch can
change the classification of a remote target region with a success rate of up
to 93% on average.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mirsky_Y/0/1/0/all/0/1"&gt;Yisroel Mirsky&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fusing Context Into Knowledge Graph for Commonsense Question Answering. (arXiv:2012.04808v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.04808</id>
        <link href="http://arxiv.org/abs/2012.04808"/>
        <updated>2021-06-03T02:10:37.160Z</updated>
        <summary type="html"><![CDATA[Commonsense question answering (QA) requires a model to grasp commonsense and
factual knowledge to answer questions about world events. Many prior methods
couple language modeling with knowledge graphs (KG). However, although a KG
contains rich structural information, it lacks the context to provide a more
precise understanding of the concepts. This creates a gap when fusing knowledge
graphs into language modeling, especially when there is insufficient labeled
data. Thus, we propose to employ external entity descriptions to provide
contextual information for knowledge understanding. We retrieve descriptions of
related concepts from Wiktionary and feed them as additional input to
pre-trained language models. The resulting model achieves state-of-the-art
result in the CommonsenseQA dataset and the best result among non-generative
models in OpenBookQA.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yichong Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1"&gt;Chenguang Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1"&gt;Ruochen Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1"&gt;Michael Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1"&gt;Xuedong Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Distribution, Sparsity, and Inference-time Quantization of Attention Values in Transformers. (arXiv:2106.01335v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01335</id>
        <link href="http://arxiv.org/abs/2106.01335"/>
        <updated>2021-06-03T02:10:37.155Z</updated>
        <summary type="html"><![CDATA[How much information do NLP tasks really need from a transformer's attention
mechanism at application-time (inference)? From recent work, we know that there
is sparsity in transformers and that the floating-points within its computation
can be discretized to fewer values with minimal loss to task accuracies.
However, this requires retraining or even creating entirely new models, both of
which can be expensive and carbon-emitting. Focused on optimizations that do
not require training, we systematically study the full range of typical
attention values necessary. This informs the design of an inference-time
quantization technique using both pruning and log-scaled mapping which produces
only a few (e.g. $2^3$) unique values. Over the tasks of question answering and
sentiment analysis, we find nearly 80% of attention values can be pruned to
zeros with minimal ($< 1.0\%$) relative loss in accuracy. We use this pruning
technique in conjunction with quantizing the attention values to only a 3-bit
format, without retraining, resulting in only a 0.8% accuracy reduction on
question answering with fine-tuned RoBERTa.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ji_T/0/1/0/all/0/1"&gt;Tianchu Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1"&gt;Shraddhan Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ferdman_M/0/1/0/all/0/1"&gt;Michael Ferdman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Milder_P/0/1/0/all/0/1"&gt;Peter Milder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schwartz_H/0/1/0/all/0/1"&gt;H. Andrew Schwartz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balasubramanian_N/0/1/0/all/0/1"&gt;Niranjan Balasubramanian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploiting Global Contextual Information for Document-level Named Entity Recognition. (arXiv:2106.00887v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.00887</id>
        <link href="http://arxiv.org/abs/2106.00887"/>
        <updated>2021-06-03T02:10:37.140Z</updated>
        <summary type="html"><![CDATA[Most existing named entity recognition (NER) approaches are based on sequence
labeling models, which focus on capturing the local context dependencies.
However, the way of taking one sentence as input prevents the modeling of
non-sequential global context, which is useful especially when local context
information is limited or ambiguous. To this end, we propose a model called
Global Context enhanced Document-level NER (GCDoc) to leverage global
contextual information from two levels, i.e., both word and sentence. At
word-level, a document graph is constructed to model a wider range of
dependencies between words, then obtain an enriched contextual representation
for each word via graph neural networks (GNN). To avoid the interference of
noise information, we further propose two strategies. First we apply the
epistemic uncertainty theory to find out tokens whose representations are less
reliable, thereby helping prune the document graph. Then a selective auxiliary
classifier is proposed to effectively learn the weight of edges in document
graph and reduce the importance of noisy neighbour nodes. At sentence-level,
for appropriately modeling wider context beyond single sentence, we employ a
cross-sentence module which encodes adjacent sentences and fuses it with the
current sentence representation via attention and gating mechanisms. Extensive
experiments on two benchmark NER datasets (CoNLL 2003 and Ontonotes 5.0 English
dataset) demonstrate the effectiveness of our proposed model. Our model reaches
F1 score of 92.22 (93.40 with BERT) on CoNLL 2003 dataset and 88.32 (90.49 with
BERT) on Ontonotes 5.0 dataset, achieving new state-of-the-art performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zanbo Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_W/0/1/0/all/0/1"&gt;Wei Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mao_X/0/1/0/all/0/1"&gt;Xianling Mao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1"&gt;Shanshan Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1"&gt;Pan Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1"&gt;Zhiyong He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1"&gt;Sheng Jiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generating Informative Conclusions for Argumentative Texts. (arXiv:2106.01064v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01064</id>
        <link href="http://arxiv.org/abs/2106.01064"/>
        <updated>2021-06-03T02:10:37.130Z</updated>
        <summary type="html"><![CDATA[The purpose of an argumentative text is to support a certain conclusion. Yet,
they are often omitted, expecting readers to infer them rather. While
appropriate when reading an individual text, this rhetorical device limits
accessibility when browsing many texts (e.g., on a search engine or on social
media). In these scenarios, an explicit conclusion makes for a good candidate
summary of an argumentative text. This is especially true if the conclusion is
informative, emphasizing specific concepts from the text. With this paper we
introduce the task of generating informative conclusions: First,
Webis-ConcluGen-21 is compiled, a large-scale corpus of 136,996 samples of
argumentative texts and their conclusions. Second, two paradigms for conclusion
generation are investigated; one extractive, the other abstractive in nature.
The latter exploits argumentative knowledge that augment the data via control
codes and finetuning the BART model on several subsets of the corpus. Third,
insights are provided into the suitability of our corpus for the task, the
differences between the two generation paradigms, the trade-off between
informativeness and conciseness, and the impact of encoding argumentative
knowledge. The corpus, code, and the trained models are publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Syed_S/0/1/0/all/0/1"&gt;Shahbaz Syed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Al_Khatib_K/0/1/0/all/0/1"&gt;Khalid Al-Khatib&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alshomary_M/0/1/0/all/0/1"&gt;Milad Alshomary&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wachsmuth_H/0/1/0/all/0/1"&gt;Henning Wachsmuth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Potthast_M/0/1/0/all/0/1"&gt;Martin Potthast&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[COM2SENSE: A Commonsense Reasoning Benchmark with Complementary Sentences. (arXiv:2106.00969v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.00969</id>
        <link href="http://arxiv.org/abs/2106.00969"/>
        <updated>2021-06-03T02:10:37.115Z</updated>
        <summary type="html"><![CDATA[Commonsense reasoning is intuitive for humans but has been a long-term
challenge for artificial intelligence (AI). Recent advancements in pretrained
language models have shown promising results on several commonsense benchmark
datasets. However, the reliability and comprehensiveness of these benchmarks
towards assessing model's commonsense reasoning ability remains unclear. To
this end, we introduce a new commonsense reasoning benchmark dataset comprising
natural language true/false statements, with each sample paired with its
complementary counterpart, resulting in 4k sentence pairs. We propose a
pairwise accuracy metric to reliably measure an agent's ability to perform
commonsense reasoning over a given situation. The dataset is crowdsourced and
enhanced with an adversarial model-in-the-loop setup to incentivize challenging
samples. To facilitate a systematic analysis of commonsense capabilities, we
design our dataset along the dimensions of knowledge domains, reasoning
scenarios and numeracy. Experimental results demonstrate that our strongest
baseline (UnifiedQA-3B), after fine-tuning, achieves ~71% standard accuracy and
~51% pairwise accuracy, well below human performance (~95% for both metrics).
The dataset is available at https://github.com/PlusLabNLP/Com2Sense.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1"&gt;Shikhar Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_N/0/1/0/all/0/1"&gt;Nuan Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1"&gt;Yu Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alipoormolabashi_P/0/1/0/all/0/1"&gt;Pegah Alipoormolabashi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1"&gt;Te-Lin Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1"&gt;Xuezhe Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1"&gt;Nanyun Peng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[One-Pixel Attack Deceives Computer-Assisted Diagnosis of Cancer. (arXiv:2012.00517v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.00517</id>
        <link href="http://arxiv.org/abs/2012.00517"/>
        <updated>2021-06-03T02:10:37.086Z</updated>
        <summary type="html"><![CDATA[Computer vision and machine learning can be used to automate various tasks in
cancer diagnostic and detection. If an attacker can manipulate the automated
processing, the results can be devastating and in the worst case lead to wrong
diagnosis and treatment. In this research, the goal is to demonstrate the use
of one-pixel attacks in a real-life scenario with a real pathology dataset,
TUPAC16, which consists of digitized whole-slide images. We attack against the
IBM CODAIT's MAX breast cancer detector using adversarial images. These
adversarial examples are found using differential evolution to perform the
one-pixel modification to the images in the dataset. The results indicate that
a minor one-pixel modification of a whole slide image under analysis can affect
the diagnosis by reversing the automatic diagnosis result. The attack poses a
threat from the cyber security perspective: the one-pixel method can be used as
an attack vector by a motivated attacker.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Korpihalkola_J/0/1/0/all/0/1"&gt;Joni Korpihalkola&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sipola_T/0/1/0/all/0/1"&gt;Tuomo Sipola&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Puuska_S/0/1/0/all/0/1"&gt;Samir Puuska&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kokkonen_T/0/1/0/all/0/1"&gt;Tero Kokkonen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Finding the $K$-best Non-projective Dependency Trees. (arXiv:2106.00780v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.00780</id>
        <link href="http://arxiv.org/abs/2106.00780"/>
        <updated>2021-06-03T02:10:37.081Z</updated>
        <summary type="html"><![CDATA[The connection between the maximum spanning tree in a directed graph and the
best dependency tree of a sentence has been exploited by the NLP community.
However, for many dependency parsing schemes, an important detail of this
approach is that the spanning tree must have exactly one edge emanating from
the root. While work has been done to efficiently solve this problem for
finding the one-best dependency tree, no research has attempted to extend this
solution to finding the $K$-best dependency trees. This is arguably a more
important extension as a larger proportion of decoded trees will not be subject
to the root constraint of dependency trees. Indeed, we show that the rate of
root constraint violations increases by an average of $13$ times when decoding
with $K\!=\!50$ as opposed to $K\!=\!1$. In this paper, we provide a
simplification of the $K$-best spanning tree algorithm of Camerini et al.
(1980). Our simplification allows us to obtain a constant time speed-up over
the original algorithm. Furthermore, we present a novel extension of the
algorithm for decoding the $K$-best dependency trees of a graph which are
subject to a root constraint.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zmigrod_R/0/1/0/all/0/1"&gt;Ran Zmigrod&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vieira_T/0/1/0/all/0/1"&gt;Tim Vieira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1"&gt;Ryan Cotterell&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring Discourse Structures for Argument Impact Classification. (arXiv:2106.00976v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.00976</id>
        <link href="http://arxiv.org/abs/2106.00976"/>
        <updated>2021-06-03T02:10:37.076Z</updated>
        <summary type="html"><![CDATA[Discourse relations among arguments reveal logical structures of a debate
conversation. However, no prior work has explicitly studied how the sequence of
discourse relations influence a claim's impact. This paper empirically shows
that the discourse relations between two arguments along the context path are
essential factors for identifying the persuasive power of an argument. We
further propose DisCOC to inject and fuse the sentence-level structural
discourse information with contextualized features derived from large-scale
language models. Experimental results and extensive analysis show that the
attention and gate mechanisms that explicitly model contexts and texts can
indeed help the argument impact classification task defined by Durmus et al.
(2019), and discourse structures among the context path of the claim to be
classified can further boost the performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ou_J/0/1/0/all/0/1"&gt;Jiefu Ou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1"&gt;Yangqiu Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1"&gt;Xin Jiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MPASNET: Motion Prior-Aware Siamese Network for Unsupervised Deep Crowd Segmentation in Video Scenes. (arXiv:2101.08609v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.08609</id>
        <link href="http://arxiv.org/abs/2101.08609"/>
        <updated>2021-06-03T02:10:37.070Z</updated>
        <summary type="html"><![CDATA[Crowd segmentation is a fundamental task serving as the basis of crowded
scene analysis, and it is highly desirable to obtain refined pixel-level
segmentation maps. However, it remains a challenging problem, as existing
approaches either require dense pixel-level annotations to train deep learning
models or merely produce rough segmentation maps from optical or particle flows
with physical models. In this paper, we propose the Motion Prior-Aware Siamese
Network (MPASNET) for unsupervised crowd semantic segmentation. This model not
only eliminates the need for annotation but also yields high-quality
segmentation maps. Specially, we first analyze the coherent motion patterns
across the frames and then apply a circular region merging strategy on the
collective particles to generate pseudo-labels. Moreover, we equip MPASNET with
siamese branches for augmentation-invariant regularization and siamese feature
aggregation. Experiments over benchmark datasets indicate that our model
outperforms the state-of-the-arts by more than 12% in terms of mIoU.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jinhai Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Hua Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fidelity and Privacy of Synthetic Medical Data. (arXiv:2101.08658v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.08658</id>
        <link href="http://arxiv.org/abs/2101.08658"/>
        <updated>2021-06-03T02:10:37.054Z</updated>
        <summary type="html"><![CDATA[The digitization of medical records ushered in a new era of big data to
clinical science, and with it the possibility that data could be shared, to
multiply insights beyond what investigators could abstract from paper records.
The need to share individual-level medical data to accelerate innovation in
precision medicine continues to grow, and has never been more urgent, as
scientists grapple with the COVID-19 pandemic. However, enthusiasm for the use
of big data has been tempered by a fully appropriate concern for patient
autonomy and privacy. That is, the ability to extract private or confidential
information about an individual, in practice, renders it difficult to share
data, since significant infrastructure and data governance must be established
before data can be shared. Although HIPAA provided de-identification as an
approved mechanism for data sharing, linkage attacks were identified as a major
vulnerability. A variety of mechanisms have been established to avoid leaking
private information, such as field suppression or abstraction, strictly
limiting the amount of information that can be shared, or employing
mathematical techniques such as differential privacy. Another approach, which
we focus on here, is creating synthetic data that mimics the underlying data.
For synthetic data to be a useful mechanism in support of medical innovation
and a proxy for real-world evidence, one must demonstrate two properties of the
synthetic dataset: (1) any analysis on the real data must be matched by
analysis of the synthetic data (statistical fidelity) and (2) the synthetic
data must preserve privacy, with minimal risk of re-identification (privacy
guarantee). In this paper we propose a framework for quantifying the
statistical fidelity and privacy preservation properties of synthetic datasets
and demonstrate these metrics for synthetic data generated by Syntegra
technology.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mendelevitch_O/0/1/0/all/0/1"&gt;Ofer Mendelevitch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lesh_M/0/1/0/all/0/1"&gt;Michael D. Lesh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[End-to-End NLP Knowledge Graph Construction. (arXiv:2106.01167v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01167</id>
        <link href="http://arxiv.org/abs/2106.01167"/>
        <updated>2021-06-03T02:10:37.049Z</updated>
        <summary type="html"><![CDATA[This paper studies the end-to-end construction of an NLP Knowledge Graph (KG)
from scientific papers. We focus on extracting four types of relations:
evaluatedOn between tasks and datasets, evaluatedBy between tasks and
evaluation metrics, as well as coreferent and related relations between the
same type of entities. For instance, F1-score is coreferent with F-measure. We
introduce novel methods for each of these relation types and apply our final
framework (SciNLP-KG) to 30,000 NLP papers from ACL Anthology to build a
large-scale KG, which can facilitate automatically constructing scientific
leaderboards for the NLP community. The results of our experiments indicate
that the resulting KG contains high-quality information.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mondal_I/0/1/0/all/0/1"&gt;Ishani Mondal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1"&gt;Yufang Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jochim_C/0/1/0/all/0/1"&gt;Charles Jochim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[When and Why does a Model Fail? A Human-in-the-loop Error Detection Framework for Sentiment Analysis. (arXiv:2106.00954v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.00954</id>
        <link href="http://arxiv.org/abs/2106.00954"/>
        <updated>2021-06-03T02:10:37.035Z</updated>
        <summary type="html"><![CDATA[Although deep neural networks have been widely employed and proven effective
in sentiment analysis tasks, it remains challenging for model developers to
assess their models for erroneous predictions that might exist prior to
deployment. Once deployed, emergent errors can be hard to identify in
prediction run-time and impossible to trace back to their sources. To address
such gaps, in this paper we propose an error detection framework for sentiment
analysis based on explainable features. We perform global-level feature
validation with human-in-the-loop assessment, followed by an integration of
global and local-level feature contribution analysis. Experimental results show
that, given limited human-in-the-loop intervention, our method is able to
identify erroneous model predictions on unseen data with high precision.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhe Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1"&gt;Yufan Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mahmud_J/0/1/0/all/0/1"&gt;Jalal Mahmud&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using a Neural Network to Detect Anomalies given an N-gram Profile. (arXiv:2104.05571v2 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.05571</id>
        <link href="http://arxiv.org/abs/2104.05571"/>
        <updated>2021-06-03T02:10:37.026Z</updated>
        <summary type="html"><![CDATA[In order to detect unknown intrusions and runtime errors of computer
programs, the cyber-security community has developed various detection
techniques. Anomaly detection is an approach that is designed to profile the
normal runtime behavior of computer programs in order to detect intrusions and
errors as anomalous deviations from the observed normal. However, normal but
unobserved behavior can trigger false positives. This limitation has
significantly decreased the practical viability of anomaly detection
techniques. Reported approaches to this limitation span a simple alert
threshold definition to distribution models for approximating all normal
behavior based on the limited observation. However, each assumption or
approximation poses the potential for even greater false positive rates. This
paper presents our study on how to explain the presence of anomalies using a
neural network, particularly Long Short-Term Memory, independent of actual data
distributions. We present and compare three anomaly detection models, and
report on our experience running different types of attacks on an Apache
Hypertext Transfer Protocol server. We performed a comparative study, focusing
on each model's ability to detect the onset of each attack while avoiding false
positives resulting from unknown normal behavior. Our best-performing model
detected the true onset of every attack with zero false positives.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1"&gt;Byunggu Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1"&gt;Junwhan Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DYPLOC: Dynamic Planning of Content Using Mixed Language Models for Text Generation. (arXiv:2106.00791v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.00791</id>
        <link href="http://arxiv.org/abs/2106.00791"/>
        <updated>2021-06-03T02:10:36.983Z</updated>
        <summary type="html"><![CDATA[We study the task of long-form opinion text generation, which faces at least
two distinct challenges. First, existing neural generation models fall short of
coherence, thus requiring efficient content planning. Second, diverse types of
information are needed to guide the generator to cover both subjective and
objective content. To this end, we propose DYPLOC, a generation framework that
conducts dynamic planning of content while generating the output based on a
novel design of mixed language models. To enrich the generation with diverse
content, we further propose to use large pre-trained models to predict relevant
concepts and to generate claims. We experiment with two challenging tasks on
newly collected datasets: (1) argument generation with Reddit ChangeMyView, and
(2) writing articles using New York Times' Opinion section. Automatic
evaluation shows that our model significantly outperforms competitive
comparisons. Human judges further confirm that our generations are more
coherent with richer content.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hua_X/0/1/0/all/0/1"&gt;Xinyu Hua&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sreevatsa_A/0/1/0/all/0/1"&gt;Ashwin Sreevatsa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Lu Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient and Interpretable Robot Manipulation with Graph Neural Networks. (arXiv:2102.13177v2 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.13177</id>
        <link href="http://arxiv.org/abs/2102.13177"/>
        <updated>2021-06-03T02:10:36.953Z</updated>
        <summary type="html"><![CDATA[Many manipulation tasks can be naturally cast as a sequence of spatial
relationships and constraints between objects. We aim to discover and scale
these task-specific spatial relationships by representing manipulation tasks as
operations over graphs. To do this, we pose manipulating a large, variable
number of objects as a probabilistic classification problem over actions,
objects and goals, learned using graph neural networks (GNNs). Our formulation
first transforms the environment into a graph representation, then applies a
trained GNN policy to predict which object to manipulate towards which goal
state. Our GNN policies are trained using very few expert demonstrations on
simple tasks, and exhibit generalization over number and configurations of
objects in the environment and even to new, more complex tasks, while providing
interpretable explanations for their decision-making. We present experiments
which show that a single learned GNN policy can solve a variety of long-horizon
blockstacking and rearrangement tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1"&gt;Yixin Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1"&gt;Austin S. Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rai_A/0/1/0/all/0/1"&gt;Akshara Rai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Closeness and Uncertainty Aware Adversarial Examples Detection in Adversarial Machine Learning. (arXiv:2012.06390v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.06390</id>
        <link href="http://arxiv.org/abs/2012.06390"/>
        <updated>2021-06-03T02:10:36.921Z</updated>
        <summary type="html"><![CDATA[While state-of-the-art Deep Neural Network (DNN) models are considered to be
robust to random perturbations, it was shown that these architectures are
highly vulnerable to deliberately crafted perturbations, albeit being
quasi-imperceptible. These vulnerabilities make it challenging to deploy DNN
models in security-critical areas. In recent years, many research studies have
been conducted to develop new attack methods and come up with new defense
techniques that enable more robust and reliable models. In this work, we
explore and assess the usage of different type of metrics for detecting
adversarial samples. We first leverage the usage of moment-based predictive
uncertainty estimates of a DNN classifier obtained using Monte-Carlo Dropout
Sampling. And we also introduce a new method that operates in the subspace of
deep features extracted by the model. We verified the effectiveness of our
approach on a range of standard datasets like MNIST (Digit), MNIST (Fashion)
and CIFAR-10. Our experiments show that these two different approaches
complement each other, and the combined usage of all the proposed metrics
yields up to 99 \% ROC-AUC scores regardless of the attack algorithm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tuna_O/0/1/0/all/0/1"&gt;Omer Faruk Tuna&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Catak_F/0/1/0/all/0/1"&gt;Ferhat Ozgur Catak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eskil_M/0/1/0/all/0/1"&gt;M. Taner Eskil&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Practical Lipreading with Distilled and Efficient Models. (arXiv:2007.06504v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.06504</id>
        <link href="http://arxiv.org/abs/2007.06504"/>
        <updated>2021-06-03T02:10:36.901Z</updated>
        <summary type="html"><![CDATA[Lipreading has witnessed a lot of progress due to the resurgence of neural
networks. Recent works have placed emphasis on aspects such as improving
performance by finding the optimal architecture or improving generalization.
However, there is still a significant gap between the current methodologies and
the requirements for an effective deployment of lipreading in practical
scenarios. In this work, we propose a series of innovations that significantly
bridge that gap: first, we raise the state-of-the-art performance by a wide
margin on LRW and LRW-1000 to 88.5% and 46.6%, respectively using
self-distillation. Secondly, we propose a series of architectural changes,
including a novel Depthwise Separable Temporal Convolutional Network (DS-TCN)
head, that slashes the computational cost to a fraction of the (already quite
efficient) original model. Thirdly, we show that knowledge distillation is a
very effective tool for recovering performance of the lightweight models. This
results in a range of models with different accuracy-efficiency trade-offs.
However, our most promising lightweight models are on par with the current
state-of-the-art while showing a reduction of 8.2x and 3.9x in terms of
computational cost and number of parameters, respectively, which we hope will
enable the deployment of lipreading models in practical applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ma_P/0/1/0/all/0/1"&gt;Pingchuan Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martinez_B/0/1/0/all/0/1"&gt;Brais Martinez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Petridis_S/0/1/0/all/0/1"&gt;Stavros Petridis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pantic_M/0/1/0/all/0/1"&gt;Maja Pantic&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evidence-based Factual Error Correction. (arXiv:2106.01072v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01072</id>
        <link href="http://arxiv.org/abs/2106.01072"/>
        <updated>2021-06-03T02:10:36.896Z</updated>
        <summary type="html"><![CDATA[This paper introduces the task of factual error correction: performing edits
to a claim so that the generated rewrite is better supported by evidence. This
extends the well-studied task of fact verification by providing a mechanism to
correct written texts that are refuted or only partially supported by evidence.
We demonstrate that it is feasible to train factual error correction systems
from existing fact checking datasets which only contain labeled claims
accompanied by evidence, but not the correction. We achieve this by employing a
two-stage distant supervision approach that incorporates evidence into masked
claims when generating corrections. Our approach, based on the T5 transformer
and using retrieved evidence, achieved better results than existing work which
used a pointer copy network and gold evidence, producing accurate factual error
corrections for 5x more instances in human evaluation and a .125 increase in
SARI score. The evaluation is conducted on a dataset of 65,000 instances based
on a recent fact verification shared task and we release it to enable further
work on the task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Thorne_J/0/1/0/all/0/1"&gt;James Thorne&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vlachos_A/0/1/0/all/0/1"&gt;Andreas Vlachos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Active Surface Models. (arXiv:2011.08826v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.08826</id>
        <link href="http://arxiv.org/abs/2011.08826"/>
        <updated>2021-06-03T02:10:36.892Z</updated>
        <summary type="html"><![CDATA[Active Surface Models have a long history of being useful to model complex 3D
surfaces but only Active Contours have been used in conjunction with deep
networks, and then only to produce the data term as well as meta-parameter maps
controlling them. In this paper, we advocate a much tighter integration. We
introduce layers that implement them that can be integrated seamlessly into
Graph Convolutional Networks to enforce sophisticated smoothness priors at an
acceptable computational cost. We will show that the resulting Deep Active
Surface Models outperform equivalent architectures that use traditional
regularization loss terms to impose smoothness priors for 3D surface
reconstruction from 2D images and for 3D volume segmentation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wickramasinghe_U/0/1/0/all/0/1"&gt;Udaranga Wickramasinghe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Knott_G/0/1/0/all/0/1"&gt;Graham Knott&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fua_P/0/1/0/all/0/1"&gt;Pascal Fua&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Causal Discovery in Knowledge Graphs by Exploiting Asymmetric Properties of Non-Gaussian Distributions. (arXiv:2106.01043v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01043</id>
        <link href="http://arxiv.org/abs/2106.01043"/>
        <updated>2021-06-03T02:10:36.878Z</updated>
        <summary type="html"><![CDATA[In recent years, causal modelling has been used widely to improve
generalization and to provide interpretability in machine learning models. To
determine cause-effect relationships in the absence of a randomized trial, we
can model causal systems with counterfactuals and interventions given enough
domain knowledge. However, there are several cases where domain knowledge is
almost absent and the only recourse is using a statistical method to estimate
causal relationships. While there have been several works done in estimating
causal relationships in unstructured data, we are yet to find a well-defined
framework for estimating causal relationships in Knowledge Graphs (KG). It is
commonly used to provide a semantic framework for data with complex
inter-domain relationships. In this work, we define a hybrid approach that
allows us to discover cause-effect relationships in KG. The proposed approach
is based around the finding of the instantaneous causal structure of a
non-experimental matrix using a non-Gaussian model, i.e; finding the causal
ordering of the variables in a non-Gaussian setting. The non-experimental
matrix is a low-dimensional tensor projection obtained by decomposing the
adjacency tensor of a KG. We use two different pre-existing algorithms, one for
the causal discovery and the other for decomposing the KG and combining them to
get the causal structure in a KG.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Giriraj_R/0/1/0/all/0/1"&gt;Rohan Giriraj&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thomas_S/0/1/0/all/0/1"&gt;Sinnu Susan Thomas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Distance-preserving Matrix Sketch. (arXiv:2009.03979v2 [cs.HC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.03979</id>
        <link href="http://arxiv.org/abs/2009.03979"/>
        <updated>2021-06-03T02:10:36.871Z</updated>
        <summary type="html"><![CDATA[Visualizing very large matrices involves many formidable problems. Various
popular solutions to these problems involve sampling, clustering, projection,
or feature selection to reduce the size and complexity of the original task. An
important aspect of these methods is how to preserve relative distances between
points in the higher-dimensional space after reducing rows and columns to fit
in a lower dimensional space. This aspect is important because conclusions
based on faulty visual reasoning can be harmful. Judging dissimilar points as
similar or similar points as dissimilar on the basis of a visualization can
lead to false conclusions. To ameliorate this bias and to make visualizations
of very large datasets feasible, we introduce two new algorithms that
respectively select a subset of rows and columns of a rectangular matrix. This
selection is designed to preserve relative distances as closely as possible. We
compare our matrix sketch to more traditional alternatives on a variety of
artificial and real datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wilkinson_L/0/1/0/all/0/1"&gt;Leland Wilkinson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1"&gt;Hengrui Luo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improvement over Pinball Loss Support Vector Machine. (arXiv:2106.01109v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01109</id>
        <link href="http://arxiv.org/abs/2106.01109"/>
        <updated>2021-06-03T02:10:36.854Z</updated>
        <summary type="html"><![CDATA[Recently, there have been several papers that discuss the extension of the
Pinball loss Support Vector Machine (Pin-SVM) model, originally proposed by
Huang et al.,[1][2]. Pin-SVM classifier deals with the pinball loss function,
which has been defined in terms of the parameter $\tau$. The parameter $\tau$
can take values in $[ -1,1]$. The existing Pin-SVM model requires to solve the
same optimization problem for all values of $\tau$ in $[ -1,1]$. In this paper,
we improve the existing Pin-SVM model for the binary classification task. At
first, we note that there is major difficulty in Pin-SVM model (Huang et al.
[1]) for $ -1 \leq \tau < 0$. Specifically, we show that the Pin-SVM model
requires the solution of different optimization problem for $ -1 \leq \tau <
0$. We further propose a unified model termed as Unified Pin-SVM which results
in a QPP valid for all $-1\leq \tau \leq 1$ and hence more convenient to use.
The proposed Unified Pin-SVM model can obtain a significant improvement in
accuracy over the existing Pin-SVM model which has also been empirically
justified by extensive numerical experiments with real-world datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Anand_P/0/1/0/all/0/1"&gt;Pritam Anand&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rastogi_R/0/1/0/all/0/1"&gt;Reshma Rastogi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chandra_S/0/1/0/all/0/1"&gt;Suresh Chandra&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[OntoGUM: Evaluating Contextualized SOTA Coreference Resolution on 12 More Genres. (arXiv:2106.00933v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.00933</id>
        <link href="http://arxiv.org/abs/2106.00933"/>
        <updated>2021-06-03T02:10:36.845Z</updated>
        <summary type="html"><![CDATA[SOTA coreference resolution produces increasingly impressive scores on the
OntoNotes benchmark. However lack of comparable data following the same scheme
for more genres makes it difficult to evaluate generalizability to open domain
data. This paper provides a dataset and comprehensive evaluation showing that
the latest neural LM based end-to-end systems degrade very substantially out of
domain. We make an OntoNotes-like coreference dataset called OntoGUM publicly
available, converted from GUM, an English corpus covering 12 genres, using
deterministic rules, which we evaluate. Thanks to the rich syntactic and
discourse annotations in GUM, we are able to create the largest human-annotated
coreference corpus following the OntoNotes guidelines, and the first to be
evaluated for consistency with the OntoNotes scheme. Out-of-domain evaluation
across 12 genres shows nearly 15-20% degradation for both deterministic and
deep learning systems, indicating a lack of generalizability or covert
overfitting in existing coreference resolution models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yilun Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pradhan_S/0/1/0/all/0/1"&gt;Sameer Pradhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeldes_A/0/1/0/all/0/1"&gt;Amir Zeldes&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[deep21: a Deep Learning Method for 21cm Foreground Removal. (arXiv:2010.15843v2 [astro-ph.CO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.15843</id>
        <link href="http://arxiv.org/abs/2010.15843"/>
        <updated>2021-06-03T02:10:36.840Z</updated>
        <summary type="html"><![CDATA[We seek to remove foreground contaminants from 21cm intensity mapping
observations. We demonstrate that a deep convolutional neural network (CNN)
with a UNet architecture and three-dimensional convolutions, trained on
simulated observations, can effectively separate frequency and spatial patterns
of the cosmic neutral hydrogen (HI) signal from foregrounds in the presence of
noise. Cleaned maps recover cosmological clustering statistics within 10% at
all relevant angular scales and frequencies. This amounts to a reduction in
prediction variance of over an order of magnitude on small angular scales
($\ell > 300$), and improved accuracy for small radial scales ($k_{\parallel} >
0.17\ \rm h\ Mpc^{-1})$ compared to standard Principal Component Analysis (PCA)
methods. We estimate posterior confidence intervals for the network's
prediction by training an ensemble of UNets. Our approach demonstrates the
feasibility of analyzing 21cm intensity maps, as opposed to derived summary
statistics, for upcoming radio experiments, as long as the simulated foreground
model is sufficiently realistic. We provide the code used for this analysis on
Github https://github.com/tlmakinen/deep21 as well as a browser-based tutorial
for the experiment and UNet model via the accompanying
this http URL Colab notebook.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/astro-ph/1/au:+Makinen_T/0/1/0/all/0/1"&gt;T. Lucas Makinen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Lancaster_L/0/1/0/all/0/1"&gt;Lachlan Lancaster&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Villaescusa_Navarro_F/0/1/0/all/0/1"&gt;Francisco Villaescusa-Navarro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Melchior_P/0/1/0/all/0/1"&gt;Peter Melchior&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Ho_S/0/1/0/all/0/1"&gt;Shirley Ho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Perreault_Levasseur_L/0/1/0/all/0/1"&gt;Laurence Perreault-Levasseur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Spergel_D/0/1/0/all/0/1"&gt;David N. Spergel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SyGNS: A Systematic Generalization Testbed Based on Natural Language Semantics. (arXiv:2106.01077v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01077</id>
        <link href="http://arxiv.org/abs/2106.01077"/>
        <updated>2021-06-03T02:10:36.822Z</updated>
        <summary type="html"><![CDATA[Recently, deep neural networks (DNNs) have achieved great success in
semantically challenging NLP tasks, yet it remains unclear whether DNN models
can capture compositional meanings, those aspects of meaning that have been
long studied in formal semantics. To investigate this issue, we propose a
Systematic Generalization testbed based on Natural language Semantics (SyGNS),
whose challenge is to map natural language sentences to multiple forms of
scoped meaning representations, designed to account for various semantic
phenomena. Using SyGNS, we test whether neural networks can systematically
parse sentences involving novel combinations of logical expressions such as
quantifiers and negation. Experiments show that Transformer and GRU models can
generalize to unseen combinations of quantifiers, negations, and modifiers that
are similar to given training instances in form, but not to the others. We also
find that the generalization performance to unseen combinations is better when
the form of meaning representations is simpler. The data and code for SyGNS are
publicly available at https://github.com/verypluming/SyGNS.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yanaka_H/0/1/0/all/0/1"&gt;Hitomi Yanaka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mineshima_K/0/1/0/all/0/1"&gt;Koji Mineshima&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Inui_K/0/1/0/all/0/1"&gt;Kentaro Inui&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarial Robustness of Stabilized NeuralODEs Might be from Obfuscated Gradients. (arXiv:2009.13145v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.13145</id>
        <link href="http://arxiv.org/abs/2009.13145"/>
        <updated>2021-06-03T02:10:36.817Z</updated>
        <summary type="html"><![CDATA[In this paper we introduce a provably stable architecture for Neural Ordinary
Differential Equations (ODEs) which achieves non-trivial adversarial robustness
under white-box adversarial attacks even when the network is trained naturally.
For most existing defense methods withstanding strong white-box attacks, to
improve robustness of neural networks, they need to be trained adversarially,
hence have to strike a trade-off between natural accuracy and adversarial
robustness. Inspired by dynamical system theory, we design a stabilized neural
ODE network named SONet whose ODE blocks are skew-symmetric and proved to be
input-output stable. With natural training, SONet can achieve comparable
robustness with the state-of-the-art adversarial defense methods, without
sacrificing natural accuracy. Even replacing only the first layer of a ResNet
by such a ODE block can exhibit further improvement in robustness, e.g., under
PGD-20 ($\ell_\infty=0.031$) attack on CIFAR-10 dataset, it achieves 91.57\%
and natural accuracy and 62.35\% robust accuracy, while a counterpart
architecture of ResNet trained with TRADES achieves natural and robust accuracy
76.29\% and 45.24\%, respectively. To understand possible reasons behind this
surprisingly good result, we further explore the possible mechanism underlying
such an adversarial robustness. We show that the adaptive stepsize numerical
ODE solver, DOPRI5, has a gradient masking effect that fails the PGD attacks
which are sensitive to gradient information of training loss; on the other
hand, it cannot fool the CW attack of robust gradients and the SPSA attack that
is gradient-free. This provides a new explanation that the adversarial
robustness of ODE-based networks mainly comes from the obfuscated gradients in
numerical ODE solvers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yifei Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1"&gt;Yaodong Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Hongyang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1"&gt;Yi Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1"&gt;Yuan Yao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rejuvenating Low-Frequency Words: Making the Most of Parallel Data in Non-Autoregressive Translation. (arXiv:2106.00903v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.00903</id>
        <link href="http://arxiv.org/abs/2106.00903"/>
        <updated>2021-06-03T02:10:36.811Z</updated>
        <summary type="html"><![CDATA[Knowledge distillation (KD) is commonly used to construct synthetic data for
training non-autoregressive translation (NAT) models. However, there exists a
discrepancy on low-frequency words between the distilled and the original data,
leading to more errors on predicting low-frequency words. To alleviate the
problem, we directly expose the raw data into NAT by leveraging pretraining. By
analyzing directed alignments, we found that KD makes low-frequency source
words aligned with targets more deterministically but fails to align sufficient
low-frequency words from target to source. Accordingly, we propose reverse KD
to rejuvenate more alignments for low-frequency target words. To make the most
of authentic and synthetic data, we combine these complementary approaches as a
new training strategy for further boosting NAT performance. We conduct
experiments on five translation benchmarks over two advanced architectures.
Results demonstrate that the proposed approach can significantly and
universally improve translation quality by reducing translation errors on
low-frequency words. Encouragingly, our approach achieves 28.2 and 33.9 BLEU
points on the WMT14 English-German and WMT16 Romanian-English datasets,
respectively. Our code, data, and trained models are available at
\url{https://github.com/longyuewangdcu/RLFW-NAT}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1"&gt;Liang Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Longyue Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xuebo Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wong_D/0/1/0/all/0/1"&gt;Derek F. Wong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1"&gt;Dacheng Tao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1"&gt;Zhaopeng Tu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Finite-sample Analysis of Interpolating Linear Classifiers in the Overparameterized Regime. (arXiv:2004.12019v4 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.12019</id>
        <link href="http://arxiv.org/abs/2004.12019"/>
        <updated>2021-06-03T02:10:36.806Z</updated>
        <summary type="html"><![CDATA[We prove bounds on the population risk of the maximum margin algorithm for
two-class linear classification. For linearly separable training data, the
maximum margin algorithm has been shown in previous work to be equivalent to a
limit of training with logistic loss using gradient descent, as the training
error is driven to zero. We analyze this algorithm applied to random data
including misclassification noise. Our assumptions on the clean data include
the case in which the class-conditional distributions are standard normal
distributions. The misclassification noise may be chosen by an adversary,
subject to a limit on the fraction of corrupted labels. Our bounds show that,
with sufficient over-parameterization, the maximum margin algorithm trained on
noisy data can achieve nearly optimal population risk.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Chatterji_N/0/1/0/all/0/1"&gt;Niladri S. Chatterji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Long_P/0/1/0/all/0/1"&gt;Philip M. Long&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Expected Scalarised Returns Dominance: A New Solution Concept for Multi-Objective Decision Making. (arXiv:2106.01048v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01048</id>
        <link href="http://arxiv.org/abs/2106.01048"/>
        <updated>2021-06-03T02:10:36.800Z</updated>
        <summary type="html"><![CDATA[In many real-world scenarios, the utility of a user is derived from the
single execution of a policy. In this case, to apply multi-objective
reinforcement learning, the expected utility of the returns must be optimised.
Various scenarios exist where a user's preferences over objectives (also known
as the utility function) are unknown or difficult to specify. In such
scenarios, a set of optimal policies must be learned. However, settings where
the expected utility must be maximised have been largely overlooked by the
multi-objective reinforcement learning community and, as a consequence, a set
of optimal solutions has yet to be defined. In this paper we address this
challenge by proposing first-order stochastic dominance as a criterion to build
solution sets to maximise expected utility. We also propose a new dominance
criterion, known as expected scalarised returns (ESR) dominance, that extends
first-order stochastic dominance to allow a set of optimal policies to be
learned in practice. We then define a new solution concept called the ESR set,
which is a set of policies that are ESR dominant. Finally, we define a new
multi-objective distributional tabular reinforcement learning (MOT-DRL)
algorithm to learn the ESR set in a multi-objective multi-armed bandit setting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hayes_C/0/1/0/all/0/1"&gt;Conor F. Hayes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Verstraeten_T/0/1/0/all/0/1"&gt;Timothy Verstraeten&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roijers_D/0/1/0/all/0/1"&gt;Diederik M. Roijers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Howley_E/0/1/0/all/0/1"&gt;Enda Howley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mannion_P/0/1/0/all/0/1"&gt;Patrick Mannion&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Online Coreset Selection for Rehearsal-based Continual Learning. (arXiv:2106.01085v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01085</id>
        <link href="http://arxiv.org/abs/2106.01085"/>
        <updated>2021-06-03T02:10:36.787Z</updated>
        <summary type="html"><![CDATA[A dataset is a shred of crucial evidence to describe a task. However, each
data point in the dataset does not have the same potential, as some of the data
points can be more representative or informative than others. This unequal
importance among the data points may have a large impact in rehearsal-based
continual learning, where we store a subset of the training examples (coreset)
to be replayed later to alleviate catastrophic forgetting. In continual
learning, the quality of the samples stored in the coreset directly affects the
model's effectiveness and efficiency. The coreset selection problem becomes
even more important under realistic settings, such as imbalanced continual
learning or noisy data scenarios. To tackle this problem, we propose Online
Coreset Selection (OCS), a simple yet effective method that selects the most
representative and informative coreset at each iteration and trains them in an
online manner. Our proposed method maximizes the model's adaptation to a target
dataset while selecting high-affinity samples to past tasks, which directly
inhibits catastrophic forgetting. We validate the effectiveness of our coreset
selection mechanism over various standard, imbalanced, and noisy datasets
against strong continual learning baselines, demonstrating that it improves
task adaptation and prevents catastrophic forgetting in a sample-efficient
manner.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yoon_J/0/1/0/all/0/1"&gt;Jaehong Yoon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Madaan_D/0/1/0/all/0/1"&gt;Divyam Madaan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_E/0/1/0/all/0/1"&gt;Eunho Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1"&gt;Sung Ju Hwang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FIVES: Feature Interaction Via Edge Search for Large-Scale Tabular Data. (arXiv:2007.14573v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.14573</id>
        <link href="http://arxiv.org/abs/2007.14573"/>
        <updated>2021-06-03T02:10:36.783Z</updated>
        <summary type="html"><![CDATA[High-order interactive features capture the correlation between different
columns and thus are promising to enhance various learning tasks on ubiquitous
tabular data. To automate the generation of interactive features, existing
works either explicitly traverse the feature space or implicitly express the
interactions via intermediate activations of some designed models. These two
kinds of methods show that there is essentially a trade-off between feature
interpretability and search efficiency. To possess both of their merits, we
propose a novel method named Feature Interaction Via Edge Search (FIVES), which
formulates the task of interactive feature generation as searching for edges on
the defined feature graph. Specifically, we first present our theoretical
evidence that motivates us to search for useful interactive features with
increasing order. Then we instantiate this search strategy by optimizing both a
dedicated graph neural network (GNN) and the adjacency tensor associated with
the defined feature graph. In this way, the proposed FIVES method simplifies
the time-consuming traversal as a typical training course of GNN and enables
explicit feature generation according to the learned adjacency tensor.
Experimental results on both benchmark and real-world datasets show the
advantages of FIVES over several state-of-the-art methods. Moreover, the
interactive features identified by FIVES are deployed on the recommender system
of Taobao, a worldwide leading e-commerce platform. Results of an online A/B
testing further verify the effectiveness of the proposed method FIVES, and we
further provide FIVES as AI utilities for the customers of Alibaba Cloud.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1"&gt;Yuexiang Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yaliang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_B/0/1/0/all/0/1"&gt;Bolin Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gurel_N/0/1/0/all/0/1"&gt;Nezihe Merve G&amp;#xfc;rel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Ce Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1"&gt;Minlie Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1"&gt;Wei Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jingren Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CoRI: Collective Relation Integration with Data Augmentation for Open Information Extraction. (arXiv:2106.00793v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.00793</id>
        <link href="http://arxiv.org/abs/2106.00793"/>
        <updated>2021-06-03T02:10:36.777Z</updated>
        <summary type="html"><![CDATA[Integrating extracted knowledge from the Web to knowledge graphs (KGs) can
facilitate tasks like question answering. We study relation integration that
aims to align free-text relations in subject-relation-object extractions to
relations in a target KG. To address the challenge that free-text relations are
ambiguous, previous methods exploit neighbor entities and relations for
additional context. However, the predictions are made independently, which can
be mutually inconsistent. We propose a two-stage Collective Relation
Integration (CoRI) model, where the first stage independently makes candidate
predictions, and the second stage employs a collective model that accesses all
candidate predictions to make globally coherent predictions. We further improve
the collective model with augmented data from the portion of the target KG that
is otherwise unused. Experiment results on two datasets show that CoRI can
significantly outperform the baselines, improving AUC from .677 to .748 and
from .716 to .780, respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1"&gt;Zhengbao Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1"&gt;Jialong Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sisman_B/0/1/0/all/0/1"&gt;Bunyamin Sisman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1"&gt;Xin Luna Dong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generating SOAP Notes from Doctor-Patient Conversations Using Modular Summarization Techniques. (arXiv:2005.01795v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.01795</id>
        <link href="http://arxiv.org/abs/2005.01795"/>
        <updated>2021-06-03T02:10:36.771Z</updated>
        <summary type="html"><![CDATA[Following each patient visit, physicians draft long semi-structured clinical
summaries called SOAP notes. While invaluable to clinicians and researchers,
creating digital SOAP notes is burdensome, contributing to physician burnout.
In this paper, we introduce the first complete pipelines to leverage deep
summarization models to generate these notes based on transcripts of
conversations between physicians and patients. After exploring a spectrum of
methods across the extractive-abstractive spectrum, we propose Cluster2Sent, an
algorithm that (i) extracts important utterances relevant to each summary
section; (ii) clusters together related utterances; and then (iii) generates
one summary sentence per cluster. Cluster2Sent outperforms its purely
abstractive counterpart by 8 ROUGE-1 points, and produces significantly more
factual and coherent sentences as assessed by expert human evaluators. For
reproducibility, we demonstrate similar benefits on the publicly available AMI
dataset. Our results speak to the benefits of structuring summaries into
sections and annotating supporting evidence when constructing summarization
corpora.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Krishna_K/0/1/0/all/0/1"&gt;Kundan Krishna&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khosla_S/0/1/0/all/0/1"&gt;Sopan Khosla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bigham_J/0/1/0/all/0/1"&gt;Jeffrey P. Bigham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lipton_Z/0/1/0/all/0/1"&gt;Zachary C. Lipton&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluation Metrics for Graph Generative Models: Problems, Pitfalls, and Practical Solutions. (arXiv:2106.01098v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01098</id>
        <link href="http://arxiv.org/abs/2106.01098"/>
        <updated>2021-06-03T02:10:36.766Z</updated>
        <summary type="html"><![CDATA[Graph generative models are a highly active branch of machine learning. Given
the steady development of new models of ever-increasing complexity, it is
necessary to provide a principled way to evaluate and compare them. In this
paper, we enumerate the desirable criteria for comparison metrics, discuss the
development of such metrics, and provide a comparison of their respective
expressive power. We perform a systematic evaluation of the main metrics in use
today, highlighting some of the challenges and pitfalls researchers
inadvertently can run into. We then describe a collection of suitable metrics,
give recommendations as to their practical suitability, and analyse their
behaviour on synthetically generated perturbed graphs as well as on recently
proposed graph generative models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+OBray_L/0/1/0/all/0/1"&gt;Leslie O&amp;#x27;Bray&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Horn_M/0/1/0/all/0/1"&gt;Max Horn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rieck_B/0/1/0/all/0/1"&gt;Bastian Rieck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Borgwardt_K/0/1/0/all/0/1"&gt;Karsten Borgwardt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Testing Group Fairness via Optimal Transport Projections. (arXiv:2106.01070v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.01070</id>
        <link href="http://arxiv.org/abs/2106.01070"/>
        <updated>2021-06-03T02:10:36.754Z</updated>
        <summary type="html"><![CDATA[We present a statistical testing framework to detect if a given machine
learning classifier fails to satisfy a wide range of group fairness notions.
The proposed test is a flexible, interpretable, and statistically rigorous tool
for auditing whether exhibited biases are intrinsic to the algorithm or due to
the randomness in the data. The statistical challenges, which may arise from
multiple impact criteria that define group fairness and which are discontinuous
on model parameters, are conveniently tackled by projecting the empirical
measure onto the set of group-fair probability models using optimal transport.
This statistic is efficiently computed using linear programming and its
asymptotic distribution is explicitly obtained. The proposed framework can also
be used to test for testing composite fairness hypotheses and fairness with
multiple sensitive attributes. The optimal transport testing formulation
improves interpretability by characterizing the minimal covariate perturbations
that eliminate the bias observed in the audit.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Si_N/0/1/0/all/0/1"&gt;Nian Si&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Murthy_K/0/1/0/all/0/1"&gt;Karthyek Murthy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Blanchet_J/0/1/0/all/0/1"&gt;Jose Blanchet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Nguyen_V/0/1/0/all/0/1"&gt;Viet Anh Nguyen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Needle in a Haystack: Label-Efficient Evaluation under Extreme Class Imbalance. (arXiv:2006.06963v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.06963</id>
        <link href="http://arxiv.org/abs/2006.06963"/>
        <updated>2021-06-03T02:10:36.748Z</updated>
        <summary type="html"><![CDATA[Important tasks like record linkage and extreme classification demonstrate
extreme class imbalance, with 1 minority instance to every 1 million or more
majority instances. Obtaining a sufficient sample of all classes, even just to
achieve statistically-significant evaluation, is so challenging that most
current approaches yield poor estimates or incur impractical cost. Where
importance sampling has been levied against this challenge, restrictive
constraints are placed on performance metrics, estimates do not come with
appropriate guarantees, or evaluations cannot adapt to incoming labels. This
paper develops a framework for online evaluation based on adaptive importance
sampling. Given a target performance metric and model for $p(y|x)$, the
framework adapts a distribution over items to label in order to maximize
statistical precision. We establish strong consistency and a central limit
theorem for the resulting performance estimates, and instantiate our framework
with worked examples that leverage Dirichlet-tree models. Experiments
demonstrate an average MSE superior to state-of-the-art on fixed label budgets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Marchant_N/0/1/0/all/0/1"&gt;Neil G. Marchant&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rubinstein_B/0/1/0/all/0/1"&gt;Benjamin I. P. Rubinstein&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Style is NOT a single variable: Case Studies for Cross-Style Language Understanding. (arXiv:1911.03663v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1911.03663</id>
        <link href="http://arxiv.org/abs/1911.03663"/>
        <updated>2021-06-03T02:10:36.743Z</updated>
        <summary type="html"><![CDATA[Every natural text is written in some style. Style is formed by a complex
combination of different stylistic factors, including formality markers,
emotions, metaphors, etc. One cannot form a complete understanding of a text
without considering these factors. The factors combine and co-vary in complex
ways to form styles. Studying the nature of the co-varying combinations sheds
light on stylistic language in general, sometimes called cross-style language
understanding. This paper provides the benchmark corpus (xSLUE) that combines
existing datasets and collects a new one for sentence-level cross-style
language understanding and evaluation. The benchmark contains text in 15
different styles under the proposed four theoretical groupings: figurative,
personal, affective, and interpersonal groups. For valid evaluation, we collect
an additional diagnostic set by annotating all 15 styles on the same text.
Using xSLUE, we propose three interesting cross-style applications in
classification, correlation, and generation. First, our proposed cross-style
classifier trained with multiple styles together helps improve overall
classification performance against individually-trained style classifiers.
Second, our study shows that some styles are highly dependent on each other in
human-written text. Finally, we find that combinations of some contradictive
styles likely generate stylistically less appropriate text. We believe our
benchmark and case studies help explore interesting future directions for
cross-style research. The preprocessed datasets and code are publicly
available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kang_D/0/1/0/all/0/1"&gt;Dongyeop Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hovy_E/0/1/0/all/0/1"&gt;Eduard Hovy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Rehearse in Long Sequence Memorization. (arXiv:2106.01096v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01096</id>
        <link href="http://arxiv.org/abs/2106.01096"/>
        <updated>2021-06-03T02:10:36.737Z</updated>
        <summary type="html"><![CDATA[Existing reasoning tasks often have an important assumption that the input
contents can be always accessed while reasoning, requiring unlimited storage
resources and suffering from severe time delay on long sequences. To achieve
efficient reasoning on long sequences with limited storage resources, memory
augmented neural networks introduce a human-like write-read memory to compress
and memorize the long input sequence in one pass, trying to answer subsequent
queries only based on the memory. But they have two serious drawbacks: 1) they
continually update the memory from current information and inevitably forget
the early contents; 2) they do not distinguish what information is important
and treat all contents equally. In this paper, we propose the Rehearsal Memory
(RM) to enhance long-sequence memorization by self-supervised rehearsal with a
history sampler. To alleviate the gradual forgetting of early information, we
design self-supervised rehearsal training with recollection and familiarity
tasks. Further, we design a history sampler to select informative fragments for
rehearsal training, making the memory focus on the crucial information. We
evaluate the performance of our rehearsal memory by the synthetic bAbI task and
several downstream tasks, including text/video question answering and
recommendation on long sequences.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1"&gt;Chang Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1"&gt;Jianxin Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1"&gt;Zhijie Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jingren Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Hongxia Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1"&gt;Zhou Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Statistical optimality conditions for compressive ensembles. (arXiv:2106.01092v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01092</id>
        <link href="http://arxiv.org/abs/2106.01092"/>
        <updated>2021-06-03T02:10:36.732Z</updated>
        <summary type="html"><![CDATA[We present a framework for the theoretical analysis of ensembles of
low-complexity empirical risk minimisers trained on independent random
compressions of high-dimensional data. First we introduce a general
distribution-dependent upper-bound on the excess risk, framed in terms of a
natural notion of compressibility. This bound is independent of the dimension
of the original data representation, and explains the in-built regularisation
effect of the compressive approach. We then instantiate this general bound to
classification and regression tasks, considering Johnson-Lindenstrauss mappings
as the compression scheme. For each of these tasks, our strategy is to develop
a tight upper bound on the compressibility function, and by doing so we
discover distributional conditions of geometric nature under which the
compressive algorithm attains minimax-optimal rates up to at most
poly-logarithmic factors. In the case of compressive classification, this is
achieved with a mild geometric margin condition along with a flexible moment
condition that is significantly more general than the assumption of bounded
domain. In the case of regression with strongly convex smooth loss functions we
find that compressive regression is capable of exploiting spectral decay with
near-optimal guarantees. In addition, a key ingredient for our central upper
bound is a high probability uniform upper bound on the integrated deviation of
dependent empirical processes, which may be of independent interest.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Reeve_H/0/1/0/all/0/1"&gt;Henry W. J. Reeve&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kaban_A/0/1/0/all/0/1"&gt;Ata Kaban&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Counterfactual Explanation with Multi-Agent Reinforcement Learning for Drug Target Prediction. (arXiv:2103.12983v2 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.12983</id>
        <link href="http://arxiv.org/abs/2103.12983"/>
        <updated>2021-06-03T02:10:36.719Z</updated>
        <summary type="html"><![CDATA[Motivation: Many high-performance DTA models have been proposed, but they are
mostly black-box and thus lack human interpretability. Explainable AI (XAI) can
make DTA models more trustworthy, and can also enable scientists to distill
biological knowledge from the models. Counterfactual explanation is one popular
approach to explaining the behaviour of a deep neural network, which works by
systematically answering the question "How would the model output change if the
inputs were changed in this way?". Most counterfactual explanation methods only
operate on single input data. It remains an open problem how to extend
counterfactual-based XAI methods to DTA models, which have two inputs, one for
drug and one for target, that also happen to be discrete in nature.

Methods: We propose a multi-agent reinforcement learning framework,
Multi-Agent Counterfactual Drug target binding Affinity (MACDA), to generate
counterfactual explanations for the drug-protein complex. Our proposed
framework provides human-interpretable counterfactual instances while
optimizing both the input drug and target for counterfactual generation at the
same time.

Results: We benchmark the proposed MACDA framework using the Davis dataset
and find that our framework produces more parsimonious explanations with no
loss in explanation validity, as measured by encoding similarity and QED. We
then present a case study involving ABL1 and Nilotinib to demonstrate how MACDA
can explain the behaviour of a DTA model in the underlying substructure
interaction between inputs in its prediction, revealing mechanisms that align
with prior domain knowledge.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1"&gt;Tri Minh Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Quinn_T/0/1/0/all/0/1"&gt;Thomas P Quinn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1"&gt;Thin Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1"&gt;Truyen Tran&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[More Embeddings, Better Sequence Labelers?. (arXiv:2009.08330v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.08330</id>
        <link href="http://arxiv.org/abs/2009.08330"/>
        <updated>2021-06-03T02:10:36.713Z</updated>
        <summary type="html"><![CDATA[Recent work proposes a family of contextual embeddings that significantly
improves the accuracy of sequence labelers over non-contextual embeddings.
However, there is no definite conclusion on whether we can build better
sequence labelers by combining different kinds of embeddings in various
settings. In this paper, we conduct extensive experiments on 3 tasks over 18
datasets and 8 languages to study the accuracy of sequence labeling with
various embedding concatenations and make three observations: (1) concatenating
more embedding variants leads to better accuracy in rich-resource and
cross-domain settings and some conditions of low-resource settings; (2)
concatenating additional contextual sub-word embeddings with contextual
character embeddings hurts the accuracy in extremely low-resource settings; (3)
based on the conclusion of (1), concatenating additional similar contextual
embeddings cannot lead to further improvements. We hope these conclusions can
help people build stronger sequence labelers in various settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xinyu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1"&gt;Yong Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bach_N/0/1/0/all/0/1"&gt;Nguyen Bach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1"&gt;Tao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zhongqiang Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1"&gt;Fei Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tu_K/0/1/0/all/0/1"&gt;Kewei Tu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Frequency Estimation in Data Streams: Learning the Optimal Hashing Scheme. (arXiv:2007.09261v2 [cs.DS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.09261</id>
        <link href="http://arxiv.org/abs/2007.09261"/>
        <updated>2021-06-03T02:10:36.708Z</updated>
        <summary type="html"><![CDATA[We present a novel approach for the problem of frequency estimation in data
streams that is based on optimization and machine learning. Contrary to
state-of-the-art streaming frequency estimation algorithms, which heavily rely
on random hashing to maintain the frequency distribution of the data steam
using limited storage, the proposed approach exploits an observed stream prefix
to near-optimally hash elements and compress the target frequency distribution.
We develop an exact mixed-integer linear optimization formulation, which
enables us to compute optimal or near-optimal hashing schemes for elements seen
in the observed stream prefix; then, we use machine learning to hash unseen
elements. Further, we develop an efficient block coordinate descent algorithm,
which, as we empirically show, produces high quality solutions, and, in a
special case, we are able to solve the proposed formulation exactly in linear
time using dynamic programming. We empirically evaluate the proposed approach
both on synthetic datasets and on real-world search query data. We show that
the proposed approach outperforms existing approaches by one to two orders of
magnitude in terms of its average (per element) estimation error and by 45-90%
in terms of its expected magnitude of estimation error.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bertsimas_D/0/1/0/all/0/1"&gt;Dimitris Bertsimas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Digalakis_V/0/1/0/all/0/1"&gt;Vassilis Digalakis Jr&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Why is Attention Not So Interpretable?. (arXiv:2006.05656v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.05656</id>
        <link href="http://arxiv.org/abs/2006.05656"/>
        <updated>2021-06-03T02:10:36.701Z</updated>
        <summary type="html"><![CDATA[Attention-based methods have played important roles in model interpretations,
where the calculated attention weights are expected to highlight the critical
parts of inputs~(e.g., keywords in sentences). However, recent research found
that attention-as-importance interpretations often do not work as we expected.
For example, learned attention weights sometimes highlight less meaningful
tokens like "[SEP]", ",", and ".", and are frequently uncorrelated with other
feature importance indicators like gradient-based measures. A recent debate
over whether attention is an explanation or not has drawn considerable
interest. In this paper, we demonstrate that one root cause of this phenomenon
is the combinatorial shortcuts, which means that, in addition to the
highlighted parts, the attention weights themselves may carry extra information
that could be utilized by downstream models after attention layers. As a
result, the attention weights are no longer pure importance indicators. We
theoretically analyze combinatorial shortcuts, design one intuitive experiment
to show their existence, and propose two methods to mitigate this issue. We
conduct empirical studies on attention-based interpretation models. The results
show that the proposed methods can effectively improve the interpretability of
attention mechanisms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Bai_B/0/1/0/all/0/1"&gt;Bing Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Liang_J/0/1/0/all/0/1"&gt;Jian Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zhang_G/0/1/0/all/0/1"&gt;Guanhua Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Li_H/0/1/0/all/0/1"&gt;Hao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Bai_K/0/1/0/all/0/1"&gt;Kun Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wang_F/0/1/0/all/0/1"&gt;Fei Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluating Word Embeddings with Categorical Modularity. (arXiv:2106.00877v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.00877</id>
        <link href="http://arxiv.org/abs/2106.00877"/>
        <updated>2021-06-03T02:10:36.696Z</updated>
        <summary type="html"><![CDATA[We introduce categorical modularity, a novel low-resource intrinsic metric to
evaluate word embedding quality. Categorical modularity is a graph modularity
metric based on the $k$-nearest neighbor graph constructed with embedding
vectors of words from a fixed set of semantic categories, in which the goal is
to measure the proportion of words that have nearest neighbors within the same
categories. We use a core set of 500 words belonging to 59 neurobiologically
motivated semantic categories in 29 languages and analyze three word embedding
models per language (FastText, MUSE, and subs2vec). We find moderate to strong
positive correlations between categorical modularity and performance on the
monolingual tasks of sentiment analysis and word similarity calculation and on
the cross-lingual task of bilingual lexicon induction both to and from English.
Overall, we suggest that categorical modularity provides non-trivial predictive
information about downstream task performance, with breakdowns of correlations
by model suggesting some meta-predictive properties about semantic information
loss as well.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Casacuberta_S/0/1/0/all/0/1"&gt;S&amp;#xed;lvia Casacuberta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Halevy_K/0/1/0/all/0/1"&gt;Karina Halevy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Blasi_D/0/1/0/all/0/1"&gt;Dami&amp;#xe1;n E. Blasi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Detecting Rewards Deterioration in Episodic Reinforcement Learning. (arXiv:2010.11660v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.11660</id>
        <link href="http://arxiv.org/abs/2010.11660"/>
        <updated>2021-06-03T02:10:36.690Z</updated>
        <summary type="html"><![CDATA[In many RL applications, once training ends, it is vital to detect any
deterioration in the agent performance as soon as possible. Furthermore, it
often has to be done without modifying the policy and under minimal assumptions
regarding the environment. In this paper, we address this problem by focusing
directly on the rewards and testing for degradation. We consider an episodic
framework, where the rewards within each episode are not independent, nor
identically-distributed, nor Markov. We present this problem as a multivariate
mean-shift detection problem with possibly partial observations. We define the
mean-shift in a way corresponding to deterioration of a temporal signal (such
as the rewards), and derive a test for this problem with optimal statistical
power. Empirically, on deteriorated rewards in control problems (generated
using various environment modifications), the test is demonstrated to be more
powerful than standard tests - often by orders of magnitude. We also suggest a
novel Bootstrap mechanism for False Alarm Rate control (BFAR), applicable to
episodic (non-i.i.d) signal and allowing our test to run sequentially in an
online manner. Our method does not rely on a learned model of the environment,
is entirely external to the agent, and in fact can be applied to detect changes
or drifts in any episodic signal.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Greenberg_I/0/1/0/all/0/1"&gt;Ido Greenberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mannor_S/0/1/0/all/0/1"&gt;Shie Mannor&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unbiased Gradient Estimation for Variational Auto-Encoders using Coupled Markov Chains. (arXiv:2010.01845v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.01845</id>
        <link href="http://arxiv.org/abs/2010.01845"/>
        <updated>2021-06-03T02:10:36.675Z</updated>
        <summary type="html"><![CDATA[The variational auto-encoder (VAE) is a deep latent variable model that has
two neural networks in an autoencoder-like architecture; one of them
parameterizes the model's likelihood. Fitting its parameters via maximum
likelihood (ML) is challenging since the computation of the marginal likelihood
involves an intractable integral over the latent space; thus the VAE is trained
instead by maximizing a variational lower bound. Here, we develop a ML training
scheme for VAEs by introducing unbiased estimators of the log-likelihood
gradient. We obtain the estimators by augmenting the latent space with a set of
importance samples, similarly to the importance weighted auto-encoder (IWAE),
and then constructing a Markov chain Monte Carlo coupling procedure on this
augmented space. We provide the conditions under which the estimators can be
computed in finite time and with finite variance. We show experimentally that
VAEs fitted with unbiased estimators exhibit better predictive performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ruiz_F/0/1/0/all/0/1"&gt;Francisco J. R. Ruiz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Titsias_M/0/1/0/all/0/1"&gt;Michalis K. Titsias&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cemgil_T/0/1/0/all/0/1"&gt;Taylan Cemgil&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Doucet_A/0/1/0/all/0/1"&gt;Arnaud Doucet&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Balancing Biases and Preserving Privacy on Balanced Faces in the Wild. (arXiv:2103.09118v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.09118</id>
        <link href="http://arxiv.org/abs/2103.09118"/>
        <updated>2021-06-03T02:10:36.664Z</updated>
        <summary type="html"><![CDATA[There are demographic biases in current models used for facial recognition
(FR). Our Balanced Faces In the Wild (BFW) dataset serves as a proxy to measure
bias across ethnicity and gender subgroups, allowing one to characterize FR
performances per subgroup. We show performances are non-optimal when a single
score threshold is used to determine whether sample pairs are genuine or
imposter. Across subgroups, performance ratings vary from the reported across
the entire dataset. Thus, claims of specific error rates only hold true for
populations matching that of the validation data. We mitigate the imbalanced
performances using a novel domain adaptation learning scheme on the facial
features extracted using state-of-the-art. Not only does this technique balance
performance, but it also boosts the overall performance. A benefit of the
proposed is to preserve identity information in facial features while removing
demographic knowledge in the lower dimensional features. The removal of
demographic knowledge prevents future potential biases from being injected into
decision-making. This removal satisfies privacy concerns. We explore why this
works qualitatively; we also show quantitatively that subgroup classifiers can
no longer learn from the features mapped by the proposed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Robinson_J/0/1/0/all/0/1"&gt;Joseph P Robinson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_C/0/1/0/all/0/1"&gt;Can Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Henon_Y/0/1/0/all/0/1"&gt;Yann Henon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Timoner_S/0/1/0/all/0/1"&gt;Samson Timoner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1"&gt;Yun Fu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[John praised Mary because he? Implicit Causality Bias and Its Interaction with Explicit Cues in LMs. (arXiv:2106.01060v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01060</id>
        <link href="http://arxiv.org/abs/2106.01060"/>
        <updated>2021-06-03T02:10:36.656Z</updated>
        <summary type="html"><![CDATA[Some interpersonal verbs can implicitly attribute causality to either their
subject or their object and are therefore said to carry an implicit causality
(IC) bias. Through this bias, causal links can be inferred from a narrative,
aiding language comprehension. We investigate whether pre-trained language
models (PLMs) encode IC bias and use it at inference time. We find that to be
the case, albeit to different degrees, for three distinct PLM architectures.
However, causes do not always need to be implicit -- when a cause is explicitly
stated in a subordinate clause, an incongruent IC bias associated with the verb
in the main clause leads to a delay in human processing. We hypothesize that
the temporary challenge humans face in integrating the two contradicting
signals, one from the lexical semantics of the verb, one from the
sentence-level semantics, would be reflected in higher error rates for models
on tasks dependent on causal links. The results of our study lend support to
this hypothesis, suggesting that PLMs tend to prioritize lexical patterns over
higher-order signals.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kementchedjhieva_Y/0/1/0/all/0/1"&gt;Yova Kementchedjhieva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anderson_M/0/1/0/all/0/1"&gt;Mark Anderson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sogaard_A/0/1/0/all/0/1"&gt;Anders S&amp;#xf8;gaard&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A systematic review of Hate Speech automatic detection using Natural Language Processing. (arXiv:2106.00742v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.00742</id>
        <link href="http://arxiv.org/abs/2106.00742"/>
        <updated>2021-06-03T02:10:36.609Z</updated>
        <summary type="html"><![CDATA[With the multiplication of social media platforms, which offer anonymity,
easy access and online community formation, and online debate, the issue of
hate speech detection and tracking becomes a growing challenge to society,
individual, policy-makers and researchers. Despite efforts for leveraging
automatic techniques for automatic detection and monitoring, their performances
are still far from satisfactory, which constantly calls for future research on
the issue. This paper provides a systematic review of literature in this field,
with a focus on natural language processing and deep learning technologies,
highlighting the terminology, processing pipeline, core methods employed, with
a focal point on deep learning architecture. From a methodological perspective,
we adopt PRISMA guideline of systematic review of the last 10 years literature
from ACM Digital Library and Google Scholar. In the sequel, existing surveys,
limitations, and future research directions are extensively discussed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jahan_M/0/1/0/all/0/1"&gt;Md Saroar Jahan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oussalah_M/0/1/0/all/0/1"&gt;Mourad Oussalah&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Enriching Transformers with Structured Tensor-Product Representations for Abstractive Summarization. (arXiv:2106.01317v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01317</id>
        <link href="http://arxiv.org/abs/2106.01317"/>
        <updated>2021-06-03T02:10:36.593Z</updated>
        <summary type="html"><![CDATA[Abstractive summarization, the task of generating a concise summary of input
documents, requires: (1) reasoning over the source document to determine the
salient pieces of information scattered across the long document, and (2)
composing a cohesive text by reconstructing these salient facts into a shorter
summary that faithfully reflects the complex relations connecting these facts.
In this paper, we adapt TP-TRANSFORMER (Schlag et al., 2019), an architecture
that enriches the original Transformer (Vaswani et al., 2017) with the
explicitly compositional Tensor Product Representation (TPR), for the task of
abstractive summarization. The key feature of our model is a structural bias
that we introduce by encoding two separate representations for each token to
represent the syntactic structure (with role vectors) and semantic content
(with filler vectors) separately. The model then binds the role and filler
vectors into the TPR as the layer output. We argue that the structured
intermediate representations enable the model to take better control of the
contents (salient facts) and structures (the syntax that connects the facts)
when generating the summary. Empirically, we show that our TP-TRANSFORMER
outperforms the Transformer and the original TP-TRANSFORMER significantly on
several abstractive summarization datasets based on both automatic and human
evaluations. On several syntactic and semantic probing tasks, we demonstrate
the emergent structural information in the role vectors and improved syntactic
interpretability in the TPR layer outputs. Code and models are available at
https://github.com/jiangycTarheel/TPT-Summ.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1"&gt;Yichen Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Celikyilmaz_A/0/1/0/all/0/1"&gt;Asli Celikyilmaz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smolensky_P/0/1/0/all/0/1"&gt;Paul Smolensky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soulos_P/0/1/0/all/0/1"&gt;Paul Soulos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rao_S/0/1/0/all/0/1"&gt;Sudha Rao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Palangi_H/0/1/0/all/0/1"&gt;Hamid Palangi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fernandez_R/0/1/0/all/0/1"&gt;Roland Fernandez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smith_C/0/1/0/all/0/1"&gt;Caitlin Smith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1"&gt;Mohit Bansal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1"&gt;Jianfeng Gao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Higher-order Derivatives of Weighted Finite-state Machines. (arXiv:2106.00749v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.00749</id>
        <link href="http://arxiv.org/abs/2106.00749"/>
        <updated>2021-06-03T02:10:36.584Z</updated>
        <summary type="html"><![CDATA[Weighted finite-state machines are a fundamental building block of NLP
systems. They have withstood the test of time -- from their early use in noisy
channel models in the 1990s up to modern-day neurally parameterized conditional
random fields. This work examines the computation of higher-order derivatives
with respect to the normalization constant for weighted finite-state machines.
We provide a general algorithm for evaluating derivatives of all orders, which
has not been previously described in the literature. In the case of
second-order derivatives, our scheme runs in the optimal $\mathcal{O}(A^2 N^4)$
time where $A$ is the alphabet size and $N$ is the number of states. Our
algorithm is significantly faster than prior algorithms. Additionally, our
approach leads to a significantly faster algorithm for computing second-order
expectations, such as covariance matrices and gradients of first-order
expectations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zmigrod_R/0/1/0/all/0/1"&gt;Ran Zmigrod&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vieira_T/0/1/0/all/0/1"&gt;Tim Vieira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1"&gt;Ryan Cotterell&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Topological Feature Vectors for Chatter Detection in Turning Processes. (arXiv:1905.08671v3 [eess.SP] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1905.08671</id>
        <link href="http://arxiv.org/abs/1905.08671"/>
        <updated>2021-06-03T02:10:36.569Z</updated>
        <summary type="html"><![CDATA[Machining processes are most accurately described using complex dynamical
systems that include nonlinearities, time delays, and stochastic effects. Due
to the nature of these models as well as the practical challenges which include
time-varying parameters, the transition from numerical/analytical modeling of
machining to the analysis of real cutting signals remains challenging. Some
studies have focused on studying the time series of cutting processes using
machine learning algorithms with the goal of identifying and predicting
undesirable vibrations during machining referred to as chatter. These tools
typically decompose the signal using Wavelet Packet Transforms (WPT) or
Ensemble Empirical Mode Decomposition (EEMD). However, these methods require a
significant overhead in identifying the feature vectors before a classifier can
be trained. In this study, we present an alternative approach based on
featurizing the time series of the cutting process using its topological
features. We first embed the time series as a point cloud using Takens
embedding. We then utilize Support Vector Machine, Logistic Regression, Random
Forest and Gradient Boosting classifier combined with feature vectors derived
from persistence diagrams, a tool from persistent homology, to encode chatter's
distinguishing characteristics. We present the results for several choices of
the topological feature vectors, and we compare our results to the WPT and EEMD
methods using experimental turning data. Our results show that in two out of
four cutting configurations the TDA-based features yield accuracies as high as
97%. We also show that combining Bezier curve approximation method and parallel
computing can reduce runtime for persistence diagram computation of a single
time series to less than a second thus making our approach suitable for online
chatter detection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Yesilli_M/0/1/0/all/0/1"&gt;Melih C. Yesilli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Khasawneh_F/0/1/0/all/0/1"&gt;Firas A. Khasawneh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Otto_A/0/1/0/all/0/1"&gt;Andreas Otto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Depth Separations in Neural Networks: What is Actually Being Separated?. (arXiv:1904.06984v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1904.06984</id>
        <link href="http://arxiv.org/abs/1904.06984"/>
        <updated>2021-06-03T02:10:36.562Z</updated>
        <summary type="html"><![CDATA[Existing depth separation results for constant-depth networks essentially
show that certain radial functions in $\mathbb{R}^d$, which can be easily
approximated with depth $3$ networks, cannot be approximated by depth $2$
networks, even up to constant accuracy, unless their size is exponential in
$d$. However, the functions used to demonstrate this are rapidly oscillating,
with a Lipschitz parameter scaling polynomially with the dimension $d$ (or
equivalently, by scaling the function, the hardness result applies to
$\mathcal{O}(1)$-Lipschitz functions only when the target accuracy $\epsilon$
is at most $\text{poly}(1/d)$). In this paper, we study whether such depth
separations might still hold in the natural setting of
$\mathcal{O}(1)$-Lipschitz radial functions, when $\epsilon$ does not scale
with $d$. Perhaps surprisingly, we show that the answer is negative: In
contrast to the intuition suggested by previous work, it \emph{is} possible to
approximate $\mathcal{O}(1)$-Lipschitz radial functions with depth $2$, size
$\text{poly}(d)$ networks, for every constant $\epsilon$. We complement it by
showing that approximating such functions is also possible with depth $2$, size
$\text{poly}(1/\epsilon)$ networks, for every constant $d$. Finally, we show
that it is not possible to have polynomial dependence in both $d,1/\epsilon$
simultaneously. Overall, our results indicate that in order to show depth
separations for expressing $\mathcal{O}(1)$-Lipschitz functions with constant
accuracy -- if at all possible -- one would need fundamentally different
techniques than existing ones in the literature.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Safran_I/0/1/0/all/0/1"&gt;Itay Safran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eldan_R/0/1/0/all/0/1"&gt;Ronen Eldan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shamir_O/0/1/0/all/0/1"&gt;Ohad Shamir&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NeRP: Neural Rearrangement Planning for Unknown Objects. (arXiv:2106.01352v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2106.01352</id>
        <link href="http://arxiv.org/abs/2106.01352"/>
        <updated>2021-06-03T02:10:36.536Z</updated>
        <summary type="html"><![CDATA[Robots will be expected to manipulate a wide variety of objects in complex
and arbitrary ways as they become more widely used in human environments. As
such, the rearrangement of objects has been noted to be an important benchmark
for AI capabilities in recent years. We propose NeRP (Neural Rearrangement
Planning), a deep learning based approach for multi-step neural object
rearrangement planning which works with never-before-seen objects, that is
trained on simulation data, and generalizes to the real world. We compare NeRP
to several naive and model-based baselines, demonstrating that our approach is
measurably better and can efficiently arrange unseen objects in fewer steps and
with less planning time. Finally, we demonstrate it on several challenging
rearrangement problems in the real world.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qureshi_A/0/1/0/all/0/1"&gt;Ahmed H. Qureshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mousavian_A/0/1/0/all/0/1"&gt;Arsalan Mousavian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paxton_C/0/1/0/all/0/1"&gt;Chris Paxton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yip_M/0/1/0/all/0/1"&gt;Michael C. Yip&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fox_D/0/1/0/all/0/1"&gt;Dieter Fox&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fair Principal Component Analysis and Filter Design. (arXiv:2002.06557v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.06557</id>
        <link href="http://arxiv.org/abs/2002.06557"/>
        <updated>2021-06-03T02:10:36.531Z</updated>
        <summary type="html"><![CDATA[We consider Fair Principal Component Analysis (FPCA) and search for a low
dimensional subspace that spans multiple target vectors in a fair manner. FPCA
is defined as a non-concave maximization of the worst projected target norm
within a given set. The problem arises in filter design in signal processing,
and when incorporating fairness into dimensionality reduction schemes. The
state of the art approach to FPCA is via semidefinite relaxation and involves a
polynomial yet computationally expensive optimization. To allow scalability, we
propose to address FPCA using naive sub-gradient descent. We analyze the
landscape of the underlying optimization in the case of orthogonal targets. We
prove that the landscape is benign and that all local minima are globally
optimal. Interestingly, the SDR approach leads to sub-optimal solutions in this
simple case. Finally, we discuss the equivalence between orthogonal FPCA and
the design of normalized tight frames.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zalcberg_G/0/1/0/all/0/1"&gt;Gad Zalcberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wiesel_A/0/1/0/all/0/1"&gt;Ami Wiesel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interpretable Biomanufacturing Process Risk and Sensitivity Analyses for Quality-by-Design and Stability Control. (arXiv:1909.04261v4 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1909.04261</id>
        <link href="http://arxiv.org/abs/1909.04261"/>
        <updated>2021-06-03T02:10:36.467Z</updated>
        <summary type="html"><![CDATA[While biomanufacturing plays a significant role in supporting the economy and
ensuring public health, it faces critical challenges, including complexity,
high variability, lengthy lead time, and very limited process data, especially
for personalized new cell and gene biotherapeutics. Driven by these challenges,
we propose an interpretable semantic bioprocess probabilistic knowledge graph
and develop a game theory based risk and sensitivity analyses for production
process to facilitate quality-by-design and stability control. Specifically, by
exploring the causal relationships and interactions of critical process
parameters and quality attributes (CPPs/CQAs), we create a Bayesian network
based probabilistic knowledge graph characterizing the complex causal
interdependencies of all factors. Then, we introduce a Shapley value based
sensitivity analysis, which can correctly quantify the variation contribution
from each input factor on the outputs (i.e., productivity, product quality).
Since the bioprocess model coefficients are learned from limited process
observations, we derive the Bayesian posterior distribution to quantify model
uncertainty and further develop the Shapley value based sensitivity analysis to
evaluate the impact of estimation uncertainty from each set of model
coefficients. Therefore, the proposed bioprocess risk and sensitivity analyses
can identify the bottlenecks, guide the reliable process specifications and the
most "informative" data collection, and improve production stability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Xie_W/0/1/0/all/0/1"&gt;Wei Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wang_B/0/1/0/all/0/1"&gt;Bo Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Li_C/0/1/0/all/0/1"&gt;Cheng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Xie_D/0/1/0/all/0/1"&gt;Dongming Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Auclair_J/0/1/0/all/0/1"&gt;Jared Auclair&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SAINT: Improved Neural Networks for Tabular Data via Row Attention and Contrastive Pre-Training. (arXiv:2106.01342v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01342</id>
        <link href="http://arxiv.org/abs/2106.01342"/>
        <updated>2021-06-03T02:10:36.462Z</updated>
        <summary type="html"><![CDATA[Tabular data underpins numerous high-impact applications of machine learning
from fraud detection to genomics and healthcare. Classical approaches to
solving tabular problems, such as gradient boosting and random forests, are
widely used by practitioners. However, recent deep learning methods have
achieved a degree of performance competitive with popular techniques. We devise
a hybrid deep learning approach to solving tabular data problems. Our method,
SAINT, performs attention over both rows and columns, and it includes an
enhanced embedding method. We also study a new contrastive self-supervised
pre-training method for use when labels are scarce. SAINT consistently improves
performance over previous deep learning methods, and it even outperforms
gradient boosting methods, including XGBoost, CatBoost, and LightGBM, on
average over a variety of benchmark tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Somepalli_G/0/1/0/all/0/1"&gt;Gowthami Somepalli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldblum_M/0/1/0/all/0/1"&gt;Micah Goldblum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schwarzschild_A/0/1/0/all/0/1"&gt;Avi Schwarzschild&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bruss_C/0/1/0/all/0/1"&gt;C. Bayan Bruss&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1"&gt;Tom Goldstein&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Explaining Data-Driven Decisions made by AI Systems: The Counterfactual Approach. (arXiv:2001.07417v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2001.07417</id>
        <link href="http://arxiv.org/abs/2001.07417"/>
        <updated>2021-06-03T02:10:36.457Z</updated>
        <summary type="html"><![CDATA[We examine counterfactual explanations for explaining the decisions made by
model-based AI systems. The counterfactual approach we consider defines an
explanation as a set of the system's data inputs that causally drives the
decision (i.e., changing the inputs in the set changes the decision) and is
irreducible (i.e., changing any subset of the inputs does not change the
decision). We (1) demonstrate how this framework may be used to provide
explanations for decisions made by general, data-driven AI systems that may
incorporate features with arbitrary data types and multiple predictive models,
and (2) propose a heuristic procedure to find the most useful explanations
depending on the context. We then contrast counterfactual explanations with
methods that explain model predictions by weighting features according to their
importance (e.g., SHAP, LIME) and present two fundamental reasons why we should
carefully consider whether importance-weight explanations are well-suited to
explain system decisions. Specifically, we show that (i) features that have a
large importance weight for a model prediction may not affect the corresponding
decision, and (ii) importance weights are insufficient to communicate whether
and how features influence decisions. We demonstrate this with several concise
examples and three detailed case studies that compare the counterfactual
approach with SHAP to illustrate various conditions under which counterfactual
explanations explain data-driven decisions better than importance weights.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fernandez_Loria_C/0/1/0/all/0/1"&gt;Carlos Fern&amp;#xe1;ndez-Lor&amp;#xed;a&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Provost_F/0/1/0/all/0/1"&gt;Foster Provost&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1"&gt;Xintian Han&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Decision Transformer: Reinforcement Learning via Sequence Modeling. (arXiv:2106.01345v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01345</id>
        <link href="http://arxiv.org/abs/2106.01345"/>
        <updated>2021-06-03T02:10:36.422Z</updated>
        <summary type="html"><![CDATA[We present a framework that abstracts Reinforcement Learning (RL) as a
sequence modeling problem. This allows us to draw upon the simplicity and
scalability of the Transformer architecture, and associated advances in
language modeling such as GPT-x and BERT. In particular, we present Decision
Transformer, an architecture that casts the problem of RL as conditional
sequence modeling. Unlike prior approaches to RL that fit value functions or
compute policy gradients, Decision Transformer simply outputs the optimal
actions by leveraging a causally masked Transformer. By conditioning an
autoregressive model on the desired return (reward), past states, and actions,
our Decision Transformer model can generate future actions that achieve the
desired return. Despite its simplicity, Decision Transformer matches or exceeds
the performance of state-of-the-art model-free offline RL baselines on Atari,
OpenAI Gym, and Key-to-Door tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Lili Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_K/0/1/0/all/0/1"&gt;Kevin Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rajeswaran_A/0/1/0/all/0/1"&gt;Aravind Rajeswaran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1"&gt;Kimin Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grover_A/0/1/0/all/0/1"&gt;Aditya Grover&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Laskin_M/0/1/0/all/0/1"&gt;Michael Laskin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1"&gt;Pieter Abbeel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srinivas_A/0/1/0/all/0/1"&gt;Aravind Srinivas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mordatch_I/0/1/0/all/0/1"&gt;Igor Mordatch&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Compressing Large-Scale Transformer-Based Models: A Case Study on BERT. (arXiv:2002.11985v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.11985</id>
        <link href="http://arxiv.org/abs/2002.11985"/>
        <updated>2021-06-03T02:10:36.417Z</updated>
        <summary type="html"><![CDATA[Pre-trained Transformer-based models have achieved state-of-the-art
performance for various Natural Language Processing (NLP) tasks. However, these
models often have billions of parameters, and, thus, are too resource-hungry
and computation-intensive to suit low-capability devices or applications with
strict latency requirements. One potential remedy for this is model
compression, which has attracted a lot of research attention. Here, we
summarize the research in compressing Transformers, focusing on the especially
popular BERT model. In particular, we survey the state of the art in
compression for BERT, we clarify the current best practices for compressing
large-scale Transformer models, and we provide insights into the workings of
various methods. Our categorization and analysis also shed light on promising
future research directions for achieving lightweight, accurate, and generic NLP
models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ganesh_P/0/1/0/all/0/1"&gt;Prakhar Ganesh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lou_X/0/1/0/all/0/1"&gt;Xin Lou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1"&gt;Mohammad Ali Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sajjad_H/0/1/0/all/0/1"&gt;Hassan Sajjad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1"&gt;Preslav Nakov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1"&gt;Deming Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Winslett_M/0/1/0/all/0/1"&gt;Marianne Winslett&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tight High Probability Bounds for Linear Stochastic Approximation with Fixed Stepsize. (arXiv:2106.01257v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.01257</id>
        <link href="http://arxiv.org/abs/2106.01257"/>
        <updated>2021-06-03T02:10:36.412Z</updated>
        <summary type="html"><![CDATA[This paper provides a non-asymptotic analysis of linear stochastic
approximation (LSA) algorithms with fixed stepsize. This family of methods
arises in many machine learning tasks and is used to obtain approximate
solutions of a linear system $\bar{A}\theta = \bar{b}$ for which $\bar{A}$ and
$\bar{b}$ can only be accessed through random estimates $\{({\bf A}_n, {\bf
b}_n): n \in \mathbb{N}^*\}$. Our analysis is based on new results regarding
moments and high probability bounds for products of matrices which are shown to
be tight. We derive high probability bounds on the performance of LSA under
weaker conditions on the sequence $\{({\bf A}_n, {\bf b}_n): n \in
\mathbb{N}^*\}$ than previous works. However, in contrast, we establish
polynomial concentration bounds with order depending on the stepsize. We show
that our conclusions cannot be improved without additional assumptions on the
sequence of random matrices $\{{\bf A}_n: n \in \mathbb{N}^*\}$, and in
particular that no Gaussian or exponential high probability bounds can hold.
Finally, we pay a particular attention to establishing bounds with sharp order
with respect to the number of iterations and the stepsize and whose leading
terms contain the covariance matrices appearing in the central limit theorems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Durmus_A/0/1/0/all/0/1"&gt;Alain Durmus&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Moulines_E/0/1/0/all/0/1"&gt;Eric Moulines&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Naumov_A/0/1/0/all/0/1"&gt;Alexey Naumov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Samsonov_S/0/1/0/all/0/1"&gt;Sergey Samsonov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Scaman_K/0/1/0/all/0/1"&gt;Kevin Scaman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wai_H/0/1/0/all/0/1"&gt;Hoi-To Wai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Addressing the Long-term Impact of ML Decisions via Policy Regret. (arXiv:2106.01325v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01325</id>
        <link href="http://arxiv.org/abs/2106.01325"/>
        <updated>2021-06-03T02:10:36.405Z</updated>
        <summary type="html"><![CDATA[Machine Learning (ML) increasingly informs the allocation of opportunities to
individuals and communities in areas such as lending, education, employment,
and beyond. Such decisions often impact their subjects' future characteristics
and capabilities in an a priori unknown fashion. The decision-maker, therefore,
faces exploration-exploitation dilemmas akin to those in multi-armed bandits.
Following prior work, we model communities as arms. To capture the long-term
effects of ML-based allocation decisions, we study a setting in which the
reward from each arm evolves every time the decision-maker pulls that arm. We
focus on reward functions that are initially increasing in the number of pulls
but may become (and remain) decreasing after a certain point. We argue that an
acceptable sequential allocation of opportunities must take an arm's potential
for growth into account. We capture these considerations through the notion of
policy regret, a much stronger notion than the often-studied external regret,
and present an algorithm with provably sub-linear policy regret for
sufficiently long time horizons. We empirically compare our algorithm with
several baselines and find that it consistently outperforms them, in particular
for long time horizons.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lindner_D/0/1/0/all/0/1"&gt;David Lindner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heidari_H/0/1/0/all/0/1"&gt;Hoda Heidari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krause_A/0/1/0/all/0/1"&gt;Andreas Krause&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Assessing the Causal Impact of COVID-19 Related Policies on Outbreak Dynamics: A Case Study in the US. (arXiv:2106.01315v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01315</id>
        <link href="http://arxiv.org/abs/2106.01315"/>
        <updated>2021-06-03T02:10:36.400Z</updated>
        <summary type="html"><![CDATA[To mitigate the spread of COVID-19 pandemic, decision-makers and public
authorities have announced various non-pharmaceutical policies. Analyzing the
causal impact of these policies in reducing the spread of COVID-19 is important
for future policy-making. The main challenge here is the existence of
unobserved confounders (e.g., vigilance of residents). Besides, as the
confounders may be time-varying during COVID-19 (e.g., vigilance of residents
changes in the course of the pandemic), it is even more difficult to capture
them. In this paper, we study the problem of assessing the causal effects of
different COVID-19 related policies on the outbreak dynamics in different
counties at any given time period. To this end, we integrate data about
different COVID-19 related policies (treatment) and outbreak dynamics (outcome)
for different United States counties over time and analyze them with respect to
variables that can infer the confounders, including the covariates of different
counties, their relational information and historical information. Based on
these data, we develop a neural network based causal effect estimation
framework which leverages above information in observational data and learns
the representations of time-varying (unobserved) confounders. In this way, it
enables us to quantify the causal impact of policies at different
granularities, ranging from a category of policies with a certain goal to a
specific policy type in this category. Besides, experimental results also
indicate the effectiveness of our proposed framework in capturing the
confounders for quantifying the causal impact of different policies. More
specifically, compared with several baseline methods, our framework captures
the outbreak dynamics more accurately, and our assessment of policies is more
consistent with existing epidemiological studies of COVID-19.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1"&gt;Jing Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1"&gt;Yushun Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zheng Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mietchen_D/0/1/0/all/0/1"&gt;Daniel Mietchen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jundong Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Semi-Supervised iNaturalist Challenge at the FGVC8 Workshop. (arXiv:2106.01364v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.01364</id>
        <link href="http://arxiv.org/abs/2106.01364"/>
        <updated>2021-06-03T02:10:36.395Z</updated>
        <summary type="html"><![CDATA[Semi-iNat is a challenging dataset for semi-supervised classification with a
long-tailed distribution of classes, fine-grained categories, and domain shifts
between labeled and unlabeled data. This dataset is behind the second iteration
of the semi-supervised recognition challenge to be held at the FGVC8 workshop
at CVPR 2021. Different from the previous one, this dataset (i) includes images
of species from different kingdoms in the natural taxonomy, (ii) is at a larger
scale --- with 810 in-class and 1629 out-of-class species for a total of 330k
images, and (iii) does not provide in/out-of-class labels, but provides coarse
taxonomic labels (kingdom and phylum) for the unlabeled images. This document
describes baseline results and the details of the dataset which is available
here: \url{https://github.com/cvl-umass/semi-inat-2021}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1"&gt;Jong-Chyi Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maji_S/0/1/0/all/0/1"&gt;Subhransu Maji&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lottery Jackpots Exist in Pre-trained Models. (arXiv:2104.08700v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.08700</id>
        <link href="http://arxiv.org/abs/2104.08700"/>
        <updated>2021-06-03T02:10:36.391Z</updated>
        <summary type="html"><![CDATA[Network pruning is an effective approach to reduce network complexity without
performance compromise. Existing studies achieve the sparsity of neural
networks via time-consuming weight tuning or complex search on networks with
expanded width, which greatly limits the applications of network pruning. In
this paper, we show that high-performing and sparse sub-networks without the
involvement of weight tuning, termed "lottery jackpots", exist in pre-trained
models with unexpanded width. For example, we obtain a lottery jackpot that has
only 10% parameters and still reaches the performance of the original dense
VGGNet-19 without any modifications on the pre-trained weights. Furthermore, we
observe that the sparse masks derived from many existing pruning criteria have
a high overlap with the searched mask of our lottery jackpot, among which, the
magnitude-based pruning results in the most similar mask with ours. Based on
this insight, we initialize our sparse mask using the magnitude pruning,
resulting in at least 3x cost reduction on the lottery jackpot search while
achieves comparable or even better performance. Specifically, our
magnitude-based lottery jackpot removes 90% weights in the ResNet-50, while
easily obtains more than 70% top-1 accuracy using only 10 searching epochs on
ImageNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yuxin Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1"&gt;Mingbao Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chao_F/0/1/0/all/0/1"&gt;Fei Chao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yongjian Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1"&gt;Feiyue Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1"&gt;Mingliang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1"&gt;Yonghong Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1"&gt;Rongrong Ji&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Survey Equivalence: A Procedure for Measuring Classifier Accuracy Against Human Labels. (arXiv:2106.01254v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01254</id>
        <link href="http://arxiv.org/abs/2106.01254"/>
        <updated>2021-06-03T02:10:36.385Z</updated>
        <summary type="html"><![CDATA[In many classification tasks, the ground truth is either noisy or subjective.
Examples include: which of two alternative paper titles is better? is this
comment toxic? what is the political leaning of this news article? We refer to
such tasks as survey settings because the ground truth is defined through a
survey of one or more human raters. In survey settings, conventional
measurements of classifier accuracy such as precision, recall, and
cross-entropy confound the quality of the classifier with the level of
agreement among human raters. Thus, they have no meaningful interpretation on
their own. We describe a procedure that, given a dataset with predictions from
a classifier and K ratings per item, rescales any accuracy measure into one
that has an intuitive interpretation. The key insight is to score the
classifier not against the best proxy for the ground truth, such as a majority
vote of the raters, but against a single human rater at a time. That score can
be compared to other predictors' scores, in particular predictors created by
combining labels from several other human raters. The survey equivalence of any
classifier is the minimum number of raters needed to produce the same expected
score as that found for the classifier.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Resnick_P/0/1/0/all/0/1"&gt;Paul Resnick&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kong_Y/0/1/0/all/0/1"&gt;Yuqing Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schoenebeck_G/0/1/0/all/0/1"&gt;Grant Schoenebeck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weninger_T/0/1/0/all/0/1"&gt;Tim Weninger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MNL-Bandit with Knapsacks. (arXiv:2106.01135v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01135</id>
        <link href="http://arxiv.org/abs/2106.01135"/>
        <updated>2021-06-03T02:10:36.372Z</updated>
        <summary type="html"><![CDATA[We consider a dynamic assortment selection problem where a seller has a fixed
inventory of $N$ substitutable products and faces an unknown demand that
arrives sequentially over $T$ periods. In each period, the seller needs to
decide on the assortment of products (of cardinality at most $K$) to offer to
the customers. The customer's response follows an unknown multinomial logit
model (MNL) with parameters $v$. The goal of the seller is to maximize the
total expected revenue given the fixed initial inventory of $N$ products. We
give a policy that achieves a regret of $\tilde O\left(K \sqrt{K N T}\left(1 +
\frac{\sqrt{v_{\max}}}{q_{\min}}\text{OPT}\right) \right)$ under a mild
assumption on the model parameters. In particular, our policy achieves a
near-optimal $\tilde O(\sqrt{T})$ regret in the large inventory setting.

Our policy builds upon the UCB-based approach for MNL-bandit without
inventory constraints in [1] and addresses the inventory constraints through an
exponentially sized LP for which we present a tractable approximation while
keeping the $\tilde O(\sqrt{T})$ regret bound.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aznag_A/0/1/0/all/0/1"&gt;Abdellah Aznag&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goyal_V/0/1/0/all/0/1"&gt;Vineet Goyal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Perivier_N/0/1/0/all/0/1"&gt;Noemie Perivier&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Inter-GPS: Interpretable Geometry Problem Solving with Formal Language and Symbolic Reasoning. (arXiv:2105.04165v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.04165</id>
        <link href="http://arxiv.org/abs/2105.04165"/>
        <updated>2021-06-03T02:10:36.348Z</updated>
        <summary type="html"><![CDATA[Geometry problem solving has attracted much attention in the NLP community
recently. The task is challenging as it requires abstract problem understanding
and symbolic reasoning with axiomatic knowledge. However, current datasets are
either small in scale or not publicly available. Thus, we construct a new
large-scale benchmark, Geometry3K, consisting of 3,002 geometry problems with
dense annotation in formal language. We further propose a novel geometry
solving approach with formal language and symbolic reasoning, called
Interpretable Geometry Problem Solver (Inter-GPS). Inter-GPS first parses the
problem text and diagram into formal language automatically via rule-based text
parsing and neural object detecting, respectively. Unlike implicit learning in
existing methods, Inter-GPS incorporates theorem knowledge as conditional rules
and performs symbolic reasoning step by step. Also, a theorem predictor is
designed to infer the theorem application sequence fed to the symbolic solver
for the more efficient and reasonable searching path. Extensive experiments on
the Geometry3K and GEOS datasets demonstrate that Inter-GPS achieves
significant improvements over existing methods. The project with code and data
is available at https://lupantech.github.io/inter-gps.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lu_P/0/1/0/all/0/1"&gt;Pan Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_R/0/1/0/all/0/1"&gt;Ran Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1"&gt;Shibiao Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_L/0/1/0/all/0/1"&gt;Liang Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1"&gt;Siyuan Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1"&gt;Xiaodan Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1"&gt;Song-Chun Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improved Rates for Differentially Private Stochastic Convex Optimization with Heavy-Tailed Data. (arXiv:2106.01336v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01336</id>
        <link href="http://arxiv.org/abs/2106.01336"/>
        <updated>2021-06-03T02:10:36.342Z</updated>
        <summary type="html"><![CDATA[We study stochastic convex optimization with heavy-tailed data under the
constraint of differential privacy. Most prior work on this problem is
restricted to the case where the loss function is Lipschitz. Instead, as
introduced by Wang, Xiao, Devadas, and Xu, we study general convex loss
functions with the assumption that the distribution of gradients has bounded
$k$-th moments. We provide improved upper bounds on the excess population risk
under approximate differential privacy of
$\tilde{O}\left(\sqrt{\frac{d}{n}}+\left(\frac{d}{\epsilon
n}\right)^{\frac{k-1}{k}}\right)$ and
$\tilde{O}\left(\frac{d}{n}+\left(\frac{d}{\epsilon
n}\right)^{\frac{2k-2}{k}}\right)$ for convex and strongly convex loss
functions, respectively. We also prove nearly-matching lower bounds under the
constraint of pure differential privacy, giving strong evidence that our bounds
are tight.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kamath_G/0/1/0/all/0/1"&gt;Gautam Kamath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xingtu Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Huanyu Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evidential Turing Processes. (arXiv:2106.01216v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01216</id>
        <link href="http://arxiv.org/abs/2106.01216"/>
        <updated>2021-06-03T02:10:36.333Z</updated>
        <summary type="html"><![CDATA[A probabilistic classifier with reliable predictive uncertainties i) fits
successfully to the target domain data, ii) provides calibrated class
probabilities in difficult regions of the target domain (e.g. class overlap),
and iii) accurately identifies queries coming out of the target domain and
reject them. We introduce an original combination of evidential deep learning,
neural processes, and neural Turing machines capable of providing all three
essential properties mentioned above for total uncertainty quantification. We
observe our method on three image classification benchmarks and two neural net
architectures to consistently give competitive or superior scores with respect
to multiple uncertainty quantification metrics against state-of-the-art methods
explicitly tailored to one or a few of them. Our unified solution delivers an
implementation-friendly and computationally efficient recipe for safety
clearance and provides intellectual economy to an investigation of algorithmic
roots of epistemic awareness in deep neural nets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kandemir_M/0/1/0/all/0/1"&gt;Melih Kandemir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Akgul_A/0/1/0/all/0/1"&gt;Abdullah Akg&amp;#xfc;l&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Haussmann_M/0/1/0/all/0/1"&gt;Manuel Haussmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Unal_G/0/1/0/all/0/1"&gt;Gozde Unal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spectral embedding for dynamic networks with stability guarantees. (arXiv:2106.01282v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.01282</id>
        <link href="http://arxiv.org/abs/2106.01282"/>
        <updated>2021-06-03T02:10:36.327Z</updated>
        <summary type="html"><![CDATA[We consider the problem of embedding a dynamic network, to obtain
time-evolving vector representations of each node, which can then be used to
describe the changes in behaviour of a single node, one or more communities, or
the entire graph. Given this open-ended remit, we wish to guarantee stability
in the spatio-temporal positioning of the nodes: assigning the same position,
up to noise, to nodes behaving similarly at a given time (cross-sectional
stability) and a constant position, up to noise, to a single node behaving
similarly across different times (longitudinal stability). These properties are
defined formally within a generic dynamic latent position model. By showing how
this model can be recast as a multilayer random dot product graph, we
demonstrate that unfolded adjacency spectral embedding satisfies both stability
conditions, allowing, for example, spatio-temporal clustering under the dynamic
stochastic block model. We also show how alternative methods, such as omnibus,
independent or time-averaged spectral embedding, lack one or the other form of
stability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Gallagher_I/0/1/0/all/0/1"&gt;Ian Gallagher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Jones_A/0/1/0/all/0/1"&gt;Andrew Jones&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Rubin_Delanchy_P/0/1/0/all/0/1"&gt;Patrick Rubin-Delanchy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Opening the Black Box of Deep Neural Networks in Physical Layer Communication. (arXiv:2106.01124v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2106.01124</id>
        <link href="http://arxiv.org/abs/2106.01124"/>
        <updated>2021-06-03T02:10:36.314Z</updated>
        <summary type="html"><![CDATA[Deep Neural Network (DNN)-based physical layer techniques are attracting
considerable interest due to their potential to enhance communication systems.
However, most studies in the physical layer have tended to focus on the
implement of DNN but not to theoretically understand how does a DNN work in a
communication system. In this letter, we aim to quantitatively analyse why DNNs
can achieve comparable performance in the physical layer comparing with
traditional techniques and its cost in terms of computational complexity. We
further investigate and also experimentally validate how information is flown
in a DNN-based communication system under the information theoretic concepts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jun Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mei_K/0/1/0/all/0/1"&gt;Kai Mei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ma_D/0/1/0/all/0/1"&gt;Dongtang Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wei_J/0/1/0/all/0/1"&gt;Jibo Wei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Benchmarking CNN on 3D Anatomical Brain MRI: Architectures, Data Augmentation and Deep Ensemble Learning. (arXiv:2106.01132v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.01132</id>
        <link href="http://arxiv.org/abs/2106.01132"/>
        <updated>2021-06-03T02:10:36.302Z</updated>
        <summary type="html"><![CDATA[Deep Learning (DL) and specifically CNN models have become a de facto method
for a wide range of vision tasks, outperforming traditional machine learning
(ML) methods. Consequently, they drew a lot of attention in the neuroimaging
field in particular for phenotype prediction or computer-aided diagnosis.
However, most of the current studies often deal with small single-site cohorts,
along with a specific pre-processing pipeline and custom CNN architectures,
which make them difficult to compare to. We propose an extensive benchmark of
recent state-of-the-art (SOTA) 3D CNN, evaluating also the benefits of data
augmentation and deep ensemble learning, on both Voxel-Based Morphometry (VBM)
pre-processing and quasi-raw images. Experiments were conducted on a large
multi-site 3D brain anatomical MRI data-set comprising N=10k scans on 3
challenging tasks: age prediction, sex classification, and schizophrenia
diagnosis. We found that all models provide significantly better predictions
with VBM images than quasi-raw data. This finding evolved as the training set
approaches 10k samples where quasi-raw data almost reach the performance of
VBM. Moreover, we showed that linear models perform comparably with SOTA CNN on
VBM data. We also demonstrated that DenseNet and tiny-DenseNet, a lighter
version that we proposed, provide a good compromise in terms of performance in
all data regime. Therefore, we suggest to employ them as the architectures by
default. Critically, we also showed that current CNN are still very biased
towards the acquisition site, even when trained with N=10k multi-site images.
In this context, VBM pre-processing provides an efficient way to limit this
site effect. Surprisingly, we did not find any clear benefit from data
augmentation techniques. Finally, we proved that deep ensemble learning is well
suited to re-calibrate big CNN models without sacrificing performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dufumier_B/0/1/0/all/0/1"&gt;Benoit Dufumier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gori_P/0/1/0/all/0/1"&gt;Pietro Gori&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Battaglia_I/0/1/0/all/0/1"&gt;Ilaria Battaglia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Victor_J/0/1/0/all/0/1"&gt;Julie Victor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grigis_A/0/1/0/all/0/1"&gt;Antoine Grigis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duchesnay_E/0/1/0/all/0/1"&gt;Edouard Duchesnay&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Matrix factorisation and the interpretation of geodesic distance. (arXiv:2106.01260v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.01260</id>
        <link href="http://arxiv.org/abs/2106.01260"/>
        <updated>2021-06-03T02:10:36.280Z</updated>
        <summary type="html"><![CDATA[Given a graph or similarity matrix, we consider the problem of recovering a
notion of true distance between the nodes, and so their true positions. Through
new insights into the manifold geometry underlying a generic latent position
model, we show that this can be accomplished in two steps: matrix
factorisation, followed by nonlinear dimension reduction. This combination is
effective because the point cloud obtained in the first step lives close to a
manifold in which latent distance is encoded as geodesic distance. Hence, a
nonlinear dimension reduction tool, approximating geodesic distance, can
recover the latent positions, up to a simple transformation. We give a detailed
account of the case where spectral embedding is used, followed by Isomap, and
provide encouraging experimental evidence for other combinations of techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Whiteley_N/0/1/0/all/0/1"&gt;Nick Whiteley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Gray_A/0/1/0/all/0/1"&gt;Annie Gray&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Rubin_Delanchy_P/0/1/0/all/0/1"&gt;Patrick Rubin-Delanchy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Deeper Deep Reinforcement Learning. (arXiv:2106.01151v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01151</id>
        <link href="http://arxiv.org/abs/2106.01151"/>
        <updated>2021-06-03T02:10:36.274Z</updated>
        <summary type="html"><![CDATA[In computer vision and natural language processing, innovations in model
architecture that lead to increases in model capacity have reliably translated
into gains in performance. In stark contrast with this trend, state-of-the-art
reinforcement learning (RL) algorithms often use only small MLPs, and gains in
performance typically originate from algorithmic innovations. It is natural to
hypothesize that small datasets in RL necessitate simple models to avoid
overfitting; however, this hypothesis is untested. In this paper we investigate
how RL agents are affected by exchanging the small MLPs with larger modern
networks with skip connections and normalization, focusing specifically on soft
actor-critic (SAC) algorithms. We verify, empirically, that na\"ively adopting
such architectures leads to instabilities and poor performance, likely
contributing to the popularity of simple models in practice. However, we show
that dataset size is not the limiting factor, and instead argue that intrinsic
instability from the actor in SAC taking gradients through the critic is the
culprit. We demonstrate that a simple smoothing method can mitigate this issue,
which enables stable training with large modern architectures. After smoothing,
larger models yield dramatic performance improvements for state-of-the-art
agents -- suggesting that more "easy" gains may be had by focusing on model
architectures in addition to algorithmic innovations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bjorck_J/0/1/0/all/0/1"&gt;Johan Bjorck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gomes_C/0/1/0/all/0/1"&gt;Carla P. Gomes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weinberger_K/0/1/0/all/0/1"&gt;Kilian Q. Weinberger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Design and Comparison of Reward Functions in Reinforcement Learning for Energy Management of Sensor Nodes. (arXiv:2106.01114v1 [eess.SY])]]></title>
        <id>http://arxiv.org/abs/2106.01114</id>
        <link href="http://arxiv.org/abs/2106.01114"/>
        <updated>2021-06-03T02:10:36.268Z</updated>
        <summary type="html"><![CDATA[Interest in remote monitoring has grown thanks to recent advancements in
Internet-of-Things (IoT) paradigms. New applications have emerged, using small
devices called sensor nodes capable of collecting data from the environment and
processing it. However, more and more data are processed and transmitted with
longer operational periods. At the same, the battery technologies have not
improved fast enough to cope with these increasing needs. This makes the energy
consumption issue increasingly challenging and thus, miniaturized energy
harvesting devices have emerged to complement traditional energy sources.
Nevertheless, the harvested energy fluctuates significantly during the node
operation, increasing uncertainty in actually available energy resources.
Recently, approaches in energy management have been developed, in particular
using reinforcement learning approaches. However, in reinforcement learning,
the algorithm's performance relies greatly on the reward function. In this
paper, we present two contributions. First, we explore five different reward
functions to identify the most suitable variables to use in such functions to
obtain the desired behaviour. Experiments were conducted using the Q-learning
algorithm to adjust the energy consumption depending on the energy harvested.
Results with the five reward functions illustrate how the choice thereof
impacts the energy consumption of the node. Secondly, we propose two additional
reward functions able to find the compromise between energy consumption and a
node performance using a non-fixed balancing parameter. Our simulation results
show that the proposed reward functions adjust the node's performance depending
on the battery level and reduce the learning time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Rioual_Y/0/1/0/all/0/1"&gt;Yohann Rioual&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/eess/1/au:+Moullec_Y/0/1/0/all/0/1"&gt;Yannick Le Moullec&lt;/a&gt; (2), &lt;a href="http://arxiv.org/find/eess/1/au:+Laurent_J/0/1/0/all/0/1"&gt;Johann Laurent&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/eess/1/au:+Khan_M/0/1/0/all/0/1"&gt;Muhidul Islam Khan&lt;/a&gt; (2), &lt;a href="http://arxiv.org/find/eess/1/au:+Diguet_J/0/1/0/all/0/1"&gt;Jean-Philippe Diguet&lt;/a&gt; (3) ((1) Lab-STICC, University Bretagne Sud, (2) Thomas Johann Seebeck Department of Electronics, Tallinn University of Technology, (3) IRL CNRS CROSSING)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Linear-Time Gromov Wasserstein Distances using Low Rank Couplings and Costs. (arXiv:2106.01128v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01128</id>
        <link href="http://arxiv.org/abs/2106.01128"/>
        <updated>2021-06-03T02:10:36.262Z</updated>
        <summary type="html"><![CDATA[The ability to compare and align related datasets living in heterogeneous
spaces plays an increasingly important role in machine learning. The
Gromov-Wasserstein (GW) formalism can help tackle this problem. Its main goal
is to seek an assignment (more generally a coupling matrix) that can register
points across otherwise incomparable datasets. As a non-convex and quadratic
generalization of optimal transport (OT), GW is NP-hard. Yet, heuristics are
known to work reasonably well in practice, the state of the art approach being
to solve a sequence of nested regularized OT problems. While popular, that
heuristic remains too costly to scale, with cubic complexity in the number of
samples $n$. We show in this paper how a recent variant of the Sinkhorn
algorithm can substantially speed up the resolution of GW. That variant
restricts the set of admissible couplings to those admitting a low rank
factorization as the product of two sub-couplings. By updating alternatively
each sub-coupling, our algorithm computes a stationary point of the problem in
quadratic time with respect to the number of samples. When cost matrices have
themselves low rank, our algorithm has time complexity $\mathcal{O}(n)$. We
demonstrate the efficiency of our method on simulated and real data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Scetbon_M/0/1/0/all/0/1"&gt;Meyer Scetbon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peyre_G/0/1/0/all/0/1"&gt;Gabriel Peyr&amp;#xe9;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cuturi_M/0/1/0/all/0/1"&gt;Marco Cuturi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Framing RNN as a kernel method: A neural ODE approach. (arXiv:2106.01202v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.01202</id>
        <link href="http://arxiv.org/abs/2106.01202"/>
        <updated>2021-06-03T02:10:36.246Z</updated>
        <summary type="html"><![CDATA[Building on the interpretation of a recurrent neural network (RNN) as a
continuous-time neural differential equation, we show, under appropriate
conditions, that the solution of a RNN can be viewed as a linear function of a
specific feature set of the input sequence, known as the signature. This
connection allows us to frame a RNN as a kernel method in a suitable
reproducing kernel Hilbert space. As a consequence, we obtain theoretical
guarantees on generalization and stability for a large class of recurrent
networks. Our results are illustrated on simulated datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Fermanian_A/0/1/0/all/0/1"&gt;Adeline Fermanian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Marion_P/0/1/0/all/0/1"&gt;Pierre Marion&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Vert_J/0/1/0/all/0/1"&gt;Jean-Philippe Vert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Biau_G/0/1/0/all/0/1"&gt;G&amp;#xe9;rard Biau&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Assessing the Reliability of Deep Learning Classifiers Through Robustness Evaluation and Operational Profiles. (arXiv:2106.01258v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01258</id>
        <link href="http://arxiv.org/abs/2106.01258"/>
        <updated>2021-06-03T02:10:36.241Z</updated>
        <summary type="html"><![CDATA[The utilisation of Deep Learning (DL) is advancing into increasingly more
sophisticated applications. While it shows great potential to provide
transformational capabilities, DL also raises new challenges regarding its
reliability in critical functions. In this paper, we present a model-agnostic
reliability assessment method for DL classifiers, based on evidence from
robustness evaluation and the operational profile (OP) of a given application.
We partition the input space into small cells and then "assemble" their
robustness (to the ground truth) according to the OP, where estimators on the
cells' robustness and OPs are provided. Reliability estimates in terms of the
probability of misclassification per input (pmi) can be derived together with
confidence levels. A prototype tool is demonstrated with simplified case
studies. Model assumptions and extension to real-world applications are also
discussed. While our model easily uncovers the inherent difficulties of
assessing the DL dependability (e.g. lack of data with ground truth and
scalability issues), we provide preliminary/compromised solutions to advance in
this research direction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1"&gt;Xingyu Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1"&gt;Wei Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Banks_A/0/1/0/all/0/1"&gt;Alec Banks&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cox_V/0/1/0/all/0/1"&gt;Victoria Cox&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Flynn_D/0/1/0/all/0/1"&gt;David Flynn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schewe_S/0/1/0/all/0/1"&gt;Sven Schewe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1"&gt;Xiaowei Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Accurate and Robust Deep Learning Framework for Solving Wave-Based Inverse Problems in the Super-Resolution Regime. (arXiv:2106.01143v1 [math.NA])]]></title>
        <id>http://arxiv.org/abs/2106.01143</id>
        <link href="http://arxiv.org/abs/2106.01143"/>
        <updated>2021-06-03T02:10:36.149Z</updated>
        <summary type="html"><![CDATA[We propose an end-to-end deep learning framework that comprehensively solves
the inverse wave scattering problem across all length scales. Our framework
consists of the newly introduced wide-band butterfly network coupled with a
simple training procedure that dynamically injects noise during training. While
our trained network provides competitive results in classical imaging regimes,
most notably it also succeeds in the super-resolution regime where other
comparable methods fail. This encompasses both (i) reconstruction of scatterers
with sub-wavelength geometric features, and (ii) accurate imaging when two or
more scatterers are separated by less than the classical diffraction limit. We
demonstrate these properties are retained even in the presence of strong noise
and extend to scatterers not previously seen in the training set. In addition,
our network is straightforward to train requiring no restarts and has an online
runtime that is an order of magnitude faster than optimization-based
algorithms. We perform experiments with a variety of wave scattering mediums
and we demonstrate that our proposed framework outperforms both classical
inversion and competing network architectures that specialize in oscillatory
wave scattering data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Li_M/0/1/0/all/0/1"&gt;Matthew Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Demanet_L/0/1/0/all/0/1"&gt;Laurent Demanet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Zepeda_Nunez_L/0/1/0/all/0/1"&gt;Leonardo Zepeda-N&amp;#xfa;&amp;#xf1;ez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Solving Large-Scale Extensive-Form Network Security Games via Neural Fictitious Self-Play. (arXiv:2106.00897v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2106.00897</id>
        <link href="http://arxiv.org/abs/2106.00897"/>
        <updated>2021-06-03T02:10:36.138Z</updated>
        <summary type="html"><![CDATA[Securing networked infrastructures is important in the real world. The
problem of deploying security resources to protect against an attacker in
networked domains can be modeled as Network Security Games (NSGs).
Unfortunately, existing approaches, including the deep learning-based
approaches, are inefficient to solve large-scale extensive-form NSGs. In this
paper, we propose a novel learning paradigm, NSG-NFSP, to solve large-scale
extensive-form NSGs based on Neural Fictitious Self-Play (NFSP). Our main
contributions include: i) reforming the best response (BR) policy network in
NFSP to be a mapping from action-state pair to action-value, to make the
calculation of BR possible in NSGs; ii) converting the average policy network
of an NFSP agent into a metric-based classifier, helping the agent to assign
distributions only on legal actions rather than all actions; iii) enabling NFSP
with high-level actions, which can benefit training efficiency and stability in
NSGs; and iv) leveraging information contained in graphs of NSGs by learning
efficient graph node embeddings. Our algorithm significantly outperforms
state-of-the-art algorithms in both scalability and solution quality.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xue_W/0/1/0/all/0/1"&gt;Wanqi Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Youzhi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shuxin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xinrun Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+An_B/0/1/0/all/0/1"&gt;Bo An&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yeo_C/0/1/0/all/0/1"&gt;Chai Kiat Yeo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Invariant Policy Learning: A Causal Perspective. (arXiv:2106.00808v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.00808</id>
        <link href="http://arxiv.org/abs/2106.00808"/>
        <updated>2021-06-03T02:10:36.133Z</updated>
        <summary type="html"><![CDATA[In the past decade, contextual bandit and reinforcement learning algorithms
have been successfully used in various interactive learning systems such as
online advertising, recommender systems, and dynamic pricing. However, they
have yet to be widely adopted in high-stakes application domains, such as
healthcare. One reason may be that existing approaches assume that the
underlying mechanisms are static in the sense that they do not change over time
or over different environments. In many real world systems, however, the
mechanisms are subject to shifts across environments which may invalidate the
static environment assumption. In this paper, we tackle the problem of
environmental shifts under the framework of offline contextual bandits. We view
the environmental shift problem through the lens of causality and propose
multi-environment contextual bandits that allow for changes in the underlying
mechanisms. We adopt the concept of invariance from the causality literature
and introduce the notion of policy invariance. We argue that policy invariance
is only relevant if unobserved confounders are present and show that, in that
case, an optimal invariant policy is guaranteed, under certain assumptions, to
generalize across environments. Our results do not only provide a solution to
the environmental shift problem but also establish concrete connections among
causality, invariance and contextual bandits.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Saengkyongam_S/0/1/0/all/0/1"&gt;Sorawit Saengkyongam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thams_N/0/1/0/all/0/1"&gt;Nikolaj Thams&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peters_J/0/1/0/all/0/1"&gt;Jonas Peters&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pfister_N/0/1/0/all/0/1"&gt;Niklas Pfister&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Few-Shot Partial-Label Learning. (arXiv:2106.00984v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.00984</id>
        <link href="http://arxiv.org/abs/2106.00984"/>
        <updated>2021-06-03T02:10:36.127Z</updated>
        <summary type="html"><![CDATA[Partial-label learning (PLL) generally focuses on inducing a noise-tolerant
multi-class classifier by training on overly-annotated samples, each of which
is annotated with a set of labels, but only one is the valid label. A basic
promise of existing PLL solutions is that there are sufficient partial-label
(PL) samples for training. However, it is more common than not to have just few
PL samples at hand when dealing with new tasks. Furthermore, existing few-shot
learning algorithms assume precise labels of the support set; as such,
irrelevant labels may seriously mislead the meta-learner and thus lead to a
compromised performance. How to enable PLL under a few-shot learning setting is
an important problem, but not yet well studied. In this paper, we introduce an
approach called FsPLL (Few-shot PLL). FsPLL first performs adaptive distance
metric learning by an embedding network and rectifying prototypes on the tasks
previously encountered. Next, it calculates the prototype of each class of a
new task in the embedding network. An unseen example can then be classified via
its distance to each prototype. Experimental results on widely-used few-shot
datasets (Omniglot and miniImageNet) demonstrate that our FsPLL can achieve a
superior performance than the state-of-the-art methods across different
settings, and it needs fewer samples for quickly adapting to new tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yunfeng Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_G/0/1/0/all/0/1"&gt;Guoxian Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Lei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1"&gt;Zhongmin Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1"&gt;Lizhen Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Domeniconi_C/0/1/0/all/0/1"&gt;Carlotta Domeniconi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Communication-Efficient Split Learning Based on Analog Communication and Over the Air Aggregation. (arXiv:2106.00999v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.00999</id>
        <link href="http://arxiv.org/abs/2106.00999"/>
        <updated>2021-06-03T02:10:36.111Z</updated>
        <summary type="html"><![CDATA[Split-learning (SL) has recently gained popularity due to its inherent
privacy-preserving capabilities and ability to enable collaborative inference
for devices with limited computational power. Standard SL algorithms assume an
ideal underlying digital communication system and ignore the problem of scarce
communication bandwidth. However, for a large number of agents, limited
bandwidth resources, and time-varying communication channels, the communication
bandwidth can become the bottleneck. To address this challenge, in this work,
we propose a novel SL framework to solve the remote inference problem that
introduces an additional layer at the agent side and constrains the choices of
the weights and the biases to ensure over the air aggregation. Hence, the
proposed approach maintains constant communication cost with respect to the
number of agents enabling remote inference under limited bandwidth. Numerical
results show that our proposed algorithm significantly outperforms the digital
implementation in terms of communication-efficiency, especially as the number
of agents grows large.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Krouka_M/0/1/0/all/0/1"&gt;Mounssif Krouka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Elgabli_A/0/1/0/all/0/1"&gt;Anis Elgabli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Issaid_C/0/1/0/all/0/1"&gt;Chaouki ben Issaid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bennis_M/0/1/0/all/0/1"&gt;Mehdi Bennis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learn to Predict Equilibria via Fixed Point Networks. (arXiv:2106.00906v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.00906</id>
        <link href="http://arxiv.org/abs/2106.00906"/>
        <updated>2021-06-03T02:10:36.087Z</updated>
        <summary type="html"><![CDATA[Systems of interacting agents can often be modeled as contextual games, where
the context encodes additional information, beyond the control of any agent
(e.g. weather for traffic and fiscal policy for market economies). In such
systems, the most likely outcome is given by a Nash equilibrium. In many
practical settings, only game equilibria are observed, while the optimal
parameters for a game model are unknown. This work introduces Nash Fixed Point
Networks (N-FPNs), a class of implicit-depth neural networks that output Nash
equilibria of contextual games. The N-FPN architecture fuses data-driven
modeling with provided constraints. Given equilibrium observations of a
contextual game, N-FPN parameters are learnt to predict equilibria outcomes
given only the context. We present an end-to-end training scheme for N-FPNs
that is simple and memory efficient to implement with existing
autodifferentiation tools. N-FPNs also exploit a novel constraint decoupling
scheme to avoid costly projections. Provided numerical examples show the
efficacy of N-FPNs on atomic and non-atomic games (e.g. traffic routing).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Heaton_H/0/1/0/all/0/1"&gt;Howard Heaton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McKenzie_D/0/1/0/all/0/1"&gt;Daniel McKenzie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1"&gt;Qiuwei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fung_S/0/1/0/all/0/1"&gt;Samy Wu Fung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Osher_S/0/1/0/all/0/1"&gt;Stanley Osher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_W/0/1/0/all/0/1"&gt;Wotao Yin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Search Methods for Sufficient, Socially-Aligned Feature Importance Explanations with In-Distribution Counterfactuals. (arXiv:2106.00786v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.00786</id>
        <link href="http://arxiv.org/abs/2106.00786"/>
        <updated>2021-06-03T02:10:36.081Z</updated>
        <summary type="html"><![CDATA[Feature importance (FI) estimates are a popular form of explanation, and they
are commonly created and evaluated by computing the change in model confidence
caused by removing certain input features at test time. For example, in the
standard Sufficiency metric, only the top-k most important tokens are kept. In
this paper, we study several under-explored dimensions of FI-based
explanations, providing conceptual and empirical improvements for this form of
explanation. First, we advance a new argument for why it can be problematic to
remove features from an input when creating or evaluating explanations: the
fact that these counterfactual inputs are out-of-distribution (OOD) to models
implies that the resulting explanations are socially misaligned. The crux of
the problem is that the model prior and random weight initialization influence
the explanations (and explanation metrics) in unintended ways. To resolve this
issue, we propose a simple alteration to the model training process, which
results in more socially aligned explanations and metrics. Second, we compare
among five approaches for removing features from model inputs. We find that
some methods produce more OOD counterfactuals than others, and we make
recommendations for selecting a feature-replacement function. Finally, we
introduce four search-based methods for identifying FI explanations and compare
them to strong baselines, including LIME, Integrated Gradients, and random
search. On experiments with six diverse text classification datasets, we find
that the only method that consistently outperforms random search is a Parallel
Local Search that we introduce. Improvements over the second-best method are as
large as 5.4 points for Sufficiency and 17 points for Comprehensiveness. All
supporting code is publicly available at
https://github.com/peterbhase/ExplanationSearch.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hase_P/0/1/0/all/0/1"&gt;Peter Hase&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_H/0/1/0/all/0/1"&gt;Harry Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1"&gt;Mohit Bansal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Out-of-Domain Detection via Pre-trained Transformers. (arXiv:2106.00948v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.00948</id>
        <link href="http://arxiv.org/abs/2106.00948"/>
        <updated>2021-06-03T02:10:36.076Z</updated>
        <summary type="html"><![CDATA[Deployed real-world machine learning applications are often subject to
uncontrolled and even potentially malicious inputs. Those out-of-domain inputs
can lead to unpredictable outputs and sometimes catastrophic safety issues.
Prior studies on out-of-domain detection require in-domain task labels and are
limited to supervised classification scenarios. Our work tackles the problem of
detecting out-of-domain samples with only unsupervised in-domain data. We
utilize the latent representations of pre-trained transformers and propose a
simple yet effective method to transform features across all layers to
construct out-of-domain detectors efficiently. Two domain-specific fine-tuning
approaches are further proposed to boost detection accuracy. Our empirical
evaluations of related methods on two datasets validate that our method greatly
improves out-of-domain detection ability in a more general scenario.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1"&gt;Keyang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_T/0/1/0/all/0/1"&gt;Tongzheng Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shikun Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1"&gt;Yihao Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1"&gt;Caiming Xiong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Concurrent Learning Based Tracking Control of Nonlinear Systems using Gaussian Process. (arXiv:2106.00910v1 [eess.SY])]]></title>
        <id>http://arxiv.org/abs/2106.00910</id>
        <link href="http://arxiv.org/abs/2106.00910"/>
        <updated>2021-06-03T02:10:36.072Z</updated>
        <summary type="html"><![CDATA[This paper demonstrates the applicability of the combination of concurrent
learning as a tool for parameter estimation and non-parametric Gaussian Process
for online disturbance learning. A control law is developed by using both
techniques sequentially in the context of feedback linearization. The
concurrent learning algorithm estimates the system parameters of structured
uncertainty without requiring persistent excitation, which are used in the
design of the feedback linearization law. Then, a non-parametric Gaussian
Process learns unstructured uncertainty. The closed-loop system stability for
the nth-order system is proven using the Lyapunov stability theorem. The
simulation results show that the tracking error is minimized (i) when true
values of model parameters have not been provided, (ii) in the presence of
disturbances introduced once the parameters have converged to their true values
and (iii) when system parameters have not converged to their true values in the
presence of disturbances.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Bhandari_V/0/1/0/all/0/1"&gt;Vedant Bhandari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kayacan_E/0/1/0/all/0/1"&gt;Erkan Kayacan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Active Surface Models. (arXiv:2011.08826v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.08826</id>
        <link href="http://arxiv.org/abs/2011.08826"/>
        <updated>2021-06-03T02:10:36.057Z</updated>
        <summary type="html"><![CDATA[Active Surface Models have a long history of being useful to model complex 3D
surfaces but only Active Contours have been used in conjunction with deep
networks, and then only to produce the data term as well as meta-parameter maps
controlling them. In this paper, we advocate a much tighter integration. We
introduce layers that implement them that can be integrated seamlessly into
Graph Convolutional Networks to enforce sophisticated smoothness priors at an
acceptable computational cost. We will show that the resulting Deep Active
Surface Models outperform equivalent architectures that use traditional
regularization loss terms to impose smoothness priors for 3D surface
reconstruction from 2D images and for 3D volume segmentation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wickramasinghe_U/0/1/0/all/0/1"&gt;Udaranga Wickramasinghe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Knott_G/0/1/0/all/0/1"&gt;Graham Knott&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fua_P/0/1/0/all/0/1"&gt;Pascal Fua&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Contrastive ACE: Domain Generalization Through Alignment of Causal Mechanisms. (arXiv:2106.00925v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.00925</id>
        <link href="http://arxiv.org/abs/2106.00925"/>
        <updated>2021-06-03T02:10:36.052Z</updated>
        <summary type="html"><![CDATA[Domain generalization aims to learn knowledge invariant across different
distributions while semantically meaningful for downstream tasks from multiple
source domains, to improve the model's generalization ability on unseen target
domains. The fundamental objective is to understand the underlying "invariance"
behind these observational distributions and such invariance has been shown to
have a close connection to causality. While many existing approaches make use
of the property that causal features are invariant across domains, we consider
the causal invariance of the average causal effect of the features to the
labels. This invariance regularizes our training approach in which
interventions are performed on features to enforce stability of the causal
prediction by the classifier across domains. Our work thus sheds some light on
the domain generalization problem by introducing invariance of the mechanisms
into the learning process. Experiments on several benchmark datasets
demonstrate the performance of the proposed method against SOTAs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yunqi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1"&gt;Furui Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhitang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lian_Q/0/1/0/all/0/1"&gt;Qing Lian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1"&gt;Shoubo Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hao_J/0/1/0/all/0/1"&gt;Jianye Hao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yik-Chung Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimizing Functionals on the Space of Probabilities with Input Convex Neural Networks. (arXiv:2106.00774v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.00774</id>
        <link href="http://arxiv.org/abs/2106.00774"/>
        <updated>2021-06-03T02:10:36.047Z</updated>
        <summary type="html"><![CDATA[Gradient flows are a powerful tool for optimizing functionals in general
metric spaces, including the space of probabilities endowed with the
Wasserstein metric. A typical approach to solving this optimization problem
relies on its connection to the dynamic formulation of optimal transport and
the celebrated Jordan-Kinderlehrer-Otto (JKO) scheme. However, this formulation
involves optimization over convex functions, which is challenging, especially
in high dimensions. In this work, we propose an approach that relies on the
recently introduced input-convex neural networks (ICNN) to parameterize the
space of convex functions in order to approximate the JKO scheme, as well as in
designing functionals over measures that enjoy convergence guarantees. We
derive a computationally efficient implementation of this JKO-ICNN framework
and use various experiments to demonstrate its feasibility and validity in
approximating solutions of low-dimensional partial differential equations with
known solutions. We also explore the use of our JKO-ICNN approach in high
dimensions with an experiment in controlled generation for molecular discovery.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Alvarez_Melis_D/0/1/0/all/0/1"&gt;David Alvarez-Melis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Schiff_Y/0/1/0/all/0/1"&gt;Yair Schiff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Mroueh_Y/0/1/0/all/0/1"&gt;Youssef Mroueh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Compositionality of Neural Networks by Decoding Representations to Inputs. (arXiv:2106.00769v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.00769</id>
        <link href="http://arxiv.org/abs/2106.00769"/>
        <updated>2021-06-03T02:10:36.042Z</updated>
        <summary type="html"><![CDATA[In traditional software programs, we take for granted how easy it is to debug
code by tracing program logic from variables back to input, apply unit tests
and assertion statements to block erroneous behavior, and compose programs
together. But as the programs we write grow more complex, it becomes hard to
apply traditional software to applications like computer vision or natural
language. Although deep learning programs have demonstrated strong performance
on these applications, they sacrifice many of the functionalities of
traditional software programs. In this paper, we work towards bridging the
benefits of traditional and deep learning programs by jointly training a
generative model to constrain neural network activations to "decode" back to
inputs. Doing so enables practitioners to probe and track information encoded
in activation(s), apply assertion-like constraints on what information is
encoded in an activation, and compose separate neural networks together in a
plug-and-play fashion. In our experiments, we demonstrate applications of
decodable representations to out-of-distribution detection, adversarial
examples, calibration, and fairness -- while matching standard neural networks
in accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1"&gt;Mike Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goodman_N/0/1/0/all/0/1"&gt;Noah Goodman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ermon_S/0/1/0/all/0/1"&gt;Stefano Ermon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Some Ethical Issues in the Review Process of Machine Learning Conferences. (arXiv:2106.00810v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.00810</id>
        <link href="http://arxiv.org/abs/2106.00810"/>
        <updated>2021-06-03T02:10:36.038Z</updated>
        <summary type="html"><![CDATA[Recent successes in the Machine Learning community have led to a steep
increase in the number of papers submitted to conferences. This increase made
more prominent some of the issues that affect the current review process used
by these conferences. The review process has several issues that may undermine
the nature of scientific research, which is of being fully objective,
apolitical, unbiased and free of misconduct (such as plagiarism, cheating,
improper influence, and other improprieties). In this work, we study the
problem of reviewers' recruitment, infringements of the double-blind process,
fraudulent behaviors, biases in numerical ratings, and the appendix phenomenon
(i.e., the fact that it is becoming more common to publish results in the
appendix section of a paper). For each of these problems, we provide a short
description and possible solutions. The goal of this work is to raise awareness
in the Machine Learning community regarding these issues.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Russo_A/0/1/0/all/0/1"&gt;Alessio Russo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Efficacy of Adversarial Data Collection for Question Answering: Results from a Large-Scale Randomized Study. (arXiv:2106.00872v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.00872</id>
        <link href="http://arxiv.org/abs/2106.00872"/>
        <updated>2021-06-03T02:10:36.019Z</updated>
        <summary type="html"><![CDATA[In adversarial data collection (ADC), a human workforce interacts with a
model in real time, attempting to produce examples that elicit incorrect
predictions. Researchers hope that models trained on these more challenging
datasets will rely less on superficial patterns, and thus be less brittle.
However, despite ADC's intuitive appeal, it remains unclear when training on
adversarial datasets produces more robust models. In this paper, we conduct a
large-scale controlled study focused on question answering, assigning workers
at random to compose questions either (i) adversarially (with a model in the
loop); or (ii) in the standard fashion (without a model). Across a variety of
models and datasets, we find that models trained on adversarial data usually
perform better on other adversarial datasets but worse on a diverse collection
of out-of-domain evaluation sets. Finally, we provide a qualitative analysis of
adversarial (vs standard) data, identifying key differences and offering
guidance for future research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kaushik_D/0/1/0/all/0/1"&gt;Divyansh Kaushik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kiela_D/0/1/0/all/0/1"&gt;Douwe Kiela&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lipton_Z/0/1/0/all/0/1"&gt;Zachary C. Lipton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yih_W/0/1/0/all/0/1"&gt;Wen-tau Yih&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multiresolution Graph Variational Autoencoder. (arXiv:2106.00967v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.00967</id>
        <link href="http://arxiv.org/abs/2106.00967"/>
        <updated>2021-06-03T02:10:36.014Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose Multiresolution Graph Networks (MGN) and
Multiresolution Graph Variational Autoencoders (MGVAE) to learn and generate
graphs in a multiresolution and equivariant manner. At each resolution level,
MGN employs higher order message passing to encode the graph while learning to
partition it into mutually exclusive clusters and coarsening into a lower
resolution. MGVAE constructs a hierarchical generative model based on MGN to
variationally autoencode the hierarchy of coarsened graphs. Our proposed
framework is end-to-end permutation equivariant with respect to node ordering.
Our methods have been successful with several generative tasks including link
prediction on citation graphs, unsupervised molecular representation learning
to predict molecular properties, molecular generation, general graph generation
and graph-based image generation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hy_T/0/1/0/all/0/1"&gt;Truong Son Hy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kondor_R/0/1/0/all/0/1"&gt;Risi Kondor&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Generalizable Approach to Learning Optimizers. (arXiv:2106.00958v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.00958</id>
        <link href="http://arxiv.org/abs/2106.00958"/>
        <updated>2021-06-03T02:10:36.010Z</updated>
        <summary type="html"><![CDATA[A core issue with learning to optimize neural networks has been the lack of
generalization to real world problems. To address this, we describe a system
designed from a generalization-first perspective, learning to update optimizer
hyperparameters instead of model parameters directly using novel features,
actions, and a reward function. This system outperforms Adam at all neural
network tasks including on modalities not seen during training. We achieve 2x
speedups on ImageNet, and a 2.5x speedup on a language modeling task using over
5 orders of magnitude more compute than the training tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Almeida_D/0/1/0/all/0/1"&gt;Diogo Almeida&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Winter_C/0/1/0/all/0/1"&gt;Clemens Winter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1"&gt;Jie Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zaremba_W/0/1/0/all/0/1"&gt;Wojciech Zaremba&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[End-to-End Information Extraction by Character-Level Embedding and Multi-Stage Attentional U-Net. (arXiv:2106.00952v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.00952</id>
        <link href="http://arxiv.org/abs/2106.00952"/>
        <updated>2021-06-03T02:10:36.004Z</updated>
        <summary type="html"><![CDATA[Information extraction from document images has received a lot of attention
recently, due to the need for digitizing a large volume of unstructured
documents such as invoices, receipts, bank transfers, etc. In this paper, we
propose a novel deep learning architecture for end-to-end information
extraction on the 2D character-grid embedding of the document, namely the
\textit{Multi-Stage Attentional U-Net}. To effectively capture the textual and
spatial relations between 2D elements, our model leverages a specialized
multi-stage encoder-decoders design, in conjunction with efficient uses of the
self-attention mechanism and the box convolution. Experimental results on
different datasets show that our model outperforms the baseline U-Net
architecture by a large margin while using 40\% fewer parameters. Moreover, it
also significantly improved the baseline in erroneous OCR and limited training
data scenario, thus becomes practical for real-world applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dang_T/0/1/0/all/0/1"&gt;Tuan-Anh Nguyen Dang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1"&gt;Dat-Thanh Nguyen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Weighting vectors for machine learning: numerical harmonic analysis applied to boundary detection. (arXiv:2106.00827v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.00827</id>
        <link href="http://arxiv.org/abs/2106.00827"/>
        <updated>2021-06-03T02:10:35.999Z</updated>
        <summary type="html"><![CDATA[Metric space magnitude, an active field of research in algebraic topology, is
a scalar quantity that summarizes the effective number of distinct points that
live in a general metric space. The {\em weighting vector} is a closely-related
concept that captures, in a nontrivial way, much of the underlying geometry of
the original metric space. Recent work has demonstrated that when the metric
space is Euclidean, the weighting vector serves as an effective tool for
boundary detection. We recast this result and show the weighting vector may be
viewed as a solution to a kernelized SVM. As one consequence, we apply this new
insight to the task of outlier detection, and we demonstrate performance that
is competitive or exceeds performance of state-of-the-art techniques on
benchmark data sets. Under mild assumptions, we show the weighting vector,
which has computational cost of matrix inversion, can be efficiently
approximated in linear time. We show how nearest neighbor methods can
approximate solutions to the minimization problems defined by SVMs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bunch_E/0/1/0/all/0/1"&gt;Eric Bunch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kline_J/0/1/0/all/0/1"&gt;Jeffery Kline&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dickinson_D/0/1/0/all/0/1"&gt;Daniel Dickinson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhat_S/0/1/0/all/0/1"&gt;Suhaas Bhat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fung_G/0/1/0/all/0/1"&gt;Glenn Fung&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Generalized Mean Densest Subgraph Problem. (arXiv:2106.00909v1 [cs.DS])]]></title>
        <id>http://arxiv.org/abs/2106.00909</id>
        <link href="http://arxiv.org/abs/2106.00909"/>
        <updated>2021-06-03T02:10:35.984Z</updated>
        <summary type="html"><![CDATA[Finding dense subgraphs of a large graph is a standard problem in graph
mining that has been studied extensively both for its theoretical richness and
its many practical applications. In this paper we introduce a new family of
dense subgraph objectives, parameterized by a single parameter $p$, based on
computing generalized means of degree sequences of a subgraph. Our objective
captures both the standard densest subgraph problem and the maximum $k$-core as
special cases, and provides a way to interpolate between and extrapolate beyond
these two objectives when searching for other notions of dense subgraphs. In
terms of algorithmic contributions, we first show that our objective can be
minimized in polynomial time for all $p \geq 1$ using repeated submodular
minimization. A major contribution of our work is analyzing the performance of
different types of peeling algorithms for dense subgraphs both in theory and
practice. We prove that the standard peeling algorithm can perform arbitrarily
poorly on our generalized objective, but we then design a more sophisticated
peeling method which for $p \geq 1$ has an approximation guarantee that is
always at least $1/2$ and converges to 1 as $p \rightarrow \infty$. In
practice, we show that this algorithm obtains extremely good approximations to
the optimal solution, scales to large graphs, and highlights a range of
different meaningful notions of density on graphs coming from numerous domains.
Furthermore, it is typically able to approximate the densest subgraph problem
better than the standard peeling algorithm, by better accounting for how the
removal of one node affects other nodes in its neighborhood.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Veldt_N/0/1/0/all/0/1"&gt;Nate Veldt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Benson_A/0/1/0/all/0/1"&gt;Austin R. Benson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kleinberg_J/0/1/0/all/0/1"&gt;Jon Kleinberg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Energy-aware placement optimization of UAV base stations via decentralized multi-agent Q-learning. (arXiv:2106.00845v1 [cs.MA])]]></title>
        <id>http://arxiv.org/abs/2106.00845</id>
        <link href="http://arxiv.org/abs/2106.00845"/>
        <updated>2021-06-03T02:10:35.979Z</updated>
        <summary type="html"><![CDATA[Unmanned aerial vehicles serving as aerial base stations (UAV-BSs) can be
deployed to provide wireless connectivity to ground devices in events of
increased network demand, points-of-failure in existing infrastructure, or
disasters. However, it is challenging to conserve the energy of UAVs during
prolonged coverage tasks, considering their limited on-board battery capacity.
Reinforcement learning-based (RL) approaches have been previously used to
improve energy utilization of multiple UAVs, however, a central cloud
controller is assumed to have complete knowledge of the end-devices' locations,
i.e., the controller periodically scans and sends updates for UAV
decision-making. This assumption is impractical in dynamic network environments
with mobile ground devices. To address this problem, we propose a decentralized
Q-learning approach, where each UAV-BS is equipped with an autonomous agent
that maximizes the connectivity to ground devices while improving its energy
utilization. Experimental results show that the proposed design significantly
outperforms the centralized approaches in jointly maximizing the number of
connected ground devices and the energy utilization of the UAV-BSs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Omoniwa_B/0/1/0/all/0/1"&gt;Babatunji Omoniwa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Galkin_B/0/1/0/all/0/1"&gt;Boris Galkin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dusparic_I/0/1/0/all/0/1"&gt;Ivana Dusparic&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GraphDF: A Discrete Flow Model for Molecular Graph Generation. (arXiv:2102.01189v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.01189</id>
        <link href="http://arxiv.org/abs/2102.01189"/>
        <updated>2021-06-03T02:10:35.967Z</updated>
        <summary type="html"><![CDATA[We consider the problem of molecular graph generation using deep models.
While graphs are discrete, most existing methods use continuous latent
variables, resulting in inaccurate modeling of discrete graph structures. In
this work, we propose GraphDF, a novel discrete latent variable model for
molecular graph generation based on normalizing flow methods. GraphDF uses
invertible modulo shift transforms to map discrete latent variables to graph
nodes and edges. We show that the use of discrete latent variables reduces
computational costs and eliminates the negative effect of dequantization.
Comprehensive experimental results show that GraphDF outperforms prior methods
on random generation, property optimization, and constrained optimization
tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1"&gt;Youzhi Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_K/0/1/0/all/0/1"&gt;Keqiang Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1"&gt;Shuiwang Ji&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[QLSD: Quantised Langevin stochastic dynamics for Bayesian federated learning. (arXiv:2106.00797v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.00797</id>
        <link href="http://arxiv.org/abs/2106.00797"/>
        <updated>2021-06-03T02:10:35.961Z</updated>
        <summary type="html"><![CDATA[Federated learning aims at conducting inference when data are decentralised
and locally stored on several clients, under two main constraints: data
ownership and communication overhead. In this paper, we address these issues
under the Bayesian paradigm. To this end, we propose a novel Markov chain Monte
Carlo algorithm coined \texttt{QLSD} built upon quantised versions of
stochastic gradient Langevin dynamics. To improve performance in a big data
regime, we introduce variance-reduced alternatives of our methodology referred
to as \texttt{QLSD}$^\star$ and \texttt{QLSD}$^{++}$. We provide both
non-asymptotic and asymptotic convergence guarantees for the proposed
algorithms and illustrate their benefits on several federated learning
benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vono_M/0/1/0/all/0/1"&gt;Maxime Vono&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Plassier_V/0/1/0/all/0/1"&gt;Vincent Plassier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Durmus_A/0/1/0/all/0/1"&gt;Alain Durmus&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dieuleveut_A/0/1/0/all/0/1"&gt;Aymeric Dieuleveut&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moulines_E/0/1/0/all/0/1"&gt;Eric Moulines&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Online and Real-Time Tracking in a Surveillance Scenario. (arXiv:2106.01153v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.01153</id>
        <link href="http://arxiv.org/abs/2106.01153"/>
        <updated>2021-06-03T02:10:35.898Z</updated>
        <summary type="html"><![CDATA[This paper presents an approach for tracking in a surveillance scenario.
Typical aspects for this scenario are a 24/7 operation with a static camera
mounted above the height of a human with many objects or people. The Multiple
Object Tracking Benchmark 20 (MOT20) reflects this scenario best. We can show
that our approach is real-time capable on this benchmark and outperforms all
other real-time capable approaches in HOTA, MOTA, and IDF1. We achieve this by
contributing a fast Siamese network reformulated for linear runtime (instead of
quadratic) to generate fingerprints from detections. Thus, it is possible to
associate the detections to Kalman filters based on multiple tracking specific
ratings: Cosine similarity of fingerprints, Intersection over Union, and pixel
distance ratio in the image.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Urbann_O/0/1/0/all/0/1"&gt;Oliver Urbann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bredtmann_O/0/1/0/all/0/1"&gt;Oliver Bredtmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Otten_M/0/1/0/all/0/1"&gt;Maximilian Otten&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Richter_J/0/1/0/all/0/1"&gt;Jan-Philip Richter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bauer_T/0/1/0/all/0/1"&gt;Thilo Bauer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zibriczky_D/0/1/0/all/0/1"&gt;David Zibriczky&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Differentiable Point Process with Its Application to Spiking Neural Networks. (arXiv:2106.00901v1 [cs.NE])]]></title>
        <id>http://arxiv.org/abs/2106.00901</id>
        <link href="http://arxiv.org/abs/2106.00901"/>
        <updated>2021-06-03T02:10:35.856Z</updated>
        <summary type="html"><![CDATA[This paper is concerned about a learning algorithm for a probabilistic model
of spiking neural networks (SNNs). Jimenez Rezende & Gerstner (2014) proposed a
stochastic variational inference algorithm to train SNNs with hidden neurons.
The algorithm updates the variational distribution using the score function
gradient estimator, whose high variance often impedes the whole learning
algorithm. This paper presents an alternative gradient estimator for SNNs based
on the path-wise gradient estimator. The main technical difficulty is a lack of
a general method to differentiate a realization of an arbitrary point process,
which is necessary to derive the path-wise gradient estimator. We develop a
differentiable point process, which is the technical highlight of this paper,
and apply it to derive the path-wise gradient estimator for SNNs. We
investigate the effectiveness of our gradient estimator through numerical
simulation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kajino_H/0/1/0/all/0/1"&gt;Hiroshi Kajino&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Prediction of the Position of External Markers Using a Recurrent Neural Network Trained With Unbiased Online Recurrent Optimization for Safe Lung Cancer Radiotherapy. (arXiv:2106.01100v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.01100</id>
        <link href="http://arxiv.org/abs/2106.01100"/>
        <updated>2021-06-03T02:10:35.827Z</updated>
        <summary type="html"><![CDATA[During lung cancer radiotherapy, the position of infrared reflective objects
on the chest can be recorded to estimate the tumor location. However,
radiotherapy systems usually have a latency inherent to robot control
limitations that impedes the radiation delivery precision. Not taking this
phenomenon into account may cause unwanted damage to healthy tissues and lead
to side effects such as radiation pneumonitis. In this research, we use nine
observation records of the three-dimensional position of three external markers
on the chest and abdomen of healthy individuals breathing during intervals from
73s to 222s. The sampling frequency is equal to 10Hz and the amplitudes of the
recorded trajectories range from 6mm to 40mm in the superior-inferior
direction. We forecast the location of each marker simultaneously with a
horizon value (the time interval in advance for which the prediction is made)
between 0.1s and 2.0s, using a recurrent neural network (RNN) trained with
unbiased online recurrent optimization (UORO). We compare its performance with
an RNN trained with real-time recurrent learning, least mean squares (LMS), and
offline linear regression. Training and cross-validation are performed during
the first minute of each sequence. On average, UORO achieves the lowest
root-mean-square (RMS) and maximum error, equal respectively to 1.3mm and
8.8mm, with a prediction time per time step lower than 2.8ms (Dell Intel core
i9-9900K 3.60Ghz). Linear regression has the lowest RMS error for the horizon
values 0.1s and 0.2s, followed by LMS for horizon values between 0.3s and 0.5s,
and UORO for horizon values greater than 0.6s.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Pohl_M/0/1/0/all/0/1"&gt;Michel Pohl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Uesaka_M/0/1/0/all/0/1"&gt;Mitsuru Uesaka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Takahashi_H/0/1/0/all/0/1"&gt;Hiroyuki Takahashi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Demachi_K/0/1/0/all/0/1"&gt;Kazuyuki Demachi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chhatkuli_R/0/1/0/all/0/1"&gt;Ritu Bhusal Chhatkuli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Cluster-based Approach for Improving Isotropy in Contextual Embedding Space. (arXiv:2106.01183v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01183</id>
        <link href="http://arxiv.org/abs/2106.01183"/>
        <updated>2021-06-03T02:10:35.822Z</updated>
        <summary type="html"><![CDATA[The representation degeneration problem in Contextual Word Representations
(CWRs) hurts the expressiveness of the embedding space by forming an
anisotropic cone where even unrelated words have excessively positive
correlations. Existing techniques for tackling this issue require a learning
process to re-train models with additional objectives and mostly employ a
global assessment to study isotropy. Our quantitative analysis over isotropy
shows that a local assessment could be more accurate due to the clustered
structure of CWRs. Based on this observation, we propose a local cluster-based
method to address the degeneration issue in contextual embedding spaces. We
show that in clusters including punctuations and stop words, local dominant
directions encode structural information, removing which can improve CWRs
performance on semantic tasks. Moreover, we find that tense information in verb
representations dominates sense semantics. We show that removing dominant
directions of verb representations can transform the space to better suit
semantic applications. Our experiments demonstrate that the proposed
cluster-based method can mitigate the degeneration problem on multiple tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rajaee_S/0/1/0/all/0/1"&gt;Sara Rajaee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pilehvar_M/0/1/0/all/0/1"&gt;Mohammad Taher Pilehvar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Thorough View of Exact Inference in Graphs from the Degree-4 Sum-of-Squares Hierarchy. (arXiv:2102.08019v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.08019</id>
        <link href="http://arxiv.org/abs/2102.08019"/>
        <updated>2021-06-03T02:10:35.817Z</updated>
        <summary type="html"><![CDATA[Performing inference in graphs is a common task within several machine
learning problems, e.g., image segmentation, community detection, among others.
For a given undirected connected graph, we tackle the statistical problem of
exactly recovering an unknown ground-truth binary labeling of the nodes from a
single corrupted observation of each edge. Such problem can be formulated as a
quadratic combinatorial optimization problem over the boolean hypercube, where
it has been shown before that one can (with high probability and in polynomial
time) exactly recover the ground-truth labeling of graphs that have an
isoperimetric number that grows with respect to the number of nodes (e.g.,
complete graphs, regular expanders). In this work, we apply a powerful
hierarchy of relaxations, known as the sum-of-squares (SoS) hierarchy, to the
combinatorial problem. Motivated by empirical evidence on the improvement in
exact recoverability, we center our attention on the degree-4 SoS relaxation
and set out to understand the origin of such improvement from a graph
theoretical perspective. We show that the solution of the dual of the relaxed
problem is related to finding edge weights of the Johnson and Kneser graphs,
where the weights fulfill the SoS constraints and intuitively allow the input
graph to increase its algebraic connectivity. Finally, as byproduct of our
analysis, we derive a novel Cheeger-type lower bound for the algebraic
connectivity of graphs with signed edge weights.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bello_K/0/1/0/all/0/1"&gt;Kevin Bello&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ke_C/0/1/0/all/0/1"&gt;Chuyang Ke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Honorio_J/0/1/0/all/0/1"&gt;Jean Honorio&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Enhanced Universal Dependency Parsing with Second-Order Inference and Mixture of Training Data. (arXiv:2006.01414v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.01414</id>
        <link href="http://arxiv.org/abs/2006.01414"/>
        <updated>2021-06-03T02:10:35.812Z</updated>
        <summary type="html"><![CDATA[This paper presents the system used in our submission to the \textit{IWPT
2020 Shared Task}. Our system is a graph-based parser with second-order
inference. For the low-resource Tamil corpus, we specially mixed the training
data of Tamil with other languages and significantly improved the performance
of Tamil. Due to our misunderstanding of the submission requirements, we
submitted graphs that are not connected, which makes our system only rank
\textbf{6th} over 10 teams. However, after we fixed this problem, our system is
0.6 ELAS higher than the team that ranked \textbf{1st} in the official results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xinyu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1"&gt;Yong Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tu_K/0/1/0/all/0/1"&gt;Kewei Tu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Elaborative Simplification: Content Addition and Explanation Generation in Text Simplification. (arXiv:2010.10035v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.10035</id>
        <link href="http://arxiv.org/abs/2010.10035"/>
        <updated>2021-06-03T02:10:35.797Z</updated>
        <summary type="html"><![CDATA[Much of modern-day text simplification research focuses on sentence-level
simplification, transforming original, more complex sentences into simplified
versions. However, adding content can often be useful when difficult concepts
and reasoning need to be explained. In this work, we present the first
data-driven study of content addition in text simplification, which we call
elaborative simplification. We introduce a new annotated dataset of 1.3K
instances of elaborative simplification in the Newsela corpus, and analyze how
entities, ideas, and concepts are elaborated through the lens of contextual
specificity. We establish baselines for elaboration generation using
large-scale pre-trained language models, and demonstrate that considering
contextual specificity during generation can improve performance. Our results
illustrate the complexities of elaborative simplification, suggesting many
interesting directions for future work.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Srikanth_N/0/1/0/all/0/1"&gt;Neha Srikanth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Junyi Jessy Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Emotional Support Dialog Systems. (arXiv:2106.01144v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01144</id>
        <link href="http://arxiv.org/abs/2106.01144"/>
        <updated>2021-06-03T02:10:35.792Z</updated>
        <summary type="html"><![CDATA[Emotional support is a crucial ability for many conversation scenarios,
including social interactions, mental health support, and customer service
chats. Following reasonable procedures and using various support skills can
help to effectively provide support. However, due to the lack of a
well-designed task and corpora of effective emotional support conversations,
research on building emotional support into dialog systems remains untouched.
In this paper, we define the Emotional Support Conversation (ESC) task and
propose an ESC Framework, which is grounded on the Helping Skills Theory. We
construct an Emotion Support Conversation dataset (ESConv) with rich annotation
(especially support strategy) in a help-seeker and supporter mode. To ensure a
corpus of high-quality conversations that provide examples of effective
emotional support, we take extensive effort to design training tutorials for
supporters and several mechanisms for quality control during data collection.
Finally, we evaluate state-of-the-art dialog models with respect to the ability
to provide emotional support. Our results show the importance of support
strategies in providing effective emotional support and the utility of ESConv
in training more emotional support systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Siyang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1"&gt;Chujie Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Demasi_O/0/1/0/all/0/1"&gt;Orianna Demasi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sabour_S/0/1/0/all/0/1"&gt;Sahand Sabour&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1"&gt;Zhou Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1"&gt;Yong Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1"&gt;Minlie Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Named Entity Recognition by External Context Retrieving and Cooperative Learning. (arXiv:2105.03654v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.03654</id>
        <link href="http://arxiv.org/abs/2105.03654"/>
        <updated>2021-06-03T02:10:35.788Z</updated>
        <summary type="html"><![CDATA[Recent advances in Named Entity Recognition (NER) show that document-level
contexts can significantly improve model performance. In many application
scenarios, however, such contexts are not available. In this paper, we propose
to find external contexts of a sentence by retrieving and selecting a set of
semantically relevant texts through a search engine, with the original sentence
as the query. We find empirically that the contextual representations computed
on the retrieval-based input view, constructed through the concatenation of a
sentence and its external contexts, can achieve significantly improved
performance compared to the original input view based only on the sentence.
Furthermore, we can improve the model performance of both input views by
Cooperative Learning, a training method that encourages the two input views to
produce similar contextual representations or output label distributions.
Experiments show that our approach can achieve new state-of-the-art performance
on 8 NER data sets across 5 domains.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xinyu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1"&gt;Yong Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bach_N/0/1/0/all/0/1"&gt;Nguyen Bach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1"&gt;Tao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zhongqiang Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1"&gt;Fei Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tu_K/0/1/0/all/0/1"&gt;Kewei Tu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Parametrization invariant interpretation of priors and posteriors. (arXiv:2105.08304v2 [math.ST] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.08304</id>
        <link href="http://arxiv.org/abs/2105.08304"/>
        <updated>2021-06-03T02:10:35.782Z</updated>
        <summary type="html"><![CDATA[In this paper we leverage on probability over Riemannian manifolds to rethink
the interpretation of priors and posteriors in Bayesian inference. The main
mindshift is to move away from the idea that "a prior distribution establishes
a probability distribution over the parameters of our model" to the idea that
"a prior distribution establishes a probability distribution over probability
distributions". To do that we assume that our probabilistic model is a
Riemannian manifold with the Fisher metric. Under this mindset, any
distribution over probability distributions should be "intrinsic", that is,
invariant to the specific parametrization which is selected for the manifold.
We exemplify our ideas through a simple analysis of distributions over the
manifold of Bernoulli distributions. One of the major shortcomings of maximum a
posteriori estimates is that they depend on the parametrization. Based on the
understanding developed here, we can define the maximum a posteriori estimate
which is independent of the parametrization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Cerquides_J/0/1/0/all/0/1"&gt;Jesus Cerquides&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deterministic Variational Inference for Neural SDEs. (arXiv:2006.08973v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.08973</id>
        <link href="http://arxiv.org/abs/2006.08973"/>
        <updated>2021-06-03T02:10:35.777Z</updated>
        <summary type="html"><![CDATA[Neural Stochastic Differential Equations (NSDEs) model the drift and
diffusion functions of a stochastic process as neural networks. While NSDEs are
known to predict time series accurately, their uncertainty quantification
properties remain unexplored. Currently, there are no approximate inference
methods, which allow flexible models and provide at the same time high quality
uncertainty estimates at a reasonable computational cost. Existing SDE
inference methods either make overly restrictive assumptions, e.g. linearity,
or rely on Monte Carlo integration that requires many samples at prediction
time for reliable uncertainty quantification. However, many real-world safety
critical applications necessitate highly expressive models that can quantify
prediction uncertainty at affordable computational cost. We introduce a
variational inference scheme that approximates the posterior distribution of a
NSDE governing a latent state space by a deterministic chain of operations. We
approximate the intractable data fit term of the evidence lower bound by a
novel bidimensional moment matching algorithm: vertical along the neural net
layers and horizontal along the time direction. Our algorithm achieves
uncertainty calibration scores that can be matched by its sampling-based
counterparts only at significantly higher computation cost, while providing as
accurate forecasts on system dynamics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Look_A/0/1/0/all/0/1"&gt;Andreas Look&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kandemir_M/0/1/0/all/0/1"&gt;Melih Kandemir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peters_J/0/1/0/all/0/1"&gt;Jan Peters&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Leveraging Abstract Meaning Representation for Knowledge Base Question Answering. (arXiv:2012.01707v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.01707</id>
        <link href="http://arxiv.org/abs/2012.01707"/>
        <updated>2021-06-03T02:10:35.772Z</updated>
        <summary type="html"><![CDATA[Knowledge base question answering (KBQA)is an important task in Natural
Language Processing. Existing approaches face significant challenges including
complex question understanding, necessity for reasoning, and lack of large
end-to-end training datasets. In this work, we propose Neuro-Symbolic Question
Answering (NSQA), a modular KBQA system, that leverages (1) Abstract Meaning
Representation (AMR) parses for task-independent question understanding; (2) a
simple yet effective graph transformation approach to convert AMR parses into
candidate logical queries that are aligned to the KB; (3) a pipeline-based
approach which integrates multiple, reusable modules that are trained
specifically for their individual tasks (semantic parser, entity
andrelationship linkers, and neuro-symbolic reasoner) and do not require
end-to-end training data. NSQA achieves state-of-the-art performance on two
prominent KBQA datasets based on DBpedia (QALD-9 and LC-QuAD1.0). Furthermore,
our analysis emphasizes that AMR is a powerful tool for KBQA systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kapanipathi_P/0/1/0/all/0/1"&gt;Pavan Kapanipathi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abdelaziz_I/0/1/0/all/0/1"&gt;Ibrahim Abdelaziz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ravishankar_S/0/1/0/all/0/1"&gt;Srinivas Ravishankar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roukos_S/0/1/0/all/0/1"&gt;Salim Roukos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gray_A/0/1/0/all/0/1"&gt;Alexander Gray&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Astudillo_R/0/1/0/all/0/1"&gt;Ramon Astudillo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_M/0/1/0/all/0/1"&gt;Maria Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cornelio_C/0/1/0/all/0/1"&gt;Cristina Cornelio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dana_S/0/1/0/all/0/1"&gt;Saswati Dana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fokoue_A/0/1/0/all/0/1"&gt;Achille Fokoue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Garg_D/0/1/0/all/0/1"&gt;Dinesh Garg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gliozzo_A/0/1/0/all/0/1"&gt;Alfio Gliozzo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gurajada_S/0/1/0/all/0/1"&gt;Sairam Gurajada&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karanam_H/0/1/0/all/0/1"&gt;Hima Karanam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khan_N/0/1/0/all/0/1"&gt;Naweed Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khandelwal_D/0/1/0/all/0/1"&gt;Dinesh Khandelwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1"&gt;Young-Suk Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yunyao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luus_F/0/1/0/all/0/1"&gt;Francois Luus&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Makondo_N/0/1/0/all/0/1"&gt;Ndivhuwo Makondo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mihindukulasooriya_N/0/1/0/all/0/1"&gt;Nandana Mihindukulasooriya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Naseem_T/0/1/0/all/0/1"&gt;Tahira Naseem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neelam_S/0/1/0/all/0/1"&gt;Sumit Neelam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Popa_L/0/1/0/all/0/1"&gt;Lucian Popa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reddy_R/0/1/0/all/0/1"&gt;Revanth Reddy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Riegel_R/0/1/0/all/0/1"&gt;Ryan Riegel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rossiello_G/0/1/0/all/0/1"&gt;Gaetano Rossiello&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_U/0/1/0/all/0/1"&gt;Udit Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhargav_G/0/1/0/all/0/1"&gt;G P Shrivatsa Bhargav&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_M/0/1/0/all/0/1"&gt;Mo Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Semi-Supervised iNaturalist Challenge at the FGVC8 Workshop. (arXiv:2106.01364v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.01364</id>
        <link href="http://arxiv.org/abs/2106.01364"/>
        <updated>2021-06-03T02:10:35.755Z</updated>
        <summary type="html"><![CDATA[Semi-iNat is a challenging dataset for semi-supervised classification with a
long-tailed distribution of classes, fine-grained categories, and domain shifts
between labeled and unlabeled data. This dataset is behind the second iteration
of the semi-supervised recognition challenge to be held at the FGVC8 workshop
at CVPR 2021. Different from the previous one, this dataset (i) includes images
of species from different kingdoms in the natural taxonomy, (ii) is at a larger
scale --- with 810 in-class and 1629 out-of-class species for a total of 330k
images, and (iii) does not provide in/out-of-class labels, but provides coarse
taxonomic labels (kingdom and phylum) for the unlabeled images. This document
describes baseline results and the details of the dataset which is available
here: \url{https://github.com/cvl-umass/semi-inat-2021}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1"&gt;Jong-Chyi Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maji_S/0/1/0/all/0/1"&gt;Subhransu Maji&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Complex Momentum for Optimization in Games. (arXiv:2102.08431v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.08431</id>
        <link href="http://arxiv.org/abs/2102.08431"/>
        <updated>2021-06-03T02:10:35.747Z</updated>
        <summary type="html"><![CDATA[We generalize gradient descent with momentum for optimization in
differentiable games to have complex-valued momentum. We give theoretical
motivation for our method by proving convergence on bilinear zero-sum games for
simultaneous and alternating updates. Our method gives real-valued parameter
updates, making it a drop-in replacement for standard optimizers. We
empirically demonstrate that complex-valued momentum can improve convergence in
realistic adversarial games - like generative adversarial networks - by showing
we can find better solutions with an almost identical computational cost. We
also show a practical generalization to a complex-valued Adam variant, which we
use to train BigGAN to better inception scores on CIFAR-10.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lorraine_J/0/1/0/all/0/1"&gt;Jonathan Lorraine&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Acuna_D/0/1/0/all/0/1"&gt;David Acuna&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vicol_P/0/1/0/all/0/1"&gt;Paul Vicol&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duvenaud_D/0/1/0/all/0/1"&gt;David Duvenaud&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Online Continual Learning in Image Classification: An Empirical Survey. (arXiv:2101.10423v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.10423</id>
        <link href="http://arxiv.org/abs/2101.10423"/>
        <updated>2021-06-03T02:10:35.742Z</updated>
        <summary type="html"><![CDATA[Online continual learning for image classification studies the problem of
learning to classify images from an online stream of data and tasks, where
tasks may include new classes (class incremental) or data nonstationarity
(domain incremental). One of the key challenges of continual learning is to
avoid catastrophic forgetting (CF), i.e., forgetting old tasks in the presence
of more recent tasks. Over the past few years, many methods and tricks have
been introduced to address this problem, but many have not been fairly and
systematically compared under a variety of realistic and practical settings. To
better understand the relative advantages of various approaches and the
settings where they work best, this survey aims to (1) compare state-of-the-art
methods such as MIR, iCARL, and GDumb and determine which works best at
different experimental settings; (2) determine if the best class incremental
methods are also competitive in domain incremental setting; (3) evaluate the
performance of 7 simple but effective trick such as "review" trick and nearest
class mean (NCM) classifier to assess their relative impact. Regarding (1), we
observe iCaRL remains competitive when the memory buffer is small; GDumb
outperforms many recently proposed methods in medium-size datasets and MIR
performs the best in larger-scale datasets. For (2), we note that GDumb
performs quite poorly while MIR -- already competitive for (1) -- is also
strongly competitive in this very different but important setting. Overall,
this allows us to conclude that MIR is overall a strong and versatile method
across a wide variety of settings. For (3), we find that all 7 tricks are
beneficial, and when augmented with the "review" trick and NCM classifier, MIR
produces performance levels that bring online continual learning much closer to
its ultimate goal of matching offline training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mai_Z/0/1/0/all/0/1"&gt;Zheda Mai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1"&gt;Ruiwen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jeong_J/0/1/0/all/0/1"&gt;Jihwan Jeong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Quispe_D/0/1/0/all/0/1"&gt;David Quispe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1"&gt;Hyunwoo Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sanner_S/0/1/0/all/0/1"&gt;Scott Sanner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Topic-Driven and Knowledge-Aware Transformer for Dialogue Emotion Detection. (arXiv:2106.01071v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01071</id>
        <link href="http://arxiv.org/abs/2106.01071"/>
        <updated>2021-06-03T02:10:35.737Z</updated>
        <summary type="html"><![CDATA[Emotion detection in dialogues is challenging as it often requires the
identification of thematic topics underlying a conversation, the relevant
commonsense knowledge, and the intricate transition patterns between the
affective states. In this paper, we propose a Topic-Driven Knowledge-Aware
Transformer to handle the challenges above. We firstly design a topic-augmented
language model (LM) with an additional layer specialized for topic detection.
The topic-augmented LM is then combined with commonsense statements derived
from a knowledge base based on the dialogue contextual information. Finally, a
transformer-based encoder-decoder architecture fuses the topical and
commonsense information, and performs the emotion label sequence prediction.
The model has been experimented on four datasets in dialogue emotion detection,
demonstrating its superiority empirically over the existing state-of-the-art
approaches. Quantitative and qualitative results show that the model can
discover topics which help in distinguishing emotion categories.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1"&gt;Lixing Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pergola_G/0/1/0/all/0/1"&gt;Gabriele Pergola&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gui_L/0/1/0/all/0/1"&gt;Lin Gui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1"&gt;Deyu Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1"&gt;Yulan He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multilayer Network Analysis for Improved Credit Risk Prediction. (arXiv:2010.09559v3 [cs.SI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.09559</id>
        <link href="http://arxiv.org/abs/2010.09559"/>
        <updated>2021-06-03T02:10:35.732Z</updated>
        <summary type="html"><![CDATA[We present a multilayer network model for credit risk assessment. Our model
accounts for multiple connections between borrowers (such as their geographic
location and their economic activity) and allows for explicitly modelling the
interaction between connected borrowers. We develop a multilayer personalized
PageRank algorithm that allows quantifying the strength of the default exposure
of any borrower in the network. We test our methodology in an agricultural
lending framework, where it has been suspected for a long time default
correlates between borrowers when they are subject to the same structural
risks. Our results show there are significant predictive gains just by
including centrality multilayer network information in the model, and these
gains are increased by more complex information such as the multilayer PageRank
variables. The results suggest default risk is highest when an individual is
connected to many defaulters, but this risk is mitigated by the size of the
neighbourhood of the individual, showing both default risk and financial
stability propagate throughout the network.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Oskarsdottir_M/0/1/0/all/0/1"&gt;Mar&amp;#xed;a &amp;#xd3;skarsd&amp;#xf3;ttir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bravo_C/0/1/0/all/0/1"&gt;Cristi&amp;#xe1;n Bravo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Representation Learning for Time Series with Temporal Neighborhood Coding. (arXiv:2106.00750v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.00750</id>
        <link href="http://arxiv.org/abs/2106.00750"/>
        <updated>2021-06-03T02:10:35.717Z</updated>
        <summary type="html"><![CDATA[Time series are often complex and rich in information but sparsely labeled
and therefore challenging to model. In this paper, we propose a self-supervised
framework for learning generalizable representations for non-stationary time
series. Our approach, called Temporal Neighborhood Coding (TNC), takes
advantage of the local smoothness of a signal's generative process to define
neighborhoods in time with stationary properties. Using a debiased contrastive
objective, our framework learns time series representations by ensuring that in
the encoding space, the distribution of signals from within a neighborhood is
distinguishable from the distribution of non-neighboring signals. Our
motivation stems from the medical field, where the ability to model the dynamic
nature of time series data is especially valuable for identifying, tracking,
and predicting the underlying patients' latent states in settings where
labeling data is practically impossible. We compare our method to recently
developed unsupervised representation learning approaches and demonstrate
superior performance on clustering and classification tasks for multiple
datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tonekaboni_S/0/1/0/all/0/1"&gt;Sana Tonekaboni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eytan_D/0/1/0/all/0/1"&gt;Danny Eytan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldenberg_A/0/1/0/all/0/1"&gt;Anna Goldenberg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robustifying Algorithms of Learning Latent Trees with Vector Variables. (arXiv:2106.00885v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.00885</id>
        <link href="http://arxiv.org/abs/2106.00885"/>
        <updated>2021-06-03T02:10:35.711Z</updated>
        <summary type="html"><![CDATA[We consider learning the structures of Gaussian latent tree models with
vector observations when a subset of them are arbitrarily corrupted. First, we
present the sample complexities of Recursive Grouping (RG) and Chow-Liu
Recursive Grouping (CLRG) without the assumption that the effective depth is
bounded in the number of observed nodes, significantly generalizing the results
in Choi et al. (2011). We show that Chow-Liu initialization in CLRG greatly
reduces the sample complexity of RG from being exponential in the diameter of
the tree to only logarithmic in the diameter for the hidden Markov model (HMM).
Second, we robustify RG, CLRG, Neighbor Joining (NJ) and Spectral NJ (SNJ) by
using the truncated inner product. These robustified algorithms can tolerate a
number of corruptions up to the square root of the number of clean samples.
Finally, we derive the first known instance-dependent impossibility result for
structure learning of latent trees. The optimalities of the robust version of
CLRG and NJ are verified by comparing their sample complexities and the
impossibility result.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Zhang_F/0/1/0/all/0/1"&gt;Fengzhuo Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Tan_V/0/1/0/all/0/1"&gt;Vincent Y. F. Tan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predicting trends in the quality of state-of-the-art neural networks without access to training or testing data. (arXiv:2002.06716v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.06716</id>
        <link href="http://arxiv.org/abs/2002.06716"/>
        <updated>2021-06-03T02:10:35.706Z</updated>
        <summary type="html"><![CDATA[In many applications, one works with neural network models trained by someone
else. For such pretrained models, one may not have access to training data or
test data. Moreover, one may not know details about the model, e.g., the
specifics of the training data, the loss function, the hyperparameter values,
etc. Given one or many pretrained models, it is a challenge to say anything
about the expected performance or quality of the models. Here, we address this
challenge by providing a detailed meta-analysis of hundreds of
publicly-available pretrained models. We examine norm based capacity control
metrics as well as power law based metrics from the recently-developed Theory
of Heavy-Tailed Self Regularization. We find that norm based metrics correlate
well with reported test accuracies for well-trained models, but that they often
cannot distinguish well-trained versus poorly-trained models. We also find that
power law based metrics can do much better -- quantitatively better at
discriminating among series of well-trained models with a given architecture;
and qualitatively better at discriminating well-trained versus poorly-trained
models. These methods can be used to identify when a pretrained neural network
has problems that cannot be detected simply by examining training/test
accuracies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Martin_C/0/1/0/all/0/1"&gt;Charles H. Martin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tongsu/0/1/0/all/0/1"&gt;Tongsu&lt;/a&gt; (Serena) &lt;a href="http://arxiv.org/find/cs/1/au:+Peng/0/1/0/all/0/1"&gt;Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mahoney_M/0/1/0/all/0/1"&gt;Michael W. Mahoney&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Partial Wasserstein Covering. (arXiv:2106.00886v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.00886</id>
        <link href="http://arxiv.org/abs/2106.00886"/>
        <updated>2021-06-03T02:10:35.701Z</updated>
        <summary type="html"><![CDATA[We consider a general task called partial Wasserstein covering with the goal
of emulating a large dataset (e.g., application dataset) using a small dataset
(e.g., development dataset) in terms of the empirical distribution by selecting
a small subset from a candidate dataset and adding it to the small dataset. We
model this task as a discrete optimization problem with partial Wasserstein
divergence as an objective function. Although this problem is NP-hard, we prove
that it has the submodular property, allowing us to use a greedy algorithm with
a 0.63 approximation. However, the greedy algorithm is still inefficient
because it requires linear programming for each objective function evaluation.
To overcome this difficulty, we propose quasi-greedy algorithms for
acceleration, which consist of a series of techniques such as sensitivity
analysis based on strong duality and the so-called $C$-transform in the optimal
transport field. Experimentally, we demonstrate that we can efficiently make
two datasets similar in terms of partial Wasserstein divergence, including
driving scene datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kawano_K/0/1/0/all/0/1"&gt;Keisuke Kawano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koide_S/0/1/0/all/0/1"&gt;Satoshi Koide&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Otaki_K/0/1/0/all/0/1"&gt;Keisuke Otaki&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hyperdimensional Computing for Efficient Distributed Classification with Randomized Neural Networks. (arXiv:2106.00881v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.00881</id>
        <link href="http://arxiv.org/abs/2106.00881"/>
        <updated>2021-06-03T02:10:35.696Z</updated>
        <summary type="html"><![CDATA[In the supervised learning domain, considering the recent prevalence of
algorithms with high computational cost, the attention is steering towards
simpler, lighter, and less computationally extensive training and inference
approaches. In particular, randomized algorithms are currently having a
resurgence, given their generalized elementary approach. By using randomized
neural networks, we study distributed classification, which can be employed in
situations were data cannot be stored at a central location nor shared. We
propose a more efficient solution for distributed classification by making use
of a lossy compression approach applied when sharing the local classifiers with
other agents. This approach originates from the framework of hyperdimensional
computing, and is adapted herein. The results of experiments on a collection of
datasets demonstrate that the proposed approach has usually higher accuracy
than local classifiers and getting close to the benchmark - the centralized
classifier. This work can be considered as the first step towards analyzing the
variegated horizon of distributed randomized neural networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rosato_A/0/1/0/all/0/1"&gt;Antonello Rosato&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Panella_M/0/1/0/all/0/1"&gt;Massimo Panella&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kleyko_D/0/1/0/all/0/1"&gt;Denis Kleyko&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Digital homotopy relations and digital homology theories. (arXiv:2106.01171v1 [math.AT])]]></title>
        <id>http://arxiv.org/abs/2106.01171</id>
        <link href="http://arxiv.org/abs/2106.01171"/>
        <updated>2021-06-03T02:10:35.681Z</updated>
        <summary type="html"><![CDATA[In this paper we prove results relating to two homotopy relations and four
homology theories developed in the topology of digital images.

We introduce a new type of homotopy relation for digitally continuous
functions which we call "strong homotopy." Both digital homotopy and strong
homotopy are natural digitizations of classical topological homotopy: the
difference between them is analogous to the difference between digital
4-adjacency and 8-adjacency in the plane.

We also consider four different digital homology theories: a simplicial
homology theory by Arslan et al which is the homology of the clique complex, a
singular simplicial homology theory by D. W. Lee, a cubical homology theory by
Jamil and Ali, and a new kind of cubical homology for digital images with
$c_1$-adjacency which is easily computed, and generalizes a construction by
Karaca \& Ege. We show that the two simplicial homology theories are isomorphic
to each other, but distinct from the two cubical theories.

We also show that homotopic maps have the same induced homomorphisms in the
cubical homology theory, and strong homotopic maps additionally have the same
induced homomorphisms in the simplicial theory.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Staecker_P/0/1/0/all/0/1"&gt;P. Christopher Staecker&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Inter-GPS: Interpretable Geometry Problem Solving with Formal Language and Symbolic Reasoning. (arXiv:2105.04165v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.04165</id>
        <link href="http://arxiv.org/abs/2105.04165"/>
        <updated>2021-06-03T02:10:35.660Z</updated>
        <summary type="html"><![CDATA[Geometry problem solving has attracted much attention in the NLP community
recently. The task is challenging as it requires abstract problem understanding
and symbolic reasoning with axiomatic knowledge. However, current datasets are
either small in scale or not publicly available. Thus, we construct a new
large-scale benchmark, Geometry3K, consisting of 3,002 geometry problems with
dense annotation in formal language. We further propose a novel geometry
solving approach with formal language and symbolic reasoning, called
Interpretable Geometry Problem Solver (Inter-GPS). Inter-GPS first parses the
problem text and diagram into formal language automatically via rule-based text
parsing and neural object detecting, respectively. Unlike implicit learning in
existing methods, Inter-GPS incorporates theorem knowledge as conditional rules
and performs symbolic reasoning step by step. Also, a theorem predictor is
designed to infer the theorem application sequence fed to the symbolic solver
for the more efficient and reasonable searching path. Extensive experiments on
the Geometry3K and GEOS datasets demonstrate that Inter-GPS achieves
significant improvements over existing methods. The project with code and data
is available at https://lupantech.github.io/inter-gps.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lu_P/0/1/0/all/0/1"&gt;Pan Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_R/0/1/0/all/0/1"&gt;Ran Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1"&gt;Shibiao Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_L/0/1/0/all/0/1"&gt;Liang Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1"&gt;Siyuan Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1"&gt;Xiaodan Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1"&gt;Song-Chun Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FedHealth 2: Weighted Federated Transfer Learning via Batch Normalization for Personalized Healthcare. (arXiv:2106.01009v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01009</id>
        <link href="http://arxiv.org/abs/2106.01009"/>
        <updated>2021-06-03T02:10:35.654Z</updated>
        <summary type="html"><![CDATA[The success of machine learning applications often needs a large quantity of
data. Recently, federated learning (FL) is attracting increasing attention due
to the demand for data privacy and security, especially in the medical field.
However, the performance of existing FL approaches often deteriorates when
there exist domain shifts among clients, and few previous works focus on
personalization in healthcare. In this article, we propose FedHealth 2, an
extension of FedHealth \cite{chen2020fedhealth} to tackle domain shifts and get
personalized models for local clients. FedHealth 2 obtains the client
similarities via a pretrained model, and then it averages all weighted models
with preserving local batch normalization. Wearable activity recognition and
COVID-19 auxiliary diagnosis experiments have evaluated that FedHealth 2 can
achieve better accuracy (10%+ improvement for activity recognition) and
personalized healthcare without compromising privacy and security.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yiqiang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1"&gt;Wang Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jindong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_X/0/1/0/all/0/1"&gt;Xin Qin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pricing Algorithmic Insurance. (arXiv:2106.00839v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.00839</id>
        <link href="http://arxiv.org/abs/2106.00839"/>
        <updated>2021-06-03T02:10:35.649Z</updated>
        <summary type="html"><![CDATA[As machine learning algorithms start to get integrated into the
decision-making process of companies and organizations, insurance products will
be developed to protect their owners from risk. We introduce the concept of
algorithmic insurance and present a quantitative framework to enable the
pricing of the derived insurance contracts. We propose an optimization
formulation to estimate the risk exposure and price for a binary classification
model. Our approach outlines how properties of the model, such as accuracy,
interpretability and generalizability, can influence the insurance contract
evaluation. To showcase a practical implementation of the proposed framework,
we present a case study of medical malpractice in the context of breast cancer
detection. Our analysis focuses on measuring the effect of the model parameters
on the expected financial loss and identifying the aspects of algorithmic
performance that predominantly affect the price of the contract.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bertsimas_D/0/1/0/all/0/1"&gt;Dimitris Bertsimas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Orfanoudaki_A/0/1/0/all/0/1"&gt;Agni Orfanoudaki&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Can Sequence-to-Sequence Models Crack Substitution Ciphers?. (arXiv:2012.15229v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.15229</id>
        <link href="http://arxiv.org/abs/2012.15229"/>
        <updated>2021-06-03T02:10:35.644Z</updated>
        <summary type="html"><![CDATA[Decipherment of historical ciphers is a challenging problem. The language of
the target plaintext might be unknown, and ciphertext can have a lot of noise.
State-of-the-art decipherment methods use beam search and a neural language
model to score candidate plaintext hypotheses for a given cipher, assuming the
plaintext language is known. We propose an end-to-end multilingual model for
solving simple substitution ciphers. We test our model on synthetic and real
historical ciphers and show that our proposed method can decipher text without
explicit language identification while still being robust to noise.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aldarrab_N/0/1/0/all/0/1"&gt;Nada Aldarrab&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+May_J/0/1/0/all/0/1"&gt;Jonathan May&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Topic-Aware Evidence Reasoning and Stance-Aware Aggregation for Fact Verification. (arXiv:2106.01191v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01191</id>
        <link href="http://arxiv.org/abs/2106.01191"/>
        <updated>2021-06-03T02:10:35.629Z</updated>
        <summary type="html"><![CDATA[Fact verification is a challenging task that requires simultaneously
reasoning and aggregating over multiple retrieved pieces of evidence to
evaluate the truthfulness of a claim. Existing approaches typically (i) explore
the semantic interaction between the claim and evidence at different
granularity levels but fail to capture their topical consistency during the
reasoning process, which we believe is crucial for verification; (ii) aggregate
multiple pieces of evidence equally without considering their implicit stances
to the claim, thereby introducing spurious information. To alleviate the above
issues, we propose a novel topic-aware evidence reasoning and stance-aware
aggregation model for more accurate fact verification, with the following four
key properties: 1) checking topical consistency between the claim and evidence;
2) maintaining topical coherence among multiple pieces of evidence; 3) ensuring
semantic similarity between the global topic information and the semantic
representation of evidence; 4) aggregating evidence based on their implicit
stances to the claim. Extensive experiments conducted on the two benchmark
datasets demonstrate the superiority of the proposed model over several
state-of-the-art approaches for fact verification. The source code can be
obtained from https://github.com/jasenchn/TARSA.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Si_J/0/1/0/all/0/1"&gt;Jiasheng Si&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1"&gt;Deyu Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1"&gt;Tongzhe Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1"&gt;Xingyu Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1"&gt;Yulan He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Large-Scale Wasserstein Gradient Flows. (arXiv:2106.00736v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.00736</id>
        <link href="http://arxiv.org/abs/2106.00736"/>
        <updated>2021-06-03T02:10:35.623Z</updated>
        <summary type="html"><![CDATA[Wasserstein gradient flows provide a powerful means of understanding and
solving many diffusion equations. Specifically, Fokker-Planck equations, which
model the diffusion of probability measures, can be understood as gradient
descent over entropy functionals in Wasserstein space. This equivalence,
introduced by Jordan, Kinderlehrer and Otto, inspired the so-called JKO scheme
to approximate these diffusion processes via an implicit discretization of the
gradient flow in Wasserstein space. Solving the optimization problem associated
to each JKO step, however, presents serious computational challenges. We
introduce a scalable method to approximate Wasserstein gradient flows, targeted
to machine learning applications. Our approach relies on input-convex neural
networks (ICNNs) to discretize the JKO steps, which can be optimized by
stochastic gradient descent. Unlike previous work, our method does not require
domain discretization or particle simulation. As a result, we can sample from
the measure at each time step of the diffusion and compute its probability
density. We demonstrate our algorithm's performance by computing diffusions
following the Fokker-Planck equation and apply it to unnormalized density
sampling as well as nonlinear filtering.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mokrov_P/0/1/0/all/0/1"&gt;Petr Mokrov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Korotin_A/0/1/0/all/0/1"&gt;Alexander Korotin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lingxiao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Genevay_A/0/1/0/all/0/1"&gt;Aude Genevay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Solomon_J/0/1/0/all/0/1"&gt;Justin Solomon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Burnaev_E/0/1/0/all/0/1"&gt;Evgeny Burnaev&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Have Attention Heads in BERT Learned Constituency Grammar?. (arXiv:2102.07926v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.07926</id>
        <link href="http://arxiv.org/abs/2102.07926"/>
        <updated>2021-06-03T02:10:35.618Z</updated>
        <summary type="html"><![CDATA[With the success of pre-trained language models in recent years, more and
more researchers focus on opening the "black box" of these models. Following
this interest, we carry out a qualitative and quantitative analysis of
constituency grammar in attention heads of BERT and RoBERTa. We employ the
syntactic distance method to extract implicit constituency grammar from the
attention weights of each head. Our results show that there exist heads that
can induce some grammar types much better than baselines, suggesting that some
heads act as a proxy for constituency grammar. We also analyze how attention
heads' constituency grammar inducing (CGI) ability changes after fine-tuning
with two kinds of tasks, including sentence meaning similarity (SMS) tasks and
natural language inference (NLI) tasks. Our results suggest that SMS tasks
decrease the average CGI ability of upper layers, while NLI tasks increase it.
Lastly, we investigate the connections between CGI ability and natural language
understanding ability on QQP and MNLI tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1"&gt;Ziyang Luo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VAULT: VAriable Unified Long Text Representation for Machine Reading Comprehension. (arXiv:2105.03229v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.03229</id>
        <link href="http://arxiv.org/abs/2105.03229"/>
        <updated>2021-06-03T02:10:35.613Z</updated>
        <summary type="html"><![CDATA[Existing models on Machine Reading Comprehension (MRC) require complex model
architecture for effectively modeling long texts with paragraph representation
and classification, thereby making inference computationally inefficient for
production use. In this work, we propose VAULT: a light-weight and
parallel-efficient paragraph representation for MRC based on contextualized
representation from long document input, trained using a new Gaussian
distribution-based objective that pays close attention to the partially correct
instances that are close to the ground-truth. We validate our VAULT
architecture showing experimental results on two benchmark MRC datasets that
require long context modeling; one Wikipedia-based (Natural Questions (NQ)) and
the other on TechNotes (TechQA). VAULT can achieve comparable performance on NQ
with a state-of-the-art (SOTA) complex document modeling approach while being
16 times faster, demonstrating the efficiency of our proposed model. We also
demonstrate that our model can also be effectively adapted to a completely
different domain -- TechQA -- with large improvement over a model fine-tuned on
a previously published large PLM.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wen_H/0/1/0/all/0/1"&gt;Haoyang Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ferritto_A/0/1/0/all/0/1"&gt;Anthony Ferritto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1"&gt;Heng Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Florian_R/0/1/0/all/0/1"&gt;Radu Florian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sil_A/0/1/0/all/0/1"&gt;Avirup Sil&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FairBatch: Batch Selection for Model Fairness. (arXiv:2012.01696v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.01696</id>
        <link href="http://arxiv.org/abs/2012.01696"/>
        <updated>2021-06-03T02:10:35.606Z</updated>
        <summary type="html"><![CDATA[Training a fair machine learning model is essential to prevent demographic
disparity. Existing techniques for improving model fairness require broad
changes in either data preprocessing or model training, rendering themselves
difficult-to-adopt for potentially already complex machine learning systems. We
address this problem via the lens of bilevel optimization. While keeping the
standard training algorithm as an inner optimizer, we incorporate an outer
optimizer so as to equip the inner problem with an additional functionality:
Adaptively selecting minibatch sizes for the purpose of improving model
fairness. Our batch selection algorithm, which we call FairBatch, implements
this optimization and supports prominent fairness measures: equal opportunity,
equalized odds, and demographic parity. FairBatch comes with a significant
implementation benefit -- it does not require any modification to data
preprocessing or model training. For instance, a single-line change of PyTorch
code for replacing batch selection part of model training suffices to employ
FairBatch. Our experiments conducted both on synthetic and benchmark real data
demonstrate that FairBatch can provide such functionalities while achieving
comparable (or even greater) performances against the state of the arts.
Furthermore, FairBatch can readily improve fairness of any pre-trained model
simply via fine-tuning. It is also compatible with existing batch selection
techniques intended for different purposes, such as faster convergence, thus
gracefully achieving multiple purposes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Roh_Y/0/1/0/all/0/1"&gt;Yuji Roh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1"&gt;Kangwook Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Whang_S/0/1/0/all/0/1"&gt;Steven Euijong Whang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suh_C/0/1/0/all/0/1"&gt;Changho Suh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural message passing for joint paratope-epitope prediction. (arXiv:2106.00757v1 [q-bio.QM])]]></title>
        <id>http://arxiv.org/abs/2106.00757</id>
        <link href="http://arxiv.org/abs/2106.00757"/>
        <updated>2021-06-03T02:10:35.601Z</updated>
        <summary type="html"><![CDATA[Antibodies are proteins in the immune system which bind to antigens to detect
and neutralise them. The binding sites in an antibody-antigen interaction are
known as the paratope and epitope, respectively, and the prediction of these
regions is key to vaccine and synthetic antibody development. Contrary to prior
art, we argue that paratope and epitope predictors require asymmetric
treatment, and propose distinct neural message passing architectures that are
geared towards the specific aspects of paratope and epitope prediction,
respectively. We obtain significant improvements on both tasks, setting the new
state-of-the-art and recovering favourable qualitative predictions on antigens
of relevance to COVID-19.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Vecchio_A/0/1/0/all/0/1"&gt;Alice Del Vecchio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Deac_A/0/1/0/all/0/1"&gt;Andreea Deac&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Lio_P/0/1/0/all/0/1"&gt;Pietro Li&amp;#xf2;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Velickovic_P/0/1/0/all/0/1"&gt;Petar Veli&amp;#x10d;kovi&amp;#x107;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Motif Prediction with Graph Neural Networks. (arXiv:2106.00761v1 [cs.SI])]]></title>
        <id>http://arxiv.org/abs/2106.00761</id>
        <link href="http://arxiv.org/abs/2106.00761"/>
        <updated>2021-06-03T02:10:35.585Z</updated>
        <summary type="html"><![CDATA[Link prediction is one of the central problems in graph mining. However,
recent studies highlight the importance of the higher-order network analysis,
where complex structures called motifs are the first-class citizens. We
illustrate that existing link prediction schemes fail to predict the appearance
of complex motifs in graph data. To address this issue, we propose a general
motif prediction problem. We establish the theoretical foundation of motif
prediction and we propose several heuristics that, for a fixed set of nodes in
a graph and a specified motif, assess the chances for this motif to appear. To
make the scores realistic, our heuristics - among others - consider
correlations between links, i.e., the potential impact of some arriving links
on the appearance of other parts of a given motif. Finally, for highest
accuracy, we develop a graph neural network (GNN) architecture for motif
prediction. Our architecture offers vertex features and sampling schemes that
capture the rich structural properties of motifs. While our heuristics are fast
and do not need any training, using GNNs ensures highest accuracy when
predicting the arrival of complex graph structures, both dense (e.g.,
k-cliques) and sparse (e.g., k-stars). Importantly, its advantages over schemes
based on uncorrelated link prediction increase with the increasing motif size
and complexity. We also successfully apply our architecture for predicting more
arbitrary clusters and communities, illustrating its potential for graph mining
beyond motif analysis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Besta_M/0/1/0/all/0/1"&gt;Maciej Besta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grob_R/0/1/0/all/0/1"&gt;Raphael Grob&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Miglioli_C/0/1/0/all/0/1"&gt;Cesare Miglioli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bernold_N/0/1/0/all/0/1"&gt;Nicola Bernold&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kwasniewski_G/0/1/0/all/0/1"&gt;Grzegorz Kwasniewski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gjini_G/0/1/0/all/0/1"&gt;Gabriel Gjini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kanakagiri_R/0/1/0/all/0/1"&gt;Raghavendra Kanakagiri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ashkboos_S/0/1/0/all/0/1"&gt;Saleh Ashkboos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gianinazzi_L/0/1/0/all/0/1"&gt;Lukas Gianinazzi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dryden_N/0/1/0/all/0/1"&gt;Nikoli Dryden&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoefler_T/0/1/0/all/0/1"&gt;Torsten Hoefler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Part of Speech and Universal Dependency effects on English Arabic Machine Translation. (arXiv:2106.00745v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.00745</id>
        <link href="http://arxiv.org/abs/2106.00745"/>
        <updated>2021-06-03T02:10:35.576Z</updated>
        <summary type="html"><![CDATA[In this research paper, I will elaborate on a method to evaluate machine
translation models based on their performance on underlying syntactical
phenomena between English and Arabic languages. This method is especially
important as such "neural" and "machine learning" are hard to fine-tune and
change. Thus, finding a way to evaluate them easily and diversely would greatly
help the task of bettering them.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Abend_O/0/1/0/all/0/1"&gt;Omri Abend&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choshen_L/0/1/0/all/0/1"&gt;Leshem Choshen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nikolaev_D/0/1/0/all/0/1"&gt;Dmitry Nikolaev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rafaeli_O/0/1/0/all/0/1"&gt;Ofek Rafaeli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Second-Order Neural Dependency Parsing with Message Passing and End-to-End Training. (arXiv:2010.05003v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.05003</id>
        <link href="http://arxiv.org/abs/2010.05003"/>
        <updated>2021-06-03T02:10:35.564Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose second-order graph-based neural dependency parsing
using message passing and end-to-end neural networks. We empirically show that
our approaches match the accuracy of very recent state-of-the-art second-order
graph-based neural dependency parsers and have significantly faster speed in
both training and testing. We also empirically show the advantage of
second-order parsing over first-order parsing and observe that the usefulness
of the head-selection structured constraint vanishes when using BERT embedding.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xinyu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tu_K/0/1/0/all/0/1"&gt;Kewei Tu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PairRank: Online Pairwise Learning to Rank by Divide-and-Conquer. (arXiv:2103.00368v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.00368</id>
        <link href="http://arxiv.org/abs/2103.00368"/>
        <updated>2021-06-03T02:10:35.559Z</updated>
        <summary type="html"><![CDATA[Online Learning to Rank (OL2R) eliminates the need of explicit relevance
annotation by directly optimizing the rankers from their interactions with
users. However, the required exploration drives it away from successful
practices in offline learning to rank, which limits OL2R's empirical
performance and practical applicability. In this work, we propose to estimate a
pairwise learning to rank model online. In each round, candidate documents are
partitioned and ranked according to the model's confidence on the estimated
pairwise rank order, and exploration is only performed on the uncertain pairs
of documents, i.e., \emph{divide-and-conquer}. Regret directly defined on the
number of mis-ordered pairs is proven, which connects the online solution's
theoretical convergence with its expected ranking performance. Comparisons
against an extensive list of OL2R baselines on two public learning to rank
benchmark datasets demonstrate the effectiveness of the proposed solution.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1"&gt;Yiling Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Huazheng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1"&gt;Stephen Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hongning Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pay Attention to MLPs. (arXiv:2105.08050v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.08050</id>
        <link href="http://arxiv.org/abs/2105.08050"/>
        <updated>2021-06-03T02:10:35.545Z</updated>
        <summary type="html"><![CDATA[Transformers have become one of the most important architectural innovations
in deep learning and have enabled many breakthroughs over the past few years.
Here we propose a simple network architecture, gMLP, based on MLPs with gating,
and show that it can perform as well as Transformers in key language and vision
applications. Our comparisons show that self-attention is not critical for
Vision Transformers, as gMLP can achieve the same accuracy. For BERT, our model
achieves parity with Transformers on pretraining perplexity and is better on
some downstream NLP tasks. On finetuning tasks where gMLP performs worse,
making the gMLP model substantially larger can close the gap with Transformers.
In general, our experiments show that gMLP can scale as well as Transformers
over increased data and compute.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Hanxiao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_Z/0/1/0/all/0/1"&gt;Zihang Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+So_D/0/1/0/all/0/1"&gt;David R. So&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1"&gt;Quoc V. Le&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Making Pre-trained Language Models Better Few-shot Learners. (arXiv:2012.15723v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.15723</id>
        <link href="http://arxiv.org/abs/2012.15723"/>
        <updated>2021-06-03T02:10:35.538Z</updated>
        <summary type="html"><![CDATA[The recent GPT-3 model (Brown et al., 2020) achieves remarkable few-shot
performance solely by leveraging a natural-language prompt and a few task
demonstrations as input context. Inspired by their findings, we study few-shot
learning in a more practical scenario, where we use smaller language models for
which fine-tuning is computationally efficient. We present LM-BFF--better
few-shot fine-tuning of language models--a suite of simple and complementary
techniques for fine-tuning language models on a small number of annotated
examples. Our approach includes (1) prompt-based fine-tuning together with a
novel pipeline for automating prompt generation; and (2) a refined strategy for
dynamically and selectively incorporating demonstrations into each context.
Finally, we present a systematic evaluation for analyzing few-shot performance
on a range of NLP tasks, including classification and regression. Our
experiments demonstrate that our methods combine to dramatically outperform
standard fine-tuning procedures in this low resource setting, achieving up to
30% absolute improvement, and 11% on average across all tasks. Our approach
makes minimal assumptions on task resources and domain expertise, and hence
constitutes a strong task-agnostic method for few-shot learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_T/0/1/0/all/0/1"&gt;Tianyu Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fisch_A/0/1/0/all/0/1"&gt;Adam Fisch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1"&gt;Danqi Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning neural network potentials from experimental data via Differentiable Trajectory Reweighting. (arXiv:2106.01138v1 [physics.chem-ph])]]></title>
        <id>http://arxiv.org/abs/2106.01138</id>
        <link href="http://arxiv.org/abs/2106.01138"/>
        <updated>2021-06-03T02:10:35.525Z</updated>
        <summary type="html"><![CDATA[In molecular dynamics (MD), neural network (NN) potentials trained bottom-up
on quantum mechanical data have seen tremendous success recently. Top-down
approaches that learn NN potentials directly from experimental data have
received less attention, typically facing numerical and computational
challenges when backpropagating through MD simulations. We present the
Differentiable Trajectory Reweighting (DiffTRe) method, which bypasses
differentiation through the MD simulation for time-independent observables.
Leveraging thermodynamic perturbation theory, we avoid exploding gradients and
achieve around 2 orders of magnitude speed-up in gradient computation for
top-down learning. We show effectiveness of DiffTRe in learning NN potentials
for an atomistic model of diamond and a coarse-grained model of water based on
diverse experimental observables including thermodynamic, structural and
mechanical properties. Importantly, DiffTRe also generalizes bottom-up
structural coarse-graining methods such as iterative Boltzmann inversion to
arbitrary potentials. The presented method constitutes an important milestone
towards enriching NN potentials with experimental data, particularly when
accurate bottom-up data is unavailable.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Thaler_S/0/1/0/all/0/1"&gt;Stephan Thaler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Zavadlav_J/0/1/0/all/0/1"&gt;Julija Zavadlav&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ProSelfLC: Progressive Self Label Correction for Training Robust Deep Neural Networks. (arXiv:2005.03788v6 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.03788</id>
        <link href="http://arxiv.org/abs/2005.03788"/>
        <updated>2021-06-03T02:10:35.372Z</updated>
        <summary type="html"><![CDATA[To train robust deep neural networks (DNNs), we systematically study several
target modification approaches, which include output regularisation, self and
non-self label correction (LC). Two key issues are discovered: (1) Self LC is
the most appealing as it exploits its own knowledge and requires no extra
models. However, how to automatically decide the trust degree of a learner as
training goes is not well answered in the literature? (2) Some methods penalise
while the others reward low-entropy predictions, prompting us to ask which one
is better?

To resolve the first issue, taking two well-accepted propositions--deep
neural networks learn meaningful patterns before fitting noise [3] and minimum
entropy regularisation principle [10]--we propose a novel end-to-end method
named ProSelfLC, which is designed according to learning time and entropy.
Specifically, given a data point, we progressively increase trust in its
predicted label distribution versus its annotated one if a model has been
trained for enough time and the prediction is of low entropy (high confidence).
For the second issue, according to ProSelfLC, we empirically prove that it is
better to redefine a meaningful low-entropy status and optimise the learner
toward it. This serves as a defence of entropy minimisation.

We demonstrate the effectiveness of ProSelfLC through extensive experiments
in both clean and noisy settings. The source code is available at
https://github.com/XinshaoAmosWang/ProSelfLC-CVPR2021.

Keywords: entropy minimisation, maximum entropy, confidence penalty, self
knowledge distillation, label correction, label noise, semi-supervised
learning, output regularisation]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xinshao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hua_Y/0/1/0/all/0/1"&gt;Yang Hua&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kodirov_E/0/1/0/all/0/1"&gt;Elyor Kodirov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Clifton_D/0/1/0/all/0/1"&gt;David A. Clifton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Robertson_N/0/1/0/all/0/1"&gt;Neil M. Robertson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semi-Supervised Empirical Risk Minimization: When can unlabeled data improve prediction?. (arXiv:2009.00606v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.00606</id>
        <link href="http://arxiv.org/abs/2009.00606"/>
        <updated>2021-06-03T02:10:35.367Z</updated>
        <summary type="html"><![CDATA[We present a general methodology for using unlabeled data to design semi
supervised learning (SSL) variants of the Empirical Risk Minimization (ERM)
learning process. Focusing on generalized linear regression, we provide a
careful treatment of the effectiveness of the SSL to improve prediction
performance. The key ideas are carefully considering the null model as a
competitor, and utilizing the unlabeled data to determine signal-noise
combinations where the SSL outperforms both the ERM learning and the null
model. In the special case of linear regression with Gaussian covariates, we
show that the previously suggested semi-supervised estimator is in fact not
capable of improving on both the supervised estimator and the null model
simultaneously. However, the new estimator presented in this work, can achieve
an improvement of $O(1/n)$ term over both competitors simultaneously. On the
other hand, we show that in other scenarios, such as non-Gaussian covariates,
misspecified linear regression, or generalized linear regression with
non-linear link functions, having unlabeled data can derive substantial
improvement in practice by applying our suggested SSL approach. Moreover, it is
possible to identify the situations where SSL improves prediction, by using the
results we establish throughout this work. This is shown empirically through
extensive simulations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Yuval_O/0/1/0/all/0/1"&gt;Oren Yuval&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Rosset_S/0/1/0/all/0/1"&gt;Saharon Rosset&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BGC: Multi-Agent Group Belief with Graph Clustering. (arXiv:2008.08808v3 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.08808</id>
        <link href="http://arxiv.org/abs/2008.08808"/>
        <updated>2021-06-03T02:10:35.361Z</updated>
        <summary type="html"><![CDATA[Recent advances have witnessed that value decomposed-based multi-agent
reinforcement learning methods make an efficient performance in coordination
tasks. Most current methods assume that agents can make communication to assist
decisions, which is impractical in some situations. In this paper, we propose a
semi-communication method to enable agents can exchange information without
communication. Specifically, we introduce a group concept to help agents
learning a belief which is a type of consensus. With this consensus, adjacent
agents tend to accomplish similar sub-tasks to achieve cooperation. We design a
novel agent structure named Belief in Graph Clustering(BGC), composed of an
agent characteristic module, a belief module, and a fusion module. To represent
each agent characteristic, we use an MLP-based characteristic module to
generate agent unique features. Inspired by the neighborhood cognitive
consistency, we propose a group-based module to divide adjacent agents into a
small group and minimize in-group agents' beliefs to accomplish similar
sub-tasks. Finally, we use a hyper-network to merge these features and produce
agent actions. To overcome the agent consistent problem brought by GAT, a split
loss is introduced to distinguish different agents. Results reveal that the
proposed method achieves a significant improvement in the SMAC benchmark.
Because of the group concept, our approach maintains excellent performance with
an increase in the number of agents.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1"&gt;Tianze Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1"&gt;Fubiao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_P/0/1/0/all/0/1"&gt;Pan Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chenfei Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Diffusion Schr\"odinger Bridge with Applications to Score-Based Generative Modeling. (arXiv:2106.01357v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.01357</id>
        <link href="http://arxiv.org/abs/2106.01357"/>
        <updated>2021-06-03T02:10:35.345Z</updated>
        <summary type="html"><![CDATA[Progressively applying Gaussian noise transforms complex data distributions
to approximately Gaussian. Reversing this dynamic defines a generative model.
When the forward noising process is given by a Stochastic Differential Equation
(SDE), Song et al. (2021) demonstrate how the time inhomogeneous drift of the
associated reverse-time SDE may be estimated using score-matching. A limitation
of this approach is that the forward-time SDE must be run for a sufficiently
long time for the final distribution to be approximately Gaussian. In contrast,
solving the Schr\"odinger Bridge problem (SB), i.e. an entropy-regularized
optimal transport problem on path spaces, yields diffusions which generate
samples from the data distribution in finite time. We present Diffusion SB
(DSB), an original approximation of the Iterative Proportional Fitting (IPF)
procedure to solve the SB problem, and provide theoretical analysis along with
generative modeling experiments. The first DSB iteration recovers the
methodology proposed by Song et al. (2021), with the flexibility of using
shorter time intervals, as subsequent DSB iterations reduce the discrepancy
between the final-time marginal of the forward (resp. backward) SDE with
respect to the prior (resp. data) distribution. Beyond generative modeling, DSB
offers a widely applicable computational optimal transport tool as the
continuous state-space analogue of the popular Sinkhorn algorithm (Cuturi,
2013).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Bortoli_V/0/1/0/all/0/1"&gt;Valentin De Bortoli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Thornton_J/0/1/0/all/0/1"&gt;James Thornton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Heng_J/0/1/0/all/0/1"&gt;Jeremy Heng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Doucet_A/0/1/0/all/0/1"&gt;Arnaud Doucet&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Efficiently Explaining Graph-Based Classifiers. (arXiv:2106.01350v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2106.01350</id>
        <link href="http://arxiv.org/abs/2106.01350"/>
        <updated>2021-06-03T02:10:35.339Z</updated>
        <summary type="html"><![CDATA[Recent work has shown that not only decision trees (DTs) may not be
interpretable but also proposed a polynomial-time algorithm for computing one
PI-explanation of a DT. This paper shows that for a wide range of classifiers,
globally referred to as decision graphs, and which include decision trees and
binary decision diagrams, but also their multi-valued variants, there exist
polynomial-time algorithms for computing one PI-explanation. In addition, the
paper also proposes a polynomial-time algorithm for computing one contrastive
explanation. These novel algorithms build on explanation graphs (XpG's). XpG's
denote a graph representation that enables both theoretical and practically
efficient computation of explanations for decision graphs. Furthermore, the
paper pro- poses a practically efficient solution for the enumeration of
explanations, and studies the complexity of deciding whether a given feature is
included in some explanation. For the concrete case of decision trees, the
paper shows that the set of all contrastive explanations can be enumerated in
polynomial time. Finally, the experimental results validate the practical
applicability of the algorithms proposed in the paper on a wide range of
publicly available benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1"&gt;Xuanxiang Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Izza_Y/0/1/0/all/0/1"&gt;Yacine Izza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ignatiev_A/0/1/0/all/0/1"&gt;Alexey Ignatiev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marques_Silva_J/0/1/0/all/0/1"&gt;Joao Marques-Silva&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Convergence Rate of Off-Policy Policy Optimization Methods with Density-Ratio Correction. (arXiv:2106.00993v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.00993</id>
        <link href="http://arxiv.org/abs/2106.00993"/>
        <updated>2021-06-03T02:10:35.333Z</updated>
        <summary type="html"><![CDATA[In this paper, we study the convergence properties of off-policy policy
improvement algorithms with state-action density ratio correction under
function approximation setting, where the objective function is formulated as a
max-max-min optimization problem. We characterize the bias of the learning
objective and present two strategies with finite-time convergence guarantees.
In our first strategy, we present algorithm P-SREDA with convergence rate
$O(\epsilon^{-3})$, whose dependency on $\epsilon$ is optimal. In our second
strategy, we propose a new off-policy actor-critic style algorithm named
O-SPIM. We prove that O-SPIM converges to a stationary point with total
complexity $O(\epsilon^{-4})$, which matches the convergence rate of some
recent actor-critic algorithms in the on-policy setting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1"&gt;Jiawei Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_N/0/1/0/all/0/1"&gt;Nan Jiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How Good is SGD with Random Shuffling?. (arXiv:1908.00045v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1908.00045</id>
        <link href="http://arxiv.org/abs/1908.00045"/>
        <updated>2021-06-03T02:10:35.320Z</updated>
        <summary type="html"><![CDATA[We study the performance of stochastic gradient descent (SGD) on smooth and
strongly-convex finite-sum optimization problems. In contrast to the majority
of existing theoretical works, which assume that individual functions are
sampled with replacement, we focus here on popular but poorly-understood
heuristics, which involve going over random permutations of the individual
functions. This setting has been investigated in several recent works, but the
optimal error rates remain unclear. In this paper, we provide lower bounds on
the expected optimization error with these heuristics (using SGD with any
constant step size), which elucidate their advantages and disadvantages. In
particular, we prove that after $k$ passes over $n$ individual functions, if
the functions are re-shuffled after every pass, the best possible optimization
error for SGD is at least $\Omega\left(1/(nk)^2+1/nk^3\right)$, which partially
corresponds to recently derived upper bounds. Moreover, if the functions are
only shuffled once, then the lower bound increases to $\Omega(1/nk^2)$. Since
there are strictly smaller upper bounds for repeated reshuffling, this proves
an inherent performance gap between SGD with single shuffling and repeated
shuffling. As a more minor contribution, we also provide a non-asymptotic
$\Omega(1/k^2)$ lower bound (independent of $n$) for the incremental gradient
method, when no random shuffling takes place. Finally, we provide an indication
that our lower bounds are tight, by proving matching upper bounds for
univariate quadratic functions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Safran_I/0/1/0/all/0/1"&gt;Itay Safran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shamir_O/0/1/0/all/0/1"&gt;Ohad Shamir&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Decision-making Oriented Clustering: Application to Pricing and Power Consumption Scheduling. (arXiv:2106.01021v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01021</id>
        <link href="http://arxiv.org/abs/2106.01021"/>
        <updated>2021-06-03T02:10:35.314Z</updated>
        <summary type="html"><![CDATA[Data clustering is an instrumental tool in the area of energy resource
management. One problem with conventional clustering is that it does not take
the final use of the clustered data into account, which may lead to a very
suboptimal use of energy or computational resources. When clustered data are
used by a decision-making entity, it turns out that significant gains can be
obtained by tailoring the clustering scheme to the final task performed by the
decision-making entity. The key to having good final performance is to
automatically extract the important attributes of the data space that are
inherently relevant to the subsequent decision-making entity, and partition the
data space based on these attributes instead of partitioning the data space
based on predefined conventional metrics. For this purpose, we formulate the
framework of decision-making oriented clustering and propose an algorithm
providing a decision-based partition of the data space and good representative
decisions. By applying this novel framework and algorithm to a typical problem
of real-time pricing and that of power consumption scheduling, we obtain
several insightful analytical results such as the expression of the best
representative price profiles for real-time pricing and a very significant
reduction in terms of required clusters to perform power consumption scheduling
as shown by our simulations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lasaulce_S/0/1/0/all/0/1"&gt;Samson Lasaulce&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hennebel_M/0/1/0/all/0/1"&gt;Martin Hennebel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saludjian_L/0/1/0/all/0/1"&gt;Lucas Saludjian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Panciatici_P/0/1/0/all/0/1"&gt;Patrick Panciatici&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Poor_H/0/1/0/all/0/1"&gt;H. Vincent Poor&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Energy-Efficient Model Compression and Splitting for Collaborative Inference Over Time-Varying Channels. (arXiv:2106.00995v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.00995</id>
        <link href="http://arxiv.org/abs/2106.00995"/>
        <updated>2021-06-03T02:10:35.309Z</updated>
        <summary type="html"><![CDATA[Today's intelligent applications can achieve high performance accuracy using
machine learning (ML) techniques, such as deep neural networks (DNNs).
Traditionally, in a remote DNN inference problem, an edge device transmits raw
data to a remote node that performs the inference task. However, this may incur
high transmission energy costs and puts data privacy at risk. In this paper, we
propose a technique to reduce the total energy bill at the edge device by
utilizing model compression and time-varying model split between the edge and
remote nodes. The time-varying representation accounts for time-varying
channels and can significantly reduce the total energy at the edge device while
maintaining high accuracy (low loss). We implement our approach in an image
classification task using the MNIST dataset, and the system environment is
simulated as a trajectory navigation scenario to emulate different channel
conditions. Numerical simulations show that our proposed solution results in
minimal energy consumption and $CO_2$ emission compared to the considered
baselines while exhibiting robust performance across different channel
conditions and bandwidth regime choices.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Krouka_M/0/1/0/all/0/1"&gt;Mounssif Krouka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Elgabli_A/0/1/0/all/0/1"&gt;Anis Elgabli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Issaid_C/0/1/0/all/0/1"&gt;Chaouki Ben Issaid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bennis_M/0/1/0/all/0/1"&gt;Mehdi Bennis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[JUMBO: Scalable Multi-task Bayesian Optimization using Offline Data. (arXiv:2106.00942v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.00942</id>
        <link href="http://arxiv.org/abs/2106.00942"/>
        <updated>2021-06-03T02:10:35.304Z</updated>
        <summary type="html"><![CDATA[The goal of Multi-task Bayesian Optimization (MBO) is to minimize the number
of queries required to accurately optimize a target black-box function, given
access to offline evaluations of other auxiliary functions. When offline
datasets are large, the scalability of prior approaches comes at the expense of
expressivity and inference quality. We propose JUMBO, an MBO algorithm that
sidesteps these limitations by querying additional data based on a combination
of acquisition signals derived from training two Gaussian Processes (GP): a
cold-GP operating directly in the input domain and a warm-GP that operates in
the feature space of a deep neural network pretrained using the offline data.
Such a decomposition can dynamically control the reliability of information
derived from the online and offline data and the use of pretrained neural
networks permits scalability to large offline datasets. Theoretically, we
derive regret bounds for JUMBO and show that it achieves no-regret under
conditions analogous to GP-UCB (Srinivas et. al. 2010). Empirically, we
demonstrate significant performance improvements over existing approaches on
two real-world optimization problems: hyper-parameter optimization and
automated circuit design.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hakhamaneshi_K/0/1/0/all/0/1"&gt;Kourosh Hakhamaneshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1"&gt;Pieter Abbeel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stojanovic_V/0/1/0/all/0/1"&gt;Vladimir Stojanovic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grover_A/0/1/0/all/0/1"&gt;Aditya Grover&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hybrid Ensemble optimized algorithm based on Genetic Programming for imbalanced data classification. (arXiv:2106.01176v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01176</id>
        <link href="http://arxiv.org/abs/2106.01176"/>
        <updated>2021-06-03T02:10:35.298Z</updated>
        <summary type="html"><![CDATA[One of the most significant current discussions in the field of data mining
is classifying imbalanced data. In recent years, several ways are proposed such
as algorithm level (internal) approaches, data level (external) techniques, and
cost-sensitive methods. Although extensive research has been carried out on
imbalanced data classification, however, several unsolved challenges remain
such as no attention to the importance of samples to balance, determine the
appropriate number of classifiers, and no optimization of classifiers in the
combination of classifiers. The purpose of this paper is to improve the
efficiency of the ensemble method in the sampling of training data sets,
especially in the minority class, and to determine better basic classifiers for
combining classifiers than existing methods. We proposed a hybrid ensemble
algorithm based on Genetic Programming (GP) for two classes of imbalanced data
classification. In this study uses historical data from UCI Machine Learning
Repository to assess minority classes in imbalanced datasets. The performance
of our proposed algorithm is evaluated by Rapid-miner studio v.7.5.
Experimental results show the performance of the proposed method on the
specified data sets in the size of the training set shows 40% and 50% better
accuracy than other dimensions of the minority class prediction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Roknizadeh_M/0/1/0/all/0/1"&gt;Maliheh Roknizadeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Naeen_H/0/1/0/all/0/1"&gt;Hossein Monshizadeh Naeen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[multiPRover: Generating Multiple Proofs for Improved Interpretability in Rule Reasoning. (arXiv:2106.01354v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01354</id>
        <link href="http://arxiv.org/abs/2106.01354"/>
        <updated>2021-06-03T02:10:35.293Z</updated>
        <summary type="html"><![CDATA[We focus on a type of linguistic formal reasoning where the goal is to reason
over explicit knowledge in the form of natural language facts and rules (Clark
et al., 2020). A recent work, named PRover (Saha et al., 2020), performs such
reasoning by answering a question and also generating a proof graph that
explains the answer. However, compositional reasoning is not always unique and
there may be multiple ways of reaching the correct answer. Thus, in our work,
we address a new and challenging problem of generating multiple proof graphs
for reasoning over natural language rule-bases. Each proof provides a different
rationale for the answer, thereby improving the interpretability of such
reasoning systems. In order to jointly learn from all proof graphs and exploit
the correlations between multiple proofs for a question, we pose this task as a
set generation problem over structured output spaces where each proof is
represented as a directed graph. We propose two variants of a proof-set
generation model, multiPRover. Our first model, Multilabel-multiPRover,
generates a set of proofs via multi-label classification and implicit
conditioning between the proofs; while the second model, Iterative-multiPRover,
generates proofs iteratively by explicitly conditioning on the previously
generated proofs. Experiments on multiple synthetic, zero-shot, and
human-paraphrased datasets reveal that both multiPRover models significantly
outperform PRover on datasets containing multiple gold proofs.
Iterative-multiPRover obtains state-of-the-art proof F1 in zero-shot scenarios
where all examples have single correct proofs. It also generalizes better to
questions requiring higher depths of reasoning where multiple proofs are more
frequent. Our code and models are publicly available at
https://github.com/swarnaHub/multiPRover]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Saha_S/0/1/0/all/0/1"&gt;Swarnadeep Saha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yadav_P/0/1/0/all/0/1"&gt;Prateek Yadav&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1"&gt;Mohit Bansal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Latent Space Refinement for Deep Generative Models. (arXiv:2106.00792v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.00792</id>
        <link href="http://arxiv.org/abs/2106.00792"/>
        <updated>2021-06-03T02:10:35.287Z</updated>
        <summary type="html"><![CDATA[Deep generative models are becoming widely used across science and industry
for a variety of purposes. A common challenge is achieving a precise implicit
or explicit representation of the data probability density. Recent proposals
have suggested using classifier weights to refine the learned density of deep
generative models. We extend this idea to all types of generative models and
show how latent space refinement via iterated generative modeling can
circumvent topological obstructions and improve precision. This methodology
also applies to cases were the target model is non-differentiable and has many
internal latent dimensions which must be marginalized over before refinement.
We demonstrate our Latent Space Refinement (LaSeR) protocol on a variety of
examples, focusing on the combinations of Normalizing Flows and Generative
Adversarial Networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Winterhalder_R/0/1/0/all/0/1"&gt;Ramon Winterhalder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Bellagente_M/0/1/0/all/0/1"&gt;Marco Bellagente&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Nachman_B/0/1/0/all/0/1"&gt;Benjamin Nachman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Leveraging Pre-Images to Discover Nonlinear Relationships in Multivariate Environments. (arXiv:2106.00842v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.00842</id>
        <link href="http://arxiv.org/abs/2106.00842"/>
        <updated>2021-06-03T02:10:35.282Z</updated>
        <summary type="html"><![CDATA[Causal discovery, beyond the inference of a network as a collection of
connected dots, offers a crucial functionality in scientific discovery using
artificial intelligence. The questions that arise in multiple domains, such as
physics, physiology, the strategic decision in uncertain environments with
multiple agents, climatology, among many others, have roots in causality and
reasoning. It became apparent that many real-world temporal observations are
nonlinearly related to each other. While the number of observations can be as
high as millions of points, the number of temporal samples can be minimal due
to ethical or practical reasons, leading to the curse-of-dimensionality in
large-scale systems. This paper proposes a novel method using kernel principal
component analysis and pre-images to obtain nonlinear dependencies of
multivariate time-series data. We show that our method outperforms
state-of-the-art causal discovery methods when the observations are restricted
by time and are nonlinearly related. Extensive simulations on both real-world
and synthetic datasets with various topologies are provided to evaluate our
proposed methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vosoughi_M/0/1/0/all/0/1"&gt;M. Ali Vosoughi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wismuller_A/0/1/0/all/0/1"&gt;Axel Wismuller&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Faster Re-translation Using Non-Autoregressive Model For Simultaneous Neural Machine Translation. (arXiv:2012.14681v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.14681</id>
        <link href="http://arxiv.org/abs/2012.14681"/>
        <updated>2021-06-03T02:10:35.265Z</updated>
        <summary type="html"><![CDATA[Recently, simultaneous translation has gathered a lot of attention since it
enables compelling applications such as subtitle translation for a live event
or real-time video-call translation. Some of these translation applications
allow editing of partial translation giving rise to re-translation approaches.
The current re-translation approaches are based on autoregressive sequence
generation models (ReTA), which generate tar-get tokens in the (partial)
translation sequentially. The multiple re-translations with sequential
generation inReTAmodelslead to an increased inference time gap between the
incoming source input and the corresponding target output as the source input
grows. Besides, due to the large number of inference operations involved, the
ReTA models are not favorable for resource-constrained devices. In this work,
we propose a faster re-translation system based on a non-autoregressive
sequence generation model (FReTNA) to overcome the aforementioned limitations.
We evaluate the proposed model on multiple translation tasks and our model
reduces the inference times by several orders and achieves a competitive
BLEUscore compared to the ReTA and streaming (Wait-k) models.The proposed model
reduces the average computation time by a factor of 20 when compared to the
ReTA model by incurring a small drop in the translation quality. It also
outperforms the streaming-based Wait-k model both in terms of computation time
(1.5 times lower) and translation quality.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Han_H/0/1/0/all/0/1"&gt;Hyojung Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Indurthi_S/0/1/0/all/0/1"&gt;Sathish Indurthi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zaidi_M/0/1/0/all/0/1"&gt;Mohd Abbas Zaidi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lakumarapu_N/0/1/0/all/0/1"&gt;Nikhil Kumar Lakumarapu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_B/0/1/0/all/0/1"&gt;Beomseok Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1"&gt;Sangha Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_C/0/1/0/all/0/1"&gt;Chanwoo Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hwang_I/0/1/0/all/0/1"&gt;Inchul Hwang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Named Entity Recognition by External Context Retrieving and Cooperative Learning. (arXiv:2105.03654v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.03654</id>
        <link href="http://arxiv.org/abs/2105.03654"/>
        <updated>2021-06-03T02:10:35.258Z</updated>
        <summary type="html"><![CDATA[Recent advances in Named Entity Recognition (NER) show that document-level
contexts can significantly improve model performance. In many application
scenarios, however, such contexts are not available. In this paper, we propose
to find external contexts of a sentence by retrieving and selecting a set of
semantically relevant texts through a search engine, with the original sentence
as the query. We find empirically that the contextual representations computed
on the retrieval-based input view, constructed through the concatenation of a
sentence and its external contexts, can achieve significantly improved
performance compared to the original input view based only on the sentence.
Furthermore, we can improve the model performance of both input views by
Cooperative Learning, a training method that encourages the two input views to
produce similar contextual representations or output label distributions.
Experiments show that our approach can achieve new state-of-the-art performance
on 8 NER data sets across 5 domains.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xinyu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1"&gt;Yong Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bach_N/0/1/0/all/0/1"&gt;Nguyen Bach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1"&gt;Tao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zhongqiang Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1"&gt;Fei Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tu_K/0/1/0/all/0/1"&gt;Kewei Tu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Connections and Equivalences between the Nystr\"om Method and Sparse Variational Gaussian Processes. (arXiv:2106.01121v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.01121</id>
        <link href="http://arxiv.org/abs/2106.01121"/>
        <updated>2021-06-03T02:10:35.247Z</updated>
        <summary type="html"><![CDATA[We investigate the connections between sparse approximation methods for
making kernel methods and Gaussian processes (GPs) scalable to massive data,
focusing on the Nystr\"om method and the Sparse Variational Gaussian Processes
(SVGP). While sparse approximation methods for GPs and kernel methods share
some algebraic similarities, the literature lacks a deep understanding of how
and why they are related. This is a possible obstacle for the communications
between the GP and kernel communities, making it difficult to transfer results
from one side to the other. Our motivation is to remove this possible obstacle,
by clarifying the connections between the sparse approximations for GPs and
kernel methods. In this work, we study the two popular approaches, the
Nystr\"om and SVGP approximations, in the context of a regression problem, and
establish various connections and equivalences between them. In particular, we
provide an RKHS interpretation of the SVGP approximation, and show that the
Evidence Lower Bound of the SVGP contains the objective function of the
Nystr\"om approximation, revealing the origin of the algebraic equivalence
between the two approaches. We also study recently established convergence
results for the SVGP and how they are related to the approximation quality of
the Nystr\"om method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Wild_V/0/1/0/all/0/1"&gt;Veit Wild&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kanagawa_M/0/1/0/all/0/1"&gt;Motonobu Kanagawa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Sejdinovic_D/0/1/0/all/0/1"&gt;Dino Sejdinovic&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Uncertainty Characteristics Curves: A Systematic Assessment of Prediction Intervals. (arXiv:2106.00858v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.00858</id>
        <link href="http://arxiv.org/abs/2106.00858"/>
        <updated>2021-06-03T02:10:35.236Z</updated>
        <summary type="html"><![CDATA[Accurate quantification of model uncertainty has long been recognized as a
fundamental requirement for trusted AI. In regression tasks, uncertainty is
typically quantified using prediction intervals calibrated to a specific
operating point, making evaluation and comparison across different studies
difficult. Our work leverages: (1) the concept of operating characteristics
curves and (2) the notion of a gain over a simple reference, to derive a novel
operating point agnostic assessment methodology for prediction intervals. The
paper describes the corresponding algorithm, provides a theoretical analysis,
and demonstrates its utility in multiple scenarios. We argue that the proposed
method addresses the current need for comprehensive assessment of prediction
intervals and thus represents a valuable addition to the uncertainty
quantification toolbox.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Navratil_J/0/1/0/all/0/1"&gt;Jiri Navratil&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Elder_B/0/1/0/all/0/1"&gt;Benjamin Elder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arnold_M/0/1/0/all/0/1"&gt;Matthew Arnold&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1"&gt;Soumya Ghosh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sattigeri_P/0/1/0/all/0/1"&gt;Prasanna Sattigeri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DialoGraph: Incorporating Interpretable Strategy-Graph Networks into Negotiation Dialogues. (arXiv:2106.00920v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.00920</id>
        <link href="http://arxiv.org/abs/2106.00920"/>
        <updated>2021-06-03T02:10:35.208Z</updated>
        <summary type="html"><![CDATA[To successfully negotiate a deal, it is not enough to communicate fluently:
pragmatic planning of persuasive negotiation strategies is essential. While
modern dialogue agents excel at generating fluent sentences, they still lack
pragmatic grounding and cannot reason strategically. We present DialoGraph, a
negotiation system that incorporates pragmatic strategies in a negotiation
dialogue using graph neural networks. DialoGraph explicitly incorporates
dependencies between sequences of strategies to enable improved and
interpretable prediction of next optimal strategies, given the dialogue
context. Our graph-based method outperforms prior state-of-the-art negotiation
models both in the accuracy of strategy/dialogue act prediction and in the
quality of downstream dialogue response generation. We qualitatively show
further benefits of learned strategy-graphs in providing explicit associations
between effective negotiation strategies over the course of the dialogue,
leading to interpretable and strategic dialogues.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Joshi_R/0/1/0/all/0/1"&gt;Rishabh Joshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balachandran_V/0/1/0/all/0/1"&gt;Vidhisha Balachandran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vashishth_S/0/1/0/all/0/1"&gt;Shikhar Vashishth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Black_A/0/1/0/all/0/1"&gt;Alan Black&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsvetkov_Y/0/1/0/all/0/1"&gt;Yulia Tsvetkov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Empirical Comparison of Off-policy Prediction Learning Algorithms on the Collision Task. (arXiv:2106.00922v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.00922</id>
        <link href="http://arxiv.org/abs/2106.00922"/>
        <updated>2021-06-03T02:10:35.200Z</updated>
        <summary type="html"><![CDATA[Off-policy prediction -- learning the value function for one policy from data
generated while following another policy -- is one of the most challenging
subproblems in reinforcement learning. This paper presents empirical results
with eleven prominent off-policy learning algorithms that use linear function
approximation: five Gradient-TD methods, two Emphatic-TD methods, Off-policy
TD($\lambda$), Vtrace, and versions of Tree Backup and ABQ modified to apply to
a prediction setting. Our experiments used the Collision task, a small
idealized off-policy problem analogous to that of an autonomous car trying to
predict whether it will collide with an obstacle. We assessed the performance
of the algorithms according to their learning rate, asymptotic error level, and
sensitivity to step-size and bootstrapping parameters. By these measures, the
eleven algorithms can be partially ordered on the Collision task. In the top
tier, the two Emphatic-TD algorithms learned the fastest, reached the lowest
errors, and were robust to parameter settings. In the middle tier, the five
Gradient-TD algorithms and Off-policy TD($\lambda$) were more sensitive to the
bootstrapping parameter. The bottom tier comprised Vtrace, Tree Backup, and
ABQ; these algorithms were no faster and had higher asymptotic error than the
others. Our results are definitive for this task, though of course experiments
with more tasks are needed before an overall assessment of the algorithms'
merits can be made.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ghiassian_S/0/1/0/all/0/1"&gt;Sina Ghiassian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sutton_R/0/1/0/all/0/1"&gt;Richard S. Sutton&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sharp bounds for the number of regions of maxout networks and vertices of Minkowski sums. (arXiv:2104.08135v1 [math.CO] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2104.08135</id>
        <link href="http://arxiv.org/abs/2104.08135"/>
        <updated>2021-06-03T02:10:35.196Z</updated>
        <summary type="html"><![CDATA[We present results on the number of linear regions of the functions that can
be represented by artificial feedforward neural networks with maxout units. A
rank-k maxout unit is a function computing the maximum of $k$ linear functions.
For networks with a single layer of maxout units, the linear regions correspond
to the upper vertices of a Minkowski sum of polytopes. We obtain face counting
formulas in terms of the intersection posets of tropical hypersurfaces or the
number of upper faces of partial Minkowski sums, along with explicit sharp
upper bounds for the number of regions for any input dimension, any number of
units, and any ranks, in the cases with and without biases. Based on these
results we also obtain asymptotically sharp upper bounds for networks with
multiple layers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Montufar_G/0/1/0/all/0/1"&gt;Guido Mont&amp;#xfa;far&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Ren_Y/0/1/0/all/0/1"&gt;Yue Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Leon Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Supervised Document Similarity Ranking via Contextualized Language Models and Hierarchical Inference. (arXiv:2106.01186v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01186</id>
        <link href="http://arxiv.org/abs/2106.01186"/>
        <updated>2021-06-03T02:10:35.191Z</updated>
        <summary type="html"><![CDATA[We present a novel model for the problem of ranking a collection of documents
according to their semantic similarity to a source (query) document. While the
problem of document-to-document similarity ranking has been studied, most
modern methods are limited to relatively short documents or rely on the
existence of "ground-truth" similarity labels. Yet, in most common real-world
cases, similarity ranking is an unsupervised problem as similarity labels are
unavailable. Moreover, an ideal model should not be restricted by documents'
length. Hence, we introduce SDR, a self-supervised method for document
similarity that can be applied to documents of arbitrary length. Importantly,
SDR can be effectively applied to extremely long documents, exceeding the 4,096
maximal token limits of Longformer. Extensive evaluations on large document
datasets show that SDR significantly outperforms its alternatives across all
metrics. To accelerate future research on unlabeled long document similarity
ranking, and as an additional contribution to the community, we herein publish
two human-annotated test sets of long documents similarity evaluation. The SDR
code and datasets are publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ginzburg_D/0/1/0/all/0/1"&gt;Dvir Ginzburg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Malkiel_I/0/1/0/all/0/1"&gt;Itzik Malkiel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barkan_O/0/1/0/all/0/1"&gt;Oren Barkan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Caciularu_A/0/1/0/all/0/1"&gt;Avi Caciularu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koenigstein_N/0/1/0/all/0/1"&gt;Noam Koenigstein&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An End-to-End Framework for Molecular Conformation Generation via Bilevel Programming. (arXiv:2105.07246v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.07246</id>
        <link href="http://arxiv.org/abs/2105.07246"/>
        <updated>2021-06-03T02:10:35.128Z</updated>
        <summary type="html"><![CDATA[Predicting molecular conformations (or 3D structures) from molecular graphs
is a fundamental problem in many applications. Most existing approaches are
usually divided into two steps by first predicting the distances between atoms
and then generating a 3D structure through optimizing a distance geometry
problem. However, the distances predicted with such two-stage approaches may
not be able to consistently preserve the geometry of local atomic
neighborhoods, making the generated structures unsatisfying. In this paper, we
propose an end-to-end solution for molecular conformation prediction called
ConfVAE based on the conditional variational autoencoder framework.
Specifically, the molecular graph is first encoded in a latent space, and then
the 3D structures are generated by solving a principled bilevel optimization
program. Extensive experiments on several benchmark data sets prove the
effectiveness of our proposed approach over existing state-of-the-art
approaches. Code is available at
\url{https://github.com/MinkaiXu/ConfVAE-ICML21}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1"&gt;Minkai Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wujie Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_S/0/1/0/all/0/1"&gt;Shitong Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_C/0/1/0/all/0/1"&gt;Chence Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1"&gt;Yoshua Bengio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gomez_Bombarelli_R/0/1/0/all/0/1"&gt;Rafael Gomez-Bombarelli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1"&gt;Jian Tang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VideoForensicsHQ: Detecting High-quality Manipulated Face Videos. (arXiv:2005.10360v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.10360</id>
        <link href="http://arxiv.org/abs/2005.10360"/>
        <updated>2021-06-03T02:10:35.113Z</updated>
        <summary type="html"><![CDATA[There are concerns that new approaches to the synthesis of high quality face
videos may be misused to manipulate videos with malicious intent. The research
community therefore developed methods for the detection of modified footage and
assembled benchmark datasets for this task. In this paper, we examine how the
performance of forgery detectors depends on the presence of artefacts that the
human eye can see. We introduce a new benchmark dataset for face video forgery
detection, of unprecedented quality. It allows us to demonstrate that existing
detection techniques have difficulties detecting fakes that reliably fool the
human eye. We thus introduce a new family of detectors that examine
combinations of spatial and temporal features and outperform existing
approaches both in terms of detection accuracy and generalization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fox_G/0/1/0/all/0/1"&gt;Gereon Fox&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wentao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1"&gt;Hyeongwoo Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seidel_H/0/1/0/all/0/1"&gt;Hans-Peter Seidel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Elgharib_M/0/1/0/all/0/1"&gt;Mohamed Elgharib&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1"&gt;Christian Theobalt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generating SOAP Notes from Doctor-Patient Conversations Using Modular Summarization Techniques. (arXiv:2005.01795v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.01795</id>
        <link href="http://arxiv.org/abs/2005.01795"/>
        <updated>2021-06-03T02:10:35.109Z</updated>
        <summary type="html"><![CDATA[Following each patient visit, physicians draft long semi-structured clinical
summaries called SOAP notes. While invaluable to clinicians and researchers,
creating digital SOAP notes is burdensome, contributing to physician burnout.
In this paper, we introduce the first complete pipelines to leverage deep
summarization models to generate these notes based on transcripts of
conversations between physicians and patients. After exploring a spectrum of
methods across the extractive-abstractive spectrum, we propose Cluster2Sent, an
algorithm that (i) extracts important utterances relevant to each summary
section; (ii) clusters together related utterances; and then (iii) generates
one summary sentence per cluster. Cluster2Sent outperforms its purely
abstractive counterpart by 8 ROUGE-1 points, and produces significantly more
factual and coherent sentences as assessed by expert human evaluators. For
reproducibility, we demonstrate similar benefits on the publicly available AMI
dataset. Our results speak to the benefits of structuring summaries into
sections and annotating supporting evidence when constructing summarization
corpora.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Krishna_K/0/1/0/all/0/1"&gt;Kundan Krishna&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khosla_S/0/1/0/all/0/1"&gt;Sopan Khosla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bigham_J/0/1/0/all/0/1"&gt;Jeffrey P. Bigham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lipton_Z/0/1/0/all/0/1"&gt;Zachary C. Lipton&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CycleSegNet: Object Co-segmentation with Cycle Refinement and Region Correspondence. (arXiv:2101.01308v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.01308</id>
        <link href="http://arxiv.org/abs/2101.01308"/>
        <updated>2021-06-03T02:10:35.099Z</updated>
        <summary type="html"><![CDATA[Image co-segmentation is an active computer vision task that aims to segment
the common objects from a set of images. Recently, researchers design various
learning-based algorithms to undertake the co-segmentation task. The main
difficulty in this task is how to effectively transfer information between
images to make conditional predictions. In this paper, we present CycleSegNet,
a novel framework for the co-segmentation task. Our network design has two key
components: a region correspondence module which is the basic operation for
exchanging information between local image regions, and a cycle refinement
module, which utilizes ConvLSTMs to progressively update image representations
and exchange information in a cycle and iterative manner. Extensive experiments
demonstrate that our proposed method significantly outperforms the
state-of-the-art methods on four popular benchmark datasets -- PASCAL VOC
dataset, MSRC dataset, Internet dataset, and iCoseg dataset, by 2.6%, 7.7%,
2.2%, and 2.9%, respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1"&gt;Guankai Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1"&gt;Guosheng Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1"&gt;Qingyao Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_R/0/1/0/all/0/1"&gt;Rui Yao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pathwise Conditioning of Gaussian Processes. (arXiv:2011.04026v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.04026</id>
        <link href="http://arxiv.org/abs/2011.04026"/>
        <updated>2021-06-03T02:10:35.087Z</updated>
        <summary type="html"><![CDATA[As Gaussian processes are used to answer increasingly complex questions,
analytic solutions become scarcer and scarcer. Monte Carlo methods act as a
convenient bridge for connecting intractable mathematical expressions with
actionable estimates via sampling. Conventional approaches for simulating
Gaussian process posteriors view samples as draws from marginal distributions
of process values at finite sets of input locations. This distribution-centric
characterization leads to generative strategies that scale cubically in the
size of the desired random vector. These methods are prohibitively expensive in
cases where we would, ideally, like to draw high-dimensional vectors or even
continuous sample paths. In this work, we investigate a different line of
reasoning: rather than focusing on distributions, we articulate Gaussian
conditionals at the level of random variables. We show how this pathwise
interpretation of conditioning gives rise to a general family of approximations
that lend themselves to efficiently sampling Gaussian process posteriors.
Starting from first principles, we derive these methods and analyze the
approximation errors they introduce. We, then, ground these results by
exploring the practical implications of pathwise conditioning in various
applied settings, such as global optimization and reinforcement learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Wilson_J/0/1/0/all/0/1"&gt;James T. Wilson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Borovitskiy_V/0/1/0/all/0/1"&gt;Viacheslav Borovitskiy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Terenin_A/0/1/0/all/0/1"&gt;Alexander Terenin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Mostowsky_P/0/1/0/all/0/1"&gt;Peter Mostowsky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Deisenroth_M/0/1/0/all/0/1"&gt;Marc Peter Deisenroth&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning a Single Neuron with Bias Using Gradient Descent. (arXiv:2106.01101v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01101</id>
        <link href="http://arxiv.org/abs/2106.01101"/>
        <updated>2021-06-03T02:10:35.066Z</updated>
        <summary type="html"><![CDATA[We theoretically study the fundamental problem of learning a single neuron
with a bias term ($\mathbf{x} \mapsto \sigma(<\mathbf{w},\mathbf{x}> + b)$) in
the realizable setting with the ReLU activation, using gradient descent.
Perhaps surprisingly, we show that this is a significantly different and more
challenging problem than the bias-less case (which was the focus of previous
works on single neurons), both in terms of the optimization geometry as well as
the ability of gradient methods to succeed in some scenarios. We provide a
detailed study of this problem, characterizing the critical points of the
objective, demonstrating failure cases, and providing positive convergence
guarantees under different sets of assumptions. To prove our results, we
develop some tools which may be of independent interest, and improve previous
results on learning single neurons.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vardi_G/0/1/0/all/0/1"&gt;Gal Vardi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yehudai_G/0/1/0/all/0/1"&gt;Gilad Yehudai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shamir_O/0/1/0/all/0/1"&gt;Ohad Shamir&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Reinforcement Learning-based UAV Navigation and Control: A Soft Actor-Critic with Hindsight Experience Replay Approach. (arXiv:2106.01016v1 [eess.SY])]]></title>
        <id>http://arxiv.org/abs/2106.01016</id>
        <link href="http://arxiv.org/abs/2106.01016"/>
        <updated>2021-06-03T02:10:35.051Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose SACHER (soft actor-critic (SAC) with hindsight
experience replay (HER)), which constitutes a class of deep reinforcement
learning (DRL) algorithms. SAC is known as an off-policy model-free DRL
algorithm based on the maximum entropy framework, which outperforms earlier DRL
algorithms in terms of exploration, robustness and learning performance.
However, in SAC, maximizing the entropy-augmented objective may degrade the
optimality of the learning outcomes. HER is known as a sample-efficient replay
method that enhances the performance of off-policy DRL algorithms by allowing
them to learn from both failures and successes. We apply HER to SAC and propose
SACHER to improve the learning performance of SAC. More precisely, SACHER
achieves the desired optimal outcomes faster and more accurately than SAC,
since HER improves the sample efficiency of SAC. We apply SACHER to the
navigation and control problem of unmanned aerial vehicles (UAVs), where SACHER
generates the optimal navigation path of the UAV under various obstacles in
operation. Specifically, we show the effectiveness of SACHER in terms of the
tracking error and cumulative reward in UAV operation by comparing them with
those of state-of-the-art DRL algorithms, SAC and DDPG. Note that SACHER in UAV
navigation and control problems can be applied to arbitrary models of UAVs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Lee_M/0/1/0/all/0/1"&gt;Myoung Hoon Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Moon_J/0/1/0/all/0/1"&gt;Jun Moon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GraghVQA: Language-Guided Graph Neural Networks for Graph-based Visual Question Answering. (arXiv:2104.10283v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.10283</id>
        <link href="http://arxiv.org/abs/2104.10283"/>
        <updated>2021-06-03T02:10:35.045Z</updated>
        <summary type="html"><![CDATA[Images are more than a collection of objects or attributes -- they represent
a web of relationships among interconnected objects. Scene Graph has emerged as
a new modality for a structured graphical representation of images. Scene Graph
encodes objects as nodes connected via pairwise relations as edges. To support
question answering on scene graphs, we propose GraphVQA, a language-guided
graph neural network framework that translates and executes a natural language
question as multiple iterations of message passing among graph nodes. We
explore the design space of GraphVQA framework, and discuss the trade-off of
different design choices. Our experiments on GQA dataset show that GraphVQA
outperforms the state-of-the-art model by a large margin (88.43% vs. 94.78%).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liang_W/0/1/0/all/0/1"&gt;Weixin Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1"&gt;Yanhao Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zixuan Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Database Reasoning Over Text. (arXiv:2106.01074v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01074</id>
        <link href="http://arxiv.org/abs/2106.01074"/>
        <updated>2021-06-03T02:10:35.041Z</updated>
        <summary type="html"><![CDATA[Neural models have shown impressive performance gains in answering queries
from natural language text. However, existing works are unable to support
database queries, such as "List/Count all female athletes who were born in 20th
century", which require reasoning over sets of relevant facts with operations
such as join, filtering and aggregation. We show that while state-of-the-art
transformer models perform very well for small databases, they exhibit
limitations in processing noisy data, numerical operations, and queries that
aggregate facts. We propose a modular architecture to answer these
database-style queries over multiple spans from text and aggregating these at
scale. We evaluate the architecture using WikiNLDB, a novel dataset for
exploring such queries. Our architecture scales to databases containing
thousands of facts whereas contemporary models are limited by how many facts
can be encoded. In direct comparison on small databases, our approach increases
overall answer accuracy from 85% to 90%. On larger databases, our approach
retains its accuracy whereas transformer baselines could not encode the
context.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Thorne_J/0/1/0/all/0/1"&gt;James Thorne&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yazdani_M/0/1/0/all/0/1"&gt;Majid Yazdani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saeidi_M/0/1/0/all/0/1"&gt;Marzieh Saeidi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Silvestri_F/0/1/0/all/0/1"&gt;Fabrizio Silvestri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Riedel_S/0/1/0/all/0/1"&gt;Sebastian Riedel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Halevy_A/0/1/0/all/0/1"&gt;Alon Halevy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Enriching Transformers with Structured Tensor-Product Representations for Abstractive Summarization. (arXiv:2106.01317v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01317</id>
        <link href="http://arxiv.org/abs/2106.01317"/>
        <updated>2021-06-03T02:10:35.036Z</updated>
        <summary type="html"><![CDATA[Abstractive summarization, the task of generating a concise summary of input
documents, requires: (1) reasoning over the source document to determine the
salient pieces of information scattered across the long document, and (2)
composing a cohesive text by reconstructing these salient facts into a shorter
summary that faithfully reflects the complex relations connecting these facts.
In this paper, we adapt TP-TRANSFORMER (Schlag et al., 2019), an architecture
that enriches the original Transformer (Vaswani et al., 2017) with the
explicitly compositional Tensor Product Representation (TPR), for the task of
abstractive summarization. The key feature of our model is a structural bias
that we introduce by encoding two separate representations for each token to
represent the syntactic structure (with role vectors) and semantic content
(with filler vectors) separately. The model then binds the role and filler
vectors into the TPR as the layer output. We argue that the structured
intermediate representations enable the model to take better control of the
contents (salient facts) and structures (the syntax that connects the facts)
when generating the summary. Empirically, we show that our TP-TRANSFORMER
outperforms the Transformer and the original TP-TRANSFORMER significantly on
several abstractive summarization datasets based on both automatic and human
evaluations. On several syntactic and semantic probing tasks, we demonstrate
the emergent structural information in the role vectors and improved syntactic
interpretability in the TPR layer outputs. Code and models are available at
https://github.com/jiangycTarheel/TPT-Summ.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1"&gt;Yichen Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Celikyilmaz_A/0/1/0/all/0/1"&gt;Asli Celikyilmaz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smolensky_P/0/1/0/all/0/1"&gt;Paul Smolensky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soulos_P/0/1/0/all/0/1"&gt;Paul Soulos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rao_S/0/1/0/all/0/1"&gt;Sudha Rao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Palangi_H/0/1/0/all/0/1"&gt;Hamid Palangi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fernandez_R/0/1/0/all/0/1"&gt;Roland Fernandez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smith_C/0/1/0/all/0/1"&gt;Caitlin Smith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1"&gt;Mohit Bansal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1"&gt;Jianfeng Gao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MPASNET: Motion Prior-Aware Siamese Network for Unsupervised Deep Crowd Segmentation in Video Scenes. (arXiv:2101.08609v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.08609</id>
        <link href="http://arxiv.org/abs/2101.08609"/>
        <updated>2021-06-03T02:10:35.030Z</updated>
        <summary type="html"><![CDATA[Crowd segmentation is a fundamental task serving as the basis of crowded
scene analysis, and it is highly desirable to obtain refined pixel-level
segmentation maps. However, it remains a challenging problem, as existing
approaches either require dense pixel-level annotations to train deep learning
models or merely produce rough segmentation maps from optical or particle flows
with physical models. In this paper, we propose the Motion Prior-Aware Siamese
Network (MPASNET) for unsupervised crowd semantic segmentation. This model not
only eliminates the need for annotation but also yields high-quality
segmentation maps. Specially, we first analyze the coherent motion patterns
across the frames and then apply a circular region merging strategy on the
collective particles to generate pseudo-labels. Moreover, we equip MPASNET with
siamese branches for augmentation-invariant regularization and siamese feature
aggregation. Experiments over benchmark datasets indicate that our model
outperforms the state-of-the-arts by more than 12% in terms of mIoU.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jinhai Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Hua Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fair-Net: A Network Architecture For Reducing Performance Disparity Between Identifiable Sub-Populations. (arXiv:2106.00720v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.00720</id>
        <link href="http://arxiv.org/abs/2106.00720"/>
        <updated>2021-06-03T02:10:35.016Z</updated>
        <summary type="html"><![CDATA[In real world datasets, particular groups are under-represented, much rarer
than others, and machine learning classifiers will often preform worse on
under-represented populations. This problem is aggravated across many domains
where datasets are class imbalanced, with a minority class far rarer than the
majority class. Naive approaches to handle under-representation and class
imbalance include training sub-population specific classifiers that handle
class imbalance or training a global classifier that overlooks sub-population
disparities and aims to achieve high overall accuracy by handling class
imbalance. In this study, we find that these approaches are vulnerable in class
imbalanced datasets with minority sub-populations. We introduced Fair-Net, a
branched multitask neural network architecture that improves both
classification accuracy and probability calibration across identifiable
sub-populations in class imbalanced datasets. Fair-Nets is a straightforward
extension to the output layer and error function of a network, so can be
incorporated in far more complex architectures. Empirical studies with three
real world benchmark datasets demonstrate that Fair-Net improves classification
and calibration performance, substantially reducing performance disparity
between gender and racial sub-populations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Datta_A/0/1/0/all/0/1"&gt;Arghya Datta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Swamidass_S/0/1/0/all/0/1"&gt;S. Joshua Swamidass&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VECO: Variable and Flexible Cross-lingual Pre-training for Language Understanding and Generation. (arXiv:2010.16046v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.16046</id>
        <link href="http://arxiv.org/abs/2010.16046"/>
        <updated>2021-06-03T02:10:35.011Z</updated>
        <summary type="html"><![CDATA[Existing work in multilingual pretraining has demonstrated the potential of
cross-lingual transferability by training a unified Transformer encoder for
multiple languages. However, much of this work only relies on the shared
vocabulary and bilingual contexts to encourage the correlation across
languages, which is loose and implicit for aligning the contextual
representations between languages. In this paper, we plug a cross-attention
module into the Transformer encoder to explicitly build the interdependence
between languages. It can effectively avoid the degeneration of predicting
masked words only conditioned on the context in its own language. More
importantly, when fine-tuning on downstream tasks, the cross-attention module
can be plugged in or out on-demand, thus naturally benefiting a wider range of
cross-lingual tasks, from language understanding to generation.

As a result, the proposed cross-lingual model delivers new state-of-the-art
results on various cross-lingual understanding tasks of the XTREME benchmark,
covering text classification, sequence labeling, question answering, and
sentence retrieval. For cross-lingual generation tasks, it also outperforms all
existing cross-lingual models and state-of-the-art Transformer variants on
WMT14 English-to-German and English-to-French translation datasets, with gains
of up to 1~2 BLEU.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Luo_F/0/1/0/all/0/1"&gt;Fuli Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jiahao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yijia Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bi_B/0/1/0/all/0/1"&gt;Bin Bi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1"&gt;Songfang Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1"&gt;Fei Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1"&gt;Luo Si&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Efficacy of Adversarial Data Collection for Question Answering: Results from a Large-Scale Randomized Study. (arXiv:2106.00872v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.00872</id>
        <link href="http://arxiv.org/abs/2106.00872"/>
        <updated>2021-06-03T02:10:35.006Z</updated>
        <summary type="html"><![CDATA[In adversarial data collection (ADC), a human workforce interacts with a
model in real time, attempting to produce examples that elicit incorrect
predictions. Researchers hope that models trained on these more challenging
datasets will rely less on superficial patterns, and thus be less brittle.
However, despite ADC's intuitive appeal, it remains unclear when training on
adversarial datasets produces more robust models. In this paper, we conduct a
large-scale controlled study focused on question answering, assigning workers
at random to compose questions either (i) adversarially (with a model in the
loop); or (ii) in the standard fashion (without a model). Across a variety of
models and datasets, we find that models trained on adversarial data usually
perform better on other adversarial datasets but worse on a diverse collection
of out-of-domain evaluation sets. Finally, we provide a qualitative analysis of
adversarial (vs standard) data, identifying key differences and offering
guidance for future research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kaushik_D/0/1/0/all/0/1"&gt;Divyansh Kaushik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kiela_D/0/1/0/all/0/1"&gt;Douwe Kiela&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lipton_Z/0/1/0/all/0/1"&gt;Zachary C. Lipton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yih_W/0/1/0/all/0/1"&gt;Wen-tau Yih&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semi-Supervised Semantic Segmentation with Cross Pseudo Supervision. (arXiv:2106.01226v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.01226</id>
        <link href="http://arxiv.org/abs/2106.01226"/>
        <updated>2021-06-03T02:10:35.001Z</updated>
        <summary type="html"><![CDATA[In this paper, we study the semi-supervised semantic segmentation problem via
exploring both labeled data and extra unlabeled data. We propose a novel
consistency regularization approach, called cross pseudo supervision (CPS). Our
approach imposes the consistency on two segmentation networks perturbed with
different initialization for the same input image. The pseudo one-hot label
map, output from one perturbed segmentation network, is used to supervise the
other segmentation network with the standard cross-entropy loss, and vice
versa. The CPS consistency has two roles: encourage high similarity between the
predictions of two perturbed networks for the same input image, and expand
training data by using the unlabeled data with pseudo labels. Experiment
results show that our approach achieves the state-of-the-art semi-supervised
segmentation performance on Cityscapes and PASCAL VOC 2012.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xiaokang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1"&gt;Yuhui Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_G/0/1/0/all/0/1"&gt;Gang Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jingdong Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Robustness of Text-to-SQL Models against Synonym Substitution. (arXiv:2106.01065v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01065</id>
        <link href="http://arxiv.org/abs/2106.01065"/>
        <updated>2021-06-03T02:10:34.996Z</updated>
        <summary type="html"><![CDATA[Recently, there has been significant progress in studying neural networks to
translate text descriptions into SQL queries. Despite achieving good
performance on some public benchmarks, existing text-to-SQL models typically
rely on the lexical matching between words in natural language (NL) questions
and tokens in table schemas, which may render the models vulnerable to attacks
that break the schema linking mechanism. In this work, we investigate the
robustness of text-to-SQL models to synonym substitution. In particular, we
introduce Spider-Syn, a human-curated dataset based on the Spider benchmark for
text-to-SQL translation. NL questions in Spider-Syn are modified from Spider,
by replacing their schema-related words with manually selected synonyms that
reflect real-world question paraphrases. We observe that the accuracy
dramatically drops by eliminating such explicit correspondence between NL
questions and table schemas, even if the synonyms are not adversarially
selected to conduct worst-case adversarial attacks. Finally, we present two
categories of approaches to improve the model robustness. The first category of
approaches utilizes additional synonym annotations for table schemas by
modifying the model input, while the second category is based on adversarial
training. We demonstrate that both categories of approaches significantly
outperform their counterparts without the defense, and the first category of
approaches are more effective.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gan_Y/0/1/0/all/0/1"&gt;Yujian Gan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xinyun Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1"&gt;Qiuping Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Purver_M/0/1/0/all/0/1"&gt;Matthew Purver&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Woodward_J/0/1/0/all/0/1"&gt;John R. Woodward&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1"&gt;Jinxia Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_P/0/1/0/all/0/1"&gt;Pengsheng Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How Do Neural Networks Estimate Optical Flow? A Neuropsychology-Inspired Study. (arXiv:2004.09317v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.09317</id>
        <link href="http://arxiv.org/abs/2004.09317"/>
        <updated>2021-06-03T02:10:34.992Z</updated>
        <summary type="html"><![CDATA[End-to-end trained convolutional neural networks have led to a breakthrough
in optical flow estimation. The most recent advances focus on improving the
optical flow estimation by improving the architecture and setting a new
benchmark on the publicly available MPI-Sintel dataset. Instead, in this
article, we investigate how deep neural networks estimate optical flow. A
better understanding of how these networks function is important for (i)
assessing their generalization capabilities to unseen inputs, and (ii)
suggesting changes to improve their performance. For our investigation, we
focus on FlowNetS, as it is the prototype of an encoder-decoder neural network
for optical flow estimation. Furthermore, we use a filter identification method
that has played a major role in uncovering the motion filters present in animal
brains in neuropsychological research. The method shows that the filters in the
deepest layer of FlowNetS are sensitive to a variety of motion patterns. Not
only do we find translation filters, as demonstrated in animal brains, but
thanks to the easier measurements in artificial neural networks, we even unveil
dilation, rotation, and occlusion filters. Furthermore, we find similarities in
the refinement part of the network and the perceptual filling-in process which
occurs in the mammal primary visual cortex.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jong_D/0/1/0/all/0/1"&gt;D. B. de Jong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paredes_Valles_F/0/1/0/all/0/1"&gt;F. Paredes-Vall&amp;#xe9;s&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Croon_G/0/1/0/all/0/1"&gt;G. C. H. E. de Croon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Training Sampling with Monolingual Data Uncertainty for Neural Machine Translation. (arXiv:2106.00941v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.00941</id>
        <link href="http://arxiv.org/abs/2106.00941"/>
        <updated>2021-06-03T02:10:34.976Z</updated>
        <summary type="html"><![CDATA[Self-training has proven effective for improving NMT performance by
augmenting model training with synthetic parallel data. The common practice is
to construct synthetic data based on a randomly sampled subset of large-scale
monolingual data, which we empirically show is sub-optimal. In this work, we
propose to improve the sampling procedure by selecting the most informative
monolingual sentences to complement the parallel data. To this end, we compute
the uncertainty of monolingual sentences using the bilingual dictionary
extracted from the parallel data. Intuitively, monolingual sentences with lower
uncertainty generally correspond to easy-to-translate patterns which may not
provide additional gains. Accordingly, we design an uncertainty-based sampling
strategy to efficiently exploit the monolingual data for self-training, in
which monolingual sentences with higher uncertainty would be sampled with
higher probability. Experimental results on large-scale WMT
English$\Rightarrow$German and English$\Rightarrow$Chinese datasets demonstrate
the effectiveness of the proposed approach. Extensive analyses suggest that
emphasizing the learning on uncertain monolingual sentences by our approach
does improve the translation quality of high-uncertainty sentences and also
benefits the prediction of low-frequency words at the target side.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiao_W/0/1/0/all/0/1"&gt;Wenxiang Jiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xing Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1"&gt;Zhaopeng Tu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1"&gt;Shuming Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lyu_M/0/1/0/all/0/1"&gt;Michael R. Lyu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+King_I/0/1/0/all/0/1"&gt;Irwin King&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data augmentation and pre-trained networks for extremely low data regimes unsupervised visual inspection. (arXiv:2106.01277v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.01277</id>
        <link href="http://arxiv.org/abs/2106.01277"/>
        <updated>2021-06-03T02:10:34.969Z</updated>
        <summary type="html"><![CDATA[The use of deep features coming from pre-trained neural networks for
unsupervised anomaly detection purposes has recently gathered momentum in the
computer vision field. In particular, industrial inspection applications can
take advantage of such features, as demonstrated by the multiple successes of
related methods on the MVTec Anomaly Detection (MVTec AD) dataset. These
methods make use of neural networks pre-trained on auxiliary classification
tasks such as ImageNet. However, to our knowledge, no comparative study of
robustness to the low data regimes between these approaches has been conducted
yet. For quality inspection applications, the handling of limited sample sizes
may be crucial as large quantities of images are not available for small
series. In this work, we aim to compare three approaches based on deep
pre-trained features when varying the quantity of available data in MVTec AD:
KNN, Mahalanobis, and PaDiM. We show that although these methods are mostly
robust to small sample sizes, they still can benefit greatly from using data
augmentation in the original image space, which allows to deal with very small
production runs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gutierrez_P/0/1/0/all/0/1"&gt;Pierre Gutierrez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cordier_A/0/1/0/all/0/1"&gt;Antoine Cordier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Caldeira_T/0/1/0/all/0/1"&gt;Tha&amp;#xef;s Caldeira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sautory_T/0/1/0/all/0/1"&gt;Th&amp;#xe9;ophile Sautory&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DFGC 2021: A DeepFake Game Competition. (arXiv:2106.01217v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.01217</id>
        <link href="http://arxiv.org/abs/2106.01217"/>
        <updated>2021-06-03T02:10:34.964Z</updated>
        <summary type="html"><![CDATA[This paper presents a summary of the DFGC 2021 competition. DeepFake
technology is developing fast, and realistic face-swaps are increasingly
deceiving and hard to detect. At the same time, DeepFake detection methods are
also improving. There is a two-party game between DeepFake creators and
detectors. This competition provides a common platform for benchmarking the
adversarial game between current state-of-the-art DeepFake creation and
detection methods. In this paper, we present the organization, results and top
solutions of this competition and also share our insights obtained during this
event. We also release the DFGC-21 testing dataset collected from our
participants to further benefit the research community.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Peng_B/0/1/0/all/0/1"&gt;Bo Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1"&gt;Hongxing Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1"&gt;Jing Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yuezun Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lyu_S/0/1/0/all/0/1"&gt;Siwei Lyu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1"&gt;Qi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1"&gt;Zhenan Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Han Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1"&gt;Baoying Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1"&gt;Yanjie Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_S/0/1/0/all/0/1"&gt;Shenghai Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1"&gt;Junrui Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1"&gt;Yutong Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1"&gt;Boyuan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ling_H/0/1/0/all/0/1"&gt;Hefei Ling&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1"&gt;Guosheng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zhiliang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Miao_C/0/1/0/all/0/1"&gt;Changtao Miao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1"&gt;Changlei Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1"&gt;Shan He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1"&gt;Xiaoyan Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhuang_W/0/1/0/all/0/1"&gt;Wanyi Zhuang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stochastic Optimization of Areas Under Precision-Recall Curves with Provable Convergence. (arXiv:2104.08736v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.08736</id>
        <link href="http://arxiv.org/abs/2104.08736"/>
        <updated>2021-06-03T02:10:34.958Z</updated>
        <summary type="html"><![CDATA[Areas under ROC (AUROC) and precision-recall curves (AUPRC) are common
metrics for evaluating classification performance for imbalanced problems.
Compared with AUROC, AUPRC is a more appropriate metric for highly imbalanced
datasets. While stochastic optimization of AUROC has been studied extensively,
principled stochastic optimization of AUPRC has been rarely explored. In this
work, we propose a principled technical method to optimize AUPRC for deep
learning. Our approach is based on maximizing the averaged precision (AP),
which is an unbiased point estimator of AUPRC. We cast the objective into a sum
of {\it dependent compositional functions} with inner functions dependent on
random variables of the outer level. We propose efficient adaptive and
non-adaptive stochastic algorithms with {\it provable convergence guarantee
under mild conditions} by leveraging recent advances in stochastic
compositional optimization. Extensive experimental results on image and graph
datasets demonstrate that our proposed method outperforms prior methods on
imbalanced problems in terms of AUPRC. To the best of our knowledge, our work
represents the first attempt to optimize AUPRC with provable convergence.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qi_Q/0/1/0/all/0/1"&gt;Qi Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1"&gt;Youzhi Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zhao Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1"&gt;Shuiwang Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1"&gt;Tianbao Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Making Pre-trained Language Models Better Few-shot Learners. (arXiv:2012.15723v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.15723</id>
        <link href="http://arxiv.org/abs/2012.15723"/>
        <updated>2021-06-03T02:10:34.953Z</updated>
        <summary type="html"><![CDATA[The recent GPT-3 model (Brown et al., 2020) achieves remarkable few-shot
performance solely by leveraging a natural-language prompt and a few task
demonstrations as input context. Inspired by their findings, we study few-shot
learning in a more practical scenario, where we use smaller language models for
which fine-tuning is computationally efficient. We present LM-BFF--better
few-shot fine-tuning of language models--a suite of simple and complementary
techniques for fine-tuning language models on a small number of annotated
examples. Our approach includes (1) prompt-based fine-tuning together with a
novel pipeline for automating prompt generation; and (2) a refined strategy for
dynamically and selectively incorporating demonstrations into each context.
Finally, we present a systematic evaluation for analyzing few-shot performance
on a range of NLP tasks, including classification and regression. Our
experiments demonstrate that our methods combine to dramatically outperform
standard fine-tuning procedures in this low resource setting, achieving up to
30% absolute improvement, and 11% on average across all tasks. Our approach
makes minimal assumptions on task resources and domain expertise, and hence
constitutes a strong task-agnostic method for few-shot learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_T/0/1/0/all/0/1"&gt;Tianyu Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fisch_A/0/1/0/all/0/1"&gt;Adam Fisch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1"&gt;Danqi Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[High-Quality Diversification for Task-Oriented Dialogue Systems. (arXiv:2106.00891v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.00891</id>
        <link href="http://arxiv.org/abs/2106.00891"/>
        <updated>2021-06-03T02:10:34.937Z</updated>
        <summary type="html"><![CDATA[Many task-oriented dialogue systems use deep reinforcement learning (DRL) to
learn policies that respond to the user appropriately and complete the tasks
successfully. Training DRL agents with diverse dialogue trajectories prepare
them well for rare user requests and unseen situations. One effective
diversification method is to let the agent interact with a diverse set of
learned user models. However, trajectories created by these artificial user
models may contain generation errors, which can quickly propagate into the
agent's policy. It is thus important to control the quality of the
diversification and resist the noise. In this paper, we propose a novel
dialogue diversification method for task-oriented dialogue systems trained in
simulators. Our method, Intermittent Short Extension Ensemble (I-SEE),
constrains the intensity to interact with an ensemble of diverse user models
and effectively controls the quality of the diversification. Evaluations on the
Multiwoz dataset show that I-SEE successfully boosts the performance of several
state-of-the-art DRL dialogue agents.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1"&gt;Zhiwen Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kulkarni_H/0/1/0/all/0/1"&gt;Hrishikesh Kulkarni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1"&gt;Grace Hui Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Examining the Inductive Bias of Neural Language Models with Artificial Languages. (arXiv:2106.01044v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01044</id>
        <link href="http://arxiv.org/abs/2106.01044"/>
        <updated>2021-06-03T02:10:34.923Z</updated>
        <summary type="html"><![CDATA[Since language models are used to model a wide variety of languages, it is
natural to ask whether the neural architectures used for the task have
inductive biases towards modeling particular types of languages. Investigation
of these biases has proved complicated due to the many variables that appear in
the experimental setup. Languages vary in many typological dimensions, and it
is difficult to single out one or two to investigate without the others acting
as confounders. We propose a novel method for investigating the inductive
biases of language models using artificial languages. These languages are
constructed to allow us to create parallel corpora across languages that differ
only in the typological feature being investigated, such as word order. We then
use them to train and test language models. This constitutes a fully controlled
causal framework, and demonstrates how grammar engineering can serve as a
useful tool for analyzing neural models. Using this method, we find that
commonly used neural architectures exhibit different inductive biases: LSTMs
display little preference with respect to word ordering, while transformers
display a clear preference for some orderings over others. Further, we find
that neither the inductive bias of the LSTM nor that of the transformer appears
to reflect any tendencies that we see in attested natural languages.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+White_J/0/1/0/all/0/1"&gt;Jennifer C. White&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1"&gt;Ryan Cotterell&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Figurative Language in Recognizing Textual Entailment. (arXiv:2106.01195v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01195</id>
        <link href="http://arxiv.org/abs/2106.01195"/>
        <updated>2021-06-03T02:10:34.917Z</updated>
        <summary type="html"><![CDATA[We introduce a collection of recognizing textual entailment (RTE) datasets
focused on figurative language. We leverage five existing datasets annotated
for a variety of figurative language -- simile, metaphor, and irony -- and
frame them into over 12,500 RTE examples.We evaluate how well state-of-the-art
models trained on popular RTE datasets capture different aspects of figurative
language. Our results and analyses indicate that these models might not
sufficiently capture figurative language, struggling to perform pragmatic
inference and reasoning about world knowledge. Ultimately, our datasets provide
a challenging testbed for evaluating RTE models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chakrabarty_T/0/1/0/all/0/1"&gt;Tuhin Chakrabarty&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghosh_D/0/1/0/all/0/1"&gt;Debanjan Ghosh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Poliak_A/0/1/0/all/0/1"&gt;Adam Poliak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muresan_S/0/1/0/all/0/1"&gt;Smaranda Muresan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Masked Face Recognition: Human vs. Machine. (arXiv:2103.01924v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.01924</id>
        <link href="http://arxiv.org/abs/2103.01924"/>
        <updated>2021-06-03T02:10:34.912Z</updated>
        <summary type="html"><![CDATA[The recent COVID-19 pandemic has increased the focus on hygienic and
contactless identity verification methods. However, the pandemic led to the
wide use of face masks, essential to keep the pandemic under control. The
effect of wearing a mask on face recognition in a collaborative environment is
currently sensitive yet understudied issue. Recent reports have tackled this by
evaluating the masked probe effect on the performance of automatic face
recognition solutions. However, such solutions can fail in certain processes,
leading to performing the verification task by a human expert. This work
provides a joint evaluation and in-depth analyses of the face verification
performance of human experts in comparison to state-of-the-art automatic face
recognition solutions. This involves an extensive evaluation with 12 human
experts and 4 automatic recognition solutions. The study concludes with a set
of take-home messages on different aspects of the correlation between the
verification behavior of human and machine.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Damer_N/0/1/0/all/0/1"&gt;Naser Damer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boutros_F/0/1/0/all/0/1"&gt;Fadi Boutros&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sussmilch_M/0/1/0/all/0/1"&gt;Marius S&amp;#xfc;&amp;#xdf;milch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fang_M/0/1/0/all/0/1"&gt;Meiling Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kirchbuchner_F/0/1/0/all/0/1"&gt;Florian Kirchbuchner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kuijper_A/0/1/0/all/0/1"&gt;Arjan Kuijper&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Solving Arithmetic Word Problems with Transformers and Preprocessing of Problem Text. (arXiv:2106.00893v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.00893</id>
        <link href="http://arxiv.org/abs/2106.00893"/>
        <updated>2021-06-03T02:10:34.891Z</updated>
        <summary type="html"><![CDATA[This paper outlines the use of Transformer networks trained to translate math
word problems to equivalent arithmetic expressions in infix, prefix, and
postfix notations. We compare results produced by many neural configurations
and find that most configurations outperform previously reported approaches on
three of four datasets with significant increases in accuracy of over 20
percentage points. The best neural approaches boost accuracy by 30% when
compared to the previous state-of-the-art on some datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Griffith_K/0/1/0/all/0/1"&gt;Kaden Griffith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kalita_J/0/1/0/all/0/1"&gt;Jugal Kalita&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Enabling Efficiency-Precision Trade-offs for Label Trees in Extreme Classification. (arXiv:2106.00730v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.00730</id>
        <link href="http://arxiv.org/abs/2106.00730"/>
        <updated>2021-06-03T02:10:34.877Z</updated>
        <summary type="html"><![CDATA[Extreme multi-label classification (XMC) aims to learn a model that can tag
data points with a subset of relevant labels from an extremely large label set.
Real world e-commerce applications like personalized recommendations and
product advertising can be formulated as XMC problems, where the objective is
to predict for a user a small subset of items from a catalog of several million
products. For such applications, a common approach is to organize these labels
into a tree, enabling training and inference times that are logarithmic in the
number of labels. While training a model once a label tree is available is well
studied, designing the structure of the tree is a difficult task that is not
yet well understood, and can dramatically impact both model latency and
statistical performance. Existing approaches to tree construction fall at an
extreme point, either optimizing exclusively for statistical performance, or
for latency. We propose an efficient information theory inspired algorithm to
construct intermediary operating points that trade off between the benefits of
both. Our algorithm enables interpolation between these objectives, which was
not previously possible. We corroborate our theoretical analysis with numerical
results, showing that on the Wiki-500K benchmark dataset our method can reduce
a proxy for expected latency by up to 28% while maintaining the same accuracy
as Parabel. On several datasets derived from e-commerce customer logs, our
modified label tree is able to improve this expected latency metric by up to
20% while maintaining the same accuracy. Finally, we discuss challenges in
realizing these latency improvements in deployed models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Baharav_T/0/1/0/all/0/1"&gt;Tavor Z. Baharav&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1"&gt;Daniel L. Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kolluri_K/0/1/0/all/0/1"&gt;Kedarnath Kolluri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sanghavi_S/0/1/0/all/0/1"&gt;Sujay Sanghavi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dhillon_I/0/1/0/all/0/1"&gt;Inderjit S. Dhillon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Online Detection of Vibration Anomalies Using Balanced Spiking Neural Networks. (arXiv:2106.00687v1 [cs.NE])]]></title>
        <id>http://arxiv.org/abs/2106.00687</id>
        <link href="http://arxiv.org/abs/2106.00687"/>
        <updated>2021-06-03T02:10:34.748Z</updated>
        <summary type="html"><![CDATA[Vibration patterns yield valuable information about the health state of a
running machine, which is commonly exploited in predictive maintenance tasks
for large industrial systems. However, the overhead, in terms of size,
complexity and power budget, required by classical methods to exploit this
information is often prohibitive for smaller-scale applications such as
autonomous cars, drones or robotics. Here we propose a neuromorphic approach to
perform vibration analysis using spiking neural networks that can be applied to
a wide range of scenarios. We present a spike-based end-to-end pipeline able to
detect system anomalies from vibration data, using building blocks that are
compatible with analog-digital neuromorphic circuits. This pipeline operates in
an online unsupervised fashion, and relies on a cochlea model, on feedback
adaptation and on a balanced spiking neural network. We show that the proposed
method achieves state-of-the-art performance or better against two publicly
available data sets. Further, we demonstrate a working proof-of-concept
implemented on an asynchronous neuromorphic processor device. This work
represents a significant step towards the design and implementation of
autonomous low-power edge-computing devices for online vibration monitoring.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dennler_N/0/1/0/all/0/1"&gt;Nik Dennler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Haessig_G/0/1/0/all/0/1"&gt;Germain Haessig&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cartiglia_M/0/1/0/all/0/1"&gt;Matteo Cartiglia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Indiveri_G/0/1/0/all/0/1"&gt;Giacomo Indiveri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Enhanced Universal Dependency Parsing with Second-Order Inference and Mixture of Training Data. (arXiv:2006.01414v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.01414</id>
        <link href="http://arxiv.org/abs/2006.01414"/>
        <updated>2021-06-03T02:10:34.741Z</updated>
        <summary type="html"><![CDATA[This paper presents the system used in our submission to the \textit{IWPT
2020 Shared Task}. Our system is a graph-based parser with second-order
inference. For the low-resource Tamil corpus, we specially mixed the training
data of Tamil with other languages and significantly improved the performance
of Tamil. Due to our misunderstanding of the submission requirements, we
submitted graphs that are not connected, which makes our system only rank
\textbf{6th} over 10 teams. However, after we fixed this problem, our system is
0.6 ELAS higher than the team that ranked \textbf{1st} in the official results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xinyu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1"&gt;Yong Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tu_K/0/1/0/all/0/1"&gt;Kewei Tu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Infusing Finetuning with Semantic Dependencies. (arXiv:2012.05395v4 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.05395</id>
        <link href="http://arxiv.org/abs/2012.05395"/>
        <updated>2021-06-03T02:10:34.736Z</updated>
        <summary type="html"><![CDATA[For natural language processing systems, two kinds of evidence support the
use of text representations from neural language models "pretrained" on large
unannotated corpora: performance on application-inspired benchmarks (Peters et
al., 2018, inter alia), and the emergence of syntactic abstractions in those
representations (Tenney et al., 2019, inter alia). On the other hand, the lack
of grounded supervision calls into question how well these representations can
ever capture meaning (Bender and Koller, 2020). We apply novel probes to recent
language models -- specifically focusing on predicate-argument structure as
operationalized by semantic dependencies (Ivanova et al., 2012) -- and find
that, unlike syntax, semantics is not brought to the surface by today's
pretrained models. We then use convolutional graph encoders to explicitly
incorporate semantic parses into task-specific finetuning, yielding benefits to
natural language understanding (NLU) tasks in the GLUE benchmark. This approach
demonstrates the potential for general-purpose (rather than task-specific)
linguistic supervision, above and beyond conventional pretraining and
finetuning. Several diagnostics help to localize the benefits of our approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Zhaofeng Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1"&gt;Hao Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1"&gt;Noah A. Smith&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Accented Speech Recognition: A Survey. (arXiv:2104.10747v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.10747</id>
        <link href="http://arxiv.org/abs/2104.10747"/>
        <updated>2021-06-03T02:10:34.730Z</updated>
        <summary type="html"><![CDATA[Automatic Speech Recognition (ASR) systems generalize poorly on accented
speech. The phonetic and linguistic variability of accents present hard
challenges for ASR systems today in both data collection and modeling
strategies. The resulting bias in ASR performance across accents comes at a
cost to both users and providers of ASR.

We present a survey of current promising approaches to accented speech
recognition and highlight the key challenges in the space. Approaches mostly
focus on single model generalization and accent feature engineering. Among the
challenges, lack of a standard benchmark makes research and comparison
especially difficult.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hinsvark_A/0/1/0/all/0/1"&gt;Arthur Hinsvark&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Delworth_N/0/1/0/all/0/1"&gt;Natalie Delworth&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Rio_M/0/1/0/all/0/1"&gt;Miguel Del Rio&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+McNamara_Q/0/1/0/all/0/1"&gt;Quinten McNamara&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1"&gt;Joshua Dong&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Westerman_R/0/1/0/all/0/1"&gt;Ryan Westerman&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1"&gt;Michelle Huang&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Palakapilly_J/0/1/0/all/0/1"&gt;Joseph Palakapilly&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Drexler_J/0/1/0/all/0/1"&gt;Jennifer Drexler&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Pirkin_I/0/1/0/all/0/1"&gt;Ilya Pirkin&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Bhandari_N/0/1/0/all/0/1"&gt;Nishchal Bhandari&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Jette_M/0/1/0/all/0/1"&gt;Miguel Jette&lt;/a&gt; (1) ((1) Rev.com)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Low Complexity Recruitment for Collaborative Mobile Crowdsourcing Using Graph Neural Networks. (arXiv:2106.00717v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.00717</id>
        <link href="http://arxiv.org/abs/2106.00717"/>
        <updated>2021-06-03T02:10:34.714Z</updated>
        <summary type="html"><![CDATA[Collaborative Mobile crowdsourcing (CMCS) allows entities, e.g., local
authorities or individuals, to hire a team of workers from the crowd of
connected people, to execute complex tasks. In this paper, we investigate two
different CMCS recruitment strategies allowing task requesters to form teams of
socially connected and skilled workers: i) a platform-based strategy where the
platform exploits its own knowledge about the workers to form a team and ii) a
leader-based strategy where the platform designates a group leader that
recruits its own suitable team given its own knowledge about its Social Network
(SN) neighbors. We first formulate the recruitment as an Integer Linear Program
(ILP) that optimally forms teams according to four fuzzy-logic-based criteria:
level of expertise, social relationship strength, recruitment cost, and
recruiter's confidence level. To cope with NP-hardness, we design a novel
low-complexity CMCS recruitment approach relying on Graph Neural Networks
(GNNs), specifically graph embedding and clustering techniques, to shrink the
workers' search space and afterwards, exploiting a meta-heuristic genetic
algorithm to select appropriate workers. Simulation results applied on a
real-world dataset illustrate the performance of both proposed CMCS recruitment
approaches. It is shown that our proposed low-complexity GNN-based recruitment
algorithm achieves close performances to those of the baseline ILP with
significant computational time saving and ability to operate on large-scale
mobile crowdsourcing platforms. It is also shown that compared to the
leader-based strategy, the platform-based strategy recruits a more skilled team
but with lower SN relationships and higher cost.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hamrouni_A/0/1/0/all/0/1"&gt;Aymen Hamrouni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghazzai_H/0/1/0/all/0/1"&gt;Hakim Ghazzai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alelyani_T/0/1/0/all/0/1"&gt;Turki Alelyani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Massoud_Y/0/1/0/all/0/1"&gt;Yehia Massoud&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Collaborative Nonstationary Multivariate Gaussian Process Model. (arXiv:2106.00719v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.00719</id>
        <link href="http://arxiv.org/abs/2106.00719"/>
        <updated>2021-06-03T02:10:34.709Z</updated>
        <summary type="html"><![CDATA[Currently, multi-output Gaussian process regression models either do not
model nonstationarity or are associated with severe computational burdens and
storage demands. Nonstationary multi-variate Gaussian process models (NMGP) use
a nonstationary covariance function with an input-dependent linear model of
coregionalisation to jointly model input-dependent correlation, scale, and
smoothness of outputs. Variational sparse approximation relies on inducing
points to enable scalable computations. Here, we take the best of both worlds:
considering an inducing variable framework on the underlying latent functions
in NMGP, we propose a novel model called the collaborative nonstationary
Gaussian process model(CNMGP). For CNMGP, we derive computationally tractable
variational bounds amenable to doubly stochastic variational inference.
Together, this allows us to model data in which outputs do not share a common
input set, with a computational complexity that is independent of the size of
the inputs and outputs. We illustrate the performance of our method on
synthetic data and three real datasets and show that our model generally
pro-vides better predictive performance than the state-of-the-art, and also
provides estimates of time-varying correlations that differ across outputs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Meng_R/0/1/0/all/0/1"&gt;Rui Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1"&gt;Herbie Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bouchard_K/0/1/0/all/0/1"&gt;Kristofer Bouchard&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[embComp: Visual Interactive Comparison of Vector Embeddings. (arXiv:1911.01542v2 [cs.HC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1911.01542</id>
        <link href="http://arxiv.org/abs/1911.01542"/>
        <updated>2021-06-03T02:10:34.703Z</updated>
        <summary type="html"><![CDATA[This paper introduces embComp, a novel approach for comparing two embeddings
that capture the similarity between objects, such as word and document
embeddings. We survey scenarios where comparing these embedding spaces is
useful. From those scenarios, we derive common tasks, introduce visual analysis
methods that support these tasks, and combine them into a comprehensive system.
One of embComp's central features are overview visualizations that are based on
metrics for measuring differences in the local structure around objects.
Summarizing these local metrics over the embeddings provides global overviews
of similarities and differences. Detail views allow comparison of the local
structure around selected objects and relating this local information to the
global views. Integrating and connecting all of these components, embComp
supports a range of analysis workflows that help understand similarities and
differences between embedding spaces. We assess our approach by applying it in
several use cases, including understanding corpora differences via word vector
embeddings, and understanding algorithmic differences in generating embeddings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Heimerl_F/0/1/0/all/0/1"&gt;Florian Heimerl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kralj_C/0/1/0/all/0/1"&gt;Christoph Kralj&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moller_T/0/1/0/all/0/1"&gt;Torsten M&amp;#xf6;ller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gleicher_M/0/1/0/all/0/1"&gt;Michael Gleicher&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Entropy Regularization Free Mechanism for Policy-based Reinforcement Learning. (arXiv:2106.00707v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.00707</id>
        <link href="http://arxiv.org/abs/2106.00707"/>
        <updated>2021-06-03T02:10:34.698Z</updated>
        <summary type="html"><![CDATA[Policy-based reinforcement learning methods suffer from the policy collapse
problem. We find valued-based reinforcement learning methods with
{\epsilon}-greedy mechanism are capable of enjoying three characteristics,
Closed-form Diversity, Objective-invariant Exploration and Adaptive Trade-off,
which help value-based methods avoid the policy collapse problem. However,
there does not exist a parallel mechanism for policy-based methods that
achieves all three characteristics. In this paper, we propose an entropy
regularization free mechanism that is designed for policy-based methods, which
achieves Closed-form Diversity, Objective-invariant Exploration and Adaptive
Trade-off. Our experiments show that our mechanism is super sample-efficient
for policy-based methods and boosts a policy-based baseline to a new
State-Of-The-Art on Arcade Learning Environment.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1"&gt;Changnan Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1"&gt;Haosen Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1"&gt;Jiajun Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1"&gt;Shihong Deng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reservoir Transformers. (arXiv:2012.15045v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.15045</id>
        <link href="http://arxiv.org/abs/2012.15045"/>
        <updated>2021-06-03T02:10:34.692Z</updated>
        <summary type="html"><![CDATA[We demonstrate that transformers obtain impressive performance even when some
of the layers are randomly initialized and never updated. Inspired by old and
well-established ideas in machine learning, we explore a variety of non-linear
"reservoir" layers interspersed with regular transformer layers, and show
improvements in wall-clock compute time until convergence, as well as overall
performance, on various machine translation and (masked) language modelling
tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shen_S/0/1/0/all/0/1"&gt;Sheng Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baevski_A/0/1/0/all/0/1"&gt;Alexei Baevski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morcos_A/0/1/0/all/0/1"&gt;Ari S. Morcos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Keutzer_K/0/1/0/all/0/1"&gt;Kurt Keutzer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Auli_M/0/1/0/all/0/1"&gt;Michael Auli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kiela_D/0/1/0/all/0/1"&gt;Douwe Kiela&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[X-METRA-ADA: Cross-lingual Meta-Transfer Learning Adaptation to Natural Language Understanding and Question Answering. (arXiv:2104.09696v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.09696</id>
        <link href="http://arxiv.org/abs/2104.09696"/>
        <updated>2021-06-03T02:10:34.687Z</updated>
        <summary type="html"><![CDATA[Multilingual models, such as M-BERT and XLM-R, have gained increasing
popularity, due to their zero-shot cross-lingual transfer learning
capabilities. However, their generalization ability is still inconsistent for
typologically diverse languages and across different benchmarks. Recently,
meta-learning has garnered attention as a promising technique for enhancing
transfer learning under low-resource scenarios: particularly for cross-lingual
transfer in Natural Language Understanding (NLU). In this work, we propose
X-METRA-ADA, a cross-lingual MEta-TRAnsfer learning ADAptation approach for
NLU. Our approach adapts MAML, an optimization-based meta-learning approach, to
learn to adapt to new languages. We extensively evaluate our framework on two
challenging cross-lingual NLU tasks: multilingual task-oriented dialog and
typologically diverse question answering. We show that our approach outperforms
naive fine-tuning, reaching competitive performance on both tasks for most
languages. Our analysis reveals that X-METRA-ADA can leverage limited data for
faster adaptation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mhamdi_M/0/1/0/all/0/1"&gt;Meryem M&amp;#x27;hamdi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1"&gt;Doo Soon Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dernoncourt_F/0/1/0/all/0/1"&gt;Franck Dernoncourt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1"&gt;Trung Bui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1"&gt;Xiang Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+May_J/0/1/0/all/0/1"&gt;Jonathan May&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ARBERT & MARBERT: Deep Bidirectional Transformers for Arabic. (arXiv:2101.01785v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.01785</id>
        <link href="http://arxiv.org/abs/2101.01785"/>
        <updated>2021-06-03T02:10:34.671Z</updated>
        <summary type="html"><![CDATA[Pre-trained language models (LMs) are currently integral to many natural
language processing systems. Although multilingual LMs were also introduced to
serve many languages, these have limitations such as being costly at inference
time and the size and diversity of non-English data involved in their
pre-training. We remedy these issues for a collection of diverse Arabic
varieties by introducing two powerful deep bidirectional transformer-based
models, ARBERT and MARBERT. To evaluate our models, we also introduce ARLUE, a
new benchmark for multi-dialectal Arabic language understanding evaluation.
ARLUE is built using $42$ datasets targeting six different task clusters,
allowing us to offer a series of standardized experiments under rich
conditions. When fine-tuned on ARLUE, our models collectively achieve new
state-of-the-art results across the majority of tasks (37 out of 48
classification tasks, on the 42 datasets). Our best model acquires the highest
ARLUE score (77.40) across all six task clusters, outperforming all other
models including XLM-R Large (~ 3.4 x larger size). Our models are publicly
available at https://github.com/UBC-NLP/marbert and ARLUE will be released
through the same repository.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Abdul_Mageed_M/0/1/0/all/0/1"&gt;Muhammad Abdul-Mageed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Elmadany_A/0/1/0/all/0/1"&gt;AbdelRahim Elmadany&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nagoudi_E/0/1/0/all/0/1"&gt;El Moatez Billah Nagoudi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[speechocean762: An Open-Source Non-native English Speech Corpus For Pronunciation Assessment. (arXiv:2104.01378v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.01378</id>
        <link href="http://arxiv.org/abs/2104.01378"/>
        <updated>2021-06-03T02:10:34.665Z</updated>
        <summary type="html"><![CDATA[This paper introduces a new open-source speech corpus named "speechocean762"
designed for pronunciation assessment use, consisting of 5000 English
utterances from 250 non-native speakers, where half of the speakers are
children. Five experts annotated each of the utterances at sentence-level,
word-level and phoneme-level. A baseline system is released in open source to
illustrate the phoneme-level pronunciation assessment workflow on this corpus.
This corpus is allowed to be used freely for commercial and non-commercial
purposes. It is available for free download from OpenSLR, and the corresponding
baseline system is published in the Kaldi speech recognition toolkit.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Junbo Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhiwen Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yongqing Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1"&gt;Zhiyong Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_Q/0/1/0/all/0/1"&gt;Qiong Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yukai Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1"&gt;Ke Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Povey_D/0/1/0/all/0/1"&gt;Daniel Povey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yujun Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Speakers Fill Lexical Semantic Gaps with Context. (arXiv:2010.02172v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.02172</id>
        <link href="http://arxiv.org/abs/2010.02172"/>
        <updated>2021-06-03T02:10:34.659Z</updated>
        <summary type="html"><![CDATA[Lexical ambiguity is widespread in language, allowing for the reuse of
economical word forms and therefore making language more efficient. If
ambiguous words cannot be disambiguated from context, however, this gain in
efficiency might make language less clear -- resulting in frequent
miscommunication. For a language to be clear and efficiently encoded, we posit
that the lexical ambiguity of a word type should correlate with how much
information context provides about it, on average. To investigate whether this
is the case, we operationalise the lexical ambiguity of a word as the entropy
of meanings it can take, and provide two ways to estimate this -- one which
requires human annotation (using WordNet), and one which does not (using BERT),
making it readily applicable to a large number of languages. We validate these
measures by showing that, on six high-resource languages, there are significant
Pearson correlations between our BERT-based estimate of ambiguity and the
number of synonyms a word has in WordNet (e.g. $\rho = 0.40$ in English). We
then test our main hypothesis -- that a word's lexical ambiguity should
negatively correlate with its contextual uncertainty -- and find significant
correlations on all 18 typologically diverse languages we analyse. This
suggests that, in the presence of ambiguity, speakers compensate by making
contexts more informative.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pimentel_T/0/1/0/all/0/1"&gt;Tiago Pimentel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maudslay_R/0/1/0/all/0/1"&gt;Rowan Hall Maudslay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Blasi_D/0/1/0/all/0/1"&gt;Dami&amp;#xe1;n Blasi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1"&gt;Ryan Cotterell&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[More Embeddings, Better Sequence Labelers?. (arXiv:2009.08330v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.08330</id>
        <link href="http://arxiv.org/abs/2009.08330"/>
        <updated>2021-06-03T02:10:34.650Z</updated>
        <summary type="html"><![CDATA[Recent work proposes a family of contextual embeddings that significantly
improves the accuracy of sequence labelers over non-contextual embeddings.
However, there is no definite conclusion on whether we can build better
sequence labelers by combining different kinds of embeddings in various
settings. In this paper, we conduct extensive experiments on 3 tasks over 18
datasets and 8 languages to study the accuracy of sequence labeling with
various embedding concatenations and make three observations: (1) concatenating
more embedding variants leads to better accuracy in rich-resource and
cross-domain settings and some conditions of low-resource settings; (2)
concatenating additional contextual sub-word embeddings with contextual
character embeddings hurts the accuracy in extremely low-resource settings; (3)
based on the conclusion of (1), concatenating additional similar contextual
embeddings cannot lead to further improvements. We hope these conclusions can
help people build stronger sequence labelers in various settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xinyu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1"&gt;Yong Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bach_N/0/1/0/all/0/1"&gt;Nguyen Bach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1"&gt;Tao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zhongqiang Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1"&gt;Fei Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tu_K/0/1/0/all/0/1"&gt;Kewei Tu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Symmetry-via-Duality: Invariant Neural Network Densities from Parameter-Space Correlators. (arXiv:2106.00694v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.00694</id>
        <link href="http://arxiv.org/abs/2106.00694"/>
        <updated>2021-06-03T02:10:34.645Z</updated>
        <summary type="html"><![CDATA[Parameter-space and function-space provide two different duality frames in
which to study neural networks. We demonstrate that symmetries of network
densities may be determined via dual computations of network correlation
functions, even when the density is unknown and the network is not equivariant.
Symmetry-via-duality relies on invariance properties of the correlation
functions, which stem from the choice of network parameter distributions. Input
and output symmetries of neural network densities are determined, which recover
known Gaussian process results in the infinite width limit. The mechanism may
also be utilized to determine symmetries during training, when parameters are
correlated, as well as symmetries of the Neural Tangent Kernel. We demonstrate
that the amount of symmetry in the initialization density affects the accuracy
of networks trained on Fashion-MNIST, and that symmetry breaking helps only
when it is in the direction of ground truth.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Maiti_A/0/1/0/all/0/1"&gt;Anindita Maiti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stoner_K/0/1/0/all/0/1"&gt;Keegan Stoner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Halverson_J/0/1/0/all/0/1"&gt;James Halverson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data Augmentation with Unsupervised Machine Translation Improvesthe Structural Similarity of Cross-lingual Word Embeddings. (arXiv:2006.00262v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.00262</id>
        <link href="http://arxiv.org/abs/2006.00262"/>
        <updated>2021-06-03T02:10:34.629Z</updated>
        <summary type="html"><![CDATA[Unsupervised cross-lingual word embedding (CLWE) methods learn a linear
transformation matrix that maps two monolingual embedding spaces that are
separately trained with monolingual corpora. This method relies on the
assumption that the two embedding spaces are structurally similar, which does
not necessarily hold true in general. In this paper, we argue that using a
pseudo-parallel corpus generated by an unsupervised machine translation model
facilitates the structural similarity of the two embedding spaces and improves
the quality of CLWEs in the unsupervised mapping method. We show that our
approach outperforms other alternative approaches given the same amount of
data, and, through detailed analysis, we show that data augmentation with the
pseudo data from unsupervised machine translation is especially effective for
mapping-based CLWEs because (1) the pseudo data makes the source and target
corpora (partially) parallel; (2) the pseudo data contains information on the
original language that helps to learn similar embedding spaces between the
source and target languages.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nishikawa_S/0/1/0/all/0/1"&gt;Sosuke Nishikawa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ri_R/0/1/0/all/0/1"&gt;Ryokan Ri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsuruoka_Y/0/1/0/all/0/1"&gt;Yoshimasa Tsuruoka&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Few-Shot Text Ranking with Meta Adapted Synthetic Weak Supervision. (arXiv:2012.14862v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.14862</id>
        <link href="http://arxiv.org/abs/2012.14862"/>
        <updated>2021-06-03T02:10:34.623Z</updated>
        <summary type="html"><![CDATA[The effectiveness of Neural Information Retrieval (Neu-IR) often depends on a
large scale of in-domain relevance training signals, which are not always
available in real-world ranking scenarios. To democratize the benefits of
Neu-IR, this paper presents MetaAdaptRank, a domain adaptive learning method
that generalizes Neu-IR models from label-rich source domains to few-shot
target domains. Drawing on source-domain massive relevance supervision,
MetaAdaptRank contrastively synthesizes a large number of weak supervision
signals for target domains and meta-learns to reweight these synthetic "weak"
data based on their benefits to the target-domain ranking accuracy of Neu-IR
models. Experiments on three TREC benchmarks in the web, news, and biomedical
domains show that MetaAdaptRank significantly improves the few-shot ranking
accuracy of Neu-IR models. Further analyses indicate that MetaAdaptRank thrives
from both its contrastive weak data synthesis and meta-reweighted data
selection. The code and data of this paper can be obtained from
https://github.com/thunlp/MetaAdaptRank.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1"&gt;Si Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1"&gt;Yingzhuo Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhenghao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1"&gt;Chenyan Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1"&gt;Kaitao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1"&gt;Jie Bao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhiyuan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bennett_P/0/1/0/all/0/1"&gt;Paul Bennett&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[WARP: Word-level Adversarial ReProgramming. (arXiv:2101.00121v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.00121</id>
        <link href="http://arxiv.org/abs/2101.00121"/>
        <updated>2021-06-03T02:10:34.618Z</updated>
        <summary type="html"><![CDATA[Transfer learning from pretrained language models recently became the
dominant approach for solving many NLP tasks. A common approach to transfer
learning for multiple tasks that maximize parameter sharing trains one or more
task-specific layers on top of the language model. In this paper, we present an
alternative approach based on adversarial reprogramming, which extends earlier
work on automatic prompt generation. Adversarial reprogramming attempts to
learn task-specific word embeddings that, when concatenated to the input text,
instruct the language model to solve the specified task. Using up to 25K
trainable parameters per task, this approach outperforms all existing methods
with up to 25M trainable parameters on the public leaderboard of the GLUE
benchmark. Our method, initialized with task-specific human-readable prompts,
also works in a few-shot setting, outperforming GPT-3 on two SuperGLUE tasks
with just 32 training samples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hambardzumyan_K/0/1/0/all/0/1"&gt;Karen Hambardzumyan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khachatrian_H/0/1/0/all/0/1"&gt;Hrant Khachatrian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+May_J/0/1/0/all/0/1"&gt;Jonathan May&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning by Semantic Similarity Makes Abstractive Summarization Better. (arXiv:2002.07767v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.07767</id>
        <link href="http://arxiv.org/abs/2002.07767"/>
        <updated>2021-06-03T02:10:34.612Z</updated>
        <summary type="html"><![CDATA[By harnessing pre-trained language models, summarization models had rapid
progress recently. However, the models are mainly assessed by automatic
evaluation metrics such as ROUGE. Although ROUGE is known for having a positive
correlation with human evaluation scores, it has been criticized for its
vulnerability and the gap between actual qualities. In this paper, we compare
the generated summaries from recent LM, BART, and the reference summaries from
a benchmark dataset, CNN/DM, using a crowd-sourced human evaluation metric.
Interestingly, model-generated summaries receive higher scores relative to
reference summaries. Stemming from our experimental results, we first argue the
intrinsic characteristics of the CNN/DM dataset, the progress of pre-trained
language models, and their ability to generalize on the training data. Finally,
we share our insights into the model-generated summaries and presents our
thought on learning methods for abstractive summarization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yoon_W/0/1/0/all/0/1"&gt;Wonjin Yoon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yeo_Y/0/1/0/all/0/1"&gt;Yoon Sun Yeo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jeong_M/0/1/0/all/0/1"&gt;Minbyul Jeong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yi_B/0/1/0/all/0/1"&gt;Bong-Jun Yi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kang_J/0/1/0/all/0/1"&gt;Jaewoo Kang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Unified Generative Framework for Various NER Subtasks. (arXiv:2106.01223v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01223</id>
        <link href="http://arxiv.org/abs/2106.01223"/>
        <updated>2021-06-03T02:10:34.607Z</updated>
        <summary type="html"><![CDATA[Named Entity Recognition (NER) is the task of identifying spans that
represent entities in sentences. Whether the entity spans are nested or
discontinuous, the NER task can be categorized into the flat NER, nested NER,
and discontinuous NER subtasks. These subtasks have been mainly solved by the
token-level sequence labelling or span-level classification. However, these
solutions can hardly tackle the three kinds of NER subtasks concurrently. To
that end, we propose to formulate the NER subtasks as an entity span sequence
generation task, which can be solved by a unified sequence-to-sequence
(Seq2Seq) framework. Based on our unified framework, we can leverage the
pre-trained Seq2Seq model to solve all three kinds of NER subtasks without the
special design of the tagging schema or ways to enumerate spans. We exploit
three types of entity representations to linearize entities into a sequence.
Our proposed framework is easy-to-implement and achieves state-of-the-art
(SoTA) or near SoTA performance on eight English NER datasets, including two
flat NER datasets, three nested NER datasets, and three discontinuous NER
datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1"&gt;Hang Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gui_T/0/1/0/all/0/1"&gt;Tao Gui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1"&gt;Junqi Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1"&gt;Qipeng Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zheng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1"&gt;Xipeng Qiu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Closer Look at Few-Shot Crosslingual Transfer: The Choice of Shots Matters. (arXiv:2012.15682v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.15682</id>
        <link href="http://arxiv.org/abs/2012.15682"/>
        <updated>2021-06-03T02:10:34.591Z</updated>
        <summary type="html"><![CDATA[Few-shot crosslingual transfer has been shown to outperform its zero-shot
counterpart with pretrained encoders like multilingual BERT. Despite its
growing popularity, little to no attention has been paid to standardizing and
analyzing the design of few-shot experiments. In this work, we highlight a
fundamental risk posed by this shortcoming, illustrating that the model
exhibits a high degree of sensitivity to the selection of few shots. We conduct
a large-scale experimental study on 40 sets of sampled few shots for six
diverse NLP tasks across up to 40 languages. We provide an analysis of success
and failure cases of few-shot transfer, which highlights the role of lexical
features. Additionally, we show that a straightforward full model finetuning
approach is quite effective for few-shot transfer, outperforming several
state-of-the-art few-shot approaches. As a step towards standardizing few-shot
crosslingual experimental designs, we make our sampled few shots publicly
available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1"&gt;Mengjie Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yi Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shareghi_E/0/1/0/all/0/1"&gt;Ehsan Shareghi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vulic_I/0/1/0/all/0/1"&gt;Ivan Vuli&amp;#x107;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reichart_R/0/1/0/all/0/1"&gt;Roi Reichart&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Korhonen_A/0/1/0/all/0/1"&gt;Anna Korhonen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1"&gt;Hinrich Sch&amp;#xfc;tze&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Is Sparse Attention more Interpretable?. (arXiv:2106.01087v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01087</id>
        <link href="http://arxiv.org/abs/2106.01087"/>
        <updated>2021-06-03T02:10:34.550Z</updated>
        <summary type="html"><![CDATA[Sparse attention has been claimed to increase model interpretability under
the assumption that it highlights influential inputs. Yet the attention
distribution is typically over representations internal to the model rather
than the inputs themselves, suggesting this assumption may not have merit. We
build on the recent work exploring the interpretability of attention; we design
a set of experiments to help us understand how sparsity affects our ability to
use attention as an explainability tool. On three text classification tasks, we
verify that only a weak relationship between inputs and co-indexed intermediate
representations exists -- under sparse attention and otherwise. Further, we do
not find any plausible mappings from sparse attention distributions to a sparse
set of influential inputs through other avenues. Rather, we observe in this
setting that inducing sparsity may make it less plausible that attention can be
used as a tool for understanding model behavior.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Meister_C/0/1/0/all/0/1"&gt;Clara Meister&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lazov_S/0/1/0/all/0/1"&gt;Stefan Lazov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Augenstein_I/0/1/0/all/0/1"&gt;Isabelle Augenstein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1"&gt;Ryan Cotterell&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Superbizarre Is Not Superb: Derivational Morphology Improves BERT's Interpretation of Complex Words. (arXiv:2101.00403v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.00403</id>
        <link href="http://arxiv.org/abs/2101.00403"/>
        <updated>2021-06-03T02:10:34.544Z</updated>
        <summary type="html"><![CDATA[How does the input segmentation of pretrained language models (PLMs) affect
their interpretations of complex words? We present the first study
investigating this question, taking BERT as the example PLM and focusing on its
semantic representations of English derivatives. We show that PLMs can be
interpreted as serial dual-route models, i.e., the meanings of complex words
are either stored or else need to be computed from the subwords, which implies
that maximally meaningful input tokens should allow for the best generalization
on new words. This hypothesis is confirmed by a series of semantic probing
tasks on which DelBERT (Derivation leveraging BERT), a model with derivational
input segmentation, substantially outperforms BERT with WordPiece segmentation.
Our results suggest that the generalization capabilities of PLMs could be
further improved if a morphologically-informed vocabulary of input tokens were
used.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hofmann_V/0/1/0/all/0/1"&gt;Valentin Hofmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pierrehumbert_J/0/1/0/all/0/1"&gt;Janet B. Pierrehumbert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1"&gt;Hinrich Sch&amp;#xfc;tze&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Error-driven Fixed-Budget ASR Personalization for Accented Speakers. (arXiv:2103.03142v2 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.03142</id>
        <link href="http://arxiv.org/abs/2103.03142"/>
        <updated>2021-06-03T02:10:34.538Z</updated>
        <summary type="html"><![CDATA[We consider the task of personalizing ASR models while being constrained by a
fixed budget on recording speaker-specific utterances. Given a speaker and an
ASR model, we propose a method of identifying sentences for which the speaker's
utterances are likely to be harder for the given ASR model to recognize. We
assume a tiny amount of speaker-specific data to learn phoneme-level error
models which help us select such sentences. We show that speaker's utterances
on the sentences selected using our error model indeed have larger error rates
when compared to speaker's utterances on randomly selected sentences. We find
that fine-tuning the ASR model on the sentence utterances selected with the
help of error models yield higher WER improvements in comparison to fine-tuning
on an equal number of randomly selected sentence utterances. Thus, our method
provides an efficient way of collecting speaker utterances under budget
constraints for personalizing ASR models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Awasthi_A/0/1/0/all/0/1"&gt;Abhijeet Awasthi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kansal_A/0/1/0/all/0/1"&gt;Aman Kansal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sarawagi_S/0/1/0/all/0/1"&gt;Sunita Sarawagi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jyothi_P/0/1/0/all/0/1"&gt;Preethi Jyothi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Global-Selector: A New Benchmark Dataset and Model Architecture for Multi-turn Response Selection. (arXiv:2106.01263v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01263</id>
        <link href="http://arxiv.org/abs/2106.01263"/>
        <updated>2021-06-03T02:10:34.517Z</updated>
        <summary type="html"><![CDATA[As an essential component of dialogue systems, multi-turn response selection
aims to pick out the optimal response among a set of candidates to improve the
dialogue fluency. In this paper, we investigate three problems of current
response selection approaches, especially for generation-based conversational
agents: (i) Existing approaches are often formulated as a sentence scoring
problem, which does not consider relationships between responses. (ii) Existing
models tend to select undesirable candidates that have large overlaps with the
dialogue history. (iii) Negative instances in training are mainly constructed
by random sampling from the corpus, whereas generated candidates in practice
typically have a closer distribution. To address the above problems, we create
a new dataset called ConvAI2+ and propose a new response selector called
Global-Selector. Experimental results show that Global-Selector trained on
ConvAI2+ have noticeable improvements in both accuracy and inference speed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1"&gt;Chiyu Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1"&gt;Hongliang He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_H/0/1/0/all/0/1"&gt;Huachuan Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1"&gt;Haofei Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lan_Z/0/1/0/all/0/1"&gt;Zhenzhong Lan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Style is NOT a single variable: Case Studies for Cross-Style Language Understanding. (arXiv:1911.03663v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1911.03663</id>
        <link href="http://arxiv.org/abs/1911.03663"/>
        <updated>2021-06-03T02:10:34.497Z</updated>
        <summary type="html"><![CDATA[Every natural text is written in some style. Style is formed by a complex
combination of different stylistic factors, including formality markers,
emotions, metaphors, etc. One cannot form a complete understanding of a text
without considering these factors. The factors combine and co-vary in complex
ways to form styles. Studying the nature of the co-varying combinations sheds
light on stylistic language in general, sometimes called cross-style language
understanding. This paper provides the benchmark corpus (xSLUE) that combines
existing datasets and collects a new one for sentence-level cross-style
language understanding and evaluation. The benchmark contains text in 15
different styles under the proposed four theoretical groupings: figurative,
personal, affective, and interpersonal groups. For valid evaluation, we collect
an additional diagnostic set by annotating all 15 styles on the same text.
Using xSLUE, we propose three interesting cross-style applications in
classification, correlation, and generation. First, our proposed cross-style
classifier trained with multiple styles together helps improve overall
classification performance against individually-trained style classifiers.
Second, our study shows that some styles are highly dependent on each other in
human-written text. Finally, we find that combinations of some contradictive
styles likely generate stylistically less appropriate text. We believe our
benchmark and case studies help explore interesting future directions for
cross-style research. The preprocessed datasets and code are publicly
available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kang_D/0/1/0/all/0/1"&gt;Dongyeop Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hovy_E/0/1/0/all/0/1"&gt;Eduard Hovy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Differential Privacy for Text Analytics via Natural Text Sanitization. (arXiv:2106.01221v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01221</id>
        <link href="http://arxiv.org/abs/2106.01221"/>
        <updated>2021-06-03T02:10:34.478Z</updated>
        <summary type="html"><![CDATA[Texts convey sophisticated knowledge. However, texts also convey sensitive
information. Despite the success of general-purpose language models and
domain-specific mechanisms with differential privacy (DP), existing text
sanitization mechanisms still provide low utility, as cursed by the
high-dimensional text representation. The companion issue of utilizing
sanitized texts for downstream analytics is also under-explored. This paper
takes a direct approach to text sanitization. Our insight is to consider both
sensitivity and similarity via our new local DP notion. The sanitized texts
also contribute to our sanitization-aware pretraining and fine-tuning, enabling
privacy-preserving natural language processing over the BERT language model
with promising utility. Surprisingly, the high utility does not boost up the
success rate of inference attacks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yue_X/0/1/0/all/0/1"&gt;Xiang Yue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_M/0/1/0/all/0/1"&gt;Minxin Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1"&gt;Tianhao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yaliang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1"&gt;Huan Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chow_S/0/1/0/all/0/1"&gt;Sherman S. M. Chow&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Compositional Generalization and Natural Language Variation: Can a Semantic Parsing Approach Handle Both?. (arXiv:2010.12725v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.12725</id>
        <link href="http://arxiv.org/abs/2010.12725"/>
        <updated>2021-06-03T02:10:34.421Z</updated>
        <summary type="html"><![CDATA[Sequence-to-sequence models excel at handling natural language variation, but
have been shown to struggle with out-of-distribution compositional
generalization. This has motivated new specialized architectures with stronger
compositional biases, but most of these approaches have only been evaluated on
synthetically-generated datasets, which are not representative of natural
language variation. In this work we ask: can we develop a semantic parsing
approach that handles both natural language variation and compositional
generalization? To better assess this capability, we propose new train and test
splits of non-synthetic datasets. We demonstrate that strong existing
approaches do not perform well across a broad set of evaluations. We also
propose NQG-T5, a hybrid model that combines a high-precision grammar-based
approach with a pre-trained sequence-to-sequence model. It outperforms existing
approaches across several compositional generalization challenges on
non-synthetic data, while also being competitive with the state-of-the-art on
standard evaluations. While still far from solving this problem, our study
highlights the importance of diverse evaluations and the open challenge of
handling both compositional generalization and natural language variation in
semantic parsing.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shaw_P/0/1/0/all/0/1"&gt;Peter Shaw&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_M/0/1/0/all/0/1"&gt;Ming-Wei Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pasupat_P/0/1/0/all/0/1"&gt;Panupong Pasupat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Toutanova_K/0/1/0/all/0/1"&gt;Kristina Toutanova&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quality Estimation for Image Captions Based on Large-scale Human Evaluations. (arXiv:1909.03396v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1909.03396</id>
        <link href="http://arxiv.org/abs/1909.03396"/>
        <updated>2021-06-03T02:10:34.414Z</updated>
        <summary type="html"><![CDATA[Automatic image captioning has improved significantly over the last few
years, but the problem is far from being solved, with state of the art models
still often producing low quality captions when used in the wild. In this
paper, we focus on the task of Quality Estimation (QE) for image captions,
which attempts to model the caption quality from a human perspective and
without access to ground-truth references, so that it can be applied at
prediction time to detect low-quality captions produced on previously unseen
images. For this task, we develop a human evaluation process that collects
coarse-grained caption annotations from crowdsourced users, which is then used
to collect a large scale dataset spanning more than 600k caption quality
ratings. We then carefully validate the quality of the collected ratings and
establish baseline models for this new QE task. Finally, we further collect
fine-grained caption quality annotations from trained raters, and use them to
demonstrate that QE models trained over the coarse ratings can effectively
detect and filter out low-quality image captions, thereby improving the user
experience from captioning systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Levinboim_T/0/1/0/all/0/1"&gt;Tomer Levinboim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thapliyal_A/0/1/0/all/0/1"&gt;Ashish V. Thapliyal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_P/0/1/0/all/0/1"&gt;Piyush Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soricut_R/0/1/0/all/0/1"&gt;Radu Soricut&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lower Perplexity is Not Always Human-Like. (arXiv:2106.01229v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01229</id>
        <link href="http://arxiv.org/abs/2106.01229"/>
        <updated>2021-06-03T02:10:34.405Z</updated>
        <summary type="html"><![CDATA[In computational psycholinguistics, various language models have been
evaluated against human reading behavior (e.g., eye movement) to build
human-like computational models. However, most previous efforts have focused
almost exclusively on English, despite the recent trend towards linguistic
universal within the general community. In order to fill the gap, this paper
investigates whether the established results in computational psycholinguistics
can be generalized across languages. Specifically, we re-examine an established
generalization -- the lower perplexity a language model has, the more
human-like the language model is -- in Japanese with typologically different
structures from English. Our experiments demonstrate that this established
generalization exhibits a surprising lack of universality; namely, lower
perplexity is not always human-like. Moreover, this discrepancy between English
and Japanese is further explored from the perspective of (non-)uniform
information density. Overall, our results suggest that a cross-lingual
evaluation will be necessary to construct human-like computational models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kuribayashi_T/0/1/0/all/0/1"&gt;Tatsuki Kuribayashi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oseki_Y/0/1/0/all/0/1"&gt;Yohei Oseki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ito_T/0/1/0/all/0/1"&gt;Takumi Ito&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yoshida_R/0/1/0/all/0/1"&gt;Ryo Yoshida&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Asahara_M/0/1/0/all/0/1"&gt;Masayuki Asahara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Inui_K/0/1/0/all/0/1"&gt;Kentaro Inui&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Label-aware Event Trigger and Argument Classification. (arXiv:2012.15243v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.15243</id>
        <link href="http://arxiv.org/abs/2012.15243"/>
        <updated>2021-06-03T02:10:34.391Z</updated>
        <summary type="html"><![CDATA[Identifying events and mapping them to pre-defined event types has long been
an important natural language processing problem. Most previous work has been
heavily relying on labor-intensive and domain-specific annotations while
ignoring the semantic meaning contained in the labels of the event types. As a
result, the learned models cannot effectively generalize to new domains, where
new event types could be introduced. In this paper, we propose an unsupervised
event extraction pipeline, which first identifies events with available tools
(e.g., SRL) and then automatically maps them to pre-defined event types with
our proposed unsupervised classification model. Rather than relying on
annotated data, our model matches the semantics of identified events with those
of event type labels. Specifically, we leverage pre-trained language models to
contextually represent pre-defined types for both event triggers and arguments.
After we map identified events to the target types via representation
similarity, we use the event ontology (e.g., argument type "Victim" can only
appear as the argument of event type "Attack") as global constraints to
regularize the prediction. The proposed approach is shown to be very effective
when tested on the ACE-2005 dataset, which has 33 trigger and 22 argument
types. Without using any annotation, we successfully map 83% of the triggers
and 54% of the arguments to the correct types, almost doubling the performance
of previous zero-shot approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Hongming Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Haoyu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roth_D/0/1/0/all/0/1"&gt;Dan Roth&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Metaphor Generation with Conceptual Mappings. (arXiv:2106.01228v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01228</id>
        <link href="http://arxiv.org/abs/2106.01228"/>
        <updated>2021-06-03T02:10:34.385Z</updated>
        <summary type="html"><![CDATA[Generating metaphors is a difficult task as it requires understanding nuanced
relationships between abstract concepts. In this paper, we aim to generate a
metaphoric sentence given a literal expression by replacing relevant verbs.
Guided by conceptual metaphor theory, we propose to control the generation
process by encoding conceptual mappings between cognitive domains to generate
meaningful metaphoric expressions. To achieve this, we develop two methods: 1)
using FrameNet-based embeddings to learn mappings between domains and applying
them at the lexical level (CM-Lex), and 2) deriving source/target pairs to
train a controlled seq-to-seq generation model (CM-BART). We assess our methods
through automatic and human evaluation for basic metaphoricity and conceptual
metaphor presence. We show that the unsupervised CM-Lex model is competitive
with recent deep learning metaphor generation systems, and CM-BART outperforms
all other models both in automatic and human evaluations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Stowe_K/0/1/0/all/0/1"&gt;Kevin Stowe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chakrabarty_T/0/1/0/all/0/1"&gt;Tuhin Chakrabarty&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1"&gt;Nanyun Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muresan_S/0/1/0/all/0/1"&gt;Smaranda Muresan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1"&gt;Iryna Gurevych&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evidence-based Factual Error Correction. (arXiv:2106.01072v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01072</id>
        <link href="http://arxiv.org/abs/2106.01072"/>
        <updated>2021-06-03T02:10:34.380Z</updated>
        <summary type="html"><![CDATA[This paper introduces the task of factual error correction: performing edits
to a claim so that the generated rewrite is better supported by evidence. This
extends the well-studied task of fact verification by providing a mechanism to
correct written texts that are refuted or only partially supported by evidence.
We demonstrate that it is feasible to train factual error correction systems
from existing fact checking datasets which only contain labeled claims
accompanied by evidence, but not the correction. We achieve this by employing a
two-stage distant supervision approach that incorporates evidence into masked
claims when generating corrections. Our approach, based on the T5 transformer
and using retrieved evidence, achieved better results than existing work which
used a pointer copy network and gold evidence, producing accurate factual error
corrections for 5x more instances in human evaluation and a .125 increase in
SARI score. The evaluation is conducted on a dataset of 65,000 instances based
on a recent fact verification shared task and we release it to enable further
work on the task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Thorne_J/0/1/0/all/0/1"&gt;James Thorne&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vlachos_A/0/1/0/all/0/1"&gt;Andreas Vlachos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LGESQL: Line Graph Enhanced Text-to-SQL Model with Mixed Local and Non-Local Relations. (arXiv:2106.01093v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01093</id>
        <link href="http://arxiv.org/abs/2106.01093"/>
        <updated>2021-06-03T02:10:34.369Z</updated>
        <summary type="html"><![CDATA[This work aims to tackle the challenging heterogeneous graph encoding problem
in the text-to-SQL task. Previous methods are typically node-centric and merely
utilize different weight matrices to parameterize edge types, which 1) ignore
the rich semantics embedded in the topological structure of edges, and 2) fail
to distinguish local and non-local relations for each node. To this end, we
propose a Line Graph Enhanced Text-to-SQL (LGESQL) model to mine the underlying
relational features without constructing meta-paths. By virtue of the line
graph, messages propagate more efficiently through not only connections between
nodes, but also the topology of directed edges. Furthermore, both local and
non-local relations are integrated distinctively during the graph iteration. We
also design an auxiliary task called graph pruning to improve the
discriminative capability of the encoder. Our framework achieves
state-of-the-art results (62.8% with Glove, 72.0% with Electra) on the
cross-domain text-to-SQL benchmark Spider at the time of writing.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cao_R/0/1/0/all/0/1"&gt;Ruisheng Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Lu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1"&gt;Su Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_K/0/1/0/all/0/1"&gt;Kai Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multilingual Medical Question Answering and Information Retrieval for Rural Health Intelligence Access. (arXiv:2106.01251v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01251</id>
        <link href="http://arxiv.org/abs/2106.01251"/>
        <updated>2021-06-03T02:10:34.363Z</updated>
        <summary type="html"><![CDATA[In rural regions of several developing countries, access to quality
healthcare, medical infrastructure, and professional diagnosis is largely
unavailable. Many of these regions are gradually gaining access to internet
infrastructure, although not with a strong enough connection to allow for
sustained communication with a medical practitioner. Several deaths resulting
from this lack of medical access, absence of patient's previous health records,
and the unavailability of information in indigenous languages can be easily
prevented. In this paper, we describe an approach leveraging the phenomenal
progress in Machine Learning and NLP (Natural Language Processing) techniques
to design a model that is low-resource, multilingual, and a preliminary
first-point-of-contact medical assistant. Our contribution includes defining
the NLP pipeline required for named-entity-recognition, language-agnostic
sentence embedding, natural language translation, information retrieval,
question answering, and generative pre-training for final query processing. We
obtain promising results for this pipeline and preliminary results for EHR
(Electronic Health Record) analysis with text summarization for medical
practitioners to peruse for their diagnosis. Through this NLP pipeline, we aim
to provide preliminary medical information to the user and do not claim to
supplant diagnosis from qualified medical practitioners. Using the input from
subject matter experts, we have compiled a large corpus to pre-train and
fine-tune our BioBERT based NLP model for the specific tasks. We expect recent
advances in NLP architectures, several of which are efficient and
privacy-preserving models, to further the impact of our solution and improve on
individual task performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vinod_V/0/1/0/all/0/1"&gt;Vishal Vinod&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agrawal_S/0/1/0/all/0/1"&gt;Susmit Agrawal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gaurav_V/0/1/0/all/0/1"&gt;Vipul Gaurav&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+R_P/0/1/0/all/0/1"&gt;Pallavi R&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choudhary_S/0/1/0/all/0/1"&gt;Savita Choudhary&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Few-Shot Partial-Label Learning. (arXiv:2106.00984v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.00984</id>
        <link href="http://arxiv.org/abs/2106.00984"/>
        <updated>2021-06-03T02:10:34.349Z</updated>
        <summary type="html"><![CDATA[Partial-label learning (PLL) generally focuses on inducing a noise-tolerant
multi-class classifier by training on overly-annotated samples, each of which
is annotated with a set of labels, but only one is the valid label. A basic
promise of existing PLL solutions is that there are sufficient partial-label
(PL) samples for training. However, it is more common than not to have just few
PL samples at hand when dealing with new tasks. Furthermore, existing few-shot
learning algorithms assume precise labels of the support set; as such,
irrelevant labels may seriously mislead the meta-learner and thus lead to a
compromised performance. How to enable PLL under a few-shot learning setting is
an important problem, but not yet well studied. In this paper, we introduce an
approach called FsPLL (Few-shot PLL). FsPLL first performs adaptive distance
metric learning by an embedding network and rectifying prototypes on the tasks
previously encountered. Next, it calculates the prototype of each class of a
new task in the embedding network. An unseen example can then be classified via
its distance to each prototype. Experimental results on widely-used few-shot
datasets (Omniglot and miniImageNet) demonstrate that our FsPLL can achieve a
superior performance than the state-of-the-art methods across different
settings, and it needs fewer samples for quickly adapting to new tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yunfeng Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_G/0/1/0/all/0/1"&gt;Guoxian Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Lei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1"&gt;Zhongmin Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1"&gt;Lizhen Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Domeniconi_C/0/1/0/all/0/1"&gt;Carlotta Domeniconi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Online and Real-Time Tracking in a Surveillance Scenario. (arXiv:2106.01153v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.01153</id>
        <link href="http://arxiv.org/abs/2106.01153"/>
        <updated>2021-06-03T02:10:34.341Z</updated>
        <summary type="html"><![CDATA[This paper presents an approach for tracking in a surveillance scenario.
Typical aspects for this scenario are a 24/7 operation with a static camera
mounted above the height of a human with many objects or people. The Multiple
Object Tracking Benchmark 20 (MOT20) reflects this scenario best. We can show
that our approach is real-time capable on this benchmark and outperforms all
other real-time capable approaches in HOTA, MOTA, and IDF1. We achieve this by
contributing a fast Siamese network reformulated for linear runtime (instead of
quadratic) to generate fingerprints from detections. Thus, it is possible to
associate the detections to Kalman filters based on multiple tracking specific
ratings: Cosine similarity of fingerprints, Intersection over Union, and pixel
distance ratio in the image.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Urbann_O/0/1/0/all/0/1"&gt;Oliver Urbann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bredtmann_O/0/1/0/all/0/1"&gt;Oliver Bredtmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Otten_M/0/1/0/all/0/1"&gt;Maximilian Otten&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Richter_J/0/1/0/all/0/1"&gt;Jan-Philip Richter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bauer_T/0/1/0/all/0/1"&gt;Thilo Bauer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zibriczky_D/0/1/0/all/0/1"&gt;David Zibriczky&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Who Blames or Endorses Whom? Entity-to-Entity Directed Sentiment Extraction in News Text. (arXiv:2106.01033v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01033</id>
        <link href="http://arxiv.org/abs/2106.01033"/>
        <updated>2021-06-03T02:10:34.336Z</updated>
        <summary type="html"><![CDATA[Understanding who blames or supports whom in news text is a critical research
question in computational social science. Traditional methods and datasets for
sentiment analysis are, however, not suitable for the domain of political text
as they do not consider the direction of sentiments expressed between entities.
In this paper, we propose a novel NLP task of identifying directed sentiment
relationship between political entities from a given news document, which we
call directed sentiment extraction. From a million-scale news corpus, we
construct a dataset of news sentences where sentiment relations of political
entities are manually annotated. We present a simple but effective approach for
utilizing a pretrained transformer, which infers the target class by predicting
multiple question-answering tasks and combining the outcomes. We demonstrate
the utility of our proposed method for social science research questions by
analyzing positive and negative opinions between political entities in two
major events: 2016 U.S. presidential election and COVID-19. The newly proposed
problem, data, and method will facilitate future studies on interdisciplinary
NLP methods and applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Park_K/0/1/0/all/0/1"&gt;Kunwoo Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_Z/0/1/0/all/0/1"&gt;Zhufeng Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joo_J/0/1/0/all/0/1"&gt;Jungseock Joo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[belabBERT: a Dutch RoBERTa-based language model applied to psychiatric classification. (arXiv:2106.01091v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01091</id>
        <link href="http://arxiv.org/abs/2106.01091"/>
        <updated>2021-06-03T02:10:34.331Z</updated>
        <summary type="html"><![CDATA[Natural language processing (NLP) is becoming an important means for
automatic recognition of human traits and states, such as intoxication,
presence of psychiatric disorders, presence of airway disorders and states of
stress. Such applications have the potential to be an important pillar for
online help lines, and may gradually be introduced into eHealth modules.
However, NLP is language specific and for languages such as Dutch, NLP models
are scarce. As a result, recent Dutch NLP models have a low capture of long
range semantic dependencies over sentences. To overcome this, here we present
belabBERT, a new Dutch language model extending the RoBERTa architecture.
belabBERT is trained on a large Dutch corpus (+32 GB) of web crawled texts. We
applied belabBERT to the classification of psychiatric illnesses. First, we
evaluated the strength of text-based classification using belabBERT, and
compared the results to the existing RobBERT model. Then, we compared the
performance of belabBERT to audio classification for psychiatric disorders.
Finally, a brief exploration was performed, extending the framework to a hybrid
text- and audio-based classification. Our results show that belabBERT
outperformed the current best text classification network for Dutch, RobBERT.
belabBERT also outperformed classification based on audio alone.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wouts_J/0/1/0/all/0/1"&gt;Joppe Wouts&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boer_J/0/1/0/all/0/1"&gt;Janna de Boer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Voppel_A/0/1/0/all/0/1"&gt;Alban Voppel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brederoo_S/0/1/0/all/0/1"&gt;Sanne Brederoo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Splunter_S/0/1/0/all/0/1"&gt;Sander van Splunter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sommer_I/0/1/0/all/0/1"&gt;Iris Sommer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving low-resource ASR performance with untranscribed out-of-domain data. (arXiv:2106.01227v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01227</id>
        <link href="http://arxiv.org/abs/2106.01227"/>
        <updated>2021-06-03T02:10:34.316Z</updated>
        <summary type="html"><![CDATA[Semi-supervised training (SST) is a common approach to leverage
untranscribed/unlabeled speech data to improve automatic speech recognition
performance in low-resource languages. However, if the available unlabeled
speech is mismatched to the target domain, SST is not as effective, and in many
cases performs worse than the original system. In this paper, we address the
issue of low-resource ASR when only untranscribed out-of-domain speech data is
readily available in the target language. Specifically, we look to improve
performance on conversational/telephony speech (target domain) using web
resources, in particular YouTube data, which more closely resembles
news/topical broadcast data. Leveraging SST, we show that while in some cases
simply pooling the out-of-domain data with the training data lowers word error
rate (WER), in all cases, we see improvements if we train first with the
out-of-domain data and then fine-tune the resulting model with the original
training data. Using 2000 hours of speed perturbed YouTube audio in each target
language, with semi-supervised transcripts, we show improvements on multiple
languages/data sets, of up to 16.3% relative improvement in WER over the
baseline systems and up to 7.4% relative improvement in WER over a system that
simply pools the out-of-domain data with the training data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Billa_J/0/1/0/all/0/1"&gt;Jayadev Billa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[IrEne: Interpretable Energy Prediction for Transformers. (arXiv:2106.01199v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01199</id>
        <link href="http://arxiv.org/abs/2106.01199"/>
        <updated>2021-06-03T02:10:34.308Z</updated>
        <summary type="html"><![CDATA[Existing software-based energy measurements of NLP models are not accurate
because they do not consider the complex interactions between energy
consumption and model execution. We present IrEne, an interpretable and
extensible energy prediction system that accurately predicts the inference
energy consumption of a wide range of Transformer-based NLP models. IrEne
constructs a model tree graph that breaks down the NLP model into modules that
are further broken down into low-level machine learning (ML) primitives. IrEne
predicts the inference energy consumption of the ML primitives as a function of
generalizable features and fine-grained runtime resource usage. IrEne then
aggregates these low-level predictions recursively to predict the energy of
each module and finally of the entire model. Experiments across multiple
Transformer models show IrEne predicts inference energy consumption of
transformer models with an error of under 7% compared to the ground truth. In
contrast, existing energy models see an error of over 50%. We also show how
IrEne can be used to conduct energy bottleneck analysis and to easily evaluate
the energy impact of different architectural choices. We release the code and
data at https://github.com/StonyBrookNLP/irene.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cao_Q/0/1/0/all/0/1"&gt;Qingqing Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lal_Y/0/1/0/all/0/1"&gt;Yash Kumar Lal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Trivedi_H/0/1/0/all/0/1"&gt;Harsh Trivedi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balasubramanian_A/0/1/0/all/0/1"&gt;Aruna Balasubramanian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balasubramanian_N/0/1/0/all/0/1"&gt;Niranjan Balasubramanian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hi-Transformer: Hierarchical Interactive Transformer for Efficient and Effective Long Document Modeling. (arXiv:2106.01040v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01040</id>
        <link href="http://arxiv.org/abs/2106.01040"/>
        <updated>2021-06-03T02:10:34.302Z</updated>
        <summary type="html"><![CDATA[Transformer is important for text modeling. However, it has difficulty in
handling long documents due to the quadratic complexity with input text length.
In order to handle this problem, we propose a hierarchical interactive
Transformer (Hi-Transformer) for efficient and effective long document
modeling. Hi-Transformer models documents in a hierarchical way, i.e., first
learns sentence representations and then learns document representations. It
can effectively reduce the complexity and meanwhile capture global document
context in the modeling of each sentence. More specifically, we first use a
sentence Transformer to learn the representations of each sentence. Then we use
a document Transformer to model the global document context from these sentence
representations. Next, we use another sentence Transformer to enhance sentence
modeling using the global document context. Finally, we use hierarchical
pooling method to obtain document embedding. Extensive experiments on three
benchmark datasets validate the efficiency and effectiveness of Hi-Transformer
in long document modeling.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1"&gt;Chuhan Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1"&gt;Fangzhao Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_T/0/1/0/all/0/1"&gt;Tao Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yongfeng Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DynaEval: Unifying Turn and Dialogue Level Evaluation. (arXiv:2106.01112v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01112</id>
        <link href="http://arxiv.org/abs/2106.01112"/>
        <updated>2021-06-03T02:10:34.276Z</updated>
        <summary type="html"><![CDATA[A dialogue is essentially a multi-turn interaction among interlocutors.
Effective evaluation metrics should reflect the dynamics of such interaction.
Existing automatic metrics are focused very much on the turn-level quality,
while ignoring such dynamics. To this end, we propose DynaEval, a unified
automatic evaluation framework which is not only capable of performing
turn-level evaluation, but also holistically considers the quality of the
entire dialogue. In DynaEval, the graph convolutional network (GCN) is adopted
to model a dialogue in totality, where the graph nodes denote each individual
utterance and the edges represent the dependency between pairs of utterances. A
contrastive loss is then applied to distinguish well-formed dialogues from
carefully constructed negative samples. Experiments show that DynaEval
significantly outperforms the state-of-the-art dialogue coherence model, and
correlates strongly with human judgements across multiple dialogue evaluation
aspects at both turn and dialogue level.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chen Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yiming Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+DHaro_L/0/1/0/all/0/1"&gt;Luis Fernando D&amp;#x27;Haro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Friedrichs_T/0/1/0/all/0/1"&gt;Thomas Friedrichs&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1"&gt;Grandee Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Haizhou Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Use of Formal Ethical Reviews in NLP Literature: Historical Trends and Current Practices. (arXiv:2106.01105v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01105</id>
        <link href="http://arxiv.org/abs/2106.01105"/>
        <updated>2021-06-03T02:10:34.259Z</updated>
        <summary type="html"><![CDATA[Ethical aspects of research in language technologies have received much
attention recently. It is a standard practice to get a study involving human
subjects reviewed and approved by a professional ethics committee/board of the
institution. How commonly do we see mention of ethical approvals in NLP
research? What types of research or aspects of studies are usually subject to
such reviews? With the rising concerns and discourse around the ethics of NLP,
do we also observe a rise in formal ethical reviews of NLP studies? And, if so,
would this imply that there is a heightened awareness of ethical issues that
was previously lacking? We aim to address these questions by conducting a
detailed quantitative and qualitative analysis of the ACL Anthology, as well as
comparing the trends in our field to those of other related disciplines, such
as cognitive science, machine learning, data mining, and systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Santy_S/0/1/0/all/0/1"&gt;Sebastin Santy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rani_A/0/1/0/all/0/1"&gt;Anku Rani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choudhury_M/0/1/0/all/0/1"&gt;Monojit Choudhury&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Detecting Bot-Generated Text by Characterizing Linguistic Accommodation in Human-Bot Interactions. (arXiv:2106.01170v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01170</id>
        <link href="http://arxiv.org/abs/2106.01170"/>
        <updated>2021-06-03T02:10:34.231Z</updated>
        <summary type="html"><![CDATA[Language generation models' democratization benefits many domains, from
answering health-related questions to enhancing education by providing
AI-driven tutoring services. However, language generation models'
democratization also makes it easier to generate human-like text at-scale for
nefarious activities, from spreading misinformation to targeting specific
groups with hate speech. Thus, it is essential to understand how people
interact with bots and develop methods to detect bot-generated text. This paper
shows that bot-generated text detection methods are more robust across datasets
and models if we use information about how people respond to it rather than
using the bot's text directly. We also analyze linguistic alignment, providing
insight into differences between human-human and human-bot conversations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bhatt_P/0/1/0/all/0/1"&gt;Paras Bhatt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rios_A/0/1/0/all/0/1"&gt;Anthony Rios&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DialoGraph: Incorporating Interpretable Strategy-Graph Networks into Negotiation Dialogues. (arXiv:2106.00920v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.00920</id>
        <link href="http://arxiv.org/abs/2106.00920"/>
        <updated>2021-06-03T02:10:34.212Z</updated>
        <summary type="html"><![CDATA[To successfully negotiate a deal, it is not enough to communicate fluently:
pragmatic planning of persuasive negotiation strategies is essential. While
modern dialogue agents excel at generating fluent sentences, they still lack
pragmatic grounding and cannot reason strategically. We present DialoGraph, a
negotiation system that incorporates pragmatic strategies in a negotiation
dialogue using graph neural networks. DialoGraph explicitly incorporates
dependencies between sequences of strategies to enable improved and
interpretable prediction of next optimal strategies, given the dialogue
context. Our graph-based method outperforms prior state-of-the-art negotiation
models both in the accuracy of strategy/dialogue act prediction and in the
quality of downstream dialogue response generation. We qualitatively show
further benefits of learned strategy-graphs in providing explicit associations
between effective negotiation strategies over the course of the dialogue,
leading to interpretable and strategic dialogues.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Joshi_R/0/1/0/all/0/1"&gt;Rishabh Joshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balachandran_V/0/1/0/all/0/1"&gt;Vidhisha Balachandran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vashishth_S/0/1/0/all/0/1"&gt;Shikhar Vashishth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Black_A/0/1/0/all/0/1"&gt;Alan Black&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsvetkov_Y/0/1/0/all/0/1"&gt;Yulia Tsvetkov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[One Teacher is Enough? Pre-trained Language Model Distillation from Multiple Teachers. (arXiv:2106.01023v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01023</id>
        <link href="http://arxiv.org/abs/2106.01023"/>
        <updated>2021-06-03T02:10:34.199Z</updated>
        <summary type="html"><![CDATA[Pre-trained language models (PLMs) achieve great success in NLP. However,
their huge model sizes hinder their applications in many practical systems.
Knowledge distillation is a popular technique to compress PLMs, which learns a
small student model from a large teacher PLM. However, the knowledge learned
from a single teacher may be limited and even biased, resulting in low-quality
student model. In this paper, we propose a multi-teacher knowledge distillation
framework named MT-BERT for pre-trained language model compression, which can
train high-quality student model from multiple teacher PLMs. In MT-BERT we
design a multi-teacher co-finetuning method to jointly finetune multiple
teacher PLMs in downstream tasks with shared pooling and prediction layers to
align their output space for better collaborative teaching. In addition, we
propose a multi-teacher hidden loss and a multi-teacher distillation loss to
transfer the useful knowledge in both hidden states and soft labels from
multiple teacher PLMs to the student model. Experiments on three benchmark
datasets validate the effectiveness of MT-BERT in compressing PLMs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1"&gt;Chuhan Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1"&gt;Fangzhao Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yongfeng Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Why Machine Reading Comprehension Models Learn Shortcuts?. (arXiv:2106.01024v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01024</id>
        <link href="http://arxiv.org/abs/2106.01024"/>
        <updated>2021-06-03T02:10:34.194Z</updated>
        <summary type="html"><![CDATA[Recent studies report that many machine reading comprehension (MRC) models
can perform closely to or even better than humans on benchmark datasets.
However, existing works indicate that many MRC models may learn shortcuts to
outwit these benchmarks, but the performance is unsatisfactory in real-world
applications. In this work, we attempt to explore, instead of the expected
comprehension skills, why these models learn the shortcuts. Based on the
observation that a large portion of questions in current datasets have shortcut
solutions, we argue that larger proportion of shortcut questions in training
data make models rely on shortcut tricks excessively. To investigate this
hypothesis, we carefully design two synthetic datasets with annotations that
indicate whether a question can be answered using shortcut solutions. We
further propose two new methods to quantitatively analyze the learning
difficulty regarding shortcut and challenging questions, and revealing the
inherent learning mechanism behind the different performance between the two
kinds of questions. A thorough empirical analysis shows that MRC models tend to
learn shortcut questions earlier than challenging questions, and the high
proportions of shortcut questions in training sets hinder models from exploring
the sophisticated reasoning skills in the later stage of training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lai_Y/0/1/0/all/0/1"&gt;Yuxuan Lai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chen Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1"&gt;Yansong Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1"&gt;Quzhe Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1"&gt;Dongyan Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Out-of-Domain Detection via Pre-trained Transformers. (arXiv:2106.00948v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.00948</id>
        <link href="http://arxiv.org/abs/2106.00948"/>
        <updated>2021-06-03T02:10:34.189Z</updated>
        <summary type="html"><![CDATA[Deployed real-world machine learning applications are often subject to
uncontrolled and even potentially malicious inputs. Those out-of-domain inputs
can lead to unpredictable outputs and sometimes catastrophic safety issues.
Prior studies on out-of-domain detection require in-domain task labels and are
limited to supervised classification scenarios. Our work tackles the problem of
detecting out-of-domain samples with only unsupervised in-domain data. We
utilize the latent representations of pre-trained transformers and propose a
simple yet effective method to transform features across all layers to
construct out-of-domain detectors efficiently. Two domain-specific fine-tuning
approaches are further proposed to boost detection accuracy. Our empirical
evaluations of related methods on two datasets validate that our method greatly
improves out-of-domain detection ability in a more general scenario.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1"&gt;Keyang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_T/0/1/0/all/0/1"&gt;Tongzheng Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shikun Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1"&gt;Yihao Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1"&gt;Caiming Xiong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Minimax and Neyman-Pearson Meta-Learning for Outlier Languages. (arXiv:2106.01051v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01051</id>
        <link href="http://arxiv.org/abs/2106.01051"/>
        <updated>2021-06-03T02:10:34.176Z</updated>
        <summary type="html"><![CDATA[Model-agnostic meta-learning (MAML) has been recently put forth as a strategy
to learn resource-poor languages in a sample-efficient fashion. Nevertheless,
the properties of these languages are often not well represented by those
available during training. Hence, we argue that the i.i.d. assumption ingrained
in MAML makes it ill-suited for cross-lingual NLP. In fact, under a
decision-theoretic framework, MAML can be interpreted as minimising the
expected risk across training languages (with a uniform prior), which is known
as Bayes criterion. To increase its robustness to outlier languages, we create
two variants of MAML based on alternative criteria: Minimax MAML reduces the
maximum risk across languages, while Neyman-Pearson MAML constrains the risk in
each language to a maximum threshold. Both criteria constitute fully
differentiable two-player games. In light of this, we propose a new adaptive
optimiser solving for a local approximation to their Nash equilibrium. We
evaluate both model variants on two popular NLP tasks, part-of-speech tagging
and question answering. We report gains for their average and minimum
performance across low-resource languages in zero- and few-shot settings,
compared to joint multi-source transfer and vanilla MAML.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ponti_E/0/1/0/all/0/1"&gt;Edoardo Maria Ponti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aralikatte_R/0/1/0/all/0/1"&gt;Rahul Aralikatte&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shrivastava_D/0/1/0/all/0/1"&gt;Disha Shrivastava&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reddy_S/0/1/0/all/0/1"&gt;Siva Reddy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sogaard_A/0/1/0/all/0/1"&gt;Anders S&amp;#xf8;gaard&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Comparing Test Sets with Item Response Theory. (arXiv:2106.00840v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.00840</id>
        <link href="http://arxiv.org/abs/2106.00840"/>
        <updated>2021-06-03T02:10:34.146Z</updated>
        <summary type="html"><![CDATA[Recent years have seen numerous NLP datasets introduced to evaluate the
performance of fine-tuned models on natural language understanding tasks.
Recent results from large pretrained models, though, show that many of these
datasets are largely saturated and unlikely to be able to detect further
progress. What kind of datasets are still effective at discriminating among
strong models, and what kind of datasets should we expect to be able to detect
future improvements? To measure this uniformly across datasets, we draw on Item
Response Theory and evaluate 29 datasets using predictions from 18 pretrained
Transformer models on individual test examples. We find that Quoref, HellaSwag,
and MC-TACO are best suited for distinguishing among state-of-the-art models,
while SNLI, MNLI, and CommitmentBank seem to be saturated for current strong
models. We also observe span selection task format, which is used for QA
datasets like QAMR or SQuAD2.0, is effective in differentiating between strong
and weak models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vania_C/0/1/0/all/0/1"&gt;Clara Vania&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Htut_P/0/1/0/all/0/1"&gt;Phu Mon Htut&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1"&gt;William Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mungra_D/0/1/0/all/0/1"&gt;Dhara Mungra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pang_R/0/1/0/all/0/1"&gt;Richard Yuanzhe Pang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Phang_J/0/1/0/all/0/1"&gt;Jason Phang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Haokun Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1"&gt;Kyunghyun Cho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bowman_S/0/1/0/all/0/1"&gt;Samuel R. Bowman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Answer Generation for Retrieval-based Question Answering Systems. (arXiv:2106.00955v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.00955</id>
        <link href="http://arxiv.org/abs/2106.00955"/>
        <updated>2021-06-03T02:10:34.104Z</updated>
        <summary type="html"><![CDATA[Recent advancements in transformer-based models have greatly improved the
ability of Question Answering (QA) systems to provide correct answers; in
particular, answer sentence selection (AS2) models, core components of
retrieval-based systems, have achieved impressive results. While generally
effective, these models fail to provide a satisfying answer when all retrieved
candidates are of poor quality, even if they contain correct information. In
AS2, models are trained to select the best answer sentence among a set of
candidates retrieved for a given question. In this work, we propose to generate
answers from a set of AS2 top candidates. Rather than selecting the best
candidate, we train a sequence to sequence transformer model to generate an
answer from a candidate set. Our tests on three English AS2 datasets show
improvement up to 32 absolute points in accuracy over the state of the art.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hsu_C/0/1/0/all/0/1"&gt;Chao-Chun Hsu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lind_E/0/1/0/all/0/1"&gt;Eric Lind&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soldaini_L/0/1/0/all/0/1"&gt;Luca Soldaini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moschitti_A/0/1/0/all/0/1"&gt;Alessandro Moschitti&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What Ingredients Make for an Effective Crowdsourcing Protocol for Difficult NLU Data Collection Tasks?. (arXiv:2106.00794v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.00794</id>
        <link href="http://arxiv.org/abs/2106.00794"/>
        <updated>2021-06-03T02:10:34.059Z</updated>
        <summary type="html"><![CDATA[Crowdsourcing is widely used to create data for common natural language
understanding tasks. Despite the importance of these datasets for measuring and
refining model understanding of language, there has been little focus on the
crowdsourcing methods used for collecting the datasets. In this paper, we
compare the efficacy of interventions that have been proposed in prior work as
ways of improving data quality. We use multiple-choice question answering as a
testbed and run a randomized trial by assigning crowdworkers to write questions
under one of four different data collection protocols. We find that asking
workers to write explanations for their examples is an ineffective stand-alone
strategy for boosting NLU example difficulty. However, we find that training
crowdworkers, and then using an iterative process of collecting data, sending
feedback, and qualifying workers based on expert judgments is an effective
means of collecting challenging data. But using crowdsourced, instead of expert
judgments, to qualify workers and send feedback does not prove to be effective.
We observe that the data from the iterative protocol with expert assessments is
more challenging by several measures. Notably, the human--model gap on the
unanimous agreement portion of this data is, on average, twice as large as the
gap for the baseline protocol data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nangia_N/0/1/0/all/0/1"&gt;Nikita Nangia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sugawara_S/0/1/0/all/0/1"&gt;Saku Sugawara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Trivedi_H/0/1/0/all/0/1"&gt;Harsh Trivedi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Warstadt_A/0/1/0/all/0/1"&gt;Alex Warstadt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vania_C/0/1/0/all/0/1"&gt;Clara Vania&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bowman_S/0/1/0/all/0/1"&gt;Samuel R. Bowman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ConvoSumm: Conversation Summarization Benchmark and Improved Abstractive Summarization with Argument Mining. (arXiv:2106.00829v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.00829</id>
        <link href="http://arxiv.org/abs/2106.00829"/>
        <updated>2021-06-03T02:10:34.054Z</updated>
        <summary type="html"><![CDATA[While online conversations can cover a vast amount of information in many
different formats, abstractive text summarization has primarily focused on
modeling solely news articles. This research gap is due, in part, to the lack
of standardized datasets for summarizing online discussions. To address this
gap, we design annotation protocols motivated by an
issues--viewpoints--assertions framework to crowdsource four new datasets on
diverse online conversation forms of news comments, discussion forums,
community question answering forums, and email threads. We benchmark
state-of-the-art models on our datasets and analyze characteristics associated
with the data. To create a comprehensive benchmark, we also evaluate these
models on widely-used conversation summarization datasets to establish strong
baselines in this domain. Furthermore, we incorporate argument mining through
graph construction to directly model the issues, viewpoints, and assertions
present in a conversation and filter noisy input, showing comparable or
improved results according to automatic and human evaluations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fabbri_A/0/1/0/all/0/1"&gt;Alexander R. Fabbri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rahman_F/0/1/0/all/0/1"&gt;Faiaz Rahman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rizvi_I/0/1/0/all/0/1"&gt;Imad Rizvi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1"&gt;Borui Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Haoran Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mehdad_Y/0/1/0/all/0/1"&gt;Yashar Mehdad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Radev_D/0/1/0/all/0/1"&gt;Dragomir Radev&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RevCore: Review-augmented Conversational Recommendation. (arXiv:2106.00957v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.00957</id>
        <link href="http://arxiv.org/abs/2106.00957"/>
        <updated>2021-06-03T02:10:34.040Z</updated>
        <summary type="html"><![CDATA[Existing conversational recommendation (CR) systems usually suffer from
insufficient item information when conducted on short dialogue history and
unfamiliar items. Incorporating external information (e.g., reviews) is a
potential solution to alleviate this problem. Given that reviews often provide
a rich and detailed user experience on different interests, they are potential
ideal resources for providing high-quality recommendations within an
informative conversation. In this paper, we design a novel end-to-end
framework, namely, Review-augmented Conversational Recommender (RevCore), where
reviews are seamlessly incorporated to enrich item information and assist in
generating both coherent and informative responses. In detail, we extract
sentiment-consistent reviews, perform review-enriched and entity-based
recommendations for item suggestions, as well as use a review-attentive
encoder-decoder for response generation. Experimental results demonstrate the
superiority of our approach in yielding better performance on both
recommendation and conversation responding.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1"&gt;Yu Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1"&gt;Junwei Bao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1"&gt;Yan Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1"&gt;Zichen Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1"&gt;Shuguang Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Youzheng Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1"&gt;Xiaodong He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Multi-Level Attention Model for Evidence-Based Fact Checking. (arXiv:2106.00950v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.00950</id>
        <link href="http://arxiv.org/abs/2106.00950"/>
        <updated>2021-06-03T02:10:34.035Z</updated>
        <summary type="html"><![CDATA[Evidence-based fact checking aims to verify the truthfulness of a claim
against evidence extracted from textual sources. Learning a representation that
effectively captures relations between a claim and evidence can be challenging.
Recent state-of-the-art approaches have developed increasingly sophisticated
models based on graph structures. We present a simple model that can be trained
on sequence structures. Our model enables inter-sentence attentions at
different levels and can benefit from joint training. Results on a large-scale
dataset for Fact Extraction and VERification (FEVER) show that our model
outperforms the graph-based approaches and yields 1.09% and 1.42% improvements
in label accuracy and FEVER score, respectively, over the best published model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kruengkrai_C/0/1/0/all/0/1"&gt;Canasai Kruengkrai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yamagishi_J/0/1/0/all/0/1"&gt;Junichi Yamagishi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xin Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Long Term Motion Prediction Using Keyposes. (arXiv:2012.04731v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.04731</id>
        <link href="http://arxiv.org/abs/2012.04731"/>
        <updated>2021-06-03T02:10:34.030Z</updated>
        <summary type="html"><![CDATA[Long term human motion prediction is essential in safety-critical
applications such as human-robot interaction and autonomous driving. In this
paper, we show that, to achieve long term forecasting, predicting human pose at
every time instant is unnecessary. Instead, it is more effective to predict a
few keyposes and approximate intermediate ones by linearly interpolating the
keyposes.

We will demonstrate that our approach enables us to predict realistic motions
for up to 5 seconds in the future, which is far larger than the typical 1
second encountered in the literature. Over this extended time period, our
predictions are more realistic and better preserve the motion dynamics than
those state-of-the-art methods yield.

Furthermore, because we model future keyposes probabilistically, we can
generate multiple plausible future motions by sampling at inference time. This
is useful to model because people usually can do one of several things given
what they have already done.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kiciroglu_S/0/1/0/all/0/1"&gt;Sena Kiciroglu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salzmann_M/0/1/0/all/0/1"&gt;Mathieu Salzmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fua_P/0/1/0/all/0/1"&gt;Pascal Fua&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Claim Matching Beyond English to Scale Global Fact-Checking. (arXiv:2106.00853v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.00853</id>
        <link href="http://arxiv.org/abs/2106.00853"/>
        <updated>2021-06-03T02:10:34.025Z</updated>
        <summary type="html"><![CDATA[Manual fact-checking does not scale well to serve the needs of the internet.
This issue is further compounded in non-English contexts. In this paper, we
discuss claim matching as a possible solution to scale fact-checking. We define
claim matching as the task of identifying pairs of textual messages containing
claims that can be served with one fact-check. We construct a novel dataset of
WhatsApp tipline and public group messages alongside fact-checked claims that
are first annotated for containing "claim-like statements" and then matched
with potentially similar items and annotated for claim matching. Our dataset
contains content in high-resource (English, Hindi) and lower-resource (Bengali,
Malayalam, Tamil) languages. We train our own embedding model using knowledge
distillation and a high-quality "teacher" model in order to address the
imbalance in embedding quality between the low- and high-resource languages in
our dataset. We provide evaluations on the performance of our solution and
compare with baselines and existing state-of-the-art multilingual embedding
models, namely LASER and LaBSE. We demonstrate that our performance exceeds
LASER and LaBSE in all settings. We release our annotated datasets, codebooks,
and trained embedding model to allow for further research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kazemi_A/0/1/0/all/0/1"&gt;Ashkan Kazemi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Garimella_K/0/1/0/all/0/1"&gt;Kiran Garimella&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gaffney_D/0/1/0/all/0/1"&gt;Devin Gaffney&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hale_S/0/1/0/all/0/1"&gt;Scott A. Hale&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Style Normalization and Restitution for Domain Generalization and Adaptation. (arXiv:2101.00588v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.00588</id>
        <link href="http://arxiv.org/abs/2101.00588"/>
        <updated>2021-06-03T02:10:34.012Z</updated>
        <summary type="html"><![CDATA[For many practical computer vision applications, the learned models usually
have high performance on the datasets used for training but suffer from
significant performance degradation when deployed in new environments, where
there are usually style differences between the training images and the testing
images. An effective domain generalizable model is expected to be able to learn
feature representations that are both generalizable and discriminative. In this
paper, we design a novel Style Normalization and Restitution module (SNR) to
simultaneously ensure both high generalization and discrimination capability of
the networks. In the SNR module, particularly, we filter out the style
variations (e.g, illumination, color contrast) by performing Instance
Normalization (IN) to obtain style normalized features, where the discrepancy
among different samples and domains is reduced. However, such a process is
task-ignorant and inevitably removes some task-relevant discriminative
information, which could hurt the performance. To remedy this, we propose to
distill task-relevant discriminative features from the residual (i.e, the
difference between the original feature and the style normalized feature) and
add them back to the network to ensure high discrimination. Moreover, for
better disentanglement, we enforce a dual causality loss constraint in the
restitution step to encourage the better separation of task-relevant and
task-irrelevant features. We validate the effectiveness of our SNR on different
computer vision tasks, including classification, semantic segmentation, and
object detection. Experiments demonstrate that our SNR module is capable of
improving the performance of networks for domain generalization (DG) and
unsupervised domain adaptation (UDA) on many tasks. Code are available at
https://github.com/microsoft/SNR.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1"&gt;Xin Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lan_C/0/1/0/all/0/1"&gt;Cuiling Lan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1"&gt;Wenjun Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhibo Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rethinking conditional GAN training: An approach using geometrically structured latent manifolds. (arXiv:2011.13055v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.13055</id>
        <link href="http://arxiv.org/abs/2011.13055"/>
        <updated>2021-06-03T02:10:34.006Z</updated>
        <summary type="html"><![CDATA[Conditional GANs (cGAN), in their rudimentary form, suffer from critical
drawbacks such as the lack of diversity in generated outputs and distortion
between the latent and output manifolds. Although efforts have been made to
improve results, they can suffer from unpleasant side-effects such as the
topology mismatch between latent and output spaces. In contrast, we tackle this
problem from a geometrical perspective and propose a novel training mechanism
that increases both the diversity and the visual quality of a vanilla cGAN, by
systematically encouraging a bi-lipschitz mapping between the latent and the
output manifolds. We validate the efficacy of our solution on a baseline cGAN
(i.e., Pix2Pix) which lacks diversity, and show that by only modifying its
training mechanism (i.e., with our proposed Pix2Pix-Geo), one can achieve more
diverse and realistic outputs on a broad set of image-to-image translation
tasks. Codes are available at https://github.com/samgregoost/Rethinking-CGANs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ramasinghe_S/0/1/0/all/0/1"&gt;Sameera Ramasinghe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Farazi_M/0/1/0/all/0/1"&gt;Moshiur Farazi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1"&gt;Salman Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barnes_N/0/1/0/all/0/1"&gt;Nick Barnes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gould_S/0/1/0/all/0/1"&gt;Stephen Gould&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quality Estimation for Image Captions Based on Large-scale Human Evaluations. (arXiv:1909.03396v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1909.03396</id>
        <link href="http://arxiv.org/abs/1909.03396"/>
        <updated>2021-06-03T02:10:34.000Z</updated>
        <summary type="html"><![CDATA[Automatic image captioning has improved significantly over the last few
years, but the problem is far from being solved, with state of the art models
still often producing low quality captions when used in the wild. In this
paper, we focus on the task of Quality Estimation (QE) for image captions,
which attempts to model the caption quality from a human perspective and
without access to ground-truth references, so that it can be applied at
prediction time to detect low-quality captions produced on previously unseen
images. For this task, we develop a human evaluation process that collects
coarse-grained caption annotations from crowdsourced users, which is then used
to collect a large scale dataset spanning more than 600k caption quality
ratings. We then carefully validate the quality of the collected ratings and
establish baseline models for this new QE task. Finally, we further collect
fine-grained caption quality annotations from trained raters, and use them to
demonstrate that QE models trained over the coarse ratings can effectively
detect and filter out low-quality image captions, thereby improving the user
experience from captioning systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Levinboim_T/0/1/0/all/0/1"&gt;Tomer Levinboim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thapliyal_A/0/1/0/all/0/1"&gt;Ashish V. Thapliyal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_P/0/1/0/all/0/1"&gt;Piyush Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soricut_R/0/1/0/all/0/1"&gt;Radu Soricut&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Passage Retrieval with Hashing for Open-domain Question Answering. (arXiv:2106.00882v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.00882</id>
        <link href="http://arxiv.org/abs/2106.00882"/>
        <updated>2021-06-03T02:10:33.980Z</updated>
        <summary type="html"><![CDATA[Most state-of-the-art open-domain question answering systems use a neural
retrieval model to encode passages into continuous vectors and extract them
from a knowledge source. However, such retrieval models often require large
memory to run because of the massive size of their passage index. In this
paper, we introduce Binary Passage Retriever (BPR), a memory-efficient neural
retrieval model that integrates a learning-to-hash technique into the
state-of-the-art Dense Passage Retriever (DPR) to represent the passage index
using compact binary codes rather than continuous vectors. BPR is trained with
a multi-task objective over two tasks: efficient candidate generation based on
binary codes and accurate reranking based on continuous vectors. Compared with
DPR, BPR substantially reduces the memory cost from 65GB to 2GB without a loss
of accuracy on two standard open-domain question answering benchmarks: Natural
Questions and TriviaQA. Our code and trained models are available at
https://github.com/studio-ousia/bpr.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yamada_I/0/1/0/all/0/1"&gt;Ikuya Yamada&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Asai_A/0/1/0/all/0/1"&gt;Akari Asai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1"&gt;Hannaneh Hajishirzi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[UPFlow: Upsampling Pyramid for Unsupervised Optical Flow Learning. (arXiv:2012.00212v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.00212</id>
        <link href="http://arxiv.org/abs/2012.00212"/>
        <updated>2021-06-03T02:10:33.966Z</updated>
        <summary type="html"><![CDATA[We present an unsupervised learning approach for optical flow estimation by
improving the upsampling and learning of pyramid network. We design a
self-guided upsample module to tackle the interpolation blur problem caused by
bilinear upsampling between pyramid levels. Moreover, we propose a pyramid
distillation loss to add supervision for intermediate levels via distilling the
finest flow as pseudo labels. By integrating these two components together, our
method achieves the best performance for unsupervised optical flow learning on
multiple leading benchmarks, including MPI-SIntel, KITTI 2012 and KITTI 2015.
In particular, we achieve EPE=1.4 on KITTI 2012 and F1=9.38% on KITTI 2015,
which outperform the previous state-of-the-art methods by 22.2% and 15.7%,
respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Luo_K/0/1/0/all/0/1"&gt;Kunming Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chuan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Shuaicheng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1"&gt;Haoqiang Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jue Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1"&gt;Jian Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Discrete Cosine Transform as Universal Sentence Encoder. (arXiv:2106.00934v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.00934</id>
        <link href="http://arxiv.org/abs/2106.00934"/>
        <updated>2021-06-03T02:10:33.960Z</updated>
        <summary type="html"><![CDATA[Modern sentence encoders are used to generate dense vector representations
that capture the underlying linguistic characteristics for a sequence of words,
including phrases, sentences, or paragraphs. These kinds of representations are
ideal for training a classifier for an end task such as sentiment analysis,
question answering and text classification. Different models have been proposed
to efficiently generate general purpose sentence representations to be used in
pretraining protocols. While averaging is the most commonly used efficient
sentence encoder, Discrete Cosine Transform (DCT) was recently proposed as an
alternative that captures the underlying syntactic characteristics of a given
text without compromising practical efficiency compared to averaging. However,
as with most other sentence encoders, the DCT sentence encoder was only
evaluated in English. To this end, we utilize DCT encoder to generate universal
sentence representation for different languages such as German, French, Spanish
and Russian. The experimental results clearly show the superior effectiveness
of DCT encoding in which consistent performance improvements are achieved over
strong baselines on multiple standardized datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Almarwani_N/0/1/0/all/0/1"&gt;Nada Almarwani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Diab_M/0/1/0/all/0/1"&gt;Mona Diab&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Attention Based Semantic Segmentation on UAV Dataset for Natural Disaster Damage Assessment. (arXiv:2105.14540v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14540</id>
        <link href="http://arxiv.org/abs/2105.14540"/>
        <updated>2021-06-03T02:10:33.885Z</updated>
        <summary type="html"><![CDATA[The detrimental impacts of climate change include stronger and more
destructive hurricanes happening all over the world. Identifying different
damaged structures of an area including buildings and roads are vital since it
helps the rescue team to plan their efforts to minimize the damage caused by a
natural disaster. Semantic segmentation helps to identify different parts of an
image. We implement a novel self-attention based semantic segmentation model on
a high resolution UAV dataset and attain Mean IoU score of around 88% on the
test set. The result inspires to use self-attention schemes in natural disaster
damage assessment which will save human lives and reduce economic losses.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chowdhury_T/0/1/0/all/0/1"&gt;Tashnim Chowdhury&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rahnemoonfar_M/0/1/0/all/0/1"&gt;Maryam Rahnemoonfar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Implicit Representations of Meaning in Neural Language Models. (arXiv:2106.00737v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.00737</id>
        <link href="http://arxiv.org/abs/2106.00737"/>
        <updated>2021-06-03T02:10:33.879Z</updated>
        <summary type="html"><![CDATA[Does the effectiveness of neural language models derive entirely from
accurate modeling of surface word co-occurrence statistics, or do these models
represent and reason about the world they describe? In BART and T5 transformer
language models, we identify contextual word representations that function as
models of entities and situations as they evolve throughout a discourse. These
neural representations have functional similarities to linguistic models of
dynamic semantics: they support a linear readout of each entity's current
properties and relations, and can be manipulated with predictable effects on
language generation. Our results indicate that prediction in pretrained neural
language models is supported, at least in part, by dynamic representations of
meaning and implicit simulation of entity state, and that this behavior can be
learned with only text as training data. Code and data are available at
https://github.com/belindal/state-probes .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Belinda Z. Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nye_M/0/1/0/all/0/1"&gt;Maxwell Nye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Andreas_J/0/1/0/all/0/1"&gt;Jacob Andreas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pay Attention to MLPs. (arXiv:2105.08050v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.08050</id>
        <link href="http://arxiv.org/abs/2105.08050"/>
        <updated>2021-06-03T02:10:33.831Z</updated>
        <summary type="html"><![CDATA[Transformers have become one of the most important architectural innovations
in deep learning and have enabled many breakthroughs over the past few years.
Here we propose a simple network architecture, gMLP, based on MLPs with gating,
and show that it can perform as well as Transformers in key language and vision
applications. Our comparisons show that self-attention is not critical for
Vision Transformers, as gMLP can achieve the same accuracy. For BERT, our model
achieves parity with Transformers on pretraining perplexity and is better on
some downstream NLP tasks. On finetuning tasks where gMLP performs worse,
making the gMLP model substantially larger can close the gap with Transformers.
In general, our experiments show that gMLP can scale as well as Transformers
over increased data and compute.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Hanxiao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_Z/0/1/0/all/0/1"&gt;Zihang Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+So_D/0/1/0/all/0/1"&gt;David R. So&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1"&gt;Quoc V. Le&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ImVoxelNet: Image to Voxels Projection for Monocular and Multi-View General-Purpose 3D Object Detection. (arXiv:2106.01178v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.01178</id>
        <link href="http://arxiv.org/abs/2106.01178"/>
        <updated>2021-06-03T02:10:33.823Z</updated>
        <summary type="html"><![CDATA[In this paper, we introduce the task of multi-view RGB-based 3D object
detection as an end-to-end optimization problem. To address this problem, we
propose ImVoxelNet, a novel fully convolutional method of 3D object detection
based on monocular or multi-view RGB images. The number of monocular images in
each multi-view input can variate during training and inference; actually, this
number might be unique for each multi-view input. ImVoxelNet successfully
handles both indoor and outdoor scenes, which makes it general-purpose.
Specifically, it achieves state-of-the-art results in car detection on KITTI
(monocular) and nuScenes (multi-view) benchmarks among all methods that accept
RGB images. Moreover, it surpasses existing RGB-based 3D object detection
methods on the SUN RGB-D dataset. On ScanNet, ImVoxelNet sets a new benchmark
for multi-view 3D object detection. The source code and the trained models are
available at \url{https://github.com/saic-vul/imvoxelnet}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rukhovich_D/0/1/0/all/0/1"&gt;Danila Rukhovich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vorontsova_A/0/1/0/all/0/1"&gt;Anna Vorontsova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Konushin_A/0/1/0/all/0/1"&gt;Anton Konushin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Part of Speech and Universal Dependency effects on English Arabic Machine Translation. (arXiv:2106.00745v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.00745</id>
        <link href="http://arxiv.org/abs/2106.00745"/>
        <updated>2021-06-03T02:10:33.815Z</updated>
        <summary type="html"><![CDATA[In this research paper, I will elaborate on a method to evaluate machine
translation models based on their performance on underlying syntactical
phenomena between English and Arabic languages. This method is especially
important as such "neural" and "machine learning" are hard to fine-tune and
change. Thus, finding a way to evaluate them easily and diversely would greatly
help the task of bettering them.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Abend_O/0/1/0/all/0/1"&gt;Omri Abend&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choshen_L/0/1/0/all/0/1"&gt;Leshem Choshen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nikolaev_D/0/1/0/all/0/1"&gt;Dmitry Nikolaev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rafaeli_O/0/1/0/all/0/1"&gt;Ofek Rafaeli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Conversational Question Answering: A Survey. (arXiv:2106.00874v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.00874</id>
        <link href="http://arxiv.org/abs/2106.00874"/>
        <updated>2021-06-03T02:10:33.808Z</updated>
        <summary type="html"><![CDATA[Question answering (QA) systems provide a way of querying the information
available in various formats including, but not limited to, unstructured and
structured data in natural languages. It constitutes a considerable part of
conversational artificial intelligence (AI) which has led to the introduction
of a special research topic on Conversational Question Answering (CQA), wherein
a system is required to understand the given context and then engages in
multi-turn QA to satisfy the user's information needs. Whilst the focus of most
of the existing research work is subjected to single-turn QA, the field of
multi-turn QA has recently grasped attention and prominence owing to the
availability of large-scale, multi-turn QA datasets and the development of
pre-trained language models. With a good amount of models and research papers
adding to the literature every year recently, there is a dire need of arranging
and presenting the related work in a unified manner to streamline future
research. This survey, therefore, is an effort to present a comprehensive
review of the state-of-the-art research trends of CQA primarily based on
reviewed papers from 2016-2021. Our findings show that there has been a trend
shift from single-turn to multi-turn QA which empowers the field of
Conversational AI from different perspectives. This survey is intended to
provide an epitome for the research community with the hope of laying a strong
foundation for the field of CQA.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zaib_M/0/1/0/all/0/1"&gt;Munazza Zaib&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Wei Emma Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sheng_Q/0/1/0/all/0/1"&gt;Quan Z. Sheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mahmood_A/0/1/0/all/0/1"&gt;Adnan Mahmood&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yang Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Benchmarking CNN on 3D Anatomical Brain MRI: Architectures, Data Augmentation and Deep Ensemble Learning. (arXiv:2106.01132v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.01132</id>
        <link href="http://arxiv.org/abs/2106.01132"/>
        <updated>2021-06-03T02:10:33.787Z</updated>
        <summary type="html"><![CDATA[Deep Learning (DL) and specifically CNN models have become a de facto method
for a wide range of vision tasks, outperforming traditional machine learning
(ML) methods. Consequently, they drew a lot of attention in the neuroimaging
field in particular for phenotype prediction or computer-aided diagnosis.
However, most of the current studies often deal with small single-site cohorts,
along with a specific pre-processing pipeline and custom CNN architectures,
which make them difficult to compare to. We propose an extensive benchmark of
recent state-of-the-art (SOTA) 3D CNN, evaluating also the benefits of data
augmentation and deep ensemble learning, on both Voxel-Based Morphometry (VBM)
pre-processing and quasi-raw images. Experiments were conducted on a large
multi-site 3D brain anatomical MRI data-set comprising N=10k scans on 3
challenging tasks: age prediction, sex classification, and schizophrenia
diagnosis. We found that all models provide significantly better predictions
with VBM images than quasi-raw data. This finding evolved as the training set
approaches 10k samples where quasi-raw data almost reach the performance of
VBM. Moreover, we showed that linear models perform comparably with SOTA CNN on
VBM data. We also demonstrated that DenseNet and tiny-DenseNet, a lighter
version that we proposed, provide a good compromise in terms of performance in
all data regime. Therefore, we suggest to employ them as the architectures by
default. Critically, we also showed that current CNN are still very biased
towards the acquisition site, even when trained with N=10k multi-site images.
In this context, VBM pre-processing provides an efficient way to limit this
site effect. Surprisingly, we did not find any clear benefit from data
augmentation techniques. Finally, we proved that deep ensemble learning is well
suited to re-calibrate big CNN models without sacrificing performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dufumier_B/0/1/0/all/0/1"&gt;Benoit Dufumier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gori_P/0/1/0/all/0/1"&gt;Pietro Gori&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Battaglia_I/0/1/0/all/0/1"&gt;Ilaria Battaglia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Victor_J/0/1/0/all/0/1"&gt;Julie Victor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grigis_A/0/1/0/all/0/1"&gt;Antoine Grigis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duchesnay_E/0/1/0/all/0/1"&gt;Edouard Duchesnay&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Search Methods for Sufficient, Socially-Aligned Feature Importance Explanations with In-Distribution Counterfactuals. (arXiv:2106.00786v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.00786</id>
        <link href="http://arxiv.org/abs/2106.00786"/>
        <updated>2021-06-03T02:10:33.779Z</updated>
        <summary type="html"><![CDATA[Feature importance (FI) estimates are a popular form of explanation, and they
are commonly created and evaluated by computing the change in model confidence
caused by removing certain input features at test time. For example, in the
standard Sufficiency metric, only the top-k most important tokens are kept. In
this paper, we study several under-explored dimensions of FI-based
explanations, providing conceptual and empirical improvements for this form of
explanation. First, we advance a new argument for why it can be problematic to
remove features from an input when creating or evaluating explanations: the
fact that these counterfactual inputs are out-of-distribution (OOD) to models
implies that the resulting explanations are socially misaligned. The crux of
the problem is that the model prior and random weight initialization influence
the explanations (and explanation metrics) in unintended ways. To resolve this
issue, we propose a simple alteration to the model training process, which
results in more socially aligned explanations and metrics. Second, we compare
among five approaches for removing features from model inputs. We find that
some methods produce more OOD counterfactuals than others, and we make
recommendations for selecting a feature-replacement function. Finally, we
introduce four search-based methods for identifying FI explanations and compare
them to strong baselines, including LIME, Integrated Gradients, and random
search. On experiments with six diverse text classification datasets, we find
that the only method that consistently outperforms random search is a Parallel
Local Search that we introduce. Improvements over the second-best method are as
large as 5.4 points for Sufficiency and 17 points for Comprehensiveness. All
supporting code is publicly available at
https://github.com/peterbhase/ExplanationSearch.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hase_P/0/1/0/all/0/1"&gt;Peter Hase&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_H/0/1/0/all/0/1"&gt;Harry Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1"&gt;Mohit Bansal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Clustering Activation Maps for Emphysema Subtyping. (arXiv:2106.01351v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.01351</id>
        <link href="http://arxiv.org/abs/2106.01351"/>
        <updated>2021-06-03T02:10:33.763Z</updated>
        <summary type="html"><![CDATA[We propose a deep learning clustering method that exploits dense features
from a segmentation network for emphysema subtyping from computed tomography
(CT) scans. Using dense features enables high-resolution visualization of image
regions corresponding to the cluster assignment via dense clustering activation
maps (dCAMs). This approach provides model interpretability. We evaluated
clustering results on 500 subjects from the COPDGenestudy, where radiologists
manually annotated emphysema sub-types according to their visual CT assessment.
We achieved a 43% unsupervised clustering accuracy, outperforming our baseline
at 41% and yielding results comparable to supervised classification at 45%. The
proposed method also offers a better cluster formation than the baseline,
achieving0.54 in silhouette coefficient and 0.55 in David-Bouldin scores.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Xie_W/0/1/0/all/0/1"&gt;Weiyi Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jacobs_C/0/1/0/all/0/1"&gt;Colin Jacobs&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ginneken_B/0/1/0/all/0/1"&gt;Bram van Ginneken&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Isometric Non-Rigid Structure-from-Motion. (arXiv:2010.04690v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.04690</id>
        <link href="http://arxiv.org/abs/2010.04690"/>
        <updated>2021-06-03T02:10:33.751Z</updated>
        <summary type="html"><![CDATA[Non-Rigid Structure-from-Motion (NRSfM) reconstructs a deformable 3D object
from the correspondences established between monocular 2D images. Current NRSfM
methods lack statistical robustness, which is the ability to cope with
correspondence errors.This prevents one to use automatically established
correspondences, which are prone to errors, thereby strongly limiting the scope
of NRSfM. We propose a three-step automatic pipeline to solve NRSfM robustly by
exploiting isometry. Step 1 computes the optical flow from correspondences,
step 2 reconstructs each 3D point's normal vector using multiple reference
images and integrates them to form surfaces with the best reference and step 3
rejects the 3D points that break isometry in their local neighborhood.
Importantly, each step is designed to discard or flag erroneous
correspondences. Our contributions include the robustification of optical flow
by warp estimation, new fast analytic solutions to local normal reconstruction
and their robustification, and a new scale-independent measure of 3D local
isometric coherence. Experimental results show that our robust NRSfM method
consistently outperforms existing methods on both synthetic and real datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Parashar_S/0/1/0/all/0/1"&gt;Shaifali Parashar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bartoli_A/0/1/0/all/0/1"&gt;Adrien Bartoli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pizarro_D/0/1/0/all/0/1"&gt;Daniel Pizarro&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SpectralDefense: Detecting Adversarial Attacks on CNNs in the Fourier Domain. (arXiv:2103.03000v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.03000</id>
        <link href="http://arxiv.org/abs/2103.03000"/>
        <updated>2021-06-03T02:10:33.738Z</updated>
        <summary type="html"><![CDATA[Despite the success of convolutional neural networks (CNNs) in many computer
vision and image analysis tasks, they remain vulnerable against so-called
adversarial attacks: Small, crafted perturbations in the input images can lead
to false predictions. A possible defense is to detect adversarial examples. In
this work, we show how analysis in the Fourier domain of input images and
feature maps can be used to distinguish benign test samples from adversarial
images. We propose two novel detection methods: Our first method employs the
magnitude spectrum of the input images to detect an adversarial attack. This
simple and robust classifier can successfully detect adversarial perturbations
of three commonly used attack methods. The second method builds upon the first
and additionally extracts the phase of Fourier coefficients of feature-maps at
different layers of the network. With this extension, we are able to improve
adversarial detection rates compared to state-of-the-art detectors on five
different attack methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Harder_P/0/1/0/all/0/1"&gt;Paula Harder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pfreundt_F/0/1/0/all/0/1"&gt;Franz-Josef Pfreundt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Keuper_M/0/1/0/all/0/1"&gt;Margret Keuper&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Keuper_J/0/1/0/all/0/1"&gt;Janis Keuper&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Parameter-Efficient Neural Question Answering Models via Graph-Enriched Document Representations. (arXiv:2106.00851v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.00851</id>
        <link href="http://arxiv.org/abs/2106.00851"/>
        <updated>2021-06-03T02:10:33.723Z</updated>
        <summary type="html"><![CDATA[As the computational footprint of modern NLP systems grows, it becomes
increasingly important to arrive at more efficient models. We show that by
employing graph convolutional document representation, we can arrive at a
question answering system that performs comparably to, and in some cases
exceeds the SOTA solutions, while using less than 5\% of their resources in
terms of trainable parameters. As it currently stands, a major issue in
applying GCNs to NLP is document representation. In this paper, we show that a
GCN enriched document representation greatly improves the results seen in
HotPotQA, even when using a trivial topology. Our model (gQA), performs
admirably when compared to the current SOTA, and requires little to no
preprocessing. In Shao et al. 2020, the authors suggest that graph networks are
not necessary for good performance in multi-hop QA. In this paper, we suggest
that large language models are not necessary for good performance by showing a
na\"{i}ve implementation of a GCN performs comparably to SoTA models based on
pretrained language models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Castricato_L/0/1/0/all/0/1"&gt;Louis Castricato&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fitz_S/0/1/0/all/0/1"&gt;Stephen Fitz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shin_W/0/1/0/all/0/1"&gt;Won Young Shin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Effectiveness of Vision Transformers for Zero-shot Face Anti-Spoofing. (arXiv:2011.08019v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.08019</id>
        <link href="http://arxiv.org/abs/2011.08019"/>
        <updated>2021-06-03T02:10:33.708Z</updated>
        <summary type="html"><![CDATA[The vulnerability of face recognition systems to presentation attacks has
limited their application in security-critical scenarios. Automatic methods of
detecting such malicious attempts are essential for the safe use of facial
recognition technology. Although various methods have been suggested for
detecting such attacks, most of them over-fit the training set and fail in
generalizing to unseen attacks and environments. In this work, we use transfer
learning from the vision transformer model for the zero-shot anti-spoofing
task. The effectiveness of the proposed approach is demonstrated through
experiments in publicly available datasets. The proposed approach outperforms
the state-of-the-art methods in the zero-shot protocols in the HQ-WMCA and
SiW-M datasets by a large margin. Besides, the model achieves a significant
boost in cross-database performance as well.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+George_A/0/1/0/all/0/1"&gt;Anjith George&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marcel_S/0/1/0/all/0/1"&gt;Sebastien Marcel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ProSelfLC: Progressive Self Label Correction for Training Robust Deep Neural Networks. (arXiv:2005.03788v6 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.03788</id>
        <link href="http://arxiv.org/abs/2005.03788"/>
        <updated>2021-06-03T02:10:33.703Z</updated>
        <summary type="html"><![CDATA[To train robust deep neural networks (DNNs), we systematically study several
target modification approaches, which include output regularisation, self and
non-self label correction (LC). Two key issues are discovered: (1) Self LC is
the most appealing as it exploits its own knowledge and requires no extra
models. However, how to automatically decide the trust degree of a learner as
training goes is not well answered in the literature? (2) Some methods penalise
while the others reward low-entropy predictions, prompting us to ask which one
is better?

To resolve the first issue, taking two well-accepted propositions--deep
neural networks learn meaningful patterns before fitting noise [3] and minimum
entropy regularisation principle [10]--we propose a novel end-to-end method
named ProSelfLC, which is designed according to learning time and entropy.
Specifically, given a data point, we progressively increase trust in its
predicted label distribution versus its annotated one if a model has been
trained for enough time and the prediction is of low entropy (high confidence).
For the second issue, according to ProSelfLC, we empirically prove that it is
better to redefine a meaningful low-entropy status and optimise the learner
toward it. This serves as a defence of entropy minimisation.

We demonstrate the effectiveness of ProSelfLC through extensive experiments
in both clean and noisy settings. The source code is available at
https://github.com/XinshaoAmosWang/ProSelfLC-CVPR2021.

Keywords: entropy minimisation, maximum entropy, confidence penalty, self
knowledge distillation, label correction, label noise, semi-supervised
learning, output regularisation]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xinshao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hua_Y/0/1/0/all/0/1"&gt;Yang Hua&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kodirov_E/0/1/0/all/0/1"&gt;Elyor Kodirov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Clifton_D/0/1/0/all/0/1"&gt;David A. Clifton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Robertson_N/0/1/0/all/0/1"&gt;Neil M. Robertson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[IPatch: A Remote Adversarial Patch. (arXiv:2105.00113v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.00113</id>
        <link href="http://arxiv.org/abs/2105.00113"/>
        <updated>2021-06-03T02:10:33.697Z</updated>
        <summary type="html"><![CDATA[Applications such as autonomous vehicles and medical screening use deep
learning models to localize and identify hundreds of objects in a single frame.
In the past, it has been shown how an attacker can fool these models by placing
an adversarial patch within a scene. However, these patches must be placed in
the target location and do not explicitly alter the semantics elsewhere in the
image.

In this paper, we introduce a new type of adversarial patch which alters a
model's perception of an image's semantics. These patches can be placed
anywhere within an image to change the classification or semantics of locations
far from the patch. We call this new class of adversarial examples `remote
adversarial patches' (RAP).

We implement our own RAP called IPatch and perform an in-depth analysis on
image segmentation RAP attacks using five state-of-the-art architectures with
eight different encoders on the CamVid street view dataset. Moreover, we
demonstrate that the attack can be extended to object recognition models with
preliminary results on the popular YOLOv3 model. We found that the patch can
change the classification of a remote target region with a success rate of up
to 93% on average.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mirsky_Y/0/1/0/all/0/1"&gt;Yisroel Mirsky&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning an Animatable Detailed 3D Face Model from In-The-Wild Images. (arXiv:2012.04012v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.04012</id>
        <link href="http://arxiv.org/abs/2012.04012"/>
        <updated>2021-06-03T02:10:33.662Z</updated>
        <summary type="html"><![CDATA[While current monocular 3D face reconstruction methods can recover fine
geometric details, they suffer several limitations. Some methods produce faces
that cannot be realistically animated because they do not model how wrinkles
vary with expression. Other methods are trained on high-quality face scans and
do not generalize well to in-the-wild images. We present the first approach
that regresses 3D face shape and animatable details that are specific to an
individual but change with expression. Our model, DECA (Detailed Expression
Capture and Animation), is trained to robustly produce a UV displacement map
from a low-dimensional latent representation that consists of person-specific
detail parameters and generic expression parameters, while a regressor is
trained to predict detail, shape, albedo, expression, pose and illumination
parameters from a single image. To enable this, we introduce a novel
detail-consistency loss that disentangles person-specific details from
expression-dependent wrinkles. This disentanglement allows us to synthesize
realistic person-specific wrinkles by controlling expression parameters while
keeping person-specific details unchanged. DECA is learned from in-the-wild
images with no paired 3D supervision and achieves state-of-the-art shape
reconstruction accuracy on two benchmarks. Qualitative results on in-the-wild
data demonstrate DECA's robustness and its ability to disentangle identity- and
expression-dependent details enabling animation of reconstructed faces. The
model and code are publicly available at https://deca.is.tue.mpg.de.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1"&gt;Yao Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_H/0/1/0/all/0/1"&gt;Haiwen Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Black_M/0/1/0/all/0/1"&gt;Michael J. Black&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bolkart_T/0/1/0/all/0/1"&gt;Timo Bolkart&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ConvTransformer: A Convolutional Transformer Network for Video Frame Synthesis. (arXiv:2011.10185v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.10185</id>
        <link href="http://arxiv.org/abs/2011.10185"/>
        <updated>2021-06-03T02:10:33.648Z</updated>
        <summary type="html"><![CDATA[Deep Convolutional Neural Networks (CNNs) are powerful models that have
achieved excellent performance on difficult computer vision tasks. Although
CNNs perform well whenever large labeled training samples are available, they
work badly on video frame synthesis due to objects deforming and moving, scene
lighting changes, and cameras moving in video sequence. In this paper, we
present a novel and general end-to-end architecture, called convolutional
Transformer or ConvTransformer, for video frame sequence learning and video
frame synthesis. The core ingredient of ConvTransformer is the proposed
attention layer, i.e., multi-head convolutional self-attention layer, that
learns the sequential dependence of video sequence. ConvTransformer uses an
encoder, built upon multi-head convolutional self-attention layer, to encode
the sequential dependence between the input frames, and then a decoder decodes
the long-term dependence between the target synthesized frames and the input
frames. Experiments on video future frame extrapolation task show
ConvTransformer to be superior in quality while being more parallelizable to
recent approaches built upon convolutional LSTM (ConvLSTM). To the best of our
knowledge, this is the first time that ConvTransformer architecture is proposed
and applied to video frame synthesis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhouyong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_S/0/1/0/all/0/1"&gt;Shun Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1"&gt;Wubin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1"&gt;Jingben Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yufan Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1"&gt;Shilei Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chunguo Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1"&gt;Luxi Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Practical Lipreading with Distilled and Efficient Models. (arXiv:2007.06504v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.06504</id>
        <link href="http://arxiv.org/abs/2007.06504"/>
        <updated>2021-06-03T02:10:33.624Z</updated>
        <summary type="html"><![CDATA[Lipreading has witnessed a lot of progress due to the resurgence of neural
networks. Recent works have placed emphasis on aspects such as improving
performance by finding the optimal architecture or improving generalization.
However, there is still a significant gap between the current methodologies and
the requirements for an effective deployment of lipreading in practical
scenarios. In this work, we propose a series of innovations that significantly
bridge that gap: first, we raise the state-of-the-art performance by a wide
margin on LRW and LRW-1000 to 88.5% and 46.6%, respectively using
self-distillation. Secondly, we propose a series of architectural changes,
including a novel Depthwise Separable Temporal Convolutional Network (DS-TCN)
head, that slashes the computational cost to a fraction of the (already quite
efficient) original model. Thirdly, we show that knowledge distillation is a
very effective tool for recovering performance of the lightweight models. This
results in a range of models with different accuracy-efficiency trade-offs.
However, our most promising lightweight models are on par with the current
state-of-the-art while showing a reduction of 8.2x and 3.9x in terms of
computational cost and number of parameters, respectively, which we hope will
enable the deployment of lipreading models in practical applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ma_P/0/1/0/all/0/1"&gt;Pingchuan Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martinez_B/0/1/0/all/0/1"&gt;Brais Martinez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Petridis_S/0/1/0/all/0/1"&gt;Stavros Petridis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pantic_M/0/1/0/all/0/1"&gt;Maja Pantic&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Determining Chess Game State From an Image. (arXiv:2104.14963v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.14963</id>
        <link href="http://arxiv.org/abs/2104.14963"/>
        <updated>2021-06-03T02:10:33.618Z</updated>
        <summary type="html"><![CDATA[Identifying the configuration of chess pieces from an image of a chessboard
is a problem in computer vision that has not yet been solved accurately.
However, it is important for helping amateur chess players improve their games
by facilitating automatic computer analysis without the overhead of manually
entering the pieces. Current approaches are limited by the lack of large
datasets and are not designed to adapt to unseen chess sets. This paper puts
forth a new dataset synthesised from a 3D model that is an order of magnitude
larger than existing ones. Trained on this dataset, a novel end-to-end chess
recognition system is presented that combines traditional computer vision
techniques with deep learning. It localises the chessboard using a RANSAC-based
algorithm that computes a projective transformation of the board onto a regular
grid. Using two convolutional neural networks, it then predicts an occupancy
mask for the squares in the warped image and finally classifies the pieces. The
described system achieves an error rate of 0.23% per square on the test set, 28
times better than the current state of the art. Further, a few-shot transfer
learning approach is developed that is able to adapt the inference system to a
previously unseen chess set using just two photos of the starting position,
obtaining a per-square accuracy of 99.83% on images of that new chess set. The
code, dataset, and trained models are made available online.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wolflein_G/0/1/0/all/0/1"&gt;Georg W&amp;#xf6;lflein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arandjelovic_O/0/1/0/all/0/1"&gt;Ognjen Arandjelovi&amp;#x107;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Human-centric Spatio-Temporal Video Grounding With Visual Transformers. (arXiv:2011.05049v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.05049</id>
        <link href="http://arxiv.org/abs/2011.05049"/>
        <updated>2021-06-03T02:10:33.599Z</updated>
        <summary type="html"><![CDATA[In this work, we introduce a novel task - Humancentric Spatio-Temporal Video
Grounding (HC-STVG). Unlike the existing referring expression tasks in images
or videos, by focusing on humans, HC-STVG aims to localize a spatiotemporal
tube of the target person from an untrimmed video based on a given textural
description. This task is useful, especially for healthcare and
security-related applications, where the surveillance videos can be extremely
long but only a specific person during a specific period of time is concerned.
HC-STVG is a video grounding task that requires both spatial (where) and
temporal (when) localization. Unfortunately, the existing grounding methods
cannot handle this task well. We tackle this task by proposing an effective
baseline method named Spatio-Temporal Grounding with Visual Transformers
(STGVT), which utilizes Visual Transformers to extract cross-modal
representations for video-sentence matching and temporal localization. To
facilitate this task, we also contribute an HC-STVG dataset consisting of 5,660
video-sentence pairs on complex multi-person scenes. Specifically, each video
lasts for 20 seconds, pairing with a natural query sentence with an average of
17.25 words. Extensive experiments are conducted on this dataset, demonstrating
the newly-proposed method outperforms the existing baseline methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1"&gt;Zongheng Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liao_Y/0/1/0/all/0/1"&gt;Yue Liao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Si Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1"&gt;Guanbin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1"&gt;Xiaojie Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1"&gt;Hongxu Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1"&gt;Qian Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1"&gt;Dong Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Online Coreset Selection for Rehearsal-based Continual Learning. (arXiv:2106.01085v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.01085</id>
        <link href="http://arxiv.org/abs/2106.01085"/>
        <updated>2021-06-03T02:10:33.575Z</updated>
        <summary type="html"><![CDATA[A dataset is a shred of crucial evidence to describe a task. However, each
data point in the dataset does not have the same potential, as some of the data
points can be more representative or informative than others. This unequal
importance among the data points may have a large impact in rehearsal-based
continual learning, where we store a subset of the training examples (coreset)
to be replayed later to alleviate catastrophic forgetting. In continual
learning, the quality of the samples stored in the coreset directly affects the
model's effectiveness and efficiency. The coreset selection problem becomes
even more important under realistic settings, such as imbalanced continual
learning or noisy data scenarios. To tackle this problem, we propose Online
Coreset Selection (OCS), a simple yet effective method that selects the most
representative and informative coreset at each iteration and trains them in an
online manner. Our proposed method maximizes the model's adaptation to a target
dataset while selecting high-affinity samples to past tasks, which directly
inhibits catastrophic forgetting. We validate the effectiveness of our coreset
selection mechanism over various standard, imbalanced, and noisy datasets
against strong continual learning baselines, demonstrating that it improves
task adaptation and prevents catastrophic forgetting in a sample-efficient
manner.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yoon_J/0/1/0/all/0/1"&gt;Jaehong Yoon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Madaan_D/0/1/0/all/0/1"&gt;Divyam Madaan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_E/0/1/0/all/0/1"&gt;Eunho Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1"&gt;Sung Ju Hwang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TSI: Temporal Saliency Integration for Video Action Recognition. (arXiv:2106.01088v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.01088</id>
        <link href="http://arxiv.org/abs/2106.01088"/>
        <updated>2021-06-03T02:10:33.566Z</updated>
        <summary type="html"><![CDATA[Efficient spatiotemporal modeling is an important yet challenging problem for
video action recognition. Existing state-of-the-art methods exploit motion
clues to assist in short-term temporal modeling through temporal difference
over consecutive frames. However, background noises will be inevitably
introduced due to the camera movement. Besides, movements of different actions
can vary greatly. In this paper, we propose a Temporal Saliency Integration
(TSI) block, which mainly contains a Salient Motion Excitation (SME) module and
a Cross-scale Temporal Integration (CTI) module. Specifically, SME aims to
highlight the motion-sensitive area through local-global motion modeling, where
the background suppression and pyramidal feature difference are conducted
successively between neighboring frames to capture motion dynamics with less
background noises. CTI is designed to perform multi-scale temporal modeling
through a group of separate 1D convolutions respectively. Meanwhile, temporal
interactions across different scales are integrated with attention mechanism.
Through these two modules, long short-term temporal relationships can be
encoded efficiently by introducing limited additional parameters. Extensive
experiments are conducted on several popular benchmarks (i.e.,
Something-Something v1 & v2, Kinetics-400, UCF-101, and HMDB-51), which
demonstrate the effectiveness and superiority of our proposed method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1"&gt;Haisheng Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1"&gt;Jinyuan Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1"&gt;Dongliang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gan_W/0/1/0/all/0/1"&gt;Weihao Gan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1"&gt;Wei Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1"&gt;Yu Qiao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Learning based Full-reference and No-reference Quality Assessment Models for Compressed UGC Videos. (arXiv:2106.01111v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.01111</id>
        <link href="http://arxiv.org/abs/2106.01111"/>
        <updated>2021-06-03T02:10:33.559Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a deep learning based video quality assessment
(VQA) framework to evaluate the quality of the compressed user's generated
content (UGC) videos. The proposed VQA framework consists of three modules, the
feature extraction module, the quality regression module, and the quality
pooling module. For the feature extraction module, we fuse the features from
intermediate layers of the convolutional neural network (CNN) network into
final quality-aware feature representation, which enables the model to make
full use of visual information from low-level to high-level. Specifically, the
structure and texture similarities of feature maps extracted from all
intermediate layers are calculated as the feature representation for the full
reference (FR) VQA model, and the global mean and standard deviation of the
final feature maps fused by intermediate feature maps are calculated as the
feature representation for the no reference (NR) VQA model. For the quality
regression module, we use the fully connected (FC) layer to regress the
quality-aware features into frame-level scores. Finally, a
subjectively-inspired temporal pooling strategy is adopted to pool frame-level
scores into the video-level score. The proposed model achieves the best
performance among the state-of-the-art FR and NR VQA models on the Compressed
UGC VQA database and also achieves pretty good performance on the in-the-wild
UGC VQA databases.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Sun_W/0/1/0/all/0/1"&gt;Wei Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_T/0/1/0/all/0/1"&gt;Tao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Min_X/0/1/0/all/0/1"&gt;Xiongkuo Min&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yi_F/0/1/0/all/0/1"&gt;Fuwang Yi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhai_G/0/1/0/all/0/1"&gt;Guangtao Zhai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Unified Surgical Skill Assessment. (arXiv:2106.01035v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.01035</id>
        <link href="http://arxiv.org/abs/2106.01035"/>
        <updated>2021-06-03T02:10:33.550Z</updated>
        <summary type="html"><![CDATA[Surgical skills have a great influence on surgical safety and patients'
well-being. Traditional assessment of surgical skills involves strenuous manual
efforts, which lacks efficiency and repeatability. Therefore, we attempt to
automatically predict how well the surgery is performed using the surgical
video. In this paper, a unified multi-path framework for automatic surgical
skill assessment is proposed, which takes care of multiple composing aspects of
surgical skills, including surgical tool usage, intraoperative event pattern,
and other skill proxies. The dependency relationships among these different
aspects are specially modeled by a path dependency module in the framework. We
conduct extensive experiments on the JIGSAWS dataset of simulated surgical
tasks, and a new clinical dataset of real laparoscopic surgeries. The proposed
framework achieves promising results on both datasets, with the
state-of-the-art on the simulated dataset advanced from 0.71 Spearman's
correlation to 0.80. It is also shown that combining multiple skill aspects
yields better performance than relying on a single aspect.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1"&gt;Daochang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1"&gt;Qiyue Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1"&gt;Tingting Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yizhou Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Miao_R/0/1/0/all/0/1"&gt;Rulin Miao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shan_F/0/1/0/all/0/1"&gt;Fei Shan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Ziyu Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[End-to-End Information Extraction by Character-Level Embedding and Multi-Stage Attentional U-Net. (arXiv:2106.00952v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.00952</id>
        <link href="http://arxiv.org/abs/2106.00952"/>
        <updated>2021-06-03T02:10:33.545Z</updated>
        <summary type="html"><![CDATA[Information extraction from document images has received a lot of attention
recently, due to the need for digitizing a large volume of unstructured
documents such as invoices, receipts, bank transfers, etc. In this paper, we
propose a novel deep learning architecture for end-to-end information
extraction on the 2D character-grid embedding of the document, namely the
\textit{Multi-Stage Attentional U-Net}. To effectively capture the textual and
spatial relations between 2D elements, our model leverages a specialized
multi-stage encoder-decoders design, in conjunction with efficient uses of the
self-attention mechanism and the box convolution. Experimental results on
different datasets show that our model outperforms the baseline U-Net
architecture by a large margin while using 40\% fewer parameters. Moreover, it
also significantly improved the baseline in erroneous OCR and limited training
data scenario, thus becomes practical for real-world applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dang_T/0/1/0/all/0/1"&gt;Tuan-Anh Nguyen Dang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1"&gt;Dat-Thanh Nguyen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-supervised Lesion Change Detection and Localisation in Longitudinal Multiple Sclerosis Brain Imaging. (arXiv:2106.00919v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.00919</id>
        <link href="http://arxiv.org/abs/2106.00919"/>
        <updated>2021-06-03T02:10:33.486Z</updated>
        <summary type="html"><![CDATA[Longitudinal imaging forms an essential component in the management and
follow-up of many medical conditions. The presence of lesion changes on serial
imaging can have significant impact on clinical decision making, highlighting
the important role for automated change detection. Lesion changes can represent
anomalies in serial imaging, which implies a limited availability of
annotations and a wide variety of possible changes that need to be considered.
Hence, we introduce a new unsupervised anomaly detection and localisation
method trained exclusively with serial images that do not contain any lesion
changes. Our training automatically synthesises lesion changes in serial
images, introducing detection and localisation pseudo-labels that are used to
self-supervise the training of our model. Given the rarity of these lesion
changes in the synthesised images, we train the model with the imbalance robust
focal Tversky loss. When compared to supervised models trained on different
datasets, our method shows competitive performance in the detection and
localisation of new demyelinating lesions on longitudinal magnetic resonance
imaging in multiple sclerosis patients. Code for the models will be made
available on GitHub.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+To_M/0/1/0/all/0/1"&gt;Minh-Son To&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sarno_I/0/1/0/all/0/1"&gt;Ian G Sarno&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chong_C/0/1/0/all/0/1"&gt;Chee Chong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jenkinson_M/0/1/0/all/0/1"&gt;Mark Jenkinson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Carneiro_G/0/1/0/all/0/1"&gt;Gustavo Carneiro&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Prediction of the Position of External Markers Using a Recurrent Neural Network Trained With Unbiased Online Recurrent Optimization for Safe Lung Cancer Radiotherapy. (arXiv:2106.01100v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.01100</id>
        <link href="http://arxiv.org/abs/2106.01100"/>
        <updated>2021-06-03T02:10:33.480Z</updated>
        <summary type="html"><![CDATA[During lung cancer radiotherapy, the position of infrared reflective objects
on the chest can be recorded to estimate the tumor location. However,
radiotherapy systems usually have a latency inherent to robot control
limitations that impedes the radiation delivery precision. Not taking this
phenomenon into account may cause unwanted damage to healthy tissues and lead
to side effects such as radiation pneumonitis. In this research, we use nine
observation records of the three-dimensional position of three external markers
on the chest and abdomen of healthy individuals breathing during intervals from
73s to 222s. The sampling frequency is equal to 10Hz and the amplitudes of the
recorded trajectories range from 6mm to 40mm in the superior-inferior
direction. We forecast the location of each marker simultaneously with a
horizon value (the time interval in advance for which the prediction is made)
between 0.1s and 2.0s, using a recurrent neural network (RNN) trained with
unbiased online recurrent optimization (UORO). We compare its performance with
an RNN trained with real-time recurrent learning, least mean squares (LMS), and
offline linear regression. Training and cross-validation are performed during
the first minute of each sequence. On average, UORO achieves the lowest
root-mean-square (RMS) and maximum error, equal respectively to 1.3mm and
8.8mm, with a prediction time per time step lower than 2.8ms (Dell Intel core
i9-9900K 3.60Ghz). Linear regression has the lowest RMS error for the horizon
values 0.1s and 0.2s, followed by LMS for horizon values between 0.3s and 0.5s,
and UORO for horizon values greater than 0.6s.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Pohl_M/0/1/0/all/0/1"&gt;Michel Pohl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Uesaka_M/0/1/0/all/0/1"&gt;Mitsuru Uesaka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Takahashi_H/0/1/0/all/0/1"&gt;Hiroyuki Takahashi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Demachi_K/0/1/0/all/0/1"&gt;Kazuyuki Demachi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chhatkuli_R/0/1/0/all/0/1"&gt;Ritu Bhusal Chhatkuli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Robust Classification Model by Counterfactual and Invariant Data Generation. (arXiv:2106.01127v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.01127</id>
        <link href="http://arxiv.org/abs/2106.01127"/>
        <updated>2021-06-03T02:10:33.440Z</updated>
        <summary type="html"><![CDATA[Despite the success of machine learning applications in science, industry,
and society in general, many approaches are known to be non-robust, often
relying on spurious correlations to make predictions. Spuriousness occurs when
some features correlate with labels but are not causal; relying on such
features prevents models from generalizing to unseen environments where such
correlations break. In this work, we focus on image classification and propose
two data generation processes to reduce spuriousness. Given human annotations
of the subset of the features responsible (causal) for the labels (e.g.
bounding boxes), we modify this causal set to generate a surrogate image that
no longer has the same label (i.e. a counterfactual image). We also alter
non-causal features to generate images still recognized as the original labels,
which helps to learn a model invariant to these features. In several
challenging datasets, our data generations outperform state-of-the-art methods
in accuracy when spurious correlations break, and increase the saliency focus
on causal features providing better explanations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chang_C/0/1/0/all/0/1"&gt;Chun-Hao Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Adam_G/0/1/0/all/0/1"&gt;George Alexandru Adam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldenberg_A/0/1/0/all/0/1"&gt;Anna Goldenberg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Novel Edge Detection Operator for Identifying Buildings in Augmented Reality Applications. (arXiv:2106.01055v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.01055</id>
        <link href="http://arxiv.org/abs/2106.01055"/>
        <updated>2021-06-03T02:10:33.434Z</updated>
        <summary type="html"><![CDATA[Augmented Reality is an environment-enhancing technology, widely applied in
many domains, such as tourism and culture. One of the major challenges in this
field is precise detection and extraction of building information through
Computer Vision techniques. Edge detection is one of the building blocks
operations for many feature extraction solutions in Computer Vision. AR systems
use edge detection for building extraction or for extraction of facade details
from buildings. In this paper, we propose a novel filter operator for edge
detection that aims to extract building contours or facade features better. The
proposed filter gives more weight for finding vertical and horizontal edges
that is an important feature for our aim.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Orhei_C/0/1/0/all/0/1"&gt;Ciprian Orhei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vert_S/0/1/0/all/0/1"&gt;Silviu Vert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vasiu_R/0/1/0/all/0/1"&gt;Radu Vasiu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Feedback Network for Mutually Boosted Stereo Image Super-Resolution and Disparity Estimation. (arXiv:2106.00985v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.00985</id>
        <link href="http://arxiv.org/abs/2106.00985"/>
        <updated>2021-06-03T02:10:33.402Z</updated>
        <summary type="html"><![CDATA[Under stereo settings, the problem of image super-resolution (SR) and
disparity estimation are interrelated that the result of each problem could
help to solve the other. The effective exploitation of correspondence between
different views facilitates the SR performance, while the high-resolution (HR)
features with richer details benefit the correspondence estimation. According
to this motivation, we propose a Stereo Super-Resolution and Disparity
Estimation Feedback Network (SSRDE-FNet), which simultaneously handles the
stereo image super-resolution and disparity estimation in a unified framework
and interact them with each other to further improve their performance.
Specifically, the SSRDE-FNet is composed of two dual recursive sub-networks for
left and right views. Besides the cross-view information exploitation in the
low-resolution (LR) space, HR representations produced by the SR process are
utilized to perform HR disparity estimation with higher accuracy, through which
the HR features can be aggregated to generate a finer SR result. Afterward, the
proposed HR Disparity Information Feedback (HRDIF) mechanism delivers
information carried by HR disparity back to previous layers to further refine
the SR image reconstruction. Extensive experiments demonstrate the
effectiveness and advancement of SSRDE-FNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dai_Q/0/1/0/all/0/1"&gt;Qinyan Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Juncheng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yi_Q/0/1/0/all/0/1"&gt;Qiaosi Yi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fang_F/0/1/0/all/0/1"&gt;Faming Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1"&gt;Guixu Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rethinking Cross-modal Interaction from a Top-down Perspective for Referring Video Object Segmentation. (arXiv:2106.01061v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.01061</id>
        <link href="http://arxiv.org/abs/2106.01061"/>
        <updated>2021-06-03T02:10:33.396Z</updated>
        <summary type="html"><![CDATA[Referring video object segmentation (RVOS) aims to segment video objects with
the guidance of natural language reference. Previous methods typically tackle
RVOS through directly grounding linguistic reference over the image lattice.
Such bottom-up strategy fails to explore object-level cues, easily leading to
inferior results. In this work, we instead put forward a two-stage, top-down
RVOS solution. First, an exhaustive set of object tracklets is constructed by
propagating object masks detected from several sampled frames to the entire
video. Second, a Transformer-based tracklet-language grounding module is
proposed, which models instance-level visual relations and cross-modal
interactions simultaneously and efficiently. Our model ranks first place on
CVPR2021 Referring Youtube-VOS challenge.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liang_C/0/1/0/all/0/1"&gt;Chen Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yu Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1"&gt;Tianfei Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wenguan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Zongxin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1"&gt;Yunchao Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yi Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tips and Tricks to Improve CNN-based Chest X-ray Diagnosis: A Survey. (arXiv:2106.00997v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.00997</id>
        <link href="http://arxiv.org/abs/2106.00997"/>
        <updated>2021-06-03T02:10:33.388Z</updated>
        <summary type="html"><![CDATA[Convolutional Neural Networks (CNNs) intrinsically requires large-scale data
whereas Chest X-Ray (CXR) images tend to be data/annotation-scarce, leading to
over-fitting. Therefore, based on our development experience and related work,
this paper thoroughly introduces tricks to improve generalization in the CXR
diagnosis: how to (i) leverage additional data, (ii) augment/distillate data,
(iii) regularize training, and (iv) conduct efficient segmentation. As a
development example based on such optimization techniques, we also feature
LPIXEL's CNN-based CXR solution, EIRL Chest Nodule, which improved
radiologists/non-radiologists' nodule detection sensitivity by 0.100/0.131,
respectively, while maintaining specificity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Han_C/0/1/0/all/0/1"&gt;Changhee Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Okamoto_T/0/1/0/all/0/1"&gt;Takayuki Okamoto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Takeuchi_K/0/1/0/all/0/1"&gt;Koichi Takeuchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Katsios_D/0/1/0/all/0/1"&gt;Dimitris Katsios&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Grushnikov_A/0/1/0/all/0/1"&gt;Andrey Grushnikov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kobayashi_M/0/1/0/all/0/1"&gt;Masaaki Kobayashi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Choppin_A/0/1/0/all/0/1"&gt;Antoine Choppin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kurashina_Y/0/1/0/all/0/1"&gt;Yutaka Kurashina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shimahara_Y/0/1/0/all/0/1"&gt;Yuki Shimahara&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-task fully convolutional network for tree species mapping in dense forests using small training hyperspectral data. (arXiv:2106.00799v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.00799</id>
        <link href="http://arxiv.org/abs/2106.00799"/>
        <updated>2021-06-03T02:10:33.301Z</updated>
        <summary type="html"><![CDATA[This work proposes a multi-task fully convolutional architecture for tree
species mapping in dense forests from sparse and scarce polygon-level
annotations using hyperspectral UAV-borne data. Our model implements a partial
loss function that enables dense tree semantic labeling outcomes from non-dense
training samples, and a distance regression complementary task that enforces
tree crown boundary constraints and substantially improves the model
performance. Our multi-task architecture uses a shared backbone network that
learns common representations for both tasks and two task-specific decoders,
one for the semantic segmentation output and one for the distance map
regression. We report that introducing the complementary task boosts the
semantic segmentation performance compared to the single-task counterpart in up
to 10% reaching an overall F1 score of 87.5% and an overall accuracy of 85.9%,
achieving state-of-art performance for tree species classification in tropical
forests.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rosa_L/0/1/0/all/0/1"&gt;Laura Elena Cu&amp;#xe9; La Rosa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sothe_C/0/1/0/all/0/1"&gt;Camile Sothe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feitosa_R/0/1/0/all/0/1"&gt;Raul Queiroz Feitosa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Almeida_C/0/1/0/all/0/1"&gt;Cl&amp;#xe1;udia Maria de Almeida&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schimalski_M/0/1/0/all/0/1"&gt;Marcos Benedito Schimalski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oliveira_D/0/1/0/all/0/1"&gt;Dario Augusto Borges Oliveira&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ICDAR 2021 Competition on On-Line Signature Verification. (arXiv:2106.00739v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.00739</id>
        <link href="http://arxiv.org/abs/2106.00739"/>
        <updated>2021-06-03T02:10:33.296Z</updated>
        <summary type="html"><![CDATA[This paper describes the experimental framework and results of the ICDAR 2021
Competition on On-Line Signature Verification (SVC 2021). The goal of SVC 2021
is to evaluate the limits of on-line signature verification systems on popular
scenarios (office/mobile) and writing inputs (stylus/finger) through
large-scale public databases. Three different tasks are considered in the
competition, simulating realistic scenarios as both random and skilled
forgeries are simultaneously considered on each task. The results obtained in
SVC 2021 prove the high potential of deep learning methods. In particular, the
best on-line signature verification system of SVC 2021 obtained Equal Error
Rate (EER) values of 3.33% (Task 1), 7.41% (Task 2), and 6.04% (Task 3).

SVC 2021 will be established as an on-going competition, where researchers
can easily benchmark their systems against the state of the art in an open
common platform using large-scale public databases such as DeepSignDB and
SVC2021_EvalDB, and standard experimental protocols.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tolosana_R/0/1/0/all/0/1"&gt;Ruben Tolosana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vera_Rodriguez_R/0/1/0/all/0/1"&gt;Ruben Vera-Rodriguez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gonzalez_Garcia_C/0/1/0/all/0/1"&gt;Carlos Gonzalez-Garcia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fierrez_J/0/1/0/all/0/1"&gt;Julian Fierrez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rengifo_S/0/1/0/all/0/1"&gt;Santiago Rengifo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morales_A/0/1/0/all/0/1"&gt;Aythami Morales&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ortega_Garcia_J/0/1/0/all/0/1"&gt;Javier Ortega-Garcia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ruiz_Garcia_J/0/1/0/all/0/1"&gt;Juan Carlos Ruiz-Garcia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Romero_Tapiador_S/0/1/0/all/0/1"&gt;Sergio Romero-Tapiador&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1"&gt;Jiajia Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lai_S/0/1/0/all/0/1"&gt;Songxuan Lai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1"&gt;Lianwen Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yecheng Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Galbally_J/0/1/0/all/0/1"&gt;Javier Galbally&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Diaz_M/0/1/0/all/0/1"&gt;Moises Diaz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ferrer_M/0/1/0/all/0/1"&gt;Miguel Angel Ferrer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gomez_Barrero_M/0/1/0/all/0/1"&gt;Marta Gomez-Barrero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hodashinsky_I/0/1/0/all/0/1"&gt;Ilya Hodashinsky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sarin_K/0/1/0/all/0/1"&gt;Konstantin Sarin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Slezkin_A/0/1/0/all/0/1"&gt;Artem Slezkin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bardamova_M/0/1/0/all/0/1"&gt;Marina Bardamova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Svetlakov_M/0/1/0/all/0/1"&gt;Mikhail Svetlakov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saleem_M/0/1/0/all/0/1"&gt;Mohammad Saleem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Szucs_C/0/1/0/all/0/1"&gt;Cintia Lia Sz&amp;#xfc;cs&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kovari_B/0/1/0/all/0/1"&gt;Bence Kovari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pulsmeyer_F/0/1/0/all/0/1"&gt;Falk Pulsmeyer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wehbi_M/0/1/0/all/0/1"&gt;Mohamad Wehbi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zanca_D/0/1/0/all/0/1"&gt;Dario Zanca&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ahmad_S/0/1/0/all/0/1"&gt;Sumaiya Ahmad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1"&gt;Sarthak Mishra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jabin_S/0/1/0/all/0/1"&gt;Suraiya Jabin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PairRank: Online Pairwise Learning to Rank by Divide-and-Conquer. (arXiv:2103.00368v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.00368</id>
        <link href="http://arxiv.org/abs/2103.00368"/>
        <updated>2021-06-03T02:10:33.258Z</updated>
        <summary type="html"><![CDATA[Online Learning to Rank (OL2R) eliminates the need of explicit relevance
annotation by directly optimizing the rankers from their interactions with
users. However, the required exploration drives it away from successful
practices in offline learning to rank, which limits OL2R's empirical
performance and practical applicability. In this work, we propose to estimate a
pairwise learning to rank model online. In each round, candidate documents are
partitioned and ranked according to the model's confidence on the estimated
pairwise rank order, and exploration is only performed on the uncertain pairs
of documents, i.e., \emph{divide-and-conquer}. Regret directly defined on the
number of mis-ordered pairs is proven, which connects the online solution's
theoretical convergence with its expected ranking performance. Comparisons
against an extensive list of OL2R baselines on two public learning to rank
benchmark datasets demonstrate the effectiveness of the proposed solution.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1"&gt;Yiling Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Huazheng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1"&gt;Stephen Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hongning Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fourier Space Losses for Efficient Perceptual Image Super-Resolution. (arXiv:2106.00783v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.00783</id>
        <link href="http://arxiv.org/abs/2106.00783"/>
        <updated>2021-06-03T02:10:33.205Z</updated>
        <summary type="html"><![CDATA[Many super-resolution (SR) models are optimized for high performance only and
therefore lack efficiency due to large model complexity. As large models are
often not practical in real-world applications, we investigate and propose
novel loss functions, to enable SR with high perceptual quality from much more
efficient models. The representative power for a given low-complexity generator
network can only be fully leveraged by strong guidance towards the optimal set
of parameters. We show that it is possible to improve the performance of a
recently introduced efficient generator architecture solely with the
application of our proposed loss functions. In particular, we use a Fourier
space supervision loss for improved restoration of missing high-frequency (HF)
content from the ground truth image and design a discriminator architecture
working directly in the Fourier domain to better match the target HF
distribution. We show that our losses' direct emphasis on the frequencies in
Fourier-space significantly boosts the perceptual image quality, while at the
same time retaining high restoration quality in comparison to previously
proposed loss functions for this task. The performance is further improved by
utilizing a combination of spatial and frequency domain losses, as both
representations provide complementary information during training. On top of
that, the trained generator achieves comparable results with and is 2.4x and
48x faster than state-of-the-art perceptual SR methods RankSRGAN and SRFlow
respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Fuoli_D/0/1/0/all/0/1"&gt;Dario Fuoli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gool_L/0/1/0/all/0/1"&gt;Luc Van Gool&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Timofte_R/0/1/0/all/0/1"&gt;Radu Timofte&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[nnDetection: A Self-configuring Method for Medical Object Detection. (arXiv:2106.00817v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.00817</id>
        <link href="http://arxiv.org/abs/2106.00817"/>
        <updated>2021-06-03T02:10:33.198Z</updated>
        <summary type="html"><![CDATA[Simultaneous localisation and categorization of objects in medical images,
also referred to as medical object detection, is of high clinical relevance
because diagnostic decisions often depend on rating of objects rather than e.g.
pixels. For this task, the cumbersome and iterative process of method
configuration constitutes a major research bottleneck. Recently, nnU-Net has
tackled this challenge for the task of image segmentation with great success.
Following nnU-Net's agenda, in this work we systematize and automate the
configuration process for medical object detection. The resulting
self-configuring method, nnDetection, adapts itself without any manual
intervention to arbitrary medical detection problems while achieving results en
par with or superior to the state-of-the-art. We demonstrate the effectiveness
of nnDetection on two public benchmarks, ADAM and LUNA16, and propose 10
further medical object detection tasks on public data sets for comprehensive
method evaluation. Code is at https://github.com/MIC-DKFZ/nnDetection .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Baumgartner_M/0/1/0/all/0/1"&gt;Michael Baumgartner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jaeger_P/0/1/0/all/0/1"&gt;Paul F. Jaeger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Isensee_F/0/1/0/all/0/1"&gt;Fabian Isensee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maier_Hein_K/0/1/0/all/0/1"&gt;Klaus H. Maier-Hein&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TransMIL: Transformer based Correlated Multiple Instance Learning for Whole Slide Image Classication. (arXiv:2106.00908v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.00908</id>
        <link href="http://arxiv.org/abs/2106.00908"/>
        <updated>2021-06-03T02:10:33.189Z</updated>
        <summary type="html"><![CDATA[Multiple instance learning (MIL) is a powerful tool to solve the weakly
supervised classification in whole slide image (WSI) based pathology diagnosis.
However, the current MIL methods are usually based on independent and identical
distribution hypothesis, thus neglect the correlation among different
instances. To address this problem, we proposed a new framework, called
correlated MIL, and provided a proof for convergence. Based on this framework,
we devised a Transformer based MIL (TransMIL), which explored both
morphological and spatial information. The proposed TransMIL can effectively
deal with unbalanced/balanced and binary/multiple classification with great
visualization and interpretability. We conducted various experiments for three
different computational pathology problems and achieved better performance and
faster convergence compared with state-of-the-art methods. The test AUC for the
binary tumor classification can be up to 93.09% over CAMELYON16 dataset. And
the AUC over the cancer subtypes classification can be up to 96.03% and 98.82%
over TCGA-NSCLC dataset and TCGA-RCC dataset, respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shao_Z/0/1/0/all/0/1"&gt;Zhuchen Shao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bian_H/0/1/0/all/0/1"&gt;Hao Bian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yifeng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jian Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_X/0/1/0/all/0/1"&gt;Xiangyang Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yongbing Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cleaning and Structuring the Label Space of the iMet Collection 2020. (arXiv:2106.00815v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.00815</id>
        <link href="http://arxiv.org/abs/2106.00815"/>
        <updated>2021-06-03T02:10:33.166Z</updated>
        <summary type="html"><![CDATA[The iMet 2020 dataset is a valuable resource in the space of fine-grained art
attribution recognition, but we believe it has yet to reach its true potential.
We document the unique properties of the dataset and observe that many of the
attribute labels are noisy, more than is implied by the dataset description.
Oftentimes, there are also semantic relationships between the labels (e.g.,
identical, mutual exclusion, subsumption, overlap with uncertainty) which we
believe are underutilized. We propose an approach to cleaning and structuring
the iMet 2020 labels, and discuss the implications and value of doing so.
Further, we demonstrate the benefits of our proposed approach through several
experiments. Our code and cleaned labels are available at
https://github.com/sunniesuhyoung/iMet2020cleaned.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_V/0/1/0/all/0/1"&gt;Vivien Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1"&gt;Sunnie S. Y. Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automated Timeline Length Selection for Flexible Timeline Summarization. (arXiv:2105.14201v1 [cs.AI] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2105.14201</id>
        <link href="http://arxiv.org/abs/2105.14201"/>
        <updated>2021-06-03T02:10:33.063Z</updated>
        <summary type="html"><![CDATA[By producing summaries for long-running events, timeline summarization (TLS)
underpins many information retrieval tasks. Successful TLS requires identifying
an appropriate set of key dates (the timeline length) to cover. However, doing
so is challenging as the right length can change from one topic to another.
Existing TLS solutions either rely on an event-agnostic fixed length or an
expert-supplied setting. Neither of the strategies is desired for real-life TLS
scenarios. A fixed, event-agnostic setting ignores the diversity of events and
their development and hence can lead to low-quality TLS. Relying on
expert-crafted settings is neither scalable nor sustainable for processing many
dynamically changing events. This paper presents a better TLS approach for
automatically and dynamically determining the TLS timeline length. We achieve
this by employing the established elbow method from the machine learning
community to automatically find the minimum number of dates within the time
series to generate concise and informative summaries. We applied our approach
to four TLS datasets of English and Chinese and compared them against three
prior methods. Experimental results show that our approach delivers comparable
or even better summaries over state-of-art TLS methods, but it achieves this
without expert involvement.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mao_Q/0/1/0/all/0/1"&gt;Qianren Mao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1"&gt;Hao Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1"&gt;Hongdong Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jianxin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zheng Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Few-Shot Text Ranking with Meta Adapted Synthetic Weak Supervision. (arXiv:2012.14862v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.14862</id>
        <link href="http://arxiv.org/abs/2012.14862"/>
        <updated>2021-06-03T02:10:33.055Z</updated>
        <summary type="html"><![CDATA[The effectiveness of Neural Information Retrieval (Neu-IR) often depends on a
large scale of in-domain relevance training signals, which are not always
available in real-world ranking scenarios. To democratize the benefits of
Neu-IR, this paper presents MetaAdaptRank, a domain adaptive learning method
that generalizes Neu-IR models from label-rich source domains to few-shot
target domains. Drawing on source-domain massive relevance supervision,
MetaAdaptRank contrastively synthesizes a large number of weak supervision
signals for target domains and meta-learns to reweight these synthetic "weak"
data based on their benefits to the target-domain ranking accuracy of Neu-IR
models. Experiments on three TREC benchmarks in the web, news, and biomedical
domains show that MetaAdaptRank significantly improves the few-shot ranking
accuracy of Neu-IR models. Further analyses indicate that MetaAdaptRank thrives
from both its contrastive weak data synthesis and meta-reweighted data
selection. The code and data of this paper can be obtained from
https://github.com/thunlp/MetaAdaptRank.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1"&gt;Si Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1"&gt;Yingzhuo Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhenghao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1"&gt;Chenyan Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1"&gt;Kaitao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1"&gt;Jie Bao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhiyuan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bennett_P/0/1/0/all/0/1"&gt;Paul Bennett&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Refining the bounding volumes for lossless compression of voxelized point clouds geometry. (arXiv:2106.00828v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.00828</id>
        <link href="http://arxiv.org/abs/2106.00828"/>
        <updated>2021-06-03T02:10:33.036Z</updated>
        <summary type="html"><![CDATA[This paper describes a novel lossless compression method for point cloud
geometry, building on a recent lossy compression method that aimed at
reconstructing only the bounding volume of a point cloud. The proposed scheme
starts by partially reconstructing the geometry from the two depthmaps
associated to a single projection direction. The partial reconstruction
obtained from the depthmaps is completed to a full reconstruction of the point
cloud by sweeping section by section along one direction and encoding the
points which were not contained in the two depthmaps. The main ingredient is a
list-based encoding of the inner points (situated inside the feasible regions)
by a novel arithmetic three dimensional context coding procedure that
efficiently utilizes rotational invariances present in the input data.
State-of-the-art bits-per-voxel results are obtained on benchmark datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kaya_E/0/1/0/all/0/1"&gt;Emre Can Kaya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schwarz_S/0/1/0/all/0/1"&gt;Sebastian Schwarz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tabus_I/0/1/0/all/0/1"&gt;Ioan Tabus&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluating Recipes Generated from Functional Object-Oriented Network. (arXiv:2106.00728v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2106.00728</id>
        <link href="http://arxiv.org/abs/2106.00728"/>
        <updated>2021-06-03T02:10:33.016Z</updated>
        <summary type="html"><![CDATA[The functional object-oriented network (FOON) has been introduced as a
knowledge representation, which takes the form of a graph, for symbolic task
planning. To get a sequential plan for a manipulation task, a robot can obtain
a task tree through a knowledge retrieval process from the FOON. To evaluate
the quality of an acquired task tree, we compare it with a conventional form of
task knowledge, such as recipes or manuals. We first automatically convert task
trees to recipes, and we then compare them with the human-created recipes in
the Recipe1M+ dataset via a survey. Our preliminary study finds no significant
difference between the recipes in Recipe1M+ and the recipes generated from FOON
task trees in terms of correctness, completeness, and clarity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sakib_M/0/1/0/all/0/1"&gt;Md Sadman Sakib&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baez_H/0/1/0/all/0/1"&gt;Hailey Baez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paulius_D/0/1/0/all/0/1"&gt;David Paulius&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1"&gt;Yu Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rotation Equivariant Feature Image Pyramid Network for Object Detection in Optical Remote Sensing Imagery. (arXiv:2106.00880v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.00880</id>
        <link href="http://arxiv.org/abs/2106.00880"/>
        <updated>2021-06-03T02:10:32.995Z</updated>
        <summary type="html"><![CDATA[Over the last few years, there has been substantial progress in object
detection on remote sensing images (RSIs) where objects are generally
distributed with large-scale variations and have different types of
orientations. Nevertheless, most of the current convolution neural network
approaches lack the ability to deal with the challenges such as size and
rotation variations. To address these problems, we propose the rotation
equivariant feature image pyramid network (REFIPN), an image pyramid network
based on rotation equivariance convolution. The proposed pyramid network
extracts features in a wide range of scales and orientations by using novel
convolution filters. These features are used to generate vector fields and
determine the weight and angle of the highest-scoring orientation for all
spatial locations on an image. Finally, the extracted features go through the
prediction layers of the detector. The detection performance of the proposed
model is validated on two commonly used aerial benchmarks and the results show
our propose model can achieve state-of-the-art performance with satisfactory
efficiency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shamsolmoali_P/0/1/0/all/0/1"&gt;Pourya Shamsolmoali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zareapoor_M/0/1/0/all/0/1"&gt;Masoumeh Zareapoor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chanussot_J/0/1/0/all/0/1"&gt;Jocelyn Chanussot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1"&gt;Huiyu Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jie Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Consumer Image Quality Prediction using Recurrent Neural Networks for Spatial Pooling. (arXiv:2106.00918v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.00918</id>
        <link href="http://arxiv.org/abs/2106.00918"/>
        <updated>2021-06-03T02:10:32.987Z</updated>
        <summary type="html"><![CDATA[Promising results for subjective image quality prediction have been achieved
during the past few years by using convolutional neural networks (CNN).
However, the use of CNNs for high resolution image quality assessment remains a
challenge, since typical CNN architectures have been designed for small
resolution input images. In this study, we propose an image quality model that
attempts to mimic the attention mechanism of human visual system (HVS) by using
a recurrent neural network (RNN) for spatial pooling of the features extracted
from different spatial areas (patches) by a deep CNN-based feature extractor.
The experimental study, conducted by using images with different resolutions
from two recently published image quality datasets, indicates that the quality
prediction accuracy of the proposed method is competitive against benchmark
models representing the state-of-the-art, and the proposed method also performs
consistently on different resolution versions of the same dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Korhonen_J/0/1/0/all/0/1"&gt;Jari Korhonen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1"&gt;Yicheng Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+You_J/0/1/0/all/0/1"&gt;Junyong You&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Translational Symmetry-Aware Facade Parsing for 3D Building Reconstruction. (arXiv:2106.00912v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.00912</id>
        <link href="http://arxiv.org/abs/2106.00912"/>
        <updated>2021-06-03T02:10:32.968Z</updated>
        <summary type="html"><![CDATA[Effectively parsing the facade is essential to 3D building reconstruction,
which is an important computer vision problem with a large amount of
applications in high precision map for navigation, computer aided design, and
city generation for digital entertainments. To this end, the key is how to
obtain the shape grammars from 2D images accurately and efficiently. Although
enjoying the merits of promising results on the semantic parsing, deep learning
methods cannot directly make use of the architectural rules, which play an
important role for man-made structures. In this paper, we present a novel
translational symmetry-based approach to improving the deep neural networks.
Our method employs deep learning models as the base parser, and a module taking
advantage of translational symmetry is used to refine the initial parsing
results. In contrast to conventional semantic segmentation or bounding box
prediction, we propose a novel scheme to fuse segmentation with anchor-free
detection in a single stage network, which enables the efficient training and
better convergence. After parsing the facades into shape grammars, we employ an
off-the-shelf rendering engine like Blender to reconstruct the realistic
high-quality 3D models using procedural modeling. We conduct experiments on
three public datasets, where our proposed approach outperforms the
state-of-the-art methods. In addition, we have illustrated the 3D building
models built from 2D facade images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Hantang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1"&gt;Wentong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jianke Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multilingual Medical Question Answering and Information Retrieval for Rural Health Intelligence Access. (arXiv:2106.01251v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01251</id>
        <link href="http://arxiv.org/abs/2106.01251"/>
        <updated>2021-06-03T02:10:32.851Z</updated>
        <summary type="html"><![CDATA[In rural regions of several developing countries, access to quality
healthcare, medical infrastructure, and professional diagnosis is largely
unavailable. Many of these regions are gradually gaining access to internet
infrastructure, although not with a strong enough connection to allow for
sustained communication with a medical practitioner. Several deaths resulting
from this lack of medical access, absence of patient's previous health records,
and the unavailability of information in indigenous languages can be easily
prevented. In this paper, we describe an approach leveraging the phenomenal
progress in Machine Learning and NLP (Natural Language Processing) techniques
to design a model that is low-resource, multilingual, and a preliminary
first-point-of-contact medical assistant. Our contribution includes defining
the NLP pipeline required for named-entity-recognition, language-agnostic
sentence embedding, natural language translation, information retrieval,
question answering, and generative pre-training for final query processing. We
obtain promising results for this pipeline and preliminary results for EHR
(Electronic Health Record) analysis with text summarization for medical
practitioners to peruse for their diagnosis. Through this NLP pipeline, we aim
to provide preliminary medical information to the user and do not claim to
supplant diagnosis from qualified medical practitioners. Using the input from
subject matter experts, we have compiled a large corpus to pre-train and
fine-tune our BioBERT based NLP model for the specific tasks. We expect recent
advances in NLP architectures, several of which are efficient and
privacy-preserving models, to further the impact of our solution and improve on
individual task performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vinod_V/0/1/0/all/0/1"&gt;Vishal Vinod&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agrawal_S/0/1/0/all/0/1"&gt;Susmit Agrawal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gaurav_V/0/1/0/all/0/1"&gt;Vipul Gaurav&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+R_P/0/1/0/all/0/1"&gt;Pallavi R&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choudhary_S/0/1/0/all/0/1"&gt;Savita Choudhary&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sound-to-Imagination: Unsupervised Crossmodal Translation Using Deep Dense Network Architecture. (arXiv:2106.01266v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2106.01266</id>
        <link href="http://arxiv.org/abs/2106.01266"/>
        <updated>2021-06-03T02:10:32.822Z</updated>
        <summary type="html"><![CDATA[The motivation of our research is to develop a sound-to-image (S2I)
translation system for enabling a human receiver to visually infer the
occurrence of sound related events. We expect the computer to 'imagine' the
scene from the captured sound, generating original images that picture the
sound emitting source. Previous studies on similar topics opted for simplified
approaches using data with low content diversity and/or strong supervision.
Differently, we propose to perform unsupervised S2I translation using thousands
of distinct and unknown scenes, with slightly pre-cleaned data, just enough to
guarantee aural-visual semantic coherence. To that end, we employ conditional
generative adversarial networks (GANs) with a deep densely connected generator.
Besides, we implemented a moving-average adversarial loss to address GANs
training instability. Though the specified S2I translation problem is quite
challenging, we were able to generalize the translator model enough to obtain
more than 14%, in average, of interpretable and semantically coherent images
translated from unknown sounds. Additionally, we present a solution using
informativity classifiers to perform quantitative evaluation of S2I
translation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fanzeres_L/0/1/0/all/0/1"&gt;Leonardo A. Fanzeres&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nadeu_C/0/1/0/all/0/1"&gt;Climent Nadeu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PP-Rec: News Recommendation with Personalized User Interest and Time-aware News Popularity. (arXiv:2106.01300v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.01300</id>
        <link href="http://arxiv.org/abs/2106.01300"/>
        <updated>2021-06-03T02:10:32.808Z</updated>
        <summary type="html"><![CDATA[Personalized news recommendation methods are widely used in online news
services. These methods usually recommend news based on the matching between
news content and user interest inferred from historical behaviors. However,
these methods usually have difficulties in making accurate recommendations to
cold-start users, and tend to recommend similar news with those users have
read. In general, popular news usually contain important information and can
attract users with different interests. Besides, they are usually diverse in
content and topic. Thus, in this paper we propose to incorporate news
popularity information to alleviate the cold-start and diversity problems for
personalized news recommendation. In our method, the ranking score for
recommending a candidate news to a target user is the combination of a
personalized matching score and a news popularity score. The former is used to
capture the personalized user interest in news. The latter is used to measure
time-aware popularity of candidate news, which is predicted based on news
content, recency, and real-time CTR using a unified framework. Besides, we
propose a popularity-aware user encoder to eliminate the popularity bias in
user behaviors for accurate interest modeling. Experiments on two real-world
datasets show our method can effectively improve the accuracy and diversity for
news recommendation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qi_T/0/1/0/all/0/1"&gt;Tao Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1"&gt;Fangzhao Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1"&gt;Chuhan Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yongfeng Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A weighted unified informetrics based on Scopus and WoS. (arXiv:2106.01232v1 [cs.DL])]]></title>
        <id>http://arxiv.org/abs/2106.01232</id>
        <link href="http://arxiv.org/abs/2106.01232"/>
        <updated>2021-06-03T02:10:32.787Z</updated>
        <summary type="html"><![CDATA[Numerous indexing databases keep track of the number of publications,
citations, etc. in order to maintain the progress of science and individual.
However, the choice of journals and articles varies among these indexing
databases, hence the number of citations and h-index varies. There is no common
platform exists that can provide a single count for the number of publications,
citations, h-index, etc. To overcome this limitation, we have proposed a
weighted unified informetrics, named "conflate". The proposed system takes into
account the input from multiple indexing databases and generates a single
output. Here, we have used the data from Scopus and WoS to generate a conflate
dataset. Further, a comparative analysis of conflate has been performed with
Scopus and WoS at three levels: author, organization, and journal. Finally, a
mapping is proposed between research publications and distributed ledger
technology in order to provide a transparent and distributed view to its
stakeholders.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khurana_P/0/1/0/all/0/1"&gt;Parul Khurana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ganesan_G/0/1/0/all/0/1"&gt;Geetha Ganesan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_G/0/1/0/all/0/1"&gt;Gulshan Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_K/0/1/0/all/0/1"&gt;Kiran Sharma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Refining the bounding volumes for lossless compression of voxelized point clouds geometry. (arXiv:2106.00828v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.00828</id>
        <link href="http://arxiv.org/abs/2106.00828"/>
        <updated>2021-06-03T02:10:32.776Z</updated>
        <summary type="html"><![CDATA[This paper describes a novel lossless compression method for point cloud
geometry, building on a recent lossy compression method that aimed at
reconstructing only the bounding volume of a point cloud. The proposed scheme
starts by partially reconstructing the geometry from the two depthmaps
associated to a single projection direction. The partial reconstruction
obtained from the depthmaps is completed to a full reconstruction of the point
cloud by sweeping section by section along one direction and encoding the
points which were not contained in the two depthmaps. The main ingredient is a
list-based encoding of the inner points (situated inside the feasible regions)
by a novel arithmetic three dimensional context coding procedure that
efficiently utilizes rotational invariances present in the input data.
State-of-the-art bits-per-voxel results are obtained on benchmark datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kaya_E/0/1/0/all/0/1"&gt;Emre Can Kaya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schwarz_S/0/1/0/all/0/1"&gt;Sebastian Schwarz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tabus_I/0/1/0/all/0/1"&gt;Ioan Tabus&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Who Blames or Endorses Whom? Entity-to-Entity Directed Sentiment Extraction in News Text. (arXiv:2106.01033v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.01033</id>
        <link href="http://arxiv.org/abs/2106.01033"/>
        <updated>2021-06-03T02:10:32.732Z</updated>
        <summary type="html"><![CDATA[Understanding who blames or supports whom in news text is a critical research
question in computational social science. Traditional methods and datasets for
sentiment analysis are, however, not suitable for the domain of political text
as they do not consider the direction of sentiments expressed between entities.
In this paper, we propose a novel NLP task of identifying directed sentiment
relationship between political entities from a given news document, which we
call directed sentiment extraction. From a million-scale news corpus, we
construct a dataset of news sentences where sentiment relations of political
entities are manually annotated. We present a simple but effective approach for
utilizing a pretrained transformer, which infers the target class by predicting
multiple question-answering tasks and combining the outcomes. We demonstrate
the utility of our proposed method for social science research questions by
analyzing positive and negative opinions between political entities in two
major events: 2016 U.S. presidential election and COVID-19. The newly proposed
problem, data, and method will facilitate future studies on interdisciplinary
NLP methods and applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Park_K/0/1/0/all/0/1"&gt;Kunwoo Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_Z/0/1/0/all/0/1"&gt;Zhufeng Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joo_J/0/1/0/all/0/1"&gt;Jungseock Joo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Passage Retrieval with Hashing for Open-domain Question Answering. (arXiv:2106.00882v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.00882</id>
        <link href="http://arxiv.org/abs/2106.00882"/>
        <updated>2021-06-03T02:10:32.722Z</updated>
        <summary type="html"><![CDATA[Most state-of-the-art open-domain question answering systems use a neural
retrieval model to encode passages into continuous vectors and extract them
from a knowledge source. However, such retrieval models often require large
memory to run because of the massive size of their passage index. In this
paper, we introduce Binary Passage Retriever (BPR), a memory-efficient neural
retrieval model that integrates a learning-to-hash technique into the
state-of-the-art Dense Passage Retriever (DPR) to represent the passage index
using compact binary codes rather than continuous vectors. BPR is trained with
a multi-task objective over two tasks: efficient candidate generation based on
binary codes and accurate reranking based on continuous vectors. Compared with
DPR, BPR substantially reduces the memory cost from 65GB to 2GB without a loss
of accuracy on two standard open-domain question answering benchmarks: Natural
Questions and TriviaQA. Our code and trained models are available at
https://github.com/studio-ousia/bpr.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yamada_I/0/1/0/all/0/1"&gt;Ikuya Yamada&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Asai_A/0/1/0/all/0/1"&gt;Akari Asai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1"&gt;Hannaneh Hajishirzi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Conversational Question Answering: A Survey. (arXiv:2106.00874v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.00874</id>
        <link href="http://arxiv.org/abs/2106.00874"/>
        <updated>2021-06-03T02:10:32.692Z</updated>
        <summary type="html"><![CDATA[Question answering (QA) systems provide a way of querying the information
available in various formats including, but not limited to, unstructured and
structured data in natural languages. It constitutes a considerable part of
conversational artificial intelligence (AI) which has led to the introduction
of a special research topic on Conversational Question Answering (CQA), wherein
a system is required to understand the given context and then engages in
multi-turn QA to satisfy the user's information needs. Whilst the focus of most
of the existing research work is subjected to single-turn QA, the field of
multi-turn QA has recently grasped attention and prominence owing to the
availability of large-scale, multi-turn QA datasets and the development of
pre-trained language models. With a good amount of models and research papers
adding to the literature every year recently, there is a dire need of arranging
and presenting the related work in a unified manner to streamline future
research. This survey, therefore, is an effort to present a comprehensive
review of the state-of-the-art research trends of CQA primarily based on
reviewed papers from 2016-2021. Our findings show that there has been a trend
shift from single-turn to multi-turn QA which empowers the field of
Conversational AI from different perspectives. This survey is intended to
provide an epitome for the research community with the hope of laying a strong
foundation for the field of CQA.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zaib_M/0/1/0/all/0/1"&gt;Munazza Zaib&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Wei Emma Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sheng_Q/0/1/0/all/0/1"&gt;Quan Z. Sheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mahmood_A/0/1/0/all/0/1"&gt;Adnan Mahmood&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yang Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Human-centric Spatio-Temporal Video Grounding With Visual Transformers. (arXiv:2011.05049v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.05049</id>
        <link href="http://arxiv.org/abs/2011.05049"/>
        <updated>2021-06-03T02:10:32.553Z</updated>
        <summary type="html"><![CDATA[In this work, we introduce a novel task - Humancentric Spatio-Temporal Video
Grounding (HC-STVG). Unlike the existing referring expression tasks in images
or videos, by focusing on humans, HC-STVG aims to localize a spatiotemporal
tube of the target person from an untrimmed video based on a given textural
description. This task is useful, especially for healthcare and
security-related applications, where the surveillance videos can be extremely
long but only a specific person during a specific period of time is concerned.
HC-STVG is a video grounding task that requires both spatial (where) and
temporal (when) localization. Unfortunately, the existing grounding methods
cannot handle this task well. We tackle this task by proposing an effective
baseline method named Spatio-Temporal Grounding with Visual Transformers
(STGVT), which utilizes Visual Transformers to extract cross-modal
representations for video-sentence matching and temporal localization. To
facilitate this task, we also contribute an HC-STVG dataset consisting of 5,660
video-sentence pairs on complex multi-person scenes. Specifically, each video
lasts for 20 seconds, pairing with a natural query sentence with an average of
17.25 words. Extensive experiments are conducted on this dataset, demonstrating
the newly-proposed method outperforms the existing baseline methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1"&gt;Zongheng Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liao_Y/0/1/0/all/0/1"&gt;Yue Liao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Si Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1"&gt;Guanbin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1"&gt;Xiaojie Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1"&gt;Hongxu Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1"&gt;Qian Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1"&gt;Dong Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Needle in a Haystack: Label-Efficient Evaluation under Extreme Class Imbalance. (arXiv:2006.06963v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.06963</id>
        <link href="http://arxiv.org/abs/2006.06963"/>
        <updated>2021-06-03T02:10:32.537Z</updated>
        <summary type="html"><![CDATA[Important tasks like record linkage and extreme classification demonstrate
extreme class imbalance, with 1 minority instance to every 1 million or more
majority instances. Obtaining a sufficient sample of all classes, even just to
achieve statistically-significant evaluation, is so challenging that most
current approaches yield poor estimates or incur impractical cost. Where
importance sampling has been levied against this challenge, restrictive
constraints are placed on performance metrics, estimates do not come with
appropriate guarantees, or evaluations cannot adapt to incoming labels. This
paper develops a framework for online evaluation based on adaptive importance
sampling. Given a target performance metric and model for $p(y|x)$, the
framework adapts a distribution over items to label in order to maximize
statistical precision. We establish strong consistency and a central limit
theorem for the resulting performance estimates, and instantiate our framework
with worked examples that leverage Dirichlet-tree models. Experiments
demonstrate an average MSE superior to state-of-the-art on fixed label budgets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Marchant_N/0/1/0/all/0/1"&gt;Neil G. Marchant&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rubinstein_B/0/1/0/all/0/1"&gt;Benjamin I. P. Rubinstein&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Learning based Full-reference and No-reference Quality Assessment Models for Compressed UGC Videos. (arXiv:2106.01111v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.01111</id>
        <link href="http://arxiv.org/abs/2106.01111"/>
        <updated>2021-06-03T02:10:32.522Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a deep learning based video quality assessment
(VQA) framework to evaluate the quality of the compressed user's generated
content (UGC) videos. The proposed VQA framework consists of three modules, the
feature extraction module, the quality regression module, and the quality
pooling module. For the feature extraction module, we fuse the features from
intermediate layers of the convolutional neural network (CNN) network into
final quality-aware feature representation, which enables the model to make
full use of visual information from low-level to high-level. Specifically, the
structure and texture similarities of feature maps extracted from all
intermediate layers are calculated as the feature representation for the full
reference (FR) VQA model, and the global mean and standard deviation of the
final feature maps fused by intermediate feature maps are calculated as the
feature representation for the no reference (NR) VQA model. For the quality
regression module, we use the fully connected (FC) layer to regress the
quality-aware features into frame-level scores. Finally, a
subjectively-inspired temporal pooling strategy is adopted to pool frame-level
scores into the video-level score. The proposed model achieves the best
performance among the state-of-the-art FR and NR VQA models on the Compressed
UGC VQA database and also achieves pretty good performance on the in-the-wild
UGC VQA databases.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Sun_W/0/1/0/all/0/1"&gt;Wei Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_T/0/1/0/all/0/1"&gt;Tao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Min_X/0/1/0/all/0/1"&gt;Xiongkuo Min&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yi_F/0/1/0/all/0/1"&gt;Fuwang Yi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhai_G/0/1/0/all/0/1"&gt;Guangtao Zhai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring modality-agnostic representations for music classification. (arXiv:2106.01149v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2106.01149</id>
        <link href="http://arxiv.org/abs/2106.01149"/>
        <updated>2021-06-03T02:10:32.499Z</updated>
        <summary type="html"><![CDATA[Music information is often conveyed or recorded across multiple data
modalities including but not limited to audio, images, text and scores.
However, music information retrieval research has almost exclusively focused on
single modality recognition, requiring development of separate models for each
modality. Some multi-modal works require multiple coexisting modalities given
to the model as inputs, constraining the use of these models to the few cases
where data from all modalities are available. To the best of our knowledge, no
existing model has the ability to take inputs from varying modalities, e.g.
images or sounds, and classify them into unified music categories. We explore
the use of cross-modal retrieval as a pretext task to learn modality-agnostic
representations, which can then be used as inputs to classifiers that are
independent of modality. We select instrument classification as an example task
for our study as both visual and audio components provide relevant semantic
information. We train music instrument classifiers that can take both images or
sounds as input, and perform comparably to sound-only or image-only
classifiers. Furthermore, we explore the case when there is limited labeled
data for a given modality, and the impact in performance by using labeled data
from other modalities. We are able to achieve almost 70% of best performing
system in a zero-shot setting. We provide a detailed analysis of experimental
results to understand the potential and limitations of the approach, and
discuss future steps towards modality-agnostic classifiers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1"&gt;Ho-Hsiang Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fuentes_M/0/1/0/all/0/1"&gt;Magdalena Fuentes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bello_J/0/1/0/all/0/1"&gt;Juan P. Bello&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fixed Point Networks: Implicit Depth Models with Jacobian-Free Backprop. (arXiv:2103.12803v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.12803</id>
        <link href="http://arxiv.org/abs/2103.12803"/>
        <updated>2021-06-02T02:50:03.348Z</updated>
        <summary type="html"><![CDATA[A growing trend in deep learning replaces fixed depth models by
approximations of the limit as network depth approaches infinity. This approach
uses a portion of network weights to prescribe behavior by defining a limit
condition. This makes network depth implicit, varying based on the provided
data and an error tolerance. Moreover, existing implicit models can be
implemented and trained with fixed memory costs in exchange for additional
computational costs. In particular, backpropagation through implicit depth
models requires solving a Jacobian-based equation arising from the implicit
function theorem. We propose fixed point networks (FPNs), a simple setup for
implicit depth learning that guarantees convergence of forward propagation to a
unique limit defined by network weights and input data. Our key contribution is
to provide a new Jacobian-free backpropagation (JFB) scheme that circumvents
the need to solve Jacobian-based equations while maintaining fixed memory
costs. This makes FPNs much cheaper to train and easy to implement. Our
numerical examples yield state of the art classification results for implicit
depth models and outperform corresponding explicit models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fung_S/0/1/0/all/0/1"&gt;Samy Wu Fung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heaton_H/0/1/0/all/0/1"&gt;Howard Heaton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1"&gt;Qiuwei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McKenzie_D/0/1/0/all/0/1"&gt;Daniel McKenzie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Osher_S/0/1/0/all/0/1"&gt;Stanley Osher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_W/0/1/0/all/0/1"&gt;Wotao Yin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multiple Descent: Design Your Own Generalization Curve. (arXiv:2008.01036v6 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.01036</id>
        <link href="http://arxiv.org/abs/2008.01036"/>
        <updated>2021-06-02T02:50:03.341Z</updated>
        <summary type="html"><![CDATA[This paper explores the generalization loss of linear regression in variably
parameterized families of models, both under-parameterized and
over-parameterized. We show that the generalization curve can have an arbitrary
number of peaks, and moreover, locations of those peaks can be explicitly
controlled. Our results highlight the fact that both classical U-shaped
generalization curve and the recently observed double descent curve are not
intrinsic properties of the model family. Instead, their emergence is due to
the interaction between the properties of the data and the inductive biases of
learning algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Lin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Min_Y/0/1/0/all/0/1"&gt;Yifei Min&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Belkin_M/0/1/0/all/0/1"&gt;Mikhail Belkin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karbasi_A/0/1/0/all/0/1"&gt;Amin Karbasi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Explainable Convolutional Features for Music Audio Modeling. (arXiv:2106.00110v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2106.00110</id>
        <link href="http://arxiv.org/abs/2106.00110"/>
        <updated>2021-06-02T02:50:03.333Z</updated>
        <summary type="html"><![CDATA[Audio signals are often represented as spectrograms and treated as 2D images.
In this light, deep convolutional architectures are widely used for music audio
tasks even though these two data types have very different structures. In this
work, we attempt to "open the black-box" on deep convolutional models to inform
future architectures for music audio tasks, and explain the excellent
performance of deep convolutions that model spectrograms as 2D images. To this
end, we expand recent explainability discussions in deep learning for natural
image data to music audio data through systematic experiments using the deep
features learned by various convolutional architectures. We demonstrate that
deep convolutional features perform well across various target tasks, whether
or not they are extracted from deep architectures originally trained on that
task. Additionally, deep features exhibit high similarity to hand-crafted
wavelet features, whether the deep features are extracted from a trained or
untrained model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yanchenko_A/0/1/0/all/0/1"&gt;Anna K. Yanchenko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soltani_M/0/1/0/all/0/1"&gt;Mohammadreza Soltani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ravier_R/0/1/0/all/0/1"&gt;Robert J. Ravier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mukherjee_S/0/1/0/all/0/1"&gt;Sayan Mukherjee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tarokh_V/0/1/0/all/0/1"&gt;Vahid Tarokh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Zero-Shot Instance Segmentation. (arXiv:2104.06601v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.06601</id>
        <link href="http://arxiv.org/abs/2104.06601"/>
        <updated>2021-06-02T02:50:03.323Z</updated>
        <summary type="html"><![CDATA[Deep learning has significantly improved the precision of instance
segmentation with abundant labeled data. However, in many areas like medical
and manufacturing, collecting sufficient data is extremely hard and labeling
this data requires high professional skills. We follow this motivation and
propose a new task set named zero-shot instance segmentation (ZSI). In the
training phase of ZSI, the model is trained with seen data, while in the
testing phase, it is used to segment all seen and unseen instances. We first
formulate the ZSI task and propose a method to tackle the challenge, which
consists of Zero-shot Detector, Semantic Mask Head, Background Aware RPN and
Synchronized Background Strategy. We present a new benchmark for zero-shot
instance segmentation based on the MS-COCO dataset. The extensive empirical
results in this benchmark show that our method not only surpasses the
state-of-the-art results in zero-shot object detection task but also achieves
promising performance on ZSI. Our approach will serve as a solid baseline and
facilitate future research in zero-shot instance segmentation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1"&gt;Ye Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1"&gt;Jiahong Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1"&gt;Yongqiang Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1"&gt;Faen Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1"&gt;Li Cui&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Meta-HAR: Federated Representation Learning for Human Activity Recognition. (arXiv:2106.00615v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2106.00615</id>
        <link href="http://arxiv.org/abs/2106.00615"/>
        <updated>2021-06-02T02:50:03.317Z</updated>
        <summary type="html"><![CDATA[Human activity recognition (HAR) based on mobile sensors plays an important
role in ubiquitous computing. However, the rise of data regulatory constraints
precludes collecting private and labeled signal data from personal devices at
scale. Federated learning has emerged as a decentralized alternative solution
to model training, which iteratively aggregates locally updated models into a
shared global model, therefore being able to leverage decentralized, private
data without central collection. However, the effectiveness of federated
learning for HAR is affected by the fact that each user has different activity
types and even a different signal distribution for the same activity type.
Furthermore, it is uncertain if a single global model trained can generalize
well to individual users or new users with heterogeneous data. In this paper,
we propose Meta-HAR, a federated representation learning framework, in which a
signal embedding network is meta-learned in a federated manner, while the
learned signal representations are further fed into a personalized
classification network at each user for activity prediction. In order to boost
the representation ability of the embedding network, we treat the HAR problem
at each user as a different task and train the shared embedding network through
a Model-Agnostic Meta-learning framework, such that the embedding network can
generalize to any individual user. Personalization is further achieved on top
of the robustly learned representations in an adaptation procedure. We
conducted extensive experiments based on two publicly available HAR datasets as
well as a newly created HAR dataset. Results verify that Meta-HAR is effective
at maintaining high test accuracies for individual users, including new users,
and significantly outperforms several baselines, including Federated Averaging,
Reptile and even centralized learning in certain cases.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Li_C/0/1/0/all/0/1"&gt;Chenglin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Niu_D/0/1/0/all/0/1"&gt;Di Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jiang_B/0/1/0/all/0/1"&gt;Bei Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zuo_X/0/1/0/all/0/1"&gt;Xiao Zuo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jianming Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Do We Really Need to Access the Source Data? Source Hypothesis Transfer for Unsupervised Domain Adaptation. (arXiv:2002.08546v6 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.08546</id>
        <link href="http://arxiv.org/abs/2002.08546"/>
        <updated>2021-06-02T02:50:03.296Z</updated>
        <summary type="html"><![CDATA[Unsupervised domain adaptation (UDA) aims to leverage the knowledge learned
from a labeled source dataset to solve similar tasks in a new unlabeled domain.
Prior UDA methods typically require to access the source data when learning to
adapt the model, making them risky and inefficient for decentralized private
data. This work tackles a practical setting where only a trained source model
is available and investigates how we can effectively utilize such a model
without source data to solve UDA problems. We propose a simple yet generic
representation learning framework, named \emph{Source HypOthesis Transfer}
(SHOT). SHOT freezes the classifier module (hypothesis) of the source model and
learns the target-specific feature extraction module by exploiting both
information maximization and self-supervised pseudo-labeling to implicitly
align representations from the target domains to the source hypothesis. To
verify its versatility, we evaluate SHOT in a variety of adaptation cases
including closed-set, partial-set, and open-set domain adaptation. Experiments
indicate that SHOT yields state-of-the-art results among multiple domain
adaptation benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1"&gt;Jian Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_D/0/1/0/all/0/1"&gt;Dapeng Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1"&gt;Jiashi Feng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Structural Knowledge Distillation: Tractably Distilling Information for Structured Predictor. (arXiv:2010.05010v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.05010</id>
        <link href="http://arxiv.org/abs/2010.05010"/>
        <updated>2021-06-02T02:50:03.290Z</updated>
        <summary type="html"><![CDATA[Knowledge distillation is a critical technique to transfer knowledge between
models, typically from a large model (the teacher) to a more fine-grained one
(the student). The objective function of knowledge distillation is typically
the cross-entropy between the teacher and the student's output distributions.
However, for structured prediction problems, the output space is exponential in
size; therefore, the cross-entropy objective becomes intractable to compute and
optimize directly. In this paper, we derive a factorized form of the knowledge
distillation objective for structured prediction, which is tractable for many
typical choices of the teacher and student models. In particular, we show the
tractability and empirical effectiveness of structural knowledge distillation
between sequence labeling and dependency parsing models under four different
scenarios: 1) the teacher and student share the same factorization form of the
output structure scoring function; 2) the student factorization produces more
fine-grained substructures than the teacher factorization; 3) the teacher
factorization produces more fine-grained substructures than the student
factorization; 4) the factorization forms from the teacher and the student are
incompatible.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xinyu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1"&gt;Yong Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1"&gt;Zhaohui Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_Z/0/1/0/all/0/1"&gt;Zixia Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bach_N/0/1/0/all/0/1"&gt;Nguyen Bach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1"&gt;Tao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zhongqiang Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1"&gt;Fei Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tu_K/0/1/0/all/0/1"&gt;Kewei Tu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DRIVE: One-bit Distributed Mean Estimation. (arXiv:2105.08339v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.08339</id>
        <link href="http://arxiv.org/abs/2105.08339"/>
        <updated>2021-06-02T02:50:03.282Z</updated>
        <summary type="html"><![CDATA[We consider the problem where $n$ clients transmit $d$-dimensional
real-valued vectors using only $d(1+o(1))$ bits each, in a manner that allows a
receiver to approximately reconstruct their mean. Such compression problems
arise in federated and distributed learning, as well as in other domains. We
provide novel mathematical results and derive corresponding new algorithms that
outperform previous compression algorithms in accuracy and computational
efficiency. We evaluate our methods on a collection of distributed and
federated learning tasks, using a variety of datasets, and show a consistent
improvement over the state of the art.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vargaftik_S/0/1/0/all/0/1"&gt;Shay Vargaftik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Basat_R/0/1/0/all/0/1"&gt;Ran Ben Basat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Portnoy_A/0/1/0/all/0/1"&gt;Amit Portnoy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mendelson_G/0/1/0/all/0/1"&gt;Gal Mendelson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ben_Itzhak_Y/0/1/0/all/0/1"&gt;Yaniv Ben-Itzhak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mitzenmacher_M/0/1/0/all/0/1"&gt;Michael Mitzenmacher&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VA-GCN: A Vector Attention Graph Convolution Network for learning on Point Clouds. (arXiv:2106.00227v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.00227</id>
        <link href="http://arxiv.org/abs/2106.00227"/>
        <updated>2021-06-02T02:50:03.274Z</updated>
        <summary type="html"><![CDATA[Owing to the development of research on local aggregation operators, dramatic
breakthrough has been made in point cloud analysis models. However, existing
local aggregation operators in the current literature fail to attach decent
importance to the local information of the point cloud, which limits the power
of the models. To fit this gap, we propose an efficient Vector Attention
Convolution module (VAConv), which utilizes K-Nearest Neighbor (KNN) to extract
the neighbor points of each input point, and then uses the elevation and
azimuth relationship of the vectors between the center point and its neighbors
to construct an attention weight matrix for edge features. Afterwards, the
VAConv adopts a dual-channel structure to fuse weighted edge features and
global features. To verify the efficiency of the VAConv, we connect the VAConvs
with different receptive fields in parallel to obtain a Multi-scale graph
convolutional network, VA-GCN. The proposed VA-GCN achieves state-of-the-art
performance on standard benchmarks including ModelNet40, S3DIS and ShapeNet.
Remarkably, on the ModelNet40 dataset for 3D classification, VA-GCN increased
by 2.4% compared to the baseline.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1"&gt;Haotian Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1"&gt;Fanyi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Le_H/0/1/0/all/0/1"&gt;Huixiao Le&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Minimax Regret for Bandit Convex Optimisation of Ridge Functions. (arXiv:2106.00444v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.00444</id>
        <link href="http://arxiv.org/abs/2106.00444"/>
        <updated>2021-06-02T02:50:03.267Z</updated>
        <summary type="html"><![CDATA[We analyse adversarial bandit convex optimisation with an adversary that is
restricted to playing functions of the form $f(x) = g(\langle x,
\theta\rangle)$ for convex $g : \mathbb R \to \mathbb R$ and $\theta \in
\mathbb R^d$. We provide a short information-theoretic proof that the minimax
regret is at most $O(d\sqrt{n} \log(\operatorname{diam}\mathcal K))$ where $n$
is the number of interactions, $d$ the dimension and
$\operatorname{diam}(\mathcal K)$ is the diameter of the constraint set. Hence,
this class of functions is at most logarithmically harder than the linear case.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lattimore_T/0/1/0/all/0/1"&gt;Tor Lattimore&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep learning for COVID-19 diagnosis based feature selection using binary differential evolution algorithm. (arXiv:2104.07279v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.07279</id>
        <link href="http://arxiv.org/abs/2104.07279"/>
        <updated>2021-06-02T02:50:03.248Z</updated>
        <summary type="html"><![CDATA[The new Coronavirus is spreading rapidly and it has taken the lives of many
people so far. The virus has destructive effects on the human lung and early
detection is very important. Deep Convolution neural networks are a powerful
tool in classifying images. Therefore, in this paper a hybrid approach based on
a deep network is presented. Feature vectors were extracted by applying a deep
convolution neural network on the images and effective features were selected
by the binary differential meta-heuristic algorithm. These optimized features
were given to the SVM classifier. A database consisting of three categories of
images as COVID-19, pneumonia, and healthy included 1092 X-ray samples was
considered. The proposed method achieved an accuracy of 99.43%, a sensitivity
of 99.16%, and a specificity of 99.57%. Our results demonstrate the suggested
approach is better than recent studies on COVID-19 detection with X-ray images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Iraji_M/0/1/0/all/0/1"&gt;Mohammad Saber Iraji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Feizi_Derakhshi_M/0/1/0/all/0/1"&gt;Mohammad-Reza Feizi-Derakhshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tanha_J/0/1/0/all/0/1"&gt;Jafar Tanha&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Momentum via Primal Averaging: Theoretical Insights and Learning Rate Schedules for Non-Convex Optimization. (arXiv:2010.00406v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.00406</id>
        <link href="http://arxiv.org/abs/2010.00406"/>
        <updated>2021-06-02T02:50:03.242Z</updated>
        <summary type="html"><![CDATA[Momentum methods are now used pervasively within the machine learning
community for training non-convex models such as deep neural networks.
Empirically, they out perform traditional stochastic gradient descent (SGD)
approaches. In this work we develop a Lyapunov analysis of SGD with momentum
(SGD+M), by utilizing a equivalent rewriting of the method known as the
stochastic primal averaging (SPA) form. This analysis is much tighter than
previous theory in the non-convex case, and due to this we are able to give
precise insights into when SGD+M may out-perform SGD, and what hyper-parameter
schedules will work and why.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Defazio_A/0/1/0/all/0/1"&gt;Aaron Defazio&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Separating the Effects of Batch Normalization on CNN Training Speed and Stability Using Classical Adaptive Filter Theory. (arXiv:2002.10674v2 [cs.NE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.10674</id>
        <link href="http://arxiv.org/abs/2002.10674"/>
        <updated>2021-06-02T02:50:03.236Z</updated>
        <summary type="html"><![CDATA[Batch Normalization (BatchNorm) is commonly used in Convolutional Neural
Networks (CNNs) to improve training speed and stability. However, there is
still limited consensus on why this technique is effective. This paper uses
concepts from the traditional adaptive filter domain to provide insight into
the dynamics and inner workings of BatchNorm. First, we show that the
convolution weight updates have natural modes whose stability and convergence
speed are tied to the eigenvalues of the input autocorrelation matrices, which
are controlled by BatchNorm through the convolution layers' channel-wise
structure. Furthermore, our experiments demonstrate that the speed and
stability benefits are distinct effects. At low learning rates, it is
BatchNorm's amplification of the smallest eigenvalues that improves convergence
speed, while at high learning rates, it is BatchNorm's suppression of the
largest eigenvalues that ensures stability. Lastly, we prove that in the first
training step, when normalization is needed most, BatchNorm satisfies the same
optimization as Normalized Least Mean Square (NLMS), while it continues to
approximate this condition in subsequent steps. The analyses provided in this
paper lay the groundwork for gaining further insight into the operation of
modern neural network structures using adaptive filter theory.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chai_E/0/1/0/all/0/1"&gt;Elaina Chai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pilanci_M/0/1/0/all/0/1"&gt;Mert Pilanci&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Murmann_B/0/1/0/all/0/1"&gt;Boris Murmann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Source Data-absent Unsupervised Domain Adaptation through Hypothesis Transfer and Labeling Transfer. (arXiv:2012.07297v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.07297</id>
        <link href="http://arxiv.org/abs/2012.07297"/>
        <updated>2021-06-02T02:50:03.228Z</updated>
        <summary type="html"><![CDATA[Unsupervised domain adaptation (UDA) aims to transfer knowledge from a
related but different well-labeled source domain to a new unlabeled target
domain. Most existing UDA methods require access to the source data, and thus
are not applicable when the data are confidential and not shareable due to
privacy concerns. This paper aims to tackle a realistic setting with only a
classification model available trained over, instead of accessing to, the
source data. To effectively utilize the source model for adaptation, we propose
a novel approach called Source HypOthesis Transfer (SHOT), which learns the
feature extraction module for the target domain by fitting the target data
features to the frozen source classification module (representing
classification hypothesis). Specifically, SHOT exploits both information
maximization and self-supervised learning for the feature extraction module
learning to ensure the target features are implicitly aligned with the features
of unseen source data via the same hypothesis. Furthermore, we propose a new
labeling transfer strategy, which separates the target data into two splits
based on the confidence of predictions (labeling information), and then employ
semi-supervised learning to improve the accuracy of less-confident predictions
in the target domain. We denote labeling transfer as SHOT++ if the predictions
are obtained by SHOT. Extensive experiments on both digit classification and
object recognition tasks show that SHOT and SHOT++ achieve results surpassing
or comparable to the state-of-the-arts, demonstrating the effectiveness of our
approaches for various visual domain adaptation problems. Code will be
available at \url{https://github.com/tim-learn/SHOT-plus}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1"&gt;Jian Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_D/0/1/0/all/0/1"&gt;Dapeng Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yunbo Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1"&gt;Ran He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1"&gt;Jiashi Feng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Frivolous Units: Wider Networks Are Not Really That Wide. (arXiv:1912.04783v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1912.04783</id>
        <link href="http://arxiv.org/abs/1912.04783"/>
        <updated>2021-06-02T02:50:03.217Z</updated>
        <summary type="html"><![CDATA[A remarkable characteristic of overparameterized deep neural networks (DNNs)
is that their accuracy does not degrade when the network's width is increased.
Recent evidence suggests that developing compressible representations is key
for adjusting the complexity of large networks to the learning task at hand.
However, these compressible representations are poorly understood. A promising
strand of research inspired from biology is understanding representations at
the unit level as it offers a more granular and intuitive interpretation of the
neural mechanisms. In order to better understand what facilitates increases in
width without decreases in accuracy, we ask: Are there mechanisms at the unit
level by which networks control their effective complexity as their width is
increased? If so, how do these depend on the architecture, dataset, and
training parameters? We identify two distinct types of "frivolous" units that
proliferate when the network's width is increased: prunable units which can be
dropped out of the network without significant change to the output and
redundant units whose activities can be expressed as a linear combination of
others. These units imply complexity constraints as the function the network
represents could be expressed by a network without them. We also identify how
the development of these units can be influenced by architecture and a number
of training factors. Together, these results help to explain why the accuracy
of DNNs does not degrade when width is increased and highlight the importance
of frivolous units toward understanding implicit regularization in DNNs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Casper_S/0/1/0/all/0/1"&gt;Stephen Casper&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boix_X/0/1/0/all/0/1"&gt;Xavier Boix&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+DAmario_V/0/1/0/all/0/1"&gt;Vanessa D&amp;#x27;Amario&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_L/0/1/0/all/0/1"&gt;Ling Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schrimpf_M/0/1/0/all/0/1"&gt;Martin Schrimpf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vinken_K/0/1/0/all/0/1"&gt;Kasper Vinken&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kreiman_G/0/1/0/all/0/1"&gt;Gabriel Kreiman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A reinforcement learning approach to improve communication performance and energy utilization in fog-based IoT. (arXiv:2106.00654v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.00654</id>
        <link href="http://arxiv.org/abs/2106.00654"/>
        <updated>2021-06-02T02:50:03.185Z</updated>
        <summary type="html"><![CDATA[Recent research has shown the potential of using available mobile fog devices
(such as smartphones, drones, domestic and industrial robots) as relays to
minimize communication outages between sensors and destination devices, where
localized Internet-of-Things services (e.g., manufacturing process control,
health and security monitoring) are delivered. However, these mobile relays
deplete energy when they move and transmit to distant destinations. As such,
power-control mechanisms and intelligent mobility of the relay devices are
critical in improving communication performance and energy utilization. In this
paper, we propose a Q-learning-based decentralized approach where each mobile
fog relay agent (MFRA) is controlled by an autonomous agent which uses
reinforcement learning to simultaneously improve communication performance and
energy utilization. Each autonomous agent learns based on the feedback from the
destination and its own energy levels whether to remain active and forward the
message, or become passive for that transmission phase. We evaluate the
approach by comparing with the centralized approach, and observe that with
lesser number of MFRAs, our approach is able to ensure reliable delivery of
data and reduce overall energy cost by 56.76\% -- 88.03\%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Omoniwa_B/0/1/0/all/0/1"&gt;Babatunji Omoniwa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gueriau_M/0/1/0/all/0/1"&gt;Maxime Gueriau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dusparic_I/0/1/0/all/0/1"&gt;Ivana Dusparic&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Experiments with graph convolutional networks for solving the vertex $p$-center problem. (arXiv:2106.00357v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.00357</id>
        <link href="http://arxiv.org/abs/2106.00357"/>
        <updated>2021-06-02T02:50:03.144Z</updated>
        <summary type="html"><![CDATA[In the last few years, graph convolutional networks (GCN) have become a
popular research direction in the machine learning community to tackle NP-hard
combinatorial optimization problems (COPs) defined on graphs. While the
obtained results are usually still not competitive with problem-specific
solution approaches from the operations research community, GCNs often lead to
improvements compared to previous machine learning approaches for classical
COPs such as the traveling salesperson problem (TSP).

In this work we present a preliminary study on using GCNs for solving the
vertex p-center problem (PCP), which is another classic COP on graphs. In
particular, we investigate whether a successful model based on end-to-end
training for the TSP can be adapted to a PCP, which is defined on a similar 2D
Euclidean graph input as the usually used version of the TSP. However, the
objective of the PCP has a min-max structure which could lead to many symmetric
optimal, i.e., ground-truth solutions and other potential difficulties for
learning. Our obtained preliminary results show that indeed a direct transfer
of network architecture ideas does not seem to work too well. Thus we think
that the PCP could be an interesting benchmark problem for new ideas and
developments in the area of GCNs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gaar_E/0/1/0/all/0/1"&gt;Elisabeth Gaar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sinnl_M/0/1/0/all/0/1"&gt;Markus Sinnl&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data Cleansing for Deep Neural Networks with Storage-efficient Approximation of Influence Functions. (arXiv:2103.11807v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.11807</id>
        <link href="http://arxiv.org/abs/2103.11807"/>
        <updated>2021-06-02T02:50:03.099Z</updated>
        <summary type="html"><![CDATA[Identifying the influence of training data for data cleansing can improve the
accuracy of deep learning. An approach with stochastic gradient descent (SGD)
called SGD-influence to calculate the influence scores was proposed, but, the
calculation costs are expensive. It is necessary to temporally store the
parameters of the model during training phase for inference phase to calculate
influence sores. In close connection with the previous method, we propose a
method to reduce cache files to store the parameters in training phase for
calculating inference score. We only adopt the final parameters in last epoch
for influence functions calculation. In our experiments on classification, the
cache size of training using MNIST dataset with our approach is 1.236 MB. On
the other hand, the previous method used cache size of 1.932 GB in last epoch.
It means that cache size has been reduced to 1/1,563. We also observed the
accuracy improvement by data cleansing with removal of negatively influential
data using our approach as well as the previous method. Moreover, our simple
and general proposed method to calculate influence scores is available on our
auto ML tool without programing, Neural Network Console. The source code is
also available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Suzuki_K/0/1/0/all/0/1"&gt;Kenji Suzuki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kobayashi_Y/0/1/0/all/0/1"&gt;Yoshiyuki Kobayashi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Narihira_T/0/1/0/all/0/1"&gt;Takuya Narihira&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Asymptotics of representation learning in finite Bayesian neural networks. (arXiv:2106.00651v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.00651</id>
        <link href="http://arxiv.org/abs/2106.00651"/>
        <updated>2021-06-02T02:50:03.065Z</updated>
        <summary type="html"><![CDATA[Recent works have suggested that finite Bayesian neural networks may
outperform their infinite cousins because finite networks can flexibly adapt
their internal representations. However, our theoretical understanding of how
the learned hidden layer representations of finite networks differ from the
fixed representations of infinite networks remains incomplete. Perturbative
finite-width corrections to the network prior and posterior have been studied,
but the asymptotics of learned features have not been fully characterized.
Here, we argue that the leading finite-width corrections to the average feature
kernels for any Bayesian network with linear readout and quadratic cost have a
largely universal form. We illustrate this explicitly for two classes of fully
connected networks: deep linear networks and networks with a single nonlinear
hidden layer. Our results begin to elucidate which features of data wide
Bayesian neural networks learn to represent.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zavatone_Veth_J/0/1/0/all/0/1"&gt;Jacob A. Zavatone-Veth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Canatar_A/0/1/0/all/0/1"&gt;Abdulkadir Canatar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pehlevan_C/0/1/0/all/0/1"&gt;Cengiz Pehlevan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A survey of machine learning-based physics event generation. (arXiv:2106.00643v1 [hep-ph])]]></title>
        <id>http://arxiv.org/abs/2106.00643</id>
        <link href="http://arxiv.org/abs/2106.00643"/>
        <updated>2021-06-02T02:50:03.056Z</updated>
        <summary type="html"><![CDATA[Event generators in high-energy nuclear and particle physics play an
important role in facilitating studies of particle reactions. We survey the
state-of-the-art of machine learning (ML) efforts at building physics event
generators. We review ML generative models used in ML-based event generators
and their specific challenges, and discuss various approaches of incorporating
physics into the ML model designs to overcome these challenges. Finally, we
explore some open questions related to super-resolution, fidelity, and
extrapolation for physics event generation based on ML technology.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/hep-ph/1/au:+Alanazi_Y/0/1/0/all/0/1"&gt;Yasir Alanazi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ph/1/au:+Sato_N/0/1/0/all/0/1"&gt;N. Sato&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ph/1/au:+Ambrozewicz_P/0/1/0/all/0/1"&gt;Pawel Ambrozewicz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ph/1/au:+Blin_A/0/1/0/all/0/1"&gt;Astrid N. Hiller Blin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ph/1/au:+Melnitchouk_W/0/1/0/all/0/1"&gt;W. Melnitchouk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ph/1/au:+Battaglieri_M/0/1/0/all/0/1"&gt;Marco Battaglieri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ph/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tianbo Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ph/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yaohang Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Capacity Preserving Mapping for High-dimensional Data Visualization. (arXiv:1909.13322v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1909.13322</id>
        <link href="http://arxiv.org/abs/1909.13322"/>
        <updated>2021-06-02T02:50:03.041Z</updated>
        <summary type="html"><![CDATA[We provide a rigorous mathematical treatment to the crowding issue in data
visualization when high dimensional data sets are projected down to low
dimensions for visualization. By properly adjusting the capacity of high
dimensional balls, our method makes right enough room to prepare for the
embedding. A key component of the proposed method is an estimation of the
correlation dimension at various scales which reflects the data density
variation. The proposed adjustment to the capacity applies to any distance
(Euclidean, geodesic, diffusion) and can potentially be used in many existing
methods to mitigate the crowding during the dimension reduction. We demonstrate
the effectiveness of the new method using synthetic and real datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1"&gt;Rongrong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiaopeng Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dynamic Scheduling for Over-the-Air Federated Edge Learning with Energy Constraints. (arXiv:2106.00490v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.00490</id>
        <link href="http://arxiv.org/abs/2106.00490"/>
        <updated>2021-06-02T02:50:03.034Z</updated>
        <summary type="html"><![CDATA[Machine learning and wireless communication technologies are jointly
facilitating an intelligent edge, where federated edge learning (FEEL) is a
promising training framework. As wireless devices involved in FEEL are resource
limited in terms of communication bandwidth, computing power and battery
capacity, it is important to carefully schedule them to optimize the training
performance. In this work, we consider an over-the-air FEEL system with analog
gradient aggregation, and propose an energy-aware dynamic device scheduling
algorithm to optimize the training performance under energy constraints of
devices, where both communication energy for gradient aggregation and
computation energy for local training are included. The consideration of
computation energy makes dynamic scheduling challenging, as devices are
scheduled before local training, but the communication energy for over-the-air
aggregation depends on the l2-norm of local gradient, which is known after
local training. We thus incorporate estimation methods into scheduling to
predict the gradient norm. Taking the estimation error into account, we
characterize the performance gap between the proposed algorithm and its offline
counterpart. Experimental results show that, under a highly unbalanced local
data distribution, the proposed algorithm can increase the accuracy by 4.9% on
CIFAR-10 dataset compared with the myopic benchmark, while satisfying the
energy constraints.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1"&gt;Yuxuan Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1"&gt;Sheng Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niu_Z/0/1/0/all/0/1"&gt;Zhisheng Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gunduz_D/0/1/0/all/0/1"&gt;Deniz G&amp;#xfc;nd&amp;#xfc;z&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MARL with General Utilities via Decentralized Shadow Reward Actor-Critic. (arXiv:2106.00543v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.00543</id>
        <link href="http://arxiv.org/abs/2106.00543"/>
        <updated>2021-06-02T02:50:03.027Z</updated>
        <summary type="html"><![CDATA[We posit a new mechanism for cooperation in multi-agent reinforcement
learning (MARL) based upon any nonlinear function of the team's long-term
state-action occupancy measure, i.e., a \emph{general utility}. This subsumes
the cumulative return but also allows one to incorporate risk-sensitivity,
exploration, and priors. % We derive the {\bf D}ecentralized {\bf S}hadow
Reward {\bf A}ctor-{\bf C}ritic (DSAC) in which agents alternate between policy
evaluation (critic), weighted averaging with neighbors (information mixing),
and local gradient updates for their policy parameters (actor). DSAC augments
the classic critic step by requiring agents to (i) estimate their local
occupancy measure in order to (ii) estimate the derivative of the local utility
with respect to their occupancy measure, i.e., the "shadow reward". DSAC
converges to $\epsilon$-stationarity in $\mathcal{O}(1/\epsilon^{2.5})$
(Theorem \ref{theorem:final}) or faster $\mathcal{O}(1/\epsilon^{2})$
(Corollary \ref{corollary:communication}) steps with high probability,
depending on the amount of communications. We further establish the
non-existence of spurious stationary points for this problem, that is, DSAC
finds the globally optimal policy (Corollary \ref{corollary:global}).
Experiments demonstrate the merits of goals beyond the cumulative return in
cooperative MARL.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Junyu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Bedi_A/0/1/0/all/0/1"&gt;Amrit Singh Bedi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wang_M/0/1/0/all/0/1"&gt;Mengdi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Koppel_A/0/1/0/all/0/1"&gt;Alec Koppel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MoET: Mixture of Expert Trees and its Application to Verifiable Reinforcement Learning. (arXiv:1906.06717v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1906.06717</id>
        <link href="http://arxiv.org/abs/1906.06717"/>
        <updated>2021-06-02T02:50:03.003Z</updated>
        <summary type="html"><![CDATA[Rapid advancements in deep learning have led to many recent breakthroughs.
While deep learning models achieve superior performance, often statistically
better than humans, their adaption into safety-critical settings, such as
healthcare or self-driving cars is hindered by their inability to provide
safety guarantees or to analyze the inner workings of the model. We present
MoET, a novel model based on Mixture of Experts, consisting of decision tree
experts and a generalized linear model gating function. While decision
boundaries of decision trees (used in an existing verifiable approach), are
axis-perpendicular hyperplanes, MoET supports hyperplanes of arbitrary
orientation as the boundaries. To support non-differentiable decision trees as
experts we formulate a novel training procedure. In addition, we introduce a
hard thresholding version, MoET_h, in which predictions are made solely by a
single expert chosen via the gating function. Thanks to that property, MoET_h
allows each prediction to be easily decomposed into a set of logical rules.
Such rules can be translated into a manageable SMT formula providing rich means
for verification. While MoET is a general use model, we illustrate its power in
the reinforcement learning setting. By training MoET models using an imitation
learning procedure on deep RL agents we outperform the previous
state-of-the-art technique based on decision trees while preserving the
verifiability of the models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vasic_M/0/1/0/all/0/1"&gt;Marko Vasic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Petrovic_A/0/1/0/all/0/1"&gt;Andrija Petrovic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1"&gt;Kaiyuan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nikolic_M/0/1/0/all/0/1"&gt;Mladen Nikolic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1"&gt;Rishabh Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khurshid_S/0/1/0/all/0/1"&gt;Sarfraz Khurshid&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[One4all User Representation for Recommender Systems in E-commerce. (arXiv:2106.00573v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.00573</id>
        <link href="http://arxiv.org/abs/2106.00573"/>
        <updated>2021-06-02T02:50:02.997Z</updated>
        <summary type="html"><![CDATA[General-purpose representation learning through large-scale pre-training has
shown promising results in the various machine learning fields. For an
e-commerce domain, the objective of general-purpose, i.e., one for all,
representations would be efficient applications for extensive downstream tasks
such as user profiling, targeting, and recommendation tasks. In this paper, we
systematically compare the generalizability of two learning strategies, i.e.,
transfer learning through the proposed model, ShopperBERT, vs. learning from
scratch. ShopperBERT learns nine pretext tasks with 79.2M parameters from 0.8B
user behaviors collected over two years to produce user embeddings. As a
result, the MLPs that employ our embedding method outperform more complex
models trained from scratch for five out of six tasks. Specifically, the
pre-trained embeddings have superiority over the task-specific supervised
features and the strong baselines, which learn the auxiliary dataset for the
cold-start problem. We also show the computational efficiency and embedding
visualization of the pre-trained features.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shin_K/0/1/0/all/0/1"&gt;Kyuyong Shin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kwak_H/0/1/0/all/0/1"&gt;Hanock Kwak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1"&gt;Kyung-Min Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1"&gt;Minkyu Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_Y/0/1/0/all/0/1"&gt;Young-Jin Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jeong_J/0/1/0/all/0/1"&gt;Jisu Jeong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jung_S/0/1/0/all/0/1"&gt;Seungjae Jung&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Wireless Federated Learning with Limited Communication and Differential Privacy. (arXiv:2106.00564v1 [cs.IT])]]></title>
        <id>http://arxiv.org/abs/2106.00564</id>
        <link href="http://arxiv.org/abs/2106.00564"/>
        <updated>2021-06-02T02:50:02.979Z</updated>
        <summary type="html"><![CDATA[This paper investigates the role of dimensionality reduction in efficient
communication and differential privacy (DP) of the local datasets at the remote
users for over-the-air computation (AirComp)-based federated learning (FL)
model. More precisely, we consider the FL setting in which clients are prompted
to train a machine learning model by simultaneous channel-aware and limited
communications with a parameter server (PS) over a Gaussian multiple-access
channel (GMAC), so that transmissions sum coherently at the PS globally aware
of the channel coefficients. For this setting, an algorithm is proposed based
on applying federated stochastic gradient descent (FedSGD) for training the
minimum of a given loss function based on the local gradients,
Johnson-Lindenstrauss (JL) random projection for reducing the dimension of the
local updates, and artificial noise to further aid user's privacy. For this
scheme, our results show that the local DP performance is mainly improved due
to injecting noise of greater variance on each dimension while keeping the
sensitivity of the projected vectors unchanged. This is while the convergence
rate is slowed down compared to the case without dimensionality reduction. As
the performance outweighs for the slower convergence, the trade-off between
privacy and convergence is higher but is shown to lessen in high-dimensional
regime yielding almost the same trade-off with much less communication cost.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sonee_A/0/1/0/all/0/1"&gt;Amir Sonee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rini_S/0/1/0/all/0/1"&gt;Stefano Rini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yu-Chih Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Compression-Compilation Framework for On-mobile Real-time BERT Applications. (arXiv:2106.00526v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.00526</id>
        <link href="http://arxiv.org/abs/2106.00526"/>
        <updated>2021-06-02T02:50:02.972Z</updated>
        <summary type="html"><![CDATA[Transformer-based deep learning models have increasingly demonstrated high
accuracy on many natural language processing (NLP) tasks. In this paper, we
propose a compression-compilation co-design framework that can guarantee the
identified model to meet both resource and real-time specifications of mobile
devices. Our framework applies a compiler-aware neural architecture
optimization method (CANAO), which can generate the optimal compressed model
that balances both accuracy and latency. We are able to achieve up to 7.8x
speedup compared with TensorFlow-Lite with only minor accuracy loss. We present
two types of BERT applications on mobile devices: Question Answering (QA) and
Text Generation. Both can be executed in real-time with latency as low as 45ms.
Videos for demonstrating the framework can be found on
https://www.youtube.com/watch?v=_WIRvK_2PZI]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Niu_W/0/1/0/all/0/1"&gt;Wei Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kong_Z/0/1/0/all/0/1"&gt;Zhenglun Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_G/0/1/0/all/0/1"&gt;Geng Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1"&gt;Weiwen Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guan_J/0/1/0/all/0/1"&gt;Jiexiong Guan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1"&gt;Caiwen Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1"&gt;Pu Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Sijia Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_B/0/1/0/all/0/1"&gt;Bin Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yanzhi Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Care Label Concept: A Certification Suite for Trustworthy and Resource-Aware Machine Learning. (arXiv:2106.00512v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.00512</id>
        <link href="http://arxiv.org/abs/2106.00512"/>
        <updated>2021-06-02T02:50:02.966Z</updated>
        <summary type="html"><![CDATA[Machine learning applications have become ubiquitous. This has led to an
increased effort of making machine learning trustworthy. Explainable and fair
AI have already matured. They address knowledgeable users and application
engineers. For those who do not want to invest time into understanding the
method or the learned model, we offer care labels: easy to understand at a
glance, allowing for method or model comparisons, and, at the same time,
scientifically well-based. On one hand, this transforms descriptions as given
by, e.g., Fact Sheets or Model Cards, into a form that is well-suited for
end-users. On the other hand, care labels are the result of a certification
suite that tests whether stated guarantees hold. In this paper, we present two
experiments with our certification suite. One shows the care labels for
configurations of Markov random fields (MRFs). Based on the underlying theory
of MRFs, each choice leads to its specific rating of static properties like,
e.g., expressivity and reliability. In addition, the implementation is tested
and resource consumption is measured yielding dynamic properties. This
two-level procedure is followed by another experiment certifying deep neural
network (DNN) models. There, we draw the static properties from the literature
on a particular model and data set. At the second level, experiments are
generated that deliver measurements of robustness against certain attacks. We
illustrate this by ResNet-18 and MobileNetV3 applied to ImageNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Morik_K/0/1/0/all/0/1"&gt;Katharina Morik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kotthaus_H/0/1/0/all/0/1"&gt;Helena Kotthaus&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heppe_L/0/1/0/all/0/1"&gt;Lukas Heppe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heinrich_D/0/1/0/all/0/1"&gt;Danny Heinrich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fischer_R/0/1/0/all/0/1"&gt;Raphael Fischer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pauly_A/0/1/0/all/0/1"&gt;Andreas Pauly&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Piatkowski_N/0/1/0/all/0/1"&gt;Nico Piatkowski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fair Clustering Using Antidote Data. (arXiv:2106.00600v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.00600</id>
        <link href="http://arxiv.org/abs/2106.00600"/>
        <updated>2021-06-02T02:50:02.951Z</updated>
        <summary type="html"><![CDATA[Clustering algorithms are widely utilized for many modern data science
applications. This motivates the need to make outputs of clustering algorithms
fair. Traditionally, new fair algorithmic variants to clustering algorithms are
developed for specific notions of fairness. However, depending on the
application context, different definitions of fairness might need to be
employed. As a result, new algorithms and analysis need to be proposed for each
combination of clustering algorithm and fairness definition. Additionally, each
new algorithm would need to be reimplemented for deployment in a real-world
system. Hence, we propose an alternate approach to fairness in clustering where
we augment the original dataset with a small number of data points, called
antidote data. When clustering is undertaken on this new dataset, the output is
fair, for the chosen clustering algorithm and fairness definition. We formulate
this as a general bi-level optimization problem which can accommodate any
center-based clustering algorithms and fairness notions. We then categorize
approaches for solving this bi-level optimization for different problem
settings. Extensive experiments on different clustering algorithms and fairness
notions show that our algorithms can achieve desired levels of fairness on many
real-world datasets with a very small percentage of antidote data added. We
also find that our algorithms achieve lower fairness costs and competitive
clustering performance compared to other state-of-the-art fair clustering
algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chhabra_A/0/1/0/all/0/1"&gt;Anshuman Chhabra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singla_A/0/1/0/all/0/1"&gt;Adish Singla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mohapatra_P/0/1/0/all/0/1"&gt;Prasant Mohapatra&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Markov Localisation using Heatmap Regression and Deep Convolutional Odometry. (arXiv:2106.00371v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2106.00371</id>
        <link href="http://arxiv.org/abs/2106.00371"/>
        <updated>2021-06-02T02:50:02.935Z</updated>
        <summary type="html"><![CDATA[In the context of self-driving vehicles there is strong competition between
approaches based on visual localisation and LiDAR. While LiDAR provides
important depth information, it is sparse in resolution and expensive. On the
other hand, cameras are low-cost and recent developments in deep learning mean
they can provide high localisation performance. However, several fundamental
problems remain, particularly in the domain of uncertainty, where learning
based approaches can be notoriously over-confident.

Markov, or grid-based, localisation was an early solution to the localisation
problem but fell out of favour due to its computational complexity.
Representing the likelihood field as a grid (or volume) means there is a trade
off between accuracy and memory size. Furthermore, it is necessary to perform
expensive convolutions across the entire likelihood volume. Despite the benefit
of simultaneously maintaining a likelihood for all possible locations, grid
based approaches were superseded by more efficient particle filters and Monte
Carlo Localisation (MCL). However, MCL introduces its own problems e.g.
particle deprivation.

Recent advances in deep learning hardware allow large likelihood volumes to
be stored directly on the GPU, along with the hardware necessary to efficiently
perform GPU-bound 3D convolutions and this obviates many of the disadvantages
of grid based methods. In this work, we present a novel CNN-based localisation
approach that can leverage modern deep learning hardware. By implementing a
grid-based Markov localisation approach directly on the GPU, we create a hybrid
CNN that can perform image-based localisation and odometry-based likelihood
propagation within a single neural network. The resulting approach is capable
of outperforming direct pose regression methods as well as state-of-the-art
localisation systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mendez_O/0/1/0/all/0/1"&gt;Oscar Mendez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hadfield_S/0/1/0/all/0/1"&gt;Simon Hadfield&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bowden_R/0/1/0/all/0/1"&gt;Richard Bowden&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[UVIP: Model-Free Approach to Evaluate Reinforcement Learning Algorithms. (arXiv:2105.02135v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.02135</id>
        <link href="http://arxiv.org/abs/2105.02135"/>
        <updated>2021-06-02T02:50:02.923Z</updated>
        <summary type="html"><![CDATA[Policy evaluation is an important instrument for the comparison of different
algorithms in Reinforcement Learning (RL). Yet even a precise knowledge of the
value function $V^{\pi}$ corresponding to a policy $\pi$ does not provide
reliable information on how far is the policy $\pi$ from the optimal one. We
present a novel model-free upper value iteration procedure $({\sf UVIP})$ that
allows us to estimate the suboptimality gap $V^{\star}(x) - V^{\pi}(x)$ from
above and to construct confidence intervals for $V^\star$. Our approach relies
on upper bounds to the solution of the Bellman optimality equation via
martingale approach. We provide theoretical guarantees for ${\sf UVIP}$ under
general assumptions and illustrate its performance on a number of benchmark RL
problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Belomestny_D/0/1/0/all/0/1"&gt;D. Belomestny&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Levin_I/0/1/0/all/0/1"&gt;I. Levin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moulines_E/0/1/0/all/0/1"&gt;E. Moulines&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Naumov_A/0/1/0/all/0/1"&gt;A. Naumov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Samsonov_S/0/1/0/all/0/1"&gt;S. Samsonov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zorina_V/0/1/0/all/0/1"&gt;V. Zorina&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reliability Testing for Natural Language Processing Systems. (arXiv:2105.02590v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.02590</id>
        <link href="http://arxiv.org/abs/2105.02590"/>
        <updated>2021-06-02T02:50:02.917Z</updated>
        <summary type="html"><![CDATA[Questions of fairness, robustness, and transparency are paramount to address
before deploying NLP systems. Central to these concerns is the question of
reliability: Can NLP systems reliably treat different demographics fairly and
function correctly in diverse and noisy environments? To address this, we argue
for the need for reliability testing and contextualize it among existing work
on improving accountability. We show how adversarial attacks can be reframed
for this goal, via a framework for developing reliability tests. We argue that
reliability testing -- with an emphasis on interdisciplinary collaboration --
will enable rigorous and targeted testing, and aid in the enactment and
enforcement of industry standards.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tan_S/0/1/0/all/0/1"&gt;Samson Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1"&gt;Shafiq Joty&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baxter_K/0/1/0/all/0/1"&gt;Kathy Baxter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Taeihagh_A/0/1/0/all/0/1"&gt;Araz Taeihagh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bennett_G/0/1/0/all/0/1"&gt;Gregory A. Bennett&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kan_M/0/1/0/all/0/1"&gt;Min-Yen Kan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Learning for EEG Seizure Detection in Preterm Infants. (arXiv:2106.00611v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2106.00611</id>
        <link href="http://arxiv.org/abs/2106.00611"/>
        <updated>2021-06-02T02:50:02.898Z</updated>
        <summary type="html"><![CDATA[EEG is the gold standard for seizure detection in the newborn infant, but EEG
interpretation in the preterm group is particularly challenging; trained
experts are scarce and the task of interpreting EEG in real-time is arduous.
Preterm infants are reported to have a higher incidence of seizures compared to
term infants. Preterm EEG morphology differs from that of term infants, which
implies that seizure detection algorithms trained on term EEG may not be
appropriate. The task of developing preterm specific algorithms becomes
extra-challenging given the limited amount of annotated preterm EEG data
available. This paper explores novel deep learning (DL) architectures for the
task of neonatal seizure detection in preterm infants. The study tests and
compares several approaches to address the problem: training on data from
full-term infants; training on data from preterm infants; training on
age-specific preterm data and transfer learning. The system performance is
assessed on a large database of continuous EEG recordings of 575h in duration.
It is shown that the accuracy of a validated term-trained EEG seizure detection
algorithm, based on a support vector machine classifier, when tested on preterm
infants falls well short of the performance achieved for full-term infants. An
AUC of 88.3% was obtained when tested on preterm EEG as compared to 96.6%
obtained when tested on term EEG. When re-trained on preterm EEG, the
performance marginally increases to 89.7%. An alternative DL approach shows a
more stable trend when tested on the preterm cohort, starting with an AUC of
93.3% for the term-trained algorithm and reaching 95.0% by transfer learning
from the term model using available preterm data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+OShea_A/0/1/0/all/0/1"&gt;Alison OShea&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ahmed_R/0/1/0/all/0/1"&gt;Rehan Ahmed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lightbody_G/0/1/0/all/0/1"&gt;Gordon Lightbody&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mathieson_S/0/1/0/all/0/1"&gt;Sean Mathieson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Pavlidis_E/0/1/0/all/0/1"&gt;Elena Pavlidis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lloyd_R/0/1/0/all/0/1"&gt;Rhodri Lloyd&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Pisani_F/0/1/0/all/0/1"&gt;Francesco Pisani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Marnane_W/0/1/0/all/0/1"&gt;Willian Marnane&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Boylan_G/0/1/0/all/0/1"&gt;Geraldine Boylan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Temko_A/0/1/0/all/0/1"&gt;Andriy Temko&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Implicit Graph Neural Networks. (arXiv:2009.06211v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.06211</id>
        <link href="http://arxiv.org/abs/2009.06211"/>
        <updated>2021-06-02T02:50:02.864Z</updated>
        <summary type="html"><![CDATA[Graph Neural Networks (GNNs) are widely used deep learning models that learn
meaningful representations from graph-structured data. Due to the finite nature
of the underlying recurrent structure, current GNN methods may struggle to
capture long-range dependencies in underlying graphs. To overcome this
difficulty, we propose a graph learning framework, called Implicit Graph Neural
Networks (IGNN), where predictions are based on the solution of a fixed-point
equilibrium equation involving implicitly defined "state" vectors. We use the
Perron-Frobenius theory to derive sufficient conditions that ensure
well-posedness of the framework. Leveraging implicit differentiation, we derive
a tractable projected gradient descent method to train the framework.
Experiments on a comprehensive range of tasks show that IGNNs consistently
capture long-range dependencies and outperform the state-of-the-art GNN models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gu_F/0/1/0/all/0/1"&gt;Fangda Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1"&gt;Heng Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1"&gt;Wenwu Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sojoudi_S/0/1/0/all/0/1"&gt;Somayeh Sojoudi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghaoui_L/0/1/0/all/0/1"&gt;Laurent El Ghaoui&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Algorithmic Monoculture and Social Welfare. (arXiv:2101.05853v2 [cs.GT] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.05853</id>
        <link href="http://arxiv.org/abs/2101.05853"/>
        <updated>2021-06-02T02:50:02.848Z</updated>
        <summary type="html"><![CDATA[As algorithms are increasingly applied to screen applicants for high-stakes
decisions in employment, lending, and other domains, concerns have been raised
about the effects of algorithmic monoculture, in which many decision-makers all
rely on the same algorithm. This concern invokes analogies to agriculture,
where a monocultural system runs the risk of severe harm from unexpected
shocks. Here we show that the dangers of algorithmic monoculture run much
deeper, in that monocultural convergence on a single algorithm by a group of
decision-making agents, even when the algorithm is more accurate for any one
agent in isolation, can reduce the overall quality of the decisions being made
by the full collection of agents. Unexpected shocks are therefore not needed to
expose the risks of monoculture; it can hurt accuracy even under "normal"
operations, and even for algorithms that are more accurate when used by only a
single decision-maker. Our results rely on minimal assumptions, and involve the
development of a probabilistic framework for analyzing systems that use
multiple noisy estimates of a set of alternatives.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kleinberg_J/0/1/0/all/0/1"&gt;Jon Kleinberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raghavan_M/0/1/0/all/0/1"&gt;Manish Raghavan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Survey of Knowledge Tracing. (arXiv:2105.15106v2 [cs.CY] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.15106</id>
        <link href="http://arxiv.org/abs/2105.15106"/>
        <updated>2021-06-02T02:50:02.835Z</updated>
        <summary type="html"><![CDATA[High-quality education is one of the keys to achieving a more sustainable
world. The recent COVID-19 epidemic has triggered the outbreak of online
education, which has enabled both students and teachers to learn and teach at
home. Meanwhile, it is now possible to record and research a large amount of
learning data using online learning platforms in order to offer better
intelligent educational services. Knowledge Tracing (KT), which aims to monitor
students' evolving knowledge state, is a fundamental and crucial task to
support these intelligent services. Therefore, an increasing amount of research
attention has been paid to this emerging area and considerable progress has
been made. In this survey, we propose a new taxonomy of existing basic KT
models from a technical perspective and provide a comprehensive overview of
these models in a systematic manner. In addition, many variants of KT models
have been proposed to capture more complete learning process. We then review
these variants involved in three phases of the learning process: before,
during, and after the student learning, respectively. Moreover, we present
several typical applications of KT in different educational scenarios. Finally,
we provide some potential directions for future research in this fast-growing
field.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1"&gt;Qi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_S/0/1/0/all/0/1"&gt;Shuanghong Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zhenya Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_E/0/1/0/all/0/1"&gt;Enhong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1"&gt;Yonghe Zheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Riemannian Optimization over Positive Definite Matrices with the Bures-Wasserstein Geometry. (arXiv:2106.00286v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2106.00286</id>
        <link href="http://arxiv.org/abs/2106.00286"/>
        <updated>2021-06-02T02:50:02.829Z</updated>
        <summary type="html"><![CDATA[In this paper, we comparatively analyze the Bures-Wasserstein (BW) geometry
with the popular Affine-Invariant (AI) geometry for Riemannian optimization on
the symmetric positive definite (SPD) matrix manifold. Our study begins with an
observation that the BW metric has a linear dependence on SPD matrices in
contrast to the quadratic dependence of the AI metric. We build on this to show
that the BW metric is a more suitable and robust choice for several Riemannian
optimization problems over ill-conditioned SPD matrices. We show that the BW
geometry has a non-negative curvature, which further improves convergence rates
of algorithms over the non-positively curved AI geometry. Finally, we verify
that several popular cost functions, which are known to be geodesic convex
under the AI geometry, are also geodesic convex under the BW geometry.
Extensive experiments on various applications support our findings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Han_A/0/1/0/all/0/1"&gt;Andi Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Mishra_B/0/1/0/all/0/1"&gt;Bamdev Mishra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Jawanpuria_P/0/1/0/all/0/1"&gt;Pratik Jawanpuria&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Gao_J/0/1/0/all/0/1"&gt;Junbin Gao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Diffusion Models Beat GANs on Image Synthesis. (arXiv:2105.05233v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.05233</id>
        <link href="http://arxiv.org/abs/2105.05233"/>
        <updated>2021-06-02T02:50:02.823Z</updated>
        <summary type="html"><![CDATA[We show that diffusion models can achieve image sample quality superior to
the current state-of-the-art generative models. We achieve this on
unconditional image synthesis by finding a better architecture through a series
of ablations. For conditional image synthesis, we further improve sample
quality with classifier guidance: a simple, compute-efficient method for
trading off diversity for fidelity using gradients from a classifier. We
achieve an FID of 2.97 on ImageNet 128$\times$128, 4.59 on ImageNet
256$\times$256, and 7.72 on ImageNet 512$\times$512, and we match BigGAN-deep
even with as few as 25 forward passes per sample, all while maintaining better
coverage of the distribution. Finally, we find that classifier guidance
combines well with upsampling diffusion models, further improving FID to 3.94
on ImageNet 256$\times$256 and 3.85 on ImageNet 512$\times$512. We release our
code at https://github.com/openai/guided-diffusion]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dhariwal_P/0/1/0/all/0/1"&gt;Prafulla Dhariwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nichol_A/0/1/0/all/0/1"&gt;Alex Nichol&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring Sparse Expert Models and Beyond. (arXiv:2105.15082v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.15082</id>
        <link href="http://arxiv.org/abs/2105.15082"/>
        <updated>2021-06-02T02:50:02.815Z</updated>
        <summary type="html"><![CDATA[Mixture-of-Experts (MoE) models can achieve promising results with outrageous
large amount of parameters but constant computation cost, and thus it has
become a trend in model scaling. Still it is a mystery how MoE layers bring
quality gains by leveraging the parameters with sparse activation. In this
work, we investigate several key factors in sparse expert models. We observe
that load imbalance may not be a significant problem affecting model quality,
contrary to the perspectives of recent studies, while the number of sparsely
activated experts $k$ and expert capacity $C$ in top-$k$ routing can
significantly make a difference in this context. Furthermore, we take a step
forward to propose a simple method called expert prototyping that splits
experts into different prototypes and applies $k$ top-$1$ routing. This
strategy improves the model quality but maintains constant computational costs,
and our further exploration on extremely large-scale models reflects that it is
more effective in training larger models. We push the model scale to over $1$
trillion parameters and implement it on solely $480$ NVIDIA V100-32GB GPUs, in
comparison with the recent SOTAs on $2048$ TPU cores. The proposed giant model
achieves substantial speedup in convergence over the same-size baseline.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_A/0/1/0/all/0/1"&gt;An Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1"&gt;Junyang Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Men_R/0/1/0/all/0/1"&gt;Rui Men&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1"&gt;Chang Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1"&gt;Le Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1"&gt;Xianyan Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1"&gt;Ang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jie Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jiamang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1"&gt;Di Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1"&gt;Wei Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qu_L/0/1/0/all/0/1"&gt;Lin Qu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jingren Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Hongxia Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dynamic-Deep: ECG Task-Aware Compression. (arXiv:2106.00606v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2106.00606</id>
        <link href="http://arxiv.org/abs/2106.00606"/>
        <updated>2021-06-02T02:50:02.797Z</updated>
        <summary type="html"><![CDATA[Monitoring medical data, e.g., Electrocardiogram (ECG) signals, is a common
application of Internet of Things (IoT) devices. Compression methods are often
applied on the massive amounts of sensor data generated before sending it to
the Cloud to reduce storage and delivery costs. A lossy compression provides
high compression gain (CG) but may reduce the performance of an ECG application
(downstream task) due to information loss. Previous works on ECG monitoring
focus either on optimizing the signal reconstruction or the task's performance.
Instead, we advocate a lossy compression solution that allows configuring a
desired performance level on the downstream tasks while maintaining an
optimized CG.

We propose Dynamic-Deep, a task-aware compression that uses convolutional
autoencoders. The compression level is dynamically selected to yield an
optimized compression without violating tasks' performance requirements. We
conduct an extensive evaluation of our approach on common ECG datasets using
two popular ECG applications, which includes heart rate (HR) arrhythmia
classification. We demonstrate that Dynamic-Deep improves HR classification
F1-score by a factor of 3 and increases CG by up to 83% compared to the
previous state-of-the-art (autoencoder-based) compressor. Additionally,
Dynamic-Deep has a 67% lower memory footprint. Analyzing Dynamic-Deep on the
Google Cloud Platform, we observe a 97% reduction in cloud costs compared to a
no compression solution.

To the best of our knowledge, Dynamic-Deep is the first proposal to focus on
balancing the need for high performance of cloud-based downstream tasks and the
desire to achieve optimized compression in IoT ECG monitoring settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Brosh_E/0/1/0/all/0/1"&gt;Eli Brosh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wasserstein_E/0/1/0/all/0/1"&gt;Elad Wasserstein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bremler_Barr_A/0/1/0/all/0/1"&gt;Anat Bremler-Barr&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[More Behind Your Electricity Bill: a Dual-DNN Approach to Non-Intrusive Load Monitoring. (arXiv:2106.00297v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.00297</id>
        <link href="http://arxiv.org/abs/2106.00297"/>
        <updated>2021-06-02T02:50:02.773Z</updated>
        <summary type="html"><![CDATA[Non-intrusive load monitoring (NILM) is a well-known single-channel blind
source separation problem that aims to decompose the household energy
consumption into itemised energy usage of individual appliances. In this way,
considerable energy savings could be achieved by enhancing household's
awareness of energy usage. Recent investigations have shown that deep neural
networks (DNNs) based approaches are promising for the NILM task. Nevertheless,
they normally ignore the inherent properties of appliance operations in the
network design, potentially leading to implausible results. We are thus
motivated to develop the dual Deep Neural Networks (dual-DNN), which aims to i)
take advantage of DNNs' learning capability of latent features and ii) empower
the DNN architecture with identification ability of universal properties.
Specifically in the design of dual-DNN, we adopt one subnetwork to measure
power ratings of different appliances' operation states, and the other
subnetwork to identify the running states of target appliances. The final
result is then obtained by multiplying these two network outputs and meanwhile
considering the multi-state property of household appliances. To enforce the
sparsity property in appliance's state operating, we employ median filtering
and hard gating mechanisms to the subnetwork for state identification. Compared
with the state-of-the-art NILM methods, our dual-DNN approach demonstrates a
21.67% performance improvement in average on two public benchmark datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_G/0/1/0/all/0/1"&gt;Guoming Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1"&gt;Qianyi Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1"&gt;Hong Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Independent Prototype Propagation for Zero-Shot Compositionality. (arXiv:2106.00305v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.00305</id>
        <link href="http://arxiv.org/abs/2106.00305"/>
        <updated>2021-06-02T02:50:02.719Z</updated>
        <summary type="html"><![CDATA[Humans are good at compositional zero-shot reasoning; someone who has never
seen a zebra before could nevertheless recognize one when we tell them it looks
like a horse with black and white stripes. Machine learning systems, on the
other hand, usually leverage spurious correlations in the training data, and
while such correlations can help recognize objects in context, they hurt
generalization. To be able to deal with underspecified datasets while still
leveraging contextual clues during classification, we propose ProtoProp, a
novel prototype propagation graph method. First we learn prototypical
representations of objects (e.g., zebra) that are conditionally independent
w.r.t. their attribute labels (e.g., stripes) and vice versa. Next we propagate
the independent prototypes through a compositional graph, to learn
compositional prototypes of novel attribute-object combinations that reflect
the dependencies of the target distribution. The method does not rely on any
external data, such as class hierarchy graphs or pretrained word embeddings. We
evaluate our approach on AO-Clever, a synthetic and strongly visual dataset
with clean labels, and UT-Zappos, a noisy real-world dataset of fine-grained
shoe types. We show that in the generalized compositional zero-shot setting we
outperform state-of-the-art results, and through ablations we show the
importance of each part of the method and their contribution to the final
results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ruis_F/0/1/0/all/0/1"&gt;Frank Ruis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Burghours_G/0/1/0/all/0/1"&gt;Gertjan Burghours&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bucur_D/0/1/0/all/0/1"&gt;Doina Bucur&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quantifying Predictive Uncertainty in Medical Image Analysis with Deep Kernel Learning. (arXiv:2106.00638v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.00638</id>
        <link href="http://arxiv.org/abs/2106.00638"/>
        <updated>2021-06-02T02:50:02.709Z</updated>
        <summary type="html"><![CDATA[Deep neural networks are increasingly being used for the analysis of medical
images. However, most works neglect the uncertainty in the model's prediction.
We propose an uncertainty-aware deep kernel learning model which permits the
estimation of the uncertainty in the prediction by a pipeline of a
Convolutional Neural Network and a sparse Gaussian Process. Furthermore, we
adapt different pre-training methods to investigate their impacts on the
proposed model. We apply our approach to Bone Age Prediction and Lesion
Localization. In most cases, the proposed model shows better performance
compared to common architectures. More importantly, our model expresses
systematically higher confidence in more accurate predictions and less
confidence in less accurate ones. Our model can also be used to detect
challenging and controversial test samples. Compared to related methods such as
Monte-Carlo Dropout, our approach derives the uncertainty information in a
purely analytical fashion and is thus computationally more efficient.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Zhiliang Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yinchong Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1"&gt;Jindong Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tresp_V/0/1/0/all/0/1"&gt;Volker Tresp&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generating Query Focused Summaries from Query-Free Resources. (arXiv:2012.14774v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.14774</id>
        <link href="http://arxiv.org/abs/2012.14774"/>
        <updated>2021-06-02T02:50:02.554Z</updated>
        <summary type="html"><![CDATA[The availability of large-scale datasets has driven the development of neural
models that create generic summaries from single or multiple documents. In this
work we consider query focused summarization (QFS), a task for which training
data in the form of queries, documents, and summaries is not readily available.
We propose to decompose QFS into (1) query modeling (i.e., finding supportive
evidence within a set of documents for a query) and (2) conditional language
modeling (i.e., summary generation). We introduce MaRGE, a Masked ROUGE
Regression framework for evidence estimation and ranking which relies on a
unified representation for summaries and queries, so that summaries in generic
data can be converted into proxy queries for learning a query model.
Experiments across QFS benchmarks and query types show that our model achieves
state-of-the-art performance despite learning from weak supervision.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yumo Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lapata_M/0/1/0/all/0/1"&gt;Mirella Lapata&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Detect an Odd Restless Markov Arm with a Trembling Hand. (arXiv:2105.03603v2 [cs.IT] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.03603</id>
        <link href="http://arxiv.org/abs/2105.03603"/>
        <updated>2021-06-02T02:50:02.550Z</updated>
        <summary type="html"><![CDATA[This paper studies the problem of finding an anomalous arm in a multi-armed
bandit when (a) each arm is a finite-state Markov process, and (b) the arms are
restless. Here, anomaly means that the transition probability matrix (TPM) of
one of the arms (the odd arm) is different from the common TPM of each of the
non-odd arms. The TPMs are unknown to a decision entity that wishes to find the
index of the odd arm as quickly as possible, subject to an upper bound on the
error probability. We derive a problem instance-specific asymptotic lower bound
on the expected time required to find the odd arm index, where the asymptotics
is as the error probability vanishes. Further, we devise a policy based on the
principle of certainty equivalence, and demonstrate that under a continuous
selection assumption and a certain regularity assumption on the TPMs, the
policy achieves the lower bound arbitrarily closely. Thus, while the lower
bound is shown for all problem instances, the upper bound is shown only for
those problem instances satisfying the continuous selection and the regularity
assumptions. Our achievability analysis is based on resolving the
identifiability problem in the context of a certain lifted countable-state
controlled Markov process.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Karthik_P/0/1/0/all/0/1"&gt;P. N. Karthik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sundaresan_R/0/1/0/all/0/1"&gt;Rajesh Sundaresan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Long-Term Metrics in Recommendation Systems using Short-Horizon Offline RL. (arXiv:2106.00589v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.00589</id>
        <link href="http://arxiv.org/abs/2106.00589"/>
        <updated>2021-06-02T02:50:02.539Z</updated>
        <summary type="html"><![CDATA[We study session-based recommendation scenarios where we want to recommend
items to users during sequential interactions to improve their long-term
utility. Optimizing a long-term metric is challenging because the learning
signal (whether the recommendations achieved their desired goals) is delayed
and confounded by other user interactions with the system. Immediately
measurable proxies such as clicks can lead to suboptimal recommendations due to
misalignment with the long-term metric. Many works have applied episodic
reinforcement learning (RL) techniques for session-based recommendation but
these methods do not account for policy-induced drift in user intent across
sessions. We develop a new batch RL algorithm called Short Horizon Policy
Improvement (SHPI) that approximates policy-induced distribution shifts across
sessions. By varying the horizon hyper-parameter in SHPI, we recover well-known
policy improvement schemes in the RL literature. Empirical results on four
recommendation tasks show that SHPI can outperform matrix factorization,
offline bandits, and offline RL baselines. We also provide a stable and
computationally efficient implementation using weighted regression oracles.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mazoure_B/0/1/0/all/0/1"&gt;Bogdan Mazoure&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mineiro_P/0/1/0/all/0/1"&gt;Paul Mineiro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srinath_P/0/1/0/all/0/1"&gt;Pavithra Srinath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sedeh_R/0/1/0/all/0/1"&gt;Reza Sharifi Sedeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Precup_D/0/1/0/all/0/1"&gt;Doina Precup&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Swaminathan_A/0/1/0/all/0/1"&gt;Adith Swaminathan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automated Concatenation of Embeddings for Structured Prediction. (arXiv:2010.05006v4 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.05006</id>
        <link href="http://arxiv.org/abs/2010.05006"/>
        <updated>2021-06-02T02:50:02.538Z</updated>
        <summary type="html"><![CDATA[Pretrained contextualized embeddings are powerful word representations for
structured prediction tasks. Recent work found that better word representations
can be obtained by concatenating different types of embeddings. However, the
selection of embeddings to form the best concatenated representation usually
varies depending on the task and the collection of candidate embeddings, and
the ever-increasing number of embedding types makes it a more difficult
problem. In this paper, we propose Automated Concatenation of Embeddings (ACE)
to automate the process of finding better concatenations of embeddings for
structured prediction tasks, based on a formulation inspired by recent progress
on neural architecture search. Specifically, a controller alternately samples a
concatenation of embeddings, according to its current belief of the
effectiveness of individual embedding types in consideration for a task, and
updates the belief based on a reward. We follow strategies in reinforcement
learning to optimize the parameters of the controller and compute the reward
based on the accuracy of a task model, which is fed with the sampled
concatenation as input and trained on a task dataset. Empirical results on 6
tasks and 21 datasets show that our approach outperforms strong baselines and
achieves state-of-the-art performance with fine-tuned embeddings in all the
evaluations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xinyu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1"&gt;Yong Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bach_N/0/1/0/all/0/1"&gt;Nguyen Bach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1"&gt;Tao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zhongqiang Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1"&gt;Fei Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tu_K/0/1/0/all/0/1"&gt;Kewei Tu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Sharper Utility Bounds for Differentially Private Pairwise Learning. (arXiv:2105.03033v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.03033</id>
        <link href="http://arxiv.org/abs/2105.03033"/>
        <updated>2021-06-02T02:50:02.537Z</updated>
        <summary type="html"><![CDATA[Pairwise learning focuses on learning tasks with pairwise loss functions,
depends on pairs of training instances, and naturally fits for modeling
relationships between pairs of samples. In this paper, we focus on the privacy
of pairwise learning and propose a new differential privacy paradigm for
pairwise learning, based on gradient perturbation. Except for the privacy
guarantees, we also analyze the excess population risk and give corresponding
bounds under both expectation and high probability conditions. We use the
\textit{on-average stability} and the \textit{pairwise locally elastic
stability} theories to analyze the expectation bound and the high probability
bound, respectively. Moreover, our analyzed utility bounds do not require
convex pairwise loss functions, which means that our method is general to both
convex and non-convex conditions. Under these circumstances, the utility bounds
are similar to (or better than) previous bounds under convexity or strongly
convexity assumption, which are attractive results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1"&gt;Yilin Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jian Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Weiping Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Digital rock reconstruction with user-defined properties using conditional generative adversarial networks. (arXiv:2012.07719v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.07719</id>
        <link href="http://arxiv.org/abs/2012.07719"/>
        <updated>2021-06-02T02:50:02.534Z</updated>
        <summary type="html"><![CDATA[Uncertainty is ubiquitous with flow in subsurface rocks because of their
inherent heterogeneity and lack of in-situ measurements. To complete
uncertainty analysis in a multi-scale manner, it is a prerequisite to provide
sufficient rock samples. Even though the advent of digital rock technology
offers opportunities to reproduce rocks, it still cannot be utilized to provide
massive samples due to its high cost, thus leading to the development of
diversified mathematical methods. Among them, two-point statistics (TPS) and
multi-point statistics (MPS) are commonly utilized, which feature incorporating
low-order and high-order statistical information, respectively. Recently,
generative adversarial networks (GANs) are becoming increasingly popular since
they can reproduce training images with excellent visual and consequent
geologic realism. However, standard GANs can only incorporate information from
data, while leaving no interface for user-defined properties, and thus may
limit the representativeness of reconstructed samples. In this study, we
propose conditional GANs for digital rock reconstruction, aiming to reproduce
samples not only similar to the real training data, but also satisfying
user-specified properties. In fact, the proposed framework can realize the
targets of MPS and TPS simultaneously by incorporating high-order information
directly from rock images with the GANs scheme, while preserving low-order
counterparts through conditioning. We conduct three reconstruction experiments,
and the results demonstrate that rock type, rock porosity, and correlation
length can be successfully conditioned to affect the reconstructed rock images.
Furthermore, in contrast to existing GANs, the proposed conditioning enables
learning of multiple rock types simultaneously, and thus invisibly saves
computational cost.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Q/0/1/0/all/0/1"&gt;Qiang Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1"&gt;Dongxiao Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semi-supervised Models are Strong Unsupervised Domain Adaptation Learners. (arXiv:2106.00417v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.00417</id>
        <link href="http://arxiv.org/abs/2106.00417"/>
        <updated>2021-06-02T02:50:02.533Z</updated>
        <summary type="html"><![CDATA[Unsupervised domain adaptation (UDA) and semi-supervised learning (SSL) are
two typical strategies to reduce expensive manual annotations in machine
learning. In order to learn effective models for a target task, UDA utilizes
the available labeled source data, which may have different distributions from
unlabeled samples in the target domain, while SSL employs few manually
annotated target samples. Although UDA and SSL are seemingly very different
strategies, we find that they are closely related in terms of task objectives
and solutions, and SSL is a special case of UDA problems. Based on this
finding, we further investigate whether SSL methods work on UDA tasks. By
adapting eight representative SSL algorithms on UDA benchmarks, we show that
SSL methods are strong UDA learners. Especially, state-of-the-art SSL methods
significantly outperform existing UDA methods on the challenging UDA benchmark
of DomainNet, and state-of-the-art UDA methods could be further enhanced with
SSL techniques. We thus promote that SSL methods should be employed as
baselines in future UDA studies and expect that the revealed relationship
between UDA and SSL could shed light on future UDA development. Codes are
available at \url{https://github.com/YBZh}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yabin Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Haojian Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_B/0/1/0/all/0/1"&gt;Bin Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shuai Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_K/0/1/0/all/0/1"&gt;Kui Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lei Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automated Grading of Anatomical Objective Structured Practical Exams Using Decision Trees. (arXiv:2106.00502v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.00502</id>
        <link href="http://arxiv.org/abs/2106.00502"/>
        <updated>2021-06-02T02:50:02.532Z</updated>
        <summary type="html"><![CDATA[An Objective Structured Practical Examination (OSPE) is an effective and
robust, but resource-intensive, means of evaluating anatomical knowledge. Since
most OSPEs employ short answer or fill-in-the-blank style questions, the format
requires many people familiar with the content to mark the exams. However, the
increasing prevalence of online delivery for anatomy and physiology courses
could result in students losing the OSPE practice that they would receive in
face-to-face learning sessions. The purpose of this study was to test the
accuracy of Decision Trees (DTs) in marking OSPE questions as a potential first
step to creating an intelligent, online OSPE tutoring system. The study used
the results of the winter 2020 semester final OSPE from McMaster University's
anatomy and physiology course in the Faculty of Health Sciences (HTHSCI
2FF3/2LL3/1D06) as the data set. Ninety percent of the data set was used in a
10-fold validation algorithm to train a DT for each of the 54 questions. Each
DT was comprised of unique words that appeared in correct, student-written
answers. The remaining 10% of the data set was marked by the generated DTs.
When the answers marked by the DT were compared to the answers marked by staff
and faculty, the DT achieved an average accuracy of 94.49% across all 54
questions. This suggests that machine learning algorithms such as DTs are a
highly effective option for OSPE grading and are suitable for the development
of an intelligent, online OSPE tutoring system.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bernard_J/0/1/0/all/0/1"&gt;Jason Bernard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sonnadara_R/0/1/0/all/0/1"&gt;Ranil Sonnadara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saraco_A/0/1/0/all/0/1"&gt;Anthony N. Saraco&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mitchell_J/0/1/0/all/0/1"&gt;Josh P. Mitchell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bak_A/0/1/0/all/0/1"&gt;Alex B. Bak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bayer_I/0/1/0/all/0/1"&gt;Ilana Bayer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wainman_B/0/1/0/all/0/1"&gt;Bruce C. Wainman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CTAB-GAN: Effective Table Data Synthesizing. (arXiv:2102.08369v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.08369</id>
        <link href="http://arxiv.org/abs/2102.08369"/>
        <updated>2021-06-02T02:50:02.530Z</updated>
        <summary type="html"><![CDATA[While data sharing is crucial for knowledge development, privacy concerns and
strict regulation (e.g., European General Data Protection Regulation (GDPR))
unfortunately limit its full effectiveness. Synthetic tabular data emerges as
an alternative to enable data sharing while fulfilling regulatory and privacy
constraints. The state-of-the-art tabular data synthesizers draw methodologies
from generative Adversarial Networks (GAN) and address two main data types in
the industry, i.e., continuous and categorical. In this paper, we develop
CTAB-GAN, a novel conditional table GAN architecture that can effectively model
diverse data types, including a mix of continuous and categorical variables.
Moreover, we address data imbalance and long-tail issues, i.e., certain
variables have drastic frequency differences across large values. To achieve
those aims, we first introduce the information loss and classification loss to
the conditional GAN. Secondly, we design a novel conditional vector, which
efficiently encodes the mixed data type and skewed distribution of data
variable. We extensively evaluate CTAB-GAN with the state of the art GANs that
generate synthetic tables, in terms of data similarity and analysis utility.
The results on five datasets show that the synthetic data of CTAB-GAN
remarkably resembles the real data for all three types of variables and results
into higher accuracy for five machine learning algorithms, by up to 17%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1"&gt;Zilong Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kunar_A/0/1/0/all/0/1"&gt;Aditya Kunar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scheer_H/0/1/0/all/0/1"&gt;Hiek Van der Scheer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Birke_R/0/1/0/all/0/1"&gt;Robert Birke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Lydia Y. Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Poisson CNN: Convolutional neural networks for the solution of the Poisson equation on a Cartesian mesh. (arXiv:1910.08613v2 [physics.comp-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1910.08613</id>
        <link href="http://arxiv.org/abs/1910.08613"/>
        <updated>2021-06-02T02:50:02.523Z</updated>
        <summary type="html"><![CDATA[The Poisson equation is commonly encountered in engineering, for instance in
computational fluid dynamics (CFD) where it is needed to compute corrections to
the pressure field to ensure the incompressibility of the velocity field. In
the present work, we propose a novel fully convolutional neural network (CNN)
architecture to infer the solution of the Poisson equation on a 2D Cartesian
grid with different resolutions given the right hand side term, arbitrary
boundary conditions and grid parameters. It provides unprecedented versatility
for a CNN approach dealing with partial differential equations. The boundary
conditions are handled using a novel approach by decomposing the original
Poisson problem into a homogeneous Poisson problem plus four inhomogeneous
Laplace sub-problems. The model is trained using a novel loss function
approximating the continuous $L^p$ norm between the prediction and the target.
Even when predicting on grids denser than previously encountered, our model
demonstrates encouraging capacity to reproduce the correct solution profile.
The proposed model, which outperforms well-known neural network models, can be
included in a CFD solver to help with solving the Poisson equation. Analytical
test cases indicate that our CNN architecture is capable of predicting the
correct solution of a Poisson problem with mean percentage errors below 10%, an
improvement by comparison to the first step of conventional iterative methods.
Predictions from our model, used as the initial guess to iterative algorithms
like Multigrid, can reduce the RMS error after a single iteration by more than
90% compared to a zero initial guess.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Ozbay_A/0/1/0/all/0/1"&gt;Ali Girayhan &amp;#xd6;zbay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Hamzehloo_A/0/1/0/all/0/1"&gt;Arash Hamzehloo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Laizet_S/0/1/0/all/0/1"&gt;Sylvain Laizet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Tzirakis_P/0/1/0/all/0/1"&gt;Panagiotis Tzirakis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Rizos_G/0/1/0/all/0/1"&gt;Georgios Rizos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Schuller_B/0/1/0/all/0/1"&gt;Bj&amp;#xf6;rn Schuller&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Impact of lung segmentation on the diagnosis and explanation of COVID-19 in chest X-ray images. (arXiv:2009.09780v3 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.09780</id>
        <link href="http://arxiv.org/abs/2009.09780"/>
        <updated>2021-06-02T02:50:02.496Z</updated>
        <summary type="html"><![CDATA[The COVID-19 pandemic is undoubtedly one of the biggest public health crises
our society has ever faced in recent history. One of the main complications
caused by COVID-19 is pneumonia, which is diagnosed using imaging exams, such
as chest X-ray (CXR) and computed tomography (CT) scan. The CT scan is more
precise than the CXR. However, CXR is suitable in particular situations because
it is cheaper, faster, more widespread, and exposes the patient to less
radiation. This study aims to demonstrate the impact of lung segmentation in
COVID-19 identification using CXR images and evaluate which contents of the
image decisively contribute to its identification. We performed the lung
segmentation using a U-Net CNN architecture, and the classification using three
well-known CNN architectures: VGG, ResNet, and Inception. To estimate the
impact of lung segmentation, we applied some Explainable Artificial
Intelligence (XAI) techniques, specifically LIME and Grad-CAM. To empirically
evaluate our approach, we composed a database with three classes: lung opacity
(pneumonia), COVID-19, and normal. The segmentation achieved a Jaccard distance
of 0.034 and a Dice coefficient of 0.982. The classification using segmented
lung achieved an F1-Score of 0.88 for the multi-class setup and 0.83 for
COVID-19 identification. Further testing and XAI techniques suggest that
segmented CXR images represent a much more realistic and less biased
performance. To the best of our knowledge, no other work tried to estimate the
impact of lung segmentation in COVID-19 identification using comprehensive XAI
techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Teixeira_L/0/1/0/all/0/1"&gt;Lucas O. Teixeira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Pereira_R/0/1/0/all/0/1"&gt;Rodolfo M. Pereira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bertolini_D/0/1/0/all/0/1"&gt;Diego Bertolini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Oliveira_L/0/1/0/all/0/1"&gt;Luiz S. Oliveira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Nanni_L/0/1/0/all/0/1"&gt;Loris Nanni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cavalcanti_G/0/1/0/all/0/1"&gt;George D. C. Cavalcanti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Costa_Y/0/1/0/all/0/1"&gt;Yandre M. G. Costa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NewsEmbed: Modeling News through Pre-trained DocumentRepresentations. (arXiv:2106.00590v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.00590</id>
        <link href="http://arxiv.org/abs/2106.00590"/>
        <updated>2021-06-02T02:50:02.495Z</updated>
        <summary type="html"><![CDATA[Effectively modeling text-rich fresh content such as news articles at
document-level is a challenging problem. To ensure a content-based model
generalize well to a broad range of applications, it is critical to have a
training dataset that is large beyond the scale of human labels while achieving
desired quality. In this work, we address those two challenges by proposing a
novel approach to mine semantically-relevant fresh documents, and their topic
labels, with little human supervision. Meanwhile, we design a multitask model
called NewsEmbed that alternatively trains a contrastive learning with a
multi-label classification to derive a universal document encoder. We show that
the proposed approach can provide billions of high quality organic training
examples and can be naturally extended to multilingual setting where texts in
different languages are encoded in the same semantic space. We experimentally
demonstrate NewsEmbed's competitive performance across multiple natural
language understanding tasks, both supervised and unsupervised.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jialu Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tianqi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1"&gt;Cong Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[OpenBox: A Generalized Black-box Optimization Service. (arXiv:2106.00421v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.00421</id>
        <link href="http://arxiv.org/abs/2106.00421"/>
        <updated>2021-06-02T02:50:02.493Z</updated>
        <summary type="html"><![CDATA[Black-box optimization (BBO) has a broad range of applications, including
automatic machine learning, engineering, physics, and experimental design.
However, it remains a challenge for users to apply BBO methods to their
problems at hand with existing software packages, in terms of applicability,
performance, and efficiency. In this paper, we build OpenBox, an open-source
and general-purpose BBO service with improved usability. The modular design
behind OpenBox also facilitates flexible abstraction and optimization of basic
BBO components that are common in other existing systems. OpenBox is
distributed, fault-tolerant, and scalable. To improve efficiency, OpenBox
further utilizes "algorithm agnostic" parallelization and transfer learning.
Our experimental results demonstrate the effectiveness and efficiency of
OpenBox compared to existing systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1"&gt;Yu Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Wentao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yuanwei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1"&gt;Huaijun Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1"&gt;Mingchao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1"&gt;Jiawei Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1"&gt;Jinyang Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1"&gt;Wentao Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Zhi Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Ce Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_B/0/1/0/all/0/1"&gt;Bin Cui&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Wide-minima Density Hypothesis and the Explore-Exploit Learning Rate Schedule. (arXiv:2003.03977v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.03977</id>
        <link href="http://arxiv.org/abs/2003.03977"/>
        <updated>2021-06-02T02:50:02.489Z</updated>
        <summary type="html"><![CDATA[Several papers argue that wide minima generalize better than narrow minima.
In this paper, through detailed experiments that not only corroborate the
generalization properties of wide minima, we also provide empirical evidence
for a new hypothesis that the density of wide minima is likely lower than the
density of narrow minima. Further, motivated by this hypothesis, we design a
novel explore-exploit learning rate schedule. On a variety of image and natural
language datasets, compared to their original hand-tuned learning rate
baselines, we show that our explore-exploit schedule can result in either up to
0.84% higher absolute accuracy using the original training budget or up to 57%
reduced training time while achieving the original reported accuracy. For
example, we achieve state-of-the-art (SOTA) accuracy for IWSLT'14 (DE-EN)
dataset by just modifying the learning rate schedule of a high performing
model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Iyer_N/0/1/0/all/0/1"&gt;Nikhil Iyer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thejas_V/0/1/0/all/0/1"&gt;V Thejas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kwatra_N/0/1/0/all/0/1"&gt;Nipun Kwatra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramjee_R/0/1/0/all/0/1"&gt;Ramachandran Ramjee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sivathanu_M/0/1/0/all/0/1"&gt;Muthian Sivathanu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Counterfactual Invariance to Spurious Correlations: Why and How to Pass Stress Tests. (arXiv:2106.00545v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.00545</id>
        <link href="http://arxiv.org/abs/2106.00545"/>
        <updated>2021-06-02T02:50:02.474Z</updated>
        <summary type="html"><![CDATA[Informally, a `spurious correlation' is the dependence of a model on some
aspect of the input data that an analyst thinks shouldn't matter. In machine
learning, these have a know-it-when-you-see-it character; e.g., changing the
gender of a sentence's subject changes a sentiment predictor's output. To check
for spurious correlations, we can `stress test' models by perturbing irrelevant
parts of input data and seeing if model predictions change. In this paper, we
study stress testing using the tools of causal inference. We introduce
\emph{counterfactual invariance} as a formalization of the requirement that
changing irrelevant parts of the input shouldn't change model predictions. We
connect counterfactual invariance to out-of-domain model performance, and
provide practical schemes for learning (approximately) counterfactual invariant
predictors (without access to counterfactual examples). It turns out that both
the means and implications of counterfactual invariance depend fundamentally on
the true underlying causal structure of the data. Distinct causal structures
require distinct regularization schemes to induce counterfactual invariance.
Similarly, counterfactual invariance implies different domain shift guarantees
depending on the underlying causal structure. This theory is supported by
empirical results on text classification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Veitch_V/0/1/0/all/0/1"&gt;Victor Veitch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+DAmour_A/0/1/0/all/0/1"&gt;Alexander D&amp;#x27;Amour&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yadlowsky_S/0/1/0/all/0/1"&gt;Steve Yadlowsky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eisenstein_J/0/1/0/all/0/1"&gt;Jacob Eisenstein&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fast and Eager k-Medoids Clustering: O(k) Runtime Improvement of the PAM, CLARA, and CLARANS Algorithms. (arXiv:2008.05171v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.05171</id>
        <link href="http://arxiv.org/abs/2008.05171"/>
        <updated>2021-06-02T02:50:02.474Z</updated>
        <summary type="html"><![CDATA[Clustering non-Euclidean data is difficult, and one of the most used
algorithms besides hierarchical clustering is the popular algorithm
Partitioning Around Medoids (PAM), also simply referred to as k-medoids
clustering. In Euclidean geometry the mean-as used in k-means-is a good
estimator for the cluster center, but this does not exist for arbitrary
dissimilarities. PAM uses the medoid instead, the object with the smallest
dissimilarity to all others in the cluster. This notion of centrality can be
used with any (dis-)similarity, and thus is of high relevance to many domains
and applications. A key issue with PAM is its high run time cost. We propose
modifications to the PAM algorithm that achieve an O(k)-fold speedup in the
second ("SWAP") phase of the algorithm, but will still find the same results as
the original PAM algorithm. If we relax the choice of swaps performed (while
retaining comparable quality), we can further accelerate the algorithm by
eagerly performing additional swaps in each iteration. With the substantially
faster SWAP, we can now explore faster initialization strategies, because (i)
the classic ("BUILD") initialization now becomes the bottleneck, and (ii) our
swap is fast enough to compensate for worse starting conditions. We also show
how the CLARA and CLARANS algorithms benefit from the proposed modifications.
While we do not study the parallelization of our approach in this work, it can
easily be combined with earlier approaches to use PAM and CLARA on big data
(some of which use PAM as a subroutine, hence can immediately benefit from
these improvements), where the performance with high k becomes increasingly
important. In experiments on real data with k=100,200, we observed a 458x
respectively 1191x speedup compared to the original PAM SWAP algorithm, making
PAM applicable to larger data sets, and in particular to higher k.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schubert_E/0/1/0/all/0/1"&gt;Erich Schubert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rousseeuw_P/0/1/0/all/0/1"&gt;Peter J. Rousseeuw&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pose Invariant Person Re-Identification using Robust Pose-transformation GAN. (arXiv:2105.00930v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.00930</id>
        <link href="http://arxiv.org/abs/2105.00930"/>
        <updated>2021-06-02T02:50:02.467Z</updated>
        <summary type="html"><![CDATA[The objective of person re-identification (re-ID) is to retrieve a person's
images from an image gallery, given a single instance of the person of
interest. Despite several advancements, learning discriminative
identity-sensitive and viewpoint invariant features for robust Person
Re-identification is a major challenge owing to the large pose variation of
humans. This paper proposes a re-ID pipeline that utilizes the image generation
capability of Generative Adversarial Networks combined with pose clustering and
feature fusion to achieve pose invariant feature learning. The objective is to
model a given person under different viewpoints and large pose changes and
extract the most discriminative features from all the appearances. The pose
transformational GAN (pt-GAN) module is trained to generate a person's image in
any given pose. In order to identify the most significant poses for
discriminative feature extraction, a Pose Clustering module is proposed. The
given instance of the person is modelled in varying poses and these features
are effectively combined through the Feature Fusion Network. The final re-ID
model consisting of these 3 sub-blocks, alleviates the pose dependence in
person re-ID. Also, The proposed model is robust to occlusion, scale, rotation
and illumination, providing a framework for viewpoint invariant feature
learning. The proposed method outperforms the state-of-the-art GAN based models
in 4 benchmark datasets. It also surpasses the state-of-the-art models that
report higher re-ID accuracy in terms of improvement over baseline.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Karmakar_A/0/1/0/all/0/1"&gt;Arnab Karmakar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mishra_D/0/1/0/all/0/1"&gt;Deepak Mishra&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LIFT-SLAM: a deep-learning feature-based monocular visual SLAM method. (arXiv:2104.00099v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.00099</id>
        <link href="http://arxiv.org/abs/2104.00099"/>
        <updated>2021-06-02T02:50:02.464Z</updated>
        <summary type="html"><![CDATA[The Simultaneous Localization and Mapping (SLAM) problem addresses the
possibility of a robot to localize itself in an unknown environment and
simultaneously build a consistent map of this environment. Recently, cameras
have been successfully used to get the environment's features to perform SLAM,
which is referred to as visual SLAM (VSLAM). However, classical VSLAM
algorithms can be easily induced to fail when either the motion of the robot or
the environment is too challenging. Although new approaches based on Deep
Neural Networks (DNNs) have achieved promising results in VSLAM, they still are
unable to outperform traditional methods. To leverage the robustness of deep
learning to enhance traditional VSLAM systems, we propose to combine the
potential of deep learning-based feature descriptors with the traditional
geometry-based VSLAM, building a new VSLAM system called LIFT-SLAM. Experiments
conducted on KITTI and Euroc datasets show that deep learning can be used to
improve the performance of traditional VSLAM systems, as the proposed approach
was able to achieve results comparable to the state-of-the-art while being
robust to sensorial noise. We enhance the proposed VSLAM pipeline by avoiding
parameter tuning for specific datasets with an adaptive approach while
evaluating how transfer learning can affect the quality of the features
extracted.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bruno_H/0/1/0/all/0/1"&gt;Hudson M. S. Bruno&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Colombini_E/0/1/0/all/0/1"&gt;Esther L. Colombini&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[3D Object Detection from a Single Fisheye Image Without a Single Fisheye Training Image. (arXiv:2003.03759v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.03759</id>
        <link href="http://arxiv.org/abs/2003.03759"/>
        <updated>2021-06-02T02:50:02.463Z</updated>
        <summary type="html"><![CDATA[Existing monocular 3D object detection methods have been demonstrated on
rectilinear perspective images and fail in images with alternative projections
such as those acquired by fisheye cameras. Previous works on object detection
in fisheye images have focused on 2D object detection, partly due to the lack
of 3D datasets of such images. In this work, we show how to use existing
monocular 3D object detection models, trained only on rectilinear images, to
detect 3D objects in images from fisheye cameras, without using any fisheye
training data. We outperform the only existing method for monocular 3D object
detection in panoramas on a benchmark of synthetic data, despite the fact that
the existing method trains on the target non-rectilinear projection whereas we
train only on rectilinear images. We also experiment with an internal dataset
of real fisheye images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Plaut_E/0/1/0/all/0/1"&gt;Elad Plaut&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yaacov_E/0/1/0/all/0/1"&gt;Erez Ben Yaacov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shlomo_B/0/1/0/all/0/1"&gt;Bat El Shlomo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Diverse Image Inpainting with Bidirectional and Autoregressive Transformers. (arXiv:2104.12335v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.12335</id>
        <link href="http://arxiv.org/abs/2104.12335"/>
        <updated>2021-06-02T02:50:02.462Z</updated>
        <summary type="html"><![CDATA[Image inpainting is an underdetermined inverse problem, which naturally
allows diverse contents to fill up the missing or corrupted regions
realistically. Prevalent approaches using convolutional neural networks (CNNs)
can synthesize visually pleasant contents, but CNNs suffer from limited
perception fields for capturing global features. With image-level attention,
transformers enable to model long-range dependencies and generate diverse
contents with autoregressive modeling of pixel-sequence distributions. However,
the unidirectional attention in autoregressive transformers is suboptimal as
corrupted image regions may have arbitrary shapes with contexts from any
direction. We propose BAT-Fill, an innovative image inpainting framework that
introduces a novel bidirectional autoregressive transformer (BAT) for image
inpainting. BAT utilizes the transformers to learn autoregressive
distributions, which naturally allows the diverse generation of missing
contents. In addition, it incorporates the masked language model like BERT,
which enables bidirectionally modeling of contextual information of missing
regions for better image completion. Extensive experiments over multiple
datasets show that BAT-Fill achieves superior diversity and fidelity in image
inpainting qualitatively and quantitatively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1"&gt;Yingchen Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhan_F/0/1/0/all/0/1"&gt;Fangneng Zhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1"&gt;Rongliang Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1"&gt;Jianxiong Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_K/0/1/0/all/0/1"&gt;Kaiwen Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1"&gt;Shijian Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_F/0/1/0/all/0/1"&gt;Feiying Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1"&gt;Xuansong Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Miao_C/0/1/0/all/0/1"&gt;Chunyan Miao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Table Tennis Stroke Recognition Using Two-Dimensional Human Pose Estimation. (arXiv:2104.09907v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.09907</id>
        <link href="http://arxiv.org/abs/2104.09907"/>
        <updated>2021-06-02T02:50:02.418Z</updated>
        <summary type="html"><![CDATA[We introduce a novel method for collecting table tennis video data and
perform stroke detection and classification. A diverse dataset containing video
data of 11 basic strokes obtained from 14 professional table tennis players,
summing up to a total of 22111 videos has been collected using the proposed
setup. The temporal convolutional neural network model developed using 2D pose
estimation performs multiclass classification of these 11 table tennis strokes
with a validation accuracy of 99.37%. Moreover, the neural network generalizes
well over the data of a player excluded from the training and validation
dataset, classifying the fresh strokes with an overall best accuracy of 98.72%.
Various model architectures using machine learning and deep learning based
approaches have been trained for stroke recognition and their performances have
been compared and benchmarked. Inferences such as performance monitoring and
stroke comparison of the players using the model have been discussed.
Therefore, we are contributing to the development of a computer vision based
sports analytics system for the sport of table tennis that focuses on the
previously unexploited aspect of the sport i.e., a player's strokes, which is
extremely insightful for performance improvement.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kulkarni_K/0/1/0/all/0/1"&gt;Kaustubh Milind Kulkarni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shenoy_S/0/1/0/all/0/1"&gt;Sucheth Shenoy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CF-GNNExplainer: Counterfactual Explanations for Graph Neural Networks. (arXiv:2102.03322v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.03322</id>
        <link href="http://arxiv.org/abs/2102.03322"/>
        <updated>2021-06-02T02:50:02.418Z</updated>
        <summary type="html"><![CDATA[Given the increasing promise of Graph Neural Networks (GNNs) in real-world
applications, several methods have been developed for explaining their
predictions. So far, these methods have primarily focused on generating
subgraphs that are especially relevant for a particular prediction. However,
such methods do not provide a clear opportunity for recourse: given a
prediction, we want to understand how the prediction can be changed in order to
achieve a more desirable outcome. In this work, we propose a method for
generating counterfactual (CF) explanations for GNNs: the minimal perturbation
to the input (graph) data such that the prediction changes. Using only edge
deletions, we find that our method, CF-GNNExplainer can generate CF
explanations for the majority of instances across three widely used datasets
for GNN explanations, while removing less than 3 edges on average, with at
least 94\% accuracy. This indicates that CF-GNNExplainer primarily removes
edges that are crucial for the original predictions, resulting in minimal CF
explanations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lucic_A/0/1/0/all/0/1"&gt;Ana Lucic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoeve_M/0/1/0/all/0/1"&gt;Maartje ter Hoeve&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tolomei_G/0/1/0/all/0/1"&gt;Gabriele Tolomei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rijke_M/0/1/0/all/0/1"&gt;Maarten de Rijke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Silvestri_F/0/1/0/all/0/1"&gt;Fabrizio Silvestri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LayoutVAE: Stochastic Scene Layout Generation From a Label Set. (arXiv:1907.10719v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1907.10719</id>
        <link href="http://arxiv.org/abs/1907.10719"/>
        <updated>2021-06-02T02:50:02.417Z</updated>
        <summary type="html"><![CDATA[Recently there is an increasing interest in scene generation within the
research community. However, models used for generating scene layouts from
textual description largely ignore plausible visual variations within the
structure dictated by the text. We propose LayoutVAE, a variational autoencoder
based framework for generating stochastic scene layouts. LayoutVAE is a
versatile modeling framework that allows for generating full image layouts
given a label set, or per label layouts for an existing image given a new
label. In addition, it is also capable of detecting unusual layouts,
potentially providing a way to evaluate layout generation problem. Extensive
experiments on MNIST-Layouts and challenging COCO 2017 Panoptic dataset
verifies the effectiveness of our proposed framework.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jyothi_A/0/1/0/all/0/1"&gt;Akash Abdu Jyothi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Durand_T/0/1/0/all/0/1"&gt;Thibaut Durand&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1"&gt;Jiawei He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sigal_L/0/1/0/all/0/1"&gt;Leonid Sigal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mori_G/0/1/0/all/0/1"&gt;Greg Mori&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Moving SLAM: Fully Unsupervised Deep Learning in Non-Rigid Scenes. (arXiv:2105.02195v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.02195</id>
        <link href="http://arxiv.org/abs/2105.02195"/>
        <updated>2021-06-02T02:50:02.415Z</updated>
        <summary type="html"><![CDATA[We propose a method to train deep networks to decompose videos into 3D
geometry (camera and depth), moving objects, and their motions, with no
supervision. We build on the idea of view synthesis, which uses classical
camera geometry to re-render a source image from a different point-of-view,
specified by a predicted relative pose and depth map. By minimizing the error
between the synthetic image and the corresponding real image in a video, the
deep network that predicts pose and depth can be trained completely
unsupervised. However, the view synthesis equations rely on a strong
assumption: that objects do not move. This rigid-world assumption limits the
predictive power, and rules out learning about objects automatically. We
propose a simple solution: minimize the error on small regions of the image
instead. While the scene as a whole may be non-rigid, it is always possible to
find small regions that are approximately rigid, such as inside a moving
object. Our network can then predict different poses for each region, in a
sliding window from a learned dense pose map. This represents a significantly
richer model, including 6D object motions, with little additional complexity.
We achieve very competitive performance on unsupervised odometry and depth
prediction on KITTI. We also demonstrate new capabilities on EPIC-Kitchens, a
challenging dataset of indoor videos, where there is no ground truth
information for depth, odometry, object segmentation or motion. Yet all are
recovered automatically by our method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1"&gt;Dan Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vedaldi_A/0/1/0/all/0/1"&gt;Andrea Vedaldi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Henriques_J/0/1/0/all/0/1"&gt;Joao F. Henriques&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Clustering-friendly Representation Learning via Instance Discrimination and Feature Decorrelation. (arXiv:2106.00131v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.00131</id>
        <link href="http://arxiv.org/abs/2106.00131"/>
        <updated>2021-06-02T02:50:02.414Z</updated>
        <summary type="html"><![CDATA[Clustering is one of the most fundamental tasks in machine learning.
Recently, deep clustering has become a major trend in clustering techniques.
Representation learning often plays an important role in the effectiveness of
deep clustering, and thus can be a principal cause of performance degradation.
In this paper, we propose a clustering-friendly representation learning method
using instance discrimination and feature decorrelation. Our
deep-learning-based representation learning method is motivated by the
properties of classical spectral clustering. Instance discrimination learns
similarities among data and feature decorrelation removes redundant correlation
among features. We utilize an instance discrimination method in which learning
individual instance classes leads to learning similarity among instances.
Through detailed experiments and examination, we show that the approach can be
adapted to learning a latent space for clustering. We design novel
softmax-formulated decorrelation constraints for learning. In evaluations of
image clustering using CIFAR-10 and ImageNet-10, our method achieves accuracy
of 81.5% and 95.4%, respectively. We also show that the softmax-formulated
constraints are compatible with various neural networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tao_Y/0/1/0/all/0/1"&gt;Yaling Tao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Takagi_K/0/1/0/all/0/1"&gt;Kentaro Takagi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nakata_K/0/1/0/all/0/1"&gt;Kouta Nakata&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimal transport with $f$-divergence regularization and generalized Sinkhorn algorithm. (arXiv:2105.14337v1 [math.OC] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2105.14337</id>
        <link href="http://arxiv.org/abs/2105.14337"/>
        <updated>2021-06-02T02:50:02.414Z</updated>
        <summary type="html"><![CDATA[Entropic regularization provides a generalization of the original optimal
transport problem. It introduces a penalty term defined by the Kullback-Leibler
divergence, making the problem more tractable via the celebrated Sinkhorn
algorithm. Replacing the Kullback-Leibler divergence with a general
$f$-divergence leads to a natural generalization. Using convex analysis, we
extend the theory developed so far to include $f$-divergences defined by
functions of Legendre type, and prove that under some mild conditions, strong
duality holds, optimums in both the primal and dual problems are attained, the
generalization of the $c$-transform is well-defined, and we give sufficient
conditions for the generalized Sinkhorn algorithm to converge to an optimal
solution. We propose a practical algorithm for computing the regularized
optimal transport cost and its gradient via the generalized Sinkhorn algorithm.
Finally, we present experimental results on synthetic 2-dimensional data,
demonstrating the effects of using different $f$-divergences for
regularization, which influences convergence speed, numerical stability and
sparsity of the optimal coupling.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Terjek_D/0/1/0/all/0/1"&gt;D&amp;#xe1;vid Terj&amp;#xe9;k&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/math/1/au:+Gonzalez_Sanchez_D/0/1/0/all/0/1"&gt;Diego Gonz&amp;#xe1;lez-S&amp;#xe1;nchez&lt;/a&gt; (1) ((1) Alfr&amp;#xe9;d R&amp;#xe9;nyi Institute of Mathematics)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Continual 3D Convolutional Neural Networks for Real-time Processing of Videos. (arXiv:2106.00050v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.00050</id>
        <link href="http://arxiv.org/abs/2106.00050"/>
        <updated>2021-06-02T02:50:02.413Z</updated>
        <summary type="html"><![CDATA[This paper introduces Continual 3D Convolutional Neural Networks (Co3D CNNs),
a new computational formulation of spatio-temporal 3D CNNs, in which videos are
processed frame-by-frame rather than by clip. In online processing tasks
demanding frame-wise predictions, Co3D CNNs dispense with the computational
redundancies of regular 3D CNNs, namely the repeated convolutions over frames,
which appear in multiple clips. While yielding an order of magnitude in
computational savings, Co3D CNNs have memory requirements comparable with that
of corresponding regular 3D CNNs and are less affected by changes in the size
of the temporal receptive field. We show that Continual 3D CNNs initialised on
the weights from preexisting state-of-the-art video recognition models reduce
the floating point operations for frame-wise computations by 10.0-12.4x while
improving accuracy on Kinetics-400 by 2.3-3.8. Moreover, we investigate the
transient start-up response of Co3D CNNs and perform an extensive benchmark of
online processing speed as well as accuracy for publicly available
state-of-the-art 3D CNNs on modern hardware.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hedegaard_L/0/1/0/all/0/1"&gt;Lukas Hedegaard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iosifidis_A/0/1/0/all/0/1"&gt;Alexandros Iosifidis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Variational Autoencoders: A Harmonic Perspective. (arXiv:2105.14866v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14866</id>
        <link href="http://arxiv.org/abs/2105.14866"/>
        <updated>2021-06-02T02:50:02.413Z</updated>
        <summary type="html"><![CDATA[In this work we study Variational Autoencoders (VAEs) from the perspective of
harmonic analysis. By viewing a VAE's latent space as a Gaussian Space, a
variety of measure space, we derive a series of results that show that the
encoder variance of a VAE controls the frequency content of the functions
parameterised by the VAE encoder and decoder neural networks. In particular we
demonstrate that larger encoder variances reduce the high frequency content of
these functions. Our analysis allows us to show that increasing this variance
effectively induces a soft Lipschitz constraint on the decoder network of a
VAE, which is a core contributor to the adversarial robustness of VAEs. We
further demonstrate that adding Gaussian noise to the input of a VAE allows us
to more finely control the frequency content and the Lipschitz constant of the
VAE encoder networks. To support our theoretical analysis we run experiments
with VAEs with small fully-connected neural networks and with larger
convolutional networks, demonstrating empirically that our theory holds for a
variety of neural network architectures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Camuto_A/0/1/0/all/0/1"&gt;Alexander Camuto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Willetts_M/0/1/0/all/0/1"&gt;Matthew Willetts&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Parallelized Computation and Backpropagation Under Angle-Parametrized Orthogonal Matrices. (arXiv:2106.00003v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.00003</id>
        <link href="http://arxiv.org/abs/2106.00003"/>
        <updated>2021-06-02T02:50:02.399Z</updated>
        <summary type="html"><![CDATA[We present a methodology for parallel acceleration of learning in the
presence of matrix orthogonality and unitarity constraints of interest in
several branches of machine learning. We show how an apparently sequential
elementary rotation parametrization can be restructured into blocks of
commutative operations using a well-known tool for coloring the edges of
complete graphs, in turn widely applied to schedule round-robin
(all-against-all) sports tournaments. The resulting decomposition admits an
algorithm to compute a fully-parametrized orthogonal matrix from its rotation
parameters in $O(n)$ sequential steps and one to compute the gradient of a
training loss with respect to its parameters in $O(n\log n)$ steps. We discuss
parametric restrictions of interest to generative modeling and present
promising performance results with a prototype GPU implementation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hamze_F/0/1/0/all/0/1"&gt;Firas Hamze&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tesseract: Tensorised Actors for Multi-Agent Reinforcement Learning. (arXiv:2106.00136v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.00136</id>
        <link href="http://arxiv.org/abs/2106.00136"/>
        <updated>2021-06-02T02:50:02.399Z</updated>
        <summary type="html"><![CDATA[Reinforcement Learning in large action spaces is a challenging problem.
Cooperative multi-agent reinforcement learning (MARL) exacerbates matters by
imposing various constraints on communication and observability. In this work,
we consider the fundamental hurdle affecting both value-based and
policy-gradient approaches: an exponential blowup of the action space with the
number of agents. For value-based methods, it poses challenges in accurately
representing the optimal value function. For policy gradient methods, it makes
training the critic difficult and exacerbates the problem of the lagging
critic. We show that from a learning theory perspective, both problems can be
addressed by accurately representing the associated action-value function with
a low-complexity hypothesis class. This requires accurately modelling the agent
interactions in a sample efficient way. To this end, we propose a novel
tensorised formulation of the Bellman equation. This gives rise to our method
Tesseract, which views the Q-function as a tensor whose modes correspond to the
action spaces of different agents. Algorithms derived from Tesseract decompose
the Q-tensor across agents and utilise low-rank tensor approximations to model
agent interactions relevant to the task. We provide PAC analysis for
Tesseract-based algorithms and highlight their relevance to the class of rich
observation MDPs. Empirical results in different domains confirm Tesseract's
gains in sample efficiency predicted by the theory.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mahajan_A/0/1/0/all/0/1"&gt;Anuj Mahajan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Samvelyan_M/0/1/0/all/0/1"&gt;Mikayel Samvelyan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mao_L/0/1/0/all/0/1"&gt;Lei Mao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Makoviychuk_V/0/1/0/all/0/1"&gt;Viktor Makoviychuk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Garg_A/0/1/0/all/0/1"&gt;Animesh Garg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kossaifi_J/0/1/0/all/0/1"&gt;Jean Kossaifi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Whiteson_S/0/1/0/all/0/1"&gt;Shimon Whiteson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yuke Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1"&gt;Animashree Anandkumar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AN-GCN: An Anonymous Graph Convolutional Network Defense Against Edge-Perturbing Attack. (arXiv:2005.03482v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.03482</id>
        <link href="http://arxiv.org/abs/2005.03482"/>
        <updated>2021-06-02T02:50:02.398Z</updated>
        <summary type="html"><![CDATA[Node classification based on graph convolutional networks (GCNs) is
vulnerable to adversarial attacks by maliciously perturbing graph structures,
such as inserting or deleting graph edges. The existing research works do not
seem to be able to unify the formulation of such edge-perturbing attacks, so it
is unable to design a more essential defense scheme. Thus, in this paper,
considering that most researchers find the attack scheme by ergodically
perturbing edge in a diverse and manual way, we unify such edge-perturbing
attacks as an automatic general attack model, named edge-reading attack (ERA).
ERA can find the concealed and high success rate attack scheme by automatically
traverse and perturb edges repeatedly. ERA is also the unified description form
of edge-perturbing attacks in the form of the mathematical formula. Relying on
ERA, we further demonstrate the vulnerability of GCNs, i.e., the edge-reading
permission can easily create opportunities for adversarial attacks. To address
this problem, we propose an anonymous graph convolutional network (AN-GCN),
which allows classifying nodes without reading the edge information of GCNs.
Specifically, we propose the node localization theorem for the first time to
demonstrate how GCN locates nodes during training. Then, AN-GCN is designed to
make the nodes participate in the prediction anonymously, thus withdrawing the
edge-reading permission of the model. Since AN-GCN can predict node categories
without edge information, the administrator can withdraw the read permission of
edge information to all roles (including attackers), so attackers will lose the
basic condition of injecting edge perturbations. Extensive evaluations show
that, our proposed general attack model can accurately manipulate the
classification results of the target nodes, thus maintaining high-level
security in defending against edge-perturbing adversarial attacks on graph]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1"&gt;Ao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Beibei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1"&gt;Tao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1"&gt;Pan Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Effect of large-scale pre-training on full and few-shot transfer learning for natural and medical images. (arXiv:2106.00116v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.00116</id>
        <link href="http://arxiv.org/abs/2106.00116"/>
        <updated>2021-06-02T02:50:02.397Z</updated>
        <summary type="html"><![CDATA[Transfer learning aims to exploit pre-trained models for more efficient
follow-up training on wide range of downstream tasks and datasets, enabling
successful training also on small data. Recent line of work posits strong
benefits for model generalization and transfer when model size, data size, and
compute budget are increased for the pre-training. It remains however still
largely unclear whether the observed transfer improvement due to increase in
scale also holds when source and target data distributions are far apart from
each other. In this work we conduct large-scale pre-training on large source
datasets of either natural (ImageNet-21k/1k) or medical chest X-Ray images and
compare full and few-shot transfer using different target datasets from both
natural and medical imaging domains. Our observations provide evidence that
while pre-training and transfer on closely related datasets do show clear
benefit of increasing model and data size during pre-training, such benefits
are not clearly visible when source and target datasets are further apart.
These observations hold across both full and few-shot transfer and indicate
that scaling laws hinting improvement of generalization and transfer with
increasing model and data size are incomplete and should also take into account
the degree of how distinct the source and target data distributions are, to
correctly predict effect of model size and data size variation during
pre-training on transfer. (Repository for reproducing the experiments will be
made available.)]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cherti_M/0/1/0/all/0/1"&gt;Mehdi Cherti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jitsev_J/0/1/0/all/0/1"&gt;Jenia Jitsev&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Real-time and Light-weight Line Segment Detection. (arXiv:2106.00186v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.00186</id>
        <link href="http://arxiv.org/abs/2106.00186"/>
        <updated>2021-06-02T02:50:02.397Z</updated>
        <summary type="html"><![CDATA[Previous deep learning-based line segment detection (LSD) suffer from the
immense model size and high computational cost for line prediction. This
constrains them from real-time inference on computationally restricted
environments. In this paper, we propose a real-time and light-weight line
segment detector for resource-constrained environments named Mobile LSD
(M-LSD). We design an extremely efficient LSD architecture by minimizing the
backbone network and removing the typical multi-module process for line
prediction in previous methods. To maintain competitive performance with such a
light-weight network, we present novel training schemes: Segments of Line
segment (SoL) augmentation and geometric learning scheme. SoL augmentation
splits a line segment into multiple subparts, which are used to provide
auxiliary line data during the training process. Moreover, the geometric
learning scheme allows a model to capture additional geometry cues from
matching loss, junction and line segmentation, length and degree regression.
Compared with TP-LSD-Lite, previously the best real-time LSD method, our model
(M-LSD-tiny) achieves competitive performance with 2.5% of model size and an
increase of 130.5% in inference speed on GPU when evaluated with Wireframe and
YorkUrban datasets. Furthermore, our model runs at 56.8 FPS and 48.6 FPS on
Android and iPhone mobile devices, respectively. To the best of our knowledge,
this is the first real-time deep LSD method available on mobile devices.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gu_G/0/1/0/all/0/1"&gt;Geonmo Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ko_B/0/1/0/all/0/1"&gt;Byungsoo Ko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Go_S/0/1/0/all/0/1"&gt;SeoungHyun Go&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1"&gt;Sung-Hyun Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jingeun Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shin_M/0/1/0/all/0/1"&gt;Minchul Shin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hypothesis Testing for Class-Conditional Label Noise. (arXiv:2103.02630v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.02630</id>
        <link href="http://arxiv.org/abs/2103.02630"/>
        <updated>2021-06-02T02:50:02.397Z</updated>
        <summary type="html"><![CDATA[In this paper we provide machine learning practitioners with tools to answer
the question: is there class-conditional noise in my labels? In particular, we
present hypothesis tests to check whether a given dataset of instance-label
pairs has been corrupted with class-conditional label noise, as opposed to
uniform label noise, with the former biasing learning, while the latter --
under mild conditions -- does not. The outcome of these tests can then be used
in conjunction with other information to assess further steps. While previous
works explore the direct estimation of the noise rates, this is known to be
hard in practice and does not offer a real understanding of how trustworthy the
estimates are. These methods typically require anchor points -- examples whose
true posterior is either 0 or 1. Differently, in this paper we assume we have
access to a set of anchor points whose true posterior is approximately 1/2. The
proposed hypothesis tests are built upon the asymptotic properties of Maximum
Likelihood Estimators for Logistic Regression models. We establish the main
properties of the tests, including a theoretical and empirical analysis of the
dependence of the power on the test on the training sample size, the number of
anchor points, the difference of the noise rates and the use of relaxed
anchors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Poyiadzi_R/0/1/0/all/0/1"&gt;Rafael Poyiadzi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1"&gt;Weisong Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Twomey_N/0/1/0/all/0/1"&gt;Niall Twomey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Santos_Rodriguez_R/0/1/0/all/0/1"&gt;Raul Santos-Rodriguez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Duckworth-Lewis-Stern Method Comparison with Machine Learning Approach. (arXiv:2106.00175v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.00175</id>
        <link href="http://arxiv.org/abs/2106.00175"/>
        <updated>2021-06-02T02:50:02.396Z</updated>
        <summary type="html"><![CDATA[This work presents an analysis of the Duckworth-Lewis-Stern (DLS) method for
One Day International (ODI) cricket matches. The accuracy of the DLS method is
compared against various supervised learning algorithms for result prediction.
The result of a cricket match is predicted during the second inning. The paper
also optimized DLS resource table which is used in the Duckworth-Lewis (D/L)
formula to increase its predictive power. Finally, an Unpredictability Index is
developed that ranks different cricket playing nations according to how
unpredictable they are while playing an ODI match.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Abbas_K/0/1/0/all/0/1"&gt;Kumail Abbas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Haider_S/0/1/0/all/0/1"&gt;Sajjad Haider&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MalPhase: Fine-Grained Malware Detection Using Network Flow Data. (arXiv:2106.00541v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2106.00541</id>
        <link href="http://arxiv.org/abs/2106.00541"/>
        <updated>2021-06-02T02:50:02.395Z</updated>
        <summary type="html"><![CDATA[Economic incentives encourage malware authors to constantly develop new,
increasingly complex malware to steal sensitive data or blackmail individuals
and companies into paying large ransoms. In 2017, the worldwide economic impact
of cyberattacks is estimated to be between 445 and 600 billion USD, or 0.8% of
global GDP. Traditionally, one of the approaches used to defend against malware
is network traffic analysis, which relies on network data to detect the
presence of potentially malicious software. However, to keep up with increasing
network speeds and amount of traffic, network analysis is generally limited to
work on aggregated network data, which is traditionally challenging and yields
mixed results. In this paper we present MalPhase, a system that was designed to
cope with the limitations of aggregated flows. MalPhase features a multi-phase
pipeline for malware detection, type and family classification. The use of an
extended set of network flow features and a simultaneous multi-tier
architecture facilitates a performance improvement for deep learning models,
making them able to detect malicious flows (>98% F1) and categorize them to a
respective malware type (>93% F1) and family (>91% F1). Furthermore, the use of
robust features and denoising autoencoders allows MalPhase to perform well on
samples with varying amounts of benign traffic mixed in. Finally, MalPhase
detects unseen malware samples with performance comparable to that of known
samples, even when interlaced with benign flows to reflect realistic network
environments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Piskozub_M/0/1/0/all/0/1"&gt;Michal Piskozub&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gaspari_F/0/1/0/all/0/1"&gt;Fabio De Gaspari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barr_Smith_F/0/1/0/all/0/1"&gt;Frederick Barr-Smith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mancini_L/0/1/0/all/0/1"&gt;Luigi V. Mancini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martinovic_I/0/1/0/all/0/1"&gt;Ivan Martinovic&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transformation Models for Flexible Posteriors in Variational Bayes. (arXiv:2106.00528v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.00528</id>
        <link href="http://arxiv.org/abs/2106.00528"/>
        <updated>2021-06-02T02:50:02.394Z</updated>
        <summary type="html"><![CDATA[The main challenge in Bayesian models is to determine the posterior for the
model parameters. Already, in models with only one or few parameters, the
analytical posterior can only be determined in special settings. In Bayesian
neural networks, variational inference is widely used to approximate
difficult-to-compute posteriors by variational distributions. Usually,
Gaussians are used as variational distributions (Gaussian-VI) which limits the
quality of the approximation due to their limited flexibility. Transformation
models on the other hand are flexible enough to fit any distribution. Here we
present transformation model-based variational inference (TM-VI) and
demonstrate that it allows to accurately approximate complex posteriors in
models with one parameter and also works in a mean-field fashion for
multi-parameter models like neural networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Hortling_S/0/1/0/all/0/1"&gt;Sefan H&amp;#xf6;rtling&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Dold_D/0/1/0/all/0/1"&gt;Daniel Dold&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Durr_O/0/1/0/all/0/1"&gt;Oliver D&amp;#xfc;rr&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Sick_B/0/1/0/all/0/1"&gt;Beate Sick&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust discovery of partial differential equations in complex situations. (arXiv:2106.00008v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.00008</id>
        <link href="http://arxiv.org/abs/2106.00008"/>
        <updated>2021-06-02T02:50:02.388Z</updated>
        <summary type="html"><![CDATA[Data-driven discovery of partial differential equations (PDEs) has achieved
considerable development in recent years. Several aspects of problems have been
resolved by sparse regression-based and neural network-based methods. However,
the performances of existing methods lack stability when dealing with complex
situations, including sparse data with high noise, high-order derivatives and
shock waves, which bring obstacles to calculating derivatives accurately.
Therefore, a robust PDE discovery framework, called the robust deep
learning-genetic algorithm (R-DLGA), that incorporates the physics-informed
neural network (PINN), is proposed in this work. In the framework, a
preliminary result of potential terms provided by the deep learning-genetic
algorithm is added into the loss function of the PINN as physical constraints
to improve the accuracy of derivative calculation. It assists to optimize the
preliminary result and obtain the ultimately discovered PDE by eliminating the
error compensation terms. The stability and accuracy of the proposed R-DLGA in
several complex situations are examined for proof-and-concept, and the results
prove that the proposed framework is able to calculate derivatives accurately
with the optimization of PINN and possesses surprising robustness to complex
situations, including sparse data with high noise, high-order derivatives, and
shock waves.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1"&gt;Hao Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1"&gt;Dongxiao Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TransVOS: Video Object Segmentation with Transformers. (arXiv:2106.00588v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.00588</id>
        <link href="http://arxiv.org/abs/2106.00588"/>
        <updated>2021-06-02T02:50:02.384Z</updated>
        <summary type="html"><![CDATA[Recently, Space-Time Memory Network (STM) based methods have achieved
state-of-the-art performance in semi-supervised video object segmentation
(VOS). A critical problem in this task is how to model the dependency both
among different frames and inside every frame. However, most of these methods
neglect the spatial relationships (inside each frame) and do not make full use
of the temporal relationships (among different frames). In this paper, we
propose a new transformer-based framework, termed TransVOS, introducing a
vision transformer to fully exploit and model both the temporal and spatial
relationships. Moreover, most STM-based approaches employ two disparate
encoders to extract features of two significant inputs, i.e., reference sets
(history frames with predicted masks) and query frame, respectively, increasing
the models' parameters and complexity. To slim the popular two-encoder pipeline
while keeping the effectiveness, we design a single two-path feature extractor
to encode the above two inputs in a unified way. Extensive experiments
demonstrate the superiority of our TransVOS over state-of-the-art methods on
both DAVIS and YouTube-VOS datasets. Codes will be released when it is
published.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mei_J/0/1/0/all/0/1"&gt;Jianbiao Mei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1"&gt;Mengmeng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1"&gt;Yeneng Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yong Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predicting Driver Intention Using Deep Neural Network. (arXiv:2105.14790v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14790</id>
        <link href="http://arxiv.org/abs/2105.14790"/>
        <updated>2021-06-02T02:50:02.384Z</updated>
        <summary type="html"><![CDATA[To improve driving safety and avoid car accidents, Advanced Driver Assistance
Systems (ADAS) are given significant attention. Recent studies have focused on
predicting driver intention as a key part of these systems. In this study, we
proposed new framework in which 4 inputs are employed to anticipate diver
maneuver using Brain4Cars dataset and the maneuver prediction is achieved from
5, 4, 3, 2, 1 seconds before the actual action occurs. We evaluated our
framework in three scenarios: using only 1) inside view 2) outside view and 3)
both inside and outside view. We divided the dataset into training, validation
and test sets, also K-fold cross validation is utilized. Compared with
state-of-the-art studies, our architecture is faster and achieved higher
performance in second and third scenario. Accuracy, precision, recall and
f1-score as evaluation metrics were utilized and the result of 82.41%, 82.28%,
82,42% and 82.24% for outside view and 98.90%, 98.96%, 98.90% and 98.88% for
both inside and outside view were gained, respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bonyani_M/0/1/0/all/0/1"&gt;Mahdi Bonyani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rahmanian_M/0/1/0/all/0/1"&gt;Mina Rahmanian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jahangard_S/0/1/0/all/0/1"&gt;Simindokht Jahangard&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spatio-Temporal Point Processes with Attention for Traffic Congestion Event Modeling. (arXiv:2005.08665v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.08665</id>
        <link href="http://arxiv.org/abs/2005.08665"/>
        <updated>2021-06-02T02:50:02.374Z</updated>
        <summary type="html"><![CDATA[We present a novel framework for modeling traffic congestion events over road
networks. Using multi-modal data by combining count data from traffic sensors
with police reports that report traffic incidents, we aim to capture two types
of triggering effect for congestion events. Current traffic congestion at one
location may cause future congestion over the road network, and traffic
incidents may cause spread traffic congestion. To model the non-homogeneous
temporal dependence of the event on the past, we use a novel attention-based
mechanism based on neural networks embedding for point processes. To
incorporate the directional spatial dependence induced by the road network, we
adapt the "tail-up" model from the context of spatial statistics to the traffic
network setting. We demonstrate our approach's superior performance compared to
the state-of-the-art methods for both synthetic and real data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1"&gt;Shixiang Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_R/0/1/0/all/0/1"&gt;Ruyi Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Minghe Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hentenryck_P/0/1/0/all/0/1"&gt;Pascal Van Hentenryck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1"&gt;Yao Xie&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semi-Supervised Disparity Estimation with Deep Feature Reconstruction. (arXiv:2106.00318v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.00318</id>
        <link href="http://arxiv.org/abs/2106.00318"/>
        <updated>2021-06-02T02:50:02.363Z</updated>
        <summary type="html"><![CDATA[Despite the success of deep learning in disparity estimation, the domain
generalization gap remains an issue. We propose a semi-supervised pipeline that
successfully adapts DispNet to a real-world domain by joint supervised training
on labeled synthetic data and self-supervised training on unlabeled real data.
Furthermore, accounting for the limitations of the widely-used photometric
loss, we analyze the impact of deep feature reconstruction as a promising
supervisory signal for disparity estimation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guerrero_Viu_J/0/1/0/all/0/1"&gt;Julia Guerrero-Viu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Izquierdo_S/0/1/0/all/0/1"&gt;Sergio Izquierdo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schroppel_P/0/1/0/all/0/1"&gt;Philipp Schr&amp;#xf6;ppel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brox_T/0/1/0/all/0/1"&gt;Thomas Brox&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Interpretable Attention Networks for Cervical Cancer Analysis. (arXiv:2106.00557v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.00557</id>
        <link href="http://arxiv.org/abs/2106.00557"/>
        <updated>2021-06-02T02:50:02.363Z</updated>
        <summary type="html"><![CDATA[Recent advances in deep learning have enabled the development of automated
frameworks for analysing medical images and signals, including analysis of
cervical cancer. Many previous works focus on the analysis of isolated cervical
cells, or do not offer sufficient methods to explain and understand how the
proposed models reach their classification decisions on multi-cell images.
Here, we evaluate various state-of-the-art deep learning models and
attention-based frameworks for the classification of images of multiple
cervical cells. As we aim to provide interpretable deep learning models to
address this task, we also compare their explainability through the
visualization of their gradients. We demonstrate the importance of using images
that contain multiple cells over using isolated single-cell images. We show the
effectiveness of the residual channel attention model for extracting important
features from a group of cells, and demonstrate this model's efficiency for
this classification task. This work highlights the benefits of channel
attention mechanisms in analyzing multiple-cell images for potential relations
and distributions within a group of cells. It also provides interpretable
models to address the classification of cervical cells.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1"&gt;Ruiqi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Armin_M/0/1/0/all/0/1"&gt;Mohammad Ali Armin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Denman_S/0/1/0/all/0/1"&gt;Simon Denman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Petersson_L/0/1/0/all/0/1"&gt;Lars Petersson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ahmedt_Aristizabal_D/0/1/0/all/0/1"&gt;David Ahmedt-Aristizabal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AAPM DL-Sparse-View CT Challenge Submission Report: Designing an Iterative Network for Fanbeam-CT with Unknown Geometry. (arXiv:2106.00280v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.00280</id>
        <link href="http://arxiv.org/abs/2106.00280"/>
        <updated>2021-06-02T02:50:02.363Z</updated>
        <summary type="html"><![CDATA[This report is dedicated to a short motivation and description of our
contribution to the AAPM DL-Sparse-View CT Challenge (team name:
"robust-and-stable"). The task is to recover breast model phantom images from
limited view fanbeam measurements using data-driven reconstruction techniques.
The challenge is distinctive in the sense that participants are provided with a
collection of ground truth images and their noiseless, subsampled sinograms (as
well as the associated limited view filtered backprojection images), but not
with the actual forward model. Therefore, our approach first estimates the
fanbeam geometry in a data-driven geometric calibration step. In a subsequent
two-step procedure, we design an iterative end-to-end network that enables the
computation of near-exact solutions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Genzel_M/0/1/0/all/0/1"&gt;Martin Genzel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Macdonald_J/0/1/0/all/0/1"&gt;Jan Macdonald&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marz_M/0/1/0/all/0/1"&gt;Maximilian M&amp;#xe4;rz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Non-commutative Extension of Lee-Seung's Algorithm for Positive Semidefinite Factorizations. (arXiv:2106.00293v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2106.00293</id>
        <link href="http://arxiv.org/abs/2106.00293"/>
        <updated>2021-06-02T02:50:02.356Z</updated>
        <summary type="html"><![CDATA[Given a matrix $X\in \mathbb{R}_+^{m\times n}$ with nonnegative entries, a
Positive Semidefinite (PSD) factorization of $X$ is a collection of $r \times
r$-dimensional PSD matrices $\{A_i\}$ and $\{B_j\}$ satisfying $X_{ij}=
\mathrm{tr}(A_i B_j)$ for all $\ i\in [m],\ j\in [n]$. PSD factorizations are
fundamentally linked to understanding the expressiveness of semidefinite
programs as well as the power and limitations of quantum resources in
information theory. The PSD factorization task generalizes the Non-negative
Matrix Factorization (NMF) problem where we seek a collection of
$r$-dimensional nonnegative vectors $\{a_i\}$ and $\{b_j\}$ satisfying $X_{ij}=
a_i^\top b_j$, for all $i\in [m],\ j\in [n]$ -- one can recover the latter
problem by choosing matrices in the PSD factorization to be diagonal. The most
widely used algorithm for computing NMFs of a matrix is the Multiplicative
Update algorithm developed by Lee and Seung, in which nonnegativity of the
updates is preserved by scaling with positive diagonal matrices. In this paper,
we describe a non-commutative extension of Lee-Seung's algorithm, which we call
the Matrix Multiplicative Update (MMU) algorithm, for computing PSD
factorizations. The MMU algorithm ensures that updates remain PSD by congruence
scaling with the matrix geometric mean of appropriate PSD matrices, and it
retains the simplicity of implementation that Lee-Seung's algorithm enjoys.
Building on the Majorization-Minimization framework, we show that under our
update scheme the squared loss objective is non-increasing and fixed points
correspond to critical points. The analysis relies on Lieb's Concavity Theorem.
Beyond PSD factorizations, we use the MMU algorithm as a primitive to calculate
block-diagonal PSD factorizations and tensor PSD factorizations. We demonstrate
the utility of our method with experiments on real and synthetic data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Soh_Y/0/1/0/all/0/1"&gt;Yong Sheng Soh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Varvitsiotis_A/0/1/0/all/0/1"&gt;Antonios Varvitsiotis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Diffusion Self-Organizing Map on the Hypersphere. (arXiv:2106.00014v1 [cs.NE])]]></title>
        <id>http://arxiv.org/abs/2106.00014</id>
        <link href="http://arxiv.org/abs/2106.00014"/>
        <updated>2021-06-02T02:50:02.336Z</updated>
        <summary type="html"><![CDATA[We discuss a diffusion based implementation of the self-organizing map on the
unit hypersphere. We show that this approach can be efficiently implemented
using just linear algebra methods, we give a python numpy implementation, and
we illustrate the approach using the well known MNIST dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Andrecut_M/0/1/0/all/0/1"&gt;M. Andrecut&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What Matters for Adversarial Imitation Learning?. (arXiv:2106.00672v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.00672</id>
        <link href="http://arxiv.org/abs/2106.00672"/>
        <updated>2021-06-02T02:50:02.334Z</updated>
        <summary type="html"><![CDATA[Adversarial imitation learning has become a popular framework for imitation
in continuous control. Over the years, several variations of its components
were proposed to enhance the performance of the learned policies as well as the
sample complexity of the algorithm. In practice, these choices are rarely
tested all together in rigorous empirical studies. It is therefore difficult to
discuss and understand what choices, among the high-level algorithmic options
as well as low-level implementation details, matter. To tackle this issue, we
implement more than 50 of these choices in a generic adversarial imitation
learning framework and investigate their impacts in a large-scale study (>500k
trained agents) with both synthetic and human-generated demonstrations. While
many of our findings confirm common practices, some of them are surprising or
even contradict prior work. In particular, our results suggest that artificial
demonstrations are not a good proxy for human data and that the very common
practice of evaluating imitation algorithms only with synthetic demonstrations
may lead to algorithms which perform poorly in the more realistic scenarios
with human demonstrations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Orsini_M/0/1/0/all/0/1"&gt;Manu Orsini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raichuk_A/0/1/0/all/0/1"&gt;Anton Raichuk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hussenot_L/0/1/0/all/0/1"&gt;L&amp;#xe9;onard Hussenot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vincent_D/0/1/0/all/0/1"&gt;Damien Vincent&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dadashi_R/0/1/0/all/0/1"&gt;Robert Dadashi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Girgin_S/0/1/0/all/0/1"&gt;Sertan Girgin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geist_M/0/1/0/all/0/1"&gt;Matthieu Geist&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bachem_O/0/1/0/all/0/1"&gt;Olivier Bachem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pietquin_O/0/1/0/all/0/1"&gt;Olivier Pietquin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Andrychowicz_M/0/1/0/all/0/1"&gt;Marcin Andrychowicz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards an understanding of CNNs: analysing the recovery of activation pathways via Deep Convolutional Sparse Coding. (arXiv:1806.09888v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1806.09888</id>
        <link href="http://arxiv.org/abs/1806.09888"/>
        <updated>2021-06-02T02:50:02.327Z</updated>
        <summary type="html"><![CDATA[Deep Convolutional Sparse Coding (D-CSC) is a framework reminiscent of deep
convolutional neural networks (DCNNs), but by omitting the learning of the
dictionaries one can more transparently analyse the role of the activation
function and its ability to recover activation paths through the layers.
Papyan, Romano, and Elad conducted an analysis of such an architecture,
demonstrated the relationship with DCNNs and proved conditions under which the
D-CSC is guaranteed to recover specific activation paths. A technical
innovation of their work highlights that one can view the efficacy of the ReLU
nonlinear activation function of a DCNN through a new variant of the tensor's
sparsity, referred to as stripe-sparsity. Using this they proved that
representations with an activation density proportional to the ambient
dimension of the data are recoverable. We extend their uniform guarantees to a
modified model and prove that with high probability the true activation is
typically possible to recover for a greater density of activations per layer.
Our extension follows from incorporating the prior work on one step
thresholding by Schnass and Vandergheynst.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Murray_M/0/1/0/all/0/1"&gt;Michael Murray&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tanner_J/0/1/0/all/0/1"&gt;Jared Tanner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pose Invariant Person Re-Identification using Robust Pose-transformation GAN. (arXiv:2105.00930v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.00930</id>
        <link href="http://arxiv.org/abs/2105.00930"/>
        <updated>2021-06-02T02:50:02.327Z</updated>
        <summary type="html"><![CDATA[The objective of person re-identification (re-ID) is to retrieve a person's
images from an image gallery, given a single instance of the person of
interest. Despite several advancements, learning discriminative
identity-sensitive and viewpoint invariant features for robust Person
Re-identification is a major challenge owing to the large pose variation of
humans. This paper proposes a re-ID pipeline that utilizes the image generation
capability of Generative Adversarial Networks combined with pose clustering and
feature fusion to achieve pose invariant feature learning. The objective is to
model a given person under different viewpoints and large pose changes and
extract the most discriminative features from all the appearances. The pose
transformational GAN (pt-GAN) module is trained to generate a person's image in
any given pose. In order to identify the most significant poses for
discriminative feature extraction, a Pose Clustering module is proposed. The
given instance of the person is modelled in varying poses and these features
are effectively combined through the Feature Fusion Network. The final re-ID
model consisting of these 3 sub-blocks, alleviates the pose dependence in
person re-ID. Also, The proposed model is robust to occlusion, scale, rotation
and illumination, providing a framework for viewpoint invariant feature
learning. The proposed method outperforms the state-of-the-art GAN based models
in 4 benchmark datasets. It also surpasses the state-of-the-art models that
report higher re-ID accuracy in terms of improvement over baseline.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Karmakar_A/0/1/0/all/0/1"&gt;Arnab Karmakar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mishra_D/0/1/0/all/0/1"&gt;Deepak Mishra&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pattern Discovery in Time Series with Byte Pair Encoding. (arXiv:2106.00614v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2106.00614</id>
        <link href="http://arxiv.org/abs/2106.00614"/>
        <updated>2021-06-02T02:50:02.326Z</updated>
        <summary type="html"><![CDATA[The growing popularity of wearable sensors has generated large quantities of
temporal physiological and activity data. Ability to analyze this data offers
new opportunities for real-time health monitoring and forecasting. However,
temporal physiological data presents many analytic challenges: the data is
noisy, contains many missing values, and each series has a different length.
Most methods proposed for time series analysis and classification do not handle
datasets with these characteristics nor do they offer interpretability and
explainability, a critical requirement in the health domain. We propose an
unsupervised method for learning representations of time series based on common
patterns identified within them. The patterns are, interpretable, variable in
length, and extracted using Byte Pair Encoding compression technique. In this
way the method can capture both long-term and short-term dependencies present
in the data. We show that this method applies to both univariate and
multivariate time series and beats state-of-the-art approaches on a real world
dataset collected from wearable sensors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Tavabi_N/0/1/0/all/0/1"&gt;Nazgol Tavabi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lerman_K/0/1/0/all/0/1"&gt;Kristina Lerman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Representations for Sub-Symbolic Reasoning. (arXiv:2106.00393v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2106.00393</id>
        <link href="http://arxiv.org/abs/2106.00393"/>
        <updated>2021-06-02T02:50:02.319Z</updated>
        <summary type="html"><![CDATA[Neuro-symbolic methods integrate neural architectures, knowledge
representation and reasoning. However, they have been struggling at both
dealing with the intrinsic uncertainty of the observations and scaling to real
world applications. This paper presents Relational Reasoning Networks (R2N), a
novel end-to-end model that performs relational reasoning in the latent space
of a deep learner architecture, where the representations of constants, ground
atoms and their manipulations are learned in an integrated fashion. Unlike flat
architectures like Knowledge Graph Embedders, which can only represent
relations between entities, R2Ns define an additional computational structure,
accounting for higher-level relations among the ground atoms. The considered
relations can be explicitly known, like the ones defined by logic formulas, or
defined as unconstrained correlations among groups of ground atoms. R2Ns can be
applied to purely symbolic tasks or as a neuro-symbolic platform to integrate
learning and reasoning in heterogeneous problems with both symbolic and
feature-based represented entities. The proposed model bridges the gap between
previous neuro-symbolic methods that have been either limited in terms of
scalability or expressivity. The proposed methodology is shown to achieve
state-of-the-art results in different experimental settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Marra_G/0/1/0/all/0/1"&gt;Giuseppe Marra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Diligenti_M/0/1/0/all/0/1"&gt;Michelangelo Diligenti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Giannini_F/0/1/0/all/0/1"&gt;Francesco Giannini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maggini_M/0/1/0/all/0/1"&gt;Marco Maggini&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tight Accounting in the Shuffle Model of Differential Privacy. (arXiv:2106.00477v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2106.00477</id>
        <link href="http://arxiv.org/abs/2106.00477"/>
        <updated>2021-06-02T02:50:02.311Z</updated>
        <summary type="html"><![CDATA[Shuffle model of differential privacy is a novel distributed privacy model
based on a combination of local privacy mechanisms and a trusted shuffler. It
has been shown that the additional randomisation provided by the shuffler
improves privacy bounds compared to the purely local mechanisms. Accounting
tight bounds, especially for multi-message protocols, is complicated by the
complexity brought by the shuffler. The recently proposed Fourier Accountant
for evaluating $(\varepsilon,\delta)$-differential privacy guarantees has been
shown to give tighter bounds than commonly used methods for non-adaptive
compositions of various complex mechanisms. In this paper we show how to
compute tight privacy bounds using the Fourier Accountant for multi-message
versions of several ubiquitous mechanisms in the shuffle model and demonstrate
looseness of the existing bounds in the literature.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Koskela_A/0/1/0/all/0/1"&gt;Antti Koskela&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heikkila_M/0/1/0/all/0/1"&gt;Mikko A. Heikkil&amp;#xe4;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Honkela_A/0/1/0/all/0/1"&gt;Antti Honkela&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Wiki-Reliability: A Large Scale Dataset for Content Reliability on Wikipedia. (arXiv:2105.04117v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.04117</id>
        <link href="http://arxiv.org/abs/2105.04117"/>
        <updated>2021-06-02T02:50:02.291Z</updated>
        <summary type="html"><![CDATA[Wikipedia is the largest online encyclopedia, used by algorithms and web
users as a central hub of reliable information on the web. The quality and
reliability of Wikipedia content is maintained by a community of volunteer
editors. Machine learning and information retrieval algorithms could help scale
up editors' manual efforts around Wikipedia content reliability. However, there
is a lack of large-scale data to support the development of such research. To
fill this gap, in this paper, we propose Wiki-Reliability, the first dataset of
English Wikipedia articles annotated with a wide set of content reliability
issues. To build this dataset, we rely on Wikipedia "templates". Templates are
tags used by expert Wikipedia editors to indicate content issues, such as the
presence of "non-neutral point of view" or "contradictory articles", and serve
as a strong signal for detecting reliability issues in a revision. We select
the 10 most popular reliability-related templates on Wikipedia, and propose an
effective method to label almost 1M samples of Wikipedia article revisions as
positive or negative with respect to each template. Each positive/negative
example in the dataset comes with the full article text and 20 features from
the revision's metadata. We provide an overview of the possible downstream
tasks enabled by such data, and show that Wiki-Reliability can be used to train
large-scale models for content reliability prediction. We release all data and
code for public use.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wong_K/0/1/0/all/0/1"&gt;KayYen Wong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Redi_M/0/1/0/all/0/1"&gt;Miriam Redi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saez_Trumper_D/0/1/0/all/0/1"&gt;Diego Saez-Trumper&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Two-stage domain adapted training for better generalization in real-world image restoration and super-resolution. (arXiv:2106.00504v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.00504</id>
        <link href="http://arxiv.org/abs/2106.00504"/>
        <updated>2021-06-02T02:50:02.290Z</updated>
        <summary type="html"><![CDATA[It is well-known that in inverse problems, end-to-end trained networks
overfit the degradation model seen in the training set, i.e., they do not
generalize to other types of degradations well. Recently, an approach to first
map images downsampled by unknown filters to bicubicly downsampled look-alike
images was proposed to successfully super-resolve such images. In this paper,
we show that any inverse problem can be formulated by first mapping the input
degraded images to an intermediate domain, and then training a second network
to form output images from these intermediate images. Furthermore, the best
intermediate domain may vary according to the task. Our experimental results
demonstrate that this two-stage domain-adapted training strategy does not only
achieve better results on a given class of unknown degradations but can also
generalize to other unseen classes of degradations better.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Korkmaz_C/0/1/0/all/0/1"&gt;Cansu Korkmaz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tekalp_A/0/1/0/all/0/1"&gt;A.Murat Tekalp&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dogan_Z/0/1/0/all/0/1"&gt;Zafer Dogan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Clustering-friendly Representation Learning via Instance Discrimination and Feature Decorrelation. (arXiv:2106.00131v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.00131</id>
        <link href="http://arxiv.org/abs/2106.00131"/>
        <updated>2021-06-02T02:50:02.283Z</updated>
        <summary type="html"><![CDATA[Clustering is one of the most fundamental tasks in machine learning.
Recently, deep clustering has become a major trend in clustering techniques.
Representation learning often plays an important role in the effectiveness of
deep clustering, and thus can be a principal cause of performance degradation.
In this paper, we propose a clustering-friendly representation learning method
using instance discrimination and feature decorrelation. Our
deep-learning-based representation learning method is motivated by the
properties of classical spectral clustering. Instance discrimination learns
similarities among data and feature decorrelation removes redundant correlation
among features. We utilize an instance discrimination method in which learning
individual instance classes leads to learning similarity among instances.
Through detailed experiments and examination, we show that the approach can be
adapted to learning a latent space for clustering. We design novel
softmax-formulated decorrelation constraints for learning. In evaluations of
image clustering using CIFAR-10 and ImageNet-10, our method achieves accuracy
of 81.5% and 95.4%, respectively. We also show that the softmax-formulated
constraints are compatible with various neural networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tao_Y/0/1/0/all/0/1"&gt;Yaling Tao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Takagi_K/0/1/0/all/0/1"&gt;Kentaro Takagi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nakata_K/0/1/0/all/0/1"&gt;Kouta Nakata&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Natural Statistics of Network Activations and Implications for Knowledge Distillation. (arXiv:2106.00368v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.00368</id>
        <link href="http://arxiv.org/abs/2106.00368"/>
        <updated>2021-06-02T02:50:02.282Z</updated>
        <summary type="html"><![CDATA[In a matter that is analog to the study of natural image statistics, we study
the natural statistics of the deep neural network activations at various
layers. As we show, these statistics, similar to image statistics, follow a
power law. We also show, both analytically and empirically, that with depth the
exponent of this power law increases at a linear rate.

As a direct implication of our discoveries, we present a method for
performing Knowledge Distillation (KD). While classical KD methods consider the
logits of the teacher network, more recent methods obtain a leap in performance
by considering the activation maps. This, however, uses metrics that are
suitable for comparing images. We propose to employ two additional loss terms
that are based on the spectral properties of the intermediate activation maps.
The proposed method obtains state of the art results on multiple image
recognition KD benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rotman_M/0/1/0/all/0/1"&gt;Michael Rotman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wolf_L/0/1/0/all/0/1"&gt;Lior Wolf&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Detection of preventable fetal distress during labor from scanned cardiotocogram tracings using deep learning. (arXiv:2106.00628v1 [q-bio.QM])]]></title>
        <id>http://arxiv.org/abs/2106.00628</id>
        <link href="http://arxiv.org/abs/2106.00628"/>
        <updated>2021-06-02T02:50:02.282Z</updated>
        <summary type="html"><![CDATA[Despite broad application during labor and delivery, there remains
considerable debate about the value of electronic fetal monitoring (EFM). EFM
includes the surveillance of the fetal heart rate (FHR) patterns in conjunction
with the maternal uterine contractions providing a wealth of data about fetal
behavior and the threat of diminished oxygenation and perfusion. Adverse
outcomes universally associate a fetal injury with the failure to timely
respond to FHR pattern information. Historically, the EFM data, stored
digitally, are available only as rasterized pdf images for contemporary or
historical discussion and examination. In reality, however, they are rarely
reviewed systematically. Using a unique archive of EFM collected over 50 years
of practice in conjunction with adverse outcomes, we present a deep learning
framework for training and detection of incipient or past fetal injury. We
report 94% accuracy in identifying early, preventable fetal injury intrapartum.
This framework is suited for automating an early warning and decision support
system for maintaining fetal well-being during the stresses of labor.
Ultimately, such a system could enable a physician to timely respond during
labor and prevent adverse outcomes. When adverse outcomes cannot be avoided,
they can provide guidance to the early neuroprotective treatment of the
newborn.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Frasch_M/0/1/0/all/0/1"&gt;Martin G. Frasch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Strong_S/0/1/0/all/0/1"&gt;Shadrian B. Strong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Nilosek_D/0/1/0/all/0/1"&gt;David Nilosek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Leaverton_J/0/1/0/all/0/1"&gt;Joshua Leaverton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Schifrin_B/0/1/0/all/0/1"&gt;Barry S. Schifrin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tactical Optimism and Pessimism for Deep Reinforcement Learning. (arXiv:2102.03765v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.03765</id>
        <link href="http://arxiv.org/abs/2102.03765"/>
        <updated>2021-06-02T02:50:02.282Z</updated>
        <summary type="html"><![CDATA[In recent years, deep off-policy actor-critic algorithms have become a
dominant approach to reinforcement learning for continuous control. One of the
primary drivers of this improved performance is the use of pessimistic value
updates to address function approximation errors, which previously led to
disappointing performance. However, a direct consequence of pessimism is
reduced exploration, running counter to theoretical support for the efficacy of
optimism in the face of uncertainty. So which approach is best? In this work,
we show that the most effective degree of optimism can vary both across tasks
and over the course of learning. Inspired by this insight, we introduce a novel
deep actor-critic framework, Tactical Optimistic and Pessimistic (TOP)
estimation, which switches between optimistic and pessimistic value learning
online. This is achieved by formulating the selection as a multi-arm bandit
problem. We show in a series of continuous control tasks that TOP outperforms
existing methods which rely on a fixed degree of optimism, setting a new state
of the art in challenging pixel-based environments. Since our changes are
simple to implement, we believe these insights can easily be incorporated into
a multitude of off-policy algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Moskovitz_T/0/1/0/all/0/1"&gt;Ted Moskovitz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parker_Holder_J/0/1/0/all/0/1"&gt;Jack Parker-Holder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pacchiano_A/0/1/0/all/0/1"&gt;Aldo Pacchiano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arbel_M/0/1/0/all/0/1"&gt;Michael Arbel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jordan_M/0/1/0/all/0/1"&gt;Michael I. Jordan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Football Body-Orientation as a Matter of Classification. (arXiv:2106.00359v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.00359</id>
        <link href="http://arxiv.org/abs/2106.00359"/>
        <updated>2021-06-02T02:50:02.281Z</updated>
        <summary type="html"><![CDATA[Orientation is a crucial skill for football players that becomes a
differential factor in a large set of events, especially the ones involving
passes. However, existing orientation estimation methods, which are based on
computer-vision techniques, still have a lot of room for improvement. To the
best of our knowledge, this article presents the first deep learning model for
estimating orientation directly from video footage. By approaching this
challenge as a classification problem where classes correspond to orientation
bins, and by introducing a cyclic loss function, a well-known convolutional
network is refined to provide player orientation data. The model is trained by
using ground-truth orientation data obtained from wearable EPTS devices, which
are individually compensated with respect to the perceived orientation in the
current frame. The obtained results outperform previous methods; in particular,
the absolute median error is less than 12 degrees per player. An ablation study
is included in order to show the potential generalization to any kind of
football video footage.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Arbues_Sanguesa_A/0/1/0/all/0/1"&gt;Adri&amp;#xe0; Arbu&amp;#xe9;s-Sang&amp;#xfc;esa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martin_A/0/1/0/all/0/1"&gt;Adri&amp;#xe1;n Mart&amp;#xed;n&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Granero_P/0/1/0/all/0/1"&gt;Paulino Granero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ballester_C/0/1/0/all/0/1"&gt;Coloma Ballester&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Haro_G/0/1/0/all/0/1"&gt;Gloria Haro&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Explainability of Graph Neural Networks via Subgraph Explorations. (arXiv:2102.05152v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.05152</id>
        <link href="http://arxiv.org/abs/2102.05152"/>
        <updated>2021-06-02T02:50:02.281Z</updated>
        <summary type="html"><![CDATA[We consider the problem of explaining the predictions of graph neural
networks (GNNs), which otherwise are considered as black boxes. Existing
methods invariably focus on explaining the importance of graph nodes or edges
but ignore the substructures of graphs, which are more intuitive and
human-intelligible. In this work, we propose a novel method, known as
SubgraphX, to explain GNNs by identifying important subgraphs. Given a trained
GNN model and an input graph, our SubgraphX explains its predictions by
efficiently exploring different subgraphs with Monte Carlo tree search. To make
the tree search more effective, we propose to use Shapley values as a measure
of subgraph importance, which can also capture the interactions among different
subgraphs. To expedite computations, we propose efficient approximation schemes
to compute Shapley values for graph data. Our work represents the first attempt
to explain GNNs via identifying subgraphs explicitly and directly. Experimental
results show that our SubgraphX achieves significantly improved explanations,
while keeping computations at a reasonable level.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1"&gt;Hao Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1"&gt;Haiyang Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jie Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1"&gt;Kang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1"&gt;Shuiwang Ji&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bayesian Reasoning with Trained Neural Networks. (arXiv:2001.11031v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2001.11031</id>
        <link href="http://arxiv.org/abs/2001.11031"/>
        <updated>2021-06-02T02:50:02.279Z</updated>
        <summary type="html"><![CDATA[We showed how to use trained neural networks to perform Bayesian reasoning in
order to solve tasks outside their initial scope. Deep generative models
provide prior knowledge, and classification/regression networks impose
constraints. The tasks at hand were formulated as Bayesian inference problems,
which we approximately solved through variational or sampling techniques. The
approach built on top of already trained networks, and the addressable
questions grew super-exponentially with the number of available networks. In
its simplest form, the approach yielded conditional generative models. However,
multiple simultaneous constraints constitute elaborate questions. We compared
the approach to specifically trained generators, showed how to solve riddles,
and demonstrated its compatibility with state-of-the-art architectures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Knollmuller_J/0/1/0/all/0/1"&gt;Jakob Knollm&amp;#xfc;ller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ensslin_T/0/1/0/all/0/1"&gt;Torsten En&amp;#xdf;lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A study on the plasticity of neural networks. (arXiv:2106.00042v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.00042</id>
        <link href="http://arxiv.org/abs/2106.00042"/>
        <updated>2021-06-02T02:50:02.274Z</updated>
        <summary type="html"><![CDATA[One aim shared by multiple settings, such as continual learning or transfer
learning, is to leverage previously acquired knowledge to converge faster on
the current task. Usually this is done through fine-tuning, where an implicit
assumption is that the network maintains its plasticity, meaning that the
performance it can reach on any given task is not affected negatively by
previously seen tasks. It has been observed recently that a pretrained model on
data from the same distribution as the one it is fine-tuned on might not reach
the same generalisation as a freshly initialised one. We build and extend this
observation, providing a hypothesis for the mechanics behind it. We discuss the
implication of losing plasticity for continual learning which heavily relies on
optimising pretrained models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Berariu_T/0/1/0/all/0/1"&gt;Tudor Berariu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Czarnecki_W/0/1/0/all/0/1"&gt;Wojciech Czarnecki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+De_S/0/1/0/all/0/1"&gt;Soham De&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bornschein_J/0/1/0/all/0/1"&gt;Jorg Bornschein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Smith_S/0/1/0/all/0/1"&gt;Samuel Smith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pascanu_R/0/1/0/all/0/1"&gt;Razvan Pascanu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Clopath_C/0/1/0/all/0/1"&gt;Claudia Clopath&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sequential Domain Adaptation by Synthesizing Distributionally Robust Experts. (arXiv:2106.00322v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.00322</id>
        <link href="http://arxiv.org/abs/2106.00322"/>
        <updated>2021-06-02T02:50:02.270Z</updated>
        <summary type="html"><![CDATA[Least squares estimators, when trained on a few target domain samples, may
predict poorly. Supervised domain adaptation aims to improve the predictive
accuracy by exploiting additional labeled training samples from a source
distribution that is close to the target distribution. Given available data, we
investigate novel strategies to synthesize a family of least squares estimator
experts that are robust with regard to moment conditions. When these moment
conditions are specified using Kullback-Leibler or Wasserstein-type
divergences, we can find the robust estimators efficiently using convex
optimization. We use the Bernstein online aggregation algorithm on the proposed
family of robust experts to generate predictions for the sequential stream of
target test samples. Numerical experiments on real data show that the robust
strategies may outperform non-robust interpolations of the empirical least
squares estimators.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Taskesen_B/0/1/0/all/0/1"&gt;Bahar Taskesen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yue_M/0/1/0/all/0/1"&gt;Man-Chung Yue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Blanchet_J/0/1/0/all/0/1"&gt;Jose Blanchet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kuhn_D/0/1/0/all/0/1"&gt;Daniel Kuhn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_V/0/1/0/all/0/1"&gt;Viet Anh Nguyen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Horizontal Flows and Manifold Stochastics in Geometric Deep Learning. (arXiv:1909.06397v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1909.06397</id>
        <link href="http://arxiv.org/abs/1909.06397"/>
        <updated>2021-06-02T02:50:02.268Z</updated>
        <summary type="html"><![CDATA[We introduce two constructions in geometric deep learning for 1) transporting
orientation-dependent convolutional filters over a manifold in a continuous way
and thereby defining a convolution operator that naturally incorporates the
rotational effect of holonomy; and 2) allowing efficient evaluation of manifold
convolution layers by sampling manifold valued random variables that center
around a weighted diffusion mean. Both methods are inspired by stochastics on
manifolds and geometric statistics, and provide examples of how stochastic
methods -- here horizontal frame bundle flows and non-linear bridge sampling
schemes, can be used in geometric deep learning. We outline the theoretical
foundation of the two methods, discuss their relation to Euclidean deep
networks and existing methodology in geometric deep learning, and establish
important properties of the proposed constructions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sommer_S/0/1/0/all/0/1"&gt;Stefan Sommer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bronstein_A/0/1/0/all/0/1"&gt;Alex Bronstein&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reconfigurable Intelligent Surface Enabled Federated Learning: A Unified Communication-Learning Design Approach. (arXiv:2011.10282v4 [cs.IT] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.10282</id>
        <link href="http://arxiv.org/abs/2011.10282"/>
        <updated>2021-06-02T02:50:02.268Z</updated>
        <summary type="html"><![CDATA[To exploit massive amounts of data generated at mobile edge networks,
federated learning (FL) has been proposed as an attractive substitute for
centralized machine learning (ML). By collaboratively training a shared
learning model at edge devices, FL avoids direct data transmission and thus
overcomes high communication latency and privacy issues as compared to
centralized ML. To improve the communication efficiency in FL model
aggregation, over-the-air computation has been introduced to support a large
number of simultaneous local model uploading by exploiting the inherent
superposition property of wireless channels. However, due to the heterogeneity
of communication capacities among edge devices, over-the-air FL suffers from
the straggler issue in which the device with the weakest channel acts as a
bottleneck of the model aggregation performance. This issue can be alleviated
by device selection to some extent, but the latter still suffers from a
tradeoff between data exploitation and model communication. In this paper, we
leverage the reconfigurable intelligent surface (RIS) technology to relieve the
straggler issue in over-the-air FL. Specifically, we develop a learning
analysis framework to quantitatively characterize the impact of device
selection and model aggregation error on the convergence of over-the-air FL.
Then, we formulate a unified communication-learning optimization problem to
jointly optimize device selection, over-the-air transceiver design, and RIS
configuration. Numerical experiments show that the proposed design achieves
substantial learning accuracy improvement compared with the state-of-the-art
approaches, especially when channel conditions vary dramatically across edge
devices.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Hang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1"&gt;Xiaojun Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Ying-Jun Angela Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NOMU: Neural Optimization-based Model Uncertainty. (arXiv:2102.13640v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.13640</id>
        <link href="http://arxiv.org/abs/2102.13640"/>
        <updated>2021-06-02T02:50:02.268Z</updated>
        <summary type="html"><![CDATA[We study methods for estimating model uncertainty for neural networks (NNs).
To isolate the effect of model uncertainty, we focus on a noiseless setting
with scarce training data. We introduce five important desiderata regarding
model uncertainty that any method should satisfy. However, we find that
established benchmarks often fail to reliably capture some of these desiderata,
even those that are required by Bayesian theory. To address this, we introduce
a new approach for capturing model uncertainty for NNs, which we call Neural
Optimization-based Model Uncertainty (NOMU). The main idea of NOMU is to design
a network architecture consisting of two connected sub-NNs, one for model
prediction and one for model uncertainty, and to train it using a
carefully-designed loss function. Importantly, our design enforces that NOMU
satisfies our five desiderata. Due to its modular architecture, NOMU can
provide model uncertainty for any given (previously trained) NN if given access
to its training data. We first experimentally study noiseless regression with
scarce training data to highlight the deficiencies of the established
benchmarks. Finally, we study the important task of Bayesian optimization (BO)
with costly evaluations, where good model uncertainty estimates are essential.
Our results show that NOMU performs as well or better than state-of-the-art
benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Heiss_J/0/1/0/all/0/1"&gt;Jakob Heiss&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weissteiner_J/0/1/0/all/0/1"&gt;Jakob Weissteiner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wutte_H/0/1/0/all/0/1"&gt;Hanna Wutte&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seuken_S/0/1/0/all/0/1"&gt;Sven Seuken&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Teichmann_J/0/1/0/all/0/1"&gt;Josef Teichmann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Student Performance Prediction Using Dynamic Neural Models. (arXiv:2106.00524v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.00524</id>
        <link href="http://arxiv.org/abs/2106.00524"/>
        <updated>2021-06-02T02:50:02.267Z</updated>
        <summary type="html"><![CDATA[We address the problem of predicting the correctness of the student's
response on the next exam question based on their previous interactions in the
course of their learning and evaluation process. We model the student
performance as a dynamic problem and compare the two major classes of dynamic
neural architectures for its solution, namely the finite-memory Time Delay
Neural Networks (TDNN) and the potentially infinite-memory Recurrent Neural
Networks (RNN). Since the next response is a function of the knowledge state of
the student and this, in turn, is a function of their previous responses and
the skills associated with the previous questions, we propose a two-part
network architecture. The first part employs a dynamic neural network (either
TDNN or RNN) to trace the student knowledge state. The second part applies on
top of the dynamic part and it is a multi-layer feed-forward network which
completes the classification task of predicting the student response based on
our estimate of the student knowledge state. Both input skills and previous
responses are encoded using different embeddings. Regarding the skill
embeddings we tried two different initialization schemes using (a) random
vectors and (b) pretrained vectors matching the textual descriptions of the
skills. Our experiments show that the performance of the RNN approach is better
compared to the TDNN approach in all datasets that we have used. Also, we show
that our RNN architecture outperforms the state-of-the-art models in four out
of five datasets. It is worth noting that the TDNN approach also outperforms
the state of the art models in four out of five datasets, although it is
slightly worse than our proposed RNN approach. Finally, contrary to our
expectations, we find that the initialization of skill embeddings using
pretrained vectors offers practically no advantage over random initialization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Delianidi_M/0/1/0/all/0/1"&gt;Marina Delianidi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Diamantaras_K/0/1/0/all/0/1"&gt;Konstantinos Diamantaras&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chrysogonidis_G/0/1/0/all/0/1"&gt;George Chrysogonidis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nikiforidis_V/0/1/0/all/0/1"&gt;Vasileios Nikiforidis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Aggregated Learning: A Vector-Quantization Approach to Learning Neural Network Classifiers. (arXiv:2001.03955v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2001.03955</id>
        <link href="http://arxiv.org/abs/2001.03955"/>
        <updated>2021-06-02T02:50:02.263Z</updated>
        <summary type="html"><![CDATA[We consider the problem of learning a neural network classifier. Under the
information bottleneck (IB) principle, we associate with this classification
problem a representation learning problem, which we call "IB learning". We show
that IB learning is, in fact, equivalent to a special class of the quantization
problem. The classical results in rate-distortion theory then suggest that IB
learning can benefit from a "vector quantization" approach, namely,
simultaneously learning the representations of multiple input objects. Such an
approach assisted with some variational techniques, result in a novel learning
framework, "Aggregated Learning", for classification with neural network
models. In this framework, several objects are jointly classified by a single
neural network. The effectiveness of this framework is verified through
extensive experiments on standard image recognition and text classification
tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Soflaei_M/0/1/0/all/0/1"&gt;Masoumeh Soflaei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1"&gt;Hongyu Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Al_Bashabsheh_A/0/1/0/all/0/1"&gt;Ali Al-Bashabsheh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1"&gt;Yongyi Mao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Richong Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Discovering Diverse Nearly Optimal Policies withSuccessor Features. (arXiv:2106.00669v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2106.00669</id>
        <link href="http://arxiv.org/abs/2106.00669"/>
        <updated>2021-06-02T02:50:02.262Z</updated>
        <summary type="html"><![CDATA[Finding different solutions to the same problem is a key aspect of
intelligence associated with creativity and adaptation to novel situations. In
reinforcement learning, a set of diverse policies can be useful for
exploration, transfer, hierarchy, and robustness. We propose Diverse Successive
Policies, a method for discovering policies that are diverse in the space of
Successor Features, while assuring that they are near optimal. We formalize the
problem as a Constrained Markov Decision Process (CMDP) where the goal is to
find policies that maximize diversity, characterized by an intrinsic diversity
reward, while remaining near-optimal with respect to the extrinsic reward of
the MDP. We also analyze how recently proposed robustness and discrimination
rewards perform and find that they are sensitive to the initialization of the
procedure and may converge to sub-optimal solutions. To alleviate this, we
propose new explicit diversity rewards that aim to minimize the correlation
between the Successor Features of the policies in the set. We compare the
different diversity mechanisms in the DeepMind Control Suite and find that the
type of explicit diversity we are proposing is important to discover distinct
behavior, like for example different locomotion patterns.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zahavy_T/0/1/0/all/0/1"&gt;Tom Zahavy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+ODonoghue_B/0/1/0/all/0/1"&gt;Brendan O&amp;#x27;Donoghue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barreto_A/0/1/0/all/0/1"&gt;Andre Barreto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mnih_V/0/1/0/all/0/1"&gt;Volodymyr Mnih&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Flennerhag_S/0/1/0/all/0/1"&gt;Sebastian Flennerhag&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1"&gt;Satinder Singh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Validating GAN-BioBERT: A Methodology For Assessing Reporting Trends In Clinical Trials. (arXiv:2106.00665v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.00665</id>
        <link href="http://arxiv.org/abs/2106.00665"/>
        <updated>2021-06-02T02:50:02.254Z</updated>
        <summary type="html"><![CDATA[In the past decade, there has been much discussion about the issue of biased
reporting in clinical research. Despite this attention, there have been limited
tools developed for the systematic assessment of qualitative statements made in
clinical research, with most studies assessing qualitative statements relying
on the use of manual expert raters, which limits their size. Also, previous
attempts to develop larger scale tools, such as those using natural language
processing, were limited by both their accuracy and the number of categories
used for the classification of their findings. With these limitations in mind,
this study's goal was to develop a classification algorithm that was both
suitably accurate and finely grained to be applied on a large scale for
assessing the qualitative sentiment expressed in clinical trial abstracts.
Additionally, this study seeks to compare the performance of the proposed
algorithm, GAN-BioBERT, to previous studies as well as to expert manual rating
of clinical trial abstracts. This study develops a three-class sentiment
classification algorithm for clinical trial abstracts using a semi-supervised
natural language process model based on the Bidirectional Encoder
Representation from Transformers (BERT) model, from a series of clinical trial
abstracts annotated by a group of experts in academic medicine. Results: The
use of this algorithm was found to have a classification accuracy of 91.3%,
with a macro F1-Score of 0.92, which is a significant improvement in accuracy
when compared to previous methods and expert ratings, while also making the
sentiment classification finer grained than previous studies. The proposed
algorithm, GAN-BioBERT, is a suitable classification model for the large-scale
assessment of qualitative statements in clinical trial literature, providing an
accurate, reproducible tool for the large-scale study of clinical publication
trends.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Myszewski_J/0/1/0/all/0/1"&gt;Joshua J Myszewski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Klossowski_E/0/1/0/all/0/1"&gt;Emily Klossowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meyer_P/0/1/0/all/0/1"&gt;Patrick Meyer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bevil_K/0/1/0/all/0/1"&gt;Kristin Bevil&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Klesius_L/0/1/0/all/0/1"&gt;Lisa Klesius&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schroeder_K/0/1/0/all/0/1"&gt;Kristopher M Schroeder&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Invertible Surrogate Models: Joint surrogate modelling and reconstruction of Laser-Wakefield Acceleration by invertible neural networks. (arXiv:2106.00432v1 [physics.plasm-ph])]]></title>
        <id>http://arxiv.org/abs/2106.00432</id>
        <link href="http://arxiv.org/abs/2106.00432"/>
        <updated>2021-06-02T02:50:02.253Z</updated>
        <summary type="html"><![CDATA[Invertible neural networks are a recent technique in machine learning
promising neural network architectures that can be run in forward and reverse
mode. In this paper, we will be introducing invertible surrogate models that
approximate complex forward simulation of the physics involved in laser plasma
accelerators: iLWFA. The bijective design of the surrogate model also provides
all means for reconstruction of experimentally acquired diagnostics. The
quality of our invertible laser wakefield acceleration network will be verified
on a large set of numerical LWFA simulations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Bethke_F/0/1/0/all/0/1"&gt;Friedrich Bethke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Pausch_R/0/1/0/all/0/1"&gt;Richard Pausch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Stiller_P/0/1/0/all/0/1"&gt;Patrick Stiller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Debus_A/0/1/0/all/0/1"&gt;Alexander Debus&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Bussmann_M/0/1/0/all/0/1"&gt;Michael Bussmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Hoffmann_N/0/1/0/all/0/1"&gt;Nico Hoffmann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning and Generalization in RNNs. (arXiv:2106.00047v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.00047</id>
        <link href="http://arxiv.org/abs/2106.00047"/>
        <updated>2021-06-02T02:50:02.243Z</updated>
        <summary type="html"><![CDATA[Simple recurrent neural networks (RNNs) and their more advanced cousins LSTMs
etc. have been very successful in sequence modeling. Their theoretical
understanding, however, is lacking and has not kept pace with the progress for
feedforward networks, where a reasonably complete understanding in the special
case of highly overparametrized one-hidden-layer networks has emerged. In this
paper, we make progress towards remedying this situation by proving that RNNs
can learn functions of sequences. In contrast to the previous work that could
only deal with functions of sequences that are sums of functions of individual
tokens in the sequence, we allow general functions. Conceptually and
technically, we introduce new ideas which enable us to extract information from
the hidden state of the RNN in our proofs -- addressing a crucial weakness in
previous work. We illustrate our results on some regular language recognition
problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Panigrahi_A/0/1/0/all/0/1"&gt;Abhishek Panigrahi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goyal_N/0/1/0/all/0/1"&gt;Navin Goyal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Anti-Koopmanism. (arXiv:2106.00106v1 [math.FA])]]></title>
        <id>http://arxiv.org/abs/2106.00106</id>
        <link href="http://arxiv.org/abs/2106.00106"/>
        <updated>2021-06-02T02:50:02.243Z</updated>
        <summary type="html"><![CDATA[This article addresses several longstanding misconceptions concerning Koopman
operators, including the existence of lattices of eigenfunctions, common
eigenfunctions between Koopman operators, and boundedness and compactness of
Koopman operators, among others. Counterexamples are provided for each
misconception. This manuscript also proves that the Gaussian RBF's native space
only supports bounded Koopman operator corresponding to affine dynamics, which
shows that the assumption of boundedness is very limiting. A framework for DMD
is presented that requires only densely defined Koopman operators over
reproducing kernel Hilbert spaces, and the effectiveness of this approach is
demonstrated through reconstruction examples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Gonzalez_E/0/1/0/all/0/1"&gt;Efrain Gonzalez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Abudia_M/0/1/0/all/0/1"&gt;Moad Abudia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Jury_M/0/1/0/all/0/1"&gt;Michael Jury&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Kamalapurkar_R/0/1/0/all/0/1"&gt;Rushikesh Kamalapurkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Rosenfeld_J/0/1/0/all/0/1"&gt;Joel A. Rosenfeld&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Compact and Interpretable Convolutional Neural Network for Cross-Subject Driver Drowsiness Detection from Single-Channel EEG. (arXiv:2106.00613v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2106.00613</id>
        <link href="http://arxiv.org/abs/2106.00613"/>
        <updated>2021-06-02T02:50:02.243Z</updated>
        <summary type="html"><![CDATA[Driver drowsiness is one of main factors leading to road fatalities and
hazards in the transportation industry. Electroencephalography (EEG) has been
considered as one of the best physiological signals to detect drivers drowsy
states, since it directly measures neurophysiological activities in the brain.
However, designing a calibration-free system for driver drowsiness detection
with EEG is still a challenging task, as EEG suffers from serious mental and
physical drifts across different subjects. In this paper, we propose a compact
and interpretable Convolutional Neural Network (CNN) to discover shared EEG
features across different subjects for driver drowsiness detection. We
incorporate the Global Average Pooling (GAP) layer in the model structure,
allowing the Class Activation Map (CAM) method to be used for localizing
regions of the input signal that contribute most for classification. Results
show that the proposed model can achieve an average accuracy of 73.22% on 11
subjects for 2-class cross-subject EEG signal classification, which is higher
than conventional machine learning methods and other state-of-art deep learning
methods. It is revealed by the visualization technique that the model has
learned biologically explainable features, e.g., Alpha spindles and Theta
burst, as evidence for the drowsy state. It is also interesting to see that the
model uses artifacts that usually dominate the wakeful EEG, e.g., muscle
artifacts and sensor drifts, to recognize the alert state. The proposed model
illustrates a potential direction to use CNN models as a powerful tool to
discover shared features related to different mental states across different
subjects from EEG signals.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Cui_J/0/1/0/all/0/1"&gt;Jian Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lan_Z/0/1/0/all/0/1"&gt;Zirui Lan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yisi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_R/0/1/0/all/0/1"&gt;Ruilin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_F/0/1/0/all/0/1"&gt;Fan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sourina_O/0/1/0/all/0/1"&gt;Olga Sourina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mueller_Wittig_W/0/1/0/all/0/1"&gt;Wolfgang Mueller-Wittig&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ClustRank: a Visual Quality Measure Trained on Perceptual Data for Sorting Scatterplots by Cluster Patterns. (arXiv:2106.00599v1 [cs.HC])]]></title>
        <id>http://arxiv.org/abs/2106.00599</id>
        <link href="http://arxiv.org/abs/2106.00599"/>
        <updated>2021-06-02T02:50:02.235Z</updated>
        <summary type="html"><![CDATA[Visual quality measures (VQMs) are designed to support analysts by
automatically detecting and quantifying patterns in visualizations. We propose
a new data-driven technique called ClustRank that allows to rank scatterplots
according to visible grouping patterns. Our model first encodes scatterplots in
the parametric space of a Gaussian Mixture Model, and then uses a classifier
trained on human judgment data to estimate the perceptual complexity of
grouping patterns. The numbers of initial mixture components and final combined
groups determine the rank of the scatterplot. ClustRank improves on existing
VQM techniques by mimicking human judgments on two-Gaussian cluster patterns
and gives more accuracy when ranking general cluster patterns in scatterplots.
We demonstrate its benefit by analyzing kinship data for genome-wide
association studies, a domain in which experts rely on the visual analysis of
large sets of scatterplots. We make the three benchmark datasets and the
ClustRank VQM available for practical use and further improvements.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Abbas_M/0/1/0/all/0/1"&gt;Mostafa Abbas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ullah_E/0/1/0/all/0/1"&gt;Ehsan Ullah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baggag_A/0/1/0/all/0/1"&gt;Abdelkader Baggag&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bensmail_H/0/1/0/all/0/1"&gt;Halima Bensmail&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sedlmair_M/0/1/0/all/0/1"&gt;Michael Sedlmair&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aupetit_M/0/1/0/all/0/1"&gt;Michael Aupetit&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Learning for Reliable Classification of COVID-19, MERS, and SARS from Chest X-Ray Images. (arXiv:2005.11524v6 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.11524</id>
        <link href="http://arxiv.org/abs/2005.11524"/>
        <updated>2021-06-02T02:50:02.235Z</updated>
        <summary type="html"><![CDATA[Novel Coronavirus disease (COVID-19) is an extremely contagious and quickly
spreading Coronavirus infestation. Severe Acute Respiratory Syndrome (SARS) and
Middle East Respiratory Syndrome (MERS), which outbreak in 2002 and 2011, and
the current COVID-19 pandemic are all from the same family of coronavirus. This
work aims to classify COVID-19, SARS, and MERS chest X-ray (CXR) images using
deep Convolutional Neural Networks (CNNs). A unique database was created,
so-called QU-COVID-family, consisting of 423 COVID-19, 144 MERS, and 134 SARS
CXR images. Besides, a robust COVID-19 recognition system was proposed to
identify lung regions using a CNN segmentation model (U-Net), and then classify
the segmented lung images as COVID-19, MERS, or SARS using a pre-trained CNN
classifier. Furthermore, the Score-CAM visualization method was utilized to
visualize classification output and understand the reasoning behind the
decision of deep CNNs. Several Deep Learning classifiers were trained and
tested; four outperforming algorithms were reported. Original and preprocessed
images were used individually and all together as the input(s) to the networks.
Two recognition schemes were considered: plain CXR classification and segmented
CXR classification. For plain CXRs, it was observed that InceptionV3
outperforms other networks with a 3-channel scheme and achieves sensitivities
of 99.5%, 93.1%, and 97% for classifying COVID-19, MERS, and SARS images,
respectively. In contrast, for segmented CXRs, InceptionV3 outperformed using
the original CXR dataset and achieved sensitivities of 96.94%, 79.68%, and
90.26% for classifying COVID-19, MERS, and SARS images, respectively. All
networks showed high COVID-19 detection sensitivity (>96%) with the segmented
lung images. This indicates the unique radiographic signature of COVID-19 cases
in the eyes of AI, which is often a challenging task for medical doctors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Tahir_A/0/1/0/all/0/1"&gt;Anas Tahir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Qiblawey_Y/0/1/0/all/0/1"&gt;Yazan Qiblawey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Khandakar_A/0/1/0/all/0/1"&gt;Amith Khandakar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rahman_T/0/1/0/all/0/1"&gt;Tawsifur Rahman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Khurshid_U/0/1/0/all/0/1"&gt;Uzair Khurshid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Musharavati_F/0/1/0/all/0/1"&gt;Farayi Musharavati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Islam_M/0/1/0/all/0/1"&gt;M. T. Islam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kiranyaz_S/0/1/0/all/0/1"&gt;Serkan Kiranyaz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chowdhury_M/0/1/0/all/0/1"&gt;Muhammad E. H. Chowdhury&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Locally Valid and Discriminative Confidence Intervals for Deep Learning Models. (arXiv:2106.00225v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.00225</id>
        <link href="http://arxiv.org/abs/2106.00225"/>
        <updated>2021-06-02T02:50:02.234Z</updated>
        <summary type="html"><![CDATA[Crucial for building trust in deep learning models for critical real-world
applications is efficient and theoretically sound uncertainty quantification, a
task that continues to be challenging. Useful uncertainty information is
expected to have two key properties: It should be valid (guaranteeing coverage)
and discriminative (more uncertain when the expected risk is high). Moreover,
when combined with deep learning (DL) methods, it should be scalable and affect
the DL model performance minimally. Most existing Bayesian methods lack
frequentist coverage guarantees and usually affect model performance. The few
available frequentist methods are rarely discriminative and/or violate coverage
guarantees due to unrealistic assumptions. Moreover, many methods are expensive
or require substantial modifications to the base neural network. Building upon
recent advances in conformal prediction and leveraging the classical idea of
kernel regression, we propose Locally Valid and Discriminative confidence
intervals (LVD), a simple, efficient and lightweight method to construct
discriminative confidence intervals (CIs) for almost any DL model. With no
assumptions on the data distribution, such CIs also offer finite-sample local
coverage guarantees (contrasted to the simpler marginal coverage). Using a
diverse set of datasets, we empirically verify that besides being the only
locally valid method, LVD also exceeds or matches the performance (including
coverage rate and prediction accuracy) of existing uncertainty quantification
methods, while offering additional benefits in scalability and flexibility.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1"&gt;Zhen Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Trivedi_S/0/1/0/all/0/1"&gt;Shubhendu Trivedi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1"&gt;Jimeng Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Table Tennis Stroke Recognition Using Two-Dimensional Human Pose Estimation. (arXiv:2104.09907v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.09907</id>
        <link href="http://arxiv.org/abs/2104.09907"/>
        <updated>2021-06-02T02:50:02.233Z</updated>
        <summary type="html"><![CDATA[We introduce a novel method for collecting table tennis video data and
perform stroke detection and classification. A diverse dataset containing video
data of 11 basic strokes obtained from 14 professional table tennis players,
summing up to a total of 22111 videos has been collected using the proposed
setup. The temporal convolutional neural network model developed using 2D pose
estimation performs multiclass classification of these 11 table tennis strokes
with a validation accuracy of 99.37%. Moreover, the neural network generalizes
well over the data of a player excluded from the training and validation
dataset, classifying the fresh strokes with an overall best accuracy of 98.72%.
Various model architectures using machine learning and deep learning based
approaches have been trained for stroke recognition and their performances have
been compared and benchmarked. Inferences such as performance monitoring and
stroke comparison of the players using the model have been discussed.
Therefore, we are contributing to the development of a computer vision based
sports analytics system for the sport of table tennis that focuses on the
previously unexploited aspect of the sport i.e., a player's strokes, which is
extremely insightful for performance improvement.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kulkarni_K/0/1/0/all/0/1"&gt;Kaustubh Milind Kulkarni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shenoy_S/0/1/0/all/0/1"&gt;Sucheth Shenoy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Concurrent Adversarial Learning for Large-Batch Training. (arXiv:2106.00221v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.00221</id>
        <link href="http://arxiv.org/abs/2106.00221"/>
        <updated>2021-06-02T02:50:02.225Z</updated>
        <summary type="html"><![CDATA[Large-batch training has become a commonly used technique when training
neural networks with a large number of GPU/TPU processors. As batch size
increases, stochastic optimizers tend to converge to sharp local minima,
leading to degraded test performance. Current methods usually use extensive
data augmentation to increase the batch size, but we found the performance gain
with data augmentation decreases as batch size increases, and data augmentation
will become insufficient after certain point. In this paper, we propose to use
adversarial learning to increase the batch size in large-batch training.
Despite being a natural choice for smoothing the decision surface and biasing
towards a flat region, adversarial learning has not been successfully applied
in large-batch training since it requires at least two sequential gradient
computations at each step, which will at least double the running time compared
with vanilla training even with a large number of processors. To overcome this
issue, we propose a novel Concurrent Adversarial Learning (ConAdv) method that
decouple the sequential gradient computations in adversarial learning by
utilizing staled parameters. Experimental results demonstrate that ConAdv can
successfully increase the batch size on both ResNet-50 and EfficientNet
training on ImageNet while maintaining high accuracy. In particular, we show
ConAdv along can achieve 75.3\% top-1 accuracy on ImageNet ResNet-50 training
with 96K batch size, and the accuracy can be further improved to 76.2\% when
combining ConAdv with data augmentation. This is the first work successfully
scales ResNet-50 training batch size to 96K.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xiangning Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1"&gt;Minhao Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1"&gt;Cho-Jui Hsieh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1"&gt;Yang You&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep-Learning Discovers Macroscopic Governing Equations for Viscous Gravity Currents from Microscopic Simulation Data. (arXiv:2106.00009v1 [physics.comp-ph])]]></title>
        <id>http://arxiv.org/abs/2106.00009</id>
        <link href="http://arxiv.org/abs/2106.00009"/>
        <updated>2021-06-02T02:50:02.223Z</updated>
        <summary type="html"><![CDATA[Although deep-learning has been successfully applied in a variety of science
and engineering problems owing to its strong high-dimensional nonlinear mapping
capability, it is of limited use in scientific knowledge discovery. In this
work, we propose a deep-learning based framework to discover the macroscopic
governing equation of viscous gravity current based on high-resolution
microscopic simulation data without the need for prior knowledge of underlying
terms. For two typical scenarios with different viscosity ratios, the
deep-learning based equations exactly capture the same dominated terms as the
theoretically derived equations for describing long-term asymptotic behaviors,
which validates the proposed framework. Unknown macroscopic equations are then
obtained for describing short-term behaviors, and hidden mechanisms are
eventually discovered with deep-learned explainable compensation terms and
corresponding coefficients. Consequently, the presented deep-learning framework
shows considerable potential for discovering unrevealed intrinsic laws in
scientific semantic space from raw experimental or simulation results in data
space.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Zeng_J/0/1/0/all/0/1"&gt;Junsheng Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Xu_H/0/1/0/all/0/1"&gt;Hao Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yuntian Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Zhang_D/0/1/0/all/0/1"&gt;Dongxiao Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Networks for Entity Matching: A Survey. (arXiv:2010.11075v2 [cs.DB] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.11075</id>
        <link href="http://arxiv.org/abs/2010.11075"/>
        <updated>2021-06-02T02:50:02.222Z</updated>
        <summary type="html"><![CDATA[Entity matching is the problem of identifying which records refer to the same
real-world entity. It has been actively researched for decades, and a variety
of different approaches have been developed. Even today, it remains a
challenging problem, and there is still generous room for improvement. In
recent years we have seen new methods based upon deep learning techniques for
natural language processing emerge.

In this survey, we present how neural networks have been used for entity
matching. Specifically, we identify which steps of the entity matching process
existing work have targeted using neural networks, and provide an overview of
the different techniques used at each step. We also discuss contributions from
deep learning in entity matching compared to traditional methods, and propose a
taxonomy of deep neural networks for entity matching.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Barlaug_N/0/1/0/all/0/1"&gt;Nils Barlaug&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gulla_J/0/1/0/all/0/1"&gt;Jon Atle Gulla&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Node-Variant Graph Filters in Graph Neural Networks. (arXiv:2106.00089v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.00089</id>
        <link href="http://arxiv.org/abs/2106.00089"/>
        <updated>2021-06-02T02:50:02.219Z</updated>
        <summary type="html"><![CDATA[Graph neural networks (GNNs) have been successfully employed in a myriad of
applications involving graph-structured data. Theoretical findings establish
that GNNs use nonlinear activation functions to create low-eigenvalue frequency
content that can be processed in a stable manner by subsequent graph
convolutional filters. However, the exact shape of the frequency content
created by nonlinear functions is not known, and thus, it cannot be learned nor
controlled. In this work, node-variant graph filters (NVGFs) are shown to be
capable of creating frequency content and are thus used in lieu of nonlinear
activation functions. This results in a novel GNN architecture that, although
linear, is capable of creating frequency content as well. Furthermore, this new
frequency content can be either designed or learned from data. In this way, the
role of frequency creation is separated from the nonlinear nature of
traditional GNNs. Extensive simulations are carried out to differentiate the
contributions of frequency creation from those of the nonlinearity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gama_F/0/1/0/all/0/1"&gt;Fernando Gama&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anderson_B/0/1/0/all/0/1"&gt;Brendon G. Anderson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sojoudi_S/0/1/0/all/0/1"&gt;Somayeh Sojoudi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Low-Resource Spoken Language Identification Using Self-Attentive Pooling and Deep 1D Time-Channel Separable Convolutions. (arXiv:2106.00052v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2106.00052</id>
        <link href="http://arxiv.org/abs/2106.00052"/>
        <updated>2021-06-02T02:50:02.218Z</updated>
        <summary type="html"><![CDATA[This memo describes NTR/TSU winning submission for Low Resource ASR challenge
at Dialog2021 conference, language identification track.

Spoken Language Identification (LID) is an important step in a multilingual
Automated Speech Recognition (ASR) system pipeline. Traditionally, the ASR task
requires large volumes of labeled data that are unattainable for most of the
world's languages, including most of the languages of Russia. In this memo, we
show that a convolutional neural network with a Self-Attentive Pooling layer
shows promising results in low-resource setting for the language identification
task and set up a SOTA for the Low Resource ASR challenge dataset.

Additionally, we compare the structure of confusion matrices for this and
significantly more diverse VoxForge dataset and state and substantiate the
hypothesis that whenever the dataset is diverse enough so that the other
classification factors, like gender, age etc. are well-averaged, the confusion
matrix for LID system bears the language similarity measure.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Bedyakin_R/0/1/0/all/0/1"&gt;Roman Bedyakin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mikhaylovskiy_N/0/1/0/all/0/1"&gt;Nikolay Mikhaylovskiy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data-Driven Shadowgraph Simulation of a 3D Object. (arXiv:2106.00317v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.00317</id>
        <link href="http://arxiv.org/abs/2106.00317"/>
        <updated>2021-06-02T02:50:02.217Z</updated>
        <summary type="html"><![CDATA[In this work we propose a deep neural network based surrogate model for a
plasma shadowgraph - a technique for visualization of perturbations in a
transparent medium. We are substituting the numerical code by a computationally
cheaper projection based surrogate model that is able to approximate the
electric fields at a given time without computing all preceding electric fields
as required by numerical methods. This means that the projection based
surrogate model allows to recover the solution of the governing 3D partial
differential equation, 3D wave equation, at any point of a given compute domain
and configuration without the need to run a full simulation. This model has
shown a good quality of reconstruction in a problem of interpolation of data
within a narrow range of simulation parameters and can be used for input data
of large size.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Willmann_A/0/1/0/all/0/1"&gt;Anna Willmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stiller_P/0/1/0/all/0/1"&gt;Patrick Stiller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Debus_A/0/1/0/all/0/1"&gt;Alexander Debus&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Irman_A/0/1/0/all/0/1"&gt;Arie Irman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pausch_R/0/1/0/all/0/1"&gt;Richard Pausch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1"&gt;Yen-Yu Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bussmann_M/0/1/0/all/0/1"&gt;Michael Bussmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoffmann_N/0/1/0/all/0/1"&gt;Nico Hoffmann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What's a good imputation to predict with missing values?. (arXiv:2106.00311v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.00311</id>
        <link href="http://arxiv.org/abs/2106.00311"/>
        <updated>2021-06-02T02:50:02.214Z</updated>
        <summary type="html"><![CDATA[How to learn a good predictor on data with missing values? Most efforts focus
on first imputing as well as possible and second learning on the completed data
to predict the outcome. Yet, this widespread practice has no theoretical
grounding. Here we show that for almost all imputation functions, an
impute-then-regress procedure with a powerful learner is Bayes optimal. This
result holds for all missing-values mechanisms, in contrast with the classic
statistical results that require missing-at-random settings to use imputation
in probabilistic modeling. Moreover, it implies that perfect conditional
imputation may not be needed for good prediction asymptotically. In fact, we
show that on perfectly imputed data the best regression function will generally
be discontinuous, which makes it hard to learn. Crafting instead the imputation
so as to leave the regression function unchanged simply shifts the problem to
learning discontinuous imputations. Rather, we suggest that it is easier to
learn imputation and regression jointly. We propose such a procedure, adapting
NeuMiss, a neural network capturing the conditional links across observed and
unobserved variables whatever the missing-value pattern. Experiments confirm
that joint imputation and regression through NeuMiss is better than various two
step procedures in our experiments with finite number of samples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Morvan_M/0/1/0/all/0/1"&gt;Marine Le Morvan&lt;/a&gt; (PARIETAL, IJCLab), &lt;a href="http://arxiv.org/find/stat/1/au:+Josse_J/0/1/0/all/0/1"&gt;Julie Josse&lt;/a&gt; (CRISAM), &lt;a href="http://arxiv.org/find/stat/1/au:+Scornet_E/0/1/0/all/0/1"&gt;Erwan Scornet&lt;/a&gt; (CMAP), &lt;a href="http://arxiv.org/find/stat/1/au:+Varoquaux_G/0/1/0/all/0/1"&gt;Ga&amp;#xeb;l Varoquaux&lt;/a&gt; (PARIETAL)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Analysis of classifiers robust to noisy labels. (arXiv:2106.00274v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.00274</id>
        <link href="http://arxiv.org/abs/2106.00274"/>
        <updated>2021-06-02T02:50:02.212Z</updated>
        <summary type="html"><![CDATA[We explore contemporary robust classification algorithms for overcoming
class-dependant labelling noise: Forward, Importance Re-weighting and
T-revision. The classifiers are trained and evaluated on class-conditional
random label noise data while the final test data is clean. We demonstrate
methods for estimating the transition matrix in order to obtain better
classifier performance when working with noisy data. We apply deep learning to
three data-sets and derive an end-to-end analysis with unknown noise on the
CIFAR data-set from scratch. The effectiveness and robustness of the
classifiers are analysed, and we compare and contrast the results of each
experiment are using top-1 accuracy as our criterion.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Diaz_A/0/1/0/all/0/1"&gt;Alex D&amp;#xed;az&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Steele_D/0/1/0/all/0/1"&gt;Damian Steele&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The zoo of Fairness metrics in Machine Learning. (arXiv:2106.00467v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.00467</id>
        <link href="http://arxiv.org/abs/2106.00467"/>
        <updated>2021-06-02T02:50:02.212Z</updated>
        <summary type="html"><![CDATA[In the recent years, the problem of addressing fairness in Machine Learning
(ML) and automatic decision-making has attracted a lot of attention in the
scientific communities dealing with Artificial Intelligence. A plethora of
different definitions of fairness in ML have been proposed, that consider
different notions of what is a "fair decision" in situations impacting
individuals in the population. The precise differences, implications and
"orthogonality" between these notions have not yet been fully analyzed in the
literature. In this work, we try to make some order out of this zoo of
definitions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Castelnovo_A/0/1/0/all/0/1"&gt;Alessandro Castelnovo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Crupi_R/0/1/0/all/0/1"&gt;Riccardo Crupi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Greco_G/0/1/0/all/0/1"&gt;Greta Greco&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Regoli_D/0/1/0/all/0/1"&gt;Daniele Regoli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sample Selection with Uncertainty of Losses for Learning with Noisy Labels. (arXiv:2106.00445v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.00445</id>
        <link href="http://arxiv.org/abs/2106.00445"/>
        <updated>2021-06-02T02:50:02.211Z</updated>
        <summary type="html"><![CDATA[In learning with noisy labels, the sample selection approach is very popular,
which regards small-loss data as correctly labeled during training. However,
losses are generated on-the-fly based on the model being trained with noisy
labels, and thus large-loss data are likely but not certainly to be incorrect.
There are actually two possibilities of a large-loss data point: (a) it is
mislabeled, and then its loss decreases slower than other data, since deep
neural networks "learn patterns first"; (b) it belongs to an underrepresented
group of data and has not been selected yet. In this paper, we incorporate the
uncertainty of losses by adopting interval estimation instead of point
estimation of losses, where lower bounds of the confidence intervals of losses
derived from distribution-free concentration inequalities, but not losses
themselves, are used for sample selection. In this way, we also give large-loss
but less selected data a try; then, we can better distinguish between the cases
(a) and (b) by seeing if the losses effectively decrease with the uncertainty
after the try. As a result, we can better explore underrepresented data that
are correctly labeled but seem to be mislabeled at first glance. Experiments
demonstrate that the proposed method is superior to baselines and robust to a
broad range of label noise types.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xia_X/0/1/0/all/0/1"&gt;Xiaobo Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tongliang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1"&gt;Bo Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_M/0/1/0/all/0/1"&gt;Mingming Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1"&gt;Jun Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niu_G/0/1/0/all/0/1"&gt;Gang Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sugiyama_M/0/1/0/all/0/1"&gt;Masashi Sugiyama&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Enhancing Trajectory Prediction using Sparse Outputs: Application to Team Sports. (arXiv:2106.00173v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.00173</id>
        <link href="http://arxiv.org/abs/2106.00173"/>
        <updated>2021-06-02T02:50:02.179Z</updated>
        <summary type="html"><![CDATA[Sophisticated trajectory prediction models that effectively mimic team
dynamics have many potential uses for sports coaches, broadcasters and
spectators. However, through experiments on soccer data we found that it can be
surprisingly challenging to train a deep learning model for player trajectory
prediction which outperforms linear extrapolation on average distance between
predicted and true future trajectories. We propose and test a novel method for
improving training by predicting a sparse trajectory and interpolating using
constant acceleration, which improves performance for several models. This
interpolation can also be used on models that aren't trained with sparse
outputs, and we find that this consistently improves performance for all tested
models. Additionally, we find that the accuracy of predicted trajectories for a
subset of players can be improved by conditioning on the full trajectories of
the other players, and that this is further improved when combined with sparse
predictions. We also propose a novel architecture using graph networks and
multi-head attention (GraN-MA) which achieves better performance than other
tested state-of-the-art models on our dataset and is trivially adapted for both
sparse trajectories and full-trajectory conditioned trajectory prediction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Victor_B/0/1/0/all/0/1"&gt;Brandon Victor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nibali_A/0/1/0/all/0/1"&gt;Aiden Nibali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1"&gt;Zhen He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carey_D/0/1/0/all/0/1"&gt;David L. Carey&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Variational Combinatorial Sequential Monte Carlo Methods for Bayesian Phylogenetic Inference. (arXiv:2106.00075v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.00075</id>
        <link href="http://arxiv.org/abs/2106.00075"/>
        <updated>2021-06-02T02:50:02.141Z</updated>
        <summary type="html"><![CDATA[Bayesian phylogenetic inference is often conducted via local or sequential
search over topologies and branch lengths using algorithms such as random-walk
Markov chain Monte Carlo (MCMC) or Combinatorial Sequential Monte Carlo (CSMC).
However, when MCMC is used for evolutionary parameter learning, convergence
requires long runs with inefficient exploration of the state space. We
introduce Variational Combinatorial Sequential Monte Carlo (VCSMC), a powerful
framework that establishes variational sequential search to learn distributions
over intricate combinatorial structures. We then develop nested CSMC, an
efficient proposal distribution for CSMC and prove that nested CSMC is an exact
approximation to the (intractable) locally optimal proposal. We use nested CSMC
to define a second objective, VNCSMC which yields tighter lower bounds than
VCSMC. We show that VCSMC and VNCSMC are computationally efficient and explore
higher probability spaces than existing methods on a range of tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Moretti_A/0/1/0/all/0/1"&gt;Antonio Khalil Moretti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Liyi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Naesseth_C/0/1/0/all/0/1"&gt;Christian A. Naesseth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Venner_H/0/1/0/all/0/1"&gt;Hadiah Venner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Blei_D/0/1/0/all/0/1"&gt;David Blei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Peer_I/0/1/0/all/0/1"&gt;Itsik Pe&amp;#x27;er&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection. (arXiv:2106.00666v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.00666</id>
        <link href="http://arxiv.org/abs/2106.00666"/>
        <updated>2021-06-02T02:50:02.129Z</updated>
        <summary type="html"><![CDATA[Can Transformer perform $2\mathrm{D}$ object-level recognition from a pure
sequence-to-sequence perspective with minimal knowledge about the $2\mathrm{D}$
spatial structure? To answer this question, we present You Only Look at One
Sequence (YOLOS), a series of object detection models based on the na\"ive
Vision Transformer with the fewest possible modifications as well as inductive
biases. We find that YOLOS pre-trained on the mid-sized ImageNet-$1k$ dataset
only can already achieve competitive object detection performance on COCO,
\textit{e.g.}, YOLOS-Base directly adopted from BERT-Base can achieve $42.0$
box AP. We also discuss the impacts as well as limitations of current pre-train
schemes and model scaling strategies for Transformer in vision through object
detection. Code and model weights are available at
\url{https://github.com/hustvl/YOLOS}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1"&gt;Yuxin Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liao_B/0/1/0/all/0/1"&gt;Bencheng Liao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xinggang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fang_J/0/1/0/all/0/1"&gt;Jiemin Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_J/0/1/0/all/0/1"&gt;Jiyang Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1"&gt;Rui Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niu_J/0/1/0/all/0/1"&gt;Jianwei Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wenyu Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Source Data-absent Unsupervised Domain Adaptation through Hypothesis Transfer and Labeling Transfer. (arXiv:2012.07297v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.07297</id>
        <link href="http://arxiv.org/abs/2012.07297"/>
        <updated>2021-06-02T02:50:02.127Z</updated>
        <summary type="html"><![CDATA[Unsupervised domain adaptation (UDA) aims to transfer knowledge from a
related but different well-labeled source domain to a new unlabeled target
domain. Most existing UDA methods require access to the source data, and thus
are not applicable when the data are confidential and not shareable due to
privacy concerns. This paper aims to tackle a realistic setting with only a
classification model available trained over, instead of accessing to, the
source data. To effectively utilize the source model for adaptation, we propose
a novel approach called Source HypOthesis Transfer (SHOT), which learns the
feature extraction module for the target domain by fitting the target data
features to the frozen source classification module (representing
classification hypothesis). Specifically, SHOT exploits both information
maximization and self-supervised learning for the feature extraction module
learning to ensure the target features are implicitly aligned with the features
of unseen source data via the same hypothesis. Furthermore, we propose a new
labeling transfer strategy, which separates the target data into two splits
based on the confidence of predictions (labeling information), and then employ
semi-supervised learning to improve the accuracy of less-confident predictions
in the target domain. We denote labeling transfer as SHOT++ if the predictions
are obtained by SHOT. Extensive experiments on both digit classification and
object recognition tasks show that SHOT and SHOT++ achieve results surpassing
or comparable to the state-of-the-arts, demonstrating the effectiveness of our
approaches for various visual domain adaptation problems. Code will be
available at \url{https://github.com/tim-learn/SHOT-plus}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1"&gt;Jian Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_D/0/1/0/all/0/1"&gt;Dapeng Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yunbo Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1"&gt;Ran He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1"&gt;Jiashi Feng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quantum Federated Learning with Quantum Data. (arXiv:2106.00005v1 [quant-ph])]]></title>
        <id>http://arxiv.org/abs/2106.00005</id>
        <link href="http://arxiv.org/abs/2106.00005"/>
        <updated>2021-06-02T02:50:02.127Z</updated>
        <summary type="html"><![CDATA[Quantum machine learning (QML) has emerged as a promising field that leans on
the developments in quantum computing to explore large complex machine learning
problems. Recently, some purely quantum machine learning models were proposed
such as the quantum convolutional neural networks (QCNN) to perform
classification on quantum data. However, all of the existing QML models rely on
centralized solutions that cannot scale well for large-scale and distributed
quantum networks. Hence, it is apropos to consider more practical quantum
federated learning (QFL) solutions tailored towards emerging quantum network
architectures. Indeed, developing QFL frameworks for quantum networks is
critical given the fragile nature of computing qubits and the difficulty of
transferring them. On top of its practical momentousness, QFL allows for
distributed quantum learning by leveraging existing wireless communication
infrastructure. This paper proposes the first fully quantum federated learning
framework that can operate over quantum data and, thus, share the learning of
quantum circuit parameters in a decentralized manner. First, given the lack of
existing quantum federated datasets in the literature, the proposed framework
begins by generating the first quantum federated dataset, with a hierarchical
data format, for distributed quantum networks. Then, clients sharing QCNN
models are fed with the quantum data to perform a classification task.
Subsequently, the server aggregates the learnable quantum circuit parameters
from clients and performs federated averaging. Extensive experiments are
conducted to evaluate and validate the effectiveness of the proposed QFL
solution. This work is the first to combine Google's TensorFlow Federated and
TensorFlow Quantum in a practical implementation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Chehimi_M/0/1/0/all/0/1"&gt;Mahdi Chehimi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Saad_W/0/1/0/all/0/1"&gt;Walid Saad&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Fast Sampling of Diffusion Probabilistic Models. (arXiv:2106.00132v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.00132</id>
        <link href="http://arxiv.org/abs/2106.00132"/>
        <updated>2021-06-02T02:50:02.124Z</updated>
        <summary type="html"><![CDATA[In this work, we propose FastDPM, a unified framework for fast sampling in
diffusion probabilistic models. FastDPM generalizes previous methods and gives
rise to new algorithms with improved sample quality. We systematically
investigate the fast sampling methods under this framework across different
domains, on different datasets, and with different amount of conditional
information provided for generation. We find the performance of a particular
method depends on data domains (e.g., image or audio), the trade-off between
sampling speed and sample quality, and the amount of conditional information.
We further provide insights and recipes on the choice of methods for
practitioners.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kong_Z/0/1/0/all/0/1"&gt;Zhifeng Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ping_W/0/1/0/all/0/1"&gt;Wei Ping&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Early Detection of COVID-19 Hotspots Using Spatio-Temporal Data. (arXiv:2106.00072v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.00072</id>
        <link href="http://arxiv.org/abs/2106.00072"/>
        <updated>2021-06-02T02:50:02.122Z</updated>
        <summary type="html"><![CDATA[Recently, the Centers for Disease Control and Prevention (CDC) has worked
with other federal agencies to identify counties with increasing coronavirus
disease 2019 (COVID-19) incidence (hotspots) and offers support to local health
departments to limit the spread of the disease. Understanding the
spatio-temporal dynamics of hotspot events is of great importance to support
policy decisions and prevent large-scale outbreaks. This paper presents a
spatio-temporal Bayesian framework for early detection of COVID-19 hotspots (at
the county level) in the United States. We assume both the observed number of
cases and hotspots depend on a class of latent random variables, which encode
the underlying spatio-temporal dynamics of the transmission of COVID-19. Such
latent variables follow a zero-mean Gaussian process, whose covariance is
specified by a non-stationary kernel function. The most salient feature of our
kernel function is that deep neural networks are introduced to enhance the
model's representative power while still enjoying the interpretability of the
kernel. We derive a sparse model and fit the model using a variational learning
strategy to circumvent the computational intractability for large data sets.
Our model demonstrates better interpretability and superior hotspot-detection
performance compared to other baseline methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Zhu_S/0/1/0/all/0/1"&gt;Shixiang Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Bukharin_A/0/1/0/all/0/1"&gt;Alexander Bukharin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Xie_L/0/1/0/all/0/1"&gt;Liyan Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Yang_S/0/1/0/all/0/1"&gt;Shihao Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Keskinocak_P/0/1/0/all/0/1"&gt;Pinar Keskinocak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Xie_Y/0/1/0/all/0/1"&gt;Yao Xie&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generalized AdaGrad (G-AdaGrad) and Adam: A State-Space Perspective. (arXiv:2106.00092v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.00092</id>
        <link href="http://arxiv.org/abs/2106.00092"/>
        <updated>2021-06-02T02:50:02.121Z</updated>
        <summary type="html"><![CDATA[Accelerated gradient-based methods are being extensively used for solving
non-convex machine learning problems, especially when the data points are
abundant or the available data is distributed across several agents. Two of the
prominent accelerated gradient algorithms are AdaGrad and Adam. AdaGrad is the
simplest accelerated gradient method, which is particularly effective for
sparse data. Adam has been shown to perform favorably in deep learning problems
compared to other methods. In this paper, we propose a new fast optimizer,
Generalized AdaGrad (G-AdaGrad), for accelerating the solution of potentially
non-convex machine learning problems. Specifically, we adopt a state-space
perspective for analyzing the convergence of gradient acceleration algorithms,
namely G-AdaGrad and Adam, in machine learning. Our proposed state-space models
are governed by ordinary differential equations. We present simple convergence
proofs of these two algorithms in the deterministic settings with minimal
assumptions. Our analysis also provides intuition behind improving upon
AdaGrad's convergence rate. We provide empirical results on MNIST dataset to
reinforce our claims on the convergence and performance of G-AdaGrad and Adam.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chakrabarti_K/0/1/0/all/0/1"&gt;Kushal Chakrabarti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chopra_N/0/1/0/all/0/1"&gt;Nikhil Chopra&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Conditional Coverage via Orthogonal Quantile Regression. (arXiv:2106.00394v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.00394</id>
        <link href="http://arxiv.org/abs/2106.00394"/>
        <updated>2021-06-02T02:50:02.120Z</updated>
        <summary type="html"><![CDATA[We develop a method to generate prediction intervals that have a
user-specified coverage level across all regions of feature-space, a property
called conditional coverage. A typical approach to this task is to estimate the
conditional quantiles with quantile regression -- it is well-known that this
leads to correct coverage in the large-sample limit, although it may not be
accurate in finite samples. We find in experiments that traditional quantile
regression can have poor conditional coverage. To remedy this, we modify the
loss function to promote independence between the size of the intervals and the
indicator of a miscoverage event. For the true conditional quantiles, these two
quantities are independent (orthogonal), so the modified loss function
continues to be valid. Moreover, we empirically show that the modified loss
function leads to improved conditional coverage, as evaluated by several
metrics. We also introduce two new metrics that check conditional coverage by
looking at the strength of the dependence between the interval size and the
indicator of miscoverage.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Feldman_S/0/1/0/all/0/1"&gt;Shai Feldman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bates_S/0/1/0/all/0/1"&gt;Stephen Bates&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Romano_Y/0/1/0/all/0/1"&gt;Yaniv Romano&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hybrid Generative Models for Two-Dimensional Datasets. (arXiv:2106.00203v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.00203</id>
        <link href="http://arxiv.org/abs/2106.00203"/>
        <updated>2021-06-02T02:50:02.118Z</updated>
        <summary type="html"><![CDATA[Two-dimensional array-based datasets are pervasive in a variety of domains.
Current approaches for generative modeling have typically been limited to
conventional image datasets and performed in the pixel domain which do not
explicitly capture the correlation between pixels. Additionally, these
approaches do not extend to scientific and other applications where each
element value is continuous and is not limited to a fixed range. In this paper,
we propose a novel approach for generating two-dimensional datasets by moving
the computations to the space of representation bases and show its usefulness
for two different datasets, one from imaging and another from scientific
computing. The proposed approach is general and can be applied to any dataset,
representation basis, or generative model. We provide a comprehensive
performance comparison of various combinations of generative models and
representation basis spaces. We also propose a new evaluation metric which
captures the deficiency of generating images in pixel space.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shajari_H/0/1/0/all/0/1"&gt;Hoda Shajari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jaemoon Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ranka_S/0/1/0/all/0/1"&gt;Sanjay Ranka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rangarajan_A/0/1/0/all/0/1"&gt;Anand Rangarajan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DP-MERF: Differentially Private Mean Embeddings with Random Features for Practical Privacy-Preserving Data Generation. (arXiv:2002.11603v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.11603</id>
        <link href="http://arxiv.org/abs/2002.11603"/>
        <updated>2021-06-02T02:50:02.107Z</updated>
        <summary type="html"><![CDATA[We propose a differentially private data generation paradigm using random
feature representations of kernel mean embeddings when comparing the
distribution of true data with that of synthetic data. We exploit the random
feature representations for two important benefits. First, we require a minimal
privacy cost for training deep generative models. This is because unlike
kernel-based distance metrics that require computing the kernel matrix on all
pairs of true and synthetic data points, we can detach the data-dependent term
from the term solely dependent on synthetic data. Hence, we need to perturb the
data-dependent term only once and then use it repeatedly during the generator
training. Second, we can obtain an analytic sensitivity of the kernel mean
embedding as the random features are norm bounded by construction. This removes
the necessity of hyper-parameter search for a clipping norm to handle the
unknown sensitivity of a generator network. We provide several variants of our
algorithm, differentially-private mean embeddings with random features
(DP-MERF) to jointly generate labels and input features for datasets such as
heterogeneous tabular data and image data. Our algorithm achieves drastically
better privacy-utility trade-offs than existing methods when tested on several
datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Harder_F/0/1/0/all/0/1"&gt;Frederik Harder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Adamczewski_K/0/1/0/all/0/1"&gt;Kamil Adamczewski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_M/0/1/0/all/0/1"&gt;Mijung Park&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Explanations for Monotonic Classifiers. (arXiv:2106.00154v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.00154</id>
        <link href="http://arxiv.org/abs/2106.00154"/>
        <updated>2021-06-02T02:50:02.105Z</updated>
        <summary type="html"><![CDATA[In many classification tasks there is a requirement of monotonicity.
Concretely, if all else remains constant, increasing (resp. decreasing) the
value of one or more features must not decrease (resp. increase) the value of
the prediction. Despite comprehensive efforts on learning monotonic
classifiers, dedicated approaches for explaining monotonic classifiers are
scarce and classifier-specific. This paper describes novel algorithms for the
computation of one formal explanation of a (black-box) monotonic classifier.
These novel algorithms are polynomial in the run time complexity of the
classifier and the number of features. Furthermore, the paper presents a
practically efficient model-agnostic algorithm for enumerating formal
explanations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Marques_Silva_J/0/1/0/all/0/1"&gt;Joao Marques-Silva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gerspacher_T/0/1/0/all/0/1"&gt;Thomas Gerspacher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cooper_M/0/1/0/all/0/1"&gt;Martin Cooper&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ignatiev_A/0/1/0/all/0/1"&gt;Alexey Ignatiev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Narodytska_N/0/1/0/all/0/1"&gt;Nina Narodytska&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PUDLE: Implicit Acceleration of Dictionary Learning by Backpropagation. (arXiv:2106.00058v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.00058</id>
        <link href="http://arxiv.org/abs/2106.00058"/>
        <updated>2021-06-02T02:50:02.104Z</updated>
        <summary type="html"><![CDATA[The dictionary learning problem, representing data as a combination of few
atoms, has long stood as a popular method for learning representations in
statistics and signal processing. The most popular dictionary learning
algorithm alternates between sparse coding and dictionary update steps, and a
rich literature has studied its theoretical convergence. The growing popularity
of neurally plausible unfolded sparse coding networks has led to the empirical
finding that backpropagation through such networks performs dictionary
learning. This paper offers the first theoretical proof for these empirical
results through PUDLE, a Provable Unfolded Dictionary LEarning method. We
highlight the impact of loss, unfolding, and backpropagation on convergence. We
discover an implicit acceleration: as a function of unfolding, the
backpropagated gradient converges faster and is more accurate than the gradient
from alternating minimization. We complement our findings through synthetic and
image denoising experiments. The findings support the use of accelerated deep
learning optimizers and unfolded networks for dictionary learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tolooshams_B/0/1/0/all/0/1"&gt;Bahareh Tolooshams&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ba_D/0/1/0/all/0/1"&gt;Demba Ba&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Hop Fact Checking of Political Claims. (arXiv:2009.06401v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.06401</id>
        <link href="http://arxiv.org/abs/2009.06401"/>
        <updated>2021-06-02T02:50:02.104Z</updated>
        <summary type="html"><![CDATA[Recent work has proposed multi-hop models and datasets for studying complex
natural language reasoning. One notable task requiring multi-hop reasoning is
fact checking, where a set of connected evidence pieces leads to the final
verdict of a claim. However, existing datasets either do not provide
annotations for gold evidence pages, or the only dataset which does (FEVER)
mostly consists of claims which can be fact-checked with simple reasoning and
is constructed artificially. Here, we study more complex claim verification of
naturally occurring claims with multiple hops over interconnected evidence
chunks. We: 1) construct a small annotated dataset, PolitiHop, of evidence
sentences for claim verification; 2) compare it to existing multi-hop datasets;
and 3) study how to transfer knowledge from more extensive in- and
out-of-domain resources to PolitiHop. We find that the task is complex and
achieve the best performance with an architecture that specifically models
reasoning over evidence pieces in combination with in-domain transfer learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ostrowski_W/0/1/0/all/0/1"&gt;Wojciech Ostrowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arora_A/0/1/0/all/0/1"&gt;Arnav Arora&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Atanasova_P/0/1/0/all/0/1"&gt;Pepa Atanasova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Augenstein_I/0/1/0/all/0/1"&gt;Isabelle Augenstein&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Long-Term Human Video Generation of Multiple Futures Using Poses. (arXiv:1904.07538v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1904.07538</id>
        <link href="http://arxiv.org/abs/1904.07538"/>
        <updated>2021-06-02T02:50:02.075Z</updated>
        <summary type="html"><![CDATA[Predicting future human behavior from an input human video is a useful task
for applications such as autonomous driving and robotics. While most previous
works predict a single future, multiple futures with different behavior can
potentially occur. Moreover, if the predicted future is too short (e.g., less
than one second), it may not be fully usable by a human or other systems. In
this paper, we propose a novel method for future human pose prediction capable
of predicting multiple long-term futures. This makes the predictions more
suitable for real applications. Also, from the input video and the predicted
human behavior, we generate future videos. First, from an input human video, we
generate sequences of future human poses (i.e., the image coordinates of their
body-joints) via adversarial learning. Adversarial learning suffers from mode
collapse, which makes it difficult to generate a variety of multiple poses. We
solve this problem by utilizing two additional inputs to the generator to make
the outputs diverse, namely, a latent code (to reflect various behaviors) and
an attraction point (to reflect various trajectories). In addition, we generate
long-term future human poses using a novel approach based on unidimensional
convolutional neural networks. Last, we generate an output video based on the
generated poses for visualization. We evaluate the generated future poses and
videos using three criteria (i.e., realism, diversity and accuracy), and show
that our proposed method outperforms other state-of-the-art works.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fushishita_N/0/1/0/all/0/1"&gt;Naoya Fushishita&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tejero_de_Pablos_A/0/1/0/all/0/1"&gt;Antonio Tejero-de-Pablos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mukuta_Y/0/1/0/all/0/1"&gt;Yusuke Mukuta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Harada_T/0/1/0/all/0/1"&gt;Tatsuya Harada&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Persistent Homology Captures the Generalization of Neural Networks Without A Validation Set. (arXiv:2106.00012v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.00012</id>
        <link href="http://arxiv.org/abs/2106.00012"/>
        <updated>2021-06-02T02:50:02.069Z</updated>
        <summary type="html"><![CDATA[The training of neural networks is usually monitored with a validation
(holdout) set to estimate the generalization of the model. This is done instead
of measuring intrinsic properties of the model to determine whether it is
learning appropriately. In this work, we suggest studying the training of
neural networks with Algebraic Topology, specifically Persistent Homology (PH).
Using simplicial complex representations of neural networks, we study the PH
diagram distance evolution on the neural network learning process with
different architectures and several datasets. Results show that the PH diagram
distance between consecutive neural network states correlates with the
validation accuracy, implying that the generalization error of a neural network
could be intrinsically estimated without any holdout set.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gutierrez_Fandino_A/0/1/0/all/0/1"&gt;Asier Guti&amp;#xe9;rrez-Fandi&amp;#xf1;o&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Perez_Fernandez_D/0/1/0/all/0/1"&gt;David P&amp;#xe9;rez-Fern&amp;#xe1;ndez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Armengol_Estape_J/0/1/0/all/0/1"&gt;Jordi Armengol-Estap&amp;#xe9;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Villegas_M/0/1/0/all/0/1"&gt;Marta Villegas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Gaussian Processes with Differential Privacy. (arXiv:2106.00474v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.00474</id>
        <link href="http://arxiv.org/abs/2106.00474"/>
        <updated>2021-06-02T02:50:02.061Z</updated>
        <summary type="html"><![CDATA[Gaussian processes (GPs) are non-parametric Bayesian models that are widely
used for diverse prediction tasks. Previous work in adding strong privacy
protection to GPs via differential privacy (DP) has been limited to protecting
only the privacy of the prediction targets (model outputs) but not inputs. We
break this limitation by introducing GPs with DP protection for both model
inputs and outputs. We achieve this by using sparse GP methodology and
publishing a private variational approximation on known inducing points. The
approximation covariance is adjusted to approximately account for the added
uncertainty from DP noise. The approximation can be used to compute arbitrary
predictions using standard sparse GP techniques. We propose a method for
hyperparameter learning using a private selection protocol applied to
validation set log-likelihood. Our experiments demonstrate that given
sufficient amount of data, the method can produce accurate models under strong
privacy protection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Honkela_A/0/1/0/all/0/1"&gt;Antti Honkela&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection. (arXiv:2106.00666v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.00666</id>
        <link href="http://arxiv.org/abs/2106.00666"/>
        <updated>2021-06-02T02:50:02.031Z</updated>
        <summary type="html"><![CDATA[Can Transformer perform $2\mathrm{D}$ object-level recognition from a pure
sequence-to-sequence perspective with minimal knowledge about the $2\mathrm{D}$
spatial structure? To answer this question, we present You Only Look at One
Sequence (YOLOS), a series of object detection models based on the na\"ive
Vision Transformer with the fewest possible modifications as well as inductive
biases. We find that YOLOS pre-trained on the mid-sized ImageNet-$1k$ dataset
only can already achieve competitive object detection performance on COCO,
\textit{e.g.}, YOLOS-Base directly adopted from BERT-Base can achieve $42.0$
box AP. We also discuss the impacts as well as limitations of current pre-train
schemes and model scaling strategies for Transformer in vision through object
detection. Code and model weights are available at
\url{https://github.com/hustvl/YOLOS}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1"&gt;Yuxin Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liao_B/0/1/0/all/0/1"&gt;Bencheng Liao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xinggang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fang_J/0/1/0/all/0/1"&gt;Jiemin Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_J/0/1/0/all/0/1"&gt;Jiyang Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1"&gt;Rui Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niu_J/0/1/0/all/0/1"&gt;Jianwei Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wenyu Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rotation Invariant Point Cloud Classification: Where Local Geometry Meets Global Topology. (arXiv:1911.00195v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1911.00195</id>
        <link href="http://arxiv.org/abs/1911.00195"/>
        <updated>2021-06-02T02:50:02.004Z</updated>
        <summary type="html"><![CDATA[Point cloud analysis is a fundamental task in 3D computer vision. Most
previous works have conducted experiments on synthetic datasets with
well-aligned data; while real-world point clouds are often not pre-aligned. How
to achieve rotation invariance remains an open problem in point cloud analysis.
To meet this challenge, we propose a novel approach toward achieving
rotation-invariant (RI) representations by combining local geometry with global
topology. In our local-global-representation (LGR)-Net, we have designed a
two-branch network where one stream encodes local geometric RI features and the
other encodes global topology-preserving RI features. Motivated by the
observation that local geometry and global topology have different yet
complementary RI responses in varying regions, two-branch RI features are fused
by an innovative multi-layer perceptron (MLP) based attention module. To the
best of our knowledge, this work is the first principled approach toward
adaptively combining global and local information under the context of RI point
cloud analysis. Extensive experiments have demonstrated that our LGR-Net
achieves the state-of-the-art performance on various rotation-augmented
versions of ModelNet40, ShapeNet, ScanObjectNN, and S3DIS.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1"&gt;Chen Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jiaqi Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_X/0/1/0/all/0/1"&gt;Xin Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_A/0/1/0/all/0/1"&gt;Angfan Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1"&gt;Zhiguo Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xin Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine-Learning Non-Conservative Dynamics for New-Physics Detection. (arXiv:2106.00026v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.00026</id>
        <link href="http://arxiv.org/abs/2106.00026"/>
        <updated>2021-06-02T02:50:01.988Z</updated>
        <summary type="html"><![CDATA[Energy conservation is a basic physics principle, the breakdown of which
often implies new physics. This paper presents a method for data-driven "new
physics" discovery. Specifically, given a trajectory governed by unknown
forces, our Neural New-Physics Detector (NNPhD) aims to detect new physics by
decomposing the force field into conservative and non-conservative components,
which are represented by a Lagrangian Neural Network (LNN) and a universal
approximator network (UAN), respectively, trained to minimize the force
recovery error plus a constant $\lambda$ times the magnitude of the predicted
non-conservative force. We show that a phase transition occurs at $\lambda$=1,
universally for arbitrary forces. We demonstrate that NNPhD successfully
discovers new physics in toy numerical experiments, rediscovering friction
(1493) from a damped double pendulum, Neptune from Uranus' orbit (1846) and
gravitational waves (2017) from an inspiraling orbit. We also show how NNPhD
coupled with an integrator outperforms previous methods for predicting the
future of a damped double pendulum.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Ziming Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1"&gt;Bohan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meng_Q/0/1/0/all/0/1"&gt;Qi Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1"&gt;Wei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tegmark_M/0/1/0/all/0/1"&gt;Max Tegmark&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tie-Yan Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reinforcement Learning-based Dynamic Service Placement in Vehicular Networks. (arXiv:2105.15022v2 [cs.NI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.15022</id>
        <link href="http://arxiv.org/abs/2105.15022"/>
        <updated>2021-06-02T02:50:01.988Z</updated>
        <summary type="html"><![CDATA[The emergence of technologies such as 5G and mobile edge computing has
enabled provisioning of different types of services with different resource and
service requirements to the vehicles in a vehicular network.The growing
complexity of traffic mobility patterns and dynamics in the requests for
different types of services has made service placement a challenging task. A
typical static placement solution is not effective as it does not consider the
traffic mobility and service dynamics. In this paper, we propose a
reinforcement learning-based dynamic (RL-Dynamic) service placement framework
to find the optimal placement of services at the edge servers while considering
the vehicle's mobility and dynamics in the requests for different types of
services. We use SUMO and MATLAB to carry out simulation experiments. In our
learning framework, for the decision module, we consider two alternative
objective functions-minimizing delay and minimizing edge server utilization. We
developed an ILP based problem formulation for the two objective functions. The
experimental results show that 1) compared to static service placement,
RL-based dynamic service placement achieves fair utilization of edge server
resources and low service delay, and 2) compared to delay-optimized placement,
server utilization optimized placement utilizes resources more effectively,
achieving higher fairness with lower edge-server utilization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Talpur_A/0/1/0/all/0/1"&gt;Anum Talpur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gurusamy_M/0/1/0/all/0/1"&gt;Mohan Gurusamy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fixes That Fail: Self-Defeating Improvements in Machine-Learning Systems. (arXiv:2103.11766v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.11766</id>
        <link href="http://arxiv.org/abs/2103.11766"/>
        <updated>2021-06-02T02:50:01.987Z</updated>
        <summary type="html"><![CDATA[Machine-learning systems such as self-driving cars or virtual assistants are
composed of a large number of machine-learning models that recognize image
content, transcribe speech, analyze natural language, infer preferences, rank
options, etc. Models in these systems are often developed and trained
independently, which raises an obvious concern: Can improving a
machine-learning model make the overall system worse? We answer this question
affirmatively by showing that improving a model can deteriorate the performance
of downstream models, even after those downstream models are retrained. Such
self-defeating improvements are the result of entanglement between the models
in the system. We perform an error decomposition of systems with multiple
machine-learning models, which sheds light on the types of errors that can lead
to self-defeating improvements. We also present the results of experiments
which show that self-defeating improvements emerge in a realistic stereo-based
detection system for cars and pedestrians.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1"&gt;Ruihan Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1"&gt;Chuan Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hannun_A/0/1/0/all/0/1"&gt;Awni Hannun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maaten_L/0/1/0/all/0/1"&gt;Laurens van der Maaten&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Overfitting for Fun and Profit: Instance-Adaptive Data Compression. (arXiv:2101.08687v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.08687</id>
        <link href="http://arxiv.org/abs/2101.08687"/>
        <updated>2021-06-02T02:50:01.977Z</updated>
        <summary type="html"><![CDATA[Neural data compression has been shown to outperform classical methods in
terms of $RD$ performance, with results still improving rapidly. At a high
level, neural compression is based on an autoencoder that tries to reconstruct
the input instance from a (quantized) latent representation, coupled with a
prior that is used to losslessly compress these latents. Due to limitations on
model capacity and imperfect optimization and generalization, such models will
suboptimally compress test data in general. However, one of the great strengths
of learned compression is that if the test-time data distribution is known and
relatively low-entropy (e.g. a camera watching a static scene, a dash cam in an
autonomous car, etc.), the model can easily be finetuned or adapted to this
distribution, leading to improved $RD$ performance. In this paper we take this
concept to the extreme, adapting the full model to a single video, and sending
model updates (quantized and compressed using a parameter-space prior) along
with the latent representation. Unlike previous work, we finetune not only the
encoder/latents but the entire model, and - during finetuning - take into
account both the effect of model quantization and the additional costs incurred
by sending the model updates. We evaluate an image compression model on
I-frames (sampled at 2 fps) from videos of the Xiph dataset, and demonstrate
that full-model adaptation improves $RD$ performance by ~1 dB, with respect to
encoder-only finetuning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rozendaal_T/0/1/0/all/0/1"&gt;Ties van Rozendaal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huijben_I/0/1/0/all/0/1"&gt;Iris A.M. Huijben&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cohen_T/0/1/0/all/0/1"&gt;Taco S. Cohen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Markpainting: Adversarial Machine Learning meets Inpainting. (arXiv:2106.00660v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.00660</id>
        <link href="http://arxiv.org/abs/2106.00660"/>
        <updated>2021-06-02T02:50:01.976Z</updated>
        <summary type="html"><![CDATA[Inpainting is a learned interpolation technique that is based on generative
modeling and used to populate masked or missing pieces in an image; it has wide
applications in picture editing and retouching. Recently, inpainting started
being used for watermark removal, raising concerns. In this paper we study how
to manipulate it using our markpainting technique. First, we show how an image
owner with access to an inpainting model can augment their image in such a way
that any attempt to edit it using that model will add arbitrary visible
information. We find that we can target multiple different models
simultaneously with our technique. This can be designed to reconstitute a
watermark if the editor had been trying to remove it. Second, we show that our
markpainting technique is transferable to models that have different
architectures or were trained on different datasets, so watermarks created
using it are difficult for adversaries to remove. Markpainting is novel and can
be used as a manipulation alarm that becomes visible in the event of
inpainting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khachaturov_D/0/1/0/all/0/1"&gt;David Khachaturov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shumailov_I/0/1/0/all/0/1"&gt;Ilia Shumailov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yiren Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Papernot_N/0/1/0/all/0/1"&gt;Nicolas Papernot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anderson_R/0/1/0/all/0/1"&gt;Ross Anderson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning with Gradient Descent and Weakly Convex Losses. (arXiv:2101.04968v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.04968</id>
        <link href="http://arxiv.org/abs/2101.04968"/>
        <updated>2021-06-02T02:50:01.969Z</updated>
        <summary type="html"><![CDATA[We study the learning performance of gradient descent when the empirical risk
is weakly convex, namely, the smallest negative eigenvalue of the empirical
risk's Hessian is bounded in magnitude. By showing that this eigenvalue can
control the stability of gradient descent, generalisation error bounds are
proven that hold under a wider range of step sizes compared to previous work.
Out of sample guarantees are then achieved by decomposing the test error into
generalisation, optimisation and approximation errors, each of which can be
bounded and traded off with respect to algorithmic parameters, sample size and
magnitude of this eigenvalue. In the case of a two layer neural network, we
demonstrate that the empirical risk can satisfy a notion of local weak
convexity, specifically, the Hessian's smallest eigenvalue during training can
be controlled by the normalisation of the layers, i.e., network scaling. This
allows test error guarantees to then be achieved when the population risk
minimiser satisfies a complexity assumption. By trading off the network
complexity and scaling, insights are gained into the implicit bias of neural
network scaling, which are further supported by experimental findings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Richards_D/0/1/0/all/0/1"&gt;Dominic Richards&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Rabbat_M/0/1/0/all/0/1"&gt;Mike Rabbat&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Consistent Two-Flow Network for Tele-Registration of Point Clouds. (arXiv:2106.00329v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.00329</id>
        <link href="http://arxiv.org/abs/2106.00329"/>
        <updated>2021-06-02T02:50:01.928Z</updated>
        <summary type="html"><![CDATA[Rigid registration of partial observations is a fundamental problem in
various applied fields. In computer graphics, special attention has been given
to the registration between two partial point clouds generated by scanning
devices. State-of-the-art registration techniques still struggle when the
overlap region between the two point clouds is small, and completely fail if
there is no overlap between the scan pairs. In this paper, we present a
learning-based technique that alleviates this problem, and allows registration
between point clouds, presented in arbitrary poses, and having little or even
no overlap, a setting that has been referred to as tele-registration. Our
technique is based on a novel neural network design that learns a prior of a
class of shapes and can complete a partial shape. The key idea is combining the
registration and completion tasks in a way that reinforces each other. In
particular, we simultaneously train the registration network and completion
network using two coupled flows, one that register-and-complete, and one that
complete-and-register, and encourage the two flows to produce a consistent
result. We show that, compared with each separate flow, this two-flow training
leads to robust and reliable tele-registration, and hence to a better point
cloud prediction that completes the registered scans. It is also worth
mentioning that each of the components in our neural network outperforms
state-of-the-art methods in both completion and registration. We further
analyze our network with several ablation studies and demonstrate its
performance on a large number of partial point clouds, both synthetic and
real-world, that have only small or no overlap.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1"&gt;Zihao Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yi_Z/0/1/0/all/0/1"&gt;Zimu Yi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_R/0/1/0/all/0/1"&gt;Ruizhen Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mitra_N/0/1/0/all/0/1"&gt;Niloy J. Mitra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cohen_Or_D/0/1/0/all/0/1"&gt;Daniel Cohen-Or&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1"&gt;Hui Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Integer-Only Neural Network Quantization Scheme Based on Shift-Batch-Normalization. (arXiv:2106.00127v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.00127</id>
        <link href="http://arxiv.org/abs/2106.00127"/>
        <updated>2021-06-02T02:50:01.924Z</updated>
        <summary type="html"><![CDATA[Neural networks are very popular in many areas, but great computing
complexity makes it hard to run neural networks on devices with limited
resources. To address this problem, quantization methods are used to reduce
model size and computation cost, making it possible to use neural networks on
embedded platforms or mobile devices.

In this paper, an integer-only-quantization scheme is introduced. This scheme
uses one layer that combines shift-based batch normalization and uniform
quantization to implement 4-bit integer-only inference. Without big integer
multiplication(which is used in previous integer-only-quantization methods),
this scheme can achieve good power and latency efficiency, and is especially
suitable to be deployed on co-designed hardware platforms. Tests have proved
that this scheme works very well for easy tasks. And for tough tasks,
performance loss can be tolerated for its inference efficiency. Our work is
available on github: https://github.com/hguq/IntegerNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1"&gt;Qingyu Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yuan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_X/0/1/0/all/0/1"&gt;Xiaoxin Cui&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Control Occupation Kernel Regression for Nonlinear Control-Affine Systems. (arXiv:2106.00103v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2106.00103</id>
        <link href="http://arxiv.org/abs/2106.00103"/>
        <updated>2021-06-02T02:50:01.908Z</updated>
        <summary type="html"><![CDATA[This manuscript presents an algorithm for obtaining an approximation of
nonlinear high order control affine dynamical systems, that leverages the
controlled trajectories as the central unit of information. As the fundamental
basis elements leveraged in approximation, higher order control occupation
kernels represent iterated integration after multiplication by a given
controller in a vector valued reproducing kernel Hilbert space. In a
regularized regression setting, the unique optimizer for a particular
optimization problem is expressed as a linear combination of these occupation
kernels, which converts an infinite dimensional optimization problem to a
finite dimensional optimization problem through the representer theorem.
Interestingly, the vector valued structure of the Hilbert space allows for
simultaneous approximation of the drift and control effectiveness components of
the control affine system. Several experiments are performed to demonstrate the
effectiveness of the approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Abudia_M/0/1/0/all/0/1"&gt;Moad Abudia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Channagiri_T/0/1/0/all/0/1"&gt;Tejasvi Channagiri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Rosenfeld_J/0/1/0/all/0/1"&gt;Joel A. Rosenfeld&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Kamalapurkar_R/0/1/0/all/0/1"&gt;Rushikesh Kamalapurkar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SHINE: SHaring the INverse Estimate from the forward pass for bi-level optimization and implicit models. (arXiv:2106.00553v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.00553</id>
        <link href="http://arxiv.org/abs/2106.00553"/>
        <updated>2021-06-02T02:50:01.904Z</updated>
        <summary type="html"><![CDATA[In recent years, implicit deep learning has emerged as a method to increase
the depth of deep neural networks. While their training is memory-efficient,
they are still significantly slower to train than their explicit counterparts.
In Deep Equilibrium Models (DEQs), the training is performed as a bi-level
problem, and its computational complexity is partially driven by the iterative
inversion of a huge Jacobian matrix. In this paper, we propose a novel strategy
to tackle this computational bottleneck from which many bi-level problems
suffer. The main idea is to use the quasi-Newton matrices from the forward pass
to efficiently approximate the inverse Jacobian matrix in the direction needed
for the gradient computation. We provide a theorem that motivates using our
method with the original forward algorithms. In addition, by modifying these
forward algorithms, we further provide theoretical guarantees that our method
asymptotically estimates the true implicit gradient. We empirically study this
approach in many settings, ranging from hyperparameter optimization to large
Multiscale DEQs applied to CIFAR and ImageNet. We show that it reduces the
computational cost of the backward pass by up to two orders of magnitude. All
this is achieved while retaining the excellent performance of the original
models in hyperparameter optimization and on CIFAR, and giving encouraging and
competitive results on ImageNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ramzi_Z/0/1/0/all/0/1"&gt;Zaccharie Ramzi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mannel_F/0/1/0/all/0/1"&gt;Florian Mannel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bai_S/0/1/0/all/0/1"&gt;Shaojie Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Starck_J/0/1/0/all/0/1"&gt;Jean-Luc Starck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ciuciu_P/0/1/0/all/0/1"&gt;Philippe Ciuciu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moreau_T/0/1/0/all/0/1"&gt;Thomas Moreau&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Information-Theoretic Analysis of Epistemic Uncertainty in Bayesian Meta-learning. (arXiv:2106.00252v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.00252</id>
        <link href="http://arxiv.org/abs/2106.00252"/>
        <updated>2021-06-02T02:50:01.902Z</updated>
        <summary type="html"><![CDATA[The overall predictive uncertainty of a trained predictor can be decomposed
into separate contributions due to epistemic and aleatoric uncertainty. Under a
Bayesian formulation, assuming a well-specified model, the two contributions
can be exactly expressed (for the log-loss) or bounded (for more general
losses) in terms of information-theoretic quantities (Xu and Raginsky, 2020).
This paper addresses the study of epistemic uncertainty within an
information-theoretic framework in the broader setting of Bayesian
meta-learning. A general hierarchical Bayesian model is assumed in which
hyperparameters determine the per-task priors of the model parameters. Exact
characterizations (for the log-loss) and bounds (for more general losses) are
derived for the epistemic uncertainty - quantified by the minimum excess
meta-risk (MEMR)- of optimal meta-learning rules. This characterization is
leveraged to bring insights into the dependence of the epistemic uncertainty on
the number of tasks and on the amount of per-task training data. Experiments
are presented that compare the proposed information-theoretic bounds, evaluated
via neural mutual information estimators, with the performance of a novel
approximate fully Bayesian meta-learning strategy termed Langevin-Stein
Bayesian Meta-Learning (LS-BML).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jose_S/0/1/0/all/0/1"&gt;Sharu Theresa Jose&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1"&gt;Sangwoo Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Simeone_O/0/1/0/all/0/1"&gt;Osvaldo Simeone&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Catastrophic Fisher Explosion: Early Phase Fisher Matrix Impacts Generalization. (arXiv:2012.14193v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.14193</id>
        <link href="http://arxiv.org/abs/2012.14193"/>
        <updated>2021-06-02T02:50:01.898Z</updated>
        <summary type="html"><![CDATA[The early phase of training of deep neural networks has a dramatic effect on
the local curvature of the loss function. For instance, using a small learning
rate does not guarantee stable optimization because the optimization trajectory
has a tendency to steer towards regions of the loss surface with increasing
local curvature. We ask whether this tendency is connected to the widely
observed phenomenon that the choice of the learning rate strongly influences
generalization. We first show that stochastic gradient descent (SGD) implicitly
penalizes the trace of the Fisher Information Matrix (FIM), a measure of the
local curvature, from the beginning of training. We argue it is an implicit
regularizer in SGD by showing that explicitly penalizing the trace of the FIM
can significantly improve generalization. We highlight that poor final
generalization coincides with the trace of the FIM increasing to a large value
early in training, to which we refer as catastrophic Fisher explosion. Finally,
to gain insight into the regularization effect of penalizing the trace of the
FIM, we show that it limits memorization by reducing the learning speed of
examples with noisy labels more than that of the clean examples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jastrzebski_S/0/1/0/all/0/1"&gt;Stanislaw Jastrzebski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arpit_D/0/1/0/all/0/1"&gt;Devansh Arpit&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Astrand_O/0/1/0/all/0/1"&gt;Oliver Astrand&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kerg_G/0/1/0/all/0/1"&gt;Giancarlo Kerg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Huan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1"&gt;Caiming Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Socher_R/0/1/0/all/0/1"&gt;Richard Socher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1"&gt;Kyunghyun Cho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geras_K/0/1/0/all/0/1"&gt;Krzysztof Geras&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Explanations With Relevant Sets. (arXiv:2106.00546v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.00546</id>
        <link href="http://arxiv.org/abs/2106.00546"/>
        <updated>2021-06-02T02:50:01.887Z</updated>
        <summary type="html"><![CDATA[Recent work proposed $\delta$-relevant inputs (or sets) as a probabilistic
explanation for the predictions made by a classifier on a given input.
$\delta$-relevant sets are significant because they serve to relate
(model-agnostic) Anchors with (model-accurate) PI- explanations, among other
explanation approaches. Unfortunately, the computation of smallest size
$\delta$-relevant sets is complete for ${NP}^{PP}$, rendering their computation
largely infeasible in practice. This paper investigates solutions for tackling
the practical limitations of $\delta$-relevant sets. First, the paper
alternatively considers the computation of subset-minimal sets. Second, the
paper studies concrete families of classifiers, including decision trees among
others. For these cases, the paper shows that the computation of subset-minimal
$\delta$-relevant sets is in NP, and can be solved with a polynomial number of
calls to an NP oracle. The experimental evaluation compares the proposed
approach with heuristic explainers for the concrete case of the classifiers
studied in the paper, and confirms the advantage of the proposed solution over
the state of the art.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Izza_Y/0/1/0/all/0/1"&gt;Yacine Izza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ignatiev_A/0/1/0/all/0/1"&gt;Alexey Ignatiev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Narodytska_N/0/1/0/all/0/1"&gt;Nina Narodytska&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cooper_M/0/1/0/all/0/1"&gt;Martin C. Cooper&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marques_Silva_J/0/1/0/all/0/1"&gt;Joao Marques-Silva&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Removing Word-Level Spurious Alignment between Images and Pseudo-Captions in Unsupervised Image Captioning. (arXiv:2104.13872v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.13872</id>
        <link href="http://arxiv.org/abs/2104.13872"/>
        <updated>2021-06-02T02:50:01.880Z</updated>
        <summary type="html"><![CDATA[Unsupervised image captioning is a challenging task that aims at generating
captions without the supervision of image-sentence pairs, but only with images
and sentences drawn from different sources and object labels detected from the
images. In previous work, pseudo-captions, i.e., sentences that contain the
detected object labels, were assigned to a given image. The focus of the
previous work was on the alignment of input images and pseudo-captions at the
sentence level. However, pseudo-captions contain many words that are irrelevant
to a given image. In this work, we investigate the effect of removing
mismatched words from image-sentence alignment to determine how they make this
task difficult. We propose a simple gating mechanism that is trained to align
image features with only the most reliable words in pseudo-captions: the
detected object labels. The experimental results show that our proposed method
outperforms the previous methods without introducing complex sentence-level
learning objectives. Combined with the sentence-level alignment method of
previous work, our method further improves its performance. These results
confirm the importance of careful alignment in word-level details.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Honda_U/0/1/0/all/0/1"&gt;Ukyo Honda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ushiku_Y/0/1/0/all/0/1"&gt;Yoshitaka Ushiku&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hashimoto_A/0/1/0/all/0/1"&gt;Atsushi Hashimoto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Watanabe_T/0/1/0/all/0/1"&gt;Taro Watanabe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Matsumoto_Y/0/1/0/all/0/1"&gt;Yuji Matsumoto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised detection of mouse behavioural anomalies using two-stream convolutional autoencoders. (arXiv:2106.00598v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.00598</id>
        <link href="http://arxiv.org/abs/2106.00598"/>
        <updated>2021-06-02T02:50:01.879Z</updated>
        <summary type="html"><![CDATA[This paper explores the application of unsupervised learning to detecting
anomalies in mouse video data. The two models presented in this paper are a
dual-stream, 3D convolutional autoencoder (with residual connections) and a
dual-stream, 2D convolutional autoencoder. The publicly available dataset used
here contains twelve videos of single home-caged mice alongside frame-level
annotations. Under the pretext that the autoencoder only sees normal events,
the video data was handcrafted to treat each behaviour as a pseudo-anomaly
thereby eliminating them from the others during training. The results are
presented for one conspicuous behaviour (hang) and one inconspicuous behaviour
(groom). The performance of these models is compared to a single stream
autoencoder and a supervised learning model, which are both based on the custom
CAE. Both models are also tested on the CUHK Avenue dataset were found to
perform as well as some state-of-the-art architectures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nwokedi_E/0/1/0/all/0/1"&gt;Ezechukwu I Nwokedi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bains_R/0/1/0/all/0/1"&gt;Rasneer S Bains&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bidaut_L/0/1/0/all/0/1"&gt;Luc Bidaut&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wells_S/0/1/0/all/0/1"&gt;Sara Wells&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1"&gt;Xujiong Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brown_J/0/1/0/all/0/1"&gt;James M Brown&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Gradient Play in Multi-Agent Markov Stochastic Games: Stationary Points and Convergence. (arXiv:2106.00198v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.00198</id>
        <link href="http://arxiv.org/abs/2106.00198"/>
        <updated>2021-06-02T02:50:01.878Z</updated>
        <summary type="html"><![CDATA[We study the performance of the gradient play algorithm for multi-agent
tabular Markov decision processes (MDPs), which are also known as stochastic
games (SGs), where each agent tries to maximize its own total discounted reward
by making decisions independently based on current state information which is
shared between agents. Policies are directly parameterized by the probability
of choosing a certain action at a given state. We show that Nash equilibria
(NEs) and first order stationary policies are equivalent in this setting, and
give a non-asymptotic global convergence rate analysis to an $\epsilon$-NE for
a subclass of multi-agent MDPs called Markov potential games, which includes
the cooperative setting with identical rewards among agents as an important
special case. Our result shows that the number of iterations to reach an
$\epsilon$-NE scales linearly, instead of exponentially, with the number of
agents. Local geometry and local stability are also considered. For Markov
potential games, we prove that strict NEs are local maxima of the total
potential function and fully-mixed NEs are saddle points. We also give a local
convergence rate around strict NEs for more general settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Runyu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_Z/0/1/0/all/0/1"&gt;Zhaolin Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_N/0/1/0/all/0/1"&gt;Na Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Extended Tactile Perception: Vibration Sensing through Tools and Grasped Objects. (arXiv:2106.00489v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2106.00489</id>
        <link href="http://arxiv.org/abs/2106.00489"/>
        <updated>2021-06-02T02:50:01.877Z</updated>
        <summary type="html"><![CDATA[Humans display the remarkable ability to sense the world through tools and
other held objects. For example, we are able to pinpoint impact locations on a
held rod and tell apart different textures using a rigid probe. In this work,
we consider how we can enable robots to have a similar capacity, i.e., to
embody tools and extend perception using standard grasped objects. We propose
that vibro-tactile sensing using dynamic tactile sensors on the robot fingers,
along with machine learning models, enables robots to decipher contact
information that is transmitted as vibrations along rigid objects. This paper
reports on extensive experiments using the BioTac micro-vibration sensor and a
new event dynamic sensor, the NUSkin, capable of multi-taxel sensing at 4~kHz.
We demonstrate that fine localization on a held rod is possible using our
approach (with errors less than 1 cm on a 20 cm rod). Next, we show that
vibro-tactile perception can lead to reasonable grasp stability prediction
during object handover, and accurate food identification using a standard fork.
We find that multi-taxel vibro-tactile sensing at sufficiently high sampling
rate (above 2 kHz) led to the best performance across the various tasks and
objects. Taken together, our results provides both evidence and guidelines for
using vibro-tactile perception to extend tactile perception, which we believe
will lead to enhanced competency with tools and better physical
human-robot-interaction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Taunyazov_T/0/1/0/all/0/1"&gt;Tasbolat Taunyazov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1"&gt;Luar Shui Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lim_E/0/1/0/all/0/1"&gt;Eugene Lim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+See_H/0/1/0/all/0/1"&gt;Hian Hian See&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1"&gt;David Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tee_B/0/1/0/all/0/1"&gt;Benjamin C.K. Tee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soh_H/0/1/0/all/0/1"&gt;Harold Soh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Post-Contextual-Bandit Inference. (arXiv:2106.00418v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2106.00418</id>
        <link href="http://arxiv.org/abs/2106.00418"/>
        <updated>2021-06-02T02:50:01.874Z</updated>
        <summary type="html"><![CDATA[Contextual bandit algorithms are increasingly replacing non-adaptive A/B
tests in e-commerce, healthcare, and policymaking because they can both improve
outcomes for study participants and increase the chance of identifying good or
even best policies. To support credible inference on novel interventions at the
end of the study, nonetheless, we still want to construct valid confidence
intervals on average treatment effects, subgroup effects, or value of new
policies. The adaptive nature of the data collected by contextual bandit
algorithms, however, makes this difficult: standard estimators are no longer
asymptotically normally distributed and classic confidence intervals fail to
provide correct coverage. While this has been addressed in non-contextual
settings by using stabilized estimators, the contextual setting poses unique
challenges that we tackle for the first time in this paper. We propose the
Contextual Adaptive Doubly Robust (CADR) estimator, the first estimator for
policy value that is asymptotically normal under contextual adaptive data
collection. The main technical challenge in constructing CADR is designing
adaptive and consistent conditional standard deviation estimators for
stabilization. Extensive numerical experiments using 57 OpenML datasets
demonstrate that confidence intervals based on CADR uniquely provide correct
coverage.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Bibaut_A/0/1/0/all/0/1"&gt;Aur&amp;#xe9;lien Bibaut&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Chambaz_A/0/1/0/all/0/1"&gt;Antoine Chambaz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Dimakopoulou_M/0/1/0/all/0/1"&gt;Maria Dimakopoulou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kallus_N/0/1/0/all/0/1"&gt;Nathan Kallus&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Laan_M/0/1/0/all/0/1"&gt;Mark van der Laan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[IID-GAN: an IID Sampling Perspective for Regularizing Mode Collapse. (arXiv:2106.00563v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.00563</id>
        <link href="http://arxiv.org/abs/2106.00563"/>
        <updated>2021-06-02T02:50:01.874Z</updated>
        <summary type="html"><![CDATA[Despite its success, generative adversarial networks (GANs) still suffer from
mode collapse, namely the generator can only map latent variables to a partial
set of modes of the target distribution. In this paper, we analyze and try to
regularize this issue with an independent and identically distributed (IID)
sampling perspective and emphasize that holding the IID property for generation
in target space (i.e. real data) can naturally avoid mode collapse. This is
based on the basic IID assumption for real data in machine learning. However,
though the source samples $\mathbf{z}$ obey IID, the target generation
$G(\mathbf{z})$ may not necessarily be IID. Based on this observation, we
provide a new loss to encourage the closeness between the inverse source from
generation, and a standard Gaussian distribution in the latent space, as a way
of regularizing the generation to be IID. The logic is that the inverse samples
back from target data should also be IID for source distribution. Experiments
on both synthetic and real-world data show the superiority and robustness of
our model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shi_L/0/1/0/all/0/1"&gt;Liangliang Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1"&gt;Junchi Yan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Knowledge Transfer for Few-shot Segmentation of Novel White Matter Tracts. (arXiv:2105.14513v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14513</id>
        <link href="http://arxiv.org/abs/2105.14513"/>
        <updated>2021-06-02T02:50:01.873Z</updated>
        <summary type="html"><![CDATA[Convolutional neural networks (CNNs) have achieved stateof-the-art
performance for white matter (WM) tract segmentation based on diffusion
magnetic resonance imaging (dMRI). These CNNs require a large number of manual
delineations of the WM tracts of interest for training, which are generally
labor-intensive and costly. The expensive manual delineation can be a
particular disadvantage when novel WM tracts, i.e., tracts that have not been
included in existing manual delineations, are to be analyzed. To accurately
segment novel WM tracts, it is desirable to transfer the knowledge learned
about existing WM tracts, so that even with only a few delineations of the
novel WM tracts, CNNs can learn adequately for the segmentation. In this paper,
we explore the transfer of such knowledge to the segmentation of novel WM
tracts in the few-shot setting. Although a classic fine-tuning strategy can be
used for the purpose, the information in the last task-specific layer for
segmenting existing WM tracts is completely discarded. We hypothesize that the
weights of this last layer can bear valuable information for segmenting the
novel WM tracts and thus completely discarding the information is not optimal.
In particular, we assume that the novel WM tracts can correlate with existing
WM tracts and the segmentation of novel WM tracts can be predicted with the
logits of existing WM tracts. In this way, better initialization of the last
layer than random initialization can be achieved for fine-tuning. Further, we
show that a more adaptive use of the knowledge in the last layer for segmenting
existing WM tracts can be conveniently achieved by simply inserting a warmup
stage before classic fine-tuning. The proposed method was evaluated on a
publicly available dMRI dataset, where we demonstrate the benefit of our method
for few-shot segmentation of novel WM tracts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lu_Q/0/1/0/all/0/1"&gt;Qi Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_C/0/1/0/all/0/1"&gt;Chuyang Ye&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reward is enough for convex MDPs. (arXiv:2106.00661v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2106.00661</id>
        <link href="http://arxiv.org/abs/2106.00661"/>
        <updated>2021-06-02T02:50:01.872Z</updated>
        <summary type="html"><![CDATA[Maximising a cumulative reward function that is Markov and stationary, i.e.,
defined over state-action pairs and independent of time, is sufficient to
capture many kinds of goals in a Markov Decision Process (MDP) based on the
Reinforcement Learning (RL) problem formulation. However, not all goals can be
captured in this manner. Specifically, it is easy to see that Convex MDPs in
which goals are expressed as convex functions of stationary distributions
cannot, in general, be formulated in this manner. In this paper, we reformulate
the convex MDP problem as a min-max game between the policy and cost (negative
reward) players using Fenchel duality and propose a meta-algorithm for solving
it. We show that the average of the policies produced by an RL agent that
maximizes the non-stationary reward produced by the cost player converges to an
optimal solution to the convex MDP. Finally, we show that the meta-algorithm
unifies several disparate branches of reinforcement learning algorithms in the
literature, such as apprenticeship learning, variational intrinsic control,
constrained MDPs, and pure exploration into a single framework.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zahavy_T/0/1/0/all/0/1"&gt;Tom Zahavy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+ODonoghue_B/0/1/0/all/0/1"&gt;Brendan O&amp;#x27;Donoghue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Desjardins_G/0/1/0/all/0/1"&gt;Guillaume Desjardins&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1"&gt;Satinder Singh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reinforce Security: A Model-Free Approach Towards Secure Wiretap Coding. (arXiv:2106.00343v1 [cs.IT])]]></title>
        <id>http://arxiv.org/abs/2106.00343</id>
        <link href="http://arxiv.org/abs/2106.00343"/>
        <updated>2021-06-02T02:50:01.870Z</updated>
        <summary type="html"><![CDATA[The use of deep learning-based techniques for approximating secure encoding
functions has attracted considerable interest in wireless communications due to
impressive results obtained for general coding and decoding tasks for wireless
communication systems. Of particular importance is the development of
model-free techniques that work without knowledge about the underlying channel.
Such techniques utilize for example generative adversarial networks to estimate
and model the conditional channel distribution, mutual information estimation
as a reward function, or reinforcement learning. In this paper, the approach of
reinforcement learning is studied and, in particular, the policy gradient
method for a model-free approach of neural network-based secure encoding is
investigated. Previously developed techniques for enforcing a certain co-set
structure on the encoding process can be combined with recent reinforcement
learning approaches. This new approach is evaluated by extensive simulations,
and it is demonstrated that the resulting decoding performance of an
eavesdropper is capped at a certain error level.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fritschek_R/0/1/0/all/0/1"&gt;Rick Fritschek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schaefer_R/0/1/0/all/0/1"&gt;Rafael F. Schaefer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wunder_G/0/1/0/all/0/1"&gt;Gerhard Wunder&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Homomorphic Sensing of Subspace Arrangements. (arXiv:2006.05158v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.05158</id>
        <link href="http://arxiv.org/abs/2006.05158"/>
        <updated>2021-06-02T02:50:01.869Z</updated>
        <summary type="html"><![CDATA[Homomorphic sensing is a recent algebraic-geometric framework that studies
the unique recovery of points in a linear subspace from their images under a
given collection of linear maps. It has been successful in interpreting such a
recovery in the case of permutations composed by coordinate projections, an
important instance in applications known as unlabeled sensing, which models
data that are out of order and have missing values. In this paper, we provide
tighter and simpler conditions that guarantee the unique recovery for the
single-subspace case, extend the result to the case of a subspace arrangement,
and show that the unique recovery in a single subspace is locally stable under
noise. We specialize our results to several examples of homomorphic sensing
such as real phase retrieval and unlabeled sensing. In so doing, in a unified
way, we obtain conditions that guarantee the unique recovery for those
examples, typically known via diverse techniques in the literature, as well as
novel conditions for sparse and unsigned versions of unlabeled sensing.
Similarly, our noise result also implies that the unique recovery in unlabeled
sensing is locally stable.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Peng_L/0/1/0/all/0/1"&gt;Liangzu Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsakiris_M/0/1/0/all/0/1"&gt;Manolis C. Tsakiris&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semi-Supervised Domain Generalization with Stochastic StyleMatch. (arXiv:2106.00592v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.00592</id>
        <link href="http://arxiv.org/abs/2106.00592"/>
        <updated>2021-06-02T02:50:01.857Z</updated>
        <summary type="html"><![CDATA[Most existing research on domain generalization assumes source data gathered
from multiple domains are fully annotated. However, in real-world applications,
we might have only a few labels available from each source domain due to high
annotation cost, along with abundant unlabeled data that are much easier to
obtain. In this work, we investigate semi-supervised domain generalization
(SSDG), a more realistic and practical setting. Our proposed approach,
StyleMatch, is inspired by FixMatch, a state-of-the-art semi-supervised
learning method based on pseudo-labeling, with several new ingredients tailored
to solve SSDG. Specifically, 1) to mitigate overfitting in the scarce labeled
source data while improving robustness against noisy pseudo labels, we
introduce stochastic modeling to the classifier's weights, seen as class
prototypes, with Gaussian distributions. 2) To enhance generalization under
domain shift, we upgrade FixMatch's two-view consistency learning paradigm
based on weak and strong augmentations to a multi-view version with style
augmentation as the third complementary view. To provide a comprehensive study
and evaluation, we establish two SSDG benchmarks, which cover a wide range of
strong baseline methods developed in relevant areas including domain
generalization and semi-supervised learning. Extensive experiments demonstrate
that StyleMatch achieves the best out-of-distribution generalization
performance in the low-data regime. We hope our approach and benchmarks can
pave the way for future research on data-efficient and generalizable learning
systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1"&gt;Kaiyang Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Loy_C/0/1/0/all/0/1"&gt;Chen Change Loy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Ziwei Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Learning for Reliable Classification of COVID-19, MERS, and SARS from Chest X-Ray Images. (arXiv:2005.11524v6 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.11524</id>
        <link href="http://arxiv.org/abs/2005.11524"/>
        <updated>2021-06-02T02:50:01.833Z</updated>
        <summary type="html"><![CDATA[Novel Coronavirus disease (COVID-19) is an extremely contagious and quickly
spreading Coronavirus infestation. Severe Acute Respiratory Syndrome (SARS) and
Middle East Respiratory Syndrome (MERS), which outbreak in 2002 and 2011, and
the current COVID-19 pandemic are all from the same family of coronavirus. This
work aims to classify COVID-19, SARS, and MERS chest X-ray (CXR) images using
deep Convolutional Neural Networks (CNNs). A unique database was created,
so-called QU-COVID-family, consisting of 423 COVID-19, 144 MERS, and 134 SARS
CXR images. Besides, a robust COVID-19 recognition system was proposed to
identify lung regions using a CNN segmentation model (U-Net), and then classify
the segmented lung images as COVID-19, MERS, or SARS using a pre-trained CNN
classifier. Furthermore, the Score-CAM visualization method was utilized to
visualize classification output and understand the reasoning behind the
decision of deep CNNs. Several Deep Learning classifiers were trained and
tested; four outperforming algorithms were reported. Original and preprocessed
images were used individually and all together as the input(s) to the networks.
Two recognition schemes were considered: plain CXR classification and segmented
CXR classification. For plain CXRs, it was observed that InceptionV3
outperforms other networks with a 3-channel scheme and achieves sensitivities
of 99.5%, 93.1%, and 97% for classifying COVID-19, MERS, and SARS images,
respectively. In contrast, for segmented CXRs, InceptionV3 outperformed using
the original CXR dataset and achieved sensitivities of 96.94%, 79.68%, and
90.26% for classifying COVID-19, MERS, and SARS images, respectively. All
networks showed high COVID-19 detection sensitivity (>96%) with the segmented
lung images. This indicates the unique radiographic signature of COVID-19 cases
in the eyes of AI, which is often a challenging task for medical doctors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Tahir_A/0/1/0/all/0/1"&gt;Anas Tahir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Qiblawey_Y/0/1/0/all/0/1"&gt;Yazan Qiblawey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Khandakar_A/0/1/0/all/0/1"&gt;Amith Khandakar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rahman_T/0/1/0/all/0/1"&gt;Tawsifur Rahman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Khurshid_U/0/1/0/all/0/1"&gt;Uzair Khurshid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Musharavati_F/0/1/0/all/0/1"&gt;Farayi Musharavati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Islam_M/0/1/0/all/0/1"&gt;M. T. Islam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kiranyaz_S/0/1/0/all/0/1"&gt;Serkan Kiranyaz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chowdhury_M/0/1/0/all/0/1"&gt;Muhammad E. H. Chowdhury&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exposing Previously Undetectable Faults in Deep Neural Networks. (arXiv:2106.00576v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.00576</id>
        <link href="http://arxiv.org/abs/2106.00576"/>
        <updated>2021-06-02T02:50:01.810Z</updated>
        <summary type="html"><![CDATA[Existing methods for testing DNNs solve the oracle problem by constraining
the raw features (e.g. image pixel values) to be within a small distance of a
dataset example for which the desired DNN output is known. But this limits the
kinds of faults these approaches are able to detect. In this paper, we
introduce a novel DNN testing method that is able to find faults in DNNs that
other methods cannot. The crux is that, by leveraging generative machine
learning, we can generate fresh test inputs that vary in their high-level
features (for images, these include object shape, location, texture, and
colour). We demonstrate that our approach is capable of detecting deliberately
injected faults as well as new faults in state-of-the-art DNNs, and that in
both cases, existing methods are unable to find these faults.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dunn_I/0/1/0/all/0/1"&gt;Isaac Dunn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pouget_H/0/1/0/all/0/1"&gt;Hadrien Pouget&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kroening_D/0/1/0/all/0/1"&gt;Daniel Kroening&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Melham_T/0/1/0/all/0/1"&gt;Tom Melham&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DLA-Net: Learning Dual Local Attention Features for Semantic Segmentation of Large-Scale Building Facade Point Clouds. (arXiv:2106.00376v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.00376</id>
        <link href="http://arxiv.org/abs/2106.00376"/>
        <updated>2021-06-02T02:50:01.806Z</updated>
        <summary type="html"><![CDATA[Semantic segmentation of building facade is significant in various
applications, such as urban building reconstruction and damage assessment. As
there is a lack of 3D point clouds datasets related to the fine-grained
building facade, we construct the first large-scale building facade point
clouds benchmark dataset for semantic segmentation. The existing methods of
semantic segmentation cannot fully mine the local neighborhood information of
point clouds. Addressing this problem, we propose a learnable attention module
that learns Dual Local Attention features, called DLA in this paper. The
proposed DLA module consists of two blocks, including the self-attention block
and attentive pooling block, which both embed an enhanced position encoding
block. The DLA module could be easily embedded into various network
architectures for point cloud segmentation, naturally resulting in a new 3D
semantic segmentation network with an encoder-decoder architecture, called
DLA-Net in this work. Extensive experimental results on our constructed
building facade dataset demonstrate that the proposed DLA-Net achieves better
performance than the state-of-the-art methods for semantic segmentation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1"&gt;Yanfei Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Weiquan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1"&gt;Zhimin Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1"&gt;Ming Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhihong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1"&gt;Xuelun Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Cheng Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Probabilistic Deep Learning with Probabilistic Neural Networks and Deep Probabilistic Models. (arXiv:2106.00120v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.00120</id>
        <link href="http://arxiv.org/abs/2106.00120"/>
        <updated>2021-06-02T02:50:01.779Z</updated>
        <summary type="html"><![CDATA[Probabilistic deep learning is deep learning that accounts for uncertainty,
both model uncertainty and data uncertainty. It is based on the use of
probabilistic models and deep neural networks. We distinguish two approaches to
probabilistic deep learning: probabilistic neural networks and deep
probabilistic models. The former employs deep neural networks that utilize
probabilistic layers which can represent and process uncertainty; the latter
uses probabilistic models that incorporate deep neural network components which
capture complex non-linear stochastic relationships between the random
variables. We discuss some major examples of each approach including Bayesian
neural networks and mixed density networks (for probabilistic neural networks),
and variational autoencoders, deep Gaussian processes and deep mixed effects
models (for deep probabilistic models). TensorFlow Probability is a library for
probabilistic modeling and inference which can be used for both approaches of
probabilistic deep learning. We include its code examples for illustration.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chang_D/0/1/0/all/0/1"&gt;Daniel T. Chang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Decision Concept Lattice vs. Decision Trees and Random Forests. (arXiv:2106.00387v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.00387</id>
        <link href="http://arxiv.org/abs/2106.00387"/>
        <updated>2021-06-02T02:50:01.774Z</updated>
        <summary type="html"><![CDATA[Decision trees and their ensembles are very popular models of supervised
machine learning. In this paper we merge the ideas underlying decision trees,
their ensembles and FCA by proposing a new supervised machine learning model
which can be constructed in polynomial time and is applicable for both
classification and regression problems. Specifically, we first propose a
polynomial-time algorithm for constructing a part of the concept lattice that
is based on a decision tree. Second, we describe a prediction scheme based on a
concept lattice for solving both classification and regression tasks with
prediction quality comparable to that of state-of-the-art models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dudyrev_E/0/1/0/all/0/1"&gt;Egor Dudyrev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kuznetsov_S/0/1/0/all/0/1"&gt;Sergei O. Kuznetsov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Text Summarization with Latent Queries. (arXiv:2106.00104v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.00104</id>
        <link href="http://arxiv.org/abs/2106.00104"/>
        <updated>2021-06-02T02:50:01.773Z</updated>
        <summary type="html"><![CDATA[The availability of large-scale datasets has driven the development of neural
models that create summaries from single documents, for generic purposes. When
using a summarization system, users often have specific intents with various
language realizations, which, depending on the information need, can range from
a single keyword to a long narrative composed of multiple questions. Existing
summarization systems, however, often either fail to support or act robustly on
this query focused summarization task. We introduce LaQSum, the first unified
text summarization system that learns Latent Queries from documents for
abstractive summarization with any existing query forms. Under a deep
generative framework, our system jointly optimizes a latent query model and a
conditional language model, allowing users to plug-and-play queries of any type
at test time. Despite learning from only generic summarization data and
requiring no further optimization for downstream summarization tasks, our
system robustly outperforms strong comparison systems across summarization
benchmarks with different query types, document settings, and target domains.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yumo Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lapata_M/0/1/0/all/0/1"&gt;Mirella Lapata&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GRAVITAS: Graphical Reticulated Attack Vectors for Internet-of-Things Aggregate Security. (arXiv:2106.00073v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2106.00073</id>
        <link href="http://arxiv.org/abs/2106.00073"/>
        <updated>2021-06-02T02:50:01.767Z</updated>
        <summary type="html"><![CDATA[Internet-of-Things (IoT) and cyber-physical systems (CPSs) may consist of
thousands of devices connected in a complex network topology. The diversity and
complexity of these components present an enormous attack surface, allowing an
adversary to exploit security vulnerabilities of different devices to execute a
potent attack. Though significant efforts have been made to improve the
security of individual devices in these systems, little attention has been paid
to security at the aggregate level. In this article, we describe a
comprehensive risk management system, called GRAVITAS, for IoT/CPS that can
identify undiscovered attack vectors and optimize the placement of defenses
within the system for optimal performance and cost. While existing risk
management systems consider only known attacks, our model employs a machine
learning approach to extrapolate undiscovered exploits, enabling us to identify
attacks overlooked by manual penetration testing (pen-testing). The model is
flexible enough to analyze practically any IoT/CPS and provide the system
administrator with a concrete list of suggested defenses that can reduce system
vulnerability at optimal cost. GRAVITAS can be employed by governments,
companies, and system administrators to design secure IoT/CPS at scale,
providing a quantitative measure of security and efficiency in a world where
IoT/CPS devices will soon be ubiquitous.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Brown_J/0/1/0/all/0/1"&gt;Jacob Brown&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saha_T/0/1/0/all/0/1"&gt;Tanujay Saha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jha_N/0/1/0/all/0/1"&gt;Niraj K. Jha&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Analysis and classification of main risk factors causing stroke in Shanxi Province. (arXiv:2106.00002v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.00002</id>
        <link href="http://arxiv.org/abs/2106.00002"/>
        <updated>2021-06-02T02:50:01.763Z</updated>
        <summary type="html"><![CDATA[In China, stroke is the first leading cause of death in recent years. It is a
major cause of long-term physical and cognitive impairment, which bring great
pressure on the National Public Health System. Evaluation of the risk of
getting stroke is important for the prevention and treatment of stroke in
China. A data set with 2000 hospitalized stroke patients in 2018 and 27583
residents during the year 2017 to 2020 is analyzed in this study. Due to data
incompleteness, inconsistency, and non-structured formats, missing values in
the raw data are filled with -1 as an abnormal class. With the cleaned
features, three models on risk levels of getting stroke are built by using
machine learning methods. The importance of "8+2" factors from China National
Stroke Prevention Project (CSPP) is evaluated via decision tree and random
forest models. Except for "8+2" factors the importance of features and SHAP1
values for lifestyle information, demographic information, and medical
measurement are evaluated and ranked via a random forest model. Furthermore, a
logistic regression model is applied to evaluate the probability of getting
stroke for different risk levels. Based on the census data in both communities
and hospitals from Shanxi Province, we investigate different risk factors of
getting stroke and their ranking with interpretable machine learning models.
The results show that Hypertension (Systolic blood pressure, Diastolic blood
pressure), Physical Inactivity (Lack of sports), and Overweight (BMI) are
ranked as the top three high-risk factors of getting stroke in Shanxi province.
The probability of getting stroke for a person can also be predicted via our
machine learning model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Junjie Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1"&gt;Yiyang Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1"&gt;Jing Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tu_J/0/1/0/all/0/1"&gt;Jiachen Tu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1"&gt;Yuhui Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_P/0/1/0/all/0/1"&gt;Ping He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1"&gt;Huaxiong Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1"&gt;Xiaoshuang Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1"&gt;Shixin Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Urban Traffic Surveillance (UTS): A fully probabilistic 3D tracking approach based on 2D detections. (arXiv:2105.14993v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14993</id>
        <link href="http://arxiv.org/abs/2105.14993"/>
        <updated>2021-06-02T02:50:01.736Z</updated>
        <summary type="html"><![CDATA[Urban Traffic Surveillance (UTS) is a surveillance system based on a
monocular and calibrated video camera that detects vehicles in an urban traffic
scenario with dense traffic on multiple lanes and vehicles performing sharp
turning maneuvers. UTS then tracks the vehicles using a 3D bounding box
representation and a physically reasonable 3D motion model relying on an
unscented Kalman filter based approach. Since UTS recovers positions, shape and
motion information in a three-dimensional world coordinate system, it can be
employed to recognize diverse traffic violations or to supply intelligent
vehicles with valuable traffic information. We build on YOLOv3 as a detector
yielding 2D bounding boxes and class labels for each vehicle. A 2D detector
renders our system much more independent to different camera perspectives as a
variety of labeled training data is available. This allows for a good
generalization while also being more hardware efficient. The task of 3D
tracking based on 2D detections is supported by integrating class specific
prior knowledge about the vehicle shape. We quantitatively evaluate UTS using
self generated synthetic data and ground truth from the CARLA simulator, due to
the non-existence of datasets with an urban vehicle surveillance setting and
labeled 3D bounding boxes. Additionally, we give a qualitative impression of
how UTS performs on real-world data. Our implementation is capable of operating
in real time on a reasonably modern workstation. To the best of our knowledge,
UTS is to date the only 3D vehicle tracking system in a surveillance scenario
(static camera observing moving targets).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bradler_H/0/1/0/all/0/1"&gt;Henry Bradler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kretz_A/0/1/0/all/0/1"&gt;Adrian Kretz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mester_R/0/1/0/all/0/1"&gt;Rudolf Mester&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Nondeterminism and Instability in Neural Network Optimization. (arXiv:2103.04514v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.04514</id>
        <link href="http://arxiv.org/abs/2103.04514"/>
        <updated>2021-06-02T02:50:01.733Z</updated>
        <summary type="html"><![CDATA[Nondeterminism in neural network optimization produces uncertainty in
performance, making small improvements difficult to discern from run-to-run
variability. While uncertainty can be reduced by training multiple model
copies, doing so is time-consuming, costly, and harms reproducibility. In this
work, we establish an experimental protocol for understanding the effect of
optimization nondeterminism on model diversity, allowing us to isolate the
effects of a variety of sources of nondeterminism. Surprisingly, we find that
all sources of nondeterminism have similar effects on measures of model
diversity. To explain this intriguing fact, we identify the instability of
model training, taken as an end-to-end procedure, as the key determinant. We
show that even one-bit changes in initial parameters result in models
converging to vastly different values. Last, we propose two approaches for
reducing the effects of instability on run-to-run variability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Summers_C/0/1/0/all/0/1"&gt;Cecilia Summers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dinneen_M/0/1/0/all/0/1"&gt;Michael J. Dinneen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What Can I Do Here? Learning New Skills by Imagining Visual Affordances. (arXiv:2106.00671v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2106.00671</id>
        <link href="http://arxiv.org/abs/2106.00671"/>
        <updated>2021-06-02T02:50:01.732Z</updated>
        <summary type="html"><![CDATA[A generalist robot equipped with learned skills must be able to perform many
tasks in many different environments. However, zero-shot generalization to new
settings is not always possible. When the robot encounters a new environment or
object, it may need to finetune some of its previously learned skills to
accommodate this change. But crucially, previously learned behaviors and models
should still be suitable to accelerate this relearning. In this paper, we aim
to study how generative models of possible outcomes can allow a robot to learn
visual representations of affordances, so that the robot can sample potentially
possible outcomes in new situations, and then further train its policy to
achieve those outcomes. In effect, prior data is used to learn what kinds of
outcomes may be possible, such that when the robot encounters an unfamiliar
setting, it can sample potential outcomes from its model, attempt to reach
them, and thereby update both its skills and its outcome model. This approach,
visuomotor affordance learning (VAL), can be used to train goal-conditioned
policies that operate on raw image inputs, and can rapidly learn to manipulate
new objects via our proposed affordance-directed exploration scheme. We show
that VAL can utilize prior data to solve real-world tasks such drawer opening,
grasping, and placing objects in new scenes with only five minutes of online
experience in the new scene.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khazatsky_A/0/1/0/all/0/1"&gt;Alexander Khazatsky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nair_A/0/1/0/all/0/1"&gt;Ashvin Nair&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jing_D/0/1/0/all/0/1"&gt;Daniel Jing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1"&gt;Sergey Levine&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Synaptic Integration of Spatiotemporal Features with a Dynamic Neuromorphic Processor. (arXiv:2002.04924v2 [cs.NE] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.04924</id>
        <link href="http://arxiv.org/abs/2002.04924"/>
        <updated>2021-06-02T02:50:01.731Z</updated>
        <summary type="html"><![CDATA[Spiking neurons can perform spatiotemporal feature detection by nonlinear
synaptic and dendritic integration of presynaptic spike patterns.
Multicompartment models of non-linear dendrites and related neuromorphic
circuit designs enable faithful imitation of such dynamic integration
processes, but these approaches are also associated with a relatively high
computing cost or circuit size. Here, we investigate synaptic integration of
spatiotemporal spike patterns with multiple dynamic synapses on point-neurons
in the DYNAP-SE neuromorphic processor, which offers a complementary
resource-efficient, albeit less flexible, approach to feature detection. We
investigate how previously proposed excitatory--inhibitory pairs of dynamic
synapses can be combined to integrate multiple inputs, and we generalize that
concept to a case in which one inhibitory synapse is combined with multiple
excitatory synapses. We characterize the resulting delayed excitatory
postsynaptic potentials (EPSPs) by measuring and analyzing the membrane
potentials of the neuromorphic neuronal circuits. We find that biologically
relevant EPSP delays, with variability of order 10 milliseconds per neuron, can
be realized in the proposed manner by selecting different synapse combinations,
thanks to device mismatch. Based on these results, we demonstrate that a single
point-neuron with dynamic synapses in the DYNAP-SE can respond selectively to
presynaptic spikes with a particular spatiotemporal structure, which enables,
for instance, visual feature tuning of single neurons.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nilsson_M/0/1/0/all/0/1"&gt;Mattias Nilsson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liwicki_F/0/1/0/all/0/1"&gt;Foteini Liwicki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sandin_F/0/1/0/all/0/1"&gt;Fredrik Sandin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What Can I Do Here? Learning New Skills by Imagining Visual Affordances. (arXiv:2106.00671v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2106.00671</id>
        <link href="http://arxiv.org/abs/2106.00671"/>
        <updated>2021-06-02T02:50:01.647Z</updated>
        <summary type="html"><![CDATA[A generalist robot equipped with learned skills must be able to perform many
tasks in many different environments. However, zero-shot generalization to new
settings is not always possible. When the robot encounters a new environment or
object, it may need to finetune some of its previously learned skills to
accommodate this change. But crucially, previously learned behaviors and models
should still be suitable to accelerate this relearning. In this paper, we aim
to study how generative models of possible outcomes can allow a robot to learn
visual representations of affordances, so that the robot can sample potentially
possible outcomes in new situations, and then further train its policy to
achieve those outcomes. In effect, prior data is used to learn what kinds of
outcomes may be possible, such that when the robot encounters an unfamiliar
setting, it can sample potential outcomes from its model, attempt to reach
them, and thereby update both its skills and its outcome model. This approach,
visuomotor affordance learning (VAL), can be used to train goal-conditioned
policies that operate on raw image inputs, and can rapidly learn to manipulate
new objects via our proposed affordance-directed exploration scheme. We show
that VAL can utilize prior data to solve real-world tasks such drawer opening,
grasping, and placing objects in new scenes with only five minutes of online
experience in the new scene.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khazatsky_A/0/1/0/all/0/1"&gt;Alexander Khazatsky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nair_A/0/1/0/all/0/1"&gt;Ashvin Nair&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jing_D/0/1/0/all/0/1"&gt;Daniel Jing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1"&gt;Sergey Levine&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Interpretable Attention Networks for Cervical Cancer Analysis. (arXiv:2106.00557v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.00557</id>
        <link href="http://arxiv.org/abs/2106.00557"/>
        <updated>2021-06-02T02:50:01.555Z</updated>
        <summary type="html"><![CDATA[Recent advances in deep learning have enabled the development of automated
frameworks for analysing medical images and signals, including analysis of
cervical cancer. Many previous works focus on the analysis of isolated cervical
cells, or do not offer sufficient methods to explain and understand how the
proposed models reach their classification decisions on multi-cell images.
Here, we evaluate various state-of-the-art deep learning models and
attention-based frameworks for the classification of images of multiple
cervical cells. As we aim to provide interpretable deep learning models to
address this task, we also compare their explainability through the
visualization of their gradients. We demonstrate the importance of using images
that contain multiple cells over using isolated single-cell images. We show the
effectiveness of the residual channel attention model for extracting important
features from a group of cells, and demonstrate this model's efficiency for
this classification task. This work highlights the benefits of channel
attention mechanisms in analyzing multiple-cell images for potential relations
and distributions within a group of cells. It also provides interpretable
models to address the classification of cervical cells.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1"&gt;Ruiqi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Armin_M/0/1/0/all/0/1"&gt;Mohammad Ali Armin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Denman_S/0/1/0/all/0/1"&gt;Simon Denman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Petersson_L/0/1/0/all/0/1"&gt;Lars Petersson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ahmedt_Aristizabal_D/0/1/0/all/0/1"&gt;David Ahmedt-Aristizabal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GANs Can Play Lottery Tickets Too. (arXiv:2106.00134v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.00134</id>
        <link href="http://arxiv.org/abs/2106.00134"/>
        <updated>2021-06-02T02:50:01.503Z</updated>
        <summary type="html"><![CDATA[Deep generative adversarial networks (GANs) have gained growing popularity in
numerous scenarios, while usually suffer from high parameter complexities for
resource-constrained real-world applications. However, the compression of GANs
has less been explored. A few works show that heuristically applying
compression techniques normally leads to unsatisfactory results, due to the
notorious training instability of GANs. In parallel, the lottery ticket
hypothesis shows prevailing success on discriminative models, in locating
sparse matching subnetworks capable of training in isolation to full model
performance. In this work, we for the first time study the existence of such
trainable matching subnetworks in deep GANs. For a range of GANs, we certainly
find matching subnetworks at 67%-74% sparsity. We observe that with or without
pruning discriminator has a minor effect on the existence and quality of
matching subnetworks, while the initialization weights used in the
discriminator play a significant role. We then show the powerful
transferability of these subnetworks to unseen tasks. Furthermore, extensive
experimental results demonstrate that our found subnetworks substantially
outperform previous state-of-the-art GAN compression approaches in both image
generation (e.g. SNGAN) and image-to-image translation GANs (e.g. CycleGAN).
Codes available at https://github.com/VITA-Group/GAN-LTH.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xuxi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhenyu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sui_Y/0/1/0/all/0/1"&gt;Yongduo Sui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1"&gt;Tianlong Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Do We Really Need to Access the Source Data? Source Hypothesis Transfer for Unsupervised Domain Adaptation. (arXiv:2002.08546v6 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.08546</id>
        <link href="http://arxiv.org/abs/2002.08546"/>
        <updated>2021-06-02T02:50:01.501Z</updated>
        <summary type="html"><![CDATA[Unsupervised domain adaptation (UDA) aims to leverage the knowledge learned
from a labeled source dataset to solve similar tasks in a new unlabeled domain.
Prior UDA methods typically require to access the source data when learning to
adapt the model, making them risky and inefficient for decentralized private
data. This work tackles a practical setting where only a trained source model
is available and investigates how we can effectively utilize such a model
without source data to solve UDA problems. We propose a simple yet generic
representation learning framework, named \emph{Source HypOthesis Transfer}
(SHOT). SHOT freezes the classifier module (hypothesis) of the source model and
learns the target-specific feature extraction module by exploiting both
information maximization and self-supervised pseudo-labeling to implicitly
align representations from the target domains to the source hypothesis. To
verify its versatility, we evaluate SHOT in a variety of adaptation cases
including closed-set, partial-set, and open-set domain adaptation. Experiments
indicate that SHOT yields state-of-the-art results among multiple domain
adaptation benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1"&gt;Jian Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_D/0/1/0/all/0/1"&gt;Dapeng Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1"&gt;Jiashi Feng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Instance Correction for Learning with Open-set Noisy Labels. (arXiv:2106.00455v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.00455</id>
        <link href="http://arxiv.org/abs/2106.00455"/>
        <updated>2021-06-02T02:50:01.424Z</updated>
        <summary type="html"><![CDATA[The problem of open-set noisy labels denotes that part of training data have
a different label space that does not contain the true class. Lots of
approaches, e.g., loss correction and label correction, cannot handle such
open-set noisy labels well, since they need training data and test data to
share the same label space, which does not hold for learning with open-set
noisy labels. The state-of-the-art methods thus employ the sample selection
approach to handle open-set noisy labels, which tries to select clean data from
noisy data for network parameters updates. The discarded data are seen to be
mislabeled and do not participate in training. Such an approach is intuitive
and reasonable at first glance. However, a natural question could be raised
"can such data only be discarded during training?". In this paper, we show that
the answer is no. Specifically, we discuss that the instances of discarded data
could consist of some meaningful information for generalization. For this
reason, we do not abandon such data, but use instance correction to modify the
instances of the discarded data, which makes the predictions for the discarded
data consistent with given labels. Instance correction are performed by
targeted adversarial attacks. The corrected data are then exploited for
training to help generalization. In addition to the analytical results, a
series of empirical evidences are provided to justify our claims.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xia_X/0/1/0/all/0/1"&gt;Xiaobo Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tongliang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1"&gt;Bo Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_M/0/1/0/all/0/1"&gt;Mingming Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1"&gt;Jun Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niu_G/0/1/0/all/0/1"&gt;Gang Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sugiyama_M/0/1/0/all/0/1"&gt;Masashi Sugiyama&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Corpus-Based Paraphrase Detection Experiments and Review. (arXiv:2106.00145v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.00145</id>
        <link href="http://arxiv.org/abs/2106.00145"/>
        <updated>2021-06-02T02:50:01.416Z</updated>
        <summary type="html"><![CDATA[Paraphrase detection is important for a number of applications, including
plagiarism detection, authorship attribution, question answering, text
summarization, text mining in general, etc. In this paper, we give a
performance overview of various types of corpus-based models, especially deep
learning (DL) models, with the task of paraphrase detection. We report the
results of eight models (LSI, TF-IDF, Word2Vec, Doc2Vec, GloVe, FastText, ELMO,
and USE) evaluated on three different public available corpora: Microsoft
Research Paraphrase Corpus, Clough and Stevenson and Webis Crowd Paraphrase
Corpus 2011. Through a great number of experiments, we decided on the most
appropriate approaches for text pre-processing: hyper-parameters, sub-model
selection-where they exist (e.g., Skipgram vs. CBOW), distance measures, and
semantic similarity/paraphrase detection threshold. Our findings and those of
other researchers who have used deep learning models show that DL models are
very competitive with traditional state-of-the-art approaches and have
potential that should be further developed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vrbanec_T/0/1/0/all/0/1"&gt;Tedo Vrbanec&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mestrovic_A/0/1/0/all/0/1"&gt;Ana Mestrovic&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[InfoScrub: Towards Attribute Privacy by Targeted Obfuscation. (arXiv:2005.10329v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.10329</id>
        <link href="http://arxiv.org/abs/2005.10329"/>
        <updated>2021-06-02T02:50:01.340Z</updated>
        <summary type="html"><![CDATA[Personal photos of individuals when shared online, apart from exhibiting a
myriad of memorable details, also reveals a wide range of private information
and potentially entails privacy risks (e.g., online harassment, tracking). To
mitigate such risks, it is crucial to study techniques that allow individuals
to limit the private information leaked in visual data. We tackle this problem
in a novel image obfuscation framework: to maximize entropy on inferences over
targeted privacy attributes, while retaining image fidelity. We approach the
problem based on an encoder-decoder style architecture, with two key novelties:
(a) introducing a discriminator to perform bi-directional translation
simultaneously from multiple unpaired domains; (b) predicting an image
interpolation which maximizes uncertainty over a target set of attributes. We
find our approach generates obfuscated images faithful to the original input
images, and additionally increase uncertainty by 6.2$\times$ (or up to 0.85
bits) over the non-obfuscated counterparts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hui-Po Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Orekondy_T/0/1/0/all/0/1"&gt;Tribhuvanesh Orekondy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fritz_M/0/1/0/all/0/1"&gt;Mario Fritz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Grassmannian diffusion maps based dimension reduction and classification for high-dimensional data. (arXiv:2009.07547v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.07547</id>
        <link href="http://arxiv.org/abs/2009.07547"/>
        <updated>2021-06-02T02:50:01.333Z</updated>
        <summary type="html"><![CDATA[This work introduces the Grassmannian Diffusion Maps, a novel nonlinear
dimensionality reduction technique that defines the affinity between points
through their representation as low-dimensional subspaces corresponding to
points on the Grassmann manifold. The method is designed for applications, such
as image recognition and data-based classification of high-dimensional data
that can be compactly represented in a lower dimensional subspace. The GDMaps
is composed of two stages. The first is a pointwise linear dimensionality
reduction wherein each high-dimensional object is mapped onto the Grassmann.
The second stage is a multi-point nonlinear kernel-based dimension reduction
using Diffusion maps to identify the subspace structure of the points on the
Grassmann manifold. To this aim, an appropriate Grassmannian kernel is used to
construct the transition matrix of a random walk on a graph connecting points
on the Grassmann manifold. Spectral analysis of the transition matrix yields
low-dimensional Grassmannian diffusion coordinates embedding the data into a
low-dimensional reproducing kernel Hilbert space. Further, a novel data
classification/recognition technique is developed based on the construction of
an overcomplete dictionary of reduced dimension whose atoms are given by the
Grassmannian diffusion coordinates. Three examples are considered. First, a
"toy" example shows that the GDMaps can identify an appropriate parametrization
of structured points on the unit sphere. The second example demonstrates the
ability of the GDMaps to reveal the intrinsic subspace structure of
high-dimensional random field data. In the last example, a face recognition
problem is solved considering face images subject to varying illumination
conditions, changes in face expressions, and occurrence of occlusions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Santos_K/0/1/0/all/0/1"&gt;K. R. M. dos Santos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Giovanis_D/0/1/0/all/0/1"&gt;D. G. Giovanis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shields_M/0/1/0/all/0/1"&gt;M. D. Shields&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Look Wide and Interpret Twice: Improving Performance on Interactive Instruction-following Tasks. (arXiv:2106.00596v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.00596</id>
        <link href="http://arxiv.org/abs/2106.00596"/>
        <updated>2021-06-02T02:50:01.317Z</updated>
        <summary type="html"><![CDATA[There is a growing interest in the community in making an embodied AI agent
perform a complicated task while interacting with an environment following
natural language directives. Recent studies have tackled the problem using
ALFRED, a well-designed dataset for the task, but achieved only very low
accuracy. This paper proposes a new method, which outperforms the previous
methods by a large margin. It is based on a combination of several new ideas.
One is a two-stage interpretation of the provided instructions. The method
first selects and interprets an instruction without using visual information,
yielding a tentative action sequence prediction. It then integrates the
prediction with the visual information etc., yielding the final prediction of
an action and an object. As the object's class to interact is identified in the
first stage, it can accurately select the correct object from the input image.
Moreover, our method considers multiple egocentric views of the environment and
extracts essential information by applying hierarchical attention conditioned
on the current instruction. This contributes to the accurate prediction of
actions for navigation. A preliminary version of the method won the ALFRED
Challenge 2020. The current version achieves the unseen environment's success
rate of 4.45% with a single view, which is further improved to 8.37% with
multiple views.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_V/0/1/0/all/0/1"&gt;Van-Quang Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suganuma_M/0/1/0/all/0/1"&gt;Masanori Suganuma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Okatani_T/0/1/0/all/0/1"&gt;Takayuki Okatani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DikpolaSat Mission: Improvement of Space Flight Performance and Optimal Control Using Trained Deep Neural Network -- Trajectory Controller for Space Objects Collision Avoidance. (arXiv:2106.00007v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2106.00007</id>
        <link href="http://arxiv.org/abs/2106.00007"/>
        <updated>2021-06-02T02:50:01.300Z</updated>
        <summary type="html"><![CDATA[This paper introduced the space mission DikpolaSat Mission, how this research
fits into the mission, and the importance of having a trained DNN model instead
of the usual GN&C functionality. This paper shows how the controller
demonstration is carried out by having the spacecraft follow a desired path,
specified in the referenced model. Increases can be made by examining the route
used to construct a DNN and understanding the effects of various activating
functions on system efficiency. The obstacle avoidance algorithm is built into
the control features to respond spontaneously using inputs from the neural
network for collision avoidance while optimizing the modified trajectory. The
action of a neural network to control the adaptive nature of the nonlinear
mechanisms in the controller will make the control system capable of handling
multiple nonlinear events and also uncertainties that have not been induced in
the control algorithm. Multiple algorithms for optimizing flight controls and
fuel consumption can be implemented using knowledge of flight dynamics in
trajectory and also in the event of obstacle avoidance. This paper also
explains how a DNN can learn to control the flight path and make the system
more reliable with each launch, thereby improving the chances of predicting
collisions of space objects. The data released from this research is used to
design more advanced DNN model capable of predicting other orbital events as
well.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ntumba_M/0/1/0/all/0/1"&gt;Manuel Ntumba&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gore_S/0/1/0/all/0/1"&gt;Saurabh Gore&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Awanyo_J/0/1/0/all/0/1"&gt;Jean Baptiste Awanyo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A unified PAC-Bayesian framework for machine unlearning via information risk minimization. (arXiv:2106.00265v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.00265</id>
        <link href="http://arxiv.org/abs/2106.00265"/>
        <updated>2021-06-02T02:50:01.293Z</updated>
        <summary type="html"><![CDATA[Machine unlearning refers to mechanisms that can remove the influence of a
subset of training data upon request from a trained model without incurring the
cost of re-training from scratch. This paper develops a unified PAC-Bayesian
framework for machine unlearning that recovers the two recent design principles
- variational unlearning (Nguyen et.al., 2020) and forgetting Lagrangian
(Golatkar et.al., 2020) - as information risk minimization problems
(Zhang,2006). Accordingly, both criteria can be interpreted as PAC-Bayesian
upper bounds on the test loss of the unlearned model that take the form of free
energy metrics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jose_S/0/1/0/all/0/1"&gt;Sharu Theresa Jose&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Simeone_O/0/1/0/all/0/1"&gt;Osvaldo Simeone&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Impact of lung segmentation on the diagnosis and explanation of COVID-19 in chest X-ray images. (arXiv:2009.09780v3 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.09780</id>
        <link href="http://arxiv.org/abs/2009.09780"/>
        <updated>2021-06-02T02:50:01.262Z</updated>
        <summary type="html"><![CDATA[The COVID-19 pandemic is undoubtedly one of the biggest public health crises
our society has ever faced in recent history. One of the main complications
caused by COVID-19 is pneumonia, which is diagnosed using imaging exams, such
as chest X-ray (CXR) and computed tomography (CT) scan. The CT scan is more
precise than the CXR. However, CXR is suitable in particular situations because
it is cheaper, faster, more widespread, and exposes the patient to less
radiation. This study aims to demonstrate the impact of lung segmentation in
COVID-19 identification using CXR images and evaluate which contents of the
image decisively contribute to its identification. We performed the lung
segmentation using a U-Net CNN architecture, and the classification using three
well-known CNN architectures: VGG, ResNet, and Inception. To estimate the
impact of lung segmentation, we applied some Explainable Artificial
Intelligence (XAI) techniques, specifically LIME and Grad-CAM. To empirically
evaluate our approach, we composed a database with three classes: lung opacity
(pneumonia), COVID-19, and normal. The segmentation achieved a Jaccard distance
of 0.034 and a Dice coefficient of 0.982. The classification using segmented
lung achieved an F1-Score of 0.88 for the multi-class setup and 0.83 for
COVID-19 identification. Further testing and XAI techniques suggest that
segmented CXR images represent a much more realistic and less biased
performance. To the best of our knowledge, no other work tried to estimate the
impact of lung segmentation in COVID-19 identification using comprehensive XAI
techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Teixeira_L/0/1/0/all/0/1"&gt;Lucas O. Teixeira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Pereira_R/0/1/0/all/0/1"&gt;Rodolfo M. Pereira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bertolini_D/0/1/0/all/0/1"&gt;Diego Bertolini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Oliveira_L/0/1/0/all/0/1"&gt;Luiz S. Oliveira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Nanni_L/0/1/0/all/0/1"&gt;Loris Nanni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cavalcanti_G/0/1/0/all/0/1"&gt;George D. C. Cavalcanti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Costa_Y/0/1/0/all/0/1"&gt;Yandre M. G. Costa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CIDER: Commonsense Inference for Dialogue Explanation and Reasoning. (arXiv:2106.00510v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.00510</id>
        <link href="http://arxiv.org/abs/2106.00510"/>
        <updated>2021-06-02T02:50:01.229Z</updated>
        <summary type="html"><![CDATA[Commonsense inference to understand and explain human language is a
fundamental research problem in natural language processing. Explaining human
conversations poses a great challenge as it requires contextual understanding,
planning, inference, and several aspects of reasoning including causal,
temporal, and commonsense reasoning. In this work, we introduce CIDER -- a
manually curated dataset that contains dyadic dialogue explanations in the form
of implicit and explicit knowledge triplets inferred using contextual
commonsense inference. Extracting such rich explanations from conversations can
be conducive to improving several downstream applications. The annotated
triplets are categorized by the type of commonsense knowledge present (e.g.,
causal, conditional, temporal). We set up three different tasks conditioned on
the annotated dataset: Dialogue-level Natural Language Inference, Span
Extraction, and Multi-choice Span Selection. Baseline results obtained with
transformer-based models reveal that the tasks are difficult, paving the way
for promising future research. The dataset and the baseline implementations are
publicly available at https://github.com/declare-lab/CIDER.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ghosal_D/0/1/0/all/0/1"&gt;Deepanway Ghosal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hong_P/0/1/0/all/0/1"&gt;Pengfei Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_S/0/1/0/all/0/1"&gt;Siqi Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Majumder_N/0/1/0/all/0/1"&gt;Navonil Majumder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mihalcea_R/0/1/0/all/0/1"&gt;Rada Mihalcea&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Poria_S/0/1/0/all/0/1"&gt;Soujanya Poria&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Objective SPIBB: Seldonian Offline Policy Improvement with Safety Constraints in Finite MDPs. (arXiv:2106.00099v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.00099</id>
        <link href="http://arxiv.org/abs/2106.00099"/>
        <updated>2021-06-02T02:50:01.223Z</updated>
        <summary type="html"><![CDATA[We study the problem of Safe Policy Improvement (SPI) under constraints in
the offline Reinforcement Learning (RL) setting. We consider the scenario
where: (i) we have a dataset collected under a known baseline policy, (ii)
multiple reward signals are received from the environment inducing as many
objectives to optimize. We present an SPI formulation for this RL setting that
takes into account the preferences of the algorithm's user for handling the
trade-offs for different reward signals while ensuring that the new policy
performs at least as well as the baseline policy along each individual
objective. We build on traditional SPI algorithms and propose a novel method
based on Safe Policy Iteration with Baseline Bootstrapping (SPIBB, Laroche et
al., 2019) that provides high probability guarantees on the performance of the
agent in the true environment. We show the effectiveness of our method on a
synthetic grid-world safety task as well as in a real-world critical care
context to learn a policy for the administration of IV fluids and vasopressors
to treat sepsis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Satija_H/0/1/0/all/0/1"&gt;Harsh Satija&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thomas_P/0/1/0/all/0/1"&gt;Philip S. Thomas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pineau_J/0/1/0/all/0/1"&gt;Joelle Pineau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Laroche_R/0/1/0/all/0/1"&gt;Romain Laroche&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Worsening Perception: Real-time Degradation of Autonomous Vehicle Perception Performance for Simulation of Adverse Weather Conditions. (arXiv:2103.02760v3 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.02760</id>
        <link href="http://arxiv.org/abs/2103.02760"/>
        <updated>2021-06-02T02:50:01.217Z</updated>
        <summary type="html"><![CDATA[Autonomous vehicles rely heavily upon their perception subsystems to see the
environment in which they operate. Unfortunately, the effect of variable
weather conditions presents a significant challenge to object detection
algorithms, and thus it is imperative to test the vehicle extensively in all
conditions which it may experience. However, development of robust autonomous
vehicle subsystems requires repeatable, controlled testing - while real weather
is unpredictable and cannot be scheduled. Real-world testing in adverse
conditions is an expensive and time-consuming task, often requiring access to
specialist facilities. Simulation is commonly relied upon as a substitute, with
increasingly visually realistic representations of the real-world being
developed. In the context of the complete autonomous vehicle control pipeline,
subsystems downstream of perception need to be tested with accurate recreations
of the perception system output, rather than focusing on subjective visual
realism of the input - whether in simulation or the real world. This study
develops the untapped potential of a lightweight weather augmentation method in
an autonomous racing vehicle - focusing not on visual accuracy, but rather the
effect upon perception subsystem performance in real time. With minimal
adjustment, the prototype developed in this study can replicate the effects of
water droplets on the camera lens, and fading light conditions. This approach
introduces a latency of less than 8 ms using compute hardware well suited to
being carried in the vehicle - rendering it ideal for real-time implementation
that can be run during experiments in simulation, and augmented reality testing
in the real world.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fursa_I/0/1/0/all/0/1"&gt;Ivan Fursa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fandi_E/0/1/0/all/0/1"&gt;Elias Fandi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Musat_V/0/1/0/all/0/1"&gt;Valentina Musat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Culley_J/0/1/0/all/0/1"&gt;Jacob Culley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gil_E/0/1/0/all/0/1"&gt;Enric Gil&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Teeti_I/0/1/0/all/0/1"&gt;Izzedin Teeti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bilous_L/0/1/0/all/0/1"&gt;Louise Bilous&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sluis_I/0/1/0/all/0/1"&gt;Isaac Vander Sluis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rast_A/0/1/0/all/0/1"&gt;Alexander Rast&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bradley_A/0/1/0/all/0/1"&gt;Andrew Bradley&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[COV-ECGNET: COVID-19 detection using ECG trace images with deep convolutional neural network. (arXiv:2106.00436v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.00436</id>
        <link href="http://arxiv.org/abs/2106.00436"/>
        <updated>2021-06-02T02:50:01.211Z</updated>
        <summary type="html"><![CDATA[The reliable and rapid identification of the COVID-19 has become crucial to
prevent the rapid spread of the disease, ease lockdown restrictions and reduce
pressure on public health infrastructures. Recently, several methods and
techniques have been proposed to detect the SARS-CoV-2 virus using different
images and data. However, this is the first study that will explore the
possibility of using deep convolutional neural network (CNN) models to detect
COVID-19 from electrocardiogram (ECG) trace images. In this work, COVID-19 and
other cardiovascular diseases (CVDs) were detected using deep-learning
techniques. A public dataset of ECG images consists of 1937 images from five
distinct categories, such as Normal, COVID-19, myocardial infarction (MI),
abnormal heartbeat (AHB), and recovered myocardial infarction (RMI) were used
in this study. Six different deep CNN models (ResNet18, ResNet50, ResNet101,
InceptionV3, DenseNet201, and MobileNetv2) were used to investigate three
different classification schemes: two-class classification (Normal vs
COVID-19); three-class classification (Normal, COVID-19, and Other CVDs), and
finally, five-class classification (Normal, COVID-19, MI, AHB, and RMI). For
two-class and three-class classification, Densenet201 outperforms other
networks with an accuracy of 99.1%, and 97.36%, respectively; while for the
five-class classification, InceptionV3 outperforms others with an accuracy of
97.83%. ScoreCAM visualization confirms that the networks are learning from the
relevant area of the trace images. Since the proposed method uses ECG trace
images which can be captured by smartphones and are readily available
facilities in low-resources countries, this study will help in faster
computer-aided diagnosis of COVID-19 and other cardiac abnormalities.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Rahman_T/0/1/0/all/0/1"&gt;Tawsifur Rahman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Akinbi_A/0/1/0/all/0/1"&gt;Alex Akinbi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chowdhury_M/0/1/0/all/0/1"&gt;Muhammad E. H. Chowdhury&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rashid_T/0/1/0/all/0/1"&gt;Tarik A. Rashid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sengur_A/0/1/0/all/0/1"&gt;Abdulkadir &amp;#x15e;eng&amp;#xfc;r&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Khandakar_A/0/1/0/all/0/1"&gt;Amith Khandakar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Islam_K/0/1/0/all/0/1"&gt;Khandaker Reajul Islam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ismael_A/0/1/0/all/0/1"&gt;Aras M. Ismael&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fine-grained Generalization Analysis of Structured Output Prediction. (arXiv:2106.00115v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.00115</id>
        <link href="http://arxiv.org/abs/2106.00115"/>
        <updated>2021-06-02T02:50:01.203Z</updated>
        <summary type="html"><![CDATA[In machine learning we often encounter structured output prediction problems
(SOPPs), i.e. problems where the output space admits a rich internal structure.
Application domains where SOPPs naturally occur include natural language
processing, speech recognition, and computer vision. Typical SOPPs have an
extremely large label set, which grows exponentially as a function of the size
of the output. Existing generalization analysis implies generalization bounds
with at least a square-root dependency on the cardinality $d$ of the label set,
which can be vacuous in practice. In this paper, we significantly improve the
state of the art by developing novel high-probability bounds with a logarithmic
dependency on $d$. Moreover, we leverage the lens of algorithmic stability to
develop generalization bounds in expectation without any dependency on $d$. Our
results therefore build a solid theoretical foundation for learning in
large-scale SOPPs. Furthermore, we extend our results to learning with weakly
dependent data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mustafa_W/0/1/0/all/0/1"&gt;Waleed Mustafa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lei_Y/0/1/0/all/0/1"&gt;Yunwen Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ledent_A/0/1/0/all/0/1"&gt;Antoine Ledent&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kloft_M/0/1/0/all/0/1"&gt;Marius Kloft&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[1$\times$N Block Pattern for Network Sparsity. (arXiv:2105.14713v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14713</id>
        <link href="http://arxiv.org/abs/2105.14713"/>
        <updated>2021-06-02T02:50:01.194Z</updated>
        <summary type="html"><![CDATA[Though network sparsity emerges as a promising direction to overcome the
drastically increasing size of neural networks, it remains an open problem to
concurrently maintain model accuracy as well as achieve significant speedups on
general CPUs. In this paper, we propose one novel concept of $1\times N$ block
sparsity pattern (block pruning) to break this limitation. In particular,
consecutive $N$ output kernels with the same input channel index are grouped
into one block, which serves as a basic pruning granularity of our pruning
pattern. Our $1 \times N$ sparsity pattern prunes these blocks considered
unimportant. We also provide a workflow of filter rearrangement that first
rearranges the weight matrix in the output channel dimension to derive more
influential blocks for accuracy improvements, and then applies similar
rearrangement to the next-layer weights in the input channel dimension to
ensure correct convolutional operations. Moreover, the output computation after
our $1 \times N$ block sparsity can be realized via a parallelized block-wise
vectorized operation, leading to significant speedups on general CPUs-based
platforms. The efficacy of our pruning pattern is proved with experiments on
ILSVRC-2012. For example, in the case of 50% sparsity and $N=4$, our pattern
obtains about 3.0% improvements over filter pruning in the top-1 accuracy of
MobileNet-V2. Meanwhile, it obtains 56.04ms inference savings on Cortex-A7 CPU
over weight pruning. Code is available at https://github.com/lmbxmu/1xN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1"&gt;Mingbao Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yuchao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yuxin Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1"&gt;Bohong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chao_F/0/1/0/all/0/1"&gt;Fei Chao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1"&gt;Mengdi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jun Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1"&gt;Rongrong Ji&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AS-Net: Fast Photoacoustic Reconstruction with Multi-feature Fusion from Sparse Data. (arXiv:2101.08934v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.08934</id>
        <link href="http://arxiv.org/abs/2101.08934"/>
        <updated>2021-06-02T02:50:01.188Z</updated>
        <summary type="html"><![CDATA[Photoacoustic (PA) imaging is a biomedical imaging modality capable of
acquiring high-contrast images of optical absorption at depths much greater
than traditional optical imaging techniques. However, practical instrumentation
and geometry limit the number of available acoustic sensors surrounding the
imaging target, which results in the sparsity of sensor data. Conventional PA
image reconstruction methods give severe artifacts when they are applied
directly to the sparse PA data. In this paper, we firstly propose to employ a
novel signal processing method to make sparse PA raw data more suitable for the
neural network, concurrently speeding up image reconstruction. Then we propose
Attention Steered Network (AS-Net) for PA reconstruction with multi-feature
fusion. AS-Net is validated on different datasets, including simulated
photoacoustic data from fundus vasculature phantoms and experimental data from
in vivo fish and mice. Notably, the method is also able to eliminate some
artifacts present in the ground truth for in vivo data. Results demonstrated
that our method provides superior reconstructions at a faster speed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1"&gt;Mengjie Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lan_H/0/1/0/all/0/1"&gt;Hengrong Lan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1"&gt;Changchun Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_F/0/1/0/all/0/1"&gt;Fei Gao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[H-FL: A Hierarchical Communication-Efficient and Privacy-Protected Architecture for Federated Learning. (arXiv:2106.00275v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.00275</id>
        <link href="http://arxiv.org/abs/2106.00275"/>
        <updated>2021-06-02T02:50:01.166Z</updated>
        <summary type="html"><![CDATA[The longstanding goals of federated learning (FL) require rigorous privacy
guarantees and low communication overhead while holding a relatively high model
accuracy. However, simultaneously achieving all the goals is extremely
challenging. In this paper, we propose a novel framework called hierarchical
federated learning (H-FL) to tackle this challenge. Considering the degradation
of the model performance due to the statistic heterogeneity of the training
data, we devise a runtime distribution reconstruction strategy, which
reallocates the clients appropriately and utilizes mediators to rearrange the
local training of the clients. In addition, we design a compression-correction
mechanism incorporated into H-FL to reduce the communication overhead while not
sacrificing the model performance. To further provide privacy guarantees, we
introduce differential privacy while performing local training, which injects
moderate amount of noise into only part of the complete model. Experimental
results show that our H-FL framework achieves the state-of-art performance on
different datasets for the real-world image recognition tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;He Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[3D Axial-Attention for Lung Nodule Classification. (arXiv:2012.14117v3 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.14117</id>
        <link href="http://arxiv.org/abs/2012.14117"/>
        <updated>2021-06-02T02:50:01.158Z</updated>
        <summary type="html"><![CDATA[Purpose: In recent years, Non-Local based methods have been successfully
applied to lung nodule classification. However, these methods offer 2D
attention or limited 3D attention to low-resolution feature maps. Moreover,
they still depend on a convenient local filter such as convolution as full 3D
attention is expensive to compute and requires a big dataset, which might not
be available.

Methods: We propose to use 3D Axial-Attention, which requires a fraction of
the computing power of a regular Non-Local network (i.e., self-attention).
Unlike a regular Non-Local network, the 3D Axial-Attention network applies the
attention operation to each axis separately. Additionally, we solve the
invariant position problem of the Non-Local network by proposing to add 3D
positional encoding to shared embeddings.

Results: We validated the proposed method on 442 benign nodules and 406
malignant nodules, extracted from the public LIDC-IDRI dataset by following a
rigorous experimental setup using only nodules annotated by at least three
radiologists. Our results show that the 3D Axial-Attention model achieves
state-of-the-art performance on all evaluation metrics, including AUC and
Accuracy.

Conclusions: The proposed model provides full 3D attention, whereby every
element (i.e., pixel) in the 3D volume space attends to every other element in
the nodule effectively. Thus, the 3D Axial-Attention network can be used in all
layers without the need for local filters. The experimental results show the
importance of full 3D attention for classifying lung nodules.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Al_Shabi_M/0/1/0/all/0/1"&gt;Mundher Al-Shabi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shak_K/0/1/0/all/0/1"&gt;Kelvin Shak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tan_M/0/1/0/all/0/1"&gt;Maxine Tan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fidelity Estimation Improves Noisy-Image Classification with Pretrained Networks. (arXiv:2106.00673v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.00673</id>
        <link href="http://arxiv.org/abs/2106.00673"/>
        <updated>2021-06-02T02:50:01.140Z</updated>
        <summary type="html"><![CDATA[Image classification has significantly improved using deep learning. This is
mainly due to convolutional neural networks (CNNs) that are capable of learning
rich feature extractors from large datasets. However, most deep learning
classification methods are trained on clean images and are not robust when
handling noisy ones, even if a restoration preprocessing step is applied. While
novel methods address this problem, they rely on modified feature extractors
and thus necessitate retraining. We instead propose a method that can be
applied on a pretrained classifier. Our method exploits a fidelity map estimate
that is fused into the internal representations of the feature extractor,
thereby guiding the attention of the network and making it more robust to noisy
data. We improve the noisy-image classification (NIC) results by significantly
large margins, especially at high noise levels, and come close to the fully
retrained approaches. Furthermore, as proof of concept, we show that when using
our oracle fidelity map we even outperform the fully retrained methods, whether
trained on noisy or restored images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1"&gt;Xiaoyu Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhattacharjee_D/0/1/0/all/0/1"&gt;Deblina Bhattacharjee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Helou_M/0/1/0/all/0/1"&gt;Majed El Helou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Susstrunk_S/0/1/0/all/0/1"&gt;Sabine S&amp;#xfc;sstrunk&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Frivolous Units: Wider Networks Are Not Really That Wide. (arXiv:1912.04783v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1912.04783</id>
        <link href="http://arxiv.org/abs/1912.04783"/>
        <updated>2021-06-02T02:50:01.132Z</updated>
        <summary type="html"><![CDATA[A remarkable characteristic of overparameterized deep neural networks (DNNs)
is that their accuracy does not degrade when the network's width is increased.
Recent evidence suggests that developing compressible representations is key
for adjusting the complexity of large networks to the learning task at hand.
However, these compressible representations are poorly understood. A promising
strand of research inspired from biology is understanding representations at
the unit level as it offers a more granular and intuitive interpretation of the
neural mechanisms. In order to better understand what facilitates increases in
width without decreases in accuracy, we ask: Are there mechanisms at the unit
level by which networks control their effective complexity as their width is
increased? If so, how do these depend on the architecture, dataset, and
training parameters? We identify two distinct types of "frivolous" units that
proliferate when the network's width is increased: prunable units which can be
dropped out of the network without significant change to the output and
redundant units whose activities can be expressed as a linear combination of
others. These units imply complexity constraints as the function the network
represents could be expressed by a network without them. We also identify how
the development of these units can be influenced by architecture and a number
of training factors. Together, these results help to explain why the accuracy
of DNNs does not degrade when width is increased and highlight the importance
of frivolous units toward understanding implicit regularization in DNNs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Casper_S/0/1/0/all/0/1"&gt;Stephen Casper&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boix_X/0/1/0/all/0/1"&gt;Xavier Boix&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+DAmario_V/0/1/0/all/0/1"&gt;Vanessa D&amp;#x27;Amario&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_L/0/1/0/all/0/1"&gt;Ling Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schrimpf_M/0/1/0/all/0/1"&gt;Martin Schrimpf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vinken_K/0/1/0/all/0/1"&gt;Kasper Vinken&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kreiman_G/0/1/0/all/0/1"&gt;Gabriel Kreiman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarial Defense for Automatic Speaker Verification by Self-Supervised Learning. (arXiv:2106.00273v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2106.00273</id>
        <link href="http://arxiv.org/abs/2106.00273"/>
        <updated>2021-06-02T02:50:01.122Z</updated>
        <summary type="html"><![CDATA[Previous works have shown that automatic speaker verification (ASV) is
seriously vulnerable to malicious spoofing attacks, such as replay, synthetic
speech, and recently emerged adversarial attacks. Great efforts have been
dedicated to defending ASV against replay and synthetic speech; however, only a
few approaches have been explored to deal with adversarial attacks. All the
existing approaches to tackle adversarial attacks for ASV require the knowledge
for adversarial samples generation, but it is impractical for defenders to know
the exact attack algorithms that are applied by the in-the-wild attackers. This
work is among the first to perform adversarial defense for ASV without knowing
the specific attack algorithms. Inspired by self-supervised learning models
(SSLMs) that possess the merits of alleviating the superficial noise in the
inputs and reconstructing clean samples from the interrupted ones, this work
regards adversarial perturbations as one kind of noise and conducts adversarial
defense for ASV by SSLMs. Specifically, we propose to perform adversarial
defense from two perspectives: 1) adversarial perturbation purification and 2)
adversarial perturbation detection. Experimental results show that our
detection module effectively shields the ASV by detecting adversarial samples
with an accuracy of around 80%. Moreover, since there is no common metric for
evaluating the adversarial defense performance for ASV, this work also
formalizes evaluation metrics for adversarial defense considering both
purification and detection based approaches into account. We sincerely
encourage future works to benchmark their approaches based on the proposed
evaluation framework.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1"&gt;Haibin Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1"&gt;Andy T. Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Zhiyong Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meng_H/0/1/0/all/0/1"&gt;Helen Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1"&gt;Hung-yi Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Digital rock reconstruction with user-defined properties using conditional generative adversarial networks. (arXiv:2012.07719v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.07719</id>
        <link href="http://arxiv.org/abs/2012.07719"/>
        <updated>2021-06-02T02:50:01.116Z</updated>
        <summary type="html"><![CDATA[Uncertainty is ubiquitous with flow in subsurface rocks because of their
inherent heterogeneity and lack of in-situ measurements. To complete
uncertainty analysis in a multi-scale manner, it is a prerequisite to provide
sufficient rock samples. Even though the advent of digital rock technology
offers opportunities to reproduce rocks, it still cannot be utilized to provide
massive samples due to its high cost, thus leading to the development of
diversified mathematical methods. Among them, two-point statistics (TPS) and
multi-point statistics (MPS) are commonly utilized, which feature incorporating
low-order and high-order statistical information, respectively. Recently,
generative adversarial networks (GANs) are becoming increasingly popular since
they can reproduce training images with excellent visual and consequent
geologic realism. However, standard GANs can only incorporate information from
data, while leaving no interface for user-defined properties, and thus may
limit the representativeness of reconstructed samples. In this study, we
propose conditional GANs for digital rock reconstruction, aiming to reproduce
samples not only similar to the real training data, but also satisfying
user-specified properties. In fact, the proposed framework can realize the
targets of MPS and TPS simultaneously by incorporating high-order information
directly from rock images with the GANs scheme, while preserving low-order
counterparts through conditioning. We conduct three reconstruction experiments,
and the results demonstrate that rock type, rock porosity, and correlation
length can be successfully conditioned to affect the reconstructed rock images.
Furthermore, in contrast to existing GANs, the proposed conditioning enables
learning of multiple rock types simultaneously, and thus invisibly saves
computational cost.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Q/0/1/0/all/0/1"&gt;Qiang Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1"&gt;Dongxiao Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exposing Previously Undetectable Faults in Deep Neural Networks. (arXiv:2106.00576v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.00576</id>
        <link href="http://arxiv.org/abs/2106.00576"/>
        <updated>2021-06-02T02:50:01.108Z</updated>
        <summary type="html"><![CDATA[Existing methods for testing DNNs solve the oracle problem by constraining
the raw features (e.g. image pixel values) to be within a small distance of a
dataset example for which the desired DNN output is known. But this limits the
kinds of faults these approaches are able to detect. In this paper, we
introduce a novel DNN testing method that is able to find faults in DNNs that
other methods cannot. The crux is that, by leveraging generative machine
learning, we can generate fresh test inputs that vary in their high-level
features (for images, these include object shape, location, texture, and
colour). We demonstrate that our approach is capable of detecting deliberately
injected faults as well as new faults in state-of-the-art DNNs, and that in
both cases, existing methods are unable to find these faults.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dunn_I/0/1/0/all/0/1"&gt;Isaac Dunn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pouget_H/0/1/0/all/0/1"&gt;Hadrien Pouget&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kroening_D/0/1/0/all/0/1"&gt;Daniel Kroening&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Melham_T/0/1/0/all/0/1"&gt;Tom Melham&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Exploratory Analysis of Multilingual Word-Level Quality Estimation with Cross-Lingual Transformers. (arXiv:2106.00143v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.00143</id>
        <link href="http://arxiv.org/abs/2106.00143"/>
        <updated>2021-06-02T02:50:01.098Z</updated>
        <summary type="html"><![CDATA[Most studies on word-level Quality Estimation (QE) of machine translation
focus on language-specific models. The obvious disadvantages of these
approaches are the need for labelled data for each language pair and the high
cost required to maintain several language-specific models. To overcome these
problems, we explore different approaches to multilingual, word-level QE. We
show that these QE models perform on par with the current language-specific
models. In the cases of zero-shot and few-shot QE, we demonstrate that it is
possible to accurately predict word-level quality for any given new language
pair from models trained on other language pairs. Our findings suggest that the
word-level QE models based on powerful pre-trained transformers that we propose
in this paper generalise well across languages, making them more useful in
real-world scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ranasinghe_T/0/1/0/all/0/1"&gt;Tharindu Ranasinghe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Orasan_C/0/1/0/all/0/1"&gt;Constantin Orasan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mitkov_R/0/1/0/all/0/1"&gt;Ruslan Mitkov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Prior-Enhanced Few-Shot Segmentation with Meta-Prototypes. (arXiv:2106.00572v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.00572</id>
        <link href="http://arxiv.org/abs/2106.00572"/>
        <updated>2021-06-02T02:50:01.090Z</updated>
        <summary type="html"><![CDATA[Few-shot segmentation~(FSS) performance has been extensively promoted by
introducing episodic training and class-wise prototypes. However, the FSS
problem remains challenging due to three limitations: (1) Models are distracted
by task-unrelated information; (2) The representation ability of a single
prototype is limited; (3) Class-related prototypes ignore the prior knowledge
of base classes. We propose the Prior-Enhanced network with Meta-Prototypes to
tackle these limitations. The prior-enhanced network leverages the support and
query (pseudo-) labels in feature extraction, which guides the model to focus
on the task-related features of the foreground objects, and suppress much noise
due to the lack of supervised knowledge. Moreover, we introduce multiple
meta-prototypes to encode hierarchical features and learn class-agnostic
structural information. The hierarchical features help the model highlight the
decision boundary and focus on hard pixels, and the structural information
learned from base classes is treated as the prior knowledge for novel classes.
Experiments show that our method achieves the mean-IoU scores of 60.79% and
41.16% on PASCAL-$5^i$ and COCO-$20^i$, outperforming the state-of-the-art
method by 3.49% and 5.64% in the 5-shot setting. Moreover, comparing with
1-shot results, our method promotes 5-shot accuracy by 3.73% and 10.32% on the
above two benchmarks. The source code of our method is available at
https://github.com/Jarvis73/PEMP.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jian-Wei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lv_L/0/1/0/all/0/1"&gt;Lei Lv&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1"&gt;Yawei Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_H/0/1/0/all/0/1"&gt;Hao-Zhe Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yi Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1"&gt;Wei Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep learning for COVID-19 diagnosis based feature selection using binary differential evolution algorithm. (arXiv:2104.07279v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.07279</id>
        <link href="http://arxiv.org/abs/2104.07279"/>
        <updated>2021-06-02T02:50:01.080Z</updated>
        <summary type="html"><![CDATA[The new Coronavirus is spreading rapidly and it has taken the lives of many
people so far. The virus has destructive effects on the human lung and early
detection is very important. Deep Convolution neural networks are a powerful
tool in classifying images. Therefore, in this paper a hybrid approach based on
a deep network is presented. Feature vectors were extracted by applying a deep
convolution neural network on the images and effective features were selected
by the binary differential meta-heuristic algorithm. These optimized features
were given to the SVM classifier. A database consisting of three categories of
images as COVID-19, pneumonia, and healthy included 1092 X-ray samples was
considered. The proposed method achieved an accuracy of 99.43%, a sensitivity
of 99.16%, and a specificity of 99.57%. Our results demonstrate the suggested
approach is better than recent studies on COVID-19 detection with X-ray images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Iraji_M/0/1/0/all/0/1"&gt;Mohammad Saber Iraji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Feizi_Derakhshi_M/0/1/0/all/0/1"&gt;Mohammad-Reza Feizi-Derakhshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tanha_J/0/1/0/all/0/1"&gt;Jafar Tanha&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Decoupling Shape and Density for Liver Lesion Synthesis Using Conditional Generative Adversarial Networks. (arXiv:2106.00629v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.00629</id>
        <link href="http://arxiv.org/abs/2106.00629"/>
        <updated>2021-06-02T02:50:01.057Z</updated>
        <summary type="html"><![CDATA[Lesion synthesis received much attention with the rise of efficient
generative models for augmenting training data, drawing lesion evolution
scenarios, or aiding expert training. The quality and diversity of synthesized
data are highly dependent on the annotated data used to train the models, which
not rarely struggle to derive very different yet realistic samples from the
training ones. That adds an inherent bias to lesion segmentation algorithms and
limits synthesizing lesion evolution scenarios efficiently. This paper presents
a method for decoupling shape and density for liver lesion synthesis, creating
a framework that allows straight-forwardly driving the synthesis. We offer
qualitative results that show the synthesis control by modifying shape and
density individually, and quantitative results that demonstrate that embedding
the density information in the generator model helps to increase lesion
segmentation performance compared to using the shape solely.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Oliveira_D/0/1/0/all/0/1"&gt;Dario Augusto Borges Oliveira&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bootstrap Your Own Correspondences. (arXiv:2106.00677v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.00677</id>
        <link href="http://arxiv.org/abs/2106.00677"/>
        <updated>2021-06-02T02:50:01.045Z</updated>
        <summary type="html"><![CDATA[Geometric feature extraction is a crucial component of point cloud
registration pipelines. Recent work has demonstrated how supervised learning
can be leveraged to learn better and more compact 3D features. However, those
approaches' reliance on ground-truth annotation limits their scalability. We
propose BYOC: a self-supervised approach that learns visual and geometric
features from RGB-D video without relying on ground-truth pose or
correspondence. Our key observation is that randomly-initialized CNNs readily
provide us with good correspondences; allowing us to bootstrap the learning of
both visual and geometric features. Our approach combines classic ideas from
point cloud registration with more recent representation learning approaches.
We evaluate our approach on indoor scene datasets and find that our method
outperforms traditional and learned descriptors, while being competitive with
current state-of-the-art supervised approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Banani_M/0/1/0/all/0/1"&gt;Mohamed El Banani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Johnson_J/0/1/0/all/0/1"&gt;Justin Johnson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Compositional Learning of Image-Text Query for Image Retrieval. (arXiv:2006.11149v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.11149</id>
        <link href="http://arxiv.org/abs/2006.11149"/>
        <updated>2021-06-02T02:50:01.017Z</updated>
        <summary type="html"><![CDATA[In this paper, we investigate the problem of retrieving images from a
database based on a multi-modal (image-text) query. Specifically, the query
text prompts some modification in the query image and the task is to retrieve
images with the desired modifications. For instance, a user of an E-Commerce
platform is interested in buying a dress, which should look similar to her
friend's dress, but the dress should be of white color with a ribbon sash. In
this case, we would like the algorithm to retrieve some dresses with desired
modifications in the query dress. We propose an autoencoder based model,
ComposeAE, to learn the composition of image and text query for retrieving
images. We adopt a deep metric learning approach and learn a metric that pushes
composition of source image and text query closer to the target images. We also
propose a rotational symmetry constraint on the optimization problem. Our
approach is able to outperform the state-of-the-art method TIRG \cite{TIRG} on
three benchmark datasets, namely: MIT-States, Fashion200k and Fashion IQ. In
order to ensure fair comparison, we introduce strong baselines by enhancing
TIRG method. To ensure reproducibility of the results, we publish our code
here: \url{https://github.com/ecom-research/ComposeAE}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Anwaar_M/0/1/0/all/0/1"&gt;Muhammad Umer Anwaar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Labintcev_E/0/1/0/all/0/1"&gt;Egor Labintcev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kleinsteuber_M/0/1/0/all/0/1"&gt;Martin Kleinsteuber&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Diffusion Models Beat GANs on Image Synthesis. (arXiv:2105.05233v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.05233</id>
        <link href="http://arxiv.org/abs/2105.05233"/>
        <updated>2021-06-02T02:50:01.000Z</updated>
        <summary type="html"><![CDATA[We show that diffusion models can achieve image sample quality superior to
the current state-of-the-art generative models. We achieve this on
unconditional image synthesis by finding a better architecture through a series
of ablations. For conditional image synthesis, we further improve sample
quality with classifier guidance: a simple, compute-efficient method for
trading off diversity for fidelity using gradients from a classifier. We
achieve an FID of 2.97 on ImageNet 128$\times$128, 4.59 on ImageNet
256$\times$256, and 7.72 on ImageNet 512$\times$512, and we match BigGAN-deep
even with as few as 25 forward passes per sample, all while maintaining better
coverage of the distribution. Finally, we find that classifier guidance
combines well with upsampling diffusion models, further improving FID to 3.94
on ImageNet 256$\times$256 and 3.85 on ImageNet 512$\times$512. We release our
code at https://github.com/openai/guided-diffusion]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dhariwal_P/0/1/0/all/0/1"&gt;Prafulla Dhariwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nichol_A/0/1/0/all/0/1"&gt;Alex Nichol&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Federated Estimation of Causal Effects from Observational Data. (arXiv:2106.00456v1 [stat.ME])]]></title>
        <id>http://arxiv.org/abs/2106.00456</id>
        <link href="http://arxiv.org/abs/2106.00456"/>
        <updated>2021-06-02T02:50:00.981Z</updated>
        <summary type="html"><![CDATA[Many modern applications collect data that comes in federated spirit, with
data kept locally and undisclosed. Till date, most insight into the causal
inference requires data to be stored in a central repository. We present a
novel framework for causal inference with federated data sources. We assess and
integrate local causal effects from different private data sources without
centralizing them. Then, the treatment effects on subjects from observational
data using a non-parametric reformulation of the classical potential outcomes
framework is estimated. We model the potential outcomes as a random function
distributed by Gaussian processes, whose defining parameters can be efficiently
learned from multiple data sources, respecting privacy constraints. We
demonstrate the promise and efficiency of the proposed approach through a set
of simulated and real-world benchmark examples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Vo_T/0/1/0/all/0/1"&gt;Thanh Vinh Vo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Hoang_T/0/1/0/all/0/1"&gt;Trong Nghia Hoang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lee_Y/0/1/0/all/0/1"&gt;Young Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Leong_T/0/1/0/all/0/1"&gt;Tze-Yun Leong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Privately Learning Subspaces. (arXiv:2106.00001v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2106.00001</id>
        <link href="http://arxiv.org/abs/2106.00001"/>
        <updated>2021-06-02T02:50:00.944Z</updated>
        <summary type="html"><![CDATA[Private data analysis suffers a costly curse of dimensionality. However, the
data often has an underlying low-dimensional structure. For example, when
optimizing via gradient descent, the gradients often lie in or near a
low-dimensional subspace. If that low-dimensional structure can be identified,
then we can avoid paying (in terms of privacy or accuracy) for the high ambient
dimension.

We present differentially private algorithms that take input data sampled
from a low-dimensional linear subspace (possibly with a small amount of error)
and output that subspace (or an approximation to it). These algorithms can
serve as a pre-processing step for other procedures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Singhal_V/0/1/0/all/0/1"&gt;Vikrant Singhal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Steinke_T/0/1/0/all/0/1"&gt;Thomas Steinke&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Reinforcement Learning in Quantitative Algorithmic Trading: A Review. (arXiv:2106.00123v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.00123</id>
        <link href="http://arxiv.org/abs/2106.00123"/>
        <updated>2021-06-02T02:50:00.922Z</updated>
        <summary type="html"><![CDATA[Algorithmic stock trading has become a staple in today's financial market,
the majority of trades being now fully automated. Deep Reinforcement Learning
(DRL) agents proved to be to a force to be reckon with in many complex games
like Chess and Go. We can look at the stock market historical price series and
movements as a complex imperfect information environment in which we try to
maximize return - profit and minimize risk. This paper reviews the progress
made so far with deep reinforcement learning in the subdomain of AI in finance,
more precisely, automated low-frequency quantitative stock trading. Many of the
reviewed studies had only proof-of-concept ideals with experiments conducted in
unrealistic settings and no real-time trading applications. For the majority of
the works, despite all showing statistically significant improvements in
performance compared to established baseline strategies, no decent
profitability level was obtained. Furthermore, there is a lack of experimental
testing in real-time, online trading platforms and a lack of meaningful
comparisons between agents built on different types of DRL or human traders. We
conclude that DRL in stock trading has showed huge applicability potential
rivalling professional traders under strong assumptions, but the research is
still in the very early stages of development.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pricope_T/0/1/0/all/0/1"&gt;Tidor-Vlad Pricope&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Full-Resolution Encoder-Decoder Networks with Multi-Scale Feature Fusion for Human Pose Estimation. (arXiv:2106.00566v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.00566</id>
        <link href="http://arxiv.org/abs/2106.00566"/>
        <updated>2021-06-02T02:50:00.890Z</updated>
        <summary type="html"><![CDATA[To achieve more accurate 2D human pose estimation, we extend the successful
encoder-decoder network, simple baseline network (SBN), in three ways. To
reduce the quantization errors caused by the large output stride size, two more
decoder modules are appended to the end of the simple baseline network to get
full output resolution. Then, the global context blocks (GCBs) are added to the
encoder and decoder modules to enhance them with global context features.
Furthermore, we propose a novel spatial-attention-based multi-scale feature
collection and distribution module (SA-MFCD) to fuse and distribute multi-scale
features to boost the pose estimation. Experimental results on the MS COCO
dataset indicate that our network can remarkably improve the accuracy of human
pose estimation over SBN, our network using ResNet34 as the backbone network
can even achieve the same accuracy as SBN with ResNet152, and our networks can
achieve superior results with big backbone networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ou_J/0/1/0/all/0/1"&gt;Jie Ou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1"&gt;Mingjian Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1"&gt;Hong Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Open Domain Dialogue Generation with Latent Images. (arXiv:2004.01981v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.01981</id>
        <link href="http://arxiv.org/abs/2004.01981"/>
        <updated>2021-06-02T02:50:00.879Z</updated>
        <summary type="html"><![CDATA[We consider grounding open domain dialogues with images. Existing work
assumes that both an image and a textual context are available, but
image-grounded dialogues by nature are more difficult to obtain than textual
dialogues. Thus, we propose learning a response generation model with both
image-grounded dialogues and textual dialogues by assuming that the visual
scene information at the time of a conversation can be represented by an image,
and trying to recover the latent images of the textual dialogues through
text-to-image generation techniques. The likelihood of the two types of
dialogues is then formulated by a response generator and an image reconstructor
that are learned within a conditional variational auto-encoding framework.
Empirical studies are conducted in both image-grounded conversation and
text-based conversation. In the first scenario, image-grounded dialogues,
especially under a low-resource setting, can be effectively augmented by
textual dialogues with latent images; while in the second scenario, latent
images can enrich the content of responses and at the same time keep them
relevant to contexts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Ze Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1"&gt;Wei Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1"&gt;Huang Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Can Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhoujun Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incorporating Visual Layout Structures for Scientific Text Classification. (arXiv:2106.00676v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.00676</id>
        <link href="http://arxiv.org/abs/2106.00676"/>
        <updated>2021-06-02T02:50:00.869Z</updated>
        <summary type="html"><![CDATA[Classifying the core textual components of a scientific paper-title, author,
body text, etc.-is a critical first step in automated scientific document
understanding. Previous work has shown how using elementary layout information,
i.e., each token's 2D position on the page, leads to more accurate
classification. We introduce new methods for incorporating VIsual LAyout
structures (VILA), e.g., the grouping of page texts into text lines or text
blocks, into language models to further improve performance. We show that the
I-VILA approach, which simply adds special tokens denoting boundaries between
layout structures into model inputs, can lead to +1~4.5 F1 Score improvements
in token classification tasks. Moreover, we design a hierarchical model H-VILA
that encodes these layout structures and record a up-to 70% efficiency boost
without hurting prediction accuracy. The experiments are conducted on a newly
curated evaluation suite, S2-VLUE, with a novel metric measuring VILA awareness
and a new dataset covering 19 scientific disciplines with gold annotations.
Pre-trained weights, benchmark datasets, and source code will be available at
https://github.com/allenai/VILA}{https://github.com/allenai/VILA.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1"&gt;Zejiang Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lo_K/0/1/0/all/0/1"&gt;Kyle Lo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Lucy Lu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kuehl_B/0/1/0/all/0/1"&gt;Bailey Kuehl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weld_D/0/1/0/all/0/1"&gt;Daniel S. Weld&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Downey_D/0/1/0/all/0/1"&gt;Doug Downey&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hyperspectral Band Selection for Multispectral Image Classification with Convolutional Networks. (arXiv:2106.00645v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.00645</id>
        <link href="http://arxiv.org/abs/2106.00645"/>
        <updated>2021-06-02T02:50:00.854Z</updated>
        <summary type="html"><![CDATA[In recent years, Hyperspectral Imaging (HSI) has become a powerful source for
reliable data in applications such as remote sensing, agriculture, and
biomedicine. However, hyperspectral images are highly data-dense and often
benefit from methods to reduce the number of spectral bands while retaining the
most useful information for a specific application. We propose a novel band
selection method to select a reduced set of wavelengths, obtained from an HSI
system in the context of image classification. Our approach consists of two
main steps: the first utilizes a filter-based approach to find relevant
spectral bands based on a collinearity analysis between a band and its
neighbors. This analysis helps to remove redundant bands and dramatically
reduces the search space. The second step applies a wrapper-based approach to
select bands from the reduced set based on their information entropy values,
and trains a compact Convolutional Neural Network (CNN) to evaluate the
performance of the current selection. We present classification results
obtained from our method and compare them to other feature selection methods on
two hyperspectral image datasets. Additionally, we use the original
hyperspectral data cube to simulate the process of using actual filters in a
multispectral imager. We show that our method produces more suitable results
for a multispectral sensor design.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Morales_G/0/1/0/all/0/1"&gt;Giorgio Morales&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sheppard_J/0/1/0/all/0/1"&gt;John Sheppard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Logan_R/0/1/0/all/0/1"&gt;Riley Logan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shaw_J/0/1/0/all/0/1"&gt;Joseph Shaw&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[To trust or not to trust an explanation: using LEAF to evaluate local linear XAI methods. (arXiv:2106.00461v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2106.00461</id>
        <link href="http://arxiv.org/abs/2106.00461"/>
        <updated>2021-06-02T02:50:00.824Z</updated>
        <summary type="html"><![CDATA[The main objective of eXplainable Artificial Intelligence (XAI) is to provide
effective explanations for black-box classifiers. The existing literature lists
many desirable properties for explanations to be useful, but there is no
consensus on how to quantitatively evaluate explanations in practice. Moreover,
explanations are typically used only to inspect black-box models, and the
proactive use of explanations as a decision support is generally overlooked.
Among the many approaches to XAI, a widely adopted paradigm is Local Linear
Explanations - with LIME and SHAP emerging as state-of-the-art methods. We show
that these methods are plagued by many defects including unstable explanations,
divergence of actual implementations from the promised theoretical properties,
and explanations for the wrong label. This highlights the need to have standard
and unbiased evaluation procedures for Local Linear Explanations in the XAI
field. In this paper we address the problem of identifying a clear and
unambiguous set of metrics for the evaluation of Local Linear Explanations.
This set includes both existing and novel metrics defined specifically for this
class of explanations. All metrics have been included in an open Python
framework, named LEAF. The purpose of LEAF is to provide a reference for end
users to evaluate explanations in a standardised and unbiased way, and to guide
researchers towards developing improved explainable techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Amparore_E/0/1/0/all/0/1"&gt;Elvio G. Amparore&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Perotti_A/0/1/0/all/0/1"&gt;Alan Perotti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bajardi_P/0/1/0/all/0/1"&gt;Paolo Bajardi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Comprehensive Validation of Automated Whole Body Skeletal Muscle, Adipose Tissue, and Bone Segmentation from 3D CT images for Body Composition Analysis: Towards Extended Body Composition. (arXiv:2106.00652v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.00652</id>
        <link href="http://arxiv.org/abs/2106.00652"/>
        <updated>2021-06-02T02:50:00.784Z</updated>
        <summary type="html"><![CDATA[The latest advances in computer-assisted precision medicine are making it
feasible to move from population-wide models that are useful to discover
aggregate patterns that hold for group-based analysis to patient-specific
models that can drive patient-specific decisions with regard to treatment
choices, and predictions of outcomes of treatment. Body Composition is
recognized as an important driver and risk factor for a wide variety of
diseases, as well as a predictor of individual patient-specific clinical
outcomes to treatment choices or surgical interventions. 3D CT images are
routinely acquired in the oncological worklows and deliver accurate rendering
of internal anatomy and therefore can be used opportunistically to assess the
amount of skeletal muscle and adipose tissue compartments. Powerful tools of
artificial intelligence such as deep learning are making it feasible now to
segment the entire 3D image and generate accurate measurements of all internal
anatomy. These will enable the overcoming of the severe bottleneck that existed
previously, namely, the need for manual segmentation, which was prohibitive to
scale to the hundreds of 2D axial slices that made up a 3D volumetric image.
Automated tools such as presented here will now enable harvesting whole-body
measurements from 3D CT or MRI images, leading to a new era of discovery of the
drivers of various diseases based on individual tissue, organ volume, shape,
and functional status. These measurements were hitherto unavailable thereby
limiting the field to a very small and limited subset. These discoveries and
the potential to perform individual image segmentation with high speed and
accuracy are likely to lead to the incorporation of these 3D measures into
individual specific treatment planning models related to nutrition, aging,
chemotoxicity, surgery and survival after the onset of a major disease such as
cancer.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ma_D/0/1/0/all/0/1"&gt;Da Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chow_V/0/1/0/all/0/1"&gt;Vincent Chow&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Popuri_K/0/1/0/all/0/1"&gt;Karteek Popuri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Beg_M/0/1/0/all/0/1"&gt;Mirza Faisal Beg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Novel Graph-Theoretic Deep Representation Learning Method for Multi-Label Remote Sensing Image Retrieval. (arXiv:2106.00506v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.00506</id>
        <link href="http://arxiv.org/abs/2106.00506"/>
        <updated>2021-06-02T02:50:00.657Z</updated>
        <summary type="html"><![CDATA[This paper presents a novel graph-theoretic deep representation learning
method in the framework of multi-label remote sensing (RS) image retrieval
problems. The proposed method aims to extract and exploit multi-label
co-occurrence relationships associated to each RS image in the archive. To this
end, each training image is initially represented with a graph structure that
provides region-based image representation combining both local information and
the related spatial organization. Unlike the other graph-based methods, the
proposed method contains a novel learning strategy to train a deep neural
network for automatically predicting a graph structure of each RS image in the
archive. This strategy employs a region representation learning loss function
to characterize the image content based on its multi-label co-occurrence
relationship. Experimental results show the effectiveness of the proposed
method for retrieval problems in RS compared to state-of-the-art deep
representation learning methods. The code of the proposed method is publicly
available at https://git.tu-berlin.de/rsim/GT-DRL-CBIR .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sumbul_G/0/1/0/all/0/1"&gt;Gencer Sumbul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Demir_B/0/1/0/all/0/1"&gt;Beg&amp;#xfc;m Demir&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hardness Sampling for Self-Training Based Transductive Zero-Shot Learning. (arXiv:2106.00264v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.00264</id>
        <link href="http://arxiv.org/abs/2106.00264"/>
        <updated>2021-06-02T02:50:00.571Z</updated>
        <summary type="html"><![CDATA[Transductive zero-shot learning (T-ZSL) which could alleviate the domain
shift problem in existing ZSL works, has received much attention recently.
However, an open problem in T-ZSL: how to effectively make use of unseen-class
samples for training, still remains. Addressing this problem, we first
empirically analyze the roles of unseen-class samples with different degrees of
hardness in the training process based on the uneven prediction phenomenon
found in many ZSL methods, resulting in three observations. Then, we propose
two hardness sampling approaches for selecting a subset of diverse and hard
samples from a given unseen-class dataset according to these observations. The
first one identifies the samples based on the class-level frequency of the
model predictions while the second enhances the former by normalizing the class
frequency via an approximate class prior estimated by an explored prior
estimation algorithm. Finally, we design a new Self-Training framework with
Hardness Sampling for T-ZSL, called STHS, where an arbitrary inductive ZSL
method could be seamlessly embedded and it is iteratively trained with
unseen-class samples selected by the hardness sampling approach. We introduce
two typical ZSL methods into the STHS framework and extensive experiments
demonstrate that the derived T-ZSL methods outperform many state-of-the-art
methods on three public benchmarks. Besides, we note that the unseen-class
dataset is separately used for training in some existing transductive
generalized ZSL (T-GZSL) methods, which is not strict for a GZSL task. Hence,
we suggest a more strict T-GZSL data setting and establish a competitive
baseline on this setting by introducing the proposed STHS framework to T-GZSL.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bo_L/0/1/0/all/0/1"&gt;Liu Bo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_Q/0/1/0/all/0/1"&gt;Qiulei Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1"&gt;Zhanyi Hu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[KVT: k-NN Attention for Boosting Vision Transformers. (arXiv:2106.00515v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.00515</id>
        <link href="http://arxiv.org/abs/2106.00515"/>
        <updated>2021-06-02T02:50:00.528Z</updated>
        <summary type="html"><![CDATA[Convolutional Neural Networks (CNNs) have dominated computer vision for
years, due to its ability in capturing locality and translation invariance.
Recently, many vision transformer architectures have been proposed and they
show promising performance. A key component in vision transformers is the
fully-connected self-attention which is more powerful than CNNs in modelling
long range dependencies. However, since the current dense self-attention uses
all image patches (tokens) to compute attention matrix, it may neglect locality
of images patches and involve noisy tokens (e.g., clutter background and
occlusion), leading to a slow training process and potentially degradation of
performance. To address these problems, we propose a sparse attention scheme,
dubbed k-NN attention, for boosting vision transformers. Specifically, instead
of involving all the tokens for attention matrix calculation, we only select
the top-k similar tokens from the keys for each query to compute the attention
map. The proposed k-NN attention naturally inherits the local bias of CNNs
without introducing convolutional operations, as nearby tokens tend to be more
similar than others. In addition, the k-NN attention allows for the exploration
of long range correlation and at the same time filter out irrelevant tokens by
choosing the most similar tokens from the entire image. Despite its simplicity,
we verify, both theoretically and empirically, that $k$-NN attention is
powerful in distilling noise from input tokens and in speeding up training.
Extensive experiments are conducted by using ten different vision transformer
architectures to verify that the proposed k-NN attention can work with any
existing transformer architectures to improve its prediction performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1"&gt;Pichao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xue Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1"&gt;Fan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1"&gt;Ming Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1"&gt;Shuning Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1"&gt;Wen Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Hao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_R/0/1/0/all/0/1"&gt;Rong Jin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarial VQA: A New Benchmark for Evaluating the Robustness of VQA Models. (arXiv:2106.00245v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.00245</id>
        <link href="http://arxiv.org/abs/2106.00245"/>
        <updated>2021-06-02T02:50:00.462Z</updated>
        <summary type="html"><![CDATA[With large-scale pre-training, the past two years have witnessed significant
performance boost on the Visual Question Answering (VQA) task. Though rapid
progresses have been made, it remains unclear whether these state-of-the-art
(SOTA) VQA models are robust when encountering test examples in the wild. To
study this, we introduce Adversarial VQA, a new large-scale VQA benchmark,
collected iteratively via an adversarial human-and-model-in-the-loop procedure.
Through this new benchmark, we present several interesting findings. (i)
Surprisingly, during dataset collection, we find that non-expert annotators can
successfully attack SOTA VQA models with relative ease. (ii) We test a variety
of SOTA VQA models on our new dataset to highlight their fragility, and find
that both large-scale pre-trained models and adversarial training methods can
only achieve far lower performance than what they can achieve on the standard
VQA v2 dataset. (iii) When considered as data augmentation, our dataset can be
used to improve the performance on other robust VQA benchmarks. (iv) We present
a detailed analysis of the dataset, providing valuable insights on the
challenges it brings to the community. We hope Adversarial VQA can serve as a
valuable benchmark that will be used by future work to test the robustness of
its developed VQA models. Our dataset is publicly available at
https://adversarialvqa. github.io/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Linjie Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1"&gt;Jie Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gan_Z/0/1/0/all/0/1"&gt;Zhe Gan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jingjing Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semi-Supervised Domain Generalization with Stochastic StyleMatch. (arXiv:2106.00592v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.00592</id>
        <link href="http://arxiv.org/abs/2106.00592"/>
        <updated>2021-06-02T02:50:00.450Z</updated>
        <summary type="html"><![CDATA[Most existing research on domain generalization assumes source data gathered
from multiple domains are fully annotated. However, in real-world applications,
we might have only a few labels available from each source domain due to high
annotation cost, along with abundant unlabeled data that are much easier to
obtain. In this work, we investigate semi-supervised domain generalization
(SSDG), a more realistic and practical setting. Our proposed approach,
StyleMatch, is inspired by FixMatch, a state-of-the-art semi-supervised
learning method based on pseudo-labeling, with several new ingredients tailored
to solve SSDG. Specifically, 1) to mitigate overfitting in the scarce labeled
source data while improving robustness against noisy pseudo labels, we
introduce stochastic modeling to the classifier's weights, seen as class
prototypes, with Gaussian distributions. 2) To enhance generalization under
domain shift, we upgrade FixMatch's two-view consistency learning paradigm
based on weak and strong augmentations to a multi-view version with style
augmentation as the third complementary view. To provide a comprehensive study
and evaluation, we establish two SSDG benchmarks, which cover a wide range of
strong baseline methods developed in relevant areas including domain
generalization and semi-supervised learning. Extensive experiments demonstrate
that StyleMatch achieves the best out-of-distribution generalization
performance in the low-data regime. We hope our approach and benchmarks can
pave the way for future research on data-efficient and generalizable learning
systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1"&gt;Kaiyang Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Loy_C/0/1/0/all/0/1"&gt;Chen Change Loy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Ziwei Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Mutual Learning for Semi-supervised Semantic Segmentation. (arXiv:2106.00609v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.00609</id>
        <link href="http://arxiv.org/abs/2106.00609"/>
        <updated>2021-06-02T02:50:00.442Z</updated>
        <summary type="html"><![CDATA[Recent semi-supervised learning (SSL) methods are commonly based on pseudo
labeling. Since the SSL performance is greatly influenced by the quality of
pseudo labels, mutual learning has been proposed to effectively suppress the
noises in the pseudo supervision. In this work, we propose robust mutual
learning that improves the prior approach in two aspects. First, the vanilla
mutual learners suffer from the coupling issue that models may converge to
learn homogeneous knowledge. We resolve this issue by introducing mean teachers
to generate mutual supervisions so that there is no direct interaction between
the two students. We also show that strong data augmentations, model noises and
heterogeneous network architectures are essential to alleviate the model
coupling. Second, we notice that mutual learning fails to leverage the
network's own ability for pseudo label refinement. Therefore, we introduce
self-rectification that leverages the internal knowledge and explicitly
rectifies the pseudo labels before the mutual teaching. Such self-rectification
and mutual teaching collaboratively improve the pseudo label accuracy
throughout the learning. The proposed robust mutual learning demonstrates
state-of-the-art performance on semantic segmentation in low-data regime.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1"&gt;Pan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1"&gt;Bo Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1"&gt;Ting Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1"&gt;Dong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_F/0/1/0/all/0/1"&gt;Fang Wen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Markpainting: Adversarial Machine Learning meets Inpainting. (arXiv:2106.00660v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.00660</id>
        <link href="http://arxiv.org/abs/2106.00660"/>
        <updated>2021-06-02T02:50:00.389Z</updated>
        <summary type="html"><![CDATA[Inpainting is a learned interpolation technique that is based on generative
modeling and used to populate masked or missing pieces in an image; it has wide
applications in picture editing and retouching. Recently, inpainting started
being used for watermark removal, raising concerns. In this paper we study how
to manipulate it using our markpainting technique. First, we show how an image
owner with access to an inpainting model can augment their image in such a way
that any attempt to edit it using that model will add arbitrary visible
information. We find that we can target multiple different models
simultaneously with our technique. This can be designed to reconstitute a
watermark if the editor had been trying to remove it. Second, we show that our
markpainting technique is transferable to models that have different
architectures or were trained on different datasets, so watermarks created
using it are difficult for adversaries to remove. Markpainting is novel and can
be used as a manipulation alarm that becomes visible in the event of
inpainting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khachaturov_D/0/1/0/all/0/1"&gt;David Khachaturov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shumailov_I/0/1/0/all/0/1"&gt;Ilia Shumailov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yiren Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Papernot_N/0/1/0/all/0/1"&gt;Nicolas Papernot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anderson_R/0/1/0/all/0/1"&gt;Ross Anderson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predicting Vehicles Trajectories in Urban Scenarios with Transformer Networks and Augmented Information. (arXiv:2106.00559v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.00559</id>
        <link href="http://arxiv.org/abs/2106.00559"/>
        <updated>2021-06-02T02:50:00.381Z</updated>
        <summary type="html"><![CDATA[Understanding the behavior of road users is of vital importance for the
development of trajectory prediction systems. In this context, the latest
advances have focused on recurrent structures, establishing the social
interaction between the agents involved in the scene. More recently, simpler
structures have also been introduced for predicting pedestrian trajectories,
based on Transformer Networks, and using positional information. They allow the
individual modelling of each agent's trajectory separately without any complex
interaction terms. Our model exploits these simple structures by adding
augmented data (position and heading), and adapting their use to the problem of
vehicle trajectory prediction in urban scenarios in prediction horizons up to 5
seconds. In addition, a cross-performance analysis is performed between
different types of scenarios, including highways, intersections and
roundabouts, using recent datasets (inD, rounD, highD and INTERACTION). Our
model achieves state-of-the-art results and proves to be flexible and adaptable
to different types of urban contexts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Quintanar_A/0/1/0/all/0/1"&gt;A. Quintanar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fernandez_Llorca_D/0/1/0/all/0/1"&gt;D. Fern&amp;#xe1;ndez-Llorca&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parra_I/0/1/0/all/0/1"&gt;I. Parra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Izquierdo_R/0/1/0/all/0/1"&gt;R. Izquierdo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sotelo_M/0/1/0/all/0/1"&gt;M. A. Sotelo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Independent Prototype Propagation for Zero-Shot Compositionality. (arXiv:2106.00305v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.00305</id>
        <link href="http://arxiv.org/abs/2106.00305"/>
        <updated>2021-06-02T02:50:00.373Z</updated>
        <summary type="html"><![CDATA[Humans are good at compositional zero-shot reasoning; someone who has never
seen a zebra before could nevertheless recognize one when we tell them it looks
like a horse with black and white stripes. Machine learning systems, on the
other hand, usually leverage spurious correlations in the training data, and
while such correlations can help recognize objects in context, they hurt
generalization. To be able to deal with underspecified datasets while still
leveraging contextual clues during classification, we propose ProtoProp, a
novel prototype propagation graph method. First we learn prototypical
representations of objects (e.g., zebra) that are conditionally independent
w.r.t. their attribute labels (e.g., stripes) and vice versa. Next we propagate
the independent prototypes through a compositional graph, to learn
compositional prototypes of novel attribute-object combinations that reflect
the dependencies of the target distribution. The method does not rely on any
external data, such as class hierarchy graphs or pretrained word embeddings. We
evaluate our approach on AO-Clever, a synthetic and strongly visual dataset
with clean labels, and UT-Zappos, a noisy real-world dataset of fine-grained
shoe types. We show that in the generalized compositional zero-shot setting we
outperform state-of-the-art results, and through ablations we show the
importance of each part of the method and their contribution to the final
results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ruis_F/0/1/0/all/0/1"&gt;Frank Ruis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Burghours_G/0/1/0/all/0/1"&gt;Gertjan Burghours&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bucur_D/0/1/0/all/0/1"&gt;Doina Bucur&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TAT-QA: A Question Answering Benchmark on a Hybrid of Tabular and Textual Content in Finance. (arXiv:2105.07624v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.07624</id>
        <link href="http://arxiv.org/abs/2105.07624"/>
        <updated>2021-06-02T02:50:00.366Z</updated>
        <summary type="html"><![CDATA[Hybrid data combining both tabular and textual content (e.g., financial
reports) are quite pervasive in the real world. However, Question Answering
(QA) over such hybrid data is largely neglected in existing research. In this
work, we extract samples from real financial reports to build a new large-scale
QA dataset containing both Tabular And Textual data, named TAT-QA, where
numerical reasoning is usually required to infer the answer, such as addition,
subtraction, multiplication, division, counting, comparison/sorting, and the
compositions. We further propose a novel QA model termed TAGOP, which is
capable of reasoning over both tables and text. It adopts sequence tagging to
extract relevant cells from the table along with relevant spans from the text
to infer their semantics, and then applies symbolic reasoning over them with a
set of aggregation operators to arrive at the final answer. TAGOPachieves 58.0%
inF1, which is an 11.1% absolute increase over the previous best baseline
model, according to our experiments on TAT-QA. But this result still lags far
behind performance of expert human, i.e.90.8% in F1. It is demonstrated that
our TAT-QA is very challenging and can serve as a benchmark for training and
testing powerful QA models that address hybrid form data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1"&gt;Fengbin Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lei_W/0/1/0/all/0/1"&gt;Wenqiang Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Youcheng Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shuo Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lv_J/0/1/0/all/0/1"&gt;Jiancheng Lv&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_F/0/1/0/all/0/1"&gt;Fuli Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1"&gt;Tat-Seng Chua&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quantifying Predictive Uncertainty in Medical Image Analysis with Deep Kernel Learning. (arXiv:2106.00638v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.00638</id>
        <link href="http://arxiv.org/abs/2106.00638"/>
        <updated>2021-06-02T02:50:00.360Z</updated>
        <summary type="html"><![CDATA[Deep neural networks are increasingly being used for the analysis of medical
images. However, most works neglect the uncertainty in the model's prediction.
We propose an uncertainty-aware deep kernel learning model which permits the
estimation of the uncertainty in the prediction by a pipeline of a
Convolutional Neural Network and a sparse Gaussian Process. Furthermore, we
adapt different pre-training methods to investigate their impacts on the
proposed model. We apply our approach to Bone Age Prediction and Lesion
Localization. In most cases, the proposed model shows better performance
compared to common architectures. More importantly, our model expresses
systematically higher confidence in more accurate predictions and less
confidence in less accurate ones. Our model can also be used to detect
challenging and controversial test samples. Compared to related methods such as
Monte-Carlo Dropout, our approach derives the uncertainty information in a
purely analytical fashion and is thus computationally more efficient.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Zhiliang Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yinchong Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1"&gt;Jindong Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tresp_V/0/1/0/all/0/1"&gt;Volker Tresp&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring the Diversity and Invariance in Yourself for Visual Pre-Training Task. (arXiv:2106.00537v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.00537</id>
        <link href="http://arxiv.org/abs/2106.00537"/>
        <updated>2021-06-02T02:50:00.342Z</updated>
        <summary type="html"><![CDATA[Recently, self-supervised learning methods have achieved remarkable success
in visual pre-training task. By simply pulling the different augmented views of
each image together or other novel mechanisms, they can learn much unsupervised
knowledge and significantly improve the transfer performance of pre-training
models. However, these works still cannot avoid the representation collapse
problem, i.e., they only focus on limited regions or the extracted features on
totally different regions inside each image are nearly the same. Generally,
this problem makes the pre-training models cannot sufficiently describe the
multi-grained information inside images, which further limits the upper bound
of their transfer performance. To alleviate this issue, this paper introduces a
simple but effective mechanism, called Exploring the Diversity and Invariance
in Yourself E-DIY. By simply pushing the most different regions inside each
augmented view away, E-DIY can preserve the diversity of extracted region-level
features. By pulling the most similar regions from different augmented views of
the same image together, E-DIY can ensure the robustness of region-level
features. Benefited from the above diversity and invariance exploring
mechanism, E-DIY maximally extracts the multi-grained visual information inside
each image. Extensive experiments on downstream tasks demonstrate the
superiority of our proposed approach, e.g., there are 2.1% improvements
compared with the strong baseline BYOL on COCO while fine-tuning Mask R-CNN
with the R50-C4 backbone and 1X learning schedule.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wei_L/0/1/0/all/0/1"&gt;Longhui Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1"&gt;Lingxi Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1"&gt;Wengang Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Houqiang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1"&gt;Qi Tian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[3D WaveUNet: 3D Wavelet Integrated Encoder-Decoder Network for Neuron Segmentation. (arXiv:2106.00259v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.00259</id>
        <link href="http://arxiv.org/abs/2106.00259"/>
        <updated>2021-06-02T02:50:00.335Z</updated>
        <summary type="html"><![CDATA[3D neuron segmentation is a key step for the neuron digital reconstruction,
which is essential for exploring brain circuits and understanding brain
functions. However, the fine line-shaped nerve fibers of neuron could spread in
a large region, which brings great computational cost to the segmentation in 3D
neuronal images. Meanwhile, the strong noises and disconnected nerve fibers in
the image bring great challenges to the task. In this paper, we propose a 3D
wavelet and deep learning based 3D neuron segmentation method. The neuronal
image is first partitioned into neuronal cubes to simplify the segmentation
task. Then, we design 3D WaveUNet, the first 3D wavelet integrated
encoder-decoder network, to segment the nerve fibers in the cubes; the wavelets
could assist the deep networks in suppressing data noise and connecting the
broken fibers. We also produce a Neuronal Cube Dataset (NeuCuDa) using the
biggest available annotated neuronal image dataset, BigNeuron, to train 3D
WaveUNet. Finally, the nerve fibers segmented in cubes are assembled to
generate the complete neuron, which is digitally reconstructed using an
available automatic tracing algorithm. The experimental results show that our
neuron segmentation method could completely extract the target neuron in noisy
neuronal images. The integrated 3D wavelets can efficiently improve the
performance of 3D neuron segmentation and reconstruction. The code and
pre-trained models for this work will be available at
https://github.com/LiQiufu/3D-WaveUNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Li_Q/0/1/0/all/0/1"&gt;Qiufu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shen_L/0/1/0/all/0/1"&gt;Linlin Shen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PanoDR: Spherical Panorama Diminished Reality for Indoor Scenes. (arXiv:2106.00446v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.00446</id>
        <link href="http://arxiv.org/abs/2106.00446"/>
        <updated>2021-06-02T02:50:00.329Z</updated>
        <summary type="html"><![CDATA[The rising availability of commercial $360^\circ$ cameras that democratize
indoor scanning, has increased the interest for novel applications, such as
interior space re-design. Diminished Reality (DR) fulfills the requirement of
such applications, to remove existing objects in the scene, essentially
translating this to a counterfactual inpainting task. While recent advances in
data-driven inpainting have shown significant progress in generating realistic
samples, they are not constrained to produce results with reality mapped
structures. To preserve the `reality' in indoor (re-)planning applications, the
scene's structure preservation is crucial. To ensure structure-aware
counterfactual inpainting, we propose a model that initially predicts the
structure of an indoor scene and then uses it to guide the reconstruction of an
empty -- background only -- representation of the same scene. We train and
compare against other state-of-the-art methods on a version of the Structured3D
dataset modified for DR, showing superior results in both quantitative metrics
and qualitative results, but more interestingly, our approach exhibits a much
faster convergence rate. Code and models are available at
https://vcl3d.github.io/PanoDR/ .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gkitsas_V/0/1/0/all/0/1"&gt;V. Gkitsas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sterzentsenko_V/0/1/0/all/0/1"&gt;V. Sterzentsenko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zioulis_N/0/1/0/all/0/1"&gt;N. Zioulis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Albanis_G/0/1/0/all/0/1"&gt;G. Albanis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zarpalas_D/0/1/0/all/0/1"&gt;D. Zarpalas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dense Nested Attention Network for Infrared Small Target Detection. (arXiv:2106.00487v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.00487</id>
        <link href="http://arxiv.org/abs/2106.00487"/>
        <updated>2021-06-02T02:50:00.323Z</updated>
        <summary type="html"><![CDATA[Single-frame infrared small target (SIRST) detection aims at separating small
targets from clutter backgrounds. With the advances of deep learning, CNN-based
methods have yielded promising results in generic object detection due to their
powerful modeling capability. However, existing CNN-based methods cannot be
directly applied for infrared small targets since pooling layers in their
networks could lead to the loss of targets in deep layers. To handle this
problem, we propose a dense nested attention network (DNANet) in this paper.
Specifically, we design a dense nested interactive module (DNIM) to achieve
progressive interaction among high-level and low-level features. With the
repeated interaction in DNIM, infrared small targets in deep layers can be
maintained. Based on DNIM, we further propose a cascaded channel and spatial
attention module (CSAM) to adaptively enhance multi-level features. With our
DNANet, contextual information of small targets can be well incorporated and
fully exploited by repeated fusion and enhancement. Moreover, we develop an
infrared small target dataset (namely, NUDT-SIRST) and propose a set of
evaluation metrics to conduct comprehensive performance evaluation. Experiments
on both public and our self-developed datasets demonstrate the effectiveness of
our method. Compared to other state-of-the-art methods, our method achieves
better performance in terms of probability of detection (Pd), false-alarm rate
(Fa), and intersection of union (IoU).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Boyang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1"&gt;Chao Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Longguang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yingqian Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1"&gt;Zaiping Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1"&gt;Miao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+An_W/0/1/0/all/0/1"&gt;Wei An&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1"&gt;Yulan Guo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Detecting Anomalies in Semantic Segmentation with Prototypes. (arXiv:2106.00472v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.00472</id>
        <link href="http://arxiv.org/abs/2106.00472"/>
        <updated>2021-06-02T02:50:00.316Z</updated>
        <summary type="html"><![CDATA[Traditional semantic segmentation methods can recognize at test time only the
classes that are present in the training set. This is a significant limitation,
especially for semantic segmentation algorithms mounted on intelligent
autonomous systems, deployed in realistic settings. Regardless of how many
classes the system has seen at training time, it is inevitable that unexpected,
unknown objects will appear at test time. The failure in identifying such
anomalies may lead to incorrect, even dangerous behaviors of the autonomous
agent equipped with such segmentation model when deployed in the real world.
Current state of the art of anomaly segmentation uses generative models,
exploiting their incapability to reconstruct patterns unseen during training.
However, training these models is expensive, and their generated artifacts may
create false anomalies. In this paper we take a different route and we propose
to address anomaly segmentation through prototype learning. Our intuition is
that anomalous pixels are those that are dissimilar to all class prototypes
known by the model. We extract class prototypes from the training data in a
lightweight manner using a cosine similarity-based classifier. Experiments on
StreetHazards show that our approach achieves the new state of the art, with a
significant margin over previous works, despite the reduced computational
overhead. Code is available at https://github.com/DarioFontanel/PAnS.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fontanel_D/0/1/0/all/0/1"&gt;Dario Fontanel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cermelli_F/0/1/0/all/0/1"&gt;Fabio Cermelli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mancini_M/0/1/0/all/0/1"&gt;Massimiliano Mancini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Caputo_B/0/1/0/all/0/1"&gt;Barbara Caputo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Highlight Timestamp Detection Model for Comedy Videos via Multimodal Sentiment Analysis. (arXiv:2106.00451v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.00451</id>
        <link href="http://arxiv.org/abs/2106.00451"/>
        <updated>2021-06-02T02:50:00.295Z</updated>
        <summary type="html"><![CDATA[Nowadays, the videos on the Internet are prevailing. The precise and in-depth
understanding of the videos is a difficult but valuable problem for both
platforms and researchers. The existing video understand models do well in
object recognition tasks but currently still cannot understand the abstract and
contextual features like highlight humor frames in comedy videos. The current
industrial works are also mainly focused on the basic category classification
task based on the appearances of objects. The feature detection methods for the
abstract category remains blank. A data structure that includes the information
of video frames, audio spectrum and texts provide a new direction to explore.
The multimodal models are proposed to make this in-depth video understanding
mission possible. In this paper, we analyze the difficulties in abstract
understanding of videos and propose a multimodal structure to obtain
state-of-the-art performance in this field. Then we select several benchmarks
for multimodal video understanding and apply the most suitable model to find
the best performance. At last, we evaluate the overall spotlights and drawbacks
of the models and methods in this paper and point out the possible directions
for further improvements.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1"&gt;Fan Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Markov Localisation using Heatmap Regression and Deep Convolutional Odometry. (arXiv:2106.00371v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2106.00371</id>
        <link href="http://arxiv.org/abs/2106.00371"/>
        <updated>2021-06-02T02:50:00.289Z</updated>
        <summary type="html"><![CDATA[In the context of self-driving vehicles there is strong competition between
approaches based on visual localisation and LiDAR. While LiDAR provides
important depth information, it is sparse in resolution and expensive. On the
other hand, cameras are low-cost and recent developments in deep learning mean
they can provide high localisation performance. However, several fundamental
problems remain, particularly in the domain of uncertainty, where learning
based approaches can be notoriously over-confident.

Markov, or grid-based, localisation was an early solution to the localisation
problem but fell out of favour due to its computational complexity.
Representing the likelihood field as a grid (or volume) means there is a trade
off between accuracy and memory size. Furthermore, it is necessary to perform
expensive convolutions across the entire likelihood volume. Despite the benefit
of simultaneously maintaining a likelihood for all possible locations, grid
based approaches were superseded by more efficient particle filters and Monte
Carlo Localisation (MCL). However, MCL introduces its own problems e.g.
particle deprivation.

Recent advances in deep learning hardware allow large likelihood volumes to
be stored directly on the GPU, along with the hardware necessary to efficiently
perform GPU-bound 3D convolutions and this obviates many of the disadvantages
of grid based methods. In this work, we present a novel CNN-based localisation
approach that can leverage modern deep learning hardware. By implementing a
grid-based Markov localisation approach directly on the GPU, we create a hybrid
CNN that can perform image-based localisation and odometry-based likelihood
propagation within a single neural network. The resulting approach is capable
of outperforming direct pose regression methods as well as state-of-the-art
localisation systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mendez_O/0/1/0/all/0/1"&gt;Oscar Mendez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hadfield_S/0/1/0/all/0/1"&gt;Simon Hadfield&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bowden_R/0/1/0/all/0/1"&gt;Richard Bowden&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Gender Bias Hidden Behind Chinese Word Embeddings: The Case of Chinese Adjectives. (arXiv:2106.00181v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.00181</id>
        <link href="http://arxiv.org/abs/2106.00181"/>
        <updated>2021-06-02T02:50:00.282Z</updated>
        <summary type="html"><![CDATA[Gender bias in word embeddings gradually becomes a vivid research field in
recent years. Most studies in this field aim at measurement and debiasing
methods with English as the target language. This paper investigates gender
bias in static word embeddings from a unique perspective, Chinese adjectives.
By training word representations with different models, the gender bias behind
the vectors of adjectives is assessed. Through a comparison between the
produced results and a human-scored data set, we demonstrate how gender bias
encoded in word embeddings differentiates from people's attitudes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiao_M/0/1/0/all/0/1"&gt;Meichun Jiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1"&gt;Ziyang Luo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Anti-aliasing Semantic Reconstruction for Few-Shot Semantic Segmentation. (arXiv:2106.00184v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.00184</id>
        <link href="http://arxiv.org/abs/2106.00184"/>
        <updated>2021-06-02T02:50:00.276Z</updated>
        <summary type="html"><![CDATA[Encouraging progress in few-shot semantic segmentation has been made by
leveraging features learned upon base classes with sufficient training data to
represent novel classes with few-shot examples. However, this feature sharing
mechanism inevitably causes semantic aliasing between novel classes when they
have similar compositions of semantic concepts. In this paper, we reformulate
few-shot segmentation as a semantic reconstruction problem, and convert base
class features into a series of basis vectors which span a class-level semantic
space for novel class reconstruction. By introducing contrastive loss, we
maximize the orthogonality of basis vectors while minimizing semantic aliasing
between classes. Within the reconstructed representation space, we further
suppress interference from other classes by projecting query features to the
support vector for precise semantic activation. Our proposed approach, referred
to as anti-aliasing semantic reconstruction (ASR), provides a systematic yet
interpretable solution for few-shot learning problems. Extensive experiments on
PASCAL VOC and MS COCO datasets show that ASR achieves strong results compared
with the prior works.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1"&gt;Binghao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1"&gt;Yao Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiao_J/0/1/0/all/0/1"&gt;Jianbin Jiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_X/0/1/0/all/0/1"&gt;Xiangyang Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1"&gt;Qixiang Ye&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RAI-Net: Range-Adaptive LiDAR Point Cloud Frame Interpolation Network. (arXiv:2106.00496v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.00496</id>
        <link href="http://arxiv.org/abs/2106.00496"/>
        <updated>2021-06-02T02:50:00.270Z</updated>
        <summary type="html"><![CDATA[LiDAR point cloud frame interpolation, which synthesizes the intermediate
frame between the captured frames, has emerged as an important issue for many
applications. Especially for reducing the amounts of point cloud transmission,
it is by predicting the intermediate frame based on the reference frames to
upsample data to high frame rate ones. However, due to high-dimensional and
sparse characteristics of point clouds, it is more difficult to predict the
intermediate frame for LiDAR point clouds than videos. In this paper, we
propose a novel LiDAR point cloud frame interpolation method, which exploits
range images (RIs) as an intermediate representation with CNNs to conduct the
frame interpolation process. Considering the inherited characteristics of RIs
differ from that of color images, we introduce spatially adaptive convolutions
to extract range features adaptively, while a high-efficient flow estimation
method is presented to generate optical flows. The proposed model then warps
the input frames and range features, based on the optical flows to synthesize
the interpolated frame. Extensive experiments on the KITTI dataset have clearly
demonstrated that our method consistently achieves superior frame interpolation
results with better perceptual quality to that of using state-of-the-art video
frame interpolation methods. The proposed method could be integrated into any
LiDAR point cloud compression systems for inter prediction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zhao_L/0/1/0/all/0/1"&gt;Lili Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhu_Z/0/1/0/all/0/1"&gt;Zezhi Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lin_X/0/1/0/all/0/1"&gt;Xuhu Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Guo_X/0/1/0/all/0/1"&gt;Xuezhou Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yin_Q/0/1/0/all/0/1"&gt;Qian Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wenyi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jianwen Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What's in the Box? A Preliminary Analysis of Undesirable Content in the Common Crawl Corpus. (arXiv:2105.02732v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.02732</id>
        <link href="http://arxiv.org/abs/2105.02732"/>
        <updated>2021-06-02T02:50:00.252Z</updated>
        <summary type="html"><![CDATA[Whereas much of the success of the current generation of neural language
models has been driven by increasingly large training corpora, relatively
little research has been dedicated to analyzing these massive sources of
textual data. In this exploratory analysis, we delve deeper into the Common
Crawl, a colossal web corpus that is extensively used for training language
models. We find that it contains a significant amount of undesirable content,
including hate speech and sexually explicit content, even after filtering
procedures. We discuss the potential impacts of this content on language models
and conclude with future research directions and a more mindful approach to
corpus collection and analysis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Luccioni_A/0/1/0/all/0/1"&gt;Alexandra Sasha Luccioni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Viviano_J/0/1/0/all/0/1"&gt;Joseph D. Viviano&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Football Body-Orientation as a Matter of Classification. (arXiv:2106.00359v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.00359</id>
        <link href="http://arxiv.org/abs/2106.00359"/>
        <updated>2021-06-02T02:50:00.246Z</updated>
        <summary type="html"><![CDATA[Orientation is a crucial skill for football players that becomes a
differential factor in a large set of events, especially the ones involving
passes. However, existing orientation estimation methods, which are based on
computer-vision techniques, still have a lot of room for improvement. To the
best of our knowledge, this article presents the first deep learning model for
estimating orientation directly from video footage. By approaching this
challenge as a classification problem where classes correspond to orientation
bins, and by introducing a cyclic loss function, a well-known convolutional
network is refined to provide player orientation data. The model is trained by
using ground-truth orientation data obtained from wearable EPTS devices, which
are individually compensated with respect to the perceived orientation in the
current frame. The obtained results outperform previous methods; in particular,
the absolute median error is less than 12 degrees per player. An ablation study
is included in order to show the potential generalization to any kind of
football video footage.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Arbues_Sanguesa_A/0/1/0/all/0/1"&gt;Adri&amp;#xe0; Arbu&amp;#xe9;s-Sang&amp;#xfc;esa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martin_A/0/1/0/all/0/1"&gt;Adri&amp;#xe1;n Mart&amp;#xed;n&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Granero_P/0/1/0/all/0/1"&gt;Paulino Granero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ballester_C/0/1/0/all/0/1"&gt;Coloma Ballester&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Haro_G/0/1/0/all/0/1"&gt;Gloria Haro&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[COV-ECGNET: COVID-19 detection using ECG trace images with deep convolutional neural network. (arXiv:2106.00436v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.00436</id>
        <link href="http://arxiv.org/abs/2106.00436"/>
        <updated>2021-06-02T02:50:00.240Z</updated>
        <summary type="html"><![CDATA[The reliable and rapid identification of the COVID-19 has become crucial to
prevent the rapid spread of the disease, ease lockdown restrictions and reduce
pressure on public health infrastructures. Recently, several methods and
techniques have been proposed to detect the SARS-CoV-2 virus using different
images and data. However, this is the first study that will explore the
possibility of using deep convolutional neural network (CNN) models to detect
COVID-19 from electrocardiogram (ECG) trace images. In this work, COVID-19 and
other cardiovascular diseases (CVDs) were detected using deep-learning
techniques. A public dataset of ECG images consists of 1937 images from five
distinct categories, such as Normal, COVID-19, myocardial infarction (MI),
abnormal heartbeat (AHB), and recovered myocardial infarction (RMI) were used
in this study. Six different deep CNN models (ResNet18, ResNet50, ResNet101,
InceptionV3, DenseNet201, and MobileNetv2) were used to investigate three
different classification schemes: two-class classification (Normal vs
COVID-19); three-class classification (Normal, COVID-19, and Other CVDs), and
finally, five-class classification (Normal, COVID-19, MI, AHB, and RMI). For
two-class and three-class classification, Densenet201 outperforms other
networks with an accuracy of 99.1%, and 97.36%, respectively; while for the
five-class classification, InceptionV3 outperforms others with an accuracy of
97.83%. ScoreCAM visualization confirms that the networks are learning from the
relevant area of the trace images. Since the proposed method uses ECG trace
images which can be captured by smartphones and are readily available
facilities in low-resources countries, this study will help in faster
computer-aided diagnosis of COVID-19 and other cardiac abnormalities.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Rahman_T/0/1/0/all/0/1"&gt;Tawsifur Rahman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Akinbi_A/0/1/0/all/0/1"&gt;Alex Akinbi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chowdhury_M/0/1/0/all/0/1"&gt;Muhammad E. H. Chowdhury&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rashid_T/0/1/0/all/0/1"&gt;Tarik A. Rashid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sengur_A/0/1/0/all/0/1"&gt;Abdulkadir &amp;#x15e;eng&amp;#xfc;r&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Khandakar_A/0/1/0/all/0/1"&gt;Amith Khandakar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Islam_K/0/1/0/all/0/1"&gt;Khandaker Reajul Islam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ismael_A/0/1/0/all/0/1"&gt;Aras M. Ismael&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Structural Knowledge Distillation: Tractably Distilling Information for Structured Predictor. (arXiv:2010.05010v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.05010</id>
        <link href="http://arxiv.org/abs/2010.05010"/>
        <updated>2021-06-02T02:50:00.231Z</updated>
        <summary type="html"><![CDATA[Knowledge distillation is a critical technique to transfer knowledge between
models, typically from a large model (the teacher) to a more fine-grained one
(the student). The objective function of knowledge distillation is typically
the cross-entropy between the teacher and the student's output distributions.
However, for structured prediction problems, the output space is exponential in
size; therefore, the cross-entropy objective becomes intractable to compute and
optimize directly. In this paper, we derive a factorized form of the knowledge
distillation objective for structured prediction, which is tractable for many
typical choices of the teacher and student models. In particular, we show the
tractability and empirical effectiveness of structural knowledge distillation
between sequence labeling and dependency parsing models under four different
scenarios: 1) the teacher and student share the same factorization form of the
output structure scoring function; 2) the student factorization produces more
fine-grained substructures than the teacher factorization; 3) the teacher
factorization produces more fine-grained substructures than the student
factorization; 4) the factorization forms from the teacher and the student are
incompatible.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xinyu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1"&gt;Yong Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1"&gt;Zhaohui Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_Z/0/1/0/all/0/1"&gt;Zixia Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bach_N/0/1/0/all/0/1"&gt;Nguyen Bach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1"&gt;Tao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zhongqiang Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1"&gt;Fei Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tu_K/0/1/0/all/0/1"&gt;Kewei Tu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Efficient Cross-Modal Visual Textual Retrieval using Transformer-Encoder Deep Features. (arXiv:2106.00358v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.00358</id>
        <link href="http://arxiv.org/abs/2106.00358"/>
        <updated>2021-06-02T02:50:00.223Z</updated>
        <summary type="html"><![CDATA[Cross-modal retrieval is an important functionality in modern search engines,
as it increases the user experience by allowing queries and retrieved objects
to pertain to different modalities. In this paper, we focus on the
image-sentence retrieval task, where the objective is to efficiently find
relevant images for a given sentence (image-retrieval) or the relevant
sentences for a given image (sentence-retrieval). Computer vision literature
reports the best results on the image-sentence matching task using deep neural
networks equipped with attention and self-attention mechanisms. They evaluate
the matching performance on the retrieval task by performing sequential scans
of the whole dataset. This method does not scale well with an increasing amount
of images or captions. In this work, we explore different preprocessing
techniques to produce sparsified deep multi-modal features extracting them from
state-of-the-art deep-learning architectures for image-text matching. Our main
objective is to lay down the paths for efficient indexing of complex
multi-modal descriptions. We use the recently introduced TERN architecture as
an image-sentence features extractor. It is designed for producing fixed-size
1024-d vectors describing whole images and sentences, as well as
variable-length sets of 1024-d vectors describing the various building
components of the two modalities (image regions and sentence words
respectively). All these vectors are enforced by the TERN design to lie into
the same common space. Our experiments show interesting preliminary results on
the explored methods and suggest further experimentation in this important
research direction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Messina_N/0/1/0/all/0/1"&gt;Nicola Messina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Amato_G/0/1/0/all/0/1"&gt;Giuseppe Amato&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Falchi_F/0/1/0/all/0/1"&gt;Fabrizio Falchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gennaro_C/0/1/0/all/0/1"&gt;Claudio Gennaro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marchand_Maillet_S/0/1/0/all/0/1"&gt;St&amp;#xe9;phane Marchand-Maillet&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Recent Advances in Deep Learning Based Dialogue Systems: A Systematic Survey. (arXiv:2105.04387v4 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.04387</id>
        <link href="http://arxiv.org/abs/2105.04387"/>
        <updated>2021-06-02T02:50:00.216Z</updated>
        <summary type="html"><![CDATA[Dialogue systems are a popular Natural Language Processing (NLP) task as it
is promising in real-life applications. It is also a complicated task since
many NLP tasks deserving study are involved. As a result, a multitude of novel
works on this task are carried out, and most of them are deep learning-based
due to the outstanding performance. In this survey, we mainly focus on the deep
learning-based dialogue systems. We comprehensively review state-of-the-art
research outcomes in dialogue systems and analyze them from two angles: model
type and system type. Specifically, from the angle of model type, we discuss
the principles, characteristics, and applications of different models that are
widely used in dialogue systems. This will help researchers acquaint these
models and see how they are applied in state-of-the-art frameworks, which is
rather helpful when designing a new dialogue system. From the angle of system
type, we discuss task-oriented and open-domain dialogue systems as two streams
of research, providing insight into the hot topics related. Furthermore, we
comprehensively review the evaluation methods and datasets for dialogue systems
to pave the way for future research. Finally, some possible research trends are
identified based on the recent research outcomes. To the best of our knowledge,
this survey is the most comprehensive and up-to-date one at present in the area
of dialogue systems and dialogue-related tasks, extensively covering the
popular frameworks, topics, and datasets.

Keywords: Dialogue Systems, Chatbots, Conversational AI, Task-oriented, Open
Domain, Chit-chat, Question Answering, Artificial Intelligence, Natural
Language Processing, Information Retrieval, Deep Learning, Neural Networks,
CNN, RNN, Hierarchical Recurrent Encoder-Decoder, Memory Networks, Attention,
Transformer, Pointer Net, CopyNet, Reinforcement Learning, GANs, Knowledge
Graph, Survey, Review]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ni_J/0/1/0/all/0/1"&gt;Jinjie Ni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Young_T/0/1/0/all/0/1"&gt;Tom Young&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pandelea_V/0/1/0/all/0/1"&gt;Vlad Pandelea&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xue_F/0/1/0/all/0/1"&gt;Fuzhao Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Adiga_V/0/1/0/all/0/1"&gt;Vinay Adiga&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cambria_E/0/1/0/all/0/1"&gt;Erik Cambria&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Language-Driven Image Style Transfer. (arXiv:2106.00178v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.00178</id>
        <link href="http://arxiv.org/abs/2106.00178"/>
        <updated>2021-06-02T02:50:00.198Z</updated>
        <summary type="html"><![CDATA[Despite having promising results, style transfer, which requires preparing
style images in advance, may result in lack of creativity and accessibility.
Following human instruction, on the other hand, is the most natural way to
perform artistic style transfer that can significantly improve controllability
for visual effect applications. We introduce a new task -- language-driven
image style transfer (\texttt{LDIST}) -- to manipulate the style of a content
image, guided by a text. We propose contrastive language visual artist (CLVA)
that learns to extract visual semantics from style instructions and accomplish
\texttt{LDIST} by the patch-wise style discriminator. The discriminator
considers the correlation between language and patches of style images or
transferred results to jointly embed style instructions. CLVA further compares
contrastive pairs of content image and style instruction to improve the mutual
relativeness between transfer results. The transferred results from the same
content image can preserve consistent content structures. Besides, they should
present analogous style patterns from style instructions that contain similar
visual semantics. The experiments show that our CLVA is effective and achieves
superb transferred results on \texttt{LDIST}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fu_T/0/1/0/all/0/1"&gt;Tsu-Jui Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xin Eric Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;William Yang Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GANs Can Play Lottery Tickets Too. (arXiv:2106.00134v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.00134</id>
        <link href="http://arxiv.org/abs/2106.00134"/>
        <updated>2021-06-02T02:50:00.192Z</updated>
        <summary type="html"><![CDATA[Deep generative adversarial networks (GANs) have gained growing popularity in
numerous scenarios, while usually suffer from high parameter complexities for
resource-constrained real-world applications. However, the compression of GANs
has less been explored. A few works show that heuristically applying
compression techniques normally leads to unsatisfactory results, due to the
notorious training instability of GANs. In parallel, the lottery ticket
hypothesis shows prevailing success on discriminative models, in locating
sparse matching subnetworks capable of training in isolation to full model
performance. In this work, we for the first time study the existence of such
trainable matching subnetworks in deep GANs. For a range of GANs, we certainly
find matching subnetworks at 67%-74% sparsity. We observe that with or without
pruning discriminator has a minor effect on the existence and quality of
matching subnetworks, while the initialization weights used in the
discriminator play a significant role. We then show the powerful
transferability of these subnetworks to unseen tasks. Furthermore, extensive
experimental results demonstrate that our found subnetworks substantially
outperform previous state-of-the-art GAN compression approaches in both image
generation (e.g. SNGAN) and image-to-image translation GANs (e.g. CycleGAN).
Codes available at https://github.com/VITA-Group/GAN-LTH.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xuxi Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhenyu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sui_Y/0/1/0/all/0/1"&gt;Yongduo Sui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1"&gt;Tianlong Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ViTA: Visual-Linguistic Translation by Aligning Object Tags. (arXiv:2106.00250v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.00250</id>
        <link href="http://arxiv.org/abs/2106.00250"/>
        <updated>2021-06-02T02:50:00.186Z</updated>
        <summary type="html"><![CDATA[Multimodal Machine Translation (MMT) enriches the source text with visual
information for translation. It has gained popularity in recent years, and
several pipelines have been proposed in the same direction. Yet, the task lacks
quality datasets to illustrate the contribution of visual modality in the
translation systems. In this paper, we propose our system for the Multimodal
Translation Task of WAT 2021 from English to Hindi. We propose to use mBART, a
pretrained multilingual sequence-to-sequence model, for the textual-only
translations. Further, we bring the visual information to a textual domain by
extracting object tags from the image and enhance the input for the multimodal
task. We also explore the robustness of our system by systematically degrading
the source text. Finally, we achieve a BLEU score of 44.6 and 51.6 on the test
set and challenge set of the task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_K/0/1/0/all/0/1"&gt;Kshitij Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gautam_D/0/1/0/all/0/1"&gt;Devansh Gautam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mamidi_R/0/1/0/all/0/1"&gt;Radhika Mamidi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring Sparse Expert Models and Beyond. (arXiv:2105.15082v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.15082</id>
        <link href="http://arxiv.org/abs/2105.15082"/>
        <updated>2021-06-02T02:50:00.179Z</updated>
        <summary type="html"><![CDATA[Mixture-of-Experts (MoE) models can achieve promising results with outrageous
large amount of parameters but constant computation cost, and thus it has
become a trend in model scaling. Still it is a mystery how MoE layers bring
quality gains by leveraging the parameters with sparse activation. In this
work, we investigate several key factors in sparse expert models. We observe
that load imbalance may not be a significant problem affecting model quality,
contrary to the perspectives of recent studies, while the number of sparsely
activated experts $k$ and expert capacity $C$ in top-$k$ routing can
significantly make a difference in this context. Furthermore, we take a step
forward to propose a simple method called expert prototyping that splits
experts into different prototypes and applies $k$ top-$1$ routing. This
strategy improves the model quality but maintains constant computational costs,
and our further exploration on extremely large-scale models reflects that it is
more effective in training larger models. We push the model scale to over $1$
trillion parameters and implement it on solely $480$ NVIDIA V100-32GB GPUs, in
comparison with the recent SOTAs on $2048$ TPU cores. The proposed giant model
achieves substantial speedup in convergence over the same-size baseline.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_A/0/1/0/all/0/1"&gt;An Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1"&gt;Junyang Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Men_R/0/1/0/all/0/1"&gt;Rui Men&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1"&gt;Chang Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1"&gt;Le Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1"&gt;Xianyan Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1"&gt;Ang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jie Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jiamang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1"&gt;Di Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1"&gt;Wei Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qu_L/0/1/0/all/0/1"&gt;Lin Qu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jingren Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Hongxia Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rethinking Re-Sampling in Imbalanced Semi-Supervised Learning. (arXiv:2106.00209v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.00209</id>
        <link href="http://arxiv.org/abs/2106.00209"/>
        <updated>2021-06-02T02:50:00.171Z</updated>
        <summary type="html"><![CDATA[Semi-Supervised Learning (SSL) has shown its strong ability in utilizing
unlabeled data when labeled data is scarce. However, most SSL algorithms work
under the assumption that the class distributions are balanced in both training
and test sets. In this work, we consider the problem of SSL on class-imbalanced
data, which better reflects real-world situations but has only received limited
attention so far. In particular, we decouple the training of the representation
and the classifier, and systematically investigate the effects of different
data re-sampling techniques when training the whole network including a
classifier as well as fine-tuning the feature extractor only. We find that data
re-sampling is of critical importance to learn a good classifier as it
increases the accuracy of the pseudo-labels, in particular for the minority
classes in the unlabeled data. Interestingly, we find that accurate
pseudo-labels do not help when training the feature extractor, rather
contrariwise, data re-sampling harms the training of the feature extractor.
This finding is against the general intuition that wrong pseudo-labels always
harm the model performance in SSL. Based on these findings, we suggest to
re-think the current paradigm of having a single data re-sampling strategy and
develop a simple yet highly effective Bi-Sampling (BiS) strategy for SSL on
class-imbalanced data. BiS implements two different re-sampling strategies for
training the feature extractor and the classifier and integrates this decoupled
training into an end-to-end framework... Code will be released at
https://github.com/TACJu/Bi-Sampling.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1"&gt;Ju He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kortylewski_A/0/1/0/all/0/1"&gt;Adam Kortylewski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1"&gt;Shaokang Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Shuai Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1"&gt;Cheng Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Changhu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuille_A/0/1/0/all/0/1"&gt;Alan Yuille&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Analysis of classifiers robust to noisy labels. (arXiv:2106.00274v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.00274</id>
        <link href="http://arxiv.org/abs/2106.00274"/>
        <updated>2021-06-02T02:50:00.150Z</updated>
        <summary type="html"><![CDATA[We explore contemporary robust classification algorithms for overcoming
class-dependant labelling noise: Forward, Importance Re-weighting and
T-revision. The classifiers are trained and evaluated on class-conditional
random label noise data while the final test data is clean. We demonstrate
methods for estimating the transition matrix in order to obtain better
classifier performance when working with noisy data. We apply deep learning to
three data-sets and derive an end-to-end analysis with unknown noise on the
CIFAR data-set from scratch. The effectiveness and robustness of the
classifiers are analysed, and we compare and contrast the results of each
experiment are using top-1 accuracy as our criterion.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Diaz_A/0/1/0/all/0/1"&gt;Alex D&amp;#xed;az&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Steele_D/0/1/0/all/0/1"&gt;Damian Steele&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quantification of Carbon Sequestration in Urban Forests. (arXiv:2106.00182v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.00182</id>
        <link href="http://arxiv.org/abs/2106.00182"/>
        <updated>2021-06-02T02:50:00.143Z</updated>
        <summary type="html"><![CDATA[Vegetation, trees in particular, sequester carbon by absorbing carbon dioxide
from the atmosphere, however, the lack of efficient quantification methods of
carbon stored in trees renders it difficult to track the process. Here we
present an approach to estimate the carbon storage in trees based on fusing
multispectral aerial imagery and LiDAR data to identify tree coverage,
geometric shape, and tree species, which are crucial attributes in carbon
storage quantification. We demonstrate that tree species information and their
three-dimensional geometric shapes can be estimated from remote imagery in
order to calculate the tree's biomass. Specifically, for Manhattan, New York
City, we estimate a total of $52,000$ tons of carbon sequestered in trees.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Klein_L/0/1/0/all/0/1"&gt;Levente Klein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1"&gt;Wang Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Albrecht_C/0/1/0/all/0/1"&gt;Conrad Albrecht&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Wiki-Reliability: A Large Scale Dataset for Content Reliability on Wikipedia. (arXiv:2105.04117v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.04117</id>
        <link href="http://arxiv.org/abs/2105.04117"/>
        <updated>2021-06-02T02:50:00.137Z</updated>
        <summary type="html"><![CDATA[Wikipedia is the largest online encyclopedia, used by algorithms and web
users as a central hub of reliable information on the web. The quality and
reliability of Wikipedia content is maintained by a community of volunteer
editors. Machine learning and information retrieval algorithms could help scale
up editors' manual efforts around Wikipedia content reliability. However, there
is a lack of large-scale data to support the development of such research. To
fill this gap, in this paper, we propose Wiki-Reliability, the first dataset of
English Wikipedia articles annotated with a wide set of content reliability
issues. To build this dataset, we rely on Wikipedia "templates". Templates are
tags used by expert Wikipedia editors to indicate content issues, such as the
presence of "non-neutral point of view" or "contradictory articles", and serve
as a strong signal for detecting reliability issues in a revision. We select
the 10 most popular reliability-related templates on Wikipedia, and propose an
effective method to label almost 1M samples of Wikipedia article revisions as
positive or negative with respect to each template. Each positive/negative
example in the dataset comes with the full article text and 20 features from
the revision's metadata. We provide an overview of the possible downstream
tasks enabled by such data, and show that Wiki-Reliability can be used to train
large-scale models for content reliability prediction. We release all data and
code for public use.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wong_K/0/1/0/all/0/1"&gt;KayYen Wong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Redi_M/0/1/0/all/0/1"&gt;Miriam Redi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saez_Trumper_D/0/1/0/all/0/1"&gt;Diego Saez-Trumper&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep learning for prediction of hepatocellular carcinoma recurrence after resection or liver transplantation: a discovery and validation study. (arXiv:2106.00090v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.00090</id>
        <link href="http://arxiv.org/abs/2106.00090"/>
        <updated>2021-06-02T02:50:00.129Z</updated>
        <summary type="html"><![CDATA[This study aimed to develop a classifier of prognosis after resection or
liver transplantation (LT) for HCC by directly analysing the ubiquitously
available histological images using deep learning based neural networks.
Nucleus map set was used to train U-net to capture the nuclear architectural
information. Train set included the patients with HCC treated by resection and
has a distinct outcome. LT set contained patients with HCC treated by LT. Train
set and its nuclear architectural information extracted by U-net were used to
train MobileNet V2 based classifier (MobileNetV2_HCC_Class), purpose-built for
classifying supersized heterogeneous images. The MobileNetV2_HCC_Class
maintained relative higher discriminatory power than the other factors after
HCC resection or LT in the independent validation set. Pathological review
showed that the tumoral areas most predictive of recurrence were characterized
by presence of stroma, high degree of cytological atypia, nuclear
hyperchomasia, and a lack of immune infiltration. A clinically useful
prognostic classifier was developed using deep learning allied to histological
slides. The classifier has been extensively evaluated in independent patient
populations with different treatment, and gives consistent excellent results
across the classical clinical, biological and pathological features. The
classifier assists in refining the prognostic prediction of HCC patients and
identifying patients who would benefit from more intensive management.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhikun Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yuanpeng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1"&gt;Yuan Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meng_J/0/1/0/all/0/1"&gt;Jinwen Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jianguo Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1"&gt;Shusen Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1"&gt;Xiao Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reconciliation of Statistical and Spatial Sparsity For Robust Image and Image-Set Classification. (arXiv:2106.00256v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.00256</id>
        <link href="http://arxiv.org/abs/2106.00256"/>
        <updated>2021-06-02T02:50:00.122Z</updated>
        <summary type="html"><![CDATA[Recent image classification algorithms, by learning deep features from
large-scale datasets, have achieved significantly better results comparing to
the classic feature-based approaches. However, there are still various
challenges of image classifications in practice, such as classifying noisy
image or image-set queries and training deep image classification models over
the limited-scale dataset. Instead of applying generic deep features, the
model-based approaches can be more effective and data-efficient for robust
image and image-set classification tasks, as various image priors are exploited
for modeling the inter- and intra-set data variations while preventing
over-fitting. In this work, we propose a novel Joint Statistical and Spatial
Sparse representation, dubbed \textit{J3S}, to model the image or image-set
data for classification, by reconciling both their local patch structures and
global Gaussian distribution mapped into Riemannian manifold. To the best of
our knowledge, no work to date utilized both global statistics and local patch
structures jointly via joint sparse representation. We propose to solve the
joint sparse coding problem based on the J3S model, by coupling the local and
global image representations using joint sparsity. The learned J3S models are
used for robust image and image-set classification. Experiments show that the
proposed J3S-based image classification scheme outperforms the popular or
state-of-the-art competing methods over FMD, UIUC, ETH-80 and YTC databases.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1"&gt;Hao Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yap_K/0/1/0/all/0/1"&gt;Kim-Hui Yap&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_B/0/1/0/all/0/1"&gt;Bihan Wen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Volta at SemEval-2021 Task 6: Towards Detecting Persuasive Texts and Images using Textual and Multimodal Ensemble. (arXiv:2106.00240v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.00240</id>
        <link href="http://arxiv.org/abs/2106.00240"/>
        <updated>2021-06-02T02:50:00.104Z</updated>
        <summary type="html"><![CDATA[Memes are one of the most popular types of content used to spread information
online. They can influence a large number of people through rhetorical and
psychological techniques. The task, Detection of Persuasion Techniques in Texts
and Images, is to detect these persuasive techniques in memes. It consists of
three subtasks: (A) Multi-label classification using textual content, (B)
Multi-label classification and span identification using textual content, and
(C) Multi-label classification using visual and textual content. In this paper,
we propose a transfer learning approach to fine-tune BERT-based models in
different modalities. We also explore the effectiveness of ensembles of models
trained in different modalities. We achieve an F1-score of 57.0, 48.2, and 52.1
in the corresponding subtasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_K/0/1/0/all/0/1"&gt;Kshitij Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gautam_D/0/1/0/all/0/1"&gt;Devansh Gautam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mamidi_R/0/1/0/all/0/1"&gt;Radhika Mamidi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Real-time and Light-weight Line Segment Detection. (arXiv:2106.00186v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.00186</id>
        <link href="http://arxiv.org/abs/2106.00186"/>
        <updated>2021-06-02T02:50:00.096Z</updated>
        <summary type="html"><![CDATA[Previous deep learning-based line segment detection (LSD) suffer from the
immense model size and high computational cost for line prediction. This
constrains them from real-time inference on computationally restricted
environments. In this paper, we propose a real-time and light-weight line
segment detector for resource-constrained environments named Mobile LSD
(M-LSD). We design an extremely efficient LSD architecture by minimizing the
backbone network and removing the typical multi-module process for line
prediction in previous methods. To maintain competitive performance with such a
light-weight network, we present novel training schemes: Segments of Line
segment (SoL) augmentation and geometric learning scheme. SoL augmentation
splits a line segment into multiple subparts, which are used to provide
auxiliary line data during the training process. Moreover, the geometric
learning scheme allows a model to capture additional geometry cues from
matching loss, junction and line segmentation, length and degree regression.
Compared with TP-LSD-Lite, previously the best real-time LSD method, our model
(M-LSD-tiny) achieves competitive performance with 2.5% of model size and an
increase of 130.5% in inference speed on GPU when evaluated with Wireframe and
YorkUrban datasets. Furthermore, our model runs at 56.8 FPS and 48.6 FPS on
Android and iPhone mobile devices, respectively. To the best of our knowledge,
this is the first real-time deep LSD method available on mobile devices.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gu_G/0/1/0/all/0/1"&gt;Geonmo Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ko_B/0/1/0/all/0/1"&gt;Byungsoo Ko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Go_S/0/1/0/all/0/1"&gt;SeoungHyun Go&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1"&gt;Sung-Hyun Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jingeun Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shin_M/0/1/0/all/0/1"&gt;Minchul Shin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generating Query Focused Summaries from Query-Free Resources. (arXiv:2012.14774v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.14774</id>
        <link href="http://arxiv.org/abs/2012.14774"/>
        <updated>2021-06-02T02:50:00.089Z</updated>
        <summary type="html"><![CDATA[The availability of large-scale datasets has driven the development of neural
models that create generic summaries from single or multiple documents. In this
work we consider query focused summarization (QFS), a task for which training
data in the form of queries, documents, and summaries is not readily available.
We propose to decompose QFS into (1) query modeling (i.e., finding supportive
evidence within a set of documents for a query) and (2) conditional language
modeling (i.e., summary generation). We introduce MaRGE, a Masked ROUGE
Regression framework for evidence estimation and ranking which relies on a
unified representation for summaries and queries, so that summaries in generic
data can be converted into proxy queries for learning a query model.
Experiments across QFS benchmarks and query types show that our model achieves
state-of-the-art performance despite learning from weak supervision.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yumo Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lapata_M/0/1/0/all/0/1"&gt;Mirella Lapata&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Networks for Entity Matching: A Survey. (arXiv:2010.11075v2 [cs.DB] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.11075</id>
        <link href="http://arxiv.org/abs/2010.11075"/>
        <updated>2021-06-02T02:50:00.083Z</updated>
        <summary type="html"><![CDATA[Entity matching is the problem of identifying which records refer to the same
real-world entity. It has been actively researched for decades, and a variety
of different approaches have been developed. Even today, it remains a
challenging problem, and there is still generous room for improvement. In
recent years we have seen new methods based upon deep learning techniques for
natural language processing emerge.

In this survey, we present how neural networks have been used for entity
matching. Specifically, we identify which steps of the entity matching process
existing work have targeted using neural networks, and provide an overview of
the different techniques used at each step. We also discuss contributions from
deep learning in entity matching compared to traditional methods, and propose a
taxonomy of deep neural networks for entity matching.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Barlaug_N/0/1/0/all/0/1"&gt;Nils Barlaug&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gulla_J/0/1/0/all/0/1"&gt;Jon Atle Gulla&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reliability Testing for Natural Language Processing Systems. (arXiv:2105.02590v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.02590</id>
        <link href="http://arxiv.org/abs/2105.02590"/>
        <updated>2021-06-02T02:50:00.075Z</updated>
        <summary type="html"><![CDATA[Questions of fairness, robustness, and transparency are paramount to address
before deploying NLP systems. Central to these concerns is the question of
reliability: Can NLP systems reliably treat different demographics fairly and
function correctly in diverse and noisy environments? To address this, we argue
for the need for reliability testing and contextualize it among existing work
on improving accountability. We show how adversarial attacks can be reframed
for this goal, via a framework for developing reliability tests. We argue that
reliability testing -- with an emphasis on interdisciplinary collaboration --
will enable rigorous and targeted testing, and aid in the enactment and
enforcement of industry standards.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tan_S/0/1/0/all/0/1"&gt;Samson Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1"&gt;Shafiq Joty&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baxter_K/0/1/0/all/0/1"&gt;Kathy Baxter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Taeihagh_A/0/1/0/all/0/1"&gt;Araz Taeihagh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bennett_G/0/1/0/all/0/1"&gt;Gregory A. Bennett&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kan_M/0/1/0/all/0/1"&gt;Min-Yen Kan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Analysis of Vision-based Abnormal Red Blood Cell Classification. (arXiv:2106.00389v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.00389</id>
        <link href="http://arxiv.org/abs/2106.00389"/>
        <updated>2021-06-02T02:50:00.068Z</updated>
        <summary type="html"><![CDATA[Identification of abnormalities in red blood cells (RBC) is key to diagnosing
a range of medical conditions from anaemia to liver disease. Currently this is
done manually, a time-consuming and subjective process. This paper presents an
automated process utilising the advantages of machine learning to increase
capacity and standardisation of cell abnormality detection, and its performance
is analysed. Three different machine learning technologies were used: a Support
Vector Machine (SVM), a classical machine learning technology; TabNet, a deep
learning architecture for tabular data; U-Net, a semantic segmentation network
designed for medical image segmentation. A critical issue was the highly
imbalanced nature of the dataset which impacts the efficacy of machine
learning. To address this, synthesising minority class samples in feature space
was investigated via Synthetic Minority Over-sampling Technique (SMOTE) and
cost-sensitive learning. A combination of these two methods is investigated to
improve the overall performance. These strategies were found to increase
sensitivity to minority classes. The impact of unknown cells on semantic
segmentation is demonstrated, with some evidence of the model applying learning
of labelled cells to these anonymous cells. These findings indicate both
classical models and new deep learning networks as promising methods in
automating RBC abnormality detection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1"&gt;Annika Wong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anantrasirichai_N/0/1/0/all/0/1"&gt;Nantheera Anantrasirichai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chalidabhongse_T/0/1/0/all/0/1"&gt;Thanarat H. Chalidabhongse&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Palasuwan_D/0/1/0/all/0/1"&gt;Duangdao Palasuwan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Palasuwan_A/0/1/0/all/0/1"&gt;Attakorn Palasuwan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bull_D/0/1/0/all/0/1"&gt;David Bull&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Removing Word-Level Spurious Alignment between Images and Pseudo-Captions in Unsupervised Image Captioning. (arXiv:2104.13872v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.13872</id>
        <link href="http://arxiv.org/abs/2104.13872"/>
        <updated>2021-06-02T02:50:00.049Z</updated>
        <summary type="html"><![CDATA[Unsupervised image captioning is a challenging task that aims at generating
captions without the supervision of image-sentence pairs, but only with images
and sentences drawn from different sources and object labels detected from the
images. In previous work, pseudo-captions, i.e., sentences that contain the
detected object labels, were assigned to a given image. The focus of the
previous work was on the alignment of input images and pseudo-captions at the
sentence level. However, pseudo-captions contain many words that are irrelevant
to a given image. In this work, we investigate the effect of removing
mismatched words from image-sentence alignment to determine how they make this
task difficult. We propose a simple gating mechanism that is trained to align
image features with only the most reliable words in pseudo-captions: the
detected object labels. The experimental results show that our proposed method
outperforms the previous methods without introducing complex sentence-level
learning objectives. Combined with the sentence-level alignment method of
previous work, our method further improves its performance. These results
confirm the importance of careful alignment in word-level details.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Honda_U/0/1/0/all/0/1"&gt;Ukyo Honda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ushiku_Y/0/1/0/all/0/1"&gt;Yoshitaka Ushiku&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hashimoto_A/0/1/0/all/0/1"&gt;Atsushi Hashimoto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Watanabe_T/0/1/0/all/0/1"&gt;Taro Watanabe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Matsumoto_Y/0/1/0/all/0/1"&gt;Yuji Matsumoto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Integrative Use of Computer Vision and Unmanned Aircraft Technologies in Public Inspection: Foreign Object Debris Image Collection. (arXiv:2106.00161v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.00161</id>
        <link href="http://arxiv.org/abs/2106.00161"/>
        <updated>2021-06-02T02:50:00.042Z</updated>
        <summary type="html"><![CDATA[Unmanned Aircraft Systems (UAS) have become an important resource for public
service providers and smart cities. The purpose of this study is to expand this
research area by integrating computer vision and UAS technology to automate
public inspection. As an initial case study for this work, a dataset of common
foreign object debris (FOD) is developed to assess the potential of
light-weight automated detection. This paper presents the rationale and
creation of this dataset. Future iterations of our work will include further
technical details analyzing experimental implementation. At a local airport,
UAS and portable cameras are used to collect the data contained in the initial
version of this dataset. After collecting these videos of FOD, they were split
into individual frames and stored as several thousand images. These frames are
then annotated following standard computer vision format and stored in a
folder-structure that reflects our creation method. The dataset annotations are
validated using a custom tool that could be abstracted to fit future
applications. Initial detection models were successfully created using the
famous You Only Look Once algorithm, which indicates the practicality of the
proposed data. Finally, several potential scenarios that could utilize either
this dataset or similar methods for other public service are presented.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Munyer_T/0/1/0/all/0/1"&gt;Travis J. E. Munyer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brinkman_D/0/1/0/all/0/1"&gt;Daniel Brinkman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1"&gt;Chenyu Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_X/0/1/0/all/0/1"&gt;Xin Zhong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Interplay Between Fine-tuning and Composition in Transformers. (arXiv:2105.14668v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14668</id>
        <link href="http://arxiv.org/abs/2105.14668"/>
        <updated>2021-06-02T02:50:00.035Z</updated>
        <summary type="html"><![CDATA[Pre-trained transformer language models have shown remarkable performance on
a variety of NLP tasks. However, recent research has suggested that
phrase-level representations in these models reflect heavy influences of
lexical content, but lack evidence of sophisticated, compositional phrase
information. Here we investigate the impact of fine-tuning on the capacity of
contextualized embeddings to capture phrase meaning information beyond lexical
content. Specifically, we fine-tune models on an adversarial paraphrase
classification task with high lexical overlap, and on a sentiment
classification task. After fine-tuning, we analyze phrasal representations in
controlled settings following prior work. We find that fine-tuning largely
fails to benefit compositionality in these representations, though training on
sentiment yields a small, localized benefit for certain models. In follow-up
analyses, we identify confounding cues in the paraphrase dataset that may
explain the lack of composition benefits from that task, and we discuss
potential factors underlying the localized benefits from sentiment training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1"&gt;Lang Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ettinger_A/0/1/0/all/0/1"&gt;Allyson Ettinger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hybrid Deep Neural Network for Brachial Plexus Nerve Segmentation in Ultrasound Images. (arXiv:2106.00373v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2106.00373</id>
        <link href="http://arxiv.org/abs/2106.00373"/>
        <updated>2021-06-02T02:50:00.028Z</updated>
        <summary type="html"><![CDATA[Ultrasound-guided regional anesthesia (UGRA) can replace general anesthesia
(GA), improving pain control and recovery time. This method can be applied on
the brachial plexus (BP) after clavicular surgeries. However, identification of
the BP from ultrasound (US) images is difficult, even for trained
professionals. To address this problem, convolutional neural networks (CNNs)
and more advanced deep neural networks (DNNs) can be used for identification
and segmentation of the BP nerve region. In this paper, we propose a hybrid
model consisting of a classification model followed by a segmentation model to
segment BP nerve regions in ultrasound images. A CNN model is employed as a
classifier to precisely select the images with the BP region. Then, a U-net or
M-net model is used for the segmentation. Our experimental results indicate
that the proposed hybrid model significantly improves the segmentation
performance over a single segmentation model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Boxtel_J/0/1/0/all/0/1"&gt;Juul P.A. van Boxtel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Vousten_V/0/1/0/all/0/1"&gt;Vincent R.J. Vousten&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Pluim_J/0/1/0/all/0/1"&gt;Josien Pluim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rad_N/0/1/0/all/0/1"&gt;Nastaran Mohammadian Rad&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rethinking Pseudo Labels for Semi-Supervised Object Detection. (arXiv:2106.00168v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.00168</id>
        <link href="http://arxiv.org/abs/2106.00168"/>
        <updated>2021-06-02T02:50:00.020Z</updated>
        <summary type="html"><![CDATA[Recent advances in semi-supervised object detection (SSOD) are largely driven
by consistency-based pseudo-labeling methods for image classification tasks,
producing pseudo labels as supervisory signals. However, when using pseudo
labels, there is a lack of consideration in localization precision and
amplified class imbalance, both of which are critical for detection tasks. In
this paper, we introduce certainty-aware pseudo labels tailored for object
detection, which can effectively estimate the classification and localization
quality of derived pseudo labels. This is achieved by converting conventional
localization as a classification task followed by refinement. Conditioned on
classification and localization quality scores, we dynamically adjust the
thresholds used to generate pseudo labels and reweight loss functions for each
category to alleviate the class imbalance problem. Extensive experiments
demonstrate that our method improves state-of-the-art SSOD performance by 1-2%
and 4-6% AP on COCO and PASCAL VOC, respectively. In the limited-annotation
regime, our approach improves supervised baselines by up to 10% AP using only
1-10% labeled data from COCO.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Hengduo Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Zuxuan Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1"&gt;Abhinav Shrivastava&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Davis_L/0/1/0/all/0/1"&gt;Larry S. Davis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SemEval-2021 Task 4: Reading Comprehension of Abstract Meaning. (arXiv:2105.14879v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14879</id>
        <link href="http://arxiv.org/abs/2105.14879"/>
        <updated>2021-06-02T02:50:00.000Z</updated>
        <summary type="html"><![CDATA[This paper introduces the SemEval-2021 shared task 4: Reading Comprehension
of Abstract Meaning (ReCAM). This shared task is designed to help evaluate the
ability of machines in representing and understanding abstract concepts. Given
a passage and the corresponding question, a participating system is expected to
choose the correct answer from five candidates of abstract concepts in a
cloze-style machine reading comprehension setup. Based on two typical
definitions of abstractness, i.e., the imperceptibility and nonspecificity, our
task provides three subtasks to evaluate the participating models.
Specifically, Subtask 1 aims to evaluate how well a system can model concepts
that cannot be directly perceived in the physical world. Subtask 2 focuses on
models' ability in comprehending nonspecific concepts located high in a
hypernym hierarchy given the context of a passage. Subtask 3 aims to provide
some insights into models' generalizability over the two types of abstractness.
During the SemEval-2021 official evaluation period, we received 23 submissions
to Subtask 1 and 28 to Subtask 2. The participating teams additionally made 29
submissions to Subtask 3. The leaderboard and competition website can be found
at https://competitions.codalab.org/competitions/26153. The data and baseline
code are available at
https://github.com/boyuanzheng010/SemEval2021-Reading-Comprehension-of-Abstract-Meaning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_B/0/1/0/all/0/1"&gt;Boyuan Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xiaoyu Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ruan_Y/0/1/0/all/0/1"&gt;Yu-Ping Ruan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ling_Z/0/1/0/all/0/1"&gt;Zhenhua Ling&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1"&gt;Quan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_S/0/1/0/all/0/1"&gt;Si Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1"&gt;Xiaodan Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[EV-VGCNN: A Voxel Graph CNN for Event-based Object Classification. (arXiv:2106.00216v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.00216</id>
        <link href="http://arxiv.org/abs/2106.00216"/>
        <updated>2021-06-02T02:49:59.993Z</updated>
        <summary type="html"><![CDATA[Event cameras report sparse intensity changes and hold noticeable advantages
of low power consumption, high dynamic range, and high response speed for
visual perception and understanding on portable devices. Event-based learning
methods have recently achieved massive success on object recognition by
integrating events into dense frame-based representations to apply traditional
2D learning algorithms. However, these approaches introduce much redundant
information during the sparse-to-dense conversion and necessitate models with
heavy-weight and large capacities, limiting the potential of event cameras on
real-life applications. To address the core problem of balancing accuracy and
model complexity for event-based classification models, we (1) construct graph
representations for event data to utilize their sparsity nature better and
design a lightweight end-to-end graph neural network (EV-VGCNN) for
classification; (2) use voxel-wise vertices rather than traditional point-wise
methods to incorporate the information from more points; (3) introduce a
multi-scale feature relational layer (MFRL) to extract semantic and motion cues
from each vertex adaptively concerning its distances to neighbors.
Comprehensive experiments show that our approach advances state-of-the-art
classification accuracy while achieving nearly 20 times parameter reduction
(merely 0.84M parameters).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1"&gt;Yongjian Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Hao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Huiying Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Youfu Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Closer Look at the Uncertainty Estimation in Semantic Segmentation under Distributional Shift. (arXiv:2106.00076v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.00076</id>
        <link href="http://arxiv.org/abs/2106.00076"/>
        <updated>2021-06-02T02:49:59.987Z</updated>
        <summary type="html"><![CDATA[While recent computer vision algorithms achieve impressive performance on
many benchmarks, they lack robustness - presented with an image from a
different distribution, (e.g. weather or lighting conditions not considered
during training), they may produce an erroneous prediction. Therefore, it is
desired that such a model will be able to reliably predict its confidence
measure. In this work, uncertainty estimation for the task of semantic
segmentation is evaluated under a varying level of domain shift: in a
cross-dataset setting and when adapting a model trained on data from the
simulation. It was shown that simple color transformations already provide a
strong baseline, comparable to using more sophisticated style-transfer data
augmentation. Further, by constructing an ensemble consisting of models using
different backbones and/or augmentation methods, it was possible to improve
significantly model performance in terms of overall accuracy and uncertainty
estimation under the domain shift setting. The Expected Calibration Error (ECE)
on challenging GTA to Cityscapes adaptation was reduced from 4.05 to the
competitive value of 1.1. Further, an ensemble of models was utilized in the
self-training setting to improve the pseudo-labels generation, which resulted
in a significant gain in the final model accuracy, compared to the standard
fine-tuning (without ensemble).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cygert_S/0/1/0/all/0/1"&gt;Sebastian Cygert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wroblewski_B/0/1/0/all/0/1"&gt;Bart&amp;#x142;omiej Wr&amp;#xf3;blewski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wozniak_K/0/1/0/all/0/1"&gt;Karol Wo&amp;#x17a;niak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Slowinski_R/0/1/0/all/0/1"&gt;Rados&amp;#x142;aw S&amp;#x142;owi&amp;#x144;ski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Czyzewski_A/0/1/0/all/0/1"&gt;Andrzej Czy&amp;#x17c;ewski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[3D map creation using crowdsourced GNSS data. (arXiv:2106.00107v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2106.00107</id>
        <link href="http://arxiv.org/abs/2106.00107"/>
        <updated>2021-06-02T02:49:59.980Z</updated>
        <summary type="html"><![CDATA[3D maps are increasingly useful for many applications such as drone
navigation, emergency services, and urban planning. However, creating 3D maps
and keeping them up-to-date using existing technologies, such as laser
scanners, is expensive. This paper proposes and implements a novel approach to
generate 2.5D (otherwise known as 3D level-of-detail (LOD) 1) maps for free
using Global Navigation Satellite Systems (GNSS) signals, which are globally
available and are blocked only by obstacles between the satellites and the
receivers. This enables us to find the patterns of GNSS signal availability and
create 3D maps. The paper applies algorithms to GNSS signal strength patterns
based on a boot-strapped technique that iteratively trains the signal
classifiers while generating the map. Results of the proposed technique
demonstrate the ability to create 3D maps using automatically processed GNSS
data. The results show that the third dimension, i.e. height of the buildings,
can be estimated with below 5 metre accuracy, which is the benchmark
recommended by the CityGML standard.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lines_T/0/1/0/all/0/1"&gt;Terence Lines&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Basiri_A/0/1/0/all/0/1"&gt;Ana Basiri&lt;/a&gt; (1) ((1) School of Geographical and Earth Sciences, University of Glasgow)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dual Normalization Multitasking for Audio-Visual Sounding Object Localization. (arXiv:2106.00180v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.00180</id>
        <link href="http://arxiv.org/abs/2106.00180"/>
        <updated>2021-06-02T02:49:59.958Z</updated>
        <summary type="html"><![CDATA[Although several research works have been reported on audio-visual sound
source localization in unconstrained videos, no datasets and metrics have been
proposed in the literature to quantitatively evaluate its performance. Defining
the ground truth for sound source localization is difficult, because the
location where the sound is produced is not limited to the range of the source
object, but the vibrations propagate and spread through the surrounding
objects. Therefore we propose a new concept, Sounding Object, to reduce the
ambiguity of the visual location of sound, making it possible to annotate the
location of the wide range of sound sources. With newly proposed metrics for
quantitative evaluation, we formulate the problem of Audio-Visual Sounding
Object Localization (AVSOL). We also created the evaluation dataset (AVSOL-E
dataset) by manually annotating the test set of well-known Audio-Visual Event
(AVE) dataset. To tackle this new AVSOL problem, we propose a novel multitask
training strategy and architecture called Dual Normalization Multitasking
(DNM), which aggregates the Audio-Visual Correspondence (AVC) task and the
classification task for video events into a single audio-visual similarity map.
By efficiently utilize both supervisions by DNM, our proposed architecture
significantly outperforms the baseline methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nishikawa_T/0/1/0/all/0/1"&gt;Tokuhiro Nishikawa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shimada_D/0/1/0/all/0/1"&gt;Daiki Shimada&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yokono_J/0/1/0/all/0/1"&gt;Jerry Jun Yokono&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Effect of large-scale pre-training on full and few-shot transfer learning for natural and medical images. (arXiv:2106.00116v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2106.00116</id>
        <link href="http://arxiv.org/abs/2106.00116"/>
        <updated>2021-06-02T02:49:59.939Z</updated>
        <summary type="html"><![CDATA[Transfer learning aims to exploit pre-trained models for more efficient
follow-up training on wide range of downstream tasks and datasets, enabling
successful training also on small data. Recent line of work posits strong
benefits for model generalization and transfer when model size, data size, and
compute budget are increased for the pre-training. It remains however still
largely unclear whether the observed transfer improvement due to increase in
scale also holds when source and target data distributions are far apart from
each other. In this work we conduct large-scale pre-training on large source
datasets of either natural (ImageNet-21k/1k) or medical chest X-Ray images and
compare full and few-shot transfer using different target datasets from both
natural and medical imaging domains. Our observations provide evidence that
while pre-training and transfer on closely related datasets do show clear
benefit of increasing model and data size during pre-training, such benefits
are not clearly visible when source and target datasets are further apart.
These observations hold across both full and few-shot transfer and indicate
that scaling laws hinting improvement of generalization and transfer with
increasing model and data size are incomplete and should also take into account
the degree of how distinct the source and target data distributions are, to
correctly predict effect of model size and data size variation during
pre-training on transfer. (Repository for reproducing the experiments will be
made available.)]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cherti_M/0/1/0/all/0/1"&gt;Mehdi Cherti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jitsev_J/0/1/0/all/0/1"&gt;Jenia Jitsev&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Continual 3D Convolutional Neural Networks for Real-time Processing of Videos. (arXiv:2106.00050v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.00050</id>
        <link href="http://arxiv.org/abs/2106.00050"/>
        <updated>2021-06-02T02:49:59.902Z</updated>
        <summary type="html"><![CDATA[This paper introduces Continual 3D Convolutional Neural Networks (Co3D CNNs),
a new computational formulation of spatio-temporal 3D CNNs, in which videos are
processed frame-by-frame rather than by clip. In online processing tasks
demanding frame-wise predictions, Co3D CNNs dispense with the computational
redundancies of regular 3D CNNs, namely the repeated convolutions over frames,
which appear in multiple clips. While yielding an order of magnitude in
computational savings, Co3D CNNs have memory requirements comparable with that
of corresponding regular 3D CNNs and are less affected by changes in the size
of the temporal receptive field. We show that Continual 3D CNNs initialised on
the weights from preexisting state-of-the-art video recognition models reduce
the floating point operations for frame-wise computations by 10.0-12.4x while
improving accuracy on Kinetics-400 by 2.3-3.8. Moreover, we investigate the
transient start-up response of Co3D CNNs and perform an extensive benchmark of
online processing speed as well as accuracy for publicly available
state-of-the-art 3D CNNs on modern hardware.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hedegaard_L/0/1/0/all/0/1"&gt;Lukas Hedegaard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iosifidis_A/0/1/0/all/0/1"&gt;Alexandros Iosifidis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploration and Exploitation: Two Ways to Improve Chinese Spelling Correction Models. (arXiv:2105.14813v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14813</id>
        <link href="http://arxiv.org/abs/2105.14813"/>
        <updated>2021-06-02T02:49:59.892Z</updated>
        <summary type="html"><![CDATA[A sequence-to-sequence learning with neural networks has empirically proven
to be an effective framework for Chinese Spelling Correction (CSC), which takes
a sentence with some spelling errors as input and outputs the corrected one.
However, CSC models may fail to correct spelling errors covered by the
confusion sets, and also will encounter unseen ones. We propose a method, which
continually identifies the weak spots of a model to generate more valuable
training instances, and apply a task-specific pre-training strategy to enhance
the model. The generated adversarial examples are gradually added to the
training set. Experimental results show that such an adversarial training
method combined with the pretraining strategy can improve both the
generalization and robustness of multiple CSC models across three different
datasets, achieving stateof-the-art performance for CSC task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Cenyuan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1"&gt;Xiaoqing Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1"&gt;Xuanjing Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HiddenCut: Simple Data Augmentation for Natural Language Understanding with Better Generalization. (arXiv:2106.00149v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.00149</id>
        <link href="http://arxiv.org/abs/2106.00149"/>
        <updated>2021-06-02T02:49:59.814Z</updated>
        <summary type="html"><![CDATA[Fine-tuning large pre-trained models with task-specific data has achieved
great success in NLP. However, it has been demonstrated that the majority of
information within the self-attention networks is redundant and not utilized
effectively during the fine-tuning stage. This leads to inferior results when
generalizing the obtained models to out-of-domain distributions. To this end,
we propose a simple yet effective data augmentation technique, HiddenCut, to
better regularize the model and encourage it to learn more generalizable
features. Specifically, contiguous spans within the hidden space are
dynamically and strategically dropped during training. Experiments show that
our HiddenCut method outperforms the state-of-the-art augmentation methods on
the GLUE benchmark, and consistently exhibits superior generalization
performances on out-of-distribution and challenging counterexamples. We have
publicly released our code at https://github.com/GT-SALT/HiddenCut.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jiaao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_D/0/1/0/all/0/1"&gt;Dinghan Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1"&gt;Weizhu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1"&gt;Diyi Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Replicating and Extending "\textit{Because Their Treebanks Leak}": Graph Isomorphism, Covariants, and Parser Performance. (arXiv:2106.00352v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.00352</id>
        <link href="http://arxiv.org/abs/2106.00352"/>
        <updated>2021-06-02T02:49:59.806Z</updated>
        <summary type="html"><![CDATA[S{\o}gaard (2020) obtained results suggesting the fraction of trees occurring
in the test data isomorphic to trees in the training set accounts for a
non-trivial variation in parser performance. Similar to other statistical
analyses in NLP, the results were based on evaluating linear regressions.
However, the study had methodological issues and was undertaken using a small
sample size leading to unreliable results. We present a replication study in
which we also bin sentences by length and find that only a small subset of
sentences vary in performance with respect to graph isomorphism. Further, the
correlation observed between parser performance and graph isomorphism in the
wild disappears when controlling for covariants. However, in a controlled
experiment, where covariants are kept fixed, we do observe a strong
correlation. We suggest that conclusions drawn from statistical analyses like
this need to be tempered and that controlled experiments can complement them by
more readily teasing factors apart.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Anderson_M/0/1/0/all/0/1"&gt;Mark Anderson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sogaard_A/0/1/0/all/0/1"&gt;Anders S&amp;#xf8;gaard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rodriguez_C/0/1/0/all/0/1"&gt;Carlos G&amp;#xf3;mez Rodr&amp;#xed;guez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GLGE: A New General Language Generation Evaluation Benchmark. (arXiv:2011.11928v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.11928</id>
        <link href="http://arxiv.org/abs/2011.11928"/>
        <updated>2021-06-02T02:49:59.740Z</updated>
        <summary type="html"><![CDATA[Multi-task benchmarks such as GLUE and SuperGLUE have driven great progress
of pretraining and transfer learning in Natural Language Processing (NLP).
These benchmarks mostly focus on a range of Natural Language Understanding
(NLU) tasks, without considering the Natural Language Generation (NLG) models.
In this paper, we present the General Language Generation Evaluation (GLGE), a
new multi-task benchmark for evaluating the generalization capabilities of NLG
models across eight language generation tasks. For each task, we continue to
design three subtasks in terms of task difficulty (GLGE-Easy, GLGE-Medium, and
GLGE-Hard). This introduces 24 subtasks to comprehensively compare model
performance. To encourage research on pretraining and transfer learning on NLG
models, we make GLGE publicly available and build a leaderboard with strong
baselines including MASS, BART, and ProphetNet (The source code and dataset are
publicly available at https://github.com/microsoft/glge).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1"&gt;Dayiheng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1"&gt;Yu Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1"&gt;Yeyun Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_W/0/1/0/all/0/1"&gt;Weizhen Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Hang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiao_J/0/1/0/all/0/1"&gt;Jian Jiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1"&gt;Weizhu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1"&gt;Jie Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shou_L/0/1/0/all/0/1"&gt;Linjun Shou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_M/0/1/0/all/0/1"&gt;Ming Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1"&gt;Pengcheng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jiusheng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1"&gt;Daxin Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lv_J/0/1/0/all/0/1"&gt;Jiancheng Lv&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Ruofei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1"&gt;Winnie Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1"&gt;Ming Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1"&gt;Nan Duan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PIGLeT: Language Grounding Through Neuro-Symbolic Interaction in a 3D World. (arXiv:2106.00188v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.00188</id>
        <link href="http://arxiv.org/abs/2106.00188"/>
        <updated>2021-06-02T02:49:59.733Z</updated>
        <summary type="html"><![CDATA[We propose PIGLeT: a model that learns physical commonsense knowledge through
interaction, and then uses this knowledge to ground language. We factorize
PIGLeT into a physical dynamics model, and a separate language model. Our
dynamics model learns not just what objects are but also what they do: glass
cups break when thrown, plastic ones don't. We then use it as the interface to
our language model, giving us a unified model of linguistic form and grounded
meaning. PIGLeT can read a sentence, simulate neurally what might happen next,
and then communicate that result through a literal symbolic representation, or
natural language.

Experimental results show that our model effectively learns world dynamics,
along with how to communicate them. It is able to correctly forecast "what
happens next" given an English sentence over 80% of the time, outperforming a
100x larger, text-to-text approach by over 10%. Likewise, its natural language
summaries of physical interactions are also judged by humans as more accurate
than LM alternatives. We present comprehensive analysis showing room for future
work.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zellers_R/0/1/0/all/0/1"&gt;Rowan Zellers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Holtzman_A/0/1/0/all/0/1"&gt;Ari Holtzman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peters_M/0/1/0/all/0/1"&gt;Matthew Peters&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mottaghi_R/0/1/0/all/0/1"&gt;Roozbeh Mottaghi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kembhavi_A/0/1/0/all/0/1"&gt;Aniruddha Kembhavi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Farhadi_A/0/1/0/all/0/1"&gt;Ali Farhadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1"&gt;Yejin Choi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Low-Resource Spoken Language Identification Using Self-Attentive Pooling and Deep 1D Time-Channel Separable Convolutions. (arXiv:2106.00052v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2106.00052</id>
        <link href="http://arxiv.org/abs/2106.00052"/>
        <updated>2021-06-02T02:49:59.725Z</updated>
        <summary type="html"><![CDATA[This memo describes NTR/TSU winning submission for Low Resource ASR challenge
at Dialog2021 conference, language identification track.

Spoken Language Identification (LID) is an important step in a multilingual
Automated Speech Recognition (ASR) system pipeline. Traditionally, the ASR task
requires large volumes of labeled data that are unattainable for most of the
world's languages, including most of the languages of Russia. In this memo, we
show that a convolutional neural network with a Self-Attentive Pooling layer
shows promising results in low-resource setting for the language identification
task and set up a SOTA for the Low Resource ASR challenge dataset.

Additionally, we compare the structure of confusion matrices for this and
significantly more diverse VoxForge dataset and state and substantiate the
hypothesis that whenever the dataset is diverse enough so that the other
classification factors, like gender, age etc. are well-averaged, the confusion
matrix for LID system bears the language similarity measure.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Bedyakin_R/0/1/0/all/0/1"&gt;Roman Bedyakin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mikhaylovskiy_N/0/1/0/all/0/1"&gt;Nikolay Mikhaylovskiy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NewsEmbed: Modeling News through Pre-trained DocumentRepresentations. (arXiv:2106.00590v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.00590</id>
        <link href="http://arxiv.org/abs/2106.00590"/>
        <updated>2021-06-02T02:49:59.719Z</updated>
        <summary type="html"><![CDATA[Effectively modeling text-rich fresh content such as news articles at
document-level is a challenging problem. To ensure a content-based model
generalize well to a broad range of applications, it is critical to have a
training dataset that is large beyond the scale of human labels while achieving
desired quality. In this work, we address those two challenges by proposing a
novel approach to mine semantically-relevant fresh documents, and their topic
labels, with little human supervision. Meanwhile, we design a multitask model
called NewsEmbed that alternatively trains a contrastive learning with a
multi-label classification to derive a universal document encoder. We show that
the proposed approach can provide billions of high quality organic training
examples and can be naturally extended to multilingual setting where texts in
different languages are encoded in the same semantic space. We experimentally
demonstrate NewsEmbed's competitive performance across multiple natural
language understanding tasks, both supervised and unsupervised.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jialu Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tianqi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1"&gt;Cong Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dialogue-oriented Pre-training. (arXiv:2106.00420v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.00420</id>
        <link href="http://arxiv.org/abs/2106.00420"/>
        <updated>2021-06-02T02:49:59.704Z</updated>
        <summary type="html"><![CDATA[Pre-trained language models (PrLM) has been shown powerful in enhancing a
broad range of downstream tasks including various dialogue related ones.
However, PrLMs are usually trained on general plain text with common language
model (LM) training objectives, which cannot sufficiently capture dialogue
exclusive features due to the limitation of such training setting, so that
there is an immediate need to fill the gap between a specific dialogue task and
the LM task. As it is unlikely to collect huge dialogue data for
dialogue-oriented pre-training, in this paper, we propose three strategies to
simulate the conversation features on general plain text. Our proposed method
differs from existing post-training methods that it may yield a general-purpose
PrLM and does not individualize to any detailed task while keeping the
capability of learning dialogue related features including speaker awareness,
continuity and consistency. The resulted Dialog-PrLM is fine-tuned on three
public multi-turn dialogue datasets and helps achieve significant and
consistent improvement over the plain PrLMs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yi Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1"&gt;Hai Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SpanNer: Named Entity Re-/Recognition as Span Prediction. (arXiv:2106.00641v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.00641</id>
        <link href="http://arxiv.org/abs/2106.00641"/>
        <updated>2021-06-02T02:49:59.669Z</updated>
        <summary type="html"><![CDATA[Recent years have seen the paradigm shift of Named Entity Recognition (NER)
systems from sequence labeling to span prediction. Despite its preliminary
effectiveness, the span prediction model's architectural bias has not been
fully understood. In this paper, we first investigate the strengths and
weaknesses when the span prediction model is used for named entity recognition
compared with the sequence labeling framework and how to further improve it,
which motivates us to make complementary advantages of systems based on
different paradigms. We then reveal that span prediction, simultaneously, can
serve as a system combiner to re-recognize named entities from different
systems' outputs. We experimentally implement 154 systems on 11 datasets,
covering three languages, comprehensive results show the effectiveness of span
prediction models that both serve as base NER systems and system combiners. We
make all code and datasets available: \url{https://github.com/neulab/spanner},
as well as an online system demo: \url{this http URL}. Our model also has
been deployed into the ExplainaBoard platform, which allows users to flexibly
perform a system combination of top-scoring systems in an interactive way:
\url{this http URL}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1"&gt;Jinlan Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1"&gt;Xuanjing Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1"&gt;Pengfei Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Language Model Evaluation Beyond Perplexity. (arXiv:2106.00085v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.00085</id>
        <link href="http://arxiv.org/abs/2106.00085"/>
        <updated>2021-06-02T02:49:59.644Z</updated>
        <summary type="html"><![CDATA[We propose an alternate approach to quantifying how well language models
learn natural language: we ask how well they match the statistical tendencies
of natural language. To answer this question, we analyze whether text generated
from language models exhibits the statistical tendencies present in the
human-generated text on which they were trained. We provide a framework--paired
with significance tests--for evaluating the fit of language models to certain
statistical tendencies of natural language. We find that neural language models
appear to learn only a subset of the statistical tendencies considered, but
align much more closely with empirical trends than theoretical laws (when
present). Further, the fit to different distributions is dependent on both
model architecture and generation strategy. As concrete examples, text
generated under the nucleus sampling scheme adheres more closely to the
type--token relationship of natural language than text produced using standard
ancestral sampling; text from LSTMs reflects the natural language distributions
over length, stopwords, and symbols suprisingly well.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Meister_C/0/1/0/all/0/1"&gt;Clara Meister&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1"&gt;Ryan Cotterell&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Polyjuice: Generating Counterfactuals for Explaining, Evaluating, and Improving Models. (arXiv:2101.00288v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.00288</id>
        <link href="http://arxiv.org/abs/2101.00288"/>
        <updated>2021-06-02T02:49:59.638Z</updated>
        <summary type="html"><![CDATA[While counterfactual examples are useful for analysis and training of NLP
models, current generation methods either rely on manual labor to create very
few counterfactuals, or only instantiate limited types of perturbations such as
paraphrases or word substitutions. We present Polyjuice, a general-purpose
counterfactual generator that allows for control over perturbation types and
locations, trained by finetuning GPT-2 on multiple datasets of paired
sentences. We show that Polyjuice produces diverse sets of realistic
counterfactuals, which in turn are useful in various distinct applications:
improving training and evaluation on three different tasks (with around 70%
less annotation effort than manual generation), augmenting state-of-the-art
explanation techniques, and supporting systematic counterfactual error analysis
by revealing behaviors easily missed by human experts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1"&gt;Tongshuang Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ribeiro_M/0/1/0/all/0/1"&gt;Marco Tulio Ribeiro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heer_J/0/1/0/all/0/1"&gt;Jeffrey Heer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weld_D/0/1/0/all/0/1"&gt;Daniel S. Weld&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Quantifiable Dialogue Coherence Evaluation. (arXiv:2106.00507v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.00507</id>
        <link href="http://arxiv.org/abs/2106.00507"/>
        <updated>2021-06-02T02:49:59.621Z</updated>
        <summary type="html"><![CDATA[Automatic dialogue coherence evaluation has attracted increasing attention
and is crucial for developing promising dialogue systems. However, existing
metrics have two major limitations: (a) they are mostly trained in a simplified
two-level setting (coherent vs. incoherent), while humans give Likert-type
multi-level coherence scores, dubbed as "quantifiable"; (b) their predicted
coherence scores cannot align with the actual human rating standards due to the
absence of human guidance during training. To address these limitations, we
propose Quantifiable Dialogue Coherence Evaluation (QuantiDCE), a novel
framework aiming to train a quantifiable dialogue coherence metric that can
reflect the actual human rating standards. Specifically, QuantiDCE includes two
training stages, Multi-Level Ranking (MLR) pre-training and Knowledge
Distillation (KD) fine-tuning. During MLR pre-training, a new MLR loss is
proposed for enabling the model to learn the coarse judgement of coherence
degrees. Then, during KD fine-tuning, the pretrained model is further finetuned
to learn the actual human rating standards with only very few human-annotated
data. To advocate the generalizability even with limited fine-tuning data, a
novel KD regularization is introduced to retain the knowledge learned at the
pre-training stage. Experimental results show that the model trained by
QuantiDCE presents stronger correlations with human judgements than the other
state-of-the-art metrics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ye_Z/0/1/0/all/0/1"&gt;Zheng Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_L/0/1/0/all/0/1"&gt;Liucun Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1"&gt;Lishan Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1"&gt;Liang Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1"&gt;Xiaodan Liang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automated Concatenation of Embeddings for Structured Prediction. (arXiv:2010.05006v4 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.05006</id>
        <link href="http://arxiv.org/abs/2010.05006"/>
        <updated>2021-06-02T02:49:59.614Z</updated>
        <summary type="html"><![CDATA[Pretrained contextualized embeddings are powerful word representations for
structured prediction tasks. Recent work found that better word representations
can be obtained by concatenating different types of embeddings. However, the
selection of embeddings to form the best concatenated representation usually
varies depending on the task and the collection of candidate embeddings, and
the ever-increasing number of embedding types makes it a more difficult
problem. In this paper, we propose Automated Concatenation of Embeddings (ACE)
to automate the process of finding better concatenations of embeddings for
structured prediction tasks, based on a formulation inspired by recent progress
on neural architecture search. Specifically, a controller alternately samples a
concatenation of embeddings, according to its current belief of the
effectiveness of individual embedding types in consideration for a task, and
updates the belief based on a reward. We follow strategies in reinforcement
learning to optimize the parameters of the controller and compute the reward
based on the accuracy of a task model, which is fed with the sampled
concatenation as input and trained on a task dataset. Empirical results on 6
tasks and 21 datasets show that our approach outperforms strong baselines and
achieves state-of-the-art performance with fine-tuned embeddings in all the
evaluations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xinyu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1"&gt;Yong Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bach_N/0/1/0/all/0/1"&gt;Nguyen Bach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1"&gt;Tao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zhongqiang Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1"&gt;Fei Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tu_K/0/1/0/all/0/1"&gt;Kewei Tu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SemEval-2021 Task 1: Lexical Complexity Prediction. (arXiv:2106.00473v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.00473</id>
        <link href="http://arxiv.org/abs/2106.00473"/>
        <updated>2021-06-02T02:49:59.602Z</updated>
        <summary type="html"><![CDATA[This paper presents the results and main findings of SemEval-2021 Task 1 -
Lexical Complexity Prediction. We provided participants with an augmented
version of the CompLex Corpus (Shardlow et al 2020). CompLex is an English
multi-domain corpus in which words and multi-word expressions (MWEs) were
annotated with respect to their complexity using a five point Likert scale.
SemEval-2021 Task 1 featured two Sub-tasks: Sub-task 1 focused on single words
and Sub-task 2 focused on MWEs. The competition attracted 198 teams in total,
of which 54 teams submitted official runs on the test data to Sub-task 1 and 37
to Sub-task 2.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shardlow_M/0/1/0/all/0/1"&gt;Matthew Shardlow&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Evans_R/0/1/0/all/0/1"&gt;Richard Evans&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paetzold_G/0/1/0/all/0/1"&gt;Gustavo Henrique Paetzold&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zampieri_M/0/1/0/all/0/1"&gt;Marcos Zampieri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Code Summarization with Structure-induced Transformer. (arXiv:2012.14710v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.14710</id>
        <link href="http://arxiv.org/abs/2012.14710"/>
        <updated>2021-06-02T02:49:59.502Z</updated>
        <summary type="html"><![CDATA[Code summarization (CS) is becoming a promising area in recent language
understanding, which aims to generate sensible human language automatically for
programming language in the format of source code, serving in the most
convenience of programmer developing. It is well known that programming
languages are highly structured. Thus previous works attempt to apply
structure-based traversal (SBT) or non-sequential models like Tree-LSTM and
graph neural network (GNN) to learn structural program semantics. However, it
is surprising that incorporating SBT into advanced encoder like Transformer
instead of LSTM has been shown no performance gain, which lets GNN become the
only rest means modeling such necessary structural clue in source code. To
release such inconvenience, we propose structure-induced Transformer, which
encodes sequential code inputs with multi-view structural clues in terms of a
newly-proposed structure-induced self-attention mechanism. Extensive
experiments show that our proposed structure-induced Transformer helps achieve
new state-of-the-art results on benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1"&gt;Hongqiu Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1"&gt;Hai Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Min Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarial VQA: A New Benchmark for Evaluating the Robustness of VQA Models. (arXiv:2106.00245v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.00245</id>
        <link href="http://arxiv.org/abs/2106.00245"/>
        <updated>2021-06-02T02:49:59.483Z</updated>
        <summary type="html"><![CDATA[With large-scale pre-training, the past two years have witnessed significant
performance boost on the Visual Question Answering (VQA) task. Though rapid
progresses have been made, it remains unclear whether these state-of-the-art
(SOTA) VQA models are robust when encountering test examples in the wild. To
study this, we introduce Adversarial VQA, a new large-scale VQA benchmark,
collected iteratively via an adversarial human-and-model-in-the-loop procedure.
Through this new benchmark, we present several interesting findings. (i)
Surprisingly, during dataset collection, we find that non-expert annotators can
successfully attack SOTA VQA models with relative ease. (ii) We test a variety
of SOTA VQA models on our new dataset to highlight their fragility, and find
that both large-scale pre-trained models and adversarial training methods can
only achieve far lower performance than what they can achieve on the standard
VQA v2 dataset. (iii) When considered as data augmentation, our dataset can be
used to improve the performance on other robust VQA benchmarks. (iv) We present
a detailed analysis of the dataset, providing valuable insights on the
challenges it brings to the community. We hope Adversarial VQA can serve as a
valuable benchmark that will be used by future work to test the robustness of
its developed VQA models. Our dataset is publicly available at
https://adversarialvqa. github.io/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Linjie Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1"&gt;Jie Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gan_Z/0/1/0/all/0/1"&gt;Zhe Gan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jingjing Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[End-to-End Multihop Retrieval for Compositional Question Answering over Long Documents. (arXiv:2106.00200v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.00200</id>
        <link href="http://arxiv.org/abs/2106.00200"/>
        <updated>2021-06-02T02:49:59.457Z</updated>
        <summary type="html"><![CDATA[Answering complex questions from long documents requires aggregating multiple
pieces of evidence and then predicting the answers. In this paper, we propose a
multi-hop retrieval method, DocHopper, to answer compositional questions over
long documents. At each step, DocHopper retrieves a paragraph or sentence
embedding from the document, mixes the retrieved result with the query, and
updates the query for the next step. In contrast to many other retrieval-based
methods (e.g., RAG or REALM) the query is not augmented with a token sequence:
instead, it is augmented by "numerically" combining it with another neural
representation. This means that model is end-to-end differentiable. We
demonstrate that utilizing document structure in this was can largely improve
question-answering and retrieval performance on long documents. We experimented
with DocHopper on three different QA tasks that require reading long documents
to answer compositional questions: discourse entailment reasoning, factual QA
with table and text, and information seeking QA from academic papers. DocHopper
outperforms all baseline models and achieves state-of-the-art results on all
datasets. Additionally, DocHopper is efficient at inference time, being 3~10
times faster than the baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1"&gt;Haitian Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cohen_W/0/1/0/all/0/1"&gt;William W. Cohen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1"&gt;Ruslan Salakhutdinov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Gender Bias Amplification During Speed-Quality Optimization in Neural Machine Translation. (arXiv:2106.00169v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.00169</id>
        <link href="http://arxiv.org/abs/2106.00169"/>
        <updated>2021-06-02T02:49:59.449Z</updated>
        <summary type="html"><![CDATA[Is bias amplified when neural machine translation (NMT) models are optimized
for speed and evaluated on generic test sets using BLEU? We investigate
architectures and techniques commonly used to speed up decoding in
Transformer-based models, such as greedy search, quantization, average
attention networks (AANs) and shallow decoder models and show their effect on
gendered noun translation. We construct a new gender bias test set, SimpleGEN,
based on gendered noun phrases in which there is a single, unambiguous, correct
answer. While we find minimal overall BLEU degradation as we apply speed
optimizations, we observe that gendered noun translation performance degrades
at a much faster rate.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Renduchintala_A/0/1/0/all/0/1"&gt;Adithya Renduchintala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Diaz_D/0/1/0/all/0/1"&gt;Denise Diaz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heafield_K/0/1/0/all/0/1"&gt;Kenneth Heafield&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xian Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Diab_M/0/1/0/all/0/1"&gt;Mona Diab&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Text Summarization with Latent Queries. (arXiv:2106.00104v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.00104</id>
        <link href="http://arxiv.org/abs/2106.00104"/>
        <updated>2021-06-02T02:49:59.441Z</updated>
        <summary type="html"><![CDATA[The availability of large-scale datasets has driven the development of neural
models that create summaries from single documents, for generic purposes. When
using a summarization system, users often have specific intents with various
language realizations, which, depending on the information need, can range from
a single keyword to a long narrative composed of multiple questions. Existing
summarization systems, however, often either fail to support or act robustly on
this query focused summarization task. We introduce LaQSum, the first unified
text summarization system that learns Latent Queries from documents for
abstractive summarization with any existing query forms. Under a deep
generative framework, our system jointly optimizes a latent query model and a
conditional language model, allowing users to plug-and-play queries of any type
at test time. Despite learning from only generic summarization data and
requiring no further optimization for downstream summarization tasks, our
system robustly outperforms strong comparison systems across summarization
benchmarks with different query types, document settings, and target domains.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yumo Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lapata_M/0/1/0/all/0/1"&gt;Mirella Lapata&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predicting User Engagement Status for Online Evaluation of Intelligent Assistants. (arXiv:2010.00656v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.00656</id>
        <link href="http://arxiv.org/abs/2010.00656"/>
        <updated>2021-06-02T02:49:59.416Z</updated>
        <summary type="html"><![CDATA[Evaluation of intelligent assistants in large-scale and online settings
remains an open challenge. User behavior-based online evaluation metrics have
demonstrated great effectiveness for monitoring large-scale web search and
recommender systems. Therefore, we consider predicting user engagement status
as the very first and critical step to online evaluation for intelligent
assistants. In this work, we first proposed a novel framework for classifying
user engagement status into four categories -- fulfillment, continuation,
reformulation and abandonment. We then demonstrated how to design simple but
indicative metrics based on the framework to quantify user engagement levels.
We also aim for automating user engagement prediction with machine learning
methods. We compare various models and features for predicting engagement
status using four real-world datasets. We conducted detailed analyses on
features and failure cases to discuss the performance of current models as well
as challenges.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Meng_R/0/1/0/all/0/1"&gt;Rui Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yue_Z/0/1/0/all/0/1"&gt;Zhen Yue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Glass_A/0/1/0/all/0/1"&gt;Alyssa Glass&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CIDER: Commonsense Inference for Dialogue Explanation and Reasoning. (arXiv:2106.00510v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.00510</id>
        <link href="http://arxiv.org/abs/2106.00510"/>
        <updated>2021-06-02T02:49:59.407Z</updated>
        <summary type="html"><![CDATA[Commonsense inference to understand and explain human language is a
fundamental research problem in natural language processing. Explaining human
conversations poses a great challenge as it requires contextual understanding,
planning, inference, and several aspects of reasoning including causal,
temporal, and commonsense reasoning. In this work, we introduce CIDER -- a
manually curated dataset that contains dyadic dialogue explanations in the form
of implicit and explicit knowledge triplets inferred using contextual
commonsense inference. Extracting such rich explanations from conversations can
be conducive to improving several downstream applications. The annotated
triplets are categorized by the type of commonsense knowledge present (e.g.,
causal, conditional, temporal). We set up three different tasks conditioned on
the annotated dataset: Dialogue-level Natural Language Inference, Span
Extraction, and Multi-choice Span Selection. Baseline results obtained with
transformer-based models reveal that the tasks are difficult, paving the way
for promising future research. The dataset and the baseline implementations are
publicly available at https://github.com/declare-lab/CIDER.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ghosal_D/0/1/0/all/0/1"&gt;Deepanway Ghosal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hong_P/0/1/0/all/0/1"&gt;Pengfei Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_S/0/1/0/all/0/1"&gt;Siqi Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Majumder_N/0/1/0/all/0/1"&gt;Navonil Majumder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mihalcea_R/0/1/0/all/0/1"&gt;Rada Mihalcea&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Poria_S/0/1/0/all/0/1"&gt;Soujanya Poria&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[KGPool: Dynamic Knowledge Graph Context Selection for Relation Extraction. (arXiv:2106.00459v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.00459</id>
        <link href="http://arxiv.org/abs/2106.00459"/>
        <updated>2021-06-02T02:49:59.382Z</updated>
        <summary type="html"><![CDATA[We present a novel method for relation extraction (RE) from a single
sentence, mapping the sentence and two given entities to a canonical fact in a
knowledge graph (KG). Especially in this presumed sentential RE setting, the
context of a single sentence is often sparse. This paper introduces the KGPool
method to address this sparsity, dynamically expanding the context with
additional facts from the KG. It learns the representation of these facts
(entity alias, entity descriptions, etc.) using neural methods, supplementing
the sentential context. Unlike existing methods that statically use all
expanded facts, KGPool conditions this expansion on the sentence. We study the
efficacy of KGPool by evaluating it with different neural models and KGs
(Wikidata and NYT Freebase). Our experimental evaluation on standard datasets
shows that by feeding the KGPool representation into a Graph Neural Network,
the overall method is significantly more accurate than state-of-the-art
methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nadgeri_A/0/1/0/all/0/1"&gt;Abhishek Nadgeri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bastos_A/0/1/0/all/0/1"&gt;Anson Bastos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_K/0/1/0/all/0/1"&gt;Kuldeep Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mulang_I/0/1/0/all/0/1"&gt;Isaiah Onando Mulang&amp;#x27;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoffart_J/0/1/0/all/0/1"&gt;Johannes Hoffart&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shekarpour_S/0/1/0/all/0/1"&gt;Saeedeh Shekarpour&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saraswat_V/0/1/0/all/0/1"&gt;Vijay Saraswat&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Hop Fact Checking of Political Claims. (arXiv:2009.06401v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.06401</id>
        <link href="http://arxiv.org/abs/2009.06401"/>
        <updated>2021-06-02T02:49:59.376Z</updated>
        <summary type="html"><![CDATA[Recent work has proposed multi-hop models and datasets for studying complex
natural language reasoning. One notable task requiring multi-hop reasoning is
fact checking, where a set of connected evidence pieces leads to the final
verdict of a claim. However, existing datasets either do not provide
annotations for gold evidence pages, or the only dataset which does (FEVER)
mostly consists of claims which can be fact-checked with simple reasoning and
is constructed artificially. Here, we study more complex claim verification of
naturally occurring claims with multiple hops over interconnected evidence
chunks. We: 1) construct a small annotated dataset, PolitiHop, of evidence
sentences for claim verification; 2) compare it to existing multi-hop datasets;
and 3) study how to transfer knowledge from more extensive in- and
out-of-domain resources to PolitiHop. We find that the task is complex and
achieve the best performance with an architecture that specifically models
reasoning over evidence pieces in combination with in-domain transfer learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ostrowski_W/0/1/0/all/0/1"&gt;Wojciech Ostrowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arora_A/0/1/0/all/0/1"&gt;Arnav Arora&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Atanasova_P/0/1/0/all/0/1"&gt;Pepa Atanasova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Augenstein_I/0/1/0/all/0/1"&gt;Isabelle Augenstein&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Preview, Attend and Review: Schema-Aware Curriculum Learning for Multi-Domain Dialog State Tracking. (arXiv:2106.00291v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.00291</id>
        <link href="http://arxiv.org/abs/2106.00291"/>
        <updated>2021-06-02T02:49:59.356Z</updated>
        <summary type="html"><![CDATA[Existing dialog state tracking (DST) models are trained with dialog data in a
random order, neglecting rich structural information in a dataset. In this
paper, we propose to use curriculum learning (CL) to better leverage both the
curriculum structure and schema structure for task-oriented dialogs.
Specifically, we propose a model-agnostic framework called Schema-aware
Curriculum Learning for Dialog State Tracking (SaCLog), which consists of a
preview module that pre-trains a DST model with schema information, a
curriculum module that optimizes the model with CL, and a review module that
augments mispredicted data to reinforce the CL training. We show that our
proposed approach improves DST performance over both a transformer-based and
RNN-based DST model (TripPy and TRADE) and achieves new state-of-the-art
results on WOZ2.0 and MultiWOZ2.1.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1"&gt;Yinpei Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Hangyu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yongbin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1"&gt;Jian Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1"&gt;Fei Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1"&gt;Luo Si&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1"&gt;Xiaodan Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incorporating Visual Layout Structures for Scientific Text Classification. (arXiv:2106.00676v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.00676</id>
        <link href="http://arxiv.org/abs/2106.00676"/>
        <updated>2021-06-02T02:49:59.349Z</updated>
        <summary type="html"><![CDATA[Classifying the core textual components of a scientific paper-title, author,
body text, etc.-is a critical first step in automated scientific document
understanding. Previous work has shown how using elementary layout information,
i.e., each token's 2D position on the page, leads to more accurate
classification. We introduce new methods for incorporating VIsual LAyout
structures (VILA), e.g., the grouping of page texts into text lines or text
blocks, into language models to further improve performance. We show that the
I-VILA approach, which simply adds special tokens denoting boundaries between
layout structures into model inputs, can lead to +1~4.5 F1 Score improvements
in token classification tasks. Moreover, we design a hierarchical model H-VILA
that encodes these layout structures and record a up-to 70% efficiency boost
without hurting prediction accuracy. The experiments are conducted on a newly
curated evaluation suite, S2-VLUE, with a novel metric measuring VILA awareness
and a new dataset covering 19 scientific disciplines with gold annotations.
Pre-trained weights, benchmark datasets, and source code will be available at
https://github.com/allenai/VILA}{https://github.com/allenai/VILA.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1"&gt;Zejiang Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lo_K/0/1/0/all/0/1"&gt;Kyle Lo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Lucy Lu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kuehl_B/0/1/0/all/0/1"&gt;Bailey Kuehl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weld_D/0/1/0/all/0/1"&gt;Daniel S. Weld&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Downey_D/0/1/0/all/0/1"&gt;Doug Downey&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Keyphrase Generation. (arXiv:1704.06879v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1704.06879</id>
        <link href="http://arxiv.org/abs/1704.06879"/>
        <updated>2021-06-02T02:49:59.342Z</updated>
        <summary type="html"><![CDATA[Keyphrase provides highly-condensed information that can be effectively used
for understanding, organizing and retrieving text content. Though previous
studies have provided many workable solutions for automated keyphrase
extraction, they commonly divided the to-be-summarized content into multiple
text chunks, then ranked and selected the most meaningful ones. These
approaches could neither identify keyphrases that do not appear in the text,
nor capture the real semantic meaning behind the text. We propose a generative
model for keyphrase prediction with an encoder-decoder framework, which can
effectively overcome the above drawbacks. We name it as deep keyphrase
generation since it attempts to capture the deep semantic meaning of the
content with a deep learning method. Empirical analysis on six datasets
demonstrates that our proposed model not only achieves a significant
performance boost on extracting keyphrases that appear in the source text, but
also can generate absent keyphrases based on the semantic meaning of the text.
Code and dataset are available at
https://github.com/memray/OpenNMT-kpg-release.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Meng_R/0/1/0/all/0/1"&gt;Rui Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1"&gt;Sanqiang Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1"&gt;Shuguang Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1"&gt;Daqing He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brusilovsky_P/0/1/0/all/0/1"&gt;Peter Brusilovsky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chi_Y/0/1/0/all/0/1"&gt;Yu Chi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Validating GAN-BioBERT: A Methodology For Assessing Reporting Trends In Clinical Trials. (arXiv:2106.00665v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.00665</id>
        <link href="http://arxiv.org/abs/2106.00665"/>
        <updated>2021-06-02T02:49:59.313Z</updated>
        <summary type="html"><![CDATA[In the past decade, there has been much discussion about the issue of biased
reporting in clinical research. Despite this attention, there have been limited
tools developed for the systematic assessment of qualitative statements made in
clinical research, with most studies assessing qualitative statements relying
on the use of manual expert raters, which limits their size. Also, previous
attempts to develop larger scale tools, such as those using natural language
processing, were limited by both their accuracy and the number of categories
used for the classification of their findings. With these limitations in mind,
this study's goal was to develop a classification algorithm that was both
suitably accurate and finely grained to be applied on a large scale for
assessing the qualitative sentiment expressed in clinical trial abstracts.
Additionally, this study seeks to compare the performance of the proposed
algorithm, GAN-BioBERT, to previous studies as well as to expert manual rating
of clinical trial abstracts. This study develops a three-class sentiment
classification algorithm for clinical trial abstracts using a semi-supervised
natural language process model based on the Bidirectional Encoder
Representation from Transformers (BERT) model, from a series of clinical trial
abstracts annotated by a group of experts in academic medicine. Results: The
use of this algorithm was found to have a classification accuracy of 91.3%,
with a macro F1-Score of 0.92, which is a significant improvement in accuracy
when compared to previous methods and expert ratings, while also making the
sentiment classification finer grained than previous studies. The proposed
algorithm, GAN-BioBERT, is a suitable classification model for the large-scale
assessment of qualitative statements in clinical trial literature, providing an
accurate, reproducible tool for the large-scale study of clinical publication
trends.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Myszewski_J/0/1/0/all/0/1"&gt;Joshua J Myszewski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Klossowski_E/0/1/0/all/0/1"&gt;Emily Klossowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meyer_P/0/1/0/all/0/1"&gt;Patrick Meyer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bevil_K/0/1/0/all/0/1"&gt;Kristin Bevil&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Klesius_L/0/1/0/all/0/1"&gt;Lisa Klesius&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schroeder_K/0/1/0/all/0/1"&gt;Kristopher M Schroeder&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Highlight Timestamp Detection Model for Comedy Videos via Multimodal Sentiment Analysis. (arXiv:2106.00451v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2106.00451</id>
        <link href="http://arxiv.org/abs/2106.00451"/>
        <updated>2021-06-02T02:49:59.293Z</updated>
        <summary type="html"><![CDATA[Nowadays, the videos on the Internet are prevailing. The precise and in-depth
understanding of the videos is a difficult but valuable problem for both
platforms and researchers. The existing video understand models do well in
object recognition tasks but currently still cannot understand the abstract and
contextual features like highlight humor frames in comedy videos. The current
industrial works are also mainly focused on the basic category classification
task based on the appearances of objects. The feature detection methods for the
abstract category remains blank. A data structure that includes the information
of video frames, audio spectrum and texts provide a new direction to explore.
The multimodal models are proposed to make this in-depth video understanding
mission possible. In this paper, we analyze the difficulties in abstract
understanding of videos and propose a multimodal structure to obtain
state-of-the-art performance in this field. Then we select several benchmarks
for multimodal video understanding and apply the most suitable model to find
the best performance. At last, we evaluate the overall spotlights and drawbacks
of the models and methods in this paper and point out the possible directions
for further improvements.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1"&gt;Fan Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[More than just Frequency? Demasking Unsupervised Hypernymy Prediction Methods. (arXiv:2106.00055v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.00055</id>
        <link href="http://arxiv.org/abs/2106.00055"/>
        <updated>2021-06-02T02:49:59.272Z</updated>
        <summary type="html"><![CDATA[This paper presents a comparison of unsupervised methods of hypernymy
prediction (i.e., to predict which word in a pair of words such as fish-cod is
the hypernym and which the hyponym). Most importantly, we demonstrate across
datasets for English and for German that the predictions of three methods
(WeedsPrec, invCL, SLQS Row) strongly overlap and are highly correlated with
frequency-based predictions. In contrast, the second-order method SLQS shows an
overall lower accuracy but makes correct predictions where the others go wrong.
Our study once more confirms the general need to check the frequency bias of a
computational method in order to identify frequency-(un)related effects.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bott_T/0/1/0/all/0/1"&gt;Thomas Bott&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schlechtweg_D/0/1/0/all/0/1"&gt;Dominik Schlechtweg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Walde_S/0/1/0/all/0/1"&gt;Sabine Schulte im Walde&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Open Domain Dialogue Generation with Latent Images. (arXiv:2004.01981v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.01981</id>
        <link href="http://arxiv.org/abs/2004.01981"/>
        <updated>2021-06-02T02:49:59.254Z</updated>
        <summary type="html"><![CDATA[We consider grounding open domain dialogues with images. Existing work
assumes that both an image and a textual context are available, but
image-grounded dialogues by nature are more difficult to obtain than textual
dialogues. Thus, we propose learning a response generation model with both
image-grounded dialogues and textual dialogues by assuming that the visual
scene information at the time of a conversation can be represented by an image,
and trying to recover the latent images of the textual dialogues through
text-to-image generation techniques. The likelihood of the two types of
dialogues is then formulated by a response generator and an image reconstructor
that are learned within a conditional variational auto-encoding framework.
Empirical studies are conducted in both image-grounded conversation and
text-based conversation. In the first scenario, image-grounded dialogues,
especially under a low-resource setting, can be effectively augmented by
textual dialogues with latent images; while in the second scenario, latent
images can enrich the content of responses and at the same time keep them
relevant to contexts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Ze Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1"&gt;Wei Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1"&gt;Huang Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Can Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhoujun Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Exploratory Analysis of Multilingual Word-Level Quality Estimation with Cross-Lingual Transformers. (arXiv:2106.00143v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.00143</id>
        <link href="http://arxiv.org/abs/2106.00143"/>
        <updated>2021-06-02T02:49:59.230Z</updated>
        <summary type="html"><![CDATA[Most studies on word-level Quality Estimation (QE) of machine translation
focus on language-specific models. The obvious disadvantages of these
approaches are the need for labelled data for each language pair and the high
cost required to maintain several language-specific models. To overcome these
problems, we explore different approaches to multilingual, word-level QE. We
show that these QE models perform on par with the current language-specific
models. In the cases of zero-shot and few-shot QE, we demonstrate that it is
possible to accurately predict word-level quality for any given new language
pair from models trained on other language pairs. Our findings suggest that the
word-level QE models based on powerful pre-trained transformers that we propose
in this paper generalise well across languages, making them more useful in
real-world scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ranasinghe_T/0/1/0/all/0/1"&gt;Tharindu Ranasinghe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Orasan_C/0/1/0/all/0/1"&gt;Constantin Orasan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mitkov_R/0/1/0/all/0/1"&gt;Ruslan Mitkov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LenAtten: An Effective Length Controlling Unit For Text Summarization. (arXiv:2106.00316v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.00316</id>
        <link href="http://arxiv.org/abs/2106.00316"/>
        <updated>2021-06-02T02:49:59.198Z</updated>
        <summary type="html"><![CDATA[Fixed length summarization aims at generating summaries with a preset number
of words or characters. Most recent researches incorporate length information
with word embeddings as the input to the recurrent decoding unit, causing a
compromise between length controllability and summary quality. In this work, we
present an effective length controlling unit Length Attention (LenAtten) to
break this trade-off. Experimental results show that LenAtten not only brings
improvements in length controllability and ROGUE scores but also has great
generalization ability. In the task of generating a summary with the target
length, our model is 732 times better than the best-performing length
controllable summarizer in length controllability on the CNN/Daily Mail
dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1"&gt;Zhongyi Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Zhenghao Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1"&gt;Hao Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+XuanYuan_Z/0/1/0/all/0/1"&gt;Zhe XuanYuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fong_J/0/1/0/all/0/1"&gt;Jefferson Fong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_W/0/1/0/all/0/1"&gt;Weifeng Su&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multilingual Speech Translation with Unified Transformer: Huawei Noah's Ark Lab at IWSLT 2021. (arXiv:2106.00197v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.00197</id>
        <link href="http://arxiv.org/abs/2106.00197"/>
        <updated>2021-06-02T02:49:59.191Z</updated>
        <summary type="html"><![CDATA[This paper describes the system submitted to the IWSLT 2021 Multilingual
Speech Translation (MultiST) task from Huawei Noah's Ark Lab. We use a unified
transformer architecture for our MultiST model, so that the data from different
modalities (i.e., speech and text) and different tasks (i.e., Speech
Recognition, Machine Translation, and Speech Translation) can be exploited to
enhance the model's ability. Specifically, speech and text inputs are firstly
fed to different feature extractors to extract acoustic and textual features,
respectively. Then, these features are processed by a shared encoder--decoder
architecture. We apply several training techniques to improve the performance,
including multi-task learning, task-level curriculum learning, data
augmentation, etc. Our final system achieves significantly better results than
bilingual baselines on supervised language pairs and yields reasonable results
on zero-shot language pairs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1"&gt;Xingshan Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Liangyou Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1"&gt;Qun Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Automatic Hate Speech Detection with Multiword Expression Features. (arXiv:2106.00237v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.00237</id>
        <link href="http://arxiv.org/abs/2106.00237"/>
        <updated>2021-06-02T02:49:59.158Z</updated>
        <summary type="html"><![CDATA[The task of automatically detecting hate speech in social media is gaining
more and more attention. Given the enormous volume of content posted daily,
human monitoring of hate speech is unfeasible. In this work, we propose new
word-level features for automatic hate speech detection (HSD): multiword
expressions (MWEs). MWEs are lexical units greater than a word that have
idiomatic and compositional meanings. We propose to integrate MWE features in a
deep neural network-based HSD framework. Our baseline HSD system relies on
Universal Sentence Encoder (USE). To incorporate MWE features, we create a
three-branch deep neural network: one branch for USE, one for MWE categories,
and one for MWE embeddings. We conduct experiments on two hate speech tweet
corpora with different MWE categories and with two types of MWE embeddings,
word2vec and BERT. Our experiments demonstrate that the proposed HSD system
with MWE features significantly outperforms the baseline system in terms of
macro-F1.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zampieri_N/0/1/0/all/0/1"&gt;Nicolas Zampieri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Illina_I/0/1/0/all/0/1"&gt;Irina Illina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fohr_D/0/1/0/all/0/1"&gt;Dominique Fohr&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HERALD: An Annotation Efficient Method to Detect User Disengagement in Social Conversations. (arXiv:2106.00162v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.00162</id>
        <link href="http://arxiv.org/abs/2106.00162"/>
        <updated>2021-06-02T02:49:59.148Z</updated>
        <summary type="html"><![CDATA[Open-domain dialog systems have a user-centric goal: to provide humans with
an engaging conversation experience. User engagement is one of the most
important metrics for evaluating open-domain dialog systems, and could also be
used as real-time feedback to benefit dialog policy learning. Existing work on
detecting user disengagement typically requires hand-labeling many dialog
samples. We propose HERALD, an annotation efficient framework that reframes the
training data annotation process as a denoising problem. Specifically, instead
of manual labeling training samples, we first use a set of labeling heuristics
to automatically label training samples. We then denoise the weakly labeled
data using Shapley algorithm. Finally, we use the denoised data to train a user
engagement detector. Our experiments show that HERALD improves annotation
efficiency significantly and achieves 86% user disengagement detection accuracy
in two dialog corpora.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liang_W/0/1/0/all/0/1"&gt;Weixin Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_K/0/1/0/all/0/1"&gt;Kai-Hui Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1"&gt;Zhou Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dynamic Masking for Improved Stability in Spoken Language Translation. (arXiv:2006.00249v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.00249</id>
        <link href="http://arxiv.org/abs/2006.00249"/>
        <updated>2021-06-02T02:49:59.137Z</updated>
        <summary type="html"><![CDATA[For spoken language translation (SLT) in live scenarios such as conferences,
lectures and meetings, it is desirable to show the translation to the user as
quickly as possible, avoiding an annoying lag between speaker and translated
captions. In other words, we would like low-latency, online SLT. If we assume a
pipeline of automatic speech recognition (ASR) and machine translation (MT)
then a viable approach to online SLT is to pair an online ASR system, with a a
retranslation strategy, where the MT system re-translates every update received
from ASR. However this can result in annoying "flicker" as the MT system
updates its translation. A possible solution is to add a fixed delay, or "mask"
to the the output of the MT system, but a fixed global mask introduces
undesirable latency to the output. We show how this mask can be set
dynamically, improving the latency-flicker trade-off without sacrificing
translation quality.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1"&gt;Yuekun Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Haddow_B/0/1/0/all/0/1"&gt;Barry Haddow&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Corpus-Based Paraphrase Detection Experiments and Review. (arXiv:2106.00145v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.00145</id>
        <link href="http://arxiv.org/abs/2106.00145"/>
        <updated>2021-06-02T02:49:59.116Z</updated>
        <summary type="html"><![CDATA[Paraphrase detection is important for a number of applications, including
plagiarism detection, authorship attribution, question answering, text
summarization, text mining in general, etc. In this paper, we give a
performance overview of various types of corpus-based models, especially deep
learning (DL) models, with the task of paraphrase detection. We report the
results of eight models (LSI, TF-IDF, Word2Vec, Doc2Vec, GloVe, FastText, ELMO,
and USE) evaluated on three different public available corpora: Microsoft
Research Paraphrase Corpus, Clough and Stevenson and Webis Crowd Paraphrase
Corpus 2011. Through a great number of experiments, we decided on the most
appropriate approaches for text pre-processing: hyper-parameters, sub-model
selection-where they exist (e.g., Skipgram vs. CBOW), distance measures, and
semantic similarity/paraphrase detection threshold. Our findings and those of
other researchers who have used deep learning models show that DL models are
very competitive with traditional state-of-the-art approaches and have
potential that should be further developed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vrbanec_T/0/1/0/all/0/1"&gt;Tedo Vrbanec&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mestrovic_A/0/1/0/all/0/1"&gt;Ana Mestrovic&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Question-aware Transformer Models for Consumer Health Question Summarization. (arXiv:2106.00219v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.00219</id>
        <link href="http://arxiv.org/abs/2106.00219"/>
        <updated>2021-06-02T02:49:59.091Z</updated>
        <summary type="html"><![CDATA[Searching for health information online is becoming customary for more and
more consumers every day, which makes the need for efficient and reliable
question answering systems more pressing. An important contributor to the
success rates of these systems is their ability to fully understand the
consumers' questions. However, these questions are frequently longer than
needed and mention peripheral information that is not useful in finding
relevant answers. Question summarization is one of the potential solutions to
simplifying long and complex consumer questions before attempting to find an
answer. In this paper, we study the task of abstractive summarization for
real-world consumer health questions. We develop an abstractive question
summarization model that leverages the semantic interpretation of a question
via recognition of medical entities, which enables the generation of
informative summaries. Towards this, we propose multiple Cloze tasks (i.e. the
task of filing missing words in a given context) to identify the key medical
entities that enforce the model to have better coverage in question-focus
recognition. Additionally, we infuse the decoder inputs with question-type
information to generate question-type driven summaries. When evaluated on the
MeQSum benchmark corpus, our framework outperformed the state-of-the-art method
by 10.2 ROUGE-L points. We also conducted a manual evaluation to assess the
correctness of the generated summaries.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yadav_S/0/1/0/all/0/1"&gt;Shweta Yadav&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_D/0/1/0/all/0/1"&gt;Deepak Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abacha_A/0/1/0/all/0/1"&gt;Asma Ben Abacha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Demner_Fushman_D/0/1/0/all/0/1"&gt;Dina Demner-Fushman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[StarGAN-ZSVC: Towards Zero-Shot Voice Conversion in Low-Resource Contexts. (arXiv:2106.00043v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2106.00043</id>
        <link href="http://arxiv.org/abs/2106.00043"/>
        <updated>2021-06-02T02:49:59.074Z</updated>
        <summary type="html"><![CDATA[Voice conversion is the task of converting a spoken utterance from a source
speaker so that it appears to be said by a different target speaker while
retaining the linguistic content of the utterance. Recent advances have led to
major improvements in the quality of voice conversion systems. However, to be
useful in a wider range of contexts, voice conversion systems would need to be
(i) trainable without access to parallel data, (ii) work in a zero-shot setting
where both the source and target speakers are unseen during training, and (iii)
run in real time or faster. Recent techniques fulfil one or two of these
requirements, but not all three. This paper extends recent voice conversion
models based on generative adversarial networks (GANs), to satisfy all three of
these conditions. We specifically extend the recent StarGAN-VC model by
conditioning it on a speaker embedding (from a potentially unseen speaker).
This allows the model to be used in a zero-shot setting, and we therefore call
it StarGAN-ZSVC. We compare StarGAN-ZSVC against other voice conversion
techniques in a low-resource setting using a small 9-minute training set.
Compared to AutoVC -- another recent neural zero-shot approach -- we observe
that StarGAN-ZSVC gives small improvements in the zero-shot setting, showing
that real-time zero-shot voice conversion is possible even for a model trained
on very little data. Further work is required to see whether scaling up
StarGAN-ZSVC will also improve zero-shot voice conversion quality in
high-resource contexts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Baas_M/0/1/0/all/0/1"&gt;Matthew Baas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kamper_H/0/1/0/all/0/1"&gt;Herman Kamper&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Volta at SemEval-2021 Task 9: Statement Verification and Evidence Finding with Tables using TAPAS and Transfer Learning. (arXiv:2106.00248v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.00248</id>
        <link href="http://arxiv.org/abs/2106.00248"/>
        <updated>2021-06-02T02:49:59.065Z</updated>
        <summary type="html"><![CDATA[Tables are widely used in various kinds of documents to present information
concisely. Understanding tables is a challenging problem that requires an
understanding of language and table structure, along with numerical and logical
reasoning. In this paper, we present our systems to solve Task 9 of
SemEval-2021: Statement Verification and Evidence Finding with Tables
(SEM-TAB-FACTS). The task consists of two subtasks: (A) Given a table and a
statement, predicting whether the table supports the statement and (B)
Predicting which cells in the table provide evidence for/against the statement.
We fine-tune TAPAS (a model which extends BERT's architecture to capture
tabular structure) for both the subtasks as it has shown state-of-the-art
performance in various table understanding tasks. In subtask A, we evaluate how
transfer learning and standardizing tables to have a single header row improves
TAPAS' performance. In subtask B, we evaluate how different fine-tuning
strategies can improve TAPAS' performance. Our systems achieve an F1 score of
67.34 in subtask A three-way classification, 72.89 in subtask A two-way
classification, and 62.95 in subtask B.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gautam_D/0/1/0/all/0/1"&gt;Devansh Gautam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_K/0/1/0/all/0/1"&gt;Kshitij Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shrivastava_M/0/1/0/all/0/1"&gt;Manish Shrivastava&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SHUOWEN-JIEZI: Linguistically Informed Tokenizers For Chinese Language Model Pretraining. (arXiv:2106.00400v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.00400</id>
        <link href="http://arxiv.org/abs/2106.00400"/>
        <updated>2021-06-02T02:49:59.043Z</updated>
        <summary type="html"><![CDATA[Conventional tokenization methods for Chinese pretrained language models
(PLMs) treat each character as an indivisible token (Devlin et al., 2019),
which ignores the characteristics of the Chinese writing system. In this work,
we comprehensively study the influences of three main factors on the Chinese
tokenization for PLM: pronunciation, glyph (i.e., shape), and word boundary.
Correspondingly, we propose three kinds of tokenizers: 1) SHUOWEN (meaning Talk
Word), the pronunciation-based tokenizers; 2) JIEZI (meaning Solve Character),
the glyph-based tokenizers; 3) Word segmented tokenizers, the tokenizers with
Chinese word segmentation. To empirically compare the effectiveness of studied
tokenizers, we pretrain BERT-style language models with them and evaluate the
models on various downstream NLU tasks. We find that SHUOWEN and JIEZI
tokenizers can generally outperform conventional single-character tokenizers,
while Chinese word segmentation shows no benefit as a preprocessing step.
Moreover, the proposed SHUOWEN and JIEZI tokenizers exhibit significantly
better robustness in handling noisy texts. The code and pretrained models will
be publicly released to facilitate linguistically informed Chinese NLP.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Si_C/0/1/0/all/0/1"&gt;Chenglei Si&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhengyan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yingfa Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_F/0/1/0/all/0/1"&gt;Fanchao Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaozhi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhiyuan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1"&gt;Maosong Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DoT: An efficient Double Transformer for NLP tasks with tables. (arXiv:2106.00479v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.00479</id>
        <link href="http://arxiv.org/abs/2106.00479"/>
        <updated>2021-06-02T02:49:59.011Z</updated>
        <summary type="html"><![CDATA[Transformer-based approaches have been successfully used to obtain
state-of-the-art accuracy on natural language processing (NLP) tasks with
semi-structured tables. These model architectures are typically deep, resulting
in slow training and inference, especially for long inputs. To improve
efficiency while maintaining a high accuracy, we propose a new architecture,
DoT, a double transformer model, that decomposes the problem into two
sub-tasks: A shallow pruning transformer that selects the top-K tokens,
followed by a deep task-specific transformer that takes as input those K
tokens. Additionally, we modify the task-specific attention to incorporate the
pruning scores. The two transformers are jointly trained by optimizing the
task-specific loss. We run experiments on three benchmarks, including
entailment and question-answering. We show that for a small drop of accuracy,
DoT improves training and inference time by at least 50%. We also show that the
pruning transformer effectively selects relevant tokens enabling the end-to-end
model to maintain similar accuracy as slower baseline models. Finally, we
analyse the pruning and give some insight into its impact on the task model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Krichene_S/0/1/0/all/0/1"&gt;Syrine Krichene&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muller_T/0/1/0/all/0/1"&gt;Thomas M&amp;#xfc;ller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eisenschlos_J/0/1/0/all/0/1"&gt;Julian Martin Eisenschlos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Nora: The Well-Being Coach. (arXiv:2106.00410v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.00410</id>
        <link href="http://arxiv.org/abs/2106.00410"/>
        <updated>2021-06-02T02:49:58.901Z</updated>
        <summary type="html"><![CDATA[The current pandemic has forced people globally to remain in isolation and
practice social distancing, which creates the need for a system to combat the
resulting loneliness and negative emotions. In this paper we propose Nora, a
virtual coaching platform designed to utilize natural language understanding in
its dialogue system and suggest other recommendations based on user
interactions. It is intended to provide assistance and companionship to people
undergoing self-quarantine or work-from-home routines. Nora helps users gauge
their well-being by detecting and recording the user's emotion, sentiment, and
stress. Nora also recommends various workout, meditation, or yoga exercises to
users in support of developing a healthy daily routine. In addition, we provide
a social community inside Nora, where users can connect and share their
experiences with others undergoing a similar isolation procedure. Nora can be
accessed from anywhere via a web link and has support for both English and
Mandarin.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Winata_G/0/1/0/all/0/1"&gt;Genta Indra Winata&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lovenia_H/0/1/0/all/0/1"&gt;Holy Lovenia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ishii_E/0/1/0/all/0/1"&gt;Etsuko Ishii&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Siddique_F/0/1/0/all/0/1"&gt;Farhad Bin Siddique&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yongsheng Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fung_P/0/1/0/all/0/1"&gt;Pascale Fung&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HyperEmbed: Tradeoffs Between Resources and Performance in NLP Tasks with Hyperdimensional Computing enabled Embedding of n-gram Statistics. (arXiv:2003.01821v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.01821</id>
        <link href="http://arxiv.org/abs/2003.01821"/>
        <updated>2021-06-02T02:49:58.893Z</updated>
        <summary type="html"><![CDATA[Recent advances in Deep Learning have led to a significant performance
increase on several NLP tasks, however, the models become more and more
computationally demanding. Therefore, this paper tackles the domain of
computationally efficient algorithms for NLP tasks. In particular, it
investigates distributed representations of n-gram statistics of texts. The
representations are formed using hyperdimensional computing enabled embedding.
These representations then serve as features, which are used as input to
standard classifiers. We investigate the applicability of the embedding on one
large and three small standard datasets for classification tasks using nine
classifiers. The embedding achieved on par F1 scores while decreasing the time
and memory requirements by several times compared to the conventional n-gram
statistics, e.g., for one of the classifiers on a small dataset, the memory
reduction was 6.18 times; while train and test speed-ups were 4.62 and 3.84
times, respectively. For many classifiers on the large dataset, memory
reduction was ca. 100 times and train and test speed-ups were over 100 times.
Importantly, the usage of distributed representations formed via
hyperdimensional computing allows dissecting strict dependency between the
dimensionality of the representation and n-gram size, thus, opening a room for
tradeoffs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alonso_P/0/1/0/all/0/1"&gt;Pedro Alonso&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shridhar_K/0/1/0/all/0/1"&gt;Kumar Shridhar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kleyko_D/0/1/0/all/0/1"&gt;Denis Kleyko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Osipov_E/0/1/0/all/0/1"&gt;Evgeny Osipov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liwicki_M/0/1/0/all/0/1"&gt;Marcus Liwicki&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Coarse to Fine Question Answering System based on Reinforcement Learning. (arXiv:2106.00257v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.00257</id>
        <link href="http://arxiv.org/abs/2106.00257"/>
        <updated>2021-06-02T02:49:58.886Z</updated>
        <summary type="html"><![CDATA[In this paper, we present a coarse to fine question answering (CFQA) system
based on reinforcement learning which can efficiently processes documents with
different lengths by choosing appropriate actions. The system is designed using
an actor-critic based deep reinforcement learning model to achieve multi-step
question answering. Compared to previous QA models targeting on datasets mainly
containing either short or long documents, our multi-step coarse to fine model
takes the merits from multiple system modules, which can handle both short and
long documents. The system hence obtains a much better accuracy and faster
trainings speed compared to the current state-of-the-art models. We test our
model on four QA datasets, WIKEREADING, WIKIREADING LONG, CNN and SQuAD, and
demonstrate 1.3$\%$-1.7$\%$ accuracy improvements with 1.5x-3.4x training
speed-ups in comparison to the baselines using state-of-the-art models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1"&gt;Hongxia Jin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reinforced Iterative Knowledge Distillation for Cross-Lingual Named Entity Recognition. (arXiv:2106.00241v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.00241</id>
        <link href="http://arxiv.org/abs/2106.00241"/>
        <updated>2021-06-02T02:49:58.864Z</updated>
        <summary type="html"><![CDATA[Named entity recognition (NER) is a fundamental component in many
applications, such as Web Search and Voice Assistants. Although deep neural
networks greatly improve the performance of NER, due to the requirement of
large amounts of training data, deep neural networks can hardly scale out to
many languages in an industry setting. To tackle this challenge, cross-lingual
NER transfers knowledge from a rich-resource language to languages with low
resources through pre-trained multilingual language models. Instead of using
training data in target languages, cross-lingual NER has to rely on only
training data in source languages, and optionally adds the translated training
data derived from source languages. However, the existing cross-lingual NER
methods do not make good use of rich unlabeled data in target languages, which
is relatively easy to collect in industry applications. To address the
opportunities and challenges, in this paper we describe our novel practice in
Microsoft to leverage such large amounts of unlabeled data in target languages
in real production settings. To effectively extract weak supervision signals
from the unlabeled data, we develop a novel approach based on the ideas of
semi-supervised learning and reinforcement learning. The empirical study on
three benchmark data sets verifies that our approach establishes the new
state-of-the-art performance with clear edges. Now, the NER techniques reported
in this paper are on their way to become a fundamental component for Web
ranking, Entity Pane, Answers Triggering, and Question Answering in the
Microsoft Bing search engine. Moreover, our techniques will also serve as part
of the Spoken Language Understanding module for a commercial voice assistant.
We plan to open source the code of the prototype framework after deployment.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1"&gt;Shining Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_M/0/1/0/all/0/1"&gt;Ming Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pei_J/0/1/0/all/0/1"&gt;Jian Pei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shou_L/0/1/0/all/0/1"&gt;Linjun Shou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zuo_W/0/1/0/all/0/1"&gt;Wanli Zuo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zuo_X/0/1/0/all/0/1"&gt;Xianglin Zuo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1"&gt;Daxin Jiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring Dynamic Selection of Branch Expansion Orders for Code Generation. (arXiv:2106.00261v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.00261</id>
        <link href="http://arxiv.org/abs/2106.00261"/>
        <updated>2021-06-02T02:49:58.853Z</updated>
        <summary type="html"><![CDATA[Due to the great potential in facilitating software development, code
generation has attracted increasing attention recently. Generally, dominant
models are Seq2Tree models, which convert the input natural language
description into a sequence of tree-construction actions corresponding to the
pre-order traversal of an Abstract Syntax Tree (AST). However, such a traversal
order may not be suitable for handling all multi-branch nodes. In this paper,
we propose to equip the Seq2Tree model with a context-based Branch Selector,
which is able to dynamically determine optimal expansion orders of branches for
multi-branch nodes. Particularly, since the selection of expansion orders is a
non-differentiable multi-step operation, we optimize the selector through
reinforcement learning, and formulate the reward function as the difference of
model losses obtained through different expansion orders. Experimental results
and in-depth analysis on several commonly-used datasets demonstrate the
effectiveness and generality of our approach. We have released our code at
https://github.com/DeepLearnXMU/CG-RL.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1"&gt;Hui Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1"&gt;Chulun Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1"&gt;Fandong Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1"&gt;Biao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jie Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1"&gt;Degen Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1"&gt;Qingqiang Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1"&gt;Jinsong Su&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Volta at SemEval-2021 Task 6: Towards Detecting Persuasive Texts and Images using Textual and Multimodal Ensemble. (arXiv:2106.00240v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.00240</id>
        <link href="http://arxiv.org/abs/2106.00240"/>
        <updated>2021-06-02T02:49:58.845Z</updated>
        <summary type="html"><![CDATA[Memes are one of the most popular types of content used to spread information
online. They can influence a large number of people through rhetorical and
psychological techniques. The task, Detection of Persuasion Techniques in Texts
and Images, is to detect these persuasive techniques in memes. It consists of
three subtasks: (A) Multi-label classification using textual content, (B)
Multi-label classification and span identification using textual content, and
(C) Multi-label classification using visual and textual content. In this paper,
we propose a transfer learning approach to fine-tune BERT-based models in
different modalities. We also explore the effectiveness of ensembles of models
trained in different modalities. We achieve an F1-score of 57.0, 48.2, and 52.1
in the corresponding subtasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_K/0/1/0/all/0/1"&gt;Kshitij Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gautam_D/0/1/0/all/0/1"&gt;Devansh Gautam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mamidi_R/0/1/0/all/0/1"&gt;Radhika Mamidi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Distribution Matching for Rationalization. (arXiv:2106.00320v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.00320</id>
        <link href="http://arxiv.org/abs/2106.00320"/>
        <updated>2021-06-02T02:49:58.833Z</updated>
        <summary type="html"><![CDATA[The task of rationalization aims to extract pieces of input text as
rationales to justify neural network predictions on text classification tasks.
By definition, rationales represent key text pieces used for prediction and
thus should have similar classification feature distribution compared to the
original input text. However, previous methods mainly focused on maximizing the
mutual information between rationales and labels while neglecting the
relationship between rationales and input text. To address this issue, we
propose a novel rationalization method that matches the distributions of
rationales and input text in both the feature space and output space.
Empirically, the proposed distribution matching approach consistently
outperforms previous methods by a large margin. Our data and code are
available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yongfeng Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yujun Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1"&gt;Yulun Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Zhilin Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Formality Style Transfer with Context-Aware Rule Injection. (arXiv:2106.00210v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.00210</id>
        <link href="http://arxiv.org/abs/2106.00210"/>
        <updated>2021-06-02T02:49:58.809Z</updated>
        <summary type="html"><![CDATA[Models pre-trained on large-scale regular text corpora often do not work well
for user-generated data where the language styles differ significantly from the
mainstream text. Here we present Context-Aware Rule Injection (CARI), an
innovative method for formality style transfer (FST). CARI injects multiple
rules into an end-to-end BERT-based encoder and decoder model. It learns to
select optimal rules based on context. The intrinsic evaluation showed that
CARI achieved the new highest performance on the FST benchmark dataset. Our
extrinsic evaluation showed that CARI can greatly improve the regular
pre-trained models' performance on several tweet sentiment analysis tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1"&gt;Zonghai Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1"&gt;Hong Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ViTA: Visual-Linguistic Translation by Aligning Object Tags. (arXiv:2106.00250v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.00250</id>
        <link href="http://arxiv.org/abs/2106.00250"/>
        <updated>2021-06-02T02:49:58.754Z</updated>
        <summary type="html"><![CDATA[Multimodal Machine Translation (MMT) enriches the source text with visual
information for translation. It has gained popularity in recent years, and
several pipelines have been proposed in the same direction. Yet, the task lacks
quality datasets to illustrate the contribution of visual modality in the
translation systems. In this paper, we propose our system for the Multimodal
Translation Task of WAT 2021 from English to Hindi. We propose to use mBART, a
pretrained multilingual sequence-to-sequence model, for the textual-only
translations. Further, we bring the visual information to a textual domain by
extracting object tags from the image and enhance the input for the multimodal
task. We also explore the robustness of our system by systematically degrading
the source text. Finally, we achieve a BLEU score of 44.6 and 51.6 on the test
set and challenge set of the task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_K/0/1/0/all/0/1"&gt;Kshitij Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gautam_D/0/1/0/all/0/1"&gt;Devansh Gautam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mamidi_R/0/1/0/all/0/1"&gt;Radhika Mamidi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bringing Structure into Summaries: a Faceted Summarization Dataset for Long Scientific Documents. (arXiv:2106.00130v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.00130</id>
        <link href="http://arxiv.org/abs/2106.00130"/>
        <updated>2021-06-02T02:49:58.736Z</updated>
        <summary type="html"><![CDATA[Faceted summarization provides briefings of a document from different
perspectives. Readers can quickly comprehend the main points of a long document
with the help of a structured outline. However, little research has been
conducted on this subject, partially due to the lack of large-scale faceted
summarization datasets. In this study, we present FacetSum, a faceted
summarization benchmark built on Emerald journal articles, covering a diverse
range of domains. Different from traditional document-summary pairs, FacetSum
provides multiple summaries, each targeted at specific sections of a long
document, including the purpose, method, findings, and value. Analyses and
empirical results on our dataset reveal the importance of bringing structure
into summaries. We believe FacetSum will spur further advances in summarization
research and foster the development of NLP systems that can leverage the
structured information in both long texts and summaries.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Meng_R/0/1/0/all/0/1"&gt;Rui Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thaker_K/0/1/0/all/0/1"&gt;Khushboo Thaker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1"&gt;Yue Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1"&gt;Xingdi Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1"&gt;Tong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1"&gt;Daqing He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Compositional Learning of Image-Text Query for Image Retrieval. (arXiv:2006.11149v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.11149</id>
        <link href="http://arxiv.org/abs/2006.11149"/>
        <updated>2021-06-02T02:49:58.701Z</updated>
        <summary type="html"><![CDATA[In this paper, we investigate the problem of retrieving images from a
database based on a multi-modal (image-text) query. Specifically, the query
text prompts some modification in the query image and the task is to retrieve
images with the desired modifications. For instance, a user of an E-Commerce
platform is interested in buying a dress, which should look similar to her
friend's dress, but the dress should be of white color with a ribbon sash. In
this case, we would like the algorithm to retrieve some dresses with desired
modifications in the query dress. We propose an autoencoder based model,
ComposeAE, to learn the composition of image and text query for retrieving
images. We adopt a deep metric learning approach and learn a metric that pushes
composition of source image and text query closer to the target images. We also
propose a rotational symmetry constraint on the optimization problem. Our
approach is able to outperform the state-of-the-art method TIRG \cite{TIRG} on
three benchmark datasets, namely: MIT-States, Fashion200k and Fashion IQ. In
order to ensure fair comparison, we introduce strong baselines by enhancing
TIRG method. To ensure reproducibility of the results, we publish our code
here: \url{https://github.com/ecom-research/ComposeAE}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Anwaar_M/0/1/0/all/0/1"&gt;Muhammad Umer Anwaar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Labintcev_E/0/1/0/all/0/1"&gt;Egor Labintcev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kleinsteuber_M/0/1/0/all/0/1"&gt;Martin Kleinsteuber&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An In-depth Study on Internal Structure of Chinese Words. (arXiv:2106.00334v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.00334</id>
        <link href="http://arxiv.org/abs/2106.00334"/>
        <updated>2021-06-02T02:49:58.691Z</updated>
        <summary type="html"><![CDATA[Unlike English letters, Chinese characters have rich and specific meanings.
Usually, the meaning of a word can be derived from its constituent characters
in some way. Several previous works on syntactic parsing propose to annotate
shallow word-internal structures for better utilizing character-level
information. This work proposes to model the deep internal structures of
Chinese words as dependency trees with 11 labels for distinguishing syntactic
relationships. First, based on newly compiled annotation guidelines, we
manually annotate a word-internal structure treebank (WIST) consisting of over
30K multi-char words from Chinese Penn Treebank. To guarantee quality, each
word is independently annotated by two annotators and inconsistencies are
handled by a third senior annotator. Second, we present detailed and
interesting analysis on WIST to reveal insights on Chinese word formation.
Third, we propose word-internal structure parsing as a new task, and conduct
benchmark experiments using a competitive dependency parser. Finally, we
present two simple ways to encode word-internal structures, leading to
promising gains on the sentence-level syntactic parsing task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gong_C/0/1/0/all/0/1"&gt;Chen Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1"&gt;Saihao Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1"&gt;Houquan Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhenghua Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Min Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhefeng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huai_B/0/1/0/all/0/1"&gt;Baoxing Huai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_N/0/1/0/all/0/1"&gt;Nicholas Jing Yuan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Training ELECTRA Augmented with Multi-word Selection. (arXiv:2106.00139v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.00139</id>
        <link href="http://arxiv.org/abs/2106.00139"/>
        <updated>2021-06-02T02:49:58.680Z</updated>
        <summary type="html"><![CDATA[Pre-trained text encoders such as BERT and its variants have recently
achieved state-of-the-art performances on many NLP tasks. While being
effective, these pre-training methods typically demand massive computation
resources. To accelerate pre-training, ELECTRA trains a discriminator that
predicts whether each input token is replaced by a generator. However, this new
task, as a binary classification, is less semantically informative. In this
study, we present a new text encoder pre-training method that improves ELECTRA
based on multi-task learning. Specifically, we train the discriminator to
simultaneously detect replaced tokens and select original tokens from candidate
sets. We further develop two techniques to effectively combine all pre-training
tasks: (1) using attention-based networks for task-specific heads, and (2)
sharing bottom layers of the generator and the discriminator. Extensive
experiments on GLUE and SQuAD datasets demonstrate both the effectiveness and
the efficiency of our proposed method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1"&gt;Jiaming Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jialu Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tianqi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1"&gt;Cong Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1"&gt;Jiawei Han&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Discontinuous Named Entity Recognition as Maximal Clique Discovery. (arXiv:2106.00218v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.00218</id>
        <link href="http://arxiv.org/abs/2106.00218"/>
        <updated>2021-06-02T02:49:58.657Z</updated>
        <summary type="html"><![CDATA[Named entity recognition (NER) remains challenging when entity mentions can
be discontinuous. Existing methods break the recognition process into several
sequential steps. In training, they predict conditioned on the golden
intermediate results, while at inference relying on the model output of the
previous steps, which introduces exposure bias. To solve this problem, we first
construct a segment graph for each sentence, in which each node denotes a
segment (a continuous entity on its own, or a part of discontinuous entities),
and an edge links two nodes that belong to the same entity. The nodes and edges
can be generated respectively in one stage with a grid tagging scheme and
learned jointly using a novel architecture named Mac. Then discontinuous NER
can be reformulated as a non-parametric process of discovering maximal cliques
in the graph and concatenating the spans in each clique. Experiments on three
benchmarks show that our method outperforms the state-of-the-art (SOTA)
results, with up to 3.5 percentage points improvement on F1, and achieves 5x
speedup over the SOTA model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yucheng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1"&gt;Bowen Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1"&gt;Hongsong Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tingwen Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_N/0/1/0/all/0/1"&gt;Nan Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1"&gt;Limin Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Personalized News Recommendation with Knowledge-aware Interactive Matching. (arXiv:2104.10083v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.10083</id>
        <link href="http://arxiv.org/abs/2104.10083"/>
        <updated>2021-06-02T02:49:58.610Z</updated>
        <summary type="html"><![CDATA[The most important task in personalized news recommendation is accurate
matching between candidate news and user interest. Most of existing news
recommendation methods model candidate news from its textual content and user
interest from their clicked news in an independent way. However, a news article
may cover multiple aspects and entities, and a user usually has different kinds
of interest. Independent modeling of candidate news and user interest may lead
to inferior matching between news and users. In this paper, we propose a
knowledge-aware interactive matching method for news recommendation. Our method
interactively models candidate news and user interest to facilitate their
accurate matching. We design a knowledge-aware news co-encoder to interactively
learn representations for both clicked news and candidate news by capturing
their relatedness in both semantic and entities with the help of knowledge
graphs. We also design a user-news co-encoder to learn candidate news-aware
user interest representation and user-aware candidate news representation for
better interest matching. Experiments on two real-world datasets validate that
our method can effectively improve the performance of news recommendation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qi_T/0/1/0/all/0/1"&gt;Tao Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1"&gt;Fangzhao Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1"&gt;Chuhan Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yongfeng Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Wiki-Reliability: A Large Scale Dataset for Content Reliability on Wikipedia. (arXiv:2105.04117v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.04117</id>
        <link href="http://arxiv.org/abs/2105.04117"/>
        <updated>2021-06-02T02:49:58.602Z</updated>
        <summary type="html"><![CDATA[Wikipedia is the largest online encyclopedia, used by algorithms and web
users as a central hub of reliable information on the web. The quality and
reliability of Wikipedia content is maintained by a community of volunteer
editors. Machine learning and information retrieval algorithms could help scale
up editors' manual efforts around Wikipedia content reliability. However, there
is a lack of large-scale data to support the development of such research. To
fill this gap, in this paper, we propose Wiki-Reliability, the first dataset of
English Wikipedia articles annotated with a wide set of content reliability
issues. To build this dataset, we rely on Wikipedia "templates". Templates are
tags used by expert Wikipedia editors to indicate content issues, such as the
presence of "non-neutral point of view" or "contradictory articles", and serve
as a strong signal for detecting reliability issues in a revision. We select
the 10 most popular reliability-related templates on Wikipedia, and propose an
effective method to label almost 1M samples of Wikipedia article revisions as
positive or negative with respect to each template. Each positive/negative
example in the dataset comes with the full article text and 20 features from
the revision's metadata. We provide an overview of the possible downstream
tasks enabled by such data, and show that Wiki-Reliability can be used to train
large-scale models for content reliability prediction. We release all data and
code for public use.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wong_K/0/1/0/all/0/1"&gt;KayYen Wong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Redi_M/0/1/0/all/0/1"&gt;Miriam Redi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saez_Trumper_D/0/1/0/all/0/1"&gt;Diego Saez-Trumper&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Recent Advances in Deep Learning Based Dialogue Systems: A Systematic Survey. (arXiv:2105.04387v4 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.04387</id>
        <link href="http://arxiv.org/abs/2105.04387"/>
        <updated>2021-06-02T02:49:58.592Z</updated>
        <summary type="html"><![CDATA[Dialogue systems are a popular Natural Language Processing (NLP) task as it
is promising in real-life applications. It is also a complicated task since
many NLP tasks deserving study are involved. As a result, a multitude of novel
works on this task are carried out, and most of them are deep learning-based
due to the outstanding performance. In this survey, we mainly focus on the deep
learning-based dialogue systems. We comprehensively review state-of-the-art
research outcomes in dialogue systems and analyze them from two angles: model
type and system type. Specifically, from the angle of model type, we discuss
the principles, characteristics, and applications of different models that are
widely used in dialogue systems. This will help researchers acquaint these
models and see how they are applied in state-of-the-art frameworks, which is
rather helpful when designing a new dialogue system. From the angle of system
type, we discuss task-oriented and open-domain dialogue systems as two streams
of research, providing insight into the hot topics related. Furthermore, we
comprehensively review the evaluation methods and datasets for dialogue systems
to pave the way for future research. Finally, some possible research trends are
identified based on the recent research outcomes. To the best of our knowledge,
this survey is the most comprehensive and up-to-date one at present in the area
of dialogue systems and dialogue-related tasks, extensively covering the
popular frameworks, topics, and datasets.

Keywords: Dialogue Systems, Chatbots, Conversational AI, Task-oriented, Open
Domain, Chit-chat, Question Answering, Artificial Intelligence, Natural
Language Processing, Information Retrieval, Deep Learning, Neural Networks,
CNN, RNN, Hierarchical Recurrent Encoder-Decoder, Memory Networks, Attention,
Transformer, Pointer Net, CopyNet, Reinforcement Learning, GANs, Knowledge
Graph, Survey, Review]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ni_J/0/1/0/all/0/1"&gt;Jinjie Ni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Young_T/0/1/0/all/0/1"&gt;Tom Young&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pandelea_V/0/1/0/all/0/1"&gt;Vlad Pandelea&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xue_F/0/1/0/all/0/1"&gt;Fuzhao Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Adiga_V/0/1/0/all/0/1"&gt;Vinay Adiga&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cambria_E/0/1/0/all/0/1"&gt;Erik Cambria&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generating Query Focused Summaries from Query-Free Resources. (arXiv:2012.14774v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.14774</id>
        <link href="http://arxiv.org/abs/2012.14774"/>
        <updated>2021-06-02T02:49:58.570Z</updated>
        <summary type="html"><![CDATA[The availability of large-scale datasets has driven the development of neural
models that create generic summaries from single or multiple documents. In this
work we consider query focused summarization (QFS), a task for which training
data in the form of queries, documents, and summaries is not readily available.
We propose to decompose QFS into (1) query modeling (i.e., finding supportive
evidence within a set of documents for a query) and (2) conditional language
modeling (i.e., summary generation). We introduce MaRGE, a Masked ROUGE
Regression framework for evidence estimation and ranking which relies on a
unified representation for summaries and queries, so that summaries in generic
data can be converted into proxy queries for learning a query model.
Experiments across QFS benchmarks and query types show that our model achieves
state-of-the-art performance despite learning from weak supervision.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yumo Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lapata_M/0/1/0/all/0/1"&gt;Mirella Lapata&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploiting Group-level Behavior Pattern forSession-based Recommendation. (arXiv:2012.05422v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.05422</id>
        <link href="http://arxiv.org/abs/2012.05422"/>
        <updated>2021-06-02T02:49:58.560Z</updated>
        <summary type="html"><![CDATA[Session-based recommendation (SBR) is a challenging task, which aims to
predict users' future interests based on anonymous behavior sequences. Existing
methods leverage powerful representation learning approaches to encode sessions
into a low-dimensional space. However, despite such achievements, all the
existing studies focus on the instance-level session learning, while neglecting
the group-level users' preference, which is significant to model the users'
behavior. To this end, we propose a novel Repeat-aware Neural Mechanism for
Session-based Recommendation (RNMSR). In RNMSR, we propose to learn the user
preference from both instance-level and group-level, respectively: (i)
instance-level, which employs GNNs on a similarity-based item-pairwise session
graph to capture the users' preference in instance-level. (ii) group-level,
which converts sessions into group-level behavior patterns to model the
group-level users' preference. In RNMSR, we combine instance-level user
preference and group-level user preference to model the repeat consumption of
users, \ie whether users take repeated consumption and which items are
preferred by users. Extensive experiments are conducted on three real-world
datasets, \ie Diginetica, Yoochoose, and Nowplaying, demonstrating that the
proposed method consistently achieves state-of-the-art performance in all the
tests.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Ziyang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_W/0/1/0/all/0/1"&gt;Wei Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mao_X/0/1/0/all/0/1"&gt;Xian-Ling Mao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiao-Li Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1"&gt;Shanshan Feng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robustness of Meta Matrix Factorization Against Strict Privacy Constraints. (arXiv:2101.06927v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.06927</id>
        <link href="http://arxiv.org/abs/2101.06927"/>
        <updated>2021-06-02T02:49:58.490Z</updated>
        <summary type="html"><![CDATA[In this paper, we explore the reproducibility of MetaMF, a meta matrix
factorization framework introduced by Lin et al. MetaMF employs meta learning
for federated rating prediction to preserve users' privacy. We reproduce the
experiments of Lin et al. on five datasets, i.e., Douban, Hetrec-MovieLens,
MovieLens 1M, Ciao, and Jester. Also, we study the impact of meta learning on
the accuracy of MetaMF's recommendations. Furthermore, in our work, we
acknowledge that users may have different tolerances for revealing information
about themselves. Hence, in a second strand of experiments, we investigate the
robustness of MetaMF against strict privacy constraints. Our study illustrates
that we can reproduce most of Lin et al.'s results. Plus, we provide strong
evidence that meta learning is essential for MetaMF's robustness against strict
privacy constraints.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mullner_P/0/1/0/all/0/1"&gt;Peter M&amp;#xfc;llner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kowald_D/0/1/0/all/0/1"&gt;Dominik Kowald&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lex_E/0/1/0/all/0/1"&gt;Elisabeth Lex&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring Global Information for Session-based Recommendation. (arXiv:2011.10173v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.10173</id>
        <link href="http://arxiv.org/abs/2011.10173"/>
        <updated>2021-06-02T02:49:58.448Z</updated>
        <summary type="html"><![CDATA[Session-based recommendation (SBR) is a challenging task, which aims at
recommending items based on anonymous behavior sequences. Most existing SBR
studies model the user preferences based only on the current session while
neglecting the item-transition information from the other sessions, which
suffer from the inability of modeling the complicated item-transition pattern.
To address the limitations, we introduce global item-transition information to
strength the modeling of the dynamic item-transition. For fully exploiting the
global item-transition information, two ways of exploring global information
for SBR are studied in this work. Specifically, we first propose a basic
GNN-based framework (BGNN), which solely uses session-level item-transition
information on session graph. Based on BGNN, we propose a novel approach,
called Session-based Recommendation with Global Information (SRGI), which
infers the user preferences via fully exploring global item-transitions over
all sessions from two different perspectives: (i) Fusion-based Model (SRGI-FM),
which recursively incorporates the neighbor embeddings of each node on global
graph into the learning process of session level item representation; and (ii)
Constrained-based Model (SRGI-CM), which treats the global-level
item-transition information as a constraint to ensure the learned item
embeddings are consistent with the global item-transition. Extensive
experiments conducted on three popular benchmark datasets demonstrate that both
SRGI-FM and SRGI-CM outperform the state-of-the-art methods consistently.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Ziyang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_W/0/1/0/all/0/1"&gt;Wei Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cong_G/0/1/0/all/0/1"&gt;Gao Cong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiao-Li Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mao_X/0/1/0/all/0/1"&gt;Xian-Ling Mao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_M/0/1/0/all/0/1"&gt;Minghui Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1"&gt;Shanshan Feng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[WebMIaS on Docker: Deploying Math-Aware Search in a Single Line of Code. (arXiv:2106.00411v1 [cs.DL])]]></title>
        <id>http://arxiv.org/abs/2106.00411</id>
        <link href="http://arxiv.org/abs/2106.00411"/>
        <updated>2021-06-02T02:49:58.180Z</updated>
        <summary type="html"><![CDATA[Math informational retrieval (MIR) search engines are absent in the
wide-spread production use, even though documents in the STEM fields contain
many mathematical formulae, which are sometimes more important than text for
understanding. We have developed and open-sourced the WebMIaS MIR search engine
that has been successfully deployed in the European Digital Mathematics Library
(EuDML). However, its deployment is difficult to automate due to the complexity
of this task. Moreover, the solutions developed so far to tackle this challenge
are imperfect in terms of speed, maintenance, and robustness. In this paper, we
will describe the virtualization of WebMIaS using Docker that solves all three
problems and allows anyone to deploy containerized WebMIaS in a single line of
code. The publicly available Docker image will also help the community push the
development of math-aware search engines in the ARQMath workshop series.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Luptak_D/0/1/0/all/0/1"&gt;D&amp;#xe1;vid Lupt&amp;#xe1;k&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Novotny_V/0/1/0/all/0/1"&gt;V&amp;#xed;t Novotn&amp;#xfd;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stefanik_M/0/1/0/all/0/1"&gt;Michal &amp;#x160;tef&amp;#xe1;nik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sojka_P/0/1/0/all/0/1"&gt;Petr Sojka&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dual Graph enhanced Embedding Neural Network for CTRPrediction. (arXiv:2106.00314v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.00314</id>
        <link href="http://arxiv.org/abs/2106.00314"/>
        <updated>2021-06-02T02:49:58.122Z</updated>
        <summary type="html"><![CDATA[CTR prediction, which aims to estimate the probability that a user will click
an item, plays a crucial role in online advertising and recommender system.
Feature interaction modeling based and user interest mining based methods are
the two kinds of most popular techniques that have been extensively explored
for many years and have made great progress for CTR prediction. However, (1)
feature interaction based methods which rely heavily on the co-occurrence of
different features, may suffer from the feature sparsity problem (i.e., many
features appear few times); (2) user interest mining based methods which need
rich user behaviors to obtain user's diverse interests, are easy to encounter
the behavior sparsity problem (i.e., many users have very short behavior
sequences). To solve these problems, we propose a novel module named Dual Graph
enhanced Embedding, which is compatible with various CTR prediction models to
alleviate these two problems. We further propose a Dual Graph enhanced
Embedding Neural Network (DG-ENN) for CTR prediction. Dual Graph enhanced
Embedding exploits the strengths of graph representation with two carefully
designed learning strategies (divide-and-conquer, curriculum-learning-inspired
organized learning) to refine the embedding. We conduct comprehensive
experiments on three real-world industrial datasets. The experimental results
show that our proposed DG-ENN significantly outperforms state-of-the-art CTR
prediction models. Moreover, when applying to state-of-the-art CTR prediction
models, Dual graph enhanced embedding always obtains better performance.
Further case studies prove that our proposed dual graph enhanced embedding
could alleviate the feature sparsity and behavior sparsity problems. Our
framework will be open-source based on MindSpore in the near future.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1"&gt;Wei Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_R/0/1/0/all/0/1"&gt;Rong Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_R/0/1/0/all/0/1"&gt;Renhao Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1"&gt;Huifeng Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yingxue Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhirong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_R/0/1/0/all/0/1"&gt;Ruiming Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1"&gt;Xiuqiang He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NewsEmbed: Modeling News through Pre-trained DocumentRepresentations. (arXiv:2106.00590v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2106.00590</id>
        <link href="http://arxiv.org/abs/2106.00590"/>
        <updated>2021-06-02T02:49:58.104Z</updated>
        <summary type="html"><![CDATA[Effectively modeling text-rich fresh content such as news articles at
document-level is a challenging problem. To ensure a content-based model
generalize well to a broad range of applications, it is critical to have a
training dataset that is large beyond the scale of human labels while achieving
desired quality. In this work, we address those two challenges by proposing a
novel approach to mine semantically-relevant fresh documents, and their topic
labels, with little human supervision. Meanwhile, we design a multitask model
called NewsEmbed that alternatively trains a contrastive learning with a
multi-label classification to derive a universal document encoder. We show that
the proposed approach can provide billions of high quality organic training
examples and can be naturally extended to multilingual setting where texts in
different languages are encoded in the same semantic space. We experimentally
demonstrate NewsEmbed's competitive performance across multiple natural
language understanding tasks, both supervised and unsupervised.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jialu Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tianqi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1"&gt;Cong Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[One4all User Representation for Recommender Systems in E-commerce. (arXiv:2106.00573v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.00573</id>
        <link href="http://arxiv.org/abs/2106.00573"/>
        <updated>2021-06-02T02:49:58.086Z</updated>
        <summary type="html"><![CDATA[General-purpose representation learning through large-scale pre-training has
shown promising results in the various machine learning fields. For an
e-commerce domain, the objective of general-purpose, i.e., one for all,
representations would be efficient applications for extensive downstream tasks
such as user profiling, targeting, and recommendation tasks. In this paper, we
systematically compare the generalizability of two learning strategies, i.e.,
transfer learning through the proposed model, ShopperBERT, vs. learning from
scratch. ShopperBERT learns nine pretext tasks with 79.2M parameters from 0.8B
user behaviors collected over two years to produce user embeddings. As a
result, the MLPs that employ our embedding method outperform more complex
models trained from scratch for five out of six tasks. Specifically, the
pre-trained embeddings have superiority over the task-specific supervised
features and the strong baselines, which learn the auxiliary dataset for the
cold-start problem. We also show the computational efficiency and embedding
visualization of the pre-trained features.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shin_K/0/1/0/all/0/1"&gt;Kyuyong Shin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kwak_H/0/1/0/all/0/1"&gt;Hanock Kwak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1"&gt;Kyung-Min Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1"&gt;Minkyu Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_Y/0/1/0/all/0/1"&gt;Young-Jin Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jeong_J/0/1/0/all/0/1"&gt;Jisu Jeong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jung_S/0/1/0/all/0/1"&gt;Seungjae Jung&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[End-to-End Mispronunciation Detection and Diagnosis From Raw Waveforms. (arXiv:2103.03023v4 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.03023</id>
        <link href="http://arxiv.org/abs/2103.03023"/>
        <updated>2021-06-02T02:49:58.062Z</updated>
        <summary type="html"><![CDATA[Mispronunciation detection and diagnosis (MDD) is designed to identify
pronunciation errors and provide instructive feedback to guide non-native
language learners, which is a core component in computer-assisted pronunciation
training (CAPT) systems. However, MDD often suffers from the data-sparsity
problem due to that collecting non-native data and the associated annotations
is time-consuming and labor-intensive. To address this issue, we explore a
fully end-to-end (E2E) neural model for MDD, which processes learners' speech
directly based on raw waveforms. Compared to conventional hand-crafted acoustic
features, raw waveforms retain more acoustic phenomena and potentially can help
neural networks discover better and more customized representations. To this
end, our MDD model adopts a co-called SincNet module to take input a raw
waveform and covert it to a suitable vector representation sequence. SincNet
employs the cardinal sine (sinc) function to implement learnable bandpass
filters, drawing inspiration from the convolutional neural network (CNN). By
comparison to CNN, SincNet has fewer parameters and is more amenable to human
interpretation. Extensive experiments are conducted on the L2-ARCTIC dataset,
which is a publicly-available non-native English speech corpus compiled for
research on CAPT. We find that the sinc filters of SincNet can be adapted
quickly for non-native language learners of different nationalities.
Furthermore, our model can achieve comparable mispronunciation detection
performance in relation to state-of-the-art E2E MDD models that take input the
standard handcrafted acoustic features. Besides that, our model also provides
considerable improvements on phone error rate (PER) and diagnosis accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Yan_B/0/1/0/all/0/1"&gt;Bi-Cheng Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chen_B/0/1/0/all/0/1"&gt;Berlin Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FBAdTracker: An Interactive Data Collection and Analysis Tool for Facebook Advertisements. (arXiv:2106.00142v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.00142</id>
        <link href="http://arxiv.org/abs/2106.00142"/>
        <updated>2021-06-02T02:49:57.951Z</updated>
        <summary type="html"><![CDATA[The growing use of social media has led to drastic changes in our
decision-making. Especially, Facebook offers marketing API which promotes
business to target potential groups who are likely to consume their items.
However, this service can be abused by malicious advertisers who attempt to
deceive people by disinformation such as propaganda and divisive opinion. To
counter this problem, we introduce a new application named FBAdTracker. The
purpose of this application is to provide an integrated data collection and
analysis system for current research on fact-checking related to Facebook
advertisements. Our system is capable of monitoring up-to-date Facebook ads and
analyzing ads retrieved from Facebook Ads Library.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jeong_U/0/1/0/all/0/1"&gt;Ujun Jeong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_K/0/1/0/all/0/1"&gt;Kaize Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Huan Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Controllable Gradient Item Retrieval. (arXiv:2106.00062v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.00062</id>
        <link href="http://arxiv.org/abs/2106.00062"/>
        <updated>2021-06-02T02:49:57.928Z</updated>
        <summary type="html"><![CDATA[In this paper, we identify and study an important problem of gradient item
retrieval. We define the problem as retrieving a sequence of items with a
gradual change on a certain attribute, given a reference item and a
modification text. For example, after a customer saw a white dress, she/he
wants to buy a similar one but more floral on it. The extent of "more floral"
is subjective, thus prompting one floral dress is hard to satisfy the
customer's needs. A better way is to present a sequence of products with
increasingly floral attributes based on the white dress, and allow the customer
to select the most satisfactory one from the sequence. Existing item retrieval
methods mainly focus on whether the target items appear at the top of the
retrieved sequence, but ignore the demand for retrieving a sequence of products
with gradual change on a certain attribute. To deal with this problem, we
propose a weakly-supervised method that can learn a disentangled item
representation from user-item interaction data and ground the semantic meaning
of attributes to dimensions of the item representation. Our method takes a
reference item and a modification as a query. During inference, we start from
the reference item and "walk" along the direction of the modification in the
item representation space to retrieve a sequence of items in a gradient manner.
We demonstrate our proposed method can achieve disentanglement through weak
supervision. Besides, we empirically show that an item sequence retrieved by
our method is gradually changed on an indicated attribute and, in the item
retrieval task, our method outperforms existing approaches on three different
datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Haonan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1"&gt;Chang Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1"&gt;Carl Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Hongxia Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1"&gt;Jingrui He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Harvesting the Public MeSH Note field. (arXiv:2106.00302v1 [cs.DL])]]></title>
        <id>http://arxiv.org/abs/2106.00302</id>
        <link href="http://arxiv.org/abs/2106.00302"/>
        <updated>2021-06-02T02:49:57.905Z</updated>
        <summary type="html"><![CDATA[In this document, we report an analysis of the Public MeSH Note field of the
new descriptors introduced in the MeSH thesaurus between 2006 and 2020. The aim
of this analysis was to extract information about the previous status of these
new descriptors as Supplementary Concept Records. The Public MeSH Note field
contains information in semi-structured text, meant to be read by humans.
Therefore, we adopted a semi-automated approach, based on regular expressions,
to extract information from it. In the large majority of cases, we managed to
minimize the required manual effort for extracting the previous state of a new
descriptor as a Supplementary Concept Record. The source code for this analysis
is openly available on GitHub.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nentidis_A/0/1/0/all/0/1"&gt;Anastasios Nentidis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krithara_A/0/1/0/all/0/1"&gt;Anastasia Krithara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsoumakas_G/0/1/0/all/0/1"&gt;Grigorios Tsoumakas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paliouras_G/0/1/0/all/0/1"&gt;Georgios Paliouras&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Cold-start Problem: Minimal Users' Activity Estimation. (arXiv:2106.00102v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2106.00102</id>
        <link href="http://arxiv.org/abs/2106.00102"/>
        <updated>2021-06-02T02:49:57.827Z</updated>
        <summary type="html"><![CDATA[Cold-start problem, which arises upon the new users arrival, is one of the
fundamental problems in today's recommender approaches. Moreover, in some
domains as TV or multime-dia-items take long time to experience by users, thus
users usually do not provide rich preference information. In this paper we
analyze the minimal amount of ratings needs to be done by a user over a set of
items, in order to solve or reduce the cold-start problem. In our analysis we
applied clustering data mining technique in order to identify minimal amount of
item's ratings required from recommender system's users, in order to be
assigned to a correct cluster. In this context, cluster quality is being
monitored and in case of reaching certain cluster quality threshold, the
rec-ommender system could start to generate recommendations for given user, as
in this point cold-start problem is considered as resolved. Our proposed
approach is applicable to any domain in which user preferences are received
based on explicit items rating. Our experiments are performed within the movie
and jokes recommendation domain using the MovieLens and Jester dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Visnovsky_J/0/1/0/all/0/1"&gt;Juraj Visnovsky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kassak_O/0/1/0/all/0/1"&gt;Ondrej Kassak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kompan_M/0/1/0/all/0/1"&gt;Michal Kompan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bielikova_M/0/1/0/all/0/1"&gt;Maria Bielikova&lt;/a&gt;</name>
        </author>
    </entry>
</feed>