<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://looperxx.github.io/ArxivDaily/index.html</id>
    <title>ArxivDaily</title>
    <updated>2021-08-09T00:49:28.950Z</updated>
    <generator>osmosfeed 1.11.0</generator>
    <link rel="alternate" href="https://looperxx.github.io/ArxivDaily/index.html"/>
    <link rel="self" href="https://looperxx.github.io/ArxivDaily/feed.atom"/>
    <entry>
        <title type="html"><![CDATA[Quantum-inspired event reconstruction with Tensor Networks: Matrix Product States. (arXiv:2106.08334v2 [hep-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.08334</id>
        <link href="http://arxiv.org/abs/2106.08334"/>
        <updated>2021-08-09T00:49:28.895Z</updated>
        <summary type="html"><![CDATA[Tensor Networks are non-trivial representations of high-dimensional tensors,
originally designed to describe quantum many-body systems. We show that Tensor
Networks are ideal vehicles to connect quantum mechanical concepts to machine
learning techniques, thereby facilitating an improved interpretability of
neural networks. This study presents the discrimination of top quark signal
over QCD background processes using a Matrix Product State classifier. We show
that entanglement entropy can be used to interpret what a network learns, which
can be used to reduce the complexity of the network and feature space without
loss of generality or performance. For the optimisation of the network, we
compare the Density Matrix Renormalization Group (DMRG) algorithm to stochastic
gradient descent (SGD) and propose a joined training algorithm to harness the
explainability of DMRG with the efficiency of SGD.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/hep-ph/1/au:+Araz_J/0/1/0/all/0/1"&gt;Jack Y. Araz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-ph/1/au:+Spannowsky_M/0/1/0/all/0/1"&gt;Michael Spannowsky&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HASI: Hardware-Accelerated Stochastic Inference, A Defense Against Adversarial Machine Learning Attacks. (arXiv:2106.05825v3 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.05825</id>
        <link href="http://arxiv.org/abs/2106.05825"/>
        <updated>2021-08-09T00:49:28.877Z</updated>
        <summary type="html"><![CDATA[Deep Neural Networks (DNNs) are employed in an increasing number of
applications, some of which are safety critical. Unfortunately, DNNs are known
to be vulnerable to so-called adversarial attacks that manipulate inputs to
cause incorrect results that can be beneficial to an attacker or damaging to
the victim. Multiple defenses have been proposed to increase the robustness of
DNNs. In general, these defenses have high overhead, some require
attack-specific re-training of the model or careful tuning to adapt to
different attacks.

This paper presents HASI, a hardware-accelerated defense that uses a process
we call stochastic inference to detect adversarial inputs. We show that by
carefully injecting noise into the model at inference time, we can
differentiate adversarial inputs from benign ones. HASI uses the output
distribution characteristics of noisy inference compared to a non-noisy
reference to detect adversarial inputs. We show an adversarial detection rate
of 86% when applied to VGG16 and 93% when applied to ResNet50, which exceeds
the detection rate of the state of the art approaches, with a much lower
overhead. We demonstrate two software/hardware-accelerated co-designs, which
reduces the performance impact of stochastic inference to 1.58X-2X relative to
the unprotected baseline, compared to 15X-20X overhead for a software-only GPU
implementation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Samavatian_M/0/1/0/all/0/1"&gt;Mohammad Hossein Samavatian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Majumdar_S/0/1/0/all/0/1"&gt;Saikat Majumdar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barber_K/0/1/0/all/0/1"&gt;Kristin Barber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Teodorescu_R/0/1/0/all/0/1"&gt;Radu Teodorescu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning-based Framework for Sensor Fault-Tolerant Building HVAC Control with Model-assisted Learning. (arXiv:2106.14144v2 [eess.SY] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.14144</id>
        <link href="http://arxiv.org/abs/2106.14144"/>
        <updated>2021-08-09T00:49:28.870Z</updated>
        <summary type="html"><![CDATA[As people spend up to 87% of their time indoors, intelligent Heating,
Ventilation, and Air Conditioning (HVAC) systems in buildings are essential for
maintaining occupant comfort and reducing energy consumption. These HVAC
systems in smart buildings rely on real-time sensor readings, which in practice
often suffer from various faults and could also be vulnerable to malicious
attacks. Such faulty sensor inputs may lead to the violation of indoor
environment requirements (e.g., temperature, humidity, etc.) and the increase
of energy consumption. While many model-based approaches have been proposed in
the literature for building HVAC control, it is costly to develop accurate
physical models for ensuring their performance and even more challenging to
address the impact of sensor faults. In this work, we present a novel
learning-based framework for sensor fault-tolerant HVAC control, which includes
three deep learning based components for 1) generating temperature proposals
with the consideration of possible sensor faults, 2) selecting one of the
proposals based on the assessment of their accuracy, and 3) applying
reinforcement learning with the selected temperature proposal. Moreover, to
address the challenge of training data insufficiency in building-related tasks,
we propose a model-assisted learning method leveraging an abstract model of
building physical dynamics. Through extensive experiments, we demonstrate that
the proposed fault-tolerant HVAC control framework can significantly reduce
building temperature violations under a variety of sensor fault patterns while
maintaining energy efficiency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Xu_S/0/1/0/all/0/1"&gt;Shichao Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Fu_Y/0/1/0/all/0/1"&gt;Yangyang Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yixuan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+ONeill_Z/0/1/0/all/0/1"&gt;Zheng O&amp;#x27;Neill&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhu_Q/0/1/0/all/0/1"&gt;Qi Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Good and Bad Optimization Models: Insights from Rockafellians. (arXiv:2105.06073v3 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.06073</id>
        <link href="http://arxiv.org/abs/2105.06073"/>
        <updated>2021-08-09T00:49:28.863Z</updated>
        <summary type="html"><![CDATA[A basic requirement for a mathematical model is often that its solution
(output) shouldn't change much if the model's parameters (input) are perturbed.
This is important because the exact values of parameters may not be known and
one would like to avoid being mislead by an output obtained using incorrect
values. Thus, it's rarely enough to address an application by formulating a
model, solving the resulting optimization problem and presenting the solution
as the answer. One would need to confirm that the model is suitable, i.e.,
"good," and this can, at least in part, be achieved by considering a family of
optimization problems constructed by perturbing parameters of concern. The
resulting sensitivity analysis uncovers troubling situations with unstable
solutions, which we referred to as "bad" models, and indicates better model
formulations. Embedding an actual problem of interest within a family of
problems is also a primary path to optimality conditions as well as
computationally attractive, alternative problems, which under ideal
circumstances, and when properly tuned, may even furnish the minimum value of
the actual problem. The tuning of these alternative problems turns out to be
intimately tied to finding multipliers in optimality conditions and thus
emerges as a main component of several optimization algorithms. In fact, the
tuning amounts to solving certain dual optimization problems. In this tutorial,
we'll discuss the opportunities and insights afforded by this broad
perspective.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Royset_J/0/1/0/all/0/1"&gt;Johannes O. Royset&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generalized Tensor Summation Compressive Sensing Network (GTSNET): An Easy to Learn Compressive Sensing Operation. (arXiv:2108.03167v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2108.03167</id>
        <link href="http://arxiv.org/abs/2108.03167"/>
        <updated>2021-08-09T00:49:28.856Z</updated>
        <summary type="html"><![CDATA[In CS literature, the efforts can be divided into two groups: finding a
measurement matrix that preserves the compressed information at the maximum
level, and finding a reconstruction algorithm for the compressed information.
In the traditional CS setup, the measurement matrices are selected as random
matrices, and optimization-based iterative solutions are used to recover the
signals. However, when we handle large signals, using random matrices become
cumbersome especially when it comes to iterative optimization-based solutions.
Even though recent deep learning-based solutions boost the reconstruction
accuracy performance while speeding up the recovery, still jointly learning the
whole measurement matrix is a difficult process. In this work, we introduce a
separable multi-linear learning of the CS matrix by representing it as the
summation of arbitrary number of tensors. For a special case where the CS
operation is set as a single tensor multiplication, the model is reduced to the
learning-based separable CS; while a dense CS matrix can be approximated and
learned as the summation of multiple tensors. Both cases can be used in CS of
two or multi-dimensional signals e.g., images, multi-spectral images, videos,
etc. Structural CS matrices can also be easily approximated and learned in our
multi-linear separable learning setup with structural tensor sum
representation. Hence, our learnable generalized tensor summation CS operation
encapsulates most CS setups including separable CS, non-separable CS
(traditional vector-matrix multiplication), structural CS, and CS of the
multi-dimensional signals. For both gray-scale and RGB images, the proposed
scheme surpasses most state-of-the-art solutions, especially in lower
measurement rates. Although the performance gain remains limited from tensor to
the sum of tensor representation for gray-scale images, it becomes significant
in the RGB case.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Yamac_M/0/1/0/all/0/1"&gt;Mehmet Yamac&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Akpinar_U/0/1/0/all/0/1"&gt;Ugur Akpinar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sahin_E/0/1/0/all/0/1"&gt;Erdem Sahin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kiranyaz_S/0/1/0/all/0/1"&gt;Serkan Kiranyaz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gabbouj_M/0/1/0/all/0/1"&gt;Moncef Gabbouj&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Zero-Round Active Learning. (arXiv:2107.06703v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.06703</id>
        <link href="http://arxiv.org/abs/2107.06703"/>
        <updated>2021-08-09T00:49:28.849Z</updated>
        <summary type="html"><![CDATA[Active learning (AL) aims at reducing labeling effort by identifying the most
valuable unlabeled data points from a large pool. Traditional AL frameworks
have two limitations: First, they perform data selection in a multi-round
manner, which is time-consuming and impractical. Second, they usually assume
that there are a small amount of labeled data points available in the same
domain as the data in the unlabeled pool. Recent work proposes a solution for
one-round active learning based on data utility learning and optimization,
which fixes the first issue but still requires the initially labeled data
points in the same domain. In this paper, we propose $\mathrm{D^2ULO}$ as a
solution that solves both issues. Specifically, $\mathrm{D^2ULO}$ leverages the
idea of domain adaptation (DA) to train a data utility model which can
effectively predict the utility for any given unlabeled data in the target
domain once labeled. The trained data utility model can then be used to select
high-utility data and at the same time, provide an estimate for the utility of
the selected data. Our algorithm does not rely on any feedback from annotators
in the target domain and hence, can be used to perform zero-round active
learning or warm-start existing multi-round active learning strategies. Our
experiments show that $\mathrm{D^2ULO}$ outperforms the existing
state-of-the-art AL strategies equipped with domain adaptation over various
domain shift settings (e.g., real-to-real data and synthetic-to-real data).
Particularly, $\mathrm{D^2ULO}$ is applicable to the scenario where source and
target labels have mismatches, which is not supported by the existing works.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Si Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1"&gt;Tianhao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1"&gt;Ruoxi Jia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural networks for Anatomical Therapeutic Chemical (ATC) classification. (arXiv:2101.11713v3 [q-bio.QM] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.11713</id>
        <link href="http://arxiv.org/abs/2101.11713"/>
        <updated>2021-08-09T00:49:28.831Z</updated>
        <summary type="html"><![CDATA[Motivation: Automatic Anatomical Therapeutic Chemical (ATC) classification is
a critical and highly competitive area of research in bioinformatics because of
its potential for expediting drug develop-ment and research. Predicting an
unknown compound's therapeutic and chemical characteristics ac-cording to how
these characteristics affect multiple organs/systems makes automatic ATC
classifica-tion a challenging multi-label problem. Results: In this work, we
propose combining multiple multi-label classifiers trained on distinct sets of
features, including sets extracted from a Bidirectional Long Short-Term Memory
Network (BiLSTM). Experiments demonstrate the power of this approach, which is
shown to outperform the best methods reported in the literature, including the
state-of-the-art developed by the fast.ai research group. Availability: All
source code developed for this study is available at
https://github.com/LorisNanni. Contact: loris.nanni@unipd.it]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Nanni_L/0/1/0/all/0/1"&gt;Loris Nanni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Lumini_A/0/1/0/all/0/1"&gt;Alessandra Lumini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Brahnam_S/0/1/0/all/0/1"&gt;Sheryl Brahnam&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Clustering and attention model based for intelligent trading. (arXiv:2107.06782v2 [q-fin.ST] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.06782</id>
        <link href="http://arxiv.org/abs/2107.06782"/>
        <updated>2021-08-09T00:49:28.824Z</updated>
        <summary type="html"><![CDATA[The foreign exchange market has taken an important role in the global
financial market. While foreign exchange trading brings high-yield
opportunities to investors, it also brings certain risks. Since the
establishment of the foreign exchange market in the 20th century, foreign
exchange rate forecasting has become a hot issue studied by scholars from all
over the world. Due to the complexity and number of factors affecting the
foreign exchange market, technical analysis cannot respond to administrative
intervention or unexpected events. Our team chose several pairs of foreign
currency historical data and derived technical indicators from 2005 to 2021 as
the dataset and established different machine learning models for event-driven
price prediction for oversold scenario.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-fin/1/au:+Rana_M/0/1/0/all/0/1"&gt;Mimansa Rana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Mao_N/0/1/0/all/0/1"&gt;Nanxiang Mao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Ao_M/0/1/0/all/0/1"&gt;Ming Ao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Wu_X/0/1/0/all/0/1"&gt;Xiaohui Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Liang_P/0/1/0/all/0/1"&gt;Poning Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Khushi_M/0/1/0/all/0/1"&gt;Matloob Khushi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Designing Good Representation Learning Models. (arXiv:2107.05948v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.05948</id>
        <link href="http://arxiv.org/abs/2107.05948"/>
        <updated>2021-08-09T00:49:28.812Z</updated>
        <summary type="html"><![CDATA[The goal of representation learning is different from the ultimate objective
of machine learning such as decision making, it is therefore very difficult to
establish clear and direct objectives for training representation learning
models. It has been argued that a good representation should disentangle the
underlying variation factors, yet how to translate this into training
objectives remains unknown. This paper presents an attempt to establish direct
training criterions and design principles for developing good representation
learning models. We propose that a good representation learning model should be
maximally expressive, i.e., capable of distinguishing the maximum number of
input configurations. We formally define expressiveness and introduce the
maximum expressiveness (MEXS) theorem of a general learning model. We propose
to train a model by maximizing its expressiveness while at the same time
incorporating general priors such as model smoothness. We present a conscience
competitive learning algorithm which encourages the model to reach its MEXS
whilst at the same time adheres to model smoothness prior. We also introduce a
label consistent training (LCT) technique to boost model smoothness by
encouraging it to assign consistent labels to similar samples. We present
extensive experimental results to show that our method can indeed design
representation learning models capable of developing representations that are
as good as or better than state of the art. We also show that our technique is
computationally efficient, robust against different parameter settings and can
work effectively on a variety of datasets. Code available at
https://github.com/qlilx/odgrlm.git]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1"&gt;Qinglin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Garibaldi_J/0/1/0/all/0/1"&gt;Jonathan M Garibaldi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_G/0/1/0/all/0/1"&gt;Guoping Qiu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DNN-HMM based Speaker Adaptive Emotion Recognition using Proposed Epoch and MFCC Features. (arXiv:1806.00984v1 [cs.SD] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/1806.00984</id>
        <link href="http://arxiv.org/abs/1806.00984"/>
        <updated>2021-08-09T00:49:28.805Z</updated>
        <summary type="html"><![CDATA[Speech is produced when time varying vocal tract system is excited with time
varying excitation source. Therefore, the information present in a speech such
as message, emotion, language, speaker is due to the combined effect of both
excitation source and vocal tract system. However, there is very less
utilization of excitation source features to recognize emotion. In our earlier
work, we have proposed a novel method to extract glottal closure instants
(GCIs) known as epochs. In this paper, we have explored epoch features namely
instantaneous pitch, phase and strength of epochs for discriminating emotions.
We have combined the excitation source features and the well known
Male-frequency cepstral coefficient (MFCC) features to develop an emotion
recognition system with improved performance. DNN-HMM speaker adaptive models
have been developed using MFCC, epoch and combined features. IEMOCAP emotional
database has been used to evaluate the models. The average accuracy for emotion
recognition system when using MFCC and epoch features separately is 59.25% and
54.52% respectively. The recognition performance improves to 64.2% when MFCC
and epoch features are combined.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fahad_M/0/1/0/all/0/1"&gt;Md. Shah Fahad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yadav_J/0/1/0/all/0/1"&gt;Jainath Yadav&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pradhan_G/0/1/0/all/0/1"&gt;Gyadhar Pradhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deepak_A/0/1/0/all/0/1"&gt;Akshay Deepak&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Synthetic Benchmarks for Scientific Research in Explainable Machine Learning. (arXiv:2106.12543v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.12543</id>
        <link href="http://arxiv.org/abs/2106.12543"/>
        <updated>2021-08-09T00:49:28.797Z</updated>
        <summary type="html"><![CDATA[As machine learning models grow more complex and their applications become
more high-stakes, tools for explaining model predictions have become
increasingly important. This has spurred a flurry of research in model
explainability and has given rise to feature attribution methods such as LIME
and SHAP. Despite their widespread use, evaluating and comparing different
feature attribution methods remains challenging: evaluations ideally require
human studies, and empirical evaluation metrics are often data-intensive or
computationally prohibitive on real-world datasets. In this work, we address
this issue by releasing XAI-Bench: a suite of synthetic datasets along with a
library for benchmarking feature attribution algorithms. Unlike real-world
datasets, synthetic datasets allow the efficient computation of conditional
expected values that are needed to evaluate ground-truth Shapley values and
other metrics. The synthetic datasets we release offer a wide variety of
parameters that can be configured to simulate real-world data. We demonstrate
the power of our library by benchmarking popular explainability techniques
across several evaluation metrics and across a variety of settings. The
versatility and efficiency of our library will help researchers bring their
explainability methods from development to deployment. Our code is available at
https://github.com/abacusai/xai-bench.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khandagale_S/0/1/0/all/0/1"&gt;Sujay Khandagale&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+White_C/0/1/0/all/0/1"&gt;Colin White&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neiswanger_W/0/1/0/all/0/1"&gt;Willie Neiswanger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Responding to Illegal Activities Along the Canadian Coastlines Using Reinforcement Learning. (arXiv:2108.03169v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2108.03169</id>
        <link href="http://arxiv.org/abs/2108.03169"/>
        <updated>2021-08-09T00:49:28.775Z</updated>
        <summary type="html"><![CDATA[This article elaborates on how machine learning (ML) can leverage the
solution of a contemporary problem related to the security of maritime domains.
The worldwide ``Illegal, Unreported, and Unregulated'' (IUU) fishing incidents
have led to serious environmental and economic consequences which involve
drastic changes in our ecosystems in addition to financial losses caused by the
depletion of natural resources. The Fisheries and Aquatic Department (FAD) of
the United Nation's Food and Agriculture Organization (FAO) issued a report
which indicated that the annual losses due to IUU fishing reached $25 Billion.
This imposes negative impacts on the future-biodiversity of the marine
ecosystem and domestic Gross National Product (GNP). Hence, robust interception
mechanisms are increasingly needed for detecting and pursuing the unrelenting
illegal fishing incidents in maritime territories. This article addresses the
problem of coordinating the motion of a fleet of marine vessels (pursuers) to
catch an IUU vessel while still in local waters. The problem is formulated as a
pursuer-evader problem that is tackled within an ML framework. One or more
pursuers, such as law enforcement vessels, intercept an evader (i.e., the
illegal fishing ship) using an online reinforcement learning mechanism that is
based on a value iteration process. It employs real-time navigation
measurements of the evader ship as well as those of the pursuing vessels and
returns back model-free interception strategies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Abouheaf_M/0/1/0/all/0/1"&gt;Mohammed Abouheaf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Qu_S/0/1/0/all/0/1"&gt;Shuzheng Qu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gueaieb_W/0/1/0/all/0/1"&gt;Wail Gueaieb&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Abielmona_R/0/1/0/all/0/1"&gt;Rami Abielmona&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Harb_M/0/1/0/all/0/1"&gt;Moufid Harb&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Noise Reduction in X-ray Photon Correlation Spectroscopy with Convolutional Neural Networks Encoder-Decoder Models. (arXiv:2102.03877v2 [cond-mat.mtrl-sci] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.03877</id>
        <link href="http://arxiv.org/abs/2102.03877"/>
        <updated>2021-08-09T00:49:28.767Z</updated>
        <summary type="html"><![CDATA[Like other experimental techniques, X-ray Photon Correlation Spectroscopy is
subject to various kinds of noise. Random and correlated fluctuations and
heterogeneities can be present in a two-time correlation function and obscure
the information about the intrinsic dynamics of a sample. Simultaneously
addressing the disparate origins of noise in the experimental data is
challenging. We propose a computational approach for improving the
signal-to-noise ratio in two-time correlation functions that is based on
Convolutional Neural Network Encoder-Decoder (CNN-ED) models. Such models
extract features from an image via convolutional layers, project them to a low
dimensional space and then reconstruct a clean image from this reduced
representation via transposed convolutional layers. Not only are ED models a
general tool for random noise removal, but their application to low
signal-to-noise data can enhance the data quantitative usage since they are
able to learn the functional form of the signal. We demonstrate that the CNN-ED
models trained on real-world experimental data help to effectively extract
equilibrium dynamics parameters from two-time correlation functions, containing
statistical noise and dynamic heterogeneities. Strategies for optimizing the
models performance and their applicability limits are discussed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cond-mat/1/au:+Konstantinova_T/0/1/0/all/0/1"&gt;Tatiana Konstantinova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Wiegart_L/0/1/0/all/0/1"&gt;Lutz Wiegart&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Rakitin_M/0/1/0/all/0/1"&gt;Maksim Rakitin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+DeGennaro_A/0/1/0/all/0/1"&gt;Anthony M. DeGennaro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Barbour_A/0/1/0/all/0/1"&gt;Andi M. Barbour&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Federated Learning with Attack-Adaptive Aggregation. (arXiv:2102.05257v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.05257</id>
        <link href="http://arxiv.org/abs/2102.05257"/>
        <updated>2021-08-09T00:49:28.759Z</updated>
        <summary type="html"><![CDATA[Federated learning is vulnerable to various attacks, such as model poisoning
and backdoor attacks, even if some existing defense strategies are used. To
address this challenge, we propose an attack-adaptive aggregation strategy to
defend against various attacks for robust federated learning. The proposed
approach is based on training a neural network with an attention mechanism that
learns the vulnerability of federated learning models from a set of plausible
attacks. To the best of our knowledge, our aggregation strategy is the first
one that can be adapted to defend against various attacks in a data-driven
fashion. Our approach has achieved competitive performance in defending model
poisoning and backdoor attacks in federated learning tasks on image and text
datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wan_C/0/1/0/all/0/1"&gt;Ching Pui Wan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1"&gt;Qifeng Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Latent Programmer: Discrete Latent Codes for Program Synthesis. (arXiv:2012.00377v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.00377</id>
        <link href="http://arxiv.org/abs/2012.00377"/>
        <updated>2021-08-09T00:49:28.752Z</updated>
        <summary type="html"><![CDATA[In many sequence learning tasks, such as program synthesis and document
summarization, a key problem is searching over a large space of possible output
sequences. We propose to learn representations of the outputs that are
specifically meant for search: rich enough to specify the desired output but
compact enough to make search more efficient. Discrete latent codes are
appealing for this purpose, as they naturally allow sophisticated combinatorial
search strategies. The latent codes are learned using a self-supervised
learning principle, in which first a discrete autoencoder is trained on the
output sequences, and then the resulting latent codes are used as intermediate
targets for the end-to-end sequence prediction task. Based on these insights,
we introduce the \emph{Latent Programmer}, a program synthesis method that
first predicts a discrete latent code from input/output examples, and then
generates the program in the target language. We evaluate the Latent Programmer
on two domains: synthesis of string transformation programs, and generation of
programs from natural language descriptions. We demonstrate that the discrete
latent representation significantly improves synthesis accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hong_J/0/1/0/all/0/1"&gt;Joey Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dohan_D/0/1/0/all/0/1"&gt;David Dohan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1"&gt;Rishabh Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sutton_C/0/1/0/all/0/1"&gt;Charles Sutton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zaheer_M/0/1/0/all/0/1"&gt;Manzil Zaheer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Approximate Gradient Coding with Optimal Decoding. (arXiv:2006.09638v4 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.09638</id>
        <link href="http://arxiv.org/abs/2006.09638"/>
        <updated>2021-08-09T00:49:28.745Z</updated>
        <summary type="html"><![CDATA[In distributed optimization problems, a technique called gradient coding,
which involves replicating data points, has been used to mitigate the effect of
straggling machines. Recent work has studied approximate gradient coding, which
concerns coding schemes where the replication factor of the data is too low to
recover the full gradient exactly. Our work is motivated by the challenge of
creating approximate gradient coding schemes that simultaneously work well in
both the adversarial and stochastic models. To that end, we introduce novel
approximate gradient codes based on expander graphs, in which each machine
receives exactly two blocks of data points. We analyze the decoding error both
in the random and adversarial straggler setting, when optimal decoding
coefficients are used. We show that in the random setting, our schemes achieve
an error to the gradient that decays exponentially in the replication factor.
In the adversarial setting, the error is nearly a factor of two smaller than
any existing code with similar performance in the random setting. We show
convergence bounds both in the random and adversarial setting for gradient
descent under standard assumptions using our codes. In the random setting, our
convergence rate improves upon block-box bounds. In the adversarial setting, we
show that gradient descent can converge down to a noise floor that scales
linearly with the adversarial error to the gradient. We demonstrate empirically
that our schemes achieve near-optimal error in the random setting and converge
faster than algorithms which do not use the optimal decoding coefficients.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Glasgow_M/0/1/0/all/0/1"&gt;Margalit Glasgow&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wootters_M/0/1/0/all/0/1"&gt;Mary Wootters&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Breaking the Deadly Triad with a Target Network. (arXiv:2101.08862v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.08862</id>
        <link href="http://arxiv.org/abs/2101.08862"/>
        <updated>2021-08-09T00:49:28.725Z</updated>
        <summary type="html"><![CDATA[The deadly triad refers to the instability of a reinforcement learning
algorithm when it employs off-policy learning, function approximation, and
bootstrapping simultaneously. In this paper, we investigate the target network
as a tool for breaking the deadly triad, providing theoretical support for the
conventional wisdom that a target network stabilizes training. We first propose
and analyze a novel target network update rule which augments the commonly used
Polyak-averaging style update with two projections. We then apply the target
network and ridge regularization in several divergent algorithms and show their
convergence to regularized TD fixed points. Those algorithms are off-policy
with linear function approximation and bootstrapping, spanning both policy
evaluation and control, as well as both discounted and average-reward settings.
In particular, we provide the first convergent linear $Q$-learning algorithms
under nonrestrictive and changing behavior policies without bi-level
optimization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shangtong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_H/0/1/0/all/0/1"&gt;Hengshuai Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Whiteson_S/0/1/0/all/0/1"&gt;Shimon Whiteson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimizing Large-Scale Fleet Management on a Road Network using Multi-Agent Deep Reinforcement Learning with Graph Neural Network. (arXiv:2011.06175v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.06175</id>
        <link href="http://arxiv.org/abs/2011.06175"/>
        <updated>2021-08-09T00:49:28.717Z</updated>
        <summary type="html"><![CDATA[We propose a novel approach to optimize fleet management by combining
multi-agent reinforcement learning with graph neural network. To provide
ride-hailing service, one needs to optimize dynamic resources and demands over
spatial domain. While the spatial structure was previously approximated with a
regular grid, our approach represents the road network with a graph, which
better reflects the underlying geometric structure. Dynamic resource allocation
is formulated as multi-agent reinforcement learning, whose action-value
function (Q function) is approximated with graph neural networks. We use
stochastic policy update rule over the graph with deep Q-networks (DQN), and
achieve superior results over the greedy policy update. We design a realistic
simulator that emulates the empirical taxi call data, and confirm the
effectiveness of the proposed model under various conditions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1"&gt;Juhyeon Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1"&gt;Kihyun Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PSD2 Explainable AI Model for Credit Scoring. (arXiv:2011.10367v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.10367</id>
        <link href="http://arxiv.org/abs/2011.10367"/>
        <updated>2021-08-09T00:49:28.710Z</updated>
        <summary type="html"><![CDATA[The aim of this project is to develop and test advanced analytical methods to
improve the prediction accuracy of Credit Risk Models, preserving at the same
time the model interpretability. In particular, the project focuses on applying
an explainable machine learning model to bank-related databases. The input data
were obtained from open data. Over the total proven models, CatBoost has shown
the highest performance. The algorithm implementation produces a GINI of 0.68
after tuning the hyper-parameters. SHAP package is used to provide a global and
local interpretation of the model predictions to formulate a
human-comprehensive approach to understanding the decision-maker algorithm. The
20 most important features are selected using the Shapley values to present a
full human-understandable model that reveals how the attributes of an
individual are related to its model prediction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Torrent_N/0/1/0/all/0/1"&gt;Neus Llop Torrent&lt;/a&gt; (1 and 2), &lt;a href="http://arxiv.org/find/cs/1/au:+Visani_G/0/1/0/all/0/1"&gt;Giorgio Visani&lt;/a&gt; (2 and 3), &lt;a href="http://arxiv.org/find/cs/1/au:+Bagli_E/0/1/0/all/0/1"&gt;Enrico Bagli&lt;/a&gt; (2) ((1) Politecnico di Milano Graduate School of Business, (2) CRIF S.p.A, (3) University of Bologna School of Informatics and Engineering)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine learning for surface prediction in ACTS. (arXiv:2108.03068v1 [physics.ins-det])]]></title>
        <id>http://arxiv.org/abs/2108.03068</id>
        <link href="http://arxiv.org/abs/2108.03068"/>
        <updated>2021-08-09T00:49:28.703Z</updated>
        <summary type="html"><![CDATA[We present an ongoing R&D activity for machine-learning-assisted navigation
through detectors to be used for track reconstruction. We investigate different
approaches of training neural networks for surface prediction and compare their
results. This work is carried out in the context of the ACTS tracking toolkit.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Huth_B/0/1/0/all/0/1"&gt;Benjamin Huth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Salzburger_A/0/1/0/all/0/1"&gt;Andreas Salzburger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Wettig_T/0/1/0/all/0/1"&gt;Tilo Wettig&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Replica Analysis of the Linear Model with Markov or Hidden Markov Signal Priors. (arXiv:2009.13370v3 [cs.IT] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.13370</id>
        <link href="http://arxiv.org/abs/2009.13370"/>
        <updated>2021-08-09T00:49:28.696Z</updated>
        <summary type="html"><![CDATA[This paper estimates free energy, average mutual information, and minimum
mean square error (MMSE) of a linear model under two assumptions: (1) the
source is generated by a Markov chain, (2) the source is generated via a hidden
Markov model. Our estimates are based on the replica method in statistical
physics. We show that under the posterior mean estimator, the linear model with
Markov sources or hidden Markov sources is decoupled into single-input AWGN
channels with state information available at both encoder and decoder where the
state distribution follows the left Perron-Frobenius eigenvector with unit
Manhattan norm of the stochastic matrix of Markov chains. Numerical results
show that the free energies and MSEs obtained via the replica method are
closely approximate to their counterparts achieved by the Metropolis-Hastings
algorithm or some well-known approximate message passing algorithms in the
research literature.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Truong_L/0/1/0/all/0/1"&gt;Lan V. Truong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RockGPT: Reconstructing three-dimensional digital rocks from single two-dimensional slice from the perspective of video generation. (arXiv:2108.03132v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2108.03132</id>
        <link href="http://arxiv.org/abs/2108.03132"/>
        <updated>2021-08-09T00:49:28.677Z</updated>
        <summary type="html"><![CDATA[Random reconstruction of three-dimensional (3D) digital rocks from
two-dimensional (2D) slices is crucial for elucidating the microstructure of
rocks and its effects on pore-scale flow in terms of numerical modeling, since
massive samples are usually required to handle intrinsic uncertainties. Despite
remarkable advances achieved by traditional process-based methods, statistical
approaches and recently famous deep learning-based models, few works have
focused on producing several kinds of rocks with one trained model and allowing
the reconstructed samples to satisfy certain given properties, such as
porosity. To fill this gap, we propose a new framework, named RockGPT, which is
composed of VQ-VAE and conditional GPT, to synthesize 3D samples based on a
single 2D slice from the perspective of video generation. The VQ-VAE is
utilized to compress high-dimensional input video, i.e., the sequence of
continuous rock slices, to discrete latent codes and reconstruct them. In order
to obtain diverse reconstructions, the discrete latent codes are modeled using
conditional GPT in an autoregressive manner, while incorporating conditional
information from a given slice, rock type, and porosity. We conduct two
experiments on five kinds of rocks, and the results demonstrate that RockGPT
can produce different kinds of rocks with the same model, and the reconstructed
samples can successfully meet certain specified porosities. In a broader sense,
through leveraging the proposed conditioning scheme, RockGPT constitutes an
effective way to build a general model to produce multiple kinds of rocks
simultaneously that also satisfy user-defined properties.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zheng_Q/0/1/0/all/0/1"&gt;Qiang Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_D/0/1/0/all/0/1"&gt;Dongxiao Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Federated Learning for Physical Layer Design. (arXiv:2102.11777v2 [eess.SP] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.11777</id>
        <link href="http://arxiv.org/abs/2102.11777"/>
        <updated>2021-08-09T00:49:28.669Z</updated>
        <summary type="html"><![CDATA[Model-free techniques, such as machine learning (ML), have recently attracted
much interest towards the physical layer design, e.g., symbol detection,
channel estimation, and beamforming. Most of these ML techniques employ
centralized learning (CL) schemes and assume the availability of datasets at a
parameter server (PS), demanding the transmission of data from edge devices,
such as mobile phones, to the PS. Exploiting the data generated at the edge,
federated learning (FL) has been proposed recently as a distributed learning
scheme, in which each device computes the model parameters and sends them to
the PS for model aggregation while the datasets are kept intact at the edge.
Thus, FL is more communication-efficient and privacy-preserving than CL and
applicable to the wireless communication scenarios, wherein the data are
generated at the edge devices. This article presents the recent advances in
FL-based training for physical layer design problems. Compared to CL, the
effectiveness of FL is presented in terms of communication overhead with a
slight performance loss in the learning accuracy. The design challenges, such
as model, data, and hardware complexity, are also discussed in detail along
with possible solutions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Elbir_A/0/1/0/all/0/1"&gt;Ahmet M. Elbir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Papazafeiropoulos_A/0/1/0/all/0/1"&gt;Anastasios K. Papazafeiropoulos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chatzinotas_S/0/1/0/all/0/1"&gt;Symeon Chatzinotas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Stochastic Deep Model Reference Adaptive Control. (arXiv:2108.03120v1 [eess.SY])]]></title>
        <id>http://arxiv.org/abs/2108.03120</id>
        <link href="http://arxiv.org/abs/2108.03120"/>
        <updated>2021-08-09T00:49:28.663Z</updated>
        <summary type="html"><![CDATA[In this paper, we present a Stochastic Deep Neural Network-based Model
Reference Adaptive Control. Building on our work "Deep Model Reference Adaptive
Control", we extend the controller capability by using Bayesian deep neural
networks (DNN) to represent uncertainties and model non-linearities. Stochastic
Deep Model Reference Adaptive Control uses a Lyapunov-based method to adapt the
output-layer weights of the DNN model in real-time, while a data-driven
supervised learning algorithm is used to update the inner-layers parameters.
This asynchronous network update ensures boundedness and guaranteed tracking
performance with a learning-based real-time feedback controller. A Bayesian
approach to DNN learning helped avoid over-fitting the data and provide
confidence intervals over the predictions. The controller's stochastic nature
also ensured "Induced Persistency of excitation," leading to convergence of the
overall system signal.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Joshi_G/0/1/0/all/0/1"&gt;Girish Joshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chowdhary_G/0/1/0/all/0/1"&gt;Girish Chowdhary&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DriveML: An R Package for Driverless Machine Learning. (arXiv:2005.00478v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.00478</id>
        <link href="http://arxiv.org/abs/2005.00478"/>
        <updated>2021-08-09T00:49:28.656Z</updated>
        <summary type="html"><![CDATA[In recent years, the concept of automated machine learning has become very
popular. Automated Machine Learning (AutoML) mainly refers to the automated
methods for model selection and hyper-parameter optimization of various
algorithms such as random forests, gradient boosting, neural networks, etc. In
this paper, we introduce a new package i.e. DriveML for automated machine
learning. DriveML helps in implementing some of the pillars of an automated
machine learning pipeline such as automated data preparation, feature
engineering, model building and model explanation by running the function
instead of writing lengthy R codes. The DriveML package is available in CRAN.
We compare the DriveML package with other relevant packages in CRAN/Github and
find that DriveML performs the best across different parameters. We also
provide an illustration by applying the DriveML package with default
configuration on a real world dataset. Overall, the main benefits of DriveML
are in development time savings, reduce developer's errors, optimal tuning of
machine learning models and reproducibility.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Putatunda_S/0/1/0/all/0/1"&gt;Sayan Putatunda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ubrangala_D/0/1/0/all/0/1"&gt;Dayananda Ubrangala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rama_K/0/1/0/all/0/1"&gt;Kiran Rama&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kondapalli_R/0/1/0/all/0/1"&gt;Ravi Kondapalli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Adversarial Robustness: A Neural Architecture Search perspective. (arXiv:2007.08428v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.08428</id>
        <link href="http://arxiv.org/abs/2007.08428"/>
        <updated>2021-08-09T00:49:28.648Z</updated>
        <summary type="html"><![CDATA[Adversarial robustness of deep learning models has gained much traction in
the last few years. Various attacks and defenses are proposed to improve the
adversarial robustness of modern-day deep learning architectures. While all
these approaches help improve the robustness, one promising direction for
improving adversarial robustness is un-explored, i.e., the complex topology of
the neural network architecture. In this work, we answer the following
question: "Can the complex topology of a neural network give adversarial
robustness without any form of adversarial training?" empirically by
experimenting with different hand-crafted and NAS based architectures. Our
findings show that, for small-scale attacks, NAS-based architectures are more
robust for small-scale datasets and simple tasks than hand-crafted
architectures. However, as the dataset's size or the task's complexity
increase, hand-crafted architectures are more robust than NAS-based
architectures. We perform the first large scale study to understand adversarial
robustness purely from an architectural perspective. Our results show that
random sampling in the search space of DARTS (a popular NAS method) with simple
ensembling can improve the robustness to PGD attack by nearly ~12\%. We show
that NAS, which is popular for SoTA accuracy, can provide adversarial accuracy
as a free add-on without any form of adversarial training. Our results show
that leveraging the power of neural network topology with methods like
ensembles can be an excellent way to achieve adversarial robustness without any
form of adversarial training. We also introduce a metric that can be used to
calculate the trade-off between clean accuracy and adversarial robustness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Devaguptapu_C/0/1/0/all/0/1"&gt;Chaitanya Devaguptapu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agarwal_D/0/1/0/all/0/1"&gt;Devansh Agarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mittal_G/0/1/0/all/0/1"&gt;Gaurav Mittal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balasubramanian_V/0/1/0/all/0/1"&gt;Vineeth N Balasubramanian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Temporally Abstract Partial Models. (arXiv:2108.03213v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.03213</id>
        <link href="http://arxiv.org/abs/2108.03213"/>
        <updated>2021-08-09T00:49:28.628Z</updated>
        <summary type="html"><![CDATA[Humans and animals have the ability to reason and make predictions about
different courses of action at many time scales. In reinforcement learning,
option models (Sutton, Precup \& Singh, 1999; Precup, 2000) provide the
framework for this kind of temporally abstract prediction and reasoning.
Natural intelligent agents are also able to focus their attention on courses of
action that are relevant or feasible in a given situation, sometimes termed
affordable actions. In this paper, we define a notion of affordances for
options, and develop temporally abstract partial option models, that take into
account the fact that an option might be affordable only in certain situations.
We analyze the trade-offs between estimation and approximation error in
planning and learning when using such models, and identify some interesting
special cases. Additionally, we demonstrate empirically the potential impact of
partial option models on the efficiency of planning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khetarpal_K/0/1/0/all/0/1"&gt;Khimya Khetarpal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ahmed_Z/0/1/0/all/0/1"&gt;Zafarali Ahmed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Comanici_G/0/1/0/all/0/1"&gt;Gheorghe Comanici&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Precup_D/0/1/0/all/0/1"&gt;Doina Precup&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Meta Label Correction for Noisy Label Learning. (arXiv:1911.03809v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1911.03809</id>
        <link href="http://arxiv.org/abs/1911.03809"/>
        <updated>2021-08-09T00:49:28.622Z</updated>
        <summary type="html"><![CDATA[Leveraging weak or noisy supervision for building effective machine learning
models has long been an important research problem. Its importance has further
increased recently due to the growing need for large-scale datasets to train
deep learning models. Weak or noisy supervision could originate from multiple
sources including non-expert annotators or automatic labeling based on
heuristics or user interaction signals. There is an extensive amount of
previous work focusing on leveraging noisy labels. Most notably, recent work
has shown impressive gains by using a meta-learned instance re-weighting
approach where a meta-learning framework is used to assign instance weights to
noisy labels. In this paper, we extend this approach via posing the problem as
label correction problem within a meta-learning framework. We view the label
correction procedure as a meta-process and propose a new meta-learning based
framework termed MLC (Meta Label Correction) for learning with noisy labels.
Specifically, a label correction network is adopted as a meta-model to produce
corrected labels for noisy labels while the main model is trained to leverage
the corrected labeled. Both models are jointly trained by solving a bi-level
optimization problem. We run extensive experiments with different label noise
levels and types on both image recognition and text classification tasks. We
compare the reweighing and correction approaches showing that the correction
framing addresses some of the limitation of reweighting. We also show that the
proposed MLC approach achieves large improvements over previous methods in many
settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_G/0/1/0/all/0/1"&gt;Guoqing Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Awadallah_A/0/1/0/all/0/1"&gt;Ahmed Hassan Awadallah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dumais_S/0/1/0/all/0/1"&gt;Susan Dumais&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sample Complexity and Overparameterization Bounds for Temporal Difference Learning with Neural Network Approximation. (arXiv:2103.01391v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.01391</id>
        <link href="http://arxiv.org/abs/2103.01391"/>
        <updated>2021-08-09T00:49:28.615Z</updated>
        <summary type="html"><![CDATA[In this paper, we study the dynamics of temporal difference learning with
neural network-based value function approximation over a general state space,
namely, \emph{Neural TD learning}. We consider two practically used algorithms,
projection-free and max-norm regularized Neural TD learning, and establish the
first convergence bounds for these algorithms. An interesting observation from
our results is that max-norm regularization can dramatically improve the
performance of TD learning algorithms, both in terms of sample complexity and
overparameterization. In particular, we prove that max-norm regularization
improves state-of-the-art sample complexity and overparameterization bounds.
The results in this work rely on a novel Lyapunov drift analysis of the network
parameters as a stopped and controlled random process.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cayci_S/0/1/0/all/0/1"&gt;Semih Cayci&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Satpathi_S/0/1/0/all/0/1"&gt;Siddhartha Satpathi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_N/0/1/0/all/0/1"&gt;Niao He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srikant_R/0/1/0/all/0/1"&gt;R. Srikant&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A rigorous introduction for linear models. (arXiv:2105.04240v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.04240</id>
        <link href="http://arxiv.org/abs/2105.04240"/>
        <updated>2021-08-09T00:49:28.608Z</updated>
        <summary type="html"><![CDATA[This survey is meant to provide an introduction to linear models and the
theories behind them. Our goal is to give a rigorous introduction to the
readers with prior exposure to ordinary least squares. In machine learning, the
output is usually a nonlinear function of the input. Deep learning even aims to
find a nonlinear dependence with many layers which require a large amount of
computation. However, most of these algorithms build upon simple linear models.
We then describe linear models from different views and find the properties and
theories behind the models. The linear model is the main technique in
regression problems and the primary tool for it is the least squares
approximation which minimizes a sum of squared errors. This is a natural choice
when we're interested in finding the regression function which minimizes the
corresponding expected squared error. This survey is primarily a summary of
purpose, significance of important theories behind linear models, e.g.,
distribution theory, minimum variance estimator. We first describe ordinary
least squares from three different points of view upon which we disturb the
model with random noise and Gaussian noise. By Gaussian noise, the model gives
rise to the likelihood so that we introduce a maximum likelihood estimator. It
also develops some distribution theories via this Gaussian disturbance. The
distribution theory of least squares will help us answer various questions and
introduce related applications. We then prove least squares is the best
unbiased linear model in the sense of mean squared error and most importantly,
it actually approaches the theoretical limit. We end up with linear models with
the Bayesian approach and beyond.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1"&gt;Jun Lu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Data Augmented Approach to Transfer Learning for Covid-19 Detection. (arXiv:2108.02870v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02870</id>
        <link href="http://arxiv.org/abs/2108.02870"/>
        <updated>2021-08-09T00:49:28.600Z</updated>
        <summary type="html"><![CDATA[Covid-19 detection at an early stage can aid in an effective treatment and
isolation plan to prevent its spread. Recently, transfer learning has been used
for Covid-19 detection using X-ray, ultrasound, and CT scans. One of the major
limitations inherent to these proposed methods is limited labeled dataset size
that affects the reliability of Covid-19 diagnosis and disease progression. In
this work, we demonstrate that how we can augment limited X-ray images data by
using Contrast limited adaptive histogram equalization (CLAHE) to train the
last layer of the pre-trained deep learning models to mitigate the bias of
transfer learning for Covid-19 detection. We transfer learned various
pre-trained deep learning models including AlexNet, ZFNet, VGG-16, ResNet-18,
and GoogLeNet, and fine-tune the last layer by using CLAHE-augmented dataset.
The experiment results reveal that the CLAHE-based augmentation to various
pre-trained deep learning models significantly improves the model efficiency.
The pre-trained VCG-16 model with CLAHEbased augmented images achieves a
sensitivity of 95% using 15 epochs. AlexNet works show good sensitivity when
trained on non-augmented data. Other models demonstrate a value of less than
60% when trained on non-augmented data. Our results reveal that the sample bias
can negatively impact the performance of transfer learning which is
significantly improved by using CLAHE-based augmentation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Henna_S/0/1/0/all/0/1"&gt;Shagufta Henna&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reji_A/0/1/0/all/0/1"&gt;Aparna Reji&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Provable Guarantees for Self-Supervised Deep Learning with Spectral Contrastive Loss. (arXiv:2106.04156v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.04156</id>
        <link href="http://arxiv.org/abs/2106.04156"/>
        <updated>2021-08-09T00:49:28.592Z</updated>
        <summary type="html"><![CDATA[Recent works in self-supervised learning have advanced the state-of-the-art
by relying on the contrastive learning paradigm, which learns representations
by pushing positive pairs, or similar examples from the same class, closer
together while keeping negative pairs far apart. Despite the empirical
successes, theoretical foundations are limited -- prior analyses assume
conditional independence of the positive pairs given the same class label, but
recent empirical applications use heavily correlated positive pairs (i.e., data
augmentations of the same image). Our work analyzes contrastive learning
without assuming conditional independence of positive pairs using a novel
concept of the augmentation graph on data. Edges in this graph connect
augmentations of the same data, and ground-truth classes naturally form
connected sub-graphs. We propose a loss that performs spectral decomposition on
the population augmentation graph and can be succinctly written as a
contrastive learning objective on neural net representations. Minimizing this
objective leads to features with provable accuracy guarantees under linear
probe evaluation. By standard generalization bounds, these accuracy guarantees
also hold when minimizing the training contrastive loss. Empirically, the
features learned by our objective can match or outperform several strong
baselines on benchmark vision datasets. In all, this work provides the first
provable analysis for contrastive learning where guarantees for linear probe
evaluation can apply to realistic empirical settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+HaoChen_J/0/1/0/all/0/1"&gt;Jeff Z. HaoChen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1"&gt;Colin Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gaidon_A/0/1/0/all/0/1"&gt;Adrien Gaidon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1"&gt;Tengyu Ma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Based Proximity Matrix Factorization for Node Embedding. (arXiv:2106.05476v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.05476</id>
        <link href="http://arxiv.org/abs/2106.05476"/>
        <updated>2021-08-09T00:49:28.574Z</updated>
        <summary type="html"><![CDATA[Node embedding learns a low-dimensional representation for each node in the
graph. Recent progress on node embedding shows that proximity matrix
factorization methods gain superb performance and scale to large graphs with
millions of nodes. Existing approaches first define a proximity matrix and then
learn the embeddings that fit the proximity by matrix factorization. Most
existing matrix factorization methods adopt the same proximity for different
tasks, while it is observed that different tasks and datasets may require
different proximity, limiting their representation power.

Motivated by this, we propose {\em Lemane}, a framework with trainable
proximity measures, which can be learned to best suit the datasets and tasks at
hand automatically. Our method is end-to-end, which incorporates differentiable
SVD in the pipeline so that the parameters can be trained via backpropagation.
However, this learning process is still expensive on large graphs. To improve
the scalability, we train proximity measures only on carefully subsampled
graphs, and then apply standard proximity matrix factorization on the original
graph using the learned proximity. Note that, computing the learned proximities
for each pair is still expensive for large graphs, and existing techniques for
computing proximities are not applicable to the learned proximities. Thus, we
present generalized push techniques to make our solution scalable to large
graphs with millions of nodes. Extensive experiments show that our proposed
solution outperforms existing solutions on both link prediction and node
classification tasks on almost all datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xingyi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_K/0/1/0/all/0/1"&gt;Kun Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Sibo Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zengfeng Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Meta-strategy for Learning Tuning Parameters with Guarantees. (arXiv:2102.02504v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.02504</id>
        <link href="http://arxiv.org/abs/2102.02504"/>
        <updated>2021-08-09T00:49:28.566Z</updated>
        <summary type="html"><![CDATA[Online learning methods, like the online gradient algorithm (OGA) and
exponentially weighted aggregation (EWA), often depend on tuning parameters
that are difficult to set in practice. We consider an online meta-learning
scenario, and we propose a meta-strategy to learn these parameters from past
tasks. Our strategy is based on the minimization of a regret bound. It allows
to learn the initialization and the step size in OGA with guarantees. It also
allows to learn the prior or the learning rate in EWA. We provide a regret
analysis of the strategy. It allows to identify settings where meta-learning
indeed improves on learning each task in isolation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Meunier_D/0/1/0/all/0/1"&gt;Dimitri Meunier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Alquier_P/0/1/0/all/0/1"&gt;Pierre Alquier&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Study on Dense and Sparse (Visual) Rewards in Robot Policy Learning. (arXiv:2108.03222v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2108.03222</id>
        <link href="http://arxiv.org/abs/2108.03222"/>
        <updated>2021-08-09T00:49:28.560Z</updated>
        <summary type="html"><![CDATA[Deep Reinforcement Learning (DRL) is a promising approach for teaching robots
new behaviour. However, one of its main limitations is the need for carefully
hand-coded reward signals by an expert. We argue that it is crucial to automate
the reward learning process so that new skills can be taught to robots by their
users. To address such automation, we consider task success classifiers using
visual observations to estimate the rewards in terms of task success. In this
work, we study the performance of multiple state-of-the-art deep reinforcement
learning algorithms under different types of reward: Dense, Sparse, Visual
Dense, and Visual Sparse rewards. Our experiments in various simulation tasks
(Pendulum, Reacher, Pusher, and Fetch Reach) show that while DRL agents can
learn successful behaviours using visual rewards when the goal targets are
distinguishable, their performance may decrease if the task goal is not clearly
visible. Our results also show that visual dense rewards are more successful
than visual sparse rewards and that there is no single best algorithm for all
tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mohtasib_A/0/1/0/all/0/1"&gt;Abdalkarim Mohtasib&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neumann_G/0/1/0/all/0/1"&gt;Gerhard Neumann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cuayahuitl_H/0/1/0/all/0/1"&gt;Heriberto Cuayahuitl&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Uncertainty-Based Dynamic Graph Neighborhoods For Medical Segmentation. (arXiv:2108.03117v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03117</id>
        <link href="http://arxiv.org/abs/2108.03117"/>
        <updated>2021-08-09T00:49:28.553Z</updated>
        <summary type="html"><![CDATA[In recent years, deep learning based methods have shown success in essential
medical image analysis tasks such as segmentation. Post-processing and refining
the results of segmentation is a common practice to decrease the
misclassifications originating from the segmentation network. In addition to
widely used methods like Conditional Random Fields (CRFs) which focus on the
structure of the segmented volume/area, a graph-based recent approach makes use
of certain and uncertain points in a graph and refines the segmentation
according to a small graph convolutional network (GCN). However, there are two
drawbacks of the approach: most of the edges in the graph are assigned randomly
and the GCN is trained independently from the segmentation network. To address
these issues, we define a new neighbor-selection mechanism according to feature
distances and combine the two networks in the training procedure. According to
the experimental results on pancreas segmentation from Computed Tomography (CT)
images, we demonstrate improvement in the quantitative measures. Also,
examining the dynamic neighbors created by our method, edges between
semantically similar image parts are observed. The proposed method also shows
qualitative enhancements in the segmentation maps, as demonstrated in the
visual results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Demir_U/0/1/0/all/0/1"&gt;Ufuk Demir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ozer_A/0/1/0/all/0/1"&gt;Atahan Ozer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sahin_Y/0/1/0/all/0/1"&gt;Yusuf H. Sahin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Unal_G/0/1/0/all/0/1"&gt;Gozde Unal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI-based Aortic Vessel Tree Segmentation for Cardiovascular Diseases Treatment: Status Quo. (arXiv:2108.02998v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02998</id>
        <link href="http://arxiv.org/abs/2108.02998"/>
        <updated>2021-08-09T00:49:28.536Z</updated>
        <summary type="html"><![CDATA[The aortic vessel tree is composed of the aorta and its branching arteries,
and plays a key role in supplying the whole body with blood. Aortic diseases,
like aneurysms or dissections, can lead to an aortic rupture, whose treatment
with open surgery is highly risky. Therefore, patients commonly undergo drug
treatment under constant monitoring, which requires regular inspections of the
vessels through imaging. The standard imaging modality for diagnosis and
monitoring is computed tomography (CT), which can provide a detailed picture of
the aorta and its branching vessels if combined with a contrast agent,
resulting in a CT angiography (CTA). Optimally, the whole aortic vessel tree
geometry from consecutive CTAs, are overlaid and compared. This allows to not
only detect changes in the aorta, but also more peripheral vessel tree changes,
caused by the primary pathology or newly developed. When performed manually,
this reconstruction requires slice by slice contouring, which could easily take
a whole day for a single aortic vessel tree and, hence, is not feasible in
clinical practice. Automatic or semi-automatic vessel tree segmentation
algorithms, on the other hand, can complete this task in a fraction of the
manual execution time and run in parallel to the clinical routine of the
clinicians. In this paper, we systematically review computing techniques for
the automatic and semi-automatic segmentation of the aortic vessel tree. The
review concludes with an in-depth discussion on how close these
state-of-the-art approaches are to an application in clinical practice and how
active this research field is, taking into account the number of publications,
datasets and challenges.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1"&gt;Yuan Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pepe_A/0/1/0/all/0/1"&gt;Antonio Pepe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jianning Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gsaxner_C/0/1/0/all/0/1"&gt;Christina Gsaxner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_F/0/1/0/all/0/1"&gt;Fen-hua Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kleesiek_J/0/1/0/all/0/1"&gt;Jens Kleesiek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frangi_A/0/1/0/all/0/1"&gt;Alejandro F. Frangi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Egger_J/0/1/0/all/0/1"&gt;Jan Egger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generalization in Quantum Machine Learning: a Quantum Information Perspective. (arXiv:2102.08991v2 [quant-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.08991</id>
        <link href="http://arxiv.org/abs/2102.08991"/>
        <updated>2021-08-09T00:49:28.528Z</updated>
        <summary type="html"><![CDATA[Quantum classification and hypothesis testing are two tightly related
subjects, the main difference being that the former is data driven: how to
assign to quantum states $\rho(x)$ the corresponding class $c$ (or hypothesis)
is learnt from examples during training, where $x$ can be either tunable
experimental parameters or classical data "embedded" into quantum states. Does
the model generalize? This is the main question in any data-driven strategy,
namely the ability to predict the correct class even of previously unseen
states. Here we establish a link between quantum machine learning
classification and quantum hypothesis testing (state and channel
discrimination) and then show that the accuracy and generalization capability
of quantum classifiers depend on the (R\'enyi) mutual informations $I(C{:}Q)$
and $I_2(X{:}Q)$ between the quantum state space $Q$ and the classical
parameter space $X$ or class space $C$. Based on the above characterization, we
then show how different properties of $Q$ affect classification accuracy and
generalization, such as the dimension of the Hilbert space, the amount of
noise, and the amount of neglected information from $X$ via, e.g., pooling
layers. Moreover, we introduce a quantum version of the Information Bottleneck
principle that allows us to explore the various tradeoffs between accuracy and
generalization. Finally, in order to check our theoretical predictions, we
study the classification of the quantum phases of an Ising spin chain, and we
propose the Variational Quantum Information Bottleneck (VQIB) method to
optimize quantum embeddings of classical data to favor generalization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Banchi_L/0/1/0/all/0/1"&gt;Leonardo Banchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Pereira_J/0/1/0/all/0/1"&gt;Jason Pereira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Pirandola_S/0/1/0/all/0/1"&gt;Stefano Pirandola&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[COV-ELM classifier: An Extreme Learning Machine based identification of COVID-19 using Chest X-Ray Images. (arXiv:2007.08637v5 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.08637</id>
        <link href="http://arxiv.org/abs/2007.08637"/>
        <updated>2021-08-09T00:49:28.521Z</updated>
        <summary type="html"><![CDATA[Coronaviruses constitute a family of viruses that gives rise to respiratory
diseases. As COVID-19 is highly contagious, early diagnosis of COVID-19 is
crucial for an effective treatment strategy. However, the RT-PCR test which is
considered to be a gold standard in the diagnosis of COVID-19 suffers from a
high false-negative rate. Chest X-ray (CXR) image analysis has emerged as a
feasible and effective diagnostic technique towards this objective. In this
work, we propose the COVID-19 classification problem as a three-class
classification problem to distinguish between COVID-19, normal, and pneumonia
classes. We propose a three-stage framework, named COV-ELM. Stage one deals
with preprocessing and transformation while stage two deals with feature
extraction. These extracted features are passed as an input to the ELM at the
third stage, resulting in the identification of COVID-19. The choice of ELM in
this work has been motivated by its faster convergence, better generalization
capability, and shorter training time in comparison to the conventional
gradient-based learning algorithms. As bigger and diverse datasets become
available, ELM can be quickly retrained as compared to its gradient-based
competitor models. The proposed model achieved a macro average F1-score of 0.95
and the overall sensitivity of ${0.94 \pm 0.02} at a 95% confidence interval.
When compared to state-of-the-art machine learning algorithms, the COV-ELM is
found to outperform its competitors in this three-class classification
scenario. Further, LIME has been integrated with the proposed COV-ELM model to
generate annotated CXR images. The annotations are based on the superpixels
that have contributed to distinguish between the different classes. It was
observed that the superpixels correspond to the regions of the human lungs that
are clinically observed in COVID-19 and Pneumonia cases.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Rajpal_S/0/1/0/all/0/1"&gt;Sheetal Rajpal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Agarwal_M/0/1/0/all/0/1"&gt;Manoj Agarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rajpal_A/0/1/0/all/0/1"&gt;Ankit Rajpal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lakhyani_N/0/1/0/all/0/1"&gt;Navin Lakhyani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Saggar_A/0/1/0/all/0/1"&gt;Arpita Saggar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kumar_N/0/1/0/all/0/1"&gt;Naveen Kumar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Residual Feedback Learning for Contact-Rich Manipulation Tasks with Uncertainty. (arXiv:2106.04306v2 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.04306</id>
        <link href="http://arxiv.org/abs/2106.04306"/>
        <updated>2021-08-09T00:49:28.514Z</updated>
        <summary type="html"><![CDATA[While classic control theory offers state of the art solutions in many
problem scenarios, it is often desired to improve beyond the structure of such
solutions and surpass their limitations. To this end, residual policy learning
(RPL) offers a formulation to improve existing controllers with reinforcement
learning (RL) by learning an additive "residual" to the output of a given
controller. However, the applicability of such an approach highly depends on
the structure of the controller. Often, internal feedback signals of the
controller limit an RL algorithm to adequately change the policy and, hence,
learn the task. We propose a new formulation that addresses these limitations
by also modifying the feedback signals to the controller with an RL policy and
show superior performance of our approach on a contact-rich peg-insertion task
under position and orientation uncertainty. In addition, we use a recent
Cartesian impedance control architecture as the control framework which can be
available to us as a black-box while assuming no knowledge about its
input/output structure, and show the difficulties of standard RPL. Furthermore,
we introduce an adaptive curriculum for the given task to gradually increase
the task difficulty in terms of position and orientation uncertainty. A video
showing the results can be found at https://youtu.be/SAZm_Krze7U .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ranjbar_A/0/1/0/all/0/1"&gt;Alireza Ranjbar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vien_N/0/1/0/all/0/1"&gt;Ngo Anh Vien&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ziesche_H/0/1/0/all/0/1"&gt;Hanna Ziesche&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boedecker_J/0/1/0/all/0/1"&gt;Joschka Boedecker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neumann_G/0/1/0/all/0/1"&gt;Gerhard Neumann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Building a Foundation for Data-Driven, Interpretable, and Robust Policy Design using the AI Economist. (arXiv:2108.02904v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02904</id>
        <link href="http://arxiv.org/abs/2108.02904"/>
        <updated>2021-08-09T00:49:28.507Z</updated>
        <summary type="html"><![CDATA[Optimizing economic and public policy is critical to address socioeconomic
issues and trade-offs, e.g., improving equality, productivity, or wellness, and
poses a complex mechanism design problem. A policy designer needs to consider
multiple objectives, policy levers, and behavioral responses from strategic
actors who optimize for their individual objectives. Moreover, real-world
policies should be explainable and robust to simulation-to-reality gaps, e.g.,
due to calibration issues. Existing approaches are often limited to a narrow
set of policy levers or objectives that are hard to measure, do not yield
explicit optimal policies, or do not consider strategic behavior, for example.
Hence, it remains challenging to optimize policy in real-world scenarios. Here
we show that the AI Economist framework enables effective, flexible, and
interpretable policy design using two-level reinforcement learning (RL) and
data-driven simulations. We validate our framework on optimizing the stringency
of US state policies and Federal subsidies during a pandemic, e.g., COVID-19,
using a simulation fitted to real data. We find that log-linear policies
trained using RL significantly improve social welfare, based on both public
health and economic outcomes, compared to past outcomes. Their behavior can be
explained, e.g., well-performing policies respond strongly to changes in
recovery and vaccination rates. They are also robust to calibration errors,
e.g., infection rates that are over or underestimated. As of yet, real-world
policymaking has not seen adoption of machine learning methods at large,
including RL and AI-driven simulations. Our results show the potential of AI to
guide policy design and improve social welfare amidst the complexity of the
real world.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Trott_A/0/1/0/all/0/1"&gt;Alexander Trott&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srinivasa_S/0/1/0/all/0/1"&gt;Sunil Srinivasa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wal_D/0/1/0/all/0/1"&gt;Douwe van der Wal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Haneuse_S/0/1/0/all/0/1"&gt;Sebastien Haneuse&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1"&gt;Stephan Zheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Detecting Requirements Smells With Deep Learning: Experiences, Challenges and Future Work. (arXiv:2108.03087v1 [cs.SE])]]></title>
        <id>http://arxiv.org/abs/2108.03087</id>
        <link href="http://arxiv.org/abs/2108.03087"/>
        <updated>2021-08-09T00:49:28.489Z</updated>
        <summary type="html"><![CDATA[Requirements Engineering (RE) is the initial step towards building a software
system. The success or failure of a software project is firmly tied to this
phase, based on communication among stakeholders using natural language. The
problem with natural language is that it can easily lead to different
understandings if it is not expressed precisely by the stakeholders involved,
which results in building a product different from the expected one. Previous
work proposed to enhance the quality of the software requirements detecting
language errors based on ISO 29148 requirements language criteria. The existing
solutions apply classical Natural Language Processing (NLP) to detect them. NLP
has some limitations, such as domain dependability which results in poor
generalization capability. Therefore, this work aims to improve the previous
work by creating a manually labeled dataset and using ensemble learning, Deep
Learning (DL), and techniques such as word embeddings and transfer learning to
overcome the generalization problem that is tied with classical NLP and improve
precision and recall metrics using a manually labeled dataset. The current
findings show that the dataset is unbalanced and which class examples should be
added more. It is tempting to train algorithms even if the dataset is not
considerably representative. Whence, the results show that models are
overfitting; in Machine Learning this issue is solved by adding more instances
to the dataset, improving label quality, removing noise, and reducing the
learning algorithms complexity, which is planned for this research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Habib_M/0/1/0/all/0/1"&gt;Mohammad Kasra Habib&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wagner_S/0/1/0/all/0/1"&gt;Stefan Wagner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Graziotin_D/0/1/0/all/0/1"&gt;Daniel Graziotin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Beyond Occam's Razor in System Identification: Double-Descent when Modeling Dynamics. (arXiv:2012.06341v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.06341</id>
        <link href="http://arxiv.org/abs/2012.06341"/>
        <updated>2021-08-09T00:49:28.462Z</updated>
        <summary type="html"><![CDATA[System identification aims to build models of dynamical systems from data.
Traditionally, choosing the model requires the designer to balance between two
goals of conflicting nature; the model must be rich enough to capture the
system dynamics, but not so flexible that it learns spurious random effects
from the dataset. It is typically observed that the model validation
performance follows a U-shaped curve as the model complexity increases. Recent
developments in machine learning and statistics, however, have observed
situations where a "double-descent" curve subsumes this U-shaped
model-performance curve. With a second decrease in performance occurring beyond
the point where the model has reached the capacity of interpolating - i.e.,
(near) perfectly fitting - the training data. To the best of our knowledge,
such phenomena have not been studied within the context of dynamic systems. The
present paper aims to answer the question: "Can such a phenomenon also be
observed when estimating parameters of dynamic systems?" We show that the
answer is yes, verifying such behavior experimentally both for artificially
generated and real-world datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ribeiro_A/0/1/0/all/0/1"&gt;Ant&amp;#xf4;nio H. Ribeiro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hendriks_J/0/1/0/all/0/1"&gt;Johannes N. Hendriks&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wills_A/0/1/0/all/0/1"&gt;Adrian G. Wills&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schon_T/0/1/0/all/0/1"&gt;Thomas B. Sch&amp;#xf6;n&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SELM: Siamese Extreme Learning Machine with Application to Face Biometrics. (arXiv:2108.03140v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03140</id>
        <link href="http://arxiv.org/abs/2108.03140"/>
        <updated>2021-08-09T00:49:28.455Z</updated>
        <summary type="html"><![CDATA[Extreme Learning Machine is a powerful classification method very competitive
existing classification methods. It is extremely fast at training.
Nevertheless, it cannot perform face verification tasks properly because face
verification tasks require comparison of facial images of two individuals at
the same time and decide whether the two faces identify the same person. The
structure of Extreme Leaning Machine was not designed to feed two input data
streams simultaneously, thus, in 2-input scenarios Extreme Learning Machine
methods are normally applied using concatenated inputs. However, this setup
consumes two times more computational resources and it is not optimized for
recognition tasks where learning a separable distance metric is critical. For
these reasons, we propose and develop a Siamese Extreme Learning Machine
(SELM). SELM was designed to be fed with two data streams in parallel
simultaneously. It utilizes a dual-stream Siamese condition in the extra
Siamese layer to transform the data before passing it along to the hidden
layer. Moreover, we propose a Gender-Ethnicity-Dependent triplet feature
exclusively trained on a variety of specific demographic groups. This feature
enables learning and extracting of useful facial features of each group.
Experiments were conducted to evaluate and compare the performances of SELM,
Extreme Learning Machine, and DCNN. The experimental results showed that the
proposed feature was able to perform correct classification at 97.87% accuracy
and 99.45% AUC. They also showed that using SELM in conjunction with the
proposed feature provided 98.31% accuracy and 99.72% AUC. They outperformed the
well-known DCNN and Extreme Leaning Machine methods by a wide margin.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kudisthalert_W/0/1/0/all/0/1"&gt;Wasu Kudisthalert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pasupa_K/0/1/0/all/0/1"&gt;Kitsuchart Pasupa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morales_A/0/1/0/all/0/1"&gt;Aythami Morales&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fierrez_J/0/1/0/all/0/1"&gt;Julian Fierrez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lossless Multi-Scale Constitutive Elastic Relations with Artificial Intelligence. (arXiv:2108.02837v1 [cond-mat.mtrl-sci])]]></title>
        <id>http://arxiv.org/abs/2108.02837</id>
        <link href="http://arxiv.org/abs/2108.02837"/>
        <updated>2021-08-09T00:49:28.448Z</updated>
        <summary type="html"><![CDATA[The elastic properties of materials derive from their electronic and atomic
nature. However, simulating bulk materials fully at these scales is not
feasible, so that typically homogenized continuum descriptions are used
instead. A seamless and lossless transition of the constitutive description of
the elastic response of materials between these two scales has been so far
elusive. Here we show how this problem can be overcome by using Artificial
Intelligence (AI). A Convolutional Neural Network (CNN) model is trained, by
taking the structure image of a nanoporous material as input and the
corresponding elasticity tensor, calculated from Molecular Statics (MS), as
output. Trained with the atomistic data, the CNN model captures the size- and
pore-dependency of the material's elastic properties which, on the physics
side, can stem from surfaces and non-local effects. Such effects are often
ignored in upscaling from atomistic to classical continuum theory. To
demonstrate the accuracy and the efficiency of the trained CNN model, a Finite
Element Method (FEM) based result of an elastically deformed nanoporous beam
equipped with the CNN as constitutive law is compared with that by a full
atomistic simulation. The good agreement between the atomistic simulations and
the FEM-AI combination for a system with size and surface effects establishes a
new lossless scale bridging approach to such problems. The trained CNN model
deviates from the atomistic result by 9.6\% for porosity scenarios of up to
90\% but it is about 230 times faster than the MS calculation and does not
require to change simulation methods between different scales. The efficiency
of the CNN evaluation together with the preservation of important atomistic
effects makes the trained model an effective atomistically-informed
constitutive model for macroscopic simulations of nanoporous materials and
solving of inverse problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cond-mat/1/au:+Mianroodi_J/0/1/0/all/0/1"&gt;Jaber Rezaei Mianroodi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Rezaei_S/0/1/0/all/0/1"&gt;Shahed Rezaei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Siboni_N/0/1/0/all/0/1"&gt;Nima H. Siboni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Xu_B/0/1/0/all/0/1"&gt;Bai-Xiang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Raabe_D/0/1/0/all/0/1"&gt;Dierk Raabe&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Analyzing Information Leakage of Updates to Natural Language Models. (arXiv:1912.07942v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1912.07942</id>
        <link href="http://arxiv.org/abs/1912.07942"/>
        <updated>2021-08-09T00:49:28.441Z</updated>
        <summary type="html"><![CDATA[To continuously improve quality and reflect changes in data, machine learning
applications have to regularly retrain and update their core models. We show
that a differential analysis of language model snapshots before and after an
update can reveal a surprising amount of detailed information about changes in
the training data. We propose two new metrics---\emph{differential score} and
\emph{differential rank}---for analyzing the leakage due to updates of natural
language models. We perform leakage analysis using these metrics across models
trained on several different datasets using different methods and
configurations. We discuss the privacy implications of our findings, propose
mitigation strategies and evaluate their effect.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zanella_Beguelin_S/0/1/0/all/0/1"&gt;Santiago Zanella-B&amp;#xe9;guelin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wutschitz_L/0/1/0/all/0/1"&gt;Lukas Wutschitz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tople_S/0/1/0/all/0/1"&gt;Shruti Tople&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ruhle_V/0/1/0/all/0/1"&gt;Victor R&amp;#xfc;hle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paverd_A/0/1/0/all/0/1"&gt;Andrew Paverd&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ohrimenko_O/0/1/0/all/0/1"&gt;Olga Ohrimenko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kopf_B/0/1/0/all/0/1"&gt;Boris K&amp;#xf6;pf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brockschmidt_M/0/1/0/all/0/1"&gt;Marc Brockschmidt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Identifiable Energy-based Representations: An Application to Estimating Heterogeneous Causal Effects. (arXiv:2108.03039v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.03039</id>
        <link href="http://arxiv.org/abs/2108.03039"/>
        <updated>2021-08-09T00:49:28.423Z</updated>
        <summary type="html"><![CDATA[Conditional average treatment effects (CATEs) allow us to understand the
effect heterogeneity across a large population of individuals. However, typical
CATE learners assume all confounding variables are measured in order for the
CATE to be identifiable. Often, this requirement is satisfied by simply
collecting many variables, at the expense of increased sample complexity for
estimating CATEs. To combat this, we propose an energy-based model (EBM) that
learns a low-dimensional representation of the variables by employing a noise
contrastive loss function. With our EBM we introduce a preprocessing step that
alleviates the dimensionality curse for any existing model and learner
developed for estimating CATE. We prove that our EBM keeps the representations
partially identifiable up to some universal constant, as well as having
universal approximation capability to avoid excessive information loss from
model misspecification; these properties combined with our loss function,
enable the representations to converge and keep the CATE estimation consistent.
Experiments demonstrate the convergence of the representations, as well as show
that estimating CATEs on our representations performs better than on the
variables or the representations obtained via various benchmark dimensionality
reduction methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berrevoets_J/0/1/0/all/0/1"&gt;Jeroen Berrevoets&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schaar_M/0/1/0/all/0/1"&gt;Mihaela van der Schaar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fast and Accurate Low-Rank Tensor Completion Methods Based on QR Decomposition and $L_{2,1}$ Norm Minimization. (arXiv:2108.03002v1 [math.NA])]]></title>
        <id>http://arxiv.org/abs/2108.03002</id>
        <link href="http://arxiv.org/abs/2108.03002"/>
        <updated>2021-08-09T00:49:28.416Z</updated>
        <summary type="html"><![CDATA[More recently, an Approximate SVD Based on Qatar Riyal (QR) Decomposition
(CSVD-QR) method for matrix complete problem is presented, whose computational
complexity is $O(r^2(m+n))$, which is mainly due to that $r$ is far less than
$\min\{m,n\}$, where $r$ represents the largest number of singular values of
matrix $X$. What is particularly interesting is that after replacing the
nuclear norm with the $L_{2,1}$ norm proposed based on this decomposition, as
the upper bound of the nuclear norm, when the intermediate matrix $D$ in its
decomposition is close to the diagonal matrix, it will converge to the nuclear
norm, and is exactly equal, when the $D$ matrix is equal to the diagonal
matrix, to the nuclear norm, which ingeniously avoids the calculation of the
singular value of the matrix. To the best of our knowledge, there is no
literature to generalize and apply it to solve tensor complete problems.
Inspired by this, in this paper we propose a class of tensor minimization model
based on $L_{2,1}$ norm and CSVD-QR method for the tensor complete problem,
which is convex and therefore has a global minimum solution.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Zhang_H/0/1/0/all/0/1"&gt;HongBing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Liu_X/0/1/0/all/0/1"&gt;XinYi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Fan_H/0/1/0/all/0/1"&gt;HongTao Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Li_Y/0/1/0/all/0/1"&gt;YaJing Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Ye_Y/0/1/0/all/0/1"&gt;Yinlin Ye&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[COVID-Net US: A Tailored, Highly Efficient, Self-Attention Deep Convolutional Neural Network Design for Detection of COVID-19 Patient Cases from Point-of-care Ultrasound Imaging. (arXiv:2108.03131v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2108.03131</id>
        <link href="http://arxiv.org/abs/2108.03131"/>
        <updated>2021-08-09T00:49:28.409Z</updated>
        <summary type="html"><![CDATA[The Coronavirus Disease 2019 (COVID-19) pandemic has impacted many aspects of
life globally, and a critical factor in mitigating its effects is screening
individuals for infections, thereby allowing for both proper treatment for
those individuals as well as action to be taken to prevent further spread of
the virus. Point-of-care ultrasound (POCUS) imaging has been proposed as a
screening tool as it is a much cheaper and easier to apply imaging modality
than others that are traditionally used for pulmonary examinations, namely
chest x-ray and computed tomography. Given the scarcity of expert radiologists
for interpreting POCUS examinations in many highly affected regions around the
world, low-cost deep learning-driven clinical decision support solutions can
have a large impact during the on-going pandemic. Motivated by this, we
introduce COVID-Net US, a highly efficient, self-attention deep convolutional
neural network design tailored for COVID-19 screening from lung POCUS images.
Experimental results show that the proposed COVID-Net US can achieve an AUC of
over 0.98 while achieving 353X lower architectural complexity, 62X lower
computational complexity, and 14.3X faster inference times on a Raspberry Pi.
Clinical validation was also conducted, where select cases were reviewed and
reported on by a practicing clinician (20 years of clinical practice)
specializing in intensive care (ICU) and 15 years of expertise in POCUS
interpretation. To advocate affordable healthcare and artificial intelligence
for resource-constrained environments, we have made COVID-Net US open source
and publicly available as part of the COVID-Net open source initiative.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+MacLean_A/0/1/0/all/0/1"&gt;Alexander MacLean&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Abbasi_S/0/1/0/all/0/1"&gt;Saad Abbasi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ebadi_A/0/1/0/all/0/1"&gt;Ashkan Ebadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhao_A/0/1/0/all/0/1"&gt;Andy Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Pavlova_M/0/1/0/all/0/1"&gt;Maya Pavlova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gunraj_H/0/1/0/all/0/1"&gt;Hayden Gunraj&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xi_P/0/1/0/all/0/1"&gt;Pengcheng Xi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kohli_S/0/1/0/all/0/1"&gt;Sonny Kohli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wong_A/0/1/0/all/0/1"&gt;Alexander Wong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Two-Stage Sector Rotation Methodology Using Machine Learning and Deep Learning Techniques. (arXiv:2108.02838v1 [q-fin.GN])]]></title>
        <id>http://arxiv.org/abs/2108.02838</id>
        <link href="http://arxiv.org/abs/2108.02838"/>
        <updated>2021-08-09T00:49:28.402Z</updated>
        <summary type="html"><![CDATA[Market indicators such as CPI and GDP have been widely used over decades to
identify the stage of business cycles and also investment attractiveness of
sectors given market conditions. In this paper, we propose a two-stage
methodology that consists of predicting ETF prices for each sector using market
indicators and ranking sectors based on their predicted rate of returns. We
initially start with choosing sector specific macroeconomic indicators and
implement Recursive Feature Elimination algorithm to select the most important
features for each sector. Using our prediction tool, we implement different
Recurrent Neural Networks models to predict the future ETF prices for each
sector. We then rank the sectors based on their predicted rate of returns. We
select the best performing model by evaluating the annualized return,
annualized Sharpe ratio, and Calmar ratio of the portfolios that includes the
top four ranked sectors chosen by the model. We also test the robustness of the
model performance with respect to lookback windows and look ahead windows. Our
empirical results show that our methodology beats the equally weighted
portfolio performance even in the long run. We also find that Echo State
Networks exhibits an outstanding performance compared to other models yet it is
faster to implement compared to other RNN models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-fin/1/au:+Karatas_T/0/1/0/all/0/1"&gt;Tugce Karatas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Hirsa_A/0/1/0/all/0/1"&gt;Ali Hirsa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Analysis of Driving Scenario Trajectories with Active Learning. (arXiv:2108.03217v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.03217</id>
        <link href="http://arxiv.org/abs/2108.03217"/>
        <updated>2021-08-09T00:49:28.385Z</updated>
        <summary type="html"><![CDATA[Annotating the driving scenario trajectories based only on explicit rules
(i.e., knowledge-based methods) can be subject to errors, such as false
positive/negative classification of scenarios that lie on the border of two
scenario classes, missing unknown scenario classes, and also anomalies. On the
other side, verifying the labels by the annotators is not cost-efficient. For
this purpose, active learning (AL) could potentially improve the annotation
procedure by inclusion of an annotator/expert in an efficient way. In this
study, we develop an active learning framework to annotate driving trajectory
time-series data. At the first step, we compute an embedding of the time-series
trajectories into a latent space in order to extract the temporal nature. For
this purpose, we study three different latent space representations:
multivariate Time Series t-Distributed Stochastic Neighbor Embedding (mTSNE),
Recurrent Auto-Encoder (RAE) and Variational Recurrent Auto-Encoder (VRAE). We
then apply different active learning paradigms with different classification
models to the embedded data. In particular, we study the two classifiers Neural
Network (NN) and Support Vector Machines (SVM), with three active learning
query strategies (i.e., entropy, margin and random). In the following, we
explore the possibilities of the framework to discover unknown classes and
demonstrate how it can be used to identify the out-of-class trajectories.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jarl_S/0/1/0/all/0/1"&gt;Sanna Jarl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rahrovani_S/0/1/0/all/0/1"&gt;Sadegh Rahrovani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chehreghani_M/0/1/0/all/0/1"&gt;Morteza Haghir Chehreghani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Shift-invariant waveform learning on epileptic ECoG. (arXiv:2108.03177v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.03177</id>
        <link href="http://arxiv.org/abs/2108.03177"/>
        <updated>2021-08-09T00:49:28.378Z</updated>
        <summary type="html"><![CDATA[Seizure detection algorithms must discriminate abnormal neuronal activity
associated with a seizure from normal neural activity in a variety of
conditions. Our approach is to seek spatiotemporal waveforms with distinct
morphology in electrocorticographic (ECoG) recordings of epileptic patients
that are indicative of a subsequent seizure (preictal) versus non-seizure
segments (interictal). To find these waveforms we apply a shift-invariant
k-means algorithm to segments of spatially filtered signals to learn codebooks
of prototypical waveforms. The frequency of the cluster labels from the
codebooks is then used to train a binary classifier that predicts the class
(preictal or interictal) of a test ECoG segment. We use the Matthews
correlation coefficient to evaluate the performance of the classifier and the
quality of the codebooks. We found that our method finds recurrent
non-sinusoidal waveforms that could be used to build interpretable features for
seizure prediction and that are also physiologically meaningful.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mendoza_Cardenas_C/0/1/0/all/0/1"&gt;Carlos H. Mendoza-Cardenas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brockmeier_A/0/1/0/all/0/1"&gt;Austin J. Brockmeier&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rectified Euler k-means and Beyond. (arXiv:2108.03081v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.03081</id>
        <link href="http://arxiv.org/abs/2108.03081"/>
        <updated>2021-08-09T00:49:28.371Z</updated>
        <summary type="html"><![CDATA[Euler k-means (EulerK) first maps data onto the unit hyper-sphere surface of
equi-dimensional space via a complex mapping which induces the robust Euler
kernel and next employs the popular $k$-means. Consequently, besides enjoying
the virtues of k-means such as simplicity and scalability to large data sets,
EulerK is also robust to noises and outliers. Although so, the centroids
captured by EulerK deviate from the unit hyper-sphere surface and thus in
strict distributional sense, actually are outliers. This weird phenomenon also
occurs in some generic kernel clustering methods. Intuitively, using such
outlier-like centroids should not be quite reasonable but it is still seldom
attended. To eliminate the deviation, we propose two Rectified Euler k-means
methods, i.e., REK1 and REK2, which retain the merits of EulerK while acquire
real centroids residing on the mapped space to better characterize the data
structures. Specifically, REK1 rectifies EulerK by imposing the constraint on
the centroids while REK2 views each centroid as the mapped image from a
pre-image in the original space and optimizes these pre-images in Euler kernel
induced space. Undoubtedly, our proposed REKs can methodologically be extended
to solve problems of such a category. Finally, the experiments validate the
effectiveness of REK1 and REK2.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1"&gt;Yunxia Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+chen_S/0/1/0/all/0/1"&gt;Songcan chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Inspecting the Process of Bank Credit Rating via Visual Analytics. (arXiv:2108.03011v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.03011</id>
        <link href="http://arxiv.org/abs/2108.03011"/>
        <updated>2021-08-09T00:49:28.361Z</updated>
        <summary type="html"><![CDATA[Bank credit rating classifies banks into different levels based on publicly
disclosed and internal information, serving as an important input in financial
risk management. However, domain experts have a vague idea of exploring and
comparing different bank credit rating schemes. A loose connection between
subjective and quantitative analysis and difficulties in determining
appropriate indicator weights obscure understanding of bank credit ratings.
Furthermore, existing models fail to consider bank types by just applying a
unified indicator weight set to all banks. We propose RatingVis to assist
experts in exploring and comparing different bank credit rating schemes. It
supports interactively inferring indicator weights for banks by involving
domain knowledge and considers bank types in the analysis loop. We conduct a
case study with real-world bank data to verify the efficacy of RatingVis.
Expert feedback suggests that our approach helps them better understand
different rating schemes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1"&gt;Qiangqiang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1"&gt;Quan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1"&gt;Zhihua Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_T/0/1/0/all/0/1"&gt;Tangzhi Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1"&gt;Xiaojuan Ma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Domain Adaptation in Speech Recognition using Phonetic Features. (arXiv:2108.02850v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2108.02850</id>
        <link href="http://arxiv.org/abs/2108.02850"/>
        <updated>2021-08-09T00:49:28.354Z</updated>
        <summary type="html"><![CDATA[Automatic speech recognition is a difficult problem in pattern recognition
because several sources of variability exist in the speech input like the
channel variations, the input might be clean or noisy, the speakers may have
different accent and variations in the gender, etc. As a result, domain
adaptation is important in speech recognition where we train the model for a
particular source domain and test it on a different target domain. In this
paper, we propose a technique to perform unsupervised gender-based domain
adaptation in speech recognition using phonetic features. The experiments are
performed on the TIMIT dataset and there is a considerable decrease in the
phoneme error rate using the proposed approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Ojha_R/0/1/0/all/0/1"&gt;Rupam Ojha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sekhar_C/0/1/0/all/0/1"&gt;C Chandra Sekhar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RIS-assisted UAV Communications for IoT with Wireless Power Transfer Using Deep Reinforcement Learning. (arXiv:2108.02889v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2108.02889</id>
        <link href="http://arxiv.org/abs/2108.02889"/>
        <updated>2021-08-09T00:49:28.345Z</updated>
        <summary type="html"><![CDATA[Many of the devices used in Internet-of-Things (IoT) applications are
energy-limited, and thus supplying energy while maintaining seamless
connectivity for IoT devices is of considerable importance. In this context, we
propose a simultaneous wireless power transfer and information transmission
scheme for IoT devices with support from reconfigurable intelligent surface
(RIS)-aided unmanned aerial vehicle (UAV) communications. In particular, in a
first phase, IoT devices harvest energy from the UAV through wireless power
transfer; and then in a second phase, the UAV collects data from the IoT
devices through information transmission. To characterise the agility of the
UAV, we consider two scenarios: a hovering UAV and a mobile UAV. Aiming at
maximizing the total network sum-rate, we jointly optimize the trajectory of
the UAV, the energy harvesting scheduling of IoT devices, and the phaseshift
matrix of the RIS. We formulate a Markov decision process and propose two deep
reinforcement learning algorithms to solve the optimization problem of
maximizing the total network sum-rate. Numerical results illustrate the
effectiveness of the UAV's flying path optimization and the network's
throughput of our proposed techniques compared with other benchmark schemes.
Given the strict requirements of the RIS and UAV, the significant improvement
in processing time and throughput performance demonstrates that our proposed
scheme is well applicable for practical IoT applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Nguyen_K/0/1/0/all/0/1"&gt;Khoi Khac Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Masaracchia_A/0/1/0/all/0/1"&gt;Antonino Masaracchia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Do_Duy_T/0/1/0/all/0/1"&gt;Tan Do-Duy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Poor_H/0/1/0/all/0/1"&gt;H. Vincent Poor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Duong_T/0/1/0/all/0/1"&gt;Trung Q. Duong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Differentially Private n-gram Extraction. (arXiv:2108.02831v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02831</id>
        <link href="http://arxiv.org/abs/2108.02831"/>
        <updated>2021-08-09T00:49:28.337Z</updated>
        <summary type="html"><![CDATA[We revisit the problem of $n$-gram extraction in the differential privacy
setting. In this problem, given a corpus of private text data, the goal is to
release as many $n$-grams as possible while preserving user level privacy.
Extracting $n$-grams is a fundamental subroutine in many NLP applications such
as sentence completion, response generation for emails etc. The problem also
arises in other applications such as sequence mining, and is a generalization
of recently studied differentially private set union (DPSU). In this paper, we
develop a new differentially private algorithm for this problem which, in our
experiments, significantly outperforms the state-of-the-art. Our improvements
stem from combining recent advances in DPSU, privacy accounting, and new
heuristics for pruning in the tree-based approach initiated by Chen et al.
(2012).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1"&gt;Kunho Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gopi_S/0/1/0/all/0/1"&gt;Sivakanth Gopi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kulkarni_J/0/1/0/all/0/1"&gt;Janardhan Kulkarni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yekhanin_S/0/1/0/all/0/1"&gt;Sergey Yekhanin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transferring Knowledge Distillation for Multilingual Social Event Detection. (arXiv:2108.03084v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.03084</id>
        <link href="http://arxiv.org/abs/2108.03084"/>
        <updated>2021-08-09T00:49:28.330Z</updated>
        <summary type="html"><![CDATA[Recently published graph neural networks (GNNs) show promising performance at
social event detection tasks. However, most studies are oriented toward
monolingual data in languages with abundant training samples. This has left the
more common multilingual settings and lesser-spoken languages relatively
unexplored. Thus, we present a GNN that incorporates cross-lingual word
embeddings for detecting events in multilingual data streams. The first exploit
is to make the GNN work with multilingual data. For this, we outline a
construction strategy that aligns messages in different languages at both the
node and semantic levels. Relationships between messages are established by
merging entities that are the same but are referred to in different languages.
Non-English message representations are converted into English semantic space
via the cross-lingual word embeddings. The resulting message graph is then
uniformly encoded by a GNN model. In special cases where a lesser-spoken
language needs to be detected, a novel cross-lingual knowledge distillation
framework, called CLKD, exploits prior knowledge learned from similar threads
in English to make up for the paucity of annotated data. Experiments on both
synthetic and real-world datasets show the framework to be highly effective at
detection in both multilingual data and in languages where training samples are
scarce.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1"&gt;Jiaqian Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1"&gt;Hao Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1"&gt;Lei Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1"&gt;Jia Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tong_Y/0/1/0/all/0/1"&gt;Yongxin Tong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Lihong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1"&gt;Xu Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1"&gt;Bo Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1"&gt;Qiang Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Elementary Proof that Q-learning Converges Almost Surely. (arXiv:2108.02827v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02827</id>
        <link href="http://arxiv.org/abs/2108.02827"/>
        <updated>2021-08-09T00:49:28.310Z</updated>
        <summary type="html"><![CDATA[Watkins' and Dayan's Q-learning is a model-free reinforcement learning
algorithm that iteratively refines an estimate for the optimal action-value
function of an MDP by stochastically "visiting" many state-ation pairs [Watkins
and Dayan, 1992]. Variants of the algorithm lie at the heart of numerous recent
state-of-the-art achievements in reinforcement learning, including the
superhuman Atari-playing deep Q-network [Mnih et al., 2015]. The goal of this
paper is to reproduce a precise and (nearly) self-contained proof that
Q-learning converges. Much of the available literature leverages powerful
theory to obtain highly generalizable results in this vein. However, this
approach requires the reader to be familiar with and make many deep connections
to different research areas. A student seeking to deepen their understand of
Q-learning risks becoming caught in a vicious cycle of "RL-learning Hell". For
this reason, we give a complete proof from start to finish using only one
external result from the field of stochastic approximation, despite the fact
that this minimal dependence on other results comes at the expense of some
"shininess".]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Regehr_M/0/1/0/all/0/1"&gt;Matthew T. Regehr&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ayoub_A/0/1/0/all/0/1"&gt;Alex Ayoub&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient recurrent neural network methods for anomalously diffusing single particle short and noisy trajectories. (arXiv:2108.02834v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02834</id>
        <link href="http://arxiv.org/abs/2108.02834"/>
        <updated>2021-08-09T00:49:28.302Z</updated>
        <summary type="html"><![CDATA[Anomalous diffusion occurs at very different scales in nature, from atomic
systems to motions in cell organelles, biological tissues or ecology, and also
in artificial materials, such as cement. Being able to accurately measure the
anomalous exponent associated with a given particle trajectory, thus
determining whether the particle subdiffuses, superdiffuses or performs normal
diffusion is of key importance to understand the diffusion process. Also, it is
often important to trustingly identify the model behind the trajectory, as this
gives a large amount of information on the system dynamics. Both aspects are
particularly difficult when the input data are short and noisy trajectories. It
is even more difficult if one cannot guarantee that the trajectories output in
experiments is homogeneous, hindering the statistical methods based on
ensembles of trajectories. We present a data-driven method able to infer the
anomalous exponent and to identify the type of anomalous diffusion process
behind single, noisy and short trajectories, with good accuracy. This model was
used in our participation in the Anomalous Diffusion (AnDi) Challenge. A
combination of convolutional and recurrent neural networks were used to achieve
state-of-the-art results when compared to methods participating in the AnDi
Challenge, ranking top 4 in both classification and diffusion exponent
regression.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Orts_O/0/1/0/all/0/1"&gt;&amp;#xd2;scar Garibo i Orts&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Garcia_March_M/0/1/0/all/0/1"&gt;Miguel A. Garcia-March&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Conejero_J/0/1/0/all/0/1"&gt;J. Alberto Conejero&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Path classification by stochastic linear recurrent neural networks. (arXiv:2108.03090v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2108.03090</id>
        <link href="http://arxiv.org/abs/2108.03090"/>
        <updated>2021-08-09T00:49:28.288Z</updated>
        <summary type="html"><![CDATA[We investigate the functioning of a classifying biological neural network
from the perspective of statistical learning theory, modelled, in a simplified
setting, as a continuous-time stochastic recurrent neural network (RNN) with
identity activation function. In the purely stochastic (robust) regime, we give
a generalisation error bound that holds with high probability, thus showing
that the empirical risk minimiser is the best-in-class hypothesis. We show that
RNNs retain a partial signature of the paths they are fed as the unique
information exploited for training and classification tasks. We argue that
these RNNs are easy to train and robust and back these observations with
numerical experiments on both synthetic and real data. We also exhibit a
trade-off phenomenon between accuracy and robustness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Bartolomaeus_W/0/1/0/all/0/1"&gt;Wiebke Bartolomaeus&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Boutaib_Y/0/1/0/all/0/1"&gt;Youness Boutaib&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Nestler_S/0/1/0/all/0/1"&gt;Sandra Nestler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Rauhut_H/0/1/0/all/0/1"&gt;Holger Rauhut&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Communicative Learning with Natural Gestures for Embodied Navigation Agents with Human-in-the-Scene. (arXiv:2108.02846v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2108.02846</id>
        <link href="http://arxiv.org/abs/2108.02846"/>
        <updated>2021-08-09T00:49:28.272Z</updated>
        <summary type="html"><![CDATA[Human-robot collaboration is an essential research topic in artificial
intelligence (AI), enabling researchers to devise cognitive AI systems and
affords an intuitive means for users to interact with the robot. Of note,
communication plays a central role. To date, prior studies in embodied agent
navigation have only demonstrated that human languages facilitate communication
by instructions in natural languages. Nevertheless, a plethora of other forms
of communication is left unexplored. In fact, human communication originated in
gestures and oftentimes is delivered through multimodal cues, e.g. "go there"
with a pointing gesture. To bridge the gap and fill in the missing dimension of
communication in embodied agent navigation, we propose investigating the
effects of using gestures as the communicative interface instead of verbal
cues. Specifically, we develop a VR-based 3D simulation environment, named
Ges-THOR, based on AI2-THOR platform. In this virtual environment, a human
player is placed in the same virtual scene and shepherds the artificial agent
using only gestures. The agent is tasked to solve the navigation problem guided
by natural gestures with unknown semantics; we do not use any predefined
gestures due to the diversity and versatile nature of human gestures. We argue
that learning the semantics of natural gestures is mutually beneficial to
learning the navigation task--learn to communicate and communicate to learn. In
a series of experiments, we demonstrate that human gesture cues, even without
predefined semantics, improve the object-goal navigation for an embodied agent,
outperforming various state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1"&gt;Qi Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1"&gt;Cheng-Ju Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yixin Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joo_J/0/1/0/all/0/1"&gt;Jungseock Joo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Attainment Regions in Feature-Parameter Space for High-Level Debugging in Autonomous Robots. (arXiv:2108.03150v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2108.03150</id>
        <link href="http://arxiv.org/abs/2108.03150"/>
        <updated>2021-08-09T00:49:28.258Z</updated>
        <summary type="html"><![CDATA[Understanding a controller's performance in different scenarios is crucial
for robots that are going to be deployed in safety-critical tasks. If we do not
have a model of the dynamics of the world, which is often the case in complex
domains, we may need to approximate a performance function of the robot based
on its interaction with the environment. Such a performance function gives us
insights into the behaviour of the robot, allowing us to fine-tune the
controller with manual interventions. In high-dimensionality systems, where the
actionstate space is large, fine-tuning a controller is non-trivial. To
overcome this problem, we propose a performance function whose domain is
defined by external features and parameters of the controller. Attainment
regions are defined over such a domain defined by feature-parameter pairs, and
serve the purpose of enabling prediction of successful execution of the task.
The use of the feature-parameter space -in contrast to the action-state space-
allows us to adapt, explain and finetune the controller over a simpler (i.e.,
lower dimensional space). When the robot successfully executes the task, we use
the attainment regions to gain insights into the limits of the controller, and
its robustness. When the robot fails to execute the task, we use the regions to
debug the controller and find adaptive and counterfactual changes to the
solutions. Another advantage of this approach is that we can generalise through
the use of Gaussian processes regression of the performance function in the
high-dimensional space. To test our approach, we demonstrate learning an
approximation to the performance function in simulation, with a mobile robot
traversing different terrain conditions. Then, with a sample-efficient method,
we propagate the attainment regions to a physical robot in a similar
environment.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Smith_S/0/1/0/all/0/1"&gt;Sim&amp;#xf3;n C. Smith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramamoorthy_S/0/1/0/all/0/1"&gt;Subramanian Ramamoorthy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Simple Modifications to Improve Tabular Neural Networks. (arXiv:2108.03214v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.03214</id>
        <link href="http://arxiv.org/abs/2108.03214"/>
        <updated>2021-08-09T00:49:28.237Z</updated>
        <summary type="html"><![CDATA[There is growing interest in neural network architectures for tabular data.
Many general-purpose tabular deep learning models have been introduced
recently, with performance sometimes rivaling gradient boosted decision trees
(GBDTs). These recent models draw inspiration from various sources, including
GBDTs, factorization machines, and neural networks from other application
domains. Previous tabular neural networks are also drawn upon, but are possibly
under-considered, especially models associated with specific tabular problems.
This paper focuses on several such models, and proposes modifications for
improving their performance. When modified, these models are shown to be
competitive with leading general-purpose tabular models, including GBDTs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fiedler_J/0/1/0/all/0/1"&gt;James Fiedler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Feature Augmented Hybrid CNN for Stress Recognition Using Wrist-based Photoplethysmography Sensor. (arXiv:2108.03166v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2108.03166</id>
        <link href="http://arxiv.org/abs/2108.03166"/>
        <updated>2021-08-09T00:49:28.230Z</updated>
        <summary type="html"><![CDATA[Stress is a physiological state that hampers mental health and has serious
consequences to physical health. Moreover, the COVID-19 pandemic has increased
stress levels among people across the globe. Therefore, continuous monitoring
and detection of stress are necessary. The recent advances in wearable devices
have allowed the monitoring of several physiological signals related to stress.
Among them, wrist-worn wearable devices like smartwatches are most popular due
to their convenient usage. And the photoplethysmography (PPG) sensor is the
most prevalent sensor in almost all consumer-grade wrist-worn smartwatches.
Therefore, this paper focuses on using a wrist-based PPG sensor that collects
Blood Volume Pulse (BVP) signals to detect stress which may be applicable for
consumer-grade wristwatches. Moreover, state-of-the-art works have used either
classical machine learning algorithms to detect stress using hand-crafted
features or have used deep learning algorithms like Convolutional Neural
Network (CNN) which automatically extracts features. This paper proposes a
novel hybrid CNN (H-CNN) classifier that uses both the hand-crafted features
and the automatically extracted features by CNN to detect stress using the BVP
signal. Evaluation on the benchmark WESAD dataset shows that, for 3-class
classification (Baseline vs. Stress vs. Amusement), our proposed H-CNN
outperforms traditional classifiers and normal CNN by 5% and 7% accuracy, and
10% and 7% macro F1 score, respectively. Also for 2-class classification
(Stress vs. Non-stress), our proposed H-CNN outperforms traditional classifiers
and normal CNN by 3% and ~5% accuracy, and ~3% and ~7% macro F1 score,
respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Rashid_N/0/1/0/all/0/1"&gt;Nafiul Rashid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chen_L/0/1/0/all/0/1"&gt;Luke Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dautta_M/0/1/0/all/0/1"&gt;Manik Dautta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jimenez_A/0/1/0/all/0/1"&gt;Abel Jimenez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tseng_P/0/1/0/all/0/1"&gt;Peter Tseng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Faruque_M/0/1/0/all/0/1"&gt;Mohammad Abdullah Al Faruque&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interpolation can hurt robust generalization even when there is no noise. (arXiv:2108.02883v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2108.02883</id>
        <link href="http://arxiv.org/abs/2108.02883"/>
        <updated>2021-08-09T00:49:28.209Z</updated>
        <summary type="html"><![CDATA[Numerous recent works show that overparameterization implicitly reduces
variance for min-norm interpolators and max-margin classifiers. These findings
suggest that ridge regularization has vanishing benefits in high dimensions. We
challenge this narrative by showing that, even in the absence of noise,
avoiding interpolation through ridge regularization can significantly improve
generalization. We prove this phenomenon for the robust risk of both linear
regression and classification and hence provide the first theoretical result on
robust overfitting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Donhauser_K/0/1/0/all/0/1"&gt;Konstantin Donhauser&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Tifrea_A/0/1/0/all/0/1"&gt;Alexandru &amp;#x162;ifrea&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Aerni_M/0/1/0/all/0/1"&gt;Michael Aerni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Heckel_R/0/1/0/all/0/1"&gt;Reinhard Heckel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Yang_F/0/1/0/all/0/1"&gt;Fanny Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deriving Disinformation Insights from Geolocalized Twitter Callouts. (arXiv:2108.03067v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.03067</id>
        <link href="http://arxiv.org/abs/2108.03067"/>
        <updated>2021-08-09T00:49:28.201Z</updated>
        <summary type="html"><![CDATA[This paper demonstrates a two-stage method for deriving insights from social
media data relating to disinformation by applying a combination of geospatial
classification and embedding-based language modelling across multiple
languages. In particular, the analysis in centered on Twitter and
disinformation for three European languages: English, French and Spanish.
Firstly, Twitter data is classified into European and non-European sets using
BERT. Secondly, Word2vec is applied to the classified texts resulting in
Eurocentric, non-Eurocentric and global representations of the data for the
three target languages. This comparative analysis demonstrates not only the
efficacy of the classification method but also highlights geographic, temporal
and linguistic differences in the disinformation-related media. Thus, the
contributions of the work are threefold: (i) a novel language-independent
transformer-based geolocation method; (ii) an analytical approach that exploits
lexical specificity and word embeddings to interrogate user-generated content;
and (iii) a dataset of 36 million disinformation related tweets in English,
French and Spanish.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tuxworth_D/0/1/0/all/0/1"&gt;David Tuxworth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Antypas_D/0/1/0/all/0/1"&gt;Dimosthenis Antypas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Espinosa_Anke_L/0/1/0/all/0/1"&gt;Luis Espinosa-Anke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Camacho_Collados_J/0/1/0/all/0/1"&gt;Jose Camacho-Collados&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Preece_A/0/1/0/all/0/1"&gt;Alun Preece&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rogers_D/0/1/0/all/0/1"&gt;David Rogers&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[User Scheduling for Federated Learning Through Over-the-Air Computation. (arXiv:2108.02891v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02891</id>
        <link href="http://arxiv.org/abs/2108.02891"/>
        <updated>2021-08-09T00:49:28.181Z</updated>
        <summary type="html"><![CDATA[A new machine learning (ML) technique termed as federated learning (FL) aims
to preserve data at the edge devices and to only exchange ML model parameters
in the learning process. FL not only reduces the communication needs but also
helps to protect the local privacy. Although FL has these advantages, it can
still experience large communication latency when there are massive edge
devices connected to the central parameter server (PS) and/or millions of model
parameters involved in the learning process. Over-the-air computation (AirComp)
is capable of computing while transmitting data by allowing multiple devices to
send data simultaneously by using analog modulation. To achieve good
performance in FL through AirComp, user scheduling plays a critical role. In
this paper, we investigate and compare different user scheduling policies,
which are based on various criteria such as wireless channel conditions and the
significance of model updates. Receiver beamforming is applied to minimize the
mean-square-error (MSE) of the distortion of function aggregation result via
AirComp. Simulation results show that scheduling based on the significance of
model updates has smaller fluctuations in the training process while scheduling
based on channel condition has the advantage on energy efficiency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1"&gt;Xiang Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1"&gt;Haijian Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1"&gt;Qun Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_R/0/1/0/all/0/1"&gt;Rose Qingyang Hu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Auxiliary Class Based Multiple Choice Learning. (arXiv:2108.02949v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02949</id>
        <link href="http://arxiv.org/abs/2108.02949"/>
        <updated>2021-08-09T00:49:28.174Z</updated>
        <summary type="html"><![CDATA[The merit of ensemble learning lies in having different outputs from many
individual models on a single input, i.e., the diversity of the base models.
The high quality of diversity can be achieved when each model is specialized to
different subsets of the whole dataset. Moreover, when each model explicitly
knows to which subsets it is specialized, more opportunities arise to improve
diversity. In this paper, we propose an advanced ensemble method, called
Auxiliary class based Multiple Choice Learning (AMCL), to ultimately specialize
each model under the framework of multiple choice learning (MCL). The
advancement of AMCL is originated from three novel techniques which control the
framework from different directions: 1) the concept of auxiliary class to
provide more distinct information through the labels, 2) the strategy, named
memory-based assignment, to determine the association between the inputs and
the models, and 3) the feature fusion module to achieve generalized features.
To demonstrate the performance of our method compared to all variants of MCL
methods, we conduct extensive experiments on the image classification and
segmentation tasks. Overall, the performance of AMCL exceeds all others in most
of the public datasets trained with various networks as members of the
ensembles.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1"&gt;Sihwan Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jung_D/0/1/0/all/0/1"&gt;Dae Yon Jung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_T/0/1/0/all/0/1"&gt;Taejang Park&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Enterprise Analytics using Graph Database and Graph-based Deep Learning. (arXiv:2108.02867v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02867</id>
        <link href="http://arxiv.org/abs/2108.02867"/>
        <updated>2021-08-09T00:49:28.168Z</updated>
        <summary type="html"><![CDATA[In a business-to-business (B2B) customer relationship management (CRM) use
case, each client is a potential business organization/company with a solid
business strategy and focused and rational decisions. This paper introduces a
graph-based analytics approach to improve CRM within a B2B environment. In our
approach, in the first instance, we have designed a graph database using the
Neo4j platform. Secondly, the graph database has been investigated by using
data mining and exploratory analysis coupled with cypher graph query language.
Specifically, we have applied the graph convolution network (GCN) to enable CRM
analytics to forecast sales. This is the first step towards a GCN-based binary
classification based on graph databases in the domain of B2B CRM. We evaluate
the performance of the proposed GCN model on graph databases and compare it
with Random Forest (RF), Convolutional Neural Network (CNN), and Artificial
Neural Network (ANN). The proposed GCN approach is further augmented with the
shortest path and eigenvector centrality attribute to significantly improve the
accuracy of sales prediction. Experimental results reveal that the proposed
graph-based deep learning approach outperforms the Random Forests (RsF) and two
deep learning models, i.e., CNN and ANN under different combinations of graph
features.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Henna_S/0/1/0/all/0/1"&gt;Shagufta Henna&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kalliadan_S/0/1/0/all/0/1"&gt;Shyam Krishnan Kalliadan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spatiotemporal Contrastive Learning of Facial Expressions in Videos. (arXiv:2108.03064v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03064</id>
        <link href="http://arxiv.org/abs/2108.03064"/>
        <updated>2021-08-09T00:49:28.161Z</updated>
        <summary type="html"><![CDATA[We propose a self-supervised contrastive learning approach for facial
expression recognition (FER) in videos. We propose a novel temporal
sampling-based augmentation scheme to be utilized in addition to standard
spatial augmentations used for contrastive learning. Our proposed temporal
augmentation scheme randomly picks from one of three temporal sampling
techniques: (1) pure random sampling, (2) uniform sampling, and (3) sequential
sampling. This is followed by a combination of up to three standard spatial
augmentations. We then use a deep R(2+1)D network for FER, which we train in a
self-supervised fashion based on the augmentations and subsequently fine-tune.
Experiments are performed on the Oulu-CASIA dataset and the performance is
compared to other works in FER. The results indicate that our method achieves
an accuracy of 89.4%, setting a new state-of-the-art by outperforming other
works. Additional experiments and analysis confirm the considerable
contribution of the proposed temporal augmentation versus the existing spatial
ones.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1"&gt;Shuvendu Roy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Etemad_A/0/1/0/all/0/1"&gt;Ali Etemad&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Learning of Debiased Representations with Pseudo-Attributes. (arXiv:2108.02943v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02943</id>
        <link href="http://arxiv.org/abs/2108.02943"/>
        <updated>2021-08-09T00:49:28.153Z</updated>
        <summary type="html"><![CDATA[Dataset bias is a critical challenge in machine learning, and its negative
impact is aggravated when models capture unintended decision rules with
spurious correlations. Although existing works often handle this issue using
human supervision, the availability of the proper annotations is impractical
and even unrealistic. To better tackle this challenge, we propose a simple but
effective debiasing technique in an unsupervised manner. Specifically, we
perform clustering on the feature embedding space and identify pseudoattributes
by taking advantage of the clustering results even without an explicit
attribute supervision. Then, we employ a novel cluster-based reweighting scheme
for learning debiased representation; this prevents minority groups from being
discounted for minimizing the overall loss, which is desirable for worst-case
generalization. The extensive experiments demonstrate the outstanding
performance of our approach on multiple standard benchmarks, which is even as
competitive as the supervised counterpart.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Seo_S/0/1/0/all/0/1"&gt;Seonguk Seo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Joon-Young Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1"&gt;Bohyung Han&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mitigating dataset harms requires stewardship: Lessons from 1000 papers. (arXiv:2108.02922v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02922</id>
        <link href="http://arxiv.org/abs/2108.02922"/>
        <updated>2021-08-09T00:49:28.130Z</updated>
        <summary type="html"><![CDATA[Concerns about privacy, bias, and harmful applications have shone a light on
the ethics of machine learning datasets, even leading to the retraction of
prominent datasets including DukeMTMC, MS-Celeb-1M, TinyImages, and VGGFace2.
In response, the machine learning community has called for higher ethical
standards, transparency efforts, and technical fixes in the dataset creation
process. The premise of our work is that these efforts can be more effective if
informed by an understanding of how datasets are used in practice in the
research community. We study three influential face and person recognition
datasets - DukeMTMC, MS-Celeb-1M, and Labeled Faces in the Wild (LFW) - by
analyzing nearly 1000 papers that cite them. We found that the creation of
derivative datasets and models, broader technological and social change, the
lack of clarity of licenses, and dataset management practices can introduce a
wide range of ethical concerns. We conclude by suggesting a distributed
approach that can mitigate these harms, making recommendations to dataset
creators, conference program committees, dataset users, and the broader
research community.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Peng_K/0/1/0/all/0/1"&gt;Kenny Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mathur_A/0/1/0/all/0/1"&gt;Arunesh Mathur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Narayanan_A/0/1/0/all/0/1"&gt;Arvind Narayanan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding Human Innate Immune System Dependencies using Graph Neural Networks. (arXiv:2108.02872v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02872</id>
        <link href="http://arxiv.org/abs/2108.02872"/>
        <updated>2021-08-09T00:49:28.108Z</updated>
        <summary type="html"><![CDATA[Since the rapid outbreak of Covid-19 and with no approved vaccines to date,
profound research interest has emerged to understand the innate immune response
to viruses. This understanding can help to inhibit virus replication, prolong
adaptive immune response, accelerated virus clearance, and tissue recovery, a
key milestone to propose a vaccine to combat coronaviruses (CoVs), e.g.,
Covid-19. Although an innate immune system triggers inflammatory responses
against CoVs upon recognition of viruses, however, a vaccine is the ultimate
protection against CoV spread. The development of this vaccine is
time-consuming and requires a deep understanding of the innate immune response
system. In this work, we propose a graph neural network-based model that
exploits the interactions between pattern recognition receptors (PRRs), i.e.,
the human immune response system. These interactions can help to recognize
pathogen-associated molecular patterns (PAMPs) to predict the activation
requirements of each PRR. The immune response information of each PRR is
derived from combining its historical PAMPs activation coupled with the modeled
effect on the same from PRRs in its neighborhood. On one hand, this work can
help to understand how long Covid-19 can confer immunity where a strong immune
response means people already been infected can safely return to work. On the
other hand, this GNN-based understanding can also abode well for vaccine
development efforts. Our proposal has been evaluated using CoVs immune response
dataset, with results showing an average IFNs activation prediction accuracy of
90%, compared to 85% using feed-forward neural networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Henna_S/0/1/0/all/0/1"&gt;Shagufta Henna&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[End-to-End Semi-Supervised Object Detection with Soft Teacher. (arXiv:2106.09018v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.09018</id>
        <link href="http://arxiv.org/abs/2106.09018"/>
        <updated>2021-08-09T00:49:28.100Z</updated>
        <summary type="html"><![CDATA[This paper presents an end-to-end semi-supervised object detection approach,
in contrast to previous more complex multi-stage methods. The end-to-end
training gradually improves pseudo label qualities during the curriculum, and
the more and more accurate pseudo labels in turn benefit object detection
training. We also propose two simple yet effective techniques within this
framework: a soft teacher mechanism where the classification loss of each
unlabeled bounding box is weighed by the classification score produced by the
teacher network; a box jittering approach to select reliable pseudo boxes for
the learning of box regression. On the COCO benchmark, the proposed approach
outperforms previous methods by a large margin under various labeling ratios,
i.e. 1\%, 5\% and 10\%. Moreover, our approach proves to perform also well when
the amount of labeled data is relatively large. For example, it can improve a
40.9 mAP baseline detector trained using the full COCO training set by +3.6
mAP, reaching 44.5 mAP, by leveraging the 123K unlabeled images of COCO. On the
state-of-the-art Swin Transformer based object detector (58.9 mAP on test-dev),
it can still significantly improve the detection accuracy by +1.5 mAP, reaching
60.4 mAP, and improve the instance segmentation accuracy by +1.2 mAP, reaching
52.4 mAP. Further incorporating with the Object365 pre-trained model, the
detection accuracy reaches 61.3 mAP and the instance segmentation accuracy
reaches 53.0 mAP, pushing the new state-of-the-art.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1"&gt;Mengde Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zheng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1"&gt;Han Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jianfeng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Lijuan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1"&gt;Fangyun Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1"&gt;Xiang Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zicheng Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Is it Fake? News Disinformation Detection on South African News Websites. (arXiv:2108.02941v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.02941</id>
        <link href="http://arxiv.org/abs/2108.02941"/>
        <updated>2021-08-09T00:49:28.092Z</updated>
        <summary type="html"><![CDATA[Disinformation through fake news is an ongoing problem in our society and has
become easily spread through social media. The most cost and time effective way
to filter these large amounts of data is to use a combination of human and
technical interventions to identify it. From a technical perspective, Natural
Language Processing (NLP) is widely used in detecting fake news. Social media
companies use NLP techniques to identify the fake news and warn their users,
but fake news may still slip through undetected. It is especially a problem in
more localised contexts (outside the United States of America). How do we
adjust fake news detection systems to work better for local contexts such as in
South Africa. In this work we investigate fake news detection on South African
websites. We curate a dataset of South African fake news and then train
detection models. We contrast this with using widely available fake news
datasets (from mostly USA website). We also explore making the datasets more
diverse by combining them and observe the differences in behaviour in writing
between nations' fake news using interpretable machine learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wet_H/0/1/0/all/0/1"&gt;Harm de Wet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marivate_V/0/1/0/all/0/1"&gt;Vukosi Marivate&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multimodal Meta-Learning for Time Series Regression. (arXiv:2108.02842v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02842</id>
        <link href="http://arxiv.org/abs/2108.02842"/>
        <updated>2021-08-09T00:49:28.085Z</updated>
        <summary type="html"><![CDATA[Recent work has shown the efficiency of deep learning models such as Fully
Convolutional Networks (FCN) or Recurrent Neural Networks (RNN) to deal with
Time Series Regression (TSR) problems. These models sometimes need a lot of
data to be able to generalize, yet the time series are sometimes not long
enough to be able to learn patterns. Therefore, it is important to make use of
information across time series to improve learning. In this paper, we will
explore the idea of using meta-learning for quickly adapting model parameters
to new short-history time series by modifying the original idea of Model
Agnostic Meta-Learning (MAML) \cite{finn2017model}. Moreover, based on prior
work on multimodal MAML \cite{vuorio2019multimodal}, we propose a method for
conditioning parameters of the model through an auxiliary network that encodes
global information of the time series to extract meta-features. Finally, we
apply the data to time series of different domains, such as pollution
measurements, heart-rate sensors, and electrical battery data. We show
empirically that our proposed meta-learning method learns TSR with few data
fast and outperforms the baselines in 9 of 12 experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Arango_S/0/1/0/all/0/1"&gt;Sebastian Pineda Arango&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heinrich_F/0/1/0/all/0/1"&gt;Felix Heinrich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Madhusudhanan_K/0/1/0/all/0/1"&gt;Kiran Madhusudhanan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schmidt_Thieme_L/0/1/0/all/0/1"&gt;Lars Schmidt-Thieme&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interpretable Summaries of Black Box Incident Triaging with Subgroup Discovery. (arXiv:2108.03013v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2108.03013</id>
        <link href="http://arxiv.org/abs/2108.03013"/>
        <updated>2021-08-09T00:49:28.078Z</updated>
        <summary type="html"><![CDATA[The need of predictive maintenance comes with an increasing number of
incidents reported by monitoring systems and equipment/software users. In the
front line, on-call engineers (OCEs) have to quickly assess the degree of
severity of an incident and decide which service to contact for corrective
actions. To automate these decisions, several predictive models have been
proposed, but the most efficient models are opaque (say, black box), strongly
limiting their adoption. In this paper, we propose an efficient black box model
based on 170K incidents reported to our company over the last 7 years and
emphasize on the need of automating triage when incidents are massively
reported on thousands of servers running our product, an ERP. Recent
developments in eXplainable Artificial Intelligence (XAI) help in providing
global explanations to the model, but also, and most importantly, with local
explanations for each model prediction/outcome. Sadly, providing a human with
an explanation for each outcome is not conceivable when dealing with an
important number of daily predictions. To address this problem, we propose an
original data-mining method rooted in Subgroup Discovery, a pattern mining
technique with the natural ability to group objects that share similar
explanations of their black box predictions and provide a description for each
group. We evaluate this approach and present our preliminary results which give
us good hope towards an effective OCE's adoption. We believe that this approach
provides a new way to address the problem of model agnostic outcome
explanation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Remil_Y/0/1/0/all/0/1"&gt;Youcef Remil&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bendimerad_A/0/1/0/all/0/1"&gt;Anes Bendimerad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Plantevit_M/0/1/0/all/0/1"&gt;Marc Plantevit&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Robardet_C/0/1/0/all/0/1"&gt;C&amp;#xe9;line Robardet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kaytoue_M/0/1/0/all/0/1"&gt;Mehdi Kaytoue&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Reinforcement Learning for Intelligent Reflecting Surface-assisted D2D Communications. (arXiv:2108.02892v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2108.02892</id>
        <link href="http://arxiv.org/abs/2108.02892"/>
        <updated>2021-08-09T00:49:28.057Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a deep reinforcement learning (DRL) approach for
solving the optimisation problem of the network's sum-rate in device-to-device
(D2D) communications supported by an intelligent reflecting surface (IRS). The
IRS is deployed to mitigate the interference and enhance the signal between the
D2D transmitter and the associated D2D receiver. Our objective is to jointly
optimise the transmit power at the D2D transmitter and the phase shift matrix
at the IRS to maximise the network sum-rate. We formulate a Markov decision
process and then propose the proximal policy optimisation for solving the
maximisation game. Simulation results show impressive performance in terms of
the achievable rate and processing time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Nguyen_K/0/1/0/all/0/1"&gt;Khoi Khac Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Masaracchia_A/0/1/0/all/0/1"&gt;Antonino Masaracchia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yin_C/0/1/0/all/0/1"&gt;Cheng Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Nguyen_L/0/1/0/all/0/1"&gt;Long D. Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dobre_O/0/1/0/all/0/1"&gt;Octavia A. Dobre&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Duong_T/0/1/0/all/0/1"&gt;Trung Q. Duong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Incremental Feature Learning For Infinite Data. (arXiv:2108.02932v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02932</id>
        <link href="http://arxiv.org/abs/2108.02932"/>
        <updated>2021-08-09T00:49:28.037Z</updated>
        <summary type="html"><![CDATA[This study addresses the actual behavior of the credit-card fraud detection
environment where financial transactions containing sensitive data must not be
amassed in an enormous amount to conduct learning. We introduce a new adaptive
learning approach that adjusts frequently and efficiently to new transaction
chunks; each chunk is discarded after each incremental training step. Our
approach combines transfer learning and incremental feature learning. The
former improves the feature relevancy for subsequent chunks, and the latter, a
new paradigm, increases accuracy during training by determining the optimal
network architecture dynamically for each new chunk. The architectures of past
incremental approaches are fixed; thus, the accuracy may not improve with new
chunks. We show the effectiveness and superiority of our approach
experimentally on an actual fraud dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sadreddin_A/0/1/0/all/0/1"&gt;Armin Sadreddin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sadaoui_S/0/1/0/all/0/1"&gt;Samira Sadaoui&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semi-Supervised Semantic Segmentation with Pixel-Level Contrastive Learning from a Class-wise Memory Bank. (arXiv:2104.13415v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.13415</id>
        <link href="http://arxiv.org/abs/2104.13415"/>
        <updated>2021-08-09T00:49:28.028Z</updated>
        <summary type="html"><![CDATA[This work presents a novel approach for semi-supervised semantic
segmentation. The key element of this approach is our contrastive learning
module that enforces the segmentation network to yield similar pixel-level
feature representations for same-class samples across the whole dataset. To
achieve this, we maintain a memory bank continuously updated with relevant and
high-quality feature vectors from labeled data. In an end-to-end training, the
features from both labeled and unlabeled data are optimized to be similar to
same-class samples from the memory bank. Our approach outperforms the current
state-of-the-art for semi-supervised semantic segmentation and semi-supervised
domain adaptation on well-known public benchmarks, with larger improvements on
the most challenging scenarios, i.e., less available labeled data.
https://github.com/Shathe/SemiSeg-Contrastive]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alonso_I/0/1/0/all/0/1"&gt;Inigo Alonso&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sabater_A/0/1/0/all/0/1"&gt;Alberto Sabater&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ferstl_D/0/1/0/all/0/1"&gt;David Ferstl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Montesano_L/0/1/0/all/0/1"&gt;Luis Montesano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Murillo_A/0/1/0/all/0/1"&gt;Ana C. Murillo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Supervised Neural Networks for Illiquid Alternative Asset Cash Flow Forecasting. (arXiv:2108.02853v1 [q-fin.GN])]]></title>
        <id>http://arxiv.org/abs/2108.02853</id>
        <link href="http://arxiv.org/abs/2108.02853"/>
        <updated>2021-08-09T00:49:28.021Z</updated>
        <summary type="html"><![CDATA[Institutional investors have been increasing the allocation of the illiquid
alternative assets such as private equity funds in their portfolios, yet there
exists a very limited literature on cash flow forecasting of illiquid
alternative assets. The net cash flow of private equity funds typically follow
a J-curve pattern, however the timing and the size of the contributions and
distributions depend on the investment opportunities. In this paper, we develop
a benchmark model and present two novel approaches (direct vs. indirect) to
predict the cash flows of private equity funds. We introduce a sliding window
approach to apply on our cash flow data because different vintage year funds
contain different lengths of cash flow information. We then pass the data to an
LSTM/ GRU model to predict the future cash flows either directly or indirectly
(based on the benchmark model). We further integrate macroeconomic indicators
into our data, which allows us to consider the impact of market environment on
cash flows and to apply stress testing. Our results indicate that the direct
model is easier to implement compared to the benchmark model and the indirect
model, but still the predicted cash flows align better with the actual cash
flows. We also show that macroeconomic variables improve the performance of the
direct model whereas the impact is not obvious on the indirect model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-fin/1/au:+Karatas_T/0/1/0/all/0/1"&gt;Tugce Karatas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Klinkert_F/0/1/0/all/0/1"&gt;Federico Klinkert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Hirsa_A/0/1/0/all/0/1"&gt;Ali Hirsa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lights, Camera, Action! A Framework to Improve NLP Accuracy over OCR documents. (arXiv:2108.02899v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.02899</id>
        <link href="http://arxiv.org/abs/2108.02899"/>
        <updated>2021-08-09T00:49:28.013Z</updated>
        <summary type="html"><![CDATA[Document digitization is essential for the digital transformation of our
societies, yet a crucial step in the process, Optical Character Recognition
(OCR), is still not perfect. Even commercial OCR systems can produce
questionable output depending on the fidelity of the scanned documents. In this
paper, we demonstrate an effective framework for mitigating OCR errors for any
downstream NLP task, using Named Entity Recognition (NER) as an example. We
first address the data scarcity problem for model training by constructing a
document synthesis pipeline, generating realistic but degraded data with NER
labels. We measure the NER accuracy drop at various degradation levels and show
that a text restoration model, trained on the degraded data, significantly
closes the NER accuracy gaps caused by OCR errors, including on an
out-of-domain dataset. For the benefit of the community, we have made the
document synthesis pipeline available as an open-source project.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gupte_A/0/1/0/all/0/1"&gt;Amit Gupte&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Romanov_A/0/1/0/all/0/1"&gt;Alexey Romanov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mantravadi_S/0/1/0/all/0/1"&gt;Sahitya Mantravadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Banda_D/0/1/0/all/0/1"&gt;Dalitso Banda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jianjie Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khan_R/0/1/0/all/0/1"&gt;Raza Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meenal_L/0/1/0/all/0/1"&gt;Lakshmanan Ramu Meenal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1"&gt;Benjamin Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srinivasan_S/0/1/0/all/0/1"&gt;Soundar Srinivasan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[THALIS: Human-Machine Analysis of Longitudinal Symptoms in Cancer Therapy. (arXiv:2108.02817v1 [cs.HC])]]></title>
        <id>http://arxiv.org/abs/2108.02817</id>
        <link href="http://arxiv.org/abs/2108.02817"/>
        <updated>2021-08-09T00:49:27.993Z</updated>
        <summary type="html"><![CDATA[Although cancer patients survive years after oncologic therapy, they are
plagued with long-lasting or permanent residual symptoms, whose severity, rate
of development, and resolution after treatment vary largely between survivors.
The analysis and interpretation of symptoms is complicated by their partial
co-occurrence, variability across populations and across time, and, in the case
of cancers that use radiotherapy, by further symptom dependency on the tumor
location and prescribed treatment. We describe THALIS, an environment for
visual analysis and knowledge discovery from cancer therapy symptom data,
developed in close collaboration with oncology experts. Our approach leverages
unsupervised machine learning methodology over cohorts of patients, and, in
conjunction with custom visual encodings and interactions, provides context for
new patients based on patients with similar diagnostic features and symptom
evolution. We evaluate this approach on data collected from a cohort of head
and neck cancer patients. Feedback from our clinician collaborators indicates
that THALIS supports knowledge discovery beyond the limits of machines or
humans alone, and that it serves as a valuable tool in both the clinic and
symptom research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Floricel_C/0/1/0/all/0/1"&gt;Carla Floricel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nipu_N/0/1/0/all/0/1"&gt;Nafiul Nipu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Biggs_M/0/1/0/all/0/1"&gt;Mikayla Biggs&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wentzel_A/0/1/0/all/0/1"&gt;Andrew Wentzel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Canahuate_G/0/1/0/all/0/1"&gt;Guadalupe Canahuate&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dijk_L/0/1/0/all/0/1"&gt;Lisanne Van Dijk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mohamed_A/0/1/0/all/0/1"&gt;Abdallah Mohamed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fuller_C/0/1/0/all/0/1"&gt;C. David Fuller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marai_G/0/1/0/all/0/1"&gt;G. Elisabeta Marai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FrequentNet : A New Interpretable Deep Learning Baseline for Image Classification. (arXiv:2001.01034v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2001.01034</id>
        <link href="http://arxiv.org/abs/2001.01034"/>
        <updated>2021-08-09T00:49:27.986Z</updated>
        <summary type="html"><![CDATA[This paper has proposed a new baseline deep learning model of more benefits
for image classification. Different from the convolutional neural network(CNN)
practice where filters are trained by back propagation to represent different
patterns of an image, we are inspired by a method called "PCANet" in "PCANet: A
Simple Deep Learning Baseline for Image Classification?" to choose filter
vectors from basis vectors in frequency domain like Fourier coefficients or
wavelets without back propagation. Researchers have demonstrated that those
basis in frequency domain can usually provide physical insights, which adds to
the interpretability of the model by analyzing the frequencies selected.
Besides, the training process will also be more time efficient, mathematically
clear and interpretable compared with the "black-box" training process of CNN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yifei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_K/0/1/0/all/0/1"&gt;Kuangyan Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1"&gt;Yiming Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1"&gt;Liao Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[COV-ELM classifier: An Extreme Learning Machine based identification of COVID-19 using Chest X-Ray Images. (arXiv:2007.08637v5 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.08637</id>
        <link href="http://arxiv.org/abs/2007.08637"/>
        <updated>2021-08-09T00:49:27.980Z</updated>
        <summary type="html"><![CDATA[Coronaviruses constitute a family of viruses that gives rise to respiratory
diseases. As COVID-19 is highly contagious, early diagnosis of COVID-19 is
crucial for an effective treatment strategy. However, the RT-PCR test which is
considered to be a gold standard in the diagnosis of COVID-19 suffers from a
high false-negative rate. Chest X-ray (CXR) image analysis has emerged as a
feasible and effective diagnostic technique towards this objective. In this
work, we propose the COVID-19 classification problem as a three-class
classification problem to distinguish between COVID-19, normal, and pneumonia
classes. We propose a three-stage framework, named COV-ELM. Stage one deals
with preprocessing and transformation while stage two deals with feature
extraction. These extracted features are passed as an input to the ELM at the
third stage, resulting in the identification of COVID-19. The choice of ELM in
this work has been motivated by its faster convergence, better generalization
capability, and shorter training time in comparison to the conventional
gradient-based learning algorithms. As bigger and diverse datasets become
available, ELM can be quickly retrained as compared to its gradient-based
competitor models. The proposed model achieved a macro average F1-score of 0.95
and the overall sensitivity of ${0.94 \pm 0.02} at a 95% confidence interval.
When compared to state-of-the-art machine learning algorithms, the COV-ELM is
found to outperform its competitors in this three-class classification
scenario. Further, LIME has been integrated with the proposed COV-ELM model to
generate annotated CXR images. The annotations are based on the superpixels
that have contributed to distinguish between the different classes. It was
observed that the superpixels correspond to the regions of the human lungs that
are clinically observed in COVID-19 and Pneumonia cases.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Rajpal_S/0/1/0/all/0/1"&gt;Sheetal Rajpal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Agarwal_M/0/1/0/all/0/1"&gt;Manoj Agarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rajpal_A/0/1/0/all/0/1"&gt;Ankit Rajpal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lakhyani_N/0/1/0/all/0/1"&gt;Navin Lakhyani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Saggar_A/0/1/0/all/0/1"&gt;Arpita Saggar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kumar_N/0/1/0/all/0/1"&gt;Naveen Kumar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hate Speech Detection in Roman Urdu. (arXiv:2108.02830v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.02830</id>
        <link href="http://arxiv.org/abs/2108.02830"/>
        <updated>2021-08-09T00:49:27.972Z</updated>
        <summary type="html"><![CDATA[Hate speech is a specific type of controversial content that is widely
legislated as a crime that must be identified and blocked. However, due to the
sheer volume and velocity of the Twitter data stream, hate speech detection
cannot be performed manually. To address this issue, several studies have been
conducted for hate speech detection in European languages, whereas little
attention has been paid to low-resource South Asian languages, making the
social media vulnerable for millions of users. In particular, to the best of
our knowledge, no study has been conducted for hate speech detection in Roman
Urdu text, which is widely used in the sub-continent. In this study, we have
scrapped more than 90,000 tweets and manually parsed them to identify 5,000
Roman Urdu tweets. Subsequently, we have employed an iterative approach to
develop guidelines and used them for generating the Hate Speech Roman Urdu 2020
corpus. The tweets in the this corpus are classified at three levels:
Neutral-Hostile, Simple-Complex, and Offensive-Hate speech. As another
contribution, we have used five supervised learning techniques, including a
deep learning technique, to evaluate and compare their effectiveness for hate
speech detection. The results show that Logistic Regression outperformed all
other techniques, including deep learning techniques for the two levels of
classification, by achieved an F1 score of 0.906 for distinguishing between
Neutral-Hostile tweets, and 0.756 for distinguishing between Offensive-Hate
speech tweets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1"&gt;Moin Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shahzad_K/0/1/0/all/0/1"&gt;Khurram Shahzad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Malik_K/0/1/0/all/0/1"&gt;Kamran Malik&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Adversarial Robustness: A Neural Architecture Search perspective. (arXiv:2007.08428v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.08428</id>
        <link href="http://arxiv.org/abs/2007.08428"/>
        <updated>2021-08-09T00:49:27.964Z</updated>
        <summary type="html"><![CDATA[Adversarial robustness of deep learning models has gained much traction in
the last few years. Various attacks and defenses are proposed to improve the
adversarial robustness of modern-day deep learning architectures. While all
these approaches help improve the robustness, one promising direction for
improving adversarial robustness is un-explored, i.e., the complex topology of
the neural network architecture. In this work, we answer the following
question: "Can the complex topology of a neural network give adversarial
robustness without any form of adversarial training?" empirically by
experimenting with different hand-crafted and NAS based architectures. Our
findings show that, for small-scale attacks, NAS-based architectures are more
robust for small-scale datasets and simple tasks than hand-crafted
architectures. However, as the dataset's size or the task's complexity
increase, hand-crafted architectures are more robust than NAS-based
architectures. We perform the first large scale study to understand adversarial
robustness purely from an architectural perspective. Our results show that
random sampling in the search space of DARTS (a popular NAS method) with simple
ensembling can improve the robustness to PGD attack by nearly ~12\%. We show
that NAS, which is popular for SoTA accuracy, can provide adversarial accuracy
as a free add-on without any form of adversarial training. Our results show
that leveraging the power of neural network topology with methods like
ensembles can be an excellent way to achieve adversarial robustness without any
form of adversarial training. We also introduce a metric that can be used to
calculate the trade-off between clean accuracy and adversarial robustness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Devaguptapu_C/0/1/0/all/0/1"&gt;Chaitanya Devaguptapu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agarwal_D/0/1/0/all/0/1"&gt;Devansh Agarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mittal_G/0/1/0/all/0/1"&gt;Gaurav Mittal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balasubramanian_V/0/1/0/all/0/1"&gt;Vineeth N Balasubramanian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Designing Good Representation Learning Models. (arXiv:2107.05948v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.05948</id>
        <link href="http://arxiv.org/abs/2107.05948"/>
        <updated>2021-08-09T00:49:27.890Z</updated>
        <summary type="html"><![CDATA[The goal of representation learning is different from the ultimate objective
of machine learning such as decision making, it is therefore very difficult to
establish clear and direct objectives for training representation learning
models. It has been argued that a good representation should disentangle the
underlying variation factors, yet how to translate this into training
objectives remains unknown. This paper presents an attempt to establish direct
training criterions and design principles for developing good representation
learning models. We propose that a good representation learning model should be
maximally expressive, i.e., capable of distinguishing the maximum number of
input configurations. We formally define expressiveness and introduce the
maximum expressiveness (MEXS) theorem of a general learning model. We propose
to train a model by maximizing its expressiveness while at the same time
incorporating general priors such as model smoothness. We present a conscience
competitive learning algorithm which encourages the model to reach its MEXS
whilst at the same time adheres to model smoothness prior. We also introduce a
label consistent training (LCT) technique to boost model smoothness by
encouraging it to assign consistent labels to similar samples. We present
extensive experimental results to show that our method can indeed design
representation learning models capable of developing representations that are
as good as or better than state of the art. We also show that our technique is
computationally efficient, robust against different parameter settings and can
work effectively on a variety of datasets. Code available at
https://github.com/qlilx/odgrlm.git]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1"&gt;Qinglin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Garibaldi_J/0/1/0/all/0/1"&gt;Jonathan M Garibaldi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_G/0/1/0/all/0/1"&gt;Guoping Qiu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GLASS: Geometric Latent Augmentation for Shape Spaces. (arXiv:2108.03225v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03225</id>
        <link href="http://arxiv.org/abs/2108.03225"/>
        <updated>2021-08-09T00:49:27.882Z</updated>
        <summary type="html"><![CDATA[We investigate the problem of training generative models on a very sparse
collection of 3D models. We use geometrically motivated energies to augment and
thus boost a sparse collection of example (training) models. We analyze the
Hessian of the as-rigid-as-possible (ARAP) energy to sample from and project to
the underlying (local) shape space, and use the augmented dataset to train a
variational autoencoder (VAE). We iterate the process of building latent spaces
of VAE and augmenting the associated dataset, to progressively reveal a richer
and more expressive generative space for creating geometrically and
semantically valid samples. Our framework allows us to train generative 3D
models even with a small set of good quality 3D models, which are typically
hard to curate. We extensively evaluate our method against a set of strong
baselines, provide ablation studies and demonstrate application towards
establishing shape correspondences. We present multiple examples of interesting
and meaningful shape variations even when starting from as few as 3-10 training
shapes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Muralikrishnan_S/0/1/0/all/0/1"&gt;Sanjeev Muralikrishnan&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Chaudhuri_S/0/1/0/all/0/1"&gt;Siddhartha Chaudhuri&lt;/a&gt; (2 and 3), &lt;a href="http://arxiv.org/find/cs/1/au:+Aigerman_N/0/1/0/all/0/1"&gt;Noam Aigerman&lt;/a&gt; (2), &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_V/0/1/0/all/0/1"&gt;Vladimir Kim&lt;/a&gt; (2), &lt;a href="http://arxiv.org/find/cs/1/au:+Fisher_M/0/1/0/all/0/1"&gt;Matthew Fisher&lt;/a&gt; (2), &lt;a href="http://arxiv.org/find/cs/1/au:+Mitra_N/0/1/0/all/0/1"&gt;Niloy Mitra&lt;/a&gt; (1 and 2) ((1) University College London, (2) Adobe Research, (3) IIT Bombay)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TightCap: 3D Human Shape Capture with Clothing Tightness Field. (arXiv:1904.02601v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1904.02601</id>
        <link href="http://arxiv.org/abs/1904.02601"/>
        <updated>2021-08-09T00:49:27.874Z</updated>
        <summary type="html"><![CDATA[In this paper, we present TightCap, a data-driven scheme to capture both the
human shape and dressed garments accurately with only a single 3D human scan,
which enables numerous applications such as virtual try-on, biometrics and body
evaluation. To break the severe variations of the human poses and garments, we
propose to model the clothing tightness - the displacements from the garments
to the human shape implicitly in the global UV texturing domain. To this end,
we utilize an enhanced statistical human template and an effective multi-stage
alignment scheme to map the 3D scan into a hybrid 2D geometry image. Based on
this 2D representation, we propose a novel framework to predicted clothing
tightness via a novel tightness formulation, as well as an effective
optimization scheme to further reconstruct multi-layer human shape and garments
under various clothing categories and human postures. We further propose a new
clothing tightness dataset (CTD) of human scans with a large variety of
clothing styles, poses and corresponding ground-truth human shapes to stimulate
further research. Extensive experiments demonstrate the effectiveness of our
TightCap to achieve high-quality human shape and dressed garments
reconstruction, as well as the further applications for clothing segmentation,
retargeting and animation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pang_A/0/1/0/all/0/1"&gt;Anqi Pang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1"&gt;Yang Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xui_L/0/1/0/all/0/1"&gt;Lan Xui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1"&gt;Jingyi Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using Machine Learning to Predict Game Outcomes Based on Player-Champion Experience in League of Legends. (arXiv:2108.02799v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02799</id>
        <link href="http://arxiv.org/abs/2108.02799"/>
        <updated>2021-08-09T00:49:27.867Z</updated>
        <summary type="html"><![CDATA[League of Legends (LoL) is the most widely played multiplayer online battle
arena (MOBA) game in the world. An important aspect of LoL is competitive
ranked play, which utilizes a skill-based matchmaking system to form fair
teams. However, players' skill levels vary widely depending on which champion,
or hero, that they choose to play as. In this paper, we propose a method for
predicting game outcomes in ranked LoL games based on players' experience with
their selected champion. Using a deep neural network, we found that game
outcomes can be predicted with 75.1% accuracy after all players have selected
champions, which occurs before gameplay begins. Our results have important
implications for playing LoL and matchmaking. Firstly, individual champion
skill plays a significant role in the outcome of a match, regardless of team
composition. Secondly, even after the skill-based matchmaking, there is still a
wide variance in team skill before gameplay begins. Finally, players should
only play champions that they have mastered, if they want to win games.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Do_T/0/1/0/all/0/1"&gt;Tiffany D. Do&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Seong Ioi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1"&gt;Dylan S. Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McMillian_M/0/1/0/all/0/1"&gt;Matthew G. McMillian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McMahan_R/0/1/0/all/0/1"&gt;Ryan P. McMahan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Contrastive Semi-Supervised Learning for 2D Medical Image Segmentation. (arXiv:2106.06801v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.06801</id>
        <link href="http://arxiv.org/abs/2106.06801"/>
        <updated>2021-08-09T00:49:27.859Z</updated>
        <summary type="html"><![CDATA[Contrastive Learning (CL) is a recent representation learning approach, which
encourages inter-class separability and intra-class compactness in learned
image representations. Since medical images often contain multiple semantic
classes in an image, using CL to learn representations of local features (as
opposed to global) is important. In this work, we present a novel
semi-supervised 2D medical segmentation solution that applies CL on image
patches, instead of full images. These patches are meaningfully constructed
using the semantic information of different classes obtained via pseudo
labeling. We also propose a novel consistency regularization (CR) scheme, which
works in synergy with CL. It addresses the problem of confirmation bias, and
encourages better clustering in the feature space. We evaluate our method on
four public medical segmentation datasets and a novel histopathology dataset
that we introduce. Our method obtains consistent improvements over
state-of-the-art semi-supervised segmentation approaches for all datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pandey_P/0/1/0/all/0/1"&gt;Prashant Pandey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pai_A/0/1/0/all/0/1"&gt;Ajey Pai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhatt_N/0/1/0/all/0/1"&gt;Nisarg Bhatt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Das_P/0/1/0/all/0/1"&gt;Prasenjit Das&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Makharia_G/0/1/0/all/0/1"&gt;Govind Makharia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+AP_P/0/1/0/all/0/1"&gt;Prathosh AP&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mausam/0/1/0/all/0/1"&gt;Mausam&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quantum Continual Learning Overcoming Catastrophic Forgetting. (arXiv:2108.02786v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02786</id>
        <link href="http://arxiv.org/abs/2108.02786"/>
        <updated>2021-08-09T00:49:27.841Z</updated>
        <summary type="html"><![CDATA[Catastrophic forgetting describes the fact that machine learning models will
likely forget the knowledge of previously learned tasks after the learning
process of a new one. It is a vital problem in the continual learning scenario
and recently has attracted tremendous concern across different communities. In
this paper, we explore the catastrophic forgetting phenomena in the context of
quantum machine learning. We find that, similar to those classical learning
models based on neural networks, quantum learning systems likewise suffer from
such forgetting problem in classification tasks emerging from various
application scenes. We show that based on the local geometrical information in
the loss function landscape of the trained model, a uniform strategy can be
adapted to overcome the forgetting problem in the incremental learning setting.
Our results uncover the catastrophic forgetting phenomena in quantum machine
learning and offer a practical method to overcome this problem, which opens a
new avenue for exploring potential quantum advantages towards continual
learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1"&gt;Wenjie Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1"&gt;Zhide Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_D/0/1/0/all/0/1"&gt;Dong-Ling Deng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[YOLOX: Exceeding YOLO Series in 2021. (arXiv:2107.08430v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.08430</id>
        <link href="http://arxiv.org/abs/2107.08430"/>
        <updated>2021-08-09T00:49:27.834Z</updated>
        <summary type="html"><![CDATA[In this report, we present some experienced improvements to YOLO series,
forming a new high-performance detector -- YOLOX. We switch the YOLO detector
to an anchor-free manner and conduct other advanced detection techniques, i.e.,
a decoupled head and the leading label assignment strategy SimOTA to achieve
state-of-the-art results across a large scale range of models: For YOLO-Nano
with only 0.91M parameters and 1.08G FLOPs, we get 25.3% AP on COCO, surpassing
NanoDet by 1.8% AP; for YOLOv3, one of the most widely used detectors in
industry, we boost it to 47.3% AP on COCO, outperforming the current best
practice by 3.0% AP; for YOLOX-L with roughly the same amount of parameters as
YOLOv4-CSP, YOLOv5-L, we achieve 50.0% AP on COCO at a speed of 68.9 FPS on
Tesla V100, exceeding YOLOv5-L by 1.8% AP. Further, we won the 1st Place on
Streaming Perception Challenge (Workshop on Autonomous Driving at CVPR 2021)
using a single YOLOX-L model. We hope this report can provide useful experience
for developers and researchers in practical scenes, and we also provide deploy
versions with ONNX, TensorRT, NCNN, and Openvino supported. Source code is at
https://github.com/Megvii-BaseDetection/YOLOX.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ge_Z/0/1/0/all/0/1"&gt;Zheng Ge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Songtao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1"&gt;Feng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zeming Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1"&gt;Jian Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quantum Topological Data Analysis with Linear Depth and Exponential Speedup. (arXiv:2108.02811v1 [quant-ph])]]></title>
        <id>http://arxiv.org/abs/2108.02811</id>
        <link href="http://arxiv.org/abs/2108.02811"/>
        <updated>2021-08-09T00:49:27.826Z</updated>
        <summary type="html"><![CDATA[Quantum computing offers the potential of exponential speedups for certain
classical computations. Over the last decade, many quantum machine learning
(QML) algorithms have been proposed as candidates for such exponential
improvements. However, two issues unravel the hope of exponential speedup for
some of these QML algorithms: the data-loading problem and, more recently, the
stunning dequantization results of Tang et al. A third issue, namely the
fault-tolerance requirements of most QML algorithms, has further hindered their
practical realization. The quantum topological data analysis (QTDA) algorithm
of Lloyd, Garnerone and Zanardi was one of the first QML algorithms that
convincingly offered an expected exponential speedup. From the outset, it did
not suffer from the data-loading problem. A recent result has also shown that
the generalized problem solved by this algorithm is likely classically
intractable, and would therefore be immune to any dequantization efforts.
However, the QTDA algorithm of Lloyd et~al. has a time complexity of
$O(n^4/(\epsilon^2 \delta))$ (where $n$ is the number of data points,
$\epsilon$ is the error tolerance, and $\delta$ is the smallest nonzero
eigenvalue of the restricted Laplacian) and requires fault-tolerant quantum
computing, which has not yet been achieved. In this paper, we completely
overhaul the QTDA algorithm to achieve an improved exponential speedup and
depth complexity of $O(n\log(1/(\delta\epsilon)))$. Our approach includes three
key innovations: (a) an efficient realization of the combinatorial Laplacian as
a sum of Pauli operators; (b) a quantum rejection sampling approach to restrict
the superposition to the simplices in the complex; and (c) a stochastic rank
estimation method to estimate the Betti numbers. We present a theoretical error
analysis, and the circuit and computational time and depth complexities for
Betti number estimation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Ubaru_S/0/1/0/all/0/1"&gt;Shashanka Ubaru&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Akhalwaya_I/0/1/0/all/0/1"&gt;Ismail Yunus Akhalwaya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Squillante_M/0/1/0/all/0/1"&gt;Mark S. Squillante&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Clarkson_K/0/1/0/all/0/1"&gt;Kenneth L. Clarkson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Horesh_L/0/1/0/all/0/1"&gt;Lior Horesh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Image Fusion Transformer. (arXiv:2107.09011v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.09011</id>
        <link href="http://arxiv.org/abs/2107.09011"/>
        <updated>2021-08-09T00:49:27.815Z</updated>
        <summary type="html"><![CDATA[In image fusion, images obtained from different sensors are fused to generate
a single image with enhanced information. In recent years, state-of-the-art
methods have adopted Convolution Neural Networks (CNNs) to encode meaningful
features for image fusion. Specifically, CNN-based methods perform image fusion
by fusing local features. However, they do not consider long-range dependencies
that are present in the image. Transformer-based models are designed to
overcome this by modeling the long-range dependencies with the help of
self-attention mechanism. This motivates us to propose a novel Image Fusion
Transformer (IFT) where we develop a transformer-based multi-scale fusion
strategy that attends to both local and long-range information (or global
context). The proposed method follows a two-stage training approach. In the
first stage, we train an auto-encoder to extract deep features at multiple
scales. In the second stage, multi-scale features are fused using a
Spatio-Transformer (ST) fusion strategy. The ST fusion blocks are comprised of
a CNN and a transformer branch which capture local and long-range features,
respectively. Extensive experiments on multiple benchmark datasets show that
the proposed method performs better than many competitive fusion algorithms.
Furthermore, we show the effectiveness of the proposed ST fusion strategy with
an ablation analysis. The source code is available at:
https://github.com/Vibashan/Image-Fusion-Transformer.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+VS_V/0/1/0/all/0/1"&gt;Vibashan VS&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Valanarasu_J/0/1/0/all/0/1"&gt;Jeya Maria Jose Valanarasu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oza_P/0/1/0/all/0/1"&gt;Poojan Oza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Patel_V/0/1/0/all/0/1"&gt;Vishal M. Patel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Computer-aided Interpretable Features for Leaf Image Classification. (arXiv:2106.08077v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.08077</id>
        <link href="http://arxiv.org/abs/2106.08077"/>
        <updated>2021-08-09T00:49:27.797Z</updated>
        <summary type="html"><![CDATA[Plant species identification is time consuming, costly, and requires lots of
efforts, and expertise knowledge. In recent, many researchers use deep learning
methods to classify plants directly using plant images. While deep learning
models have achieved a great success, the lack of interpretability limit their
widespread application. To overcome this, we explore the use of interpretable,
measurable and computer-aided features extracted from plant leaf images. Image
processing is one of the most challenging, and crucial steps in
feature-extraction. The purpose of image processing is to improve the leaf
image by removing undesired distortion. The main image processing steps of our
algorithm involves: i) Convert original image to RGB (Red-Green-Blue) image,
ii) Gray scaling, iii) Gaussian smoothing, iv) Binary thresholding, v) Remove
stalk, vi) Closing holes, and vii) Resize image. The next step after image
processing is to extract features from plant leaf images. We introduced 52
computationally efficient features to classify plant species. These features
are mainly classified into four groups as: i) shape-based features, ii)
color-based features, iii) texture-based features, and iv) scagnostic features.
Length, width, area, texture correlation, monotonicity and scagnostics are to
name few of them. We explore the ability of features to discriminate the
classes of interest under supervised learning and unsupervised learning
settings. For that, supervised dimensionality reduction technique, Linear
Discriminant Analysis (LDA), and unsupervised dimensionality reduction
technique, Principal Component Analysis (PCA) are used to convert and visualize
the images from digital-image space to feature space. The results show that the
features are sufficient to discriminate the classes of interest under both
supervised and unsupervised learning settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lakshika_J/0/1/0/all/0/1"&gt;Jayani P. G. Lakshika&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Talagala_T/0/1/0/all/0/1"&gt;Thiyanga S. Talagala&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Potential Applications of Artificial Intelligence and Machine Learning in Radiochemistry and Radiochemical Engineering. (arXiv:2108.02814v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02814</id>
        <link href="http://arxiv.org/abs/2108.02814"/>
        <updated>2021-08-09T00:49:27.788Z</updated>
        <summary type="html"><![CDATA[Artificial intelligence and machine learning are poised to disrupt PET
imaging from bench to clinic. In this perspective we offer insights into how
the technology could be applied to improve the design and synthesis of new
radiopharmaceuticals for PET imaging, including identification of an optimal
labeling approach as well as strategies for radiolabeling reaction
optimization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Webb_E/0/1/0/all/0/1"&gt;E. William Webb&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scott_P/0/1/0/all/0/1"&gt;Peter J.H. Scott&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BiconNet: An Edge-preserved Connectivity-based Approach for Salient Object Detection. (arXiv:2103.00334v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.00334</id>
        <link href="http://arxiv.org/abs/2103.00334"/>
        <updated>2021-08-09T00:49:27.768Z</updated>
        <summary type="html"><![CDATA[Salient object detection (SOD) is viewed as a pixel-wise saliency modeling
task by traditional deep learning-based methods. A limitation of current SOD
models is insufficient utilization of inter-pixel information, which usually
results in imperfect segmentation near edge regions and low spatial coherence.
As we demonstrate, using a saliency mask as the only label is suboptimal. To
address this limitation, we propose a connectivity-based approach called
bilateral connectivity network (BiconNet), which uses connectivity masks
together with saliency masks as labels for effective modeling of inter-pixel
relationships and object saliency. Moreover, we propose a bilateral voting
module to enhance the output connectivity map, and a novel edge feature
enhancement method that efficiently utilizes edge-specific features. Through
comprehensive experiments on five benchmark datasets, we demonstrate that our
proposed method can be plugged into any existing state-of-the-art
saliency-based SOD framework to improve its performance with negligible
parameter increase.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Ziyun Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soltanian_Zadeh_S/0/1/0/all/0/1"&gt;Somayyeh Soltanian-Zadeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Farsiu_S/0/1/0/all/0/1"&gt;Sina Farsiu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Supervised Learning from Unlabeled Fundus Photographs Improves Segmentation of the Retina. (arXiv:2108.02798v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02798</id>
        <link href="http://arxiv.org/abs/2108.02798"/>
        <updated>2021-08-09T00:49:27.760Z</updated>
        <summary type="html"><![CDATA[Fundus photography is the primary method for retinal imaging and essential
for diabetic retinopathy prevention. Automated segmentation of fundus
photographs would improve the quality, capacity, and cost-effectiveness of eye
care screening programs. However, current segmentation methods are not robust
towards the diversity in imaging conditions and pathologies typical for
real-world clinical applications. To overcome these limitations, we utilized
contrastive self-supervised learning to exploit the large variety of unlabeled
fundus images in the publicly available EyePACS dataset. We pre-trained an
encoder of a U-Net, which we later fine-tuned on several retinal vessel and
lesion segmentation datasets. We demonstrate for the first time that by using
contrastive self-supervised learning, the pre-trained network can recognize
blood vessels, optic disc, fovea, and various lesions without being provided
any labels. Furthermore, when fine-tuned on a downstream blood vessel
segmentation task, such pre-trained networks achieve state-of-the-art
performance on images from different datasets. Additionally, the pre-training
also leads to shorter training times and an improved few-shot performance on
both blood vessel and lesion segmentation tasks. Altogether, our results
showcase the benefits of contrastive self-supervised pre-training which can
play a crucial role in real-world clinical applications requiring robust models
able to adapt to new devices with only a few annotated samples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kukacka_J/0/1/0/all/0/1"&gt;Jan Kuka&amp;#x10d;ka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zenz_A/0/1/0/all/0/1"&gt;Anja Zenz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kollovieh_M/0/1/0/all/0/1"&gt;Marcel Kollovieh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Justel_D/0/1/0/all/0/1"&gt;Dominik J&amp;#xfc;stel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ntziachristos_V/0/1/0/all/0/1"&gt;Vasilis Ntziachristos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bird's-Eye-View Panoptic Segmentation Using Monocular Frontal View Images. (arXiv:2108.03227v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03227</id>
        <link href="http://arxiv.org/abs/2108.03227"/>
        <updated>2021-08-09T00:49:27.753Z</updated>
        <summary type="html"><![CDATA[Bird's-Eye-View (BEV) maps have emerged as one of the most powerful
representations for scene understanding due to their ability to provide rich
spatial context while being easy to interpret and process. However, generating
BEV maps requires complex multi-stage paradigms that encapsulate a series of
distinct tasks such as depth estimation, ground plane estimation, and semantic
segmentation. These sub-tasks are often learned in a disjoint manner which
prevents the model from holistic reasoning and results in erroneous BEV maps.
Moreover, existing algorithms only predict the semantics in the BEV space,
which limits their use in applications where the notion of object instances is
critical. In this work, we present the first end-to-end learning approach for
directly predicting dense panoptic segmentation maps in the BEV, given a single
monocular image in the frontal view (FV). Our architecture follows the top-down
paradigm and incorporates a novel dense transformer module consisting of two
distinct transformers that learn to independently map vertical and flat regions
in the input image from the FV to the BEV. Additionally, we derive a
mathematical formulation for the sensitivity of the FV-BEV transformation which
allows us to intelligently weight pixels in the BEV space to account for the
varying descriptiveness across the FV image. Extensive evaluations on the
KITTI-360 and nuScenes datasets demonstrate that our approach exceeds the
state-of-the-art in the PQ metric by 3.61 pp and 4.93 pp respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gosala_N/0/1/0/all/0/1"&gt;Nikhil Gosala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Valada_A/0/1/0/all/0/1"&gt;Abhinav Valada&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hidden Markov Modeling for Maximum Likelihood Neuron Reconstruction. (arXiv:2106.02701v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.02701</id>
        <link href="http://arxiv.org/abs/2106.02701"/>
        <updated>2021-08-09T00:49:27.746Z</updated>
        <summary type="html"><![CDATA[Recent advances in brain clearing and imaging have made it possible to image
entire mammalian brains at sub-micron resolution. These images offer the
potential to assemble brain-wide atlases of projection neuron morphology, but
manual neuron reconstruction remains a bottleneck. In this paper we present a
probabilistic method which combines a hidden Markov state process that encodes
neuron geometric properties with a random field appearance model of the
flourescence process. Our method utilizes dynamic programming to efficiently
compute the global maximizers of what we call the "most probable" neuron path.
We applied our algorithm to the output of image segmentation models where false
negatives severed neuronal processes, and showed that it can follow axons in
the presence of noise or nearby neurons. Our method has the potential to be
integrated into a semi or fully automated reconstruction pipeline.
Additionally, it creates a framework for conditioning the probability to fixed
start and endpoints through which users can intervene with hard constraints to,
for example, rule out certain reconstructions, or assign axons to particular
cell bodies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Athey_T/0/1/0/all/0/1"&gt;Thomas L. Athey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tward_D/0/1/0/all/0/1"&gt;Daniel J. Tward&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mueller_U/0/1/0/all/0/1"&gt;Ulrich Mueller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Miller_M/0/1/0/all/0/1"&gt;Michael I. Miller&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Colonoscopy Polyp Detection and Classification: Dataset Creation and Comparative Evaluations. (arXiv:2104.10824v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.10824</id>
        <link href="http://arxiv.org/abs/2104.10824"/>
        <updated>2021-08-09T00:49:27.739Z</updated>
        <summary type="html"><![CDATA[Colorectal cancer (CRC) is one of the most common types of cancer with a high
mortality rate. Colonoscopy is the preferred procedure for CRC screening and
has proven to be effective in reducing CRC mortality. Thus, a reliable
computer-aided polyp detection and classification system can significantly
increase the effectiveness of colonoscopy. In this paper, we create an
endoscopic dataset collected from various sources and annotate the ground truth
of polyp location and classification results with the help of experienced
gastroenterologists. The dataset can serve as a benchmark platform to train and
evaluate the machine learning models for polyp classification. We have also
compared the performance of eight state-of-the-art deep learning-based object
detection models. The results demonstrate that deep CNN models are promising in
CRC screening. This work can serve as a baseline for future research in polyp
detection and classification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1"&gt;Kaidong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fathan_M/0/1/0/all/0/1"&gt;Mohammad I. Fathan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Patel_K/0/1/0/all/0/1"&gt;Krushi Patel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1"&gt;Tianxiao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_C/0/1/0/all/0/1"&gt;Cuncong Zhong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bansal_A/0/1/0/all/0/1"&gt;Ajay Bansal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rastogi_A/0/1/0/all/0/1"&gt;Amit Rastogi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jean S. Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1"&gt;Guanghui Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Human Pose Transfer with Disentangled Feature Consistency. (arXiv:2107.10984v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.10984</id>
        <link href="http://arxiv.org/abs/2107.10984"/>
        <updated>2021-08-09T00:49:27.720Z</updated>
        <summary type="html"><![CDATA[Deep generative models have made great progress in synthesizing images with
arbitrary human poses and transferring poses of one person to others. However,
most existing approaches explicitly leverage the pose information extracted
from the source images as a conditional input for the generative networks.
Meanwhile, they usually focus on the visual fidelity of the synthesized images
but neglect the inherent consistency, which further confines their performance
of pose transfer. To alleviate the current limitations and improve the quality
of the synthesized images, we propose a pose transfer network with Disentangled
Feature Consistency (DFC-Net) to facilitate human pose transfer. Given a pair
of images containing the source and target person, DFC-Net extracts pose and
static information from the source and target respectively, then synthesizes an
image of the target person with the desired pose from the source. Moreover,
DFC-Net leverages disentangled feature consistency losses in the adversarial
training to strengthen the transfer coherence and integrates the keypoint
amplifier to enhance the pose feature extraction. Additionally, an unpaired
support dataset Mixamo-Sup providing more extra pose information has been
further utilized during the training to improve the generality and robustness
of DFC-Net. Extensive experimental results on Mixamo-Pose and EDN-10k have
demonstrated DFC-Net achieves state-of-the-art performance on pose transfer.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1"&gt;Kun Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_C/0/1/0/all/0/1"&gt;Chengxiang Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Che_Z/0/1/0/all/0/1"&gt;Zhengping Che&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_B/0/1/0/all/0/1"&gt;Bo Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1"&gt;Jian Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guan_Z/0/1/0/all/0/1"&gt;Zheng Guan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_G/0/1/0/all/0/1"&gt;Gangyi Ding&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FPCC: Fast Point Cloud Clustering for Instance Segmentation. (arXiv:2012.14618v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.14618</id>
        <link href="http://arxiv.org/abs/2012.14618"/>
        <updated>2021-08-09T00:49:27.712Z</updated>
        <summary type="html"><![CDATA[Instance segmentation is an important pre-processing task in numerous
real-world applications, such as robotics, autonomous vehicles, and
human-computer interaction. Compared with the rapid development of deep
learning for two-dimensional (2D) image tasks, deep learning-based instance
segmentation of 3D point cloud still has a lot of room for development. In
particular, distinguishing a large number of occluded objects of the same class
is a highly challenging problem, which is seen in a robotic bin-picking. In a
usual bin-picking scene, many indentical objects are stacked together and the
model of the objects is known. Thus, the semantic information can be ignored;
instead, the focus in the bin-picking is put on the segmentation of instances.
Based on this task requirement, we propose a Fast Point Cloud Clustering (FPCC)
for instance segmentation of bin-picking scene. FPCC includes a network named
FPCC-Net and a fast clustering algorithm. FPCC-net has two subnets, one for
inferring the geometric centers for clustering and the other for describing
features of each point. FPCC-Net extracts features of each point and infers
geometric center points of each instance simultaneously. After that, the
proposed clustering algorithm clusters the remaining points to the closest
geometric center in feature embedding space. Experiments show that FPCC also
surpasses the existing works in bin-picking scenes and is more computationally
efficient. Our code and data are available at https://github.com/xyjbaal/FPCC.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yajun Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arai_S/0/1/0/all/0/1"&gt;Shogo Arai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1"&gt;Diyi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_F/0/1/0/all/0/1"&gt;Fangzhou Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kosuge_K/0/1/0/all/0/1"&gt;Kazuhiro Kosuge&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Supervised Multi-Modal Alignment for Whole Body Medical Imaging. (arXiv:2107.06652v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.06652</id>
        <link href="http://arxiv.org/abs/2107.06652"/>
        <updated>2021-08-09T00:49:27.704Z</updated>
        <summary type="html"><![CDATA[This paper explores the use of self-supervised deep learning in medical
imaging in cases where two scan modalities are available for the same subject.
Specifically, we use a large publicly-available dataset of over 20,000 subjects
from the UK Biobank with both whole body Dixon technique magnetic resonance
(MR) scans and also dual-energy x-ray absorptiometry (DXA) scans. We make three
contributions: (i) We introduce a multi-modal image-matching contrastive
framework, that is able to learn to match different-modality scans of the same
subject with high accuracy. (ii) Without any adaption, we show that the
correspondences learnt during this contrastive training step can be used to
perform automatic cross-modal scan registration in a completely unsupervised
manner. (iii) Finally, we use these registrations to transfer segmentation maps
from the DXA scans to the MR scans where they are used to train a network to
segment anatomical regions without requiring ground-truth MR examples. To aid
further research, our code will be made publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Windsor_R/0/1/0/all/0/1"&gt;Rhydian Windsor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jamaludin_A/0/1/0/all/0/1"&gt;Amir Jamaludin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kadir_T/0/1/0/all/0/1"&gt;Timor Kadir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zisserman_A/0/1/0/all/0/1"&gt;Andrew Zisserman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hand-Based Person Identification using Global and Part-Aware Deep Feature Representation Learning. (arXiv:2101.05260v5 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.05260</id>
        <link href="http://arxiv.org/abs/2101.05260"/>
        <updated>2021-08-09T00:49:27.687Z</updated>
        <summary type="html"><![CDATA[In cases of serious crime, including sexual abuse, often the only available
information with demonstrated potential for identification is images of the
hands. Since this evidence is captured in uncontrolled situations, it is
difficult to analyse. As global approaches to feature comparison are limited in
this case, it is important to extend to consider local information. In this
work, we propose hand-based person identification by learning both global and
local deep feature representation. Our proposed method, Global and Part-Aware
Network (GPA-Net), creates global and local branches on the conv-layer for
learning robust discriminative global and part-level features. For learning the
local (part-level) features, we perform uniform partitioning on the conv-layer
in both horizontal and vertical directions. We retrieve the parts by conducting
a soft partition without explicitly partitioning the images or requiring
external cues such as pose estimation. We make extensive evaluations on two
large multi-ethnic and publicly available hand datasets, demonstrating that our
proposed method significantly outperforms competing approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Baisa_N/0/1/0/all/0/1"&gt;Nathanael L. Baisa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1"&gt;Zheheng Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vyas_R/0/1/0/all/0/1"&gt;Ritesh Vyas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Williams_B/0/1/0/all/0/1"&gt;Bryan Williams&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rahmani_H/0/1/0/all/0/1"&gt;Hossein Rahmani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Angelov_P/0/1/0/all/0/1"&gt;Plamen Angelov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Black_S/0/1/0/all/0/1"&gt;Sue Black&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Photoacoustic Reconstruction Using Sparsity in Curvelet Frame: Image versus Data Domain. (arXiv:2011.13080v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.13080</id>
        <link href="http://arxiv.org/abs/2011.13080"/>
        <updated>2021-08-09T00:49:27.679Z</updated>
        <summary type="html"><![CDATA[Curvelet frame is of special significance for photoacoustic tomography (PAT)
due to its sparsifying and microlocalisation properties. We derive a one-to-one
map between wavefront directions in image and data spaces in PAT which suggests
near equivalence between the recovery of the initial pressure and PAT data from
compressed/subsampled measurements when assuming sparsity in Curvelet frame. As
the latter is computationally more tractable, investigation to which extent
this equivalence holds conducted in this paper is of immediate practical
significance. To this end we formulate and compare DR, a two step approach
based on the recovery of the complete volume of the photoacoustic data from the
subsampled data followed by the acoustic inversion, and p0R, a one step
approach where the photoacoustic image (the initial pressure, p0) is directly
recovered from the subsampled data. Effective representation of the
photoacoustic data requires basis defined on the range of the photoacoustic
forward operator. To this end we propose a novel wedge-restriction of Curvelet
transform which enables us to construct such basis. Both recovery problems are
formulated in a variational framework. As the Curvelet frame is heavily
overdetermined, we use reweighted l1 norm penalties to enhance the sparsity of
the solution. The data reconstruction problem DR is a standard compressed
sensing recovery problem, which we solve using an ADMMtype algorithm, SALSA.
Subsequently, the initial pressure is recovered using time reversal as
implemented in the k-Wave Toolbox. The p0 reconstruction problem, p0R, aims to
recover the photoacoustic image directly via FISTA, or ADMM when in addition
including a non-negativity constraint. We compare and discuss the relative
merits of the two approaches and illustrate them on 2D simulated and 3D real
data in a fair and rigorous manner.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pan_B/0/1/0/all/0/1"&gt;Bolin Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arridge_S/0/1/0/all/0/1"&gt;Simon R. Arridge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lucka_F/0/1/0/all/0/1"&gt;Felix Lucka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cox_B/0/1/0/all/0/1"&gt;Ben T. Cox&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huynh_N/0/1/0/all/0/1"&gt;Nam Huynh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Beard_P/0/1/0/all/0/1"&gt;Paul C. Beard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_E/0/1/0/all/0/1"&gt;Edward Z. Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Betcke_M/0/1/0/all/0/1"&gt;Marta M. Betcke&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Weakly-supervised Video Anomaly Detection with Robust Temporal Feature Magnitude Learning. (arXiv:2101.10030v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.10030</id>
        <link href="http://arxiv.org/abs/2101.10030"/>
        <updated>2021-08-09T00:49:27.654Z</updated>
        <summary type="html"><![CDATA[Anomaly detection with weakly supervised video-level labels is typically
formulated as a multiple instance learning (MIL) problem, in which we aim to
identify snippets containing abnormal events, with each video represented as a
bag of video snippets. Although current methods show effective detection
performance, their recognition of the positive instances, i.e., rare abnormal
snippets in the abnormal videos, is largely biased by the dominant negative
instances, especially when the abnormal events are subtle anomalies that
exhibit only small differences compared with normal events. This issue is
exacerbated in many methods that ignore important video temporal dependencies.
To address this issue, we introduce a novel and theoretically sound method,
named Robust Temporal Feature Magnitude learning (RTFM), which trains a feature
magnitude learning function to effectively recognise the positive instances,
substantially improving the robustness of the MIL approach to the negative
instances from abnormal videos. RTFM also adapts dilated convolutions and
self-attention mechanisms to capture long- and short-range temporal
dependencies to learn the feature magnitude more faithfully. Extensive
experiments show that the RTFM-enabled MIL model (i) outperforms several
state-of-the-art methods by a large margin on four benchmark data sets
(ShanghaiTech, UCF-Crime, XD-Violence and UCSD-Peds) and (ii) achieves
significantly improved subtle anomaly discriminability and sample efficiency.
Code is available at https://github.com/tianyu0207/RTFM.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1"&gt;Yu Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pang_G/0/1/0/all/0/1"&gt;Guansong Pang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yuanhong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1"&gt;Rajvinder Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Verjans_J/0/1/0/all/0/1"&gt;Johan W. Verjans&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carneiro_G/0/1/0/all/0/1"&gt;Gustavo Carneiro&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SofGAN: A Portrait Image Generator with Dynamic Styling. (arXiv:2007.03780v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.03780</id>
        <link href="http://arxiv.org/abs/2007.03780"/>
        <updated>2021-08-09T00:49:27.635Z</updated>
        <summary type="html"><![CDATA[Recently, Generative Adversarial Networks (GANs)} have been widely used for
portrait image generation. However, in the latent space learned by GANs,
different attributes, such as pose, shape, and texture style, are generally
entangled, making the explicit control of specific attributes difficult. To
address this issue, we propose a SofGAN image generator to decouple the latent
space of portraits into two subspaces: a geometry space and a texture space.
The latent codes sampled from the two subspaces are fed to two network branches
separately, one to generate the 3D geometry of portraits with canonical pose,
and the other to generate textures. The aligned 3D geometries also come with
semantic part segmentation, encoded as a semantic occupancy field (SOF). The
SOF allows the rendering of consistent 2D semantic segmentation maps at
arbitrary views, which are then fused with the generated texture maps and
stylized to a portrait photo using our semantic instance-wise (SIW) module.
Through extensive experiments, we show that our system can generate high
quality portrait images with independently controllable geometry and texture
attributes. The method also generalizes well in various applications such as
appearance-consistent facial animation and dynamic styling.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1"&gt;Anpei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1"&gt;Ruiyang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1"&gt;Ling Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1"&gt;Hao Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1"&gt;Jingyi Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PREDATOR: Registration of 3D Point Clouds with Low Overlap. (arXiv:2011.13005v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.13005</id>
        <link href="http://arxiv.org/abs/2011.13005"/>
        <updated>2021-08-09T00:49:27.628Z</updated>
        <summary type="html"><![CDATA[We introduce PREDATOR, a model for pairwise point-cloud registration with
deep attention to the overlap region. Different from previous work, our model
is specifically designed to handle (also) point-cloud pairs with low overlap.
Its key novelty is an overlap-attention block for early information exchange
between the latent encodings of the two point clouds. In this way the
subsequent decoding of the latent representations into per-point features is
conditioned on the respective other point cloud, and thus can predict which
points are not only salient, but also lie in the overlap region between the two
point clouds. The ability to focus on points that are relevant for matching
greatly improves performance: PREDATOR raises the rate of successful
registrations by more than 20% in the low-overlap scenario, and also sets a new
state of the art for the 3DMatch benchmark with 89% registration recall.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1"&gt;Shengyu Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gojcic_Z/0/1/0/all/0/1"&gt;Zan Gojcic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Usvyatsov_M/0/1/0/all/0/1"&gt;Mikhail Usvyatsov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wieser_A/0/1/0/all/0/1"&gt;Andreas Wieser&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schindler_K/0/1/0/all/0/1"&gt;Konrad Schindler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning and Segmenting Dense Voxel Embeddings for 3D Neuron Reconstruction. (arXiv:1909.09872v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1909.09872</id>
        <link href="http://arxiv.org/abs/1909.09872"/>
        <updated>2021-08-09T00:49:27.585Z</updated>
        <summary type="html"><![CDATA[We show dense voxel embeddings learned via deep metric learning can be
employed to produce a highly accurate segmentation of neurons from 3D electron
microscopy images. A "metric graph" on a set of edges between voxels is
constructed from the dense voxel embeddings generated by a convolutional
network. Partitioning the metric graph with long-range edges as repulsive
constraints yields an initial segmentation with high precision, with
substantial accuracy gain for very thin objects. The convolutional embedding
net is reused without any modification to agglomerate the systematic splits
caused by complex "self-contact" motifs. Our proposed method achieves
state-of-the-art accuracy on the challenging problem of 3D neuron
reconstruction from the brain images acquired by serial section electron
microscopy. Our alternative, object-centered representation could be more
generally useful for other computational tasks in automated neural circuit
reconstruction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1"&gt;Kisuk Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_R/0/1/0/all/0/1"&gt;Ran Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luther_K/0/1/0/all/0/1"&gt;Kyle Luther&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seung_H/0/1/0/all/0/1"&gt;H. Sebastian Seung&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reconstruction of 3D Porous Media From 2D Slices. (arXiv:1901.10233v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1901.10233</id>
        <link href="http://arxiv.org/abs/1901.10233"/>
        <updated>2021-08-09T00:49:27.551Z</updated>
        <summary type="html"><![CDATA[In many branches of earth sciences, the problem of rock study on the
micro-level arises. However, a significant number of representative samples is
not always feasible. Thus the problem of the generation of samples with similar
properties becomes actual. In this paper, we propose a novel deep learning
architecture for three-dimensional porous media reconstruction from
two-dimensional slices. We fit a distribution on all possible three-dimensional
structures of a specific type based on the given dataset of samples. Then,
given partial information (central slices), we recover the three-dimensional
structure around such slices as the most probable one according to that
constructed distribution. Technically, we implement this in the form of a deep
neural network with encoder, generator and discriminator modules. Numerical
experiments show that this method provides a good reconstruction in terms of
Minkowski functionals.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Volkhonskiy_D/0/1/0/all/0/1"&gt;Denis Volkhonskiy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muravleva_E/0/1/0/all/0/1"&gt;Ekaterina Muravleva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sudakov_O/0/1/0/all/0/1"&gt;Oleg Sudakov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Orlov_D/0/1/0/all/0/1"&gt;Denis Orlov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Belozerov_B/0/1/0/all/0/1"&gt;Boris Belozerov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Burnaev_E/0/1/0/all/0/1"&gt;Evgeny Burnaev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koroteev_D/0/1/0/all/0/1"&gt;Dmitry Koroteev&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pattern Recognition in Vital Signs Using Spectrograms. (arXiv:2108.03168v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2108.03168</id>
        <link href="http://arxiv.org/abs/2108.03168"/>
        <updated>2021-08-09T00:49:27.474Z</updated>
        <summary type="html"><![CDATA[Spectrograms visualize the frequency components of a given signal which may
be an audio signal or even a time-series signal. Audio signals have higher
sampling rate and high variability of frequency with time. Spectrograms can
capture such variations well. But, vital signs which are time-series signals
have less sampling frequency and low-frequency variability due to which,
spectrograms fail to express variations and patterns. In this paper, we propose
a novel solution to introduce frequency variability using frequency modulation
on vital signs. Then we apply spectrograms on frequency modulated signals to
capture the patterns. The proposed approach has been evaluated on 4 different
medical datasets across both prediction and classification tasks. Significant
results are found showing the efficacy of the approach for vital sign signals.
The results from the proposed approach are promising with an accuracy of 91.55%
and 91.67% in prediction and classification tasks respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Sribhashyam_S/0/1/0/all/0/1"&gt;Sidharth Srivatsav Sribhashyam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Salekin_M/0/1/0/all/0/1"&gt;Md Sirajus Salekin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Goldgof_D/0/1/0/all/0/1"&gt;Dmitry Goldgof&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zamzmi_G/0/1/0/all/0/1"&gt;Ghada Zamzmi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sun_Y/0/1/0/all/0/1"&gt;Yu Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dynamic Semantic Occupancy Mapping using 3D Scene Flow and Closed-Form Bayesian Inference. (arXiv:2108.03180v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2108.03180</id>
        <link href="http://arxiv.org/abs/2108.03180"/>
        <updated>2021-08-09T00:49:27.297Z</updated>
        <summary type="html"><![CDATA[This paper reports on a dynamic semantic mapping framework that incorporates
3D scene flow measurements into a closed-form Bayesian inference model.
Existence of dynamic objects in the environment cause artifacts and traces in
current mapping algorithms, leading to an inconsistent map posterior. We
leverage state-of-the-art semantic segmentation and 3D flow estimation using
deep learning to provide measurements for map inference. We develop a
continuous (i.e., can be queried at arbitrary resolution) Bayesian model that
propagates the scene with flow and infers a 3D semantic occupancy map with
better performance than its static counterpart. Experimental results using
publicly available data sets show that the proposed framework generalizes its
predecessors and improves over direct measurements from deep neural networks
consistently.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Unnikrishnan_A/0/1/0/all/0/1"&gt;Aishwarya Unnikrishnan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wilson_J/0/1/0/all/0/1"&gt;Joseph Wilson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gan_L/0/1/0/all/0/1"&gt;Lu Gan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Capodieci_A/0/1/0/all/0/1"&gt;Andrew Capodieci&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jayakumar_P/0/1/0/all/0/1"&gt;Paramsothy Jayakumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barton_K/0/1/0/all/0/1"&gt;Kira Barton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghaffari_M/0/1/0/all/0/1"&gt;Maani Ghaffari&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SELM: Siamese Extreme Learning Machine with Application to Face Biometrics. (arXiv:2108.03140v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03140</id>
        <link href="http://arxiv.org/abs/2108.03140"/>
        <updated>2021-08-09T00:49:27.273Z</updated>
        <summary type="html"><![CDATA[Extreme Learning Machine is a powerful classification method very competitive
existing classification methods. It is extremely fast at training.
Nevertheless, it cannot perform face verification tasks properly because face
verification tasks require comparison of facial images of two individuals at
the same time and decide whether the two faces identify the same person. The
structure of Extreme Leaning Machine was not designed to feed two input data
streams simultaneously, thus, in 2-input scenarios Extreme Learning Machine
methods are normally applied using concatenated inputs. However, this setup
consumes two times more computational resources and it is not optimized for
recognition tasks where learning a separable distance metric is critical. For
these reasons, we propose and develop a Siamese Extreme Learning Machine
(SELM). SELM was designed to be fed with two data streams in parallel
simultaneously. It utilizes a dual-stream Siamese condition in the extra
Siamese layer to transform the data before passing it along to the hidden
layer. Moreover, we propose a Gender-Ethnicity-Dependent triplet feature
exclusively trained on a variety of specific demographic groups. This feature
enables learning and extracting of useful facial features of each group.
Experiments were conducted to evaluate and compare the performances of SELM,
Extreme Learning Machine, and DCNN. The experimental results showed that the
proposed feature was able to perform correct classification at 97.87% accuracy
and 99.45% AUC. They also showed that using SELM in conjunction with the
proposed feature provided 98.31% accuracy and 99.72% AUC. They outperformed the
well-known DCNN and Extreme Leaning Machine methods by a wide margin.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kudisthalert_W/0/1/0/all/0/1"&gt;Wasu Kudisthalert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pasupa_K/0/1/0/all/0/1"&gt;Kitsuchart Pasupa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morales_A/0/1/0/all/0/1"&gt;Aythami Morales&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fierrez_J/0/1/0/all/0/1"&gt;Julian Fierrez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Contrastive Learning for View Classification of Echocardiograms. (arXiv:2108.03124v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03124</id>
        <link href="http://arxiv.org/abs/2108.03124"/>
        <updated>2021-08-09T00:49:27.253Z</updated>
        <summary type="html"><![CDATA[Analysis of cardiac ultrasound images is commonly performed in routine
clinical practice for quantification of cardiac function. Its increasing
automation frequently employs deep learning networks that are trained to
predict disease or detect image features. However, such models are extremely
data-hungry and training requires labelling of many thousands of images by
experienced clinicians. Here we propose the use of contrastive learning to
mitigate the labelling bottleneck. We train view classification models for
imbalanced cardiac ultrasound datasets and show improved performance for
views/classes for which minimal labelled data is available. Compared to a naive
baseline model, we achieve an improvement in F1 score of up to 26% in those
views while maintaining state-of-the-art performance for the views with
sufficiently many labelled training observations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chartsias_A/0/1/0/all/0/1"&gt;Agisilaos Chartsias&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1"&gt;Shan Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mumith_A/0/1/0/all/0/1"&gt;Angela Mumith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oliveira_J/0/1/0/all/0/1"&gt;Jorge Oliveira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhatia_K/0/1/0/all/0/1"&gt;Kanwal Bhatia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kainz_B/0/1/0/all/0/1"&gt;Bernhard Kainz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Beqiri_A/0/1/0/all/0/1"&gt;Arian Beqiri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Source-Free Domain Adaptation for Image Segmentation. (arXiv:2108.03152v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03152</id>
        <link href="http://arxiv.org/abs/2108.03152"/>
        <updated>2021-08-09T00:49:27.245Z</updated>
        <summary type="html"><![CDATA[Domain adaptation (DA) has drawn high interest for its capacity to adapt a
model trained on labeled source data to perform well on unlabeled or weakly
labeled target data from a different domain. Most common DA techniques require
concurrent access to the input images of both the source and target domains.
However, in practice, privacy concerns often impede the availability of source
images in the adaptation phase. This is a very frequent DA scenario in medical
imaging, where, for instance, the source and target images could come from
different clinical sites. We introduce a source-free domain adaptation for
image segmentation. Our formulation is based on minimizing a label-free entropy
loss defined over target-domain data, which we further guide with a
domain-invariant prior on the segmentation regions. Many priors can be derived
from anatomical information. Here, a class ratio prior is estimated from
anatomical knowledge and integrated in the form of a Kullback Leibler (KL)
divergence in our overall loss function. Furthermore, we motivate our overall
loss with an interesting link to maximizing the mutual information between the
target images and their label predictions. We show the effectiveness of our
prior aware entropy minimization in a variety of domain-adaptation scenarios,
with different modalities and applications, including spine, prostate, and
cardiac segmentation. Our method yields comparable results to several state of
the art adaptation techniques, despite having access to much less information,
as the source images are entirely absent in our adaptation phase. Our
straightforward adaptation strategy uses only one network, contrary to popular
adversarial techniques, which are not applicable to a source-free DA setting.
Our framework can be readily used in a breadth of segmentation problems, and
our code is publicly available: https://github.com/mathilde-b/SFDA]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bateson_M/0/1/0/all/0/1"&gt;Mathilde Bateson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dolz_J/0/1/0/all/0/1"&gt;Jose Dolz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kervadec_H/0/1/0/all/0/1"&gt;Hoel Kervadec&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lombaert_H/0/1/0/all/0/1"&gt;Herv&amp;#xe9; Lombaert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ayed_I/0/1/0/all/0/1"&gt;Ismail Ben Ayed&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ILVR: Conditioning Method for Denoising Diffusion Probabilistic Models. (arXiv:2108.02938v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02938</id>
        <link href="http://arxiv.org/abs/2108.02938"/>
        <updated>2021-08-09T00:49:27.238Z</updated>
        <summary type="html"><![CDATA[Denoising diffusion probabilistic models (DDPM) have shown remarkable
performance in unconditional image generation. However, due to the
stochasticity of the generative process in DDPM, it is challenging to generate
images with the desired semantics. In this work, we propose Iterative Latent
Variable Refinement (ILVR), a method to guide the generative process in DDPM to
generate high-quality images based on a given reference image. Here, the
refinement of the generative process in DDPM enables a single DDPM to sample
images from various sets directed by the reference image. The proposed ILVR
method generates high-quality images while controlling the generation. The
controllability of our method allows adaptation of a single DDPM without any
additional learning in various image generation tasks, such as generation from
various downsampling factors, multi-domain image translation, paint-to-image,
and editing with scribbles.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1"&gt;Jooyoung Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1"&gt;Sungwon Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jeong_Y/0/1/0/all/0/1"&gt;Yonghyun Jeong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gwon_Y/0/1/0/all/0/1"&gt;Youngjune Gwon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1"&gt;Sungroh Yoon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ELSED: Enhanced Line SEgment Drawing. (arXiv:2108.03144v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03144</id>
        <link href="http://arxiv.org/abs/2108.03144"/>
        <updated>2021-08-09T00:49:27.231Z</updated>
        <summary type="html"><![CDATA[Detecting local features, such as corners, segments or blobs, is the first
step in the pipeline of many Computer Vision applications. Its speed is crucial
for real time applications. In this paper we present ELSED, the fastest line
segment detector in the literature. The key for its efficiency is a local
segment growing algorithm that connects gradient aligned pixels in presence of
small discontinuities. The proposed algorithm not only runs in devices with
very low end hardware, but may also be parametrized to foster the detection of
short or longer segments, depending on the task at hand. We also introduce new
metrics to evaluate the accuracy and repeatability of segment detectors. In our
experiments with different public benchmarks we prove that our method is the
most efficient in the literature and quantify the accuracy traded for such
gain.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Suarez_I/0/1/0/all/0/1"&gt;Iago Su&amp;#xe1;rez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Buenaposada_J/0/1/0/all/0/1"&gt;Jos&amp;#xe9; M. Buenaposada&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baumela_L/0/1/0/all/0/1"&gt;Luis Baumela&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[COVID-Net US: A Tailored, Highly Efficient, Self-Attention Deep Convolutional Neural Network Design for Detection of COVID-19 Patient Cases from Point-of-care Ultrasound Imaging. (arXiv:2108.03131v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2108.03131</id>
        <link href="http://arxiv.org/abs/2108.03131"/>
        <updated>2021-08-09T00:49:27.223Z</updated>
        <summary type="html"><![CDATA[The Coronavirus Disease 2019 (COVID-19) pandemic has impacted many aspects of
life globally, and a critical factor in mitigating its effects is screening
individuals for infections, thereby allowing for both proper treatment for
those individuals as well as action to be taken to prevent further spread of
the virus. Point-of-care ultrasound (POCUS) imaging has been proposed as a
screening tool as it is a much cheaper and easier to apply imaging modality
than others that are traditionally used for pulmonary examinations, namely
chest x-ray and computed tomography. Given the scarcity of expert radiologists
for interpreting POCUS examinations in many highly affected regions around the
world, low-cost deep learning-driven clinical decision support solutions can
have a large impact during the on-going pandemic. Motivated by this, we
introduce COVID-Net US, a highly efficient, self-attention deep convolutional
neural network design tailored for COVID-19 screening from lung POCUS images.
Experimental results show that the proposed COVID-Net US can achieve an AUC of
over 0.98 while achieving 353X lower architectural complexity, 62X lower
computational complexity, and 14.3X faster inference times on a Raspberry Pi.
Clinical validation was also conducted, where select cases were reviewed and
reported on by a practicing clinician (20 years of clinical practice)
specializing in intensive care (ICU) and 15 years of expertise in POCUS
interpretation. To advocate affordable healthcare and artificial intelligence
for resource-constrained environments, we have made COVID-Net US open source
and publicly available as part of the COVID-Net open source initiative.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+MacLean_A/0/1/0/all/0/1"&gt;Alexander MacLean&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Abbasi_S/0/1/0/all/0/1"&gt;Saad Abbasi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ebadi_A/0/1/0/all/0/1"&gt;Ashkan Ebadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhao_A/0/1/0/all/0/1"&gt;Andy Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Pavlova_M/0/1/0/all/0/1"&gt;Maya Pavlova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gunraj_H/0/1/0/all/0/1"&gt;Hayden Gunraj&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xi_P/0/1/0/all/0/1"&gt;Pengcheng Xi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kohli_S/0/1/0/all/0/1"&gt;Sonny Kohli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wong_A/0/1/0/all/0/1"&gt;Alexander Wong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Full-Duplex Strategy for Video Object Segmentation. (arXiv:2108.03151v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03151</id>
        <link href="http://arxiv.org/abs/2108.03151"/>
        <updated>2021-08-09T00:49:27.203Z</updated>
        <summary type="html"><![CDATA[Appearance and motion are two important sources of information in video
object segmentation (VOS). Previous methods mainly focus on using simplex
solutions, lowering the upper bound of feature collaboration among and across
these two cues. In this paper, we study a novel framework, termed the FSNet
(Full-duplex Strategy Network), which designs a relational cross-attention
module (RCAM) to achieve the bidirectional message propagation across embedding
subspaces. Furthermore, the bidirectional purification module (BPM) is
introduced to update the inconsistent features between the spatial-temporal
embeddings, effectively improving the model robustness. By considering the
mutual restraint within the full-duplex strategy, our FSNet performs the
cross-modal feature-passing (i.e., transmission and receiving) simultaneously
before the fusion and decoding stage, making it robust to various challenging
scenarios (e.g., motion blur, occlusion) in VOS. Extensive experiments on five
popular benchmarks (i.e., DAVIS$_{16}$, FBMS, MCL, SegTrack-V2, and
DAVSOD$_{19}$) show that our FSNet outperforms other state-of-the-arts for both
the VOS and video salient object detection tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ji_G/0/1/0/all/0/1"&gt;Ge-Peng Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_K/0/1/0/all/0/1"&gt;Keren Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Zhe Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1"&gt;Deng-Ping Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1"&gt;Jianbing Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1"&gt;Ling Shao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MmWave Radar and Vision Fusion based Object Detection for Autonomous Driving: A Survey. (arXiv:2108.03004v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03004</id>
        <link href="http://arxiv.org/abs/2108.03004"/>
        <updated>2021-08-09T00:49:27.196Z</updated>
        <summary type="html"><![CDATA[With autonomous driving developing in a booming stage, accurate object
detection in complex scenarios attract wide attention to ensure the safety of
autonomous driving. Millimeter wave (mmWave) radar and vision fusion is a
mainstream solution for accurate obstacle detection. This article presents a
detailed survey on mmWave radar and vision fusion based obstacle detection
methods. Firstly, we introduce the tasks, evaluation criteria and datasets of
object detection for autonomous driving. Then, the process of mmWave radar and
vision fusion is divided into three parts: sensor deployment, sensor
calibration and sensor fusion, which are reviewed comprehensively. Especially,
we classify the fusion methods into data level, decision level and feature
level fusion methods. Besides, we introduce the fusion of lidar and vision in
autonomous driving in the aspects of obstacle detection, object classification
and road segmentation, which is promising in the future. Finally, we summarize
this article.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1"&gt;Zhiqing Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1"&gt;Fengkai Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1"&gt;Shuo Chang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yangyang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1"&gt;Huici Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1"&gt;Zhiyong Feng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Contrastive Learning by Visualizing Feature Transformation. (arXiv:2108.02982v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02982</id>
        <link href="http://arxiv.org/abs/2108.02982"/>
        <updated>2021-08-09T00:49:27.189Z</updated>
        <summary type="html"><![CDATA[Contrastive learning, which aims at minimizing the distance between positive
pairs while maximizing that of negative ones, has been widely and successfully
applied in unsupervised feature learning, where the design of positive and
negative (pos/neg) pairs is one of its keys. In this paper, we attempt to
devise a feature-level data manipulation, differing from data augmentation, to
enhance the generic contrastive self-supervised learning. To this end, we first
design a visualization scheme for pos/neg score (Pos/neg score indicates cosine
similarity of pos/neg pair.) distribution, which enables us to analyze,
interpret and understand the learning process. To our knowledge, this is the
first attempt of its kind. More importantly, leveraging this tool, we gain some
significant observations, which inspire our novel Feature Transformation
proposals including the extrapolation of positives. This operation creates
harder positives to boost the learning because hard positives enable the model
to be more view-invariant. Besides, we propose the interpolation among
negatives, which provides diversified negatives and makes the model more
discriminative. It is the first attempt to deal with both challenges
simultaneously. Experiment results show that our proposed Feature
Transformation can improve at least 6.0% accuracy on ImageNet-100 over MoCo
baseline, and about 2.0% accuracy on ImageNet-1K over the MoCoV2 baseline.
Transferring to the downstream tasks successfully demonstrate our model is less
task-bias. Visualization tools and codes
https://github.com/DTennant/CL-Visualizing-Feature-Transformation .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_R/0/1/0/all/0/1"&gt;Rui Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1"&gt;Bingchen Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jingen Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1"&gt;Zhenglong Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Chang Wen Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI-based Aortic Vessel Tree Segmentation for Cardiovascular Diseases Treatment: Status Quo. (arXiv:2108.02998v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02998</id>
        <link href="http://arxiv.org/abs/2108.02998"/>
        <updated>2021-08-09T00:49:27.181Z</updated>
        <summary type="html"><![CDATA[The aortic vessel tree is composed of the aorta and its branching arteries,
and plays a key role in supplying the whole body with blood. Aortic diseases,
like aneurysms or dissections, can lead to an aortic rupture, whose treatment
with open surgery is highly risky. Therefore, patients commonly undergo drug
treatment under constant monitoring, which requires regular inspections of the
vessels through imaging. The standard imaging modality for diagnosis and
monitoring is computed tomography (CT), which can provide a detailed picture of
the aorta and its branching vessels if combined with a contrast agent,
resulting in a CT angiography (CTA). Optimally, the whole aortic vessel tree
geometry from consecutive CTAs, are overlaid and compared. This allows to not
only detect changes in the aorta, but also more peripheral vessel tree changes,
caused by the primary pathology or newly developed. When performed manually,
this reconstruction requires slice by slice contouring, which could easily take
a whole day for a single aortic vessel tree and, hence, is not feasible in
clinical practice. Automatic or semi-automatic vessel tree segmentation
algorithms, on the other hand, can complete this task in a fraction of the
manual execution time and run in parallel to the clinical routine of the
clinicians. In this paper, we systematically review computing techniques for
the automatic and semi-automatic segmentation of the aortic vessel tree. The
review concludes with an in-depth discussion on how close these
state-of-the-art approaches are to an application in clinical practice and how
active this research field is, taking into account the number of publications,
datasets and challenges.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1"&gt;Yuan Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pepe_A/0/1/0/all/0/1"&gt;Antonio Pepe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jianning Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gsaxner_C/0/1/0/all/0/1"&gt;Christina Gsaxner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_F/0/1/0/all/0/1"&gt;Fen-hua Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kleesiek_J/0/1/0/all/0/1"&gt;Jens Kleesiek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frangi_A/0/1/0/all/0/1"&gt;Alejandro F. Frangi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Egger_J/0/1/0/all/0/1"&gt;Jan Egger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TS4Net: Two-Stage Sample Selective Strategy for Rotating Object Detection. (arXiv:2108.03116v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03116</id>
        <link href="http://arxiv.org/abs/2108.03116"/>
        <updated>2021-08-09T00:49:27.173Z</updated>
        <summary type="html"><![CDATA[Rotating object detection has wide applications in aerial photographs, remote
sensing images, UAVs, etc. At present, most of the rotating object detection
datasets focus on the field of remote sensing, and these images are usually
shot in high-altitude scenes. However, image datasets captured at low-altitude
areas also should be concerned, such as drone-based datasets. So we present a
low-altitude dronebased dataset, named UAV-ROD, aiming to promote the research
and development in rotating object detection and UAV applications. The UAV-ROD
consists of 1577 images and 30,090 instances of car category annotated by
oriented bounding boxes. In particular, The UAV-ROD can be utilized for the
rotating object detection, vehicle orientation recognition and object counting
tasks. Compared with horizontal object detection, the regression stage of the
rotation detection is a tricky problem. In this paper, we propose a rotating
object detector TS4Net, which contains anchor refinement module (ARM) and
two-stage sample selective strategy (TS4). The ARM can convert preseted
horizontal anchors into high-quality rotated anchors through twostage anchor
refinement. The TS4 module utilizes different constrained sample selective
strategies to allocate positive and negative samples, which is adaptive to the
regression task in different stages. Benefiting from the ARM and TS4, the
TS4Net can achieve superior performance for rotating object detection solely
with one preseted horizontal anchor. Extensive experimental results on UAV-ROD
dataset and three remote sensing datasets DOTA, HRSC2016 and UCAS-AOD
demonstrate that our method achieves competitive performance against most
state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Feng_K/0/1/0/all/0/1"&gt;Kai Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1"&gt;Weixing Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1"&gt;Jun Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_F/0/1/0/all/0/1"&gt;Feng Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_D/0/1/0/all/0/1"&gt;Dongdong Zheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Uncertainty-Based Dynamic Graph Neighborhoods For Medical Segmentation. (arXiv:2108.03117v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03117</id>
        <link href="http://arxiv.org/abs/2108.03117"/>
        <updated>2021-08-09T00:49:27.153Z</updated>
        <summary type="html"><![CDATA[In recent years, deep learning based methods have shown success in essential
medical image analysis tasks such as segmentation. Post-processing and refining
the results of segmentation is a common practice to decrease the
misclassifications originating from the segmentation network. In addition to
widely used methods like Conditional Random Fields (CRFs) which focus on the
structure of the segmented volume/area, a graph-based recent approach makes use
of certain and uncertain points in a graph and refines the segmentation
according to a small graph convolutional network (GCN). However, there are two
drawbacks of the approach: most of the edges in the graph are assigned randomly
and the GCN is trained independently from the segmentation network. To address
these issues, we define a new neighbor-selection mechanism according to feature
distances and combine the two networks in the training procedure. According to
the experimental results on pancreas segmentation from Computed Tomography (CT)
images, we demonstrate improvement in the quantitative measures. Also,
examining the dynamic neighbors created by our method, edges between
semantically similar image parts are observed. The proposed method also shows
qualitative enhancements in the segmentation maps, as demonstrated in the
visual results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Demir_U/0/1/0/all/0/1"&gt;Ufuk Demir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ozer_A/0/1/0/all/0/1"&gt;Atahan Ozer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sahin_Y/0/1/0/all/0/1"&gt;Yusuf H. Sahin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Unal_G/0/1/0/all/0/1"&gt;Gozde Unal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Beyond the Hausdorff Metric in Digital Topology. (arXiv:2108.03114v1 [cs.CG])]]></title>
        <id>http://arxiv.org/abs/2108.03114</id>
        <link href="http://arxiv.org/abs/2108.03114"/>
        <updated>2021-08-09T00:49:27.144Z</updated>
        <summary type="html"><![CDATA[Two objects may be close in the Hausdor? metric, yet have very different
geometric and topological properties. We examine other methods of comparing
digital images such that objects close in each of these measures have some
similar geometric or topological property. Such measures may be combined with
the Hausdorff metric to yield a metric in which close images are similar with
respect to multiple properties.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Boxer_L/0/1/0/all/0/1"&gt;Laurence Boxer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fast and Accurate Low-Rank Tensor Completion Methods Based on QR Decomposition and $L_{2,1}$ Norm Minimization. (arXiv:2108.03002v1 [math.NA])]]></title>
        <id>http://arxiv.org/abs/2108.03002</id>
        <link href="http://arxiv.org/abs/2108.03002"/>
        <updated>2021-08-09T00:49:27.136Z</updated>
        <summary type="html"><![CDATA[More recently, an Approximate SVD Based on Qatar Riyal (QR) Decomposition
(CSVD-QR) method for matrix complete problem is presented, whose computational
complexity is $O(r^2(m+n))$, which is mainly due to that $r$ is far less than
$\min\{m,n\}$, where $r$ represents the largest number of singular values of
matrix $X$. What is particularly interesting is that after replacing the
nuclear norm with the $L_{2,1}$ norm proposed based on this decomposition, as
the upper bound of the nuclear norm, when the intermediate matrix $D$ in its
decomposition is close to the diagonal matrix, it will converge to the nuclear
norm, and is exactly equal, when the $D$ matrix is equal to the diagonal
matrix, to the nuclear norm, which ingeniously avoids the calculation of the
singular value of the matrix. To the best of our knowledge, there is no
literature to generalize and apply it to solve tensor complete problems.
Inspired by this, in this paper we propose a class of tensor minimization model
based on $L_{2,1}$ norm and CSVD-QR method for the tensor complete problem,
which is convex and therefore has a global minimum solution.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Zhang_H/0/1/0/all/0/1"&gt;HongBing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Liu_X/0/1/0/all/0/1"&gt;XinYi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Fan_H/0/1/0/all/0/1"&gt;HongTao Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Li_Y/0/1/0/all/0/1"&gt;YaJing Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Ye_Y/0/1/0/all/0/1"&gt;Yinlin Ye&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spatiotemporal Contrastive Learning of Facial Expressions in Videos. (arXiv:2108.03064v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03064</id>
        <link href="http://arxiv.org/abs/2108.03064"/>
        <updated>2021-08-09T00:49:27.128Z</updated>
        <summary type="html"><![CDATA[We propose a self-supervised contrastive learning approach for facial
expression recognition (FER) in videos. We propose a novel temporal
sampling-based augmentation scheme to be utilized in addition to standard
spatial augmentations used for contrastive learning. Our proposed temporal
augmentation scheme randomly picks from one of three temporal sampling
techniques: (1) pure random sampling, (2) uniform sampling, and (3) sequential
sampling. This is followed by a combination of up to three standard spatial
augmentations. We then use a deep R(2+1)D network for FER, which we train in a
self-supervised fashion based on the augmentations and subsequently fine-tune.
Experiments are performed on the Oulu-CASIA dataset and the performance is
compared to other works in FER. The results indicate that our method achieves
an accuracy of 89.4%, setting a new state-of-the-art by outperforming other
works. Additional experiments and analysis confirm the considerable
contribution of the proposed temporal augmentation versus the existing spatial
ones.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1"&gt;Shuvendu Roy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Etemad_A/0/1/0/all/0/1"&gt;Ali Etemad&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lung Ultrasound Segmentation and Adaptation between COVID-19 and Community-Acquired Pneumonia. (arXiv:2108.03138v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03138</id>
        <link href="http://arxiv.org/abs/2108.03138"/>
        <updated>2021-08-09T00:49:27.121Z</updated>
        <summary type="html"><![CDATA[Lung ultrasound imaging has been shown effective in detecting typical
patterns for interstitial pneumonia, as a point-of-care tool for both patients
with COVID-19 and other community-acquired pneumonia (CAP). In this work, we
focus on the hyperechoic B-line segmentation task. Using deep neural networks,
we automatically outline the regions that are indicative of pathology-sensitive
artifacts and their associated sonographic patterns. With a real-world
data-scarce scenario, we investigate approaches to utilize both COVID-19 and
CAP lung ultrasound data to train the networks; comparing fine-tuning and
unsupervised domain adaptation. Segmenting either type of lung condition at
inference may support a range of clinical applications during evolving epidemic
stages, but also demonstrates value in resource-constrained clinical scenarios.
Adapting real clinical data acquired from COVID-19 patients to those from CAP
patients significantly improved Dice scores from 0.60 to 0.87 (p < 0.001) and
from 0.43 to 0.71 (p < 0.001), on independent COVID-19 and CAP test cases,
respectively. It is of practical value that the improvement was demonstrated
with only a small amount of data in both training and adaptation data sets, a
common constraint for deploying machine learning models in clinical practice.
Interestingly, we also report that the inverse adaptation, from labelled CAP
data to unlabeled COVID-19 data, did not demonstrate an improvement when tested
on either condition. Furthermore, we offer a possible explanation that
correlates the segmentation performance to label consistency and data domain
diversity in this point-of-care lung ultrasound application.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mason_H/0/1/0/all/0/1"&gt;Harry Mason&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cristoni_L/0/1/0/all/0/1"&gt;Lorenzo Cristoni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Walden_A/0/1/0/all/0/1"&gt;Andrew Walden&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lazzari_R/0/1/0/all/0/1"&gt;Roberto Lazzari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pulimood_T/0/1/0/all/0/1"&gt;Thomas Pulimood&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grandjean_L/0/1/0/all/0/1"&gt;Louis Grandjean&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wheeler_Kingshott_C/0/1/0/all/0/1"&gt;Claudia AM Gandini Wheeler-Kingshott&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1"&gt;Yipeng Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baum_Z/0/1/0/all/0/1"&gt;Zachary MC Baum&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Meta-class Memory for Few-Shot Semantic Segmentation. (arXiv:2108.02958v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02958</id>
        <link href="http://arxiv.org/abs/2108.02958"/>
        <updated>2021-08-09T00:49:27.112Z</updated>
        <summary type="html"><![CDATA[Currently, the state-of-the-art methods treat few-shot semantic segmentation
task as a conditional foreground-background segmentation problem, assuming each
class is independent. In this paper, we introduce the concept of meta-class,
which is the meta information (e.g. certain middle-level features) shareable
among all classes. To explicitly learn meta-class representations in few-shot
segmentation task, we propose a novel Meta-class Memory based few-shot
segmentation method (MM-Net), where we introduce a set of learnable memory
embeddings to memorize the meta-class information during the base class
training and transfer to novel classes during the inference stage. Moreover,
for the $k$-shot scenario, we propose a novel image quality measurement module
to select images from the set of support images. A high-quality class prototype
could be obtained with the weighted sum of support image features based on the
quality measure. Experiments on both PASCAL-$5^i$ and COCO dataset shows that
our proposed method is able to achieve state-of-the-art results in both 1-shot
and 5-shot settings. Particularly, our proposed MM-Net achieves 37.5\% mIoU on
the COCO dataset in 1-shot setting, which is 5.1\% higher than the previous
state-of-the-art.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Zhonghua Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1"&gt;Xiangxi Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+lin_G/0/1/0/all/0/1"&gt;Guosheng lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1"&gt;Jianfei Cai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[From Synthetic to Real: Image Dehazing Collaborating with Unlabeled Real Data. (arXiv:2108.02934v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02934</id>
        <link href="http://arxiv.org/abs/2108.02934"/>
        <updated>2021-08-09T00:49:27.086Z</updated>
        <summary type="html"><![CDATA[Single image dehazing is a challenging task, for which the domain shift
between synthetic training data and real-world testing images usually leads to
degradation of existing methods. To address this issue, we propose a novel
image dehazing framework collaborating with unlabeled real data. First, we
develop a disentangled image dehazing network (DID-Net), which disentangles the
feature representations into three component maps, i.e. the latent haze-free
image, the transmission map, and the global atmospheric light estimate,
respecting the physical model of a haze process. Our DID-Net predicts the three
component maps by progressively integrating features across scales, and refines
each map by passing an independent refinement network. Then a
disentangled-consistency mean-teacher network (DMT-Net) is employed to
collaborate unlabeled real data for boosting single image dehazing.
Specifically, we encourage the coarse predictions and refinements of each
disentangled component to be consistent between the student and teacher
networks by using a consistency loss on unlabeled real data. We make comparison
with 13 state-of-the-art dehazing methods on a new collected dataset (Haze4K)
and two widely-used dehazing datasets (i.e., SOTS and HazeRD), as well as on
real-world hazy images. Experimental results demonstrate that our method has
obvious quantitative and qualitative improvements over the existing methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Ye Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1"&gt;Lei Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pei_S/0/1/0/all/0/1"&gt;Shunda Pei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1"&gt;Huazhu Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1"&gt;Jing Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Qing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wan_L/0/1/0/all/0/1"&gt;Liang Wan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_W/0/1/0/all/0/1"&gt;Wei Feng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Interpretable Visual Understanding with Cognitive Attention Network. (arXiv:2108.02924v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02924</id>
        <link href="http://arxiv.org/abs/2108.02924"/>
        <updated>2021-08-09T00:49:27.078Z</updated>
        <summary type="html"><![CDATA[While image understanding on recognition-level has achieved remarkable
advancements, reliable visual scene understanding requires comprehensive image
understanding on recognition-level but also cognition-level, which calls for
exploiting the multi-source information as well as learning different levels of
understanding and extensive commonsense knowledge. In this paper, we propose a
novel Cognitive Attention Network (CAN) for visual commonsense reasoning to
achieve interpretable visual understanding. Specifically, we first introduce an
image-text fusion module to fuse information from images and text collectively.
Second, a novel inference module is designed to encode commonsense among image,
query and response. Extensive experiments on large-scale Visual Commonsense
Reasoning (VCR) benchmark dataset demonstrate the effectiveness of our
approach. The implementation is publicly available at
https://github.com/tanjatang/CAN]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1"&gt;Xuejiao Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Wenbin Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1"&gt;Yi Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Turner_K/0/1/0/all/0/1"&gt;Kea Turner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Derr_T/0/1/0/all/0/1"&gt;Tyler Derr&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1"&gt;Mengyu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ntoutsi_E/0/1/0/all/0/1"&gt;Eirini Ntoutsi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reducing Spatial Labeling Redundancy for Semi-supervised Crowd Counting. (arXiv:2108.02970v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02970</id>
        <link href="http://arxiv.org/abs/2108.02970"/>
        <updated>2021-08-09T00:49:27.071Z</updated>
        <summary type="html"><![CDATA[Labeling is onerous for crowd counting as it should annotate each individual
in crowd images. Recently, several methods have been proposed for
semi-supervised crowd counting to reduce the labeling efforts. Given a limited
labeling budget, they typically select a few crowd images and densely label all
individuals in each of them. Despite the promising results, we argue the
None-or-All labeling strategy is suboptimal as the densely labeled individuals
in each crowd image usually appear similar while the massive unlabeled crowd
images may contain entirely diverse individuals. To this end, we propose to
break the labeling chain of previous methods and make the first attempt to
reduce spatial labeling redundancy for semi-supervised crowd counting. First,
instead of annotating all the regions in each crowd image, we propose to
annotate the representative ones only. We analyze the region representativeness
from both vertical and horizontal directions, and formulate them as cluster
centers of Gaussian Mixture Models. Additionally, to leverage the rich
unlabeled regions, we exploit the similarities among individuals in each crowd
image to directly supervise the unlabeled regions via feature propagation
instead of the error-prone label propagation employed in the previous methods.
In this way, we can transfer the original spatial labeling redundancy caused by
individual similarities to effective supervision signals on the unlabeled
regions. Extensive experiments on the widely-used benchmarks demonstrate that
our method can outperform previous best approaches by a large margin.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yongtuo Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1"&gt;Sucheng Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chai_L/0/1/0/all/0/1"&gt;Liangyu Chai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1"&gt;Hanjie Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1"&gt;Jing Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1"&gt;Dan Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1"&gt;Shengfeng He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[STR-GQN: Scene Representation and Rendering for Unknown Cameras Based on Spatial Transformation Routing. (arXiv:2108.03072v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03072</id>
        <link href="http://arxiv.org/abs/2108.03072"/>
        <updated>2021-08-09T00:49:27.063Z</updated>
        <summary type="html"><![CDATA[Geometry-aware modules are widely applied in recent deep learning
architectures for scene representation and rendering. However, these modules
require intrinsic camera information that might not be obtained accurately. In
this paper, we propose a Spatial Transformation Routing (STR) mechanism to
model the spatial properties without applying any geometric prior. The STR
mechanism treats the spatial transformation as the message passing process, and
the relation between the view poses and the routing weights is modeled by an
end-to-end trainable neural network. Besides, an Occupancy Concept Mapping
(OCM) framework is proposed to provide explainable rationals for scene-fusion
processes. We conducted experiments on several datasets and show that the
proposed STR mechanism improves the performance of the Generative Query Network
(GQN). The visualization results reveal that the routing process can pass the
observed information from one location of some view to the associated location
in the other view, which demonstrates the advantage of the proposed model in
terms of spatial cognition.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1"&gt;Wen-Cheng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_M/0/1/0/all/0/1"&gt;Min-Chun Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Chu-Song Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Simpler is Better: Few-shot Semantic Segmentation with Classifier Weight Transformer. (arXiv:2108.03032v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03032</id>
        <link href="http://arxiv.org/abs/2108.03032"/>
        <updated>2021-08-09T00:49:27.056Z</updated>
        <summary type="html"><![CDATA[A few-shot semantic segmentation model is typically composed of a CNN
encoder, a CNN decoder and a simple classifier (separating foreground and
background pixels). Most existing methods meta-learn all three model components
for fast adaptation to a new class. However, given that as few as a single
support set image is available, effective model adaption of all three
components to the new class is extremely challenging. In this work we propose
to simplify the meta-learning task by focusing solely on the simplest
component, the classifier, whilst leaving the encoder and decoder to
pre-training. We hypothesize that if we pre-train an off-the-shelf segmentation
model over a set of diverse training classes with sufficient annotations, the
encoder and decoder can capture rich discriminative features applicable for any
unseen classes, rendering the subsequent meta-learning stage unnecessary. For
the classifier meta-learning, we introduce a Classifier Weight Transformer
(CWT) designed to dynamically adapt the supportset trained classifier's weights
to each query image in an inductive way. Extensive experiments on two standard
benchmarks show that despite its simplicity, our method outperforms the
state-of-the-art alternatives, often by a large margin.Code is available on
https://github.com/zhiheLu/CWTfor-FSS.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+lu_Z/0/1/0/all/0/1"&gt;Zhihe lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1"&gt;Sen He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1"&gt;Xiatian Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Li Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1"&gt;Yi-Zhe Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1"&gt;Tao Xiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AceNAS: Learning to Rank Ace Neural Architectures with Weak Supervision of Weight Sharing. (arXiv:2108.03001v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03001</id>
        <link href="http://arxiv.org/abs/2108.03001"/>
        <updated>2021-08-09T00:49:27.038Z</updated>
        <summary type="html"><![CDATA[Architecture performance predictors have been widely used in neural
architecture search (NAS). Although they are shown to be simple and effective,
the optimization objectives in previous arts (e.g., precise accuracy estimation
or perfect ranking of all architectures in the space) did not capture the
ranking nature of NAS. In addition, a large number of ground-truth
architecture-accuracy pairs are usually required to build a reliable predictor,
making the process too computationally expensive. To overcome these, in this
paper, we look at NAS from a novel point of view and introduce Learning to Rank
(LTR) methods to select the best (ace) architectures from a space.
Specifically, we propose to use Normalized Discounted Cumulative Gain (NDCG) as
the target metric and LambdaRank as the training algorithm. We also propose to
leverage weak supervision from weight sharing by pretraining architecture
representation on weak labels obtained from the super-net and then finetuning
the ranking model using a small number of architectures trained from scratch.
Extensive experiments on NAS benchmarks and large-scale search spaces
demonstrate that our approach outperforms SOTA with a significantly reduced
search cost.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yuge Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_C/0/1/0/all/0/1"&gt;Chenqian Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Quanlu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Li Lyna Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yaming Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1"&gt;Xiaotian Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yuqing Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adapting Segmentation Networks to New Domains by Disentangling Latent Representations. (arXiv:2108.03021v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03021</id>
        <link href="http://arxiv.org/abs/2108.03021"/>
        <updated>2021-08-09T00:49:27.029Z</updated>
        <summary type="html"><![CDATA[Deep learning models achieve outstanding accuracy in semantic segmentation,
however they require a huge amount of labeled data for their optimization.
Hence, domain adaptation approaches have come into play to transfer knowledge
acquired on a label-abundant source domain to a related label-scarce target
domain. However, such models do not generalize well to data with statistical
properties not perfectly matching the ones of the training samples. In this
work, we design and carefully analyze multiple latent space-shaping
regularization strategies that work in conjunction to reduce the domain
discrepancy in semantic segmentation. In particular, we devise a feature
clustering strategy to increase domain alignment, a feature perpendicularity
constraint to space apart feature belonging to different semantic classes,
including those not present in the current batch, and a feature norm alignment
strategy to separate active and inactive channels. Additionally, we propose a
novel performance metric to capture the relative efficacy of an adaptation
strategy compared to supervised training. We verify the effectiveness of our
framework in synthetic-to-real and real-to-real adaptation scenarios,
outperforming previous state-of-the-art methods on multiple road scenes
benchmarks and using different backbones.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Barbato_F/0/1/0/all/0/1"&gt;Francesco Barbato&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Michieli_U/0/1/0/all/0/1"&gt;Umberto Michieli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Toldo_M/0/1/0/all/0/1"&gt;Marco Toldo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zanuttigh_P/0/1/0/all/0/1"&gt;Pietro Zanuttigh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Vision-Based Food Analysis for Automatic Dietary Assessment. (arXiv:2108.02947v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02947</id>
        <link href="http://arxiv.org/abs/2108.02947"/>
        <updated>2021-08-09T00:49:27.021Z</updated>
        <summary type="html"><![CDATA[Background: Maintaining a healthy diet is vital to avoid health-related
issues, e.g., undernutrition, obesity and many non-communicable diseases. An
indispensable part of the health diet is dietary assessment. Traditional manual
recording methods are burdensome and contain substantial biases and errors.
Recent advances in Artificial Intelligence, especially computer vision
technologies, have made it possible to develop automatic dietary assessment
solutions, which are more convenient, less time-consuming and even more
accurate to monitor daily food intake.

Scope and approach: This review presents one unified Vision-Based Dietary
Assessment (VBDA) framework, which generally consists of three stages: food
image analysis, volume estimation and nutrient derivation. Vision-based food
analysis methods, including food recognition, detection and segmentation, are
systematically summarized, and methods of volume estimation and nutrient
derivation are also given. The prosperity of deep learning makes VBDA gradually
move to an end-to-end implementation, which applies food images to a single
network to directly estimate the nutrition. The recently proposed end-to-end
methods are also discussed. We further analyze existing dietary assessment
datasets, indicating that one large-scale benchmark is urgently needed, and
finally highlight key challenges and future trends for VBDA.

Key findings and conclusions: After thorough exploration, we find that
multi-task end-to-end deep learning approaches are one important trend of VBDA.
Despite considerable research progress, many challenges remain for VBDA due to
the meal complexity. We also provide the latest ideas for future development of
VBDA, e.g., fine-grained food analysis and accurate volume estimation. This
survey aims to encourage researchers to propose more practical solutions for
VBDA.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Min_W/0/1/0/all/0/1"&gt;Weiqing Min&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1"&gt;Tianhao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1"&gt;Xiaoxiao Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Haisheng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1"&gt;Shuqiang Jiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Learning-based Biological Anatomical Landmark Detection in Colonoscopy Videos. (arXiv:2108.02948v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02948</id>
        <link href="http://arxiv.org/abs/2108.02948"/>
        <updated>2021-08-09T00:49:27.013Z</updated>
        <summary type="html"><![CDATA[Colonoscopy is a standard imaging tool for visualizing the entire
gastrointestinal (GI) tract of patients to capture lesion areas. However, it
takes the clinicians excessive time to review a large number of images
extracted from colonoscopy videos. Thus, automatic detection of biological
anatomical landmarks within the colon is highly demanded, which can help reduce
the burden of clinicians by providing guidance information for the locations of
lesion areas. In this article, we propose a novel deep learning-based approach
to detect biological anatomical landmarks in colonoscopy videos. First, raw
colonoscopy video sequences are pre-processed to reject interference frames.
Second, a ResNet-101 based network is used to detect three biological
anatomical landmarks separately to obtain the intermediate detection results.
Third, to achieve more reliable localization of the landmark periods within the
whole video period, we propose to post-process the intermediate detection
results by identifying the incorrectly predicted frames based on their temporal
distribution and reassigning them back to the correct class. Finally, the
average detection accuracy reaches 99.75\%. Meanwhile, the average IoU of 0.91
shows a high degree of similarity between our predicted landmark periods and
ground truth. The experimental results demonstrate that our proposed model is
capable of accurately detecting and localizing biological anatomical landmarks
from colonoscopy videos.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Che_K/0/1/0/all/0/1"&gt;Kaiwei Che&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_C/0/1/0/all/0/1"&gt;Chengwei Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1"&gt;Yibing Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_N/0/1/0/all/0/1"&gt;Nachuan Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Ruo Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jiankun Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meng_M/0/1/0/all/0/1"&gt;Max Q.-H. Meng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Feature Detection for Hand Hygiene Stages. (arXiv:2108.03015v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.03015</id>
        <link href="http://arxiv.org/abs/2108.03015"/>
        <updated>2021-08-09T00:49:27.004Z</updated>
        <summary type="html"><![CDATA[The process of hand washing involves complex hand movements. There are six
principal sequential steps for washing hands as per the World Health
Organisation (WHO) guidelines. In this work, a detailed description of an
aluminium rig construction for creating a robust hand-washing dataset is
discussed. The preliminary results with the help of image processing and
computer vision algorithms for hand pose extraction and feature detection such
as Harris detector, Shi-Tomasi and SIFT are demonstrated. The hand hygiene
pose- Rub hands palm to palm was captured as an input image for running all the
experiments. The future work will focus upon processing the video recordings of
hand movements captured and applying deep-learning solutions for the
classification of hand-hygiene stages.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bakshi_R/0/1/0/all/0/1"&gt;Rashmi Bakshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Courtney_J/0/1/0/all/0/1"&gt;Jane Courtney&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berry_D/0/1/0/all/0/1"&gt;Damon Berry&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gavin_G/0/1/0/all/0/1"&gt;Graham Gavin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[High-frequency shape recovery from shading by CNN and domain adaptation. (arXiv:2108.02937v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02937</id>
        <link href="http://arxiv.org/abs/2108.02937"/>
        <updated>2021-08-09T00:49:26.997Z</updated>
        <summary type="html"><![CDATA[Importance of structured-light based one-shot scanning technique is
increasing because of its simple system configuration and ability of capturing
moving objects. One severe limitation of the technique is that it can capture
only sparse shape, but not high frequency shapes, because certain area of
projection pattern is required to encode spatial information. In this paper, we
propose a technique to recover high-frequency shapes by using shading
information, which is captured by one-shot RGB-D sensor based on structured
light with single camera. Since color image comprises shading information of
object surface, high-frequency shapes can be recovered by shape from shading
techniques. Although multiple images with different lighting positions are
required for shape from shading techniques, we propose a learning based
approach to recover shape from a single image. In addition, to overcome the
problem of preparing sufficient amount of data for training, we propose a new
data augmentation method for high-frequency shapes using synthetic data and
domain adaptation. Experimental results are shown to confirm the effectiveness
of the proposed method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tokieda_K/0/1/0/all/0/1"&gt;Kodai Tokieda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iwaguchi_T/0/1/0/all/0/1"&gt;Takafumi Iwaguchi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kawasaki_H/0/1/0/all/0/1"&gt;Hiroshi Kawasaki&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Smooth Mesh Estimation from Depth Data using Non-Smooth Convex Optimization. (arXiv:2108.02957v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02957</id>
        <link href="http://arxiv.org/abs/2108.02957"/>
        <updated>2021-08-09T00:49:26.978Z</updated>
        <summary type="html"><![CDATA[Meshes are commonly used as 3D maps since they encode the topology of the
scene while being lightweight.

Unfortunately, 3D meshes are mathematically difficult to handle directly
because of their combinatorial and discrete nature.

Therefore, most approaches generate 3D meshes of a scene after fusing depth
data using volumetric or other representations.

Nevertheless, volumetric fusion remains computationally expensive both in
terms of speed and memory.

In this paper, we leapfrog these intermediate representations and build a 3D
mesh directly from a depth map and the sparse landmarks triangulated with
visual odometry.

To this end, we formulate a non-smooth convex optimization problem that we
solve using a primal-dual method.

Our approach generates a smooth and accurate 3D mesh that substantially
improves the state-of-the-art on direct mesh reconstruction while running in
real-time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rosinol_A/0/1/0/all/0/1"&gt;Antoni Rosinol&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carlone_L/0/1/0/all/0/1"&gt;Luca Carlone&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Detailed Avatar Recovery from Single Image. (arXiv:2108.02931v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02931</id>
        <link href="http://arxiv.org/abs/2108.02931"/>
        <updated>2021-08-09T00:49:26.970Z</updated>
        <summary type="html"><![CDATA[This paper presents a novel framework to recover \emph{detailed} avatar from
a single image. It is a challenging task due to factors such as variations in
human shapes, body poses, texture, and viewpoints. Prior methods typically
attempt to recover the human body shape using a parametric-based template that
lacks the surface details. As such resulting body shape appears to be without
clothing. In this paper, we propose a novel learning-based framework that
combines the robustness of the parametric model with the flexibility of
free-form 3D deformation. We use the deep neural networks to refine the 3D
shape in a Hierarchical Mesh Deformation (HMD) framework, utilizing the
constraints from body joints, silhouettes, and per-pixel shading information.
Our method can restore detailed human body shapes with complete textures beyond
skinned models. Experiments demonstrate that our method has outperformed
previous state-of-the-art approaches, achieving better accuracy in terms of
both 2D IoU number and 3D metric distance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1"&gt;Hao Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zuo_X/0/1/0/all/0/1"&gt;Xinxin Zuo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Haotian Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Sen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1"&gt;Xun Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1"&gt;Ruigang Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[3DRIMR: 3D Reconstruction and Imaging via mmWave Radar based on Deep Learning. (arXiv:2108.02858v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02858</id>
        <link href="http://arxiv.org/abs/2108.02858"/>
        <updated>2021-08-09T00:49:26.962Z</updated>
        <summary type="html"><![CDATA[mmWave radar has been shown as an effective sensing technique in low
visibility, smoke, dusty, and dense fog environment. However tapping the
potential of radar sensing to reconstruct 3D object shapes remains a great
challenge, due to the characteristics of radar data such as sparsity, low
resolution, specularity, high noise, and multi-path induced shadow reflections
and artifacts. In this paper we propose 3D Reconstruction and Imaging via
mmWave Radar (3DRIMR), a deep learning based architecture that reconstructs 3D
shape of an object in dense detailed point cloud format, based on sparse raw
mmWave radar intensity data. The architecture consists of two back-to-back
conditional GAN deep neural networks: the first generator network generates 2D
depth images based on raw radar intensity data, and the second generator
network outputs 3D point clouds based on the results of the first generator.
The architecture exploits both convolutional neural network's convolutional
operation (that extracts local structure neighborhood information) and the
efficiency and detailed geometry capture capability of point clouds (other than
costly voxelization of 3D space or distance fields). Our experiments have
demonstrated 3DRIMR's effectiveness in reconstructing 3D objects, and its
performance improvement over standard techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1"&gt;Yue Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zhuoming Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Honggang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1"&gt;Zhi Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1"&gt;Deqiang Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Attention-based fusion of semantic boundary and non-boundary information to improve semantic segmentation. (arXiv:2108.02840v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02840</id>
        <link href="http://arxiv.org/abs/2108.02840"/>
        <updated>2021-08-09T00:49:26.954Z</updated>
        <summary type="html"><![CDATA[This paper introduces a method for image semantic segmentation grounded on a
novel fusion scheme, which takes place inside a deep convolutional neural
network. The main goal of our proposal is to explore object boundary
information to improve the overall segmentation performance. Unlike previous
works that combine boundary and segmentation features, or those that use
boundary information to regularize semantic segmentation, we instead propose a
novel approach that embodies boundary information onto segmentation. For that,
our semantic segmentation method uses two streams, which are combined through
an attention gate, forming an end-to-end Y-model. To the best of our knowledge,
ours is the first work to show that boundary detection can improve semantic
segmentation when fused through a semantic fusion gate (attention model). We
performed an extensive evaluation of our method over public data sets. We found
competitive results on all data sets after comparing our proposed model with
other twelve state-of-the-art segmenters, considering the same training
conditions. Our proposed model achieved the best mIoU on the CityScapes,
CamVid, and Pascal Context data sets, and the second best on Mapillary Vistas.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fontinele_J/0/1/0/all/0/1"&gt;Jefferson Fontinele&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lefundes_G/0/1/0/all/0/1"&gt;Gabriel Lefundes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oliveira_L/0/1/0/all/0/1"&gt;Luciano Oliveira&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dual-Tuning: Joint Prototype Transfer and Structure Regularization for Compatible Feature Learning. (arXiv:2108.02959v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02959</id>
        <link href="http://arxiv.org/abs/2108.02959"/>
        <updated>2021-08-09T00:49:26.935Z</updated>
        <summary type="html"><![CDATA[Visual retrieval system faces frequent model update and deployment. It is a
heavy workload to re-extract features of the whole database every time.Feature
compatibility enables the learned new visual features to be directly compared
with the old features stored in the database. In this way, when updating the
deployed model, we can bypass the inflexible and time-consuming feature
re-extraction process. However, the old feature space that needs to be
compatible is not ideal and faces the distribution discrepancy problem with the
new space caused by different supervision losses. In this work, we propose a
global optimization Dual-Tuning method to obtain feature compatibility against
different networks and losses. A feature-level prototype loss is proposed to
explicitly align two types of embedding features, by transferring global
prototype information. Furthermore, we design a component-level mutual
structural regularization to implicitly optimize the feature intrinsic
structure. Experimental results on million-scale datasets demonstrate that our
Dual-Tuning is able to obtain feature compatibility without sacrificing
performance. (Our code will be avaliable at
https://github.com/yanbai1993/Dual-Tuning)]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1"&gt;Yan Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiao_J/0/1/0/all/0/1"&gt;Jile Jiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1"&gt;Shengsen Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lou_Y/0/1/0/all/0/1"&gt;Yihang Lou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jun Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1"&gt;Xuetao Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duan_L/0/1/0/all/0/1"&gt;Ling-Yu Duan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Few-shot Unsupervised Domain Adaptation with Image-to-class Sparse Similarity Encoding. (arXiv:2108.02953v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02953</id>
        <link href="http://arxiv.org/abs/2108.02953"/>
        <updated>2021-08-09T00:49:26.927Z</updated>
        <summary type="html"><![CDATA[This paper investigates a valuable setting called few-shot unsupervised
domain adaptation (FS-UDA), which has not been sufficiently studied in the
literature. In this setting, the source domain data are labelled, but with
few-shot per category, while the target domain data are unlabelled. To address
the FS-UDA setting, we develop a general UDA model to solve the following two
key issues: the few-shot labeled data per category and the domain adaptation
between support and query sets. Our model is general in that once trained it
will be able to be applied to various FS-UDA tasks from the same source and
target domains. Inspired by the recent local descriptor based few-shot learning
(FSL), our general UDA model is fully built upon local descriptors (LDs) for
image classification and domain adaptation. By proposing a novel concept called
similarity patterns (SPs), our model not only effectively considers the spatial
relationship of LDs that was ignored in previous FSL methods, but also makes
the learned image similarity better serve the required domain alignment.
Specifically, we propose a novel IMage-to-class sparse Similarity Encoding
(IMSE) method. It learns SPs to extract the local discriminative information
for classification and meanwhile aligns the covariance matrix of the SPs for
domain adaptation. Also, domain adversarial training and multi-scale local
feature matching are performed upon LDs. Extensive experiments conducted on a
multi-domain benchmark dataset DomainNet demonstrates the state-of-the-art
performance of our IMSE for the novel setting of FS-UDA. In addition, for FSL,
our IMSE can also show better performance than most of recent FSL methods on
miniImageNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1"&gt;Shengqi Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1"&gt;Wanqi Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Lei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1"&gt;Luping Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1"&gt;Ming Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fine-grained Domain Adaptive Crowd Counting via Point-derived Segmentation. (arXiv:2108.02980v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02980</id>
        <link href="http://arxiv.org/abs/2108.02980"/>
        <updated>2021-08-09T00:49:26.920Z</updated>
        <summary type="html"><![CDATA[Existing domain adaptation methods for crowd counting view each crowd image
as a whole and reduce domain discrepancies on crowds and backgrounds
simultaneously. However, we argue that these methods are suboptimal, as crowds
and backgrounds have quite different characteristics and backgrounds may vary
dramatically in different crowd scenes (see Fig.~\ref{teaser}). This makes
crowds not well aligned across domains together with backgrounds in a holistic
manner. To this end, we propose to untangle crowds and backgrounds from crowd
images and design fine-grained domain adaption methods for crowd counting.
Different from other tasks which possess region-based fine-grained annotations
(e.g., segments or bounding boxes), crowd counting only annotates one point on
each human head, which impedes the implementation of fine-grained adaptation
methods. To tackle this issue, we propose a novel and effective schema to learn
crowd segmentation from point-level crowd counting annotations in the context
of Multiple Instance Learning. We further leverage the derived segments to
propose a crowd-aware fine-grained domain adaptation framework for crowd
counting, which consists of two novel adaptation modules, i.e., Crowd Region
Transfer (CRT) and Crowd Density Alignment (CDA). Specifically, the CRT module
is designed to guide crowd features transfer across domains beyond background
distractions, and the CDA module dedicates to constraining the target-domain
crowd density distributions. Extensive experiments on multiple cross-domain
settings (i.e., Synthetic $\rightarrow$ Real, Fixed $\rightarrow$ Fickle,
Normal $\rightarrow$ BadWeather) demonstrate the superiority of the proposed
method compared with state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yongtuo Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1"&gt;Dan Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1"&gt;Sucheng Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1"&gt;Hanjie Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1"&gt;Hongmin Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1"&gt;Shengfeng He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient and Generic Interactive Segmentation Framework to Correct Mispredictions during Clinical Evaluation of Medical Images. (arXiv:2108.02996v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02996</id>
        <link href="http://arxiv.org/abs/2108.02996"/>
        <updated>2021-08-09T00:49:26.910Z</updated>
        <summary type="html"><![CDATA[Semantic segmentation of medical images is an essential first step in
computer-aided diagnosis systems for many applications. However, given many
disparate imaging modalities and inherent variations in the patient data, it is
difficult to consistently achieve high accuracy using modern deep neural
networks (DNNs). This has led researchers to propose interactive image
segmentation techniques where a medical expert can interactively correct the
output of a DNN to the desired accuracy. However, these techniques often need
separate training data with the associated human interactions, and do not
generalize to various diseases, and types of medical images. In this paper, we
suggest a novel conditional inference technique for DNNs which takes the
intervention by a medical expert as test time constraints and performs
inference conditioned upon these constraints. Our technique is generic can be
used for medical images from any modality. Unlike other methods, our approach
can correct multiple structures simultaneously and add structures missed at
initial segmentation. We report an improvement of 13.3, 12.5, 17.8, 10.2, and
12.4 times in user annotation time than full human annotation for the nucleus,
multiple cells, liver and tumor, organ, and brain segmentation respectively. We
report a time saving of 2.8, 3.0, 1.9, 4.4, and 8.6 fold compared to other
interactive segmentation techniques. Our method can be useful to clinicians for
diagnosis and post-surgical follow-up with minimal intervention from the
medical expert. The source-code and the detailed results are available here
[1].]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sambaturu_B/0/1/0/all/0/1"&gt;Bhavani Sambaturu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1"&gt;Ashutosh Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jawahar_C/0/1/0/all/0/1"&gt;C.V. Jawahar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arora_C/0/1/0/all/0/1"&gt;Chetan Arora&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluating CLIP: Towards Characterization of Broader Capabilities and Downstream Implications. (arXiv:2108.02818v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02818</id>
        <link href="http://arxiv.org/abs/2108.02818"/>
        <updated>2021-08-09T00:49:26.901Z</updated>
        <summary type="html"><![CDATA[Recently, there have been breakthroughs in computer vision ("CV") models that
are more generalizable with the advent of models such as CLIP and ALIGN. In
this paper, we analyze CLIP and highlight some of the challenges such models
pose. CLIP reduces the need for task specific training data, potentially
opening up many niche tasks to automation. CLIP also allows its users to
flexibly specify image classification classes in natural language, which we
find can shift how biases manifest. Additionally, through some preliminary
probes we find that CLIP can inherit biases found in prior computer vision
systems. Given the wide and unpredictable domain of uses for such models, this
raises questions regarding what sufficiently safe behaviour for such systems
may look like. These results add evidence to the growing body of work calling
for a change in the notion of a 'better' model--to move beyond simply looking
at higher accuracy at task-oriented capability evaluations, and towards a
broader 'better' that takes into account deployment-critical features such as
different use contexts, and people who interact with the model when thinking
about model deployment.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1"&gt;Sandhini Agarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krueger_G/0/1/0/all/0/1"&gt;Gretchen Krueger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Clark_J/0/1/0/all/0/1"&gt;Jack Clark&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Radford_A/0/1/0/all/0/1"&gt;Alec Radford&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1"&gt;Jong Wook Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brundage_M/0/1/0/all/0/1"&gt;Miles Brundage&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VinaFood21: A Novel Dataset for Evaluating Vietnamese Food Recognition. (arXiv:2108.02929v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02929</id>
        <link href="http://arxiv.org/abs/2108.02929"/>
        <updated>2021-08-09T00:49:26.874Z</updated>
        <summary type="html"><![CDATA[Vietnam is such an attractive tourist destination with its stunning and
pristine landscapes and its top-rated unique food and drink. Among thousands of
Vietnamese dishes, foreigners and native people are interested in easy-to-eat
tastes and easy-to-do recipes, along with reasonable prices, mouthwatering
flavors, and popularity. Due to the diversity and almost all the dishes have
significant similarities and the lack of quality Vietnamese food datasets, it
is hard to implement an auto system to classify Vietnamese food, therefore,
make people easier to discover Vietnamese food. This paper introduces a new
Vietnamese food dataset named VinaFood21, which consists of 13,950 images
corresponding to 21 dishes. We use 10,044 images for model training and 6,682
test images to classify each food in the VinaFood21 dataset and achieved an
average accuracy of 74.81% when fine-tuning CNN EfficientNet-B0.
(https://github.com/nguyenvd-uit/uit-together-dataset)]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1"&gt;Thuan Trong Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1"&gt;Thuan Q. Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vo_D/0/1/0/all/0/1"&gt;Dung Vo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_V/0/1/0/all/0/1"&gt;Vi Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ho_N/0/1/0/all/0/1"&gt;Ngoc Ho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vo_N/0/1/0/all/0/1"&gt;Nguyen D. Vo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1"&gt;Kiet Van Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1"&gt;Khang Nguyen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bias Out-of-the-Box: An Empirical Analysis of Intersectional Occupational Biases in Popular Generative Language Models. (arXiv:2102.04130v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.04130</id>
        <link href="http://arxiv.org/abs/2102.04130"/>
        <updated>2021-08-09T00:49:26.852Z</updated>
        <summary type="html"><![CDATA[The capabilities of natural language models trained on large-scale data have
increased immensely over the past few years. Open source libraries such as
HuggingFace have made these models easily available and accessible. While prior
research has identified biases in large language models, this paper considers
biases contained in the most popular versions of these models when applied
`out-of-the-box' for downstream tasks. We focus on generative language models
as they are well-suited for extracting biases inherited from training data.
Specifically, we conduct an in-depth analysis of GPT-2, which is the most
downloaded text generation model on HuggingFace, with over half a million
downloads in the past month alone. We assess biases related to occupational
associations for different protected categories by intersecting gender with
religion, sexuality, ethnicity, political affiliation, and continental name
origin. Using a template-based data collection pipeline, we collect 396K
sentence completions made by GPT-2 and find: (i) The machine-predicted jobs are
less diverse and more stereotypical for women than for men, especially for
intersections; (ii) Intersectional interactions are highly relevant for
occupational associations, which we quantify by fitting 262 logistic models;
(iii) For most occupations, GPT-2 reflects the skewed gender and ethnicity
distribution found in US Labour Bureau data, and even pulls the
societally-skewed distribution towards gender parity in cases where its
predictions deviate from real labor market observations. This raises the
normative question of what language models _should_ learn - whether they should
reflect or correct for existing inequalities.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kirk_H/0/1/0/all/0/1"&gt;Hannah Kirk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jun_Y/0/1/0/all/0/1"&gt;Yennie Jun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iqbal_H/0/1/0/all/0/1"&gt;Haider Iqbal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Benussi_E/0/1/0/all/0/1"&gt;Elias Benussi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Volpin_F/0/1/0/all/0/1"&gt;Filippo Volpin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dreyer_F/0/1/0/all/0/1"&gt;Frederic A. Dreyer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shtedritski_A/0/1/0/all/0/1"&gt;Aleksandar Shtedritski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Asano_Y/0/1/0/all/0/1"&gt;Yuki M. Asano&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[StrucTexT: Structured Text Understanding with Multi-Modal Transformers. (arXiv:2108.02923v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02923</id>
        <link href="http://arxiv.org/abs/2108.02923"/>
        <updated>2021-08-09T00:49:26.844Z</updated>
        <summary type="html"><![CDATA[Structured text understanding on Visually Rich Documents (VRDs) is a crucial
part of Document Intelligence. Due to the complexity of content and layout in
VRDs, structured text understanding has been a challenging task. Most existing
studies decoupled this problem into two sub-tasks: entity labeling and entity
linking, which require an entire understanding of the context of documents at
both token and segment levels. However, little work has been concerned with the
solutions that efficiently extract the structured data from different levels.
This paper proposes a unified framework named StrucTexT, which is flexible and
effective for handling both sub-tasks. Specifically, based on the transformer,
we introduce a segment-token aligned encoder to deal with the entity labeling
and entity linking tasks at different levels of granularity. Moreover, we
design a novel pre-training strategy with three self-supervised tasks to learn
a richer representation. StrucTexT uses the existing Masked Visual Language
Modeling task and the new Sentence Length Prediction and Paired Boxes Direction
tasks to incorporate the multi-modal information across text, image, and
layout. We evaluate our method for structured text understanding at
segment-level and token-level and show it outperforms the state-of-the-art
counterparts with significantly superior performance on the FUNSD, SROIE, and
EPHOIE datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yulin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1"&gt;Yuxi Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1"&gt;Yuchen Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_X/0/1/0/all/0/1"&gt;Xiameng Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chengquan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_K/0/1/0/all/0/1"&gt;Kun Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1"&gt;Junyu Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jingtuo Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_E/0/1/0/all/0/1"&gt;Errui Ding&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Elaborative Rehearsal for Zero-shot Action Recognition. (arXiv:2108.02833v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02833</id>
        <link href="http://arxiv.org/abs/2108.02833"/>
        <updated>2021-08-09T00:49:26.836Z</updated>
        <summary type="html"><![CDATA[The growing number of action classes has posed a new challenge for video
understanding, making Zero-Shot Action Recognition (ZSAR) a thriving direction.
The ZSAR task aims to recognize target (unseen) actions without training
examples by leveraging semantic representations to bridge seen and unseen
actions. However, due to the complexity and diversity of actions, it remains
challenging to semantically represent action classes and transfer knowledge
from seen data. In this work, we propose an ER-enhanced ZSAR model inspired by
an effective human memory technique Elaborative Rehearsal (ER), which involves
elaborating a new concept and relating it to known concepts. Specifically, we
expand each action class as an Elaborative Description (ED) sentence, which is
more discriminative than a class name and less costly than manual-defined
attributes. Besides directly aligning class semantics with videos, we
incorporate objects from the video as Elaborative Concepts (EC) to improve
video semantics and generalization from seen actions to unseen actions. Our
ER-enhanced ZSAR model achieves state-of-the-art results on three existing
benchmarks. Moreover, we propose a new ZSAR evaluation protocol on the Kinetics
dataset to overcome limitations of current benchmarks and demonstrate the first
case where ZSAR performance is comparable to few-shot learning baselines on
this more realistic setting. We will release our codes and collected EDs at
https://github.com/DeLightCMU/ElaborativeRehearsal.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Shizhe Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1"&gt;Dong Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Bi-encoder Document Ranking Models with Two Rankers and Multi-teacher Distillation. (arXiv:2103.06523v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.06523</id>
        <link href="http://arxiv.org/abs/2103.06523"/>
        <updated>2021-08-09T00:49:26.829Z</updated>
        <summary type="html"><![CDATA[BERT-based Neural Ranking Models (NRMs) can be classified according to how
the query and document are encoded through BERT's self-attention layers -
bi-encoder versus cross-encoder. Bi-encoder models are highly efficient because
all the documents can be pre-processed before the query time, but their
performance is inferior compared to cross-encoder models. Both models utilize a
ranker that receives BERT representations as the input and generates a
relevance score as the output. In this work, we propose a method where
multi-teacher distillation is applied to a cross-encoder NRM and a bi-encoder
NRM to produce a bi-encoder NRM with two rankers. The resulting student
bi-encoder achieves an improved performance by simultaneously learning from a
cross-encoder teacher and a bi-encoder teacher and also by combining relevance
scores from the two rankers. We call this method TRMD (Two Rankers and
Multi-teacher Distillation). In the experiments, TwinBERT and ColBERT are
considered as baseline bi-encoders. When monoBERT is used as the cross-encoder
teacher, together with either TwinBERT or ColBERT as the bi-encoder teacher,
TRMD produces a student bi-encoder that performs better than the corresponding
baseline bi-encoder. For P@20, the maximum improvement was 11.4%, and the
average improvement was 6.8%. As an additional experiment, we considered
producing cross-encoder students with TRMD, and found that it could also
improve the cross-encoders.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1"&gt;Jaekeol Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jung_E/0/1/0/all/0/1"&gt;Euna Jung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suh_J/0/1/0/all/0/1"&gt;Jangwon Suh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rhee_W/0/1/0/all/0/1"&gt;Wonjong Rhee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluating Adversarial Attacks on Driving Safety in Vision-Based Autonomous Vehicles. (arXiv:2108.02940v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02940</id>
        <link href="http://arxiv.org/abs/2108.02940"/>
        <updated>2021-08-09T00:49:26.809Z</updated>
        <summary type="html"><![CDATA[In recent years, many deep learning models have been adopted in autonomous
driving. At the same time, these models introduce new vulnerabilities that may
compromise the safety of autonomous vehicles. Specifically, recent studies have
demonstrated that adversarial attacks can cause a significant decline in
detection precision of deep learning-based 3D object detection models. Although
driving safety is the ultimate concern for autonomous driving, there is no
comprehensive study on the linkage between the performance of deep learning
models and the driving safety of autonomous vehicles under adversarial attacks.
In this paper, we investigate the impact of two primary types of adversarial
attacks, perturbation attacks and patch attacks, on the driving safety of
vision-based autonomous vehicles rather than the detection precision of deep
learning models. In particular, we consider two state-of-the-art models in
vision-based 3D object detection, Stereo R-CNN and DSGN. To evaluate driving
safety, we propose an end-to-end evaluation framework with a set of driving
safety performance metrics. By analyzing the results of our extensive
evaluation experiments, we find that (1) the attack's impact on the driving
safety of autonomous vehicles and the attack's impact on the precision of 3D
object detectors are decoupled, and (2) the DSGN model demonstrates stronger
robustness to adversarial attacks than the Stereo R-CNN model. In addition, we
further investigate the causes behind the two findings with an ablation study.
The findings of this paper provide a new perspective to evaluate adversarial
attacks and guide the selection of deep learning models in autonomous driving.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jindi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lou_Y/0/1/0/all/0/1"&gt;Yang Lou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jianping Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1"&gt;Kui Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_K/0/1/0/all/0/1"&gt;Kejie Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1"&gt;Xiaohua Jia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Out-of-domain Generalization from a Single Source: A Uncertainty Quantification Approach. (arXiv:2108.02888v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02888</id>
        <link href="http://arxiv.org/abs/2108.02888"/>
        <updated>2021-08-09T00:49:26.797Z</updated>
        <summary type="html"><![CDATA[We study a worst-case scenario in generalization: Out-of-domain
generalization from a single source. The goal is to learn a robust model from a
single source and expect it to generalize over many unknown distributions. This
challenging problem has been seldom investigated while existing solutions
suffer from various limitations such as the ignorance of uncertainty assessment
and label augmentation. In this paper, we propose uncertainty-guided domain
generalization to tackle the aforementioned limitations. The key idea is to
augment the source capacity in both feature and label spaces, while the
augmentation is guided by uncertainty assessment. To the best of our knowledge,
this is the first work to (1) quantify the generalization uncertainty from a
single source and (2) leverage it to guide both feature and label augmentation
for robust generalization. The model training and deployment are effectively
organized in a Bayesian meta-learning framework. We conduct extensive
comparisons and ablation study to validate our approach. The results prove our
superior performance in a wide scope of tasks including image classification,
semantic segmentation, text classification, and speech recognition.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1"&gt;Xi Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiao_F/0/1/0/all/0/1"&gt;Fengchun Qiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1"&gt;Long Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Basis Scaling and Double Pruning for Efficient Transfer Learning. (arXiv:2108.02893v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02893</id>
        <link href="http://arxiv.org/abs/2108.02893"/>
        <updated>2021-08-09T00:49:26.790Z</updated>
        <summary type="html"><![CDATA[Transfer learning allows the reuse of deep learning features on new datasets
with limited data. However, the resulting models could be unnecessarily large
and thus inefficient. Although network pruning can be applied to improve
inference efficiency, existing algorithms usually require fine-tuning and may
not be suitable for small datasets. In this paper, we propose an algorithm that
transforms the convolutional weights into the subspaces of orthonormal bases
where a model is pruned. Using singular value decomposition, we decompose a
convolutional layer into two layers: a convolutional layer with the orthonormal
basis vectors as the filters, and a layer that we name "BasisScalingConv",
which is responsible for rescaling the features and transforming them back to
the original space. As the filters in each transformed layer are linearly
independent with known relative importance, pruning can be more effective and
stable, and fine tuning individual weights is unnecessary. Furthermore, as the
numbers of input and output channels of the original convolutional layer remain
unchanged, basis pruning is applicable to virtually all network architectures.
Basis pruning can also be combined with existing pruning algorithms for double
pruning to further increase the pruning capability. With less than 1% reduction
in the classification accuracy, we can achieve pruning ratios up to 98.9% in
parameters and 98.6% in FLOPs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wong_K/0/1/0/all/0/1"&gt;Ken C. L. Wong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kashyap_S/0/1/0/all/0/1"&gt;Satyananda Kashyap&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moradi_M/0/1/0/all/0/1"&gt;Mehdi Moradi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DOLG: Single-Stage Image Retrieval with Deep Orthogonal Fusion of Local and Global Features. (arXiv:2108.02927v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02927</id>
        <link href="http://arxiv.org/abs/2108.02927"/>
        <updated>2021-08-09T00:49:26.782Z</updated>
        <summary type="html"><![CDATA[Image Retrieval is a fundamental task of obtaining images similar to the
query one from a database. A common image retrieval practice is to firstly
retrieve candidate images via similarity search using global image features and
then re-rank the candidates by leveraging their local features. Previous
learning-based studies mainly focus on either global or local image
representation learning to tackle the retrieval task. In this paper, we abandon
the two-stage paradigm and seek to design an effective single-stage solution by
integrating local and global information inside images into compact image
representations. Specifically, we propose a Deep Orthogonal Local and Global
(DOLG) information fusion framework for end-to-end image retrieval. It
attentively extracts representative local information with multi-atrous
convolutions and self-attention at first. Components orthogonal to the global
image representation are then extracted from the local information. At last,
the orthogonal components are concatenated with the global representation as a
complementary, and then aggregation is performed to generate the final
representation. The whole framework is end-to-end differentiable and can be
trained with image-level labels. Extensive experimental results validate the
effectiveness of our solution and show that our model achieves state-of-the-art
image retrieval performances on Revisited Oxford and Paris datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1"&gt;Min Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1"&gt;Dongliang He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_M/0/1/0/all/0/1"&gt;Miao Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1"&gt;Baorong Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xue_X/0/1/0/all/0/1"&gt;Xuetong Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1"&gt;Fu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_E/0/1/0/all/0/1"&gt;Errui Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1"&gt;Jizhou Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ada-VSR: Adaptive Video Super-Resolution with Meta-Learning. (arXiv:2108.02832v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2108.02832</id>
        <link href="http://arxiv.org/abs/2108.02832"/>
        <updated>2021-08-09T00:49:26.762Z</updated>
        <summary type="html"><![CDATA[Most of the existing works in supervised spatio-temporal video
super-resolution (STVSR) heavily rely on a large-scale external dataset
consisting of paired low-resolution low-frame rate (LR-LFR)and high-resolution
high-frame-rate (HR-HFR) videos. Despite their remarkable performance, these
methods make a prior assumption that the low-resolution video is obtained by
down-scaling the high-resolution video using a known degradation kernel, which
does not hold in practical settings. Another problem with these methods is that
they cannot exploit instance-specific internal information of video at testing
time. Recently, deep internal learning approaches have gained attention due to
their ability to utilize the instance-specific statistics of a video. However,
these methods have a large inference time as they require thousands of gradient
updates to learn the intrinsic structure of the data. In this work, we
presentAdaptiveVideoSuper-Resolution (Ada-VSR) which leverages external, as
well as internal, information through meta-transfer learning and internal
learning, respectively. Specifically, meta-learning is employed to obtain
adaptive parameters, using a large-scale external dataset, that can adapt
quickly to the novel condition (degradation model) of the given test video
during the internal learning task, thereby exploiting external and internal
information of a video for super-resolution. The model trained using our
approach can quickly adapt to a specific video condition with only a few
gradient updates, which reduces the inference time significantly. Extensive
experiments on standard datasets demonstrate that our method performs favorably
against various state-of-the-art approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Gupta_A/0/1/0/all/0/1"&gt;Akash Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jonnalagedda_P/0/1/0/all/0/1"&gt;Padmaja Jonnalagedda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bhanu_B/0/1/0/all/0/1"&gt;Bir Bhanu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Roy_Chowdhury_A/0/1/0/all/0/1"&gt;Amit K. Roy-Chowdhury&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Disentangled Lifespan Face Synthesis. (arXiv:2108.02874v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02874</id>
        <link href="http://arxiv.org/abs/2108.02874"/>
        <updated>2021-08-09T00:49:26.755Z</updated>
        <summary type="html"><![CDATA[A lifespan face synthesis (LFS) model aims to generate a set of
photo-realistic face images of a person's whole life, given only one snapshot
as reference. The generated face image given a target age code is expected to
be age-sensitive reflected by bio-plausible transformations of shape and
texture, while being identity preserving. This is extremely challenging because
the shape and texture characteristics of a face undergo separate and highly
nonlinear transformations w.r.t. age. Most recent LFS models are based on
generative adversarial networks (GANs) whereby age code conditional
transformations are applied to a latent face representation. They benefit
greatly from the recent advancements of GANs. However, without explicitly
disentangling their latent representations into the texture, shape and identity
factors, they are fundamentally limited in modeling the nonlinear age-related
transformation on texture and shape whilst preserving identity. In this work, a
novel LFS model is proposed to disentangle the key face characteristics
including shape, texture and identity so that the unique shape and texture age
transformations can be modeled effectively. This is achieved by extracting
shape, texture and identity features separately from an encoder. Critically,
two transformation modules, one conditional convolution based and the other
channel attention based, are designed for modeling the nonlinear shape and
texture feature transformations respectively. This is to accommodate their
rather distinct aging processes and ensure that our synthesized images are both
age-sensitive and identity preserving. Extensive experiments show that our LFS
model is clearly superior to the state-of-the-art alternatives. Codes and demo
are available on our project website:
\url{https://senhe.github.io/projects/iccv_2021_lifespan_face}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1"&gt;Sen He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liao_W/0/1/0/all/0/1"&gt;Wentong Liao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1"&gt;Michael Ying Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1"&gt;Yi-Zhe Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rosenhahn_B/0/1/0/all/0/1"&gt;Bodo Rosenhahn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1"&gt;Tao Xiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Efficient Group-based Search Engine Marketing System for E-Commerce. (arXiv:2106.12700v4 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.12700</id>
        <link href="http://arxiv.org/abs/2106.12700"/>
        <updated>2021-08-09T00:49:26.734Z</updated>
        <summary type="html"><![CDATA[With the increasing scale of search engine marketing, designing an efficient
bidding system is becoming paramount for the success of e-commerce companies.
The critical challenges faced by a modern industrial-level bidding system
include: 1. the catalog is enormous, and the relevant bidding features are of
high sparsity; 2. the large volume of bidding requests induces significant
computation burden to both the offline and online serving. Leveraging
extraneous user-item information proves essential to mitigate the sparsity
issue, for which we exploit the natural language signals from the users' query
and the contextual knowledge from the products. In particular, we extract the
vector representations of ads via the Transformer model and leverage their
geometric relation to building collaborative bidding predictions via
clustering. The two-step procedure also significantly reduces the computation
stress of bid evaluation and optimization. In this paper, we introduce the
end-to-end structure of the bidding system for search engine marketing for
Walmart e-commerce, which successfully handles tens of millions of bids each
day. We analyze the online and offline performances of our approach and discuss
how we find it as a production-efficient solution.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jie_C/0/1/0/all/0/1"&gt;Cheng Jie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1"&gt;Da Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zigeng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Lu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1"&gt;Wei Shen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Real-Time Visual Analysis of High-Volume Social Media Posts. (arXiv:2108.03052v1 [cs.HC])]]></title>
        <id>http://arxiv.org/abs/2108.03052</id>
        <link href="http://arxiv.org/abs/2108.03052"/>
        <updated>2021-08-09T00:49:26.725Z</updated>
        <summary type="html"><![CDATA[Breaking news and first-hand reports often trend on social media platforms
before traditional news outlets cover them. The real-time analysis of posts on
such platforms can reveal valuable and timely insights for journalists,
politicians, business analysts, and first responders, but the high number and
diversity of new posts pose a challenge. In this work, we present an
interactive system that enables the visual analysis of streaming social media
data on a large scale in real-time. We propose an efficient and explainable
dynamic clustering algorithm that powers a continuously updated visualization
of the current thematic landscape as well as detailed visual summaries of
specific topics of interest. Our parallel clustering strategy provides an
adaptive stream with a digestible but diverse selection of recent posts related
to relevant topics. We also integrate familiar visual metaphors that are highly
interlinked for enabling both explorative and more focused monitoring tasks.
Analysts can gradually increase the resolution to dive deeper into particular
topics. In contrast to previous work, our system also works with non-geolocated
posts and avoids extensive preprocessing such as detecting events. We evaluated
our dynamic clustering algorithm and discuss several use cases that show the
utility of our system.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Knittel_J/0/1/0/all/0/1"&gt;Johannes Knittel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koch_S/0/1/0/all/0/1"&gt;Steffen Koch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_T/0/1/0/all/0/1"&gt;Tan Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1"&gt;Wei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yingcai Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Shixia Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ertl_T/0/1/0/all/0/1"&gt;Thomas Ertl&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Variational Bandwidth Auto-encoder for Hybrid Recommender Systems. (arXiv:2105.07597v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.07597</id>
        <link href="http://arxiv.org/abs/2105.07597"/>
        <updated>2021-08-09T00:49:26.713Z</updated>
        <summary type="html"><![CDATA[Hybrid recommendations have recently attracted a lot of attention where user
features are utilized as auxiliary information to address the sparsity problem
caused by insufficient user-item interactions. However, extracted user features
generally contain rich multimodal information, and most of them are irrelevant
to the recommendation purpose. Therefore, excessive reliance on these features
will make the model overfit on noise and difficult to generalize. In this
article, we propose a variational bandwidth auto-encoder (VBAE) for
recommendations, aiming to address the sparsity and noise problems
simultaneously. VBAE first encodes user collaborative and feature information
into Gaussian latent variables via deep neural networks to capture non-linear
user similarities. Moreover, by considering the fusion of collaborative and
feature variables as a virtual communication channel from an
information-theoretic perspective, we introduce a user-dependent channel to
dynamically control the information allowed to be accessed from the feature
embeddings. A quantum-inspired uncertainty measurement of the hidden rating
embeddings is proposed accordingly to infer the channel bandwidth by
disentangling the uncertainty information in the ratings from the semantic
information. Through this mechanism, VBAE incorporates adequate auxiliary
information from user features if collaborative information is insufficient,
while avoiding excessive reliance on noisy user features to improve its
generalization ability to new users. Extensive experiments conducted on three
real-world datasets demonstrate the effectiveness of the proposed method. Codes
and datasets are released at https://github.com/yaochenzhu/vbae.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yaochen Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhenzhong Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Data Augmented Approach to Transfer Learning for Covid-19 Detection. (arXiv:2108.02870v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02870</id>
        <link href="http://arxiv.org/abs/2108.02870"/>
        <updated>2021-08-09T00:49:26.704Z</updated>
        <summary type="html"><![CDATA[Covid-19 detection at an early stage can aid in an effective treatment and
isolation plan to prevent its spread. Recently, transfer learning has been used
for Covid-19 detection using X-ray, ultrasound, and CT scans. One of the major
limitations inherent to these proposed methods is limited labeled dataset size
that affects the reliability of Covid-19 diagnosis and disease progression. In
this work, we demonstrate that how we can augment limited X-ray images data by
using Contrast limited adaptive histogram equalization (CLAHE) to train the
last layer of the pre-trained deep learning models to mitigate the bias of
transfer learning for Covid-19 detection. We transfer learned various
pre-trained deep learning models including AlexNet, ZFNet, VGG-16, ResNet-18,
and GoogLeNet, and fine-tune the last layer by using CLAHE-augmented dataset.
The experiment results reveal that the CLAHE-based augmentation to various
pre-trained deep learning models significantly improves the model efficiency.
The pre-trained VCG-16 model with CLAHEbased augmented images achieves a
sensitivity of 95% using 15 epochs. AlexNet works show good sensitivity when
trained on non-augmented data. Other models demonstrate a value of less than
60% when trained on non-augmented data. Our results reveal that the sample bias
can negatively impact the performance of transfer learning which is
significantly improved by using CLAHE-based augmentation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Henna_S/0/1/0/all/0/1"&gt;Shagufta Henna&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reji_A/0/1/0/all/0/1"&gt;Aparna Reji&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A volumetric change detection framework using UAV oblique photogrammetry - A case study of ultra-high-resolution monitoring of progressive building collapse. (arXiv:2108.02800v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02800</id>
        <link href="http://arxiv.org/abs/2108.02800"/>
        <updated>2021-08-09T00:49:26.684Z</updated>
        <summary type="html"><![CDATA[In this paper, we present a case study that performs an unmanned aerial
vehicle (UAV) based fine-scale 3D change detection and monitoring of
progressive collapse performance of a building during a demolition event.
Multi-temporal oblique photogrammetry images are collected with 3D point clouds
generated at different stages of the demolition. The geometric accuracy of the
generated point clouds has been evaluated against both airborne and terrestrial
LiDAR point clouds, achieving an average distance of 12 cm and 16 cm for roof
and facade respectively. We propose a hierarchical volumetric change detection
framework that unifies multi-temporal UAV images for pose estimation (free of
ground control points), reconstruction, and a coarse-to-fine 3D density change
analysis. This work has provided a solution capable of addressing change
detection on full 3D time-series datasets where dramatic scene content changes
are presented progressively. Our change detection results on the building
demolition event have been evaluated against the manually marked ground-truth
changes and have achieved an F-1 score varying from 0.78 to 0.92, with
consistently high precision (0.92 - 0.99). Volumetric changes through the
demolition progress are derived from change detection and have shown to
favorably reflect the qualitative and quantitative building demolition
progression.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_N/0/1/0/all/0/1"&gt;Ningli Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1"&gt;Debao Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1"&gt;Shuang Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ling_X/0/1/0/all/0/1"&gt;Xiao Ling&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Strasbaugh_C/0/1/0/all/0/1"&gt;Chris Strasbaugh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yilmaz_A/0/1/0/all/0/1"&gt;Alper Yilmaz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sezen_H/0/1/0/all/0/1"&gt;Halil Sezen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_R/0/1/0/all/0/1"&gt;Rongjun Qin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Analyzing Information Leakage of Updates to Natural Language Models. (arXiv:1912.07942v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1912.07942</id>
        <link href="http://arxiv.org/abs/1912.07942"/>
        <updated>2021-08-09T00:49:26.673Z</updated>
        <summary type="html"><![CDATA[To continuously improve quality and reflect changes in data, machine learning
applications have to regularly retrain and update their core models. We show
that a differential analysis of language model snapshots before and after an
update can reveal a surprising amount of detailed information about changes in
the training data. We propose two new metrics---\emph{differential score} and
\emph{differential rank}---for analyzing the leakage due to updates of natural
language models. We perform leakage analysis using these metrics across models
trained on several different datasets using different methods and
configurations. We discuss the privacy implications of our findings, propose
mitigation strategies and evaluate their effect.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zanella_Beguelin_S/0/1/0/all/0/1"&gt;Santiago Zanella-B&amp;#xe9;guelin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wutschitz_L/0/1/0/all/0/1"&gt;Lukas Wutschitz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tople_S/0/1/0/all/0/1"&gt;Shruti Tople&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ruhle_V/0/1/0/all/0/1"&gt;Victor R&amp;#xfc;hle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paverd_A/0/1/0/all/0/1"&gt;Andrew Paverd&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ohrimenko_O/0/1/0/all/0/1"&gt;Olga Ohrimenko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kopf_B/0/1/0/all/0/1"&gt;Boris K&amp;#xf6;pf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brockschmidt_M/0/1/0/all/0/1"&gt;Marc Brockschmidt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Supervised Learning from Unlabeled Fundus Photographs Improves Segmentation of the Retina. (arXiv:2108.02798v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02798</id>
        <link href="http://arxiv.org/abs/2108.02798"/>
        <updated>2021-08-09T00:49:26.664Z</updated>
        <summary type="html"><![CDATA[Fundus photography is the primary method for retinal imaging and essential
for diabetic retinopathy prevention. Automated segmentation of fundus
photographs would improve the quality, capacity, and cost-effectiveness of eye
care screening programs. However, current segmentation methods are not robust
towards the diversity in imaging conditions and pathologies typical for
real-world clinical applications. To overcome these limitations, we utilized
contrastive self-supervised learning to exploit the large variety of unlabeled
fundus images in the publicly available EyePACS dataset. We pre-trained an
encoder of a U-Net, which we later fine-tuned on several retinal vessel and
lesion segmentation datasets. We demonstrate for the first time that by using
contrastive self-supervised learning, the pre-trained network can recognize
blood vessels, optic disc, fovea, and various lesions without being provided
any labels. Furthermore, when fine-tuned on a downstream blood vessel
segmentation task, such pre-trained networks achieve state-of-the-art
performance on images from different datasets. Additionally, the pre-training
also leads to shorter training times and an improved few-shot performance on
both blood vessel and lesion segmentation tasks. Altogether, our results
showcase the benefits of contrastive self-supervised pre-training which can
play a crucial role in real-world clinical applications requiring robust models
able to adapt to new devices with only a few annotated samples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kukacka_J/0/1/0/all/0/1"&gt;Jan Kuka&amp;#x10d;ka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zenz_A/0/1/0/all/0/1"&gt;Anja Zenz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kollovieh_M/0/1/0/all/0/1"&gt;Marcel Kollovieh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Justel_D/0/1/0/all/0/1"&gt;Dominik J&amp;#xfc;stel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ntziachristos_V/0/1/0/all/0/1"&gt;Vasilis Ntziachristos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SWSR: A Chinese Dataset and Lexicon for Online Sexism Detection. (arXiv:2108.03070v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.03070</id>
        <link href="http://arxiv.org/abs/2108.03070"/>
        <updated>2021-08-09T00:49:26.638Z</updated>
        <summary type="html"><![CDATA[Online sexism has become an increasing concern in social media platforms as
it has affected the healthy development of the Internet and can have negative
effects in society. While research in the sexism detection domain is growing,
most of this research focuses on English as the language and on Twitter as the
platform. Our objective here is to broaden the scope of this research by
considering the Chinese language on Sina Weibo. We propose the first Chinese
sexism dataset -- Sina Weibo Sexism Review (SWSR) dataset --, as well as a
large Chinese lexicon SexHateLex made of abusive and gender-related terms. We
introduce our data collection and annotation process, and provide an
exploratory analysis of the dataset characteristics to validate its quality and
to show how sexism is manifested in Chinese. The SWSR dataset provides labels
at different levels of granularity including (i) sexism or non-sexism, (ii)
sexism category and (iii) target type, which can be exploited, among others,
for building computational methods to identify and investigate finer-grained
gender-related abusive language. We conduct experiments for the three sexism
classification tasks making use of state-of-the-art machine learning models.
Our results show competitive performance, providing a benchmark for sexism
detection in the Chinese language, as well as an error analysis highlighting
open challenges needing more research in Chinese NLP. The SWSR dataset and
SexHateLex lexicon are publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_A/0/1/0/all/0/1"&gt;Aiqi Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xiaohan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zubiaga_A/0/1/0/all/0/1"&gt;Arkaitz Zubiaga&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cross-lingual Capsule Network for Hate Speech Detection in Social Media. (arXiv:2108.03089v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.03089</id>
        <link href="http://arxiv.org/abs/2108.03089"/>
        <updated>2021-08-09T00:49:26.629Z</updated>
        <summary type="html"><![CDATA[Most hate speech detection research focuses on a single language, generally
English, which limits their generalisability to other languages. In this paper
we investigate the cross-lingual hate speech detection task, tackling the
problem by adapting the hate speech resources from one language to another. We
propose a cross-lingual capsule network learning model coupled with extra
domain-specific lexical semantics for hate speech (CCNL-Ex). Our model achieves
state-of-the-art performance on benchmark datasets from AMI@Evalita2018 and
AMI@Ibereval2018 involving three languages: English, Spanish and Italian,
outperforming state-of-the-art baselines on all six language pairs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_A/0/1/0/all/0/1"&gt;Aiqi Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zubiaga_A/0/1/0/all/0/1"&gt;Arkaitz Zubiaga&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Communicative Learning with Natural Gestures for Embodied Navigation Agents with Human-in-the-Scene. (arXiv:2108.02846v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2108.02846</id>
        <link href="http://arxiv.org/abs/2108.02846"/>
        <updated>2021-08-09T00:49:26.621Z</updated>
        <summary type="html"><![CDATA[Human-robot collaboration is an essential research topic in artificial
intelligence (AI), enabling researchers to devise cognitive AI systems and
affords an intuitive means for users to interact with the robot. Of note,
communication plays a central role. To date, prior studies in embodied agent
navigation have only demonstrated that human languages facilitate communication
by instructions in natural languages. Nevertheless, a plethora of other forms
of communication is left unexplored. In fact, human communication originated in
gestures and oftentimes is delivered through multimodal cues, e.g. "go there"
with a pointing gesture. To bridge the gap and fill in the missing dimension of
communication in embodied agent navigation, we propose investigating the
effects of using gestures as the communicative interface instead of verbal
cues. Specifically, we develop a VR-based 3D simulation environment, named
Ges-THOR, based on AI2-THOR platform. In this virtual environment, a human
player is placed in the same virtual scene and shepherds the artificial agent
using only gestures. The agent is tasked to solve the navigation problem guided
by natural gestures with unknown semantics; we do not use any predefined
gestures due to the diversity and versatile nature of human gestures. We argue
that learning the semantics of natural gestures is mutually beneficial to
learning the navigation task--learn to communicate and communicate to learn. In
a series of experiments, we demonstrate that human gesture cues, even without
predefined semantics, improve the object-goal navigation for an embodied agent,
outperforming various state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1"&gt;Qi Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1"&gt;Cheng-Ju Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yixin Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joo_J/0/1/0/all/0/1"&gt;Jungseock Joo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Twins Talk & Alternative Calculations. (arXiv:2108.02807v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02807</id>
        <link href="http://arxiv.org/abs/2108.02807"/>
        <updated>2021-08-09T00:49:26.609Z</updated>
        <summary type="html"><![CDATA[Inspired by how the human brain employs a higher number of neural pathways
when describing a highly focused subject, we show that deep attentive models
used for the main vision-language task of image captioning, could be extended
to achieve better performance. Image captioning bridges a gap between computer
vision and natural language processing. Automated image captioning is used as a
tool to eliminate the need for human agent for creating descriptive captions
for unseen images.Automated image captioning is challenging and yet
interesting. One reason is that AI based systems capable of generating
sentences that describe an input image could be used in a wide variety of tasks
beyond generating captions for unseen images found on web or uploaded to social
media. For example, in biology and medical sciences, these systems could
provide researchers and physicians with a brief linguistic description of
relevant images, potentially expediting their work.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zohourianshahzadi_Z/0/1/0/all/0/1"&gt;Zanyar Zohourianshahzadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kalita_J/0/1/0/all/0/1"&gt;Jugal K. Kalita&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LadRa-Net: Locally-Aware Dynamic Re-read Attention Net for Sentence Semantic Matching. (arXiv:2108.02915v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.02915</id>
        <link href="http://arxiv.org/abs/2108.02915"/>
        <updated>2021-08-09T00:49:26.493Z</updated>
        <summary type="html"><![CDATA[Sentence semantic matching requires an agent to determine the semantic
relation between two sentences, which is widely used in various natural
language tasks, such as Natural Language Inference (NLI), Paraphrase
Identification (PI), and so on. Much recent progress has been made in this
area, especially attention-based methods and pre-trained language model based
methods. However, most of these methods focus on all the important parts in
sentences in a static way and only emphasize how important the words are to the
query, inhibiting the ability of attention mechanism. In order to overcome this
problem and boost the performance of attention mechanism, we propose a novel
dynamic re-read attention, which can pay close attention to one small region of
sentences at each step and re-read the important parts for better sentence
representations. Based on this attention variation, we develop a novel Dynamic
Re-read Network (DRr-Net) for sentence semantic matching. Moreover, selecting
one small region in dynamic re-read attention seems insufficient for sentence
semantics, and employing pre-trained language models as input encoders will
introduce incomplete and fragile representation problems. To this end, we
extend DRrNet to Locally-Aware Dynamic Re-read Attention Net (LadRa-Net), in
which local structure of sentences is employed to alleviate the shortcoming of
Byte-Pair Encoding (BPE) in pre-trained language models and boost the
performance of dynamic reread attention. Extensive experiments on two popular
sentence semantic matching tasks demonstrate that DRr-Net can significantly
improve the performance of sentence semantic matching. Meanwhile, LadRa-Net is
able to achieve better performance by considering the local structures of
sentences. In addition, it is exceedingly interesting that some discoveries in
our experiments are consistent with some findings of psychological research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1"&gt;Kun Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lv_G/0/1/0/all/0/1"&gt;Guangyi Lv&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1"&gt;Le Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_E/0/1/0/all/0/1"&gt;Enhong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1"&gt;Qi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1"&gt;Meng Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evolution of emotion semantics. (arXiv:2108.02887v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.02887</id>
        <link href="http://arxiv.org/abs/2108.02887"/>
        <updated>2021-08-09T00:49:26.483Z</updated>
        <summary type="html"><![CDATA[Humans possess the unique ability to communicate emotions through language.
Although concepts like anger or awe are abstract, there is a shared consensus
about what these English emotion words mean. This consensus may give the
impression that their meaning is static, but we propose this is not the case.
We cannot travel back to earlier periods to study emotion concepts directly,
but we can examine text corpora, which have partially preserved the meaning of
emotion words. Using natural language processing of historical text, we found
evidence for semantic change in emotion words over the past century and that
varying rates of change were predicted in part by an emotion concept's
prototypicality - how representative it is of the broader category of
"emotion". Prototypicality negatively correlated with historical rates of
emotion semantic change obtained from text-based word embeddings, beyond more
established variables including usage frequency in English and a second
comparison language, French. This effect for prototypicality did not
consistently extend to the semantic category of birds, suggesting its relevance
for predicting semantic change may be category-dependent. Our results suggest
emotion semantics are evolving over time, with prototypical emotion words
remaining semantically stable, while other emotion words evolve more freely.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_A/0/1/0/all/0/1"&gt;Aotao Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stellar_J/0/1/0/all/0/1"&gt;Jennifer E. Stellar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yang Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Analyzing the Abstractiveness-Factuality Tradeoff With Nonlinear Abstractiveness Constraints. (arXiv:2108.02859v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.02859</id>
        <link href="http://arxiv.org/abs/2108.02859"/>
        <updated>2021-08-09T00:49:26.458Z</updated>
        <summary type="html"><![CDATA[We analyze the tradeoff between factuality and abstractiveness of summaries.
We introduce abstractiveness constraints to control the degree of
abstractiveness at decoding time, and we apply this technique to characterize
the abstractiveness-factuality tradeoff across multiple widely-studied
datasets, using extensive human evaluations. We train a neural summarization
model on each dataset and visualize the rates of change in factuality as we
gradually increase abstractiveness using our abstractiveness constraints. We
observe that, while factuality generally drops with increased abstractiveness,
different datasets lead to different rates of factuality decay. We propose new
measures to quantify the tradeoff between factuality and abstractiveness, incl.
muQAGS, which balances factuality with abstractiveness. We also quantify this
tradeoff in previous works, aiming to establish baselines for the
abstractiveness-factuality tradeoff that future publications can compare
against.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dreyer_M/0/1/0/all/0/1"&gt;Markus Dreyer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1"&gt;Mengwen Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nan_F/0/1/0/all/0/1"&gt;Feng Nan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Atluri_S/0/1/0/all/0/1"&gt;Sandeep Atluri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ravi_S/0/1/0/all/0/1"&gt;Sujith Ravi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dual Reader-Parser on Hybrid Textual and Tabular Evidence for Open Domain Question Answering. (arXiv:2108.02866v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.02866</id>
        <link href="http://arxiv.org/abs/2108.02866"/>
        <updated>2021-08-09T00:49:26.438Z</updated>
        <summary type="html"><![CDATA[The current state-of-the-art generative models for open-domain question
answering (ODQA) have focused on generating direct answers from unstructured
textual information. However, a large amount of world's knowledge is stored in
structured databases, and need to be accessed using query languages such as
SQL. Furthermore, query languages can answer questions that require complex
reasoning, as well as offering full explainability. In this paper, we propose a
hybrid framework that takes both textual and tabular evidence as input and
generates either direct answers or SQL queries depending on which form could
better answer the question. The generated SQL queries can then be executed on
the associated databases to obtain the final answers. To the best of our
knowledge, this is the first paper that applies Text2SQL to ODQA tasks.
Empirically, we demonstrate that on several ODQA datasets, the hybrid methods
consistently outperforms the baseline models that only take homogeneous input
by a large margin. Specifically we achieve state-of-the-art performance on
OpenSQuAD dataset using a T5-base model. In a detailed analysis, we demonstrate
that the being able to generate structural SQL queries can always bring gains,
especially for those questions that requires complex reasoning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1"&gt;Alexander Hanbo Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ng_P/0/1/0/all/0/1"&gt;Patrick Ng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1"&gt;Peng Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1"&gt;Henghui Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhiguo Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiang_B/0/1/0/all/0/1"&gt;Bing Xiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deriving Disinformation Insights from Geolocalized Twitter Callouts. (arXiv:2108.03067v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.03067</id>
        <link href="http://arxiv.org/abs/2108.03067"/>
        <updated>2021-08-09T00:49:26.427Z</updated>
        <summary type="html"><![CDATA[This paper demonstrates a two-stage method for deriving insights from social
media data relating to disinformation by applying a combination of geospatial
classification and embedding-based language modelling across multiple
languages. In particular, the analysis in centered on Twitter and
disinformation for three European languages: English, French and Spanish.
Firstly, Twitter data is classified into European and non-European sets using
BERT. Secondly, Word2vec is applied to the classified texts resulting in
Eurocentric, non-Eurocentric and global representations of the data for the
three target languages. This comparative analysis demonstrates not only the
efficacy of the classification method but also highlights geographic, temporal
and linguistic differences in the disinformation-related media. Thus, the
contributions of the work are threefold: (i) a novel language-independent
transformer-based geolocation method; (ii) an analytical approach that exploits
lexical specificity and word embeddings to interrogate user-generated content;
and (iii) a dataset of 36 million disinformation related tweets in English,
French and Spanish.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tuxworth_D/0/1/0/all/0/1"&gt;David Tuxworth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Antypas_D/0/1/0/all/0/1"&gt;Dimosthenis Antypas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Espinosa_Anke_L/0/1/0/all/0/1"&gt;Luis Espinosa-Anke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Camacho_Collados_J/0/1/0/all/0/1"&gt;Jose Camacho-Collados&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Preece_A/0/1/0/all/0/1"&gt;Alun Preece&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rogers_D/0/1/0/all/0/1"&gt;David Rogers&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tell me a story about yourself: The words of shopping experience and self-satisfaction. (arXiv:2108.03016v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.03016</id>
        <link href="http://arxiv.org/abs/2108.03016"/>
        <updated>2021-08-09T00:49:26.413Z</updated>
        <summary type="html"><![CDATA[In this paper we investigate the verbal expression of shopping experience
obtained by a sample of customers asked to freely verbalize how they felt when
entering a store. Using novel tools of Text Mining and Social Network Analysis,
we analyzed the interviews to understand the connection between the emotions
aroused during the shopping experience, satisfaction and the way participants
link these concepts to self-satisfaction and self-identity. The results show a
prominent role of emotions in the discourse about the shopping experience
before purchasing and an inward-looking connection to the self. Our results
also suggest that modern retail environment should enhance the hedonic shopping
experience in terms of fun, fantasy, moods, and emotions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Petruzzellis_L/0/1/0/all/0/1"&gt;L Petruzzellis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Colladon_A/0/1/0/all/0/1"&gt;A Fronzetti Colladon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Visentin_M/0/1/0/all/0/1"&gt;M Visentin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chebat_J/0/1/0/all/0/1"&gt;J.-C. Chebat&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lights, Camera, Action! A Framework to Improve NLP Accuracy over OCR documents. (arXiv:2108.02899v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.02899</id>
        <link href="http://arxiv.org/abs/2108.02899"/>
        <updated>2021-08-09T00:49:26.398Z</updated>
        <summary type="html"><![CDATA[Document digitization is essential for the digital transformation of our
societies, yet a crucial step in the process, Optical Character Recognition
(OCR), is still not perfect. Even commercial OCR systems can produce
questionable output depending on the fidelity of the scanned documents. In this
paper, we demonstrate an effective framework for mitigating OCR errors for any
downstream NLP task, using Named Entity Recognition (NER) as an example. We
first address the data scarcity problem for model training by constructing a
document synthesis pipeline, generating realistic but degraded data with NER
labels. We measure the NER accuracy drop at various degradation levels and show
that a text restoration model, trained on the degraded data, significantly
closes the NER accuracy gaps caused by OCR errors, including on an
out-of-domain dataset. For the benefit of the community, we have made the
document synthesis pipeline available as an open-source project.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gupte_A/0/1/0/all/0/1"&gt;Amit Gupte&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Romanov_A/0/1/0/all/0/1"&gt;Alexey Romanov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mantravadi_S/0/1/0/all/0/1"&gt;Sahitya Mantravadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Banda_D/0/1/0/all/0/1"&gt;Dalitso Banda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jianjie Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khan_R/0/1/0/all/0/1"&gt;Raza Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meenal_L/0/1/0/all/0/1"&gt;Lakshmanan Ramu Meenal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1"&gt;Benjamin Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srinivasan_S/0/1/0/all/0/1"&gt;Soundar Srinivasan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[StrucTexT: Structured Text Understanding with Multi-Modal Transformers. (arXiv:2108.02923v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02923</id>
        <link href="http://arxiv.org/abs/2108.02923"/>
        <updated>2021-08-09T00:49:26.382Z</updated>
        <summary type="html"><![CDATA[Structured text understanding on Visually Rich Documents (VRDs) is a crucial
part of Document Intelligence. Due to the complexity of content and layout in
VRDs, structured text understanding has been a challenging task. Most existing
studies decoupled this problem into two sub-tasks: entity labeling and entity
linking, which require an entire understanding of the context of documents at
both token and segment levels. However, little work has been concerned with the
solutions that efficiently extract the structured data from different levels.
This paper proposes a unified framework named StrucTexT, which is flexible and
effective for handling both sub-tasks. Specifically, based on the transformer,
we introduce a segment-token aligned encoder to deal with the entity labeling
and entity linking tasks at different levels of granularity. Moreover, we
design a novel pre-training strategy with three self-supervised tasks to learn
a richer representation. StrucTexT uses the existing Masked Visual Language
Modeling task and the new Sentence Length Prediction and Paired Boxes Direction
tasks to incorporate the multi-modal information across text, image, and
layout. We evaluate our method for structured text understanding at
segment-level and token-level and show it outperforms the state-of-the-art
counterparts with significantly superior performance on the FUNSD, SROIE, and
EPHOIE datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yulin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1"&gt;Yuxi Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1"&gt;Yuchen Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_X/0/1/0/all/0/1"&gt;Xiameng Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chengquan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_K/0/1/0/all/0/1"&gt;Kun Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1"&gt;Junyu Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jingtuo Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_E/0/1/0/all/0/1"&gt;Errui Ding&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Is it Fake? News Disinformation Detection on South African News Websites. (arXiv:2108.02941v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.02941</id>
        <link href="http://arxiv.org/abs/2108.02941"/>
        <updated>2021-08-09T00:49:26.371Z</updated>
        <summary type="html"><![CDATA[Disinformation through fake news is an ongoing problem in our society and has
become easily spread through social media. The most cost and time effective way
to filter these large amounts of data is to use a combination of human and
technical interventions to identify it. From a technical perspective, Natural
Language Processing (NLP) is widely used in detecting fake news. Social media
companies use NLP techniques to identify the fake news and warn their users,
but fake news may still slip through undetected. It is especially a problem in
more localised contexts (outside the United States of America). How do we
adjust fake news detection systems to work better for local contexts such as in
South Africa. In this work we investigate fake news detection on South African
websites. We curate a dataset of South African fake news and then train
detection models. We contrast this with using widely available fake news
datasets (from mostly USA website). We also explore making the datasets more
diverse by combining them and observe the differences in behaviour in writing
between nations' fake news using interpretable machine learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wet_H/0/1/0/all/0/1"&gt;Harm de Wet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marivate_V/0/1/0/all/0/1"&gt;Vukosi Marivate&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sentence Semantic Regression for Text Generation. (arXiv:2108.02984v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.02984</id>
        <link href="http://arxiv.org/abs/2108.02984"/>
        <updated>2021-08-09T00:49:26.323Z</updated>
        <summary type="html"><![CDATA[Recall the classical text generation works, the generation framework can be
briefly divided into two phases: \textbf{idea reasoning} and \textbf{surface
realization}. The target of idea reasoning is to figure out the main idea which
will be presented in the following talking/writing periods. Surface realization
aims to arrange the most appropriate sentence to depict and convey the
information distilled from the main idea. However, the current popular
token-by-token text generation methods ignore this crucial process and suffer
from many serious issues, such as idea/topic drift. To tackle the problems and
realize this two-phase paradigm, we propose a new framework named Sentence
Semantic Regression (\textbf{SSR}) based on sentence-level language modeling.
For idea reasoning, two architectures \textbf{SSR-AR} and \textbf{SSR-NonAR}
are designed to conduct sentence semantic regression autoregressively (like
GPT2/3) and bidirectionally (like BERT). In the phase of surface realization, a
mixed-granularity sentence decoder is designed to generate text with better
consistency by jointly incorporating the predicted sentence-level main idea as
well as the preceding contextual token-level information. We conduct
experiments on four tasks of story ending prediction, story ending generation,
dialogue generation, and sentence infilling. The results show that SSR can
obtain better performance in terms of automatic metrics and human evaluation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1"&gt;Piji Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1"&gt;Hai-Tao Zheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GENder-IT: An Annotated English-Italian Parallel Challenge Set for Cross-Linguistic Natural Gender Phenomena. (arXiv:2108.02854v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.02854</id>
        <link href="http://arxiv.org/abs/2108.02854"/>
        <updated>2021-08-09T00:49:26.127Z</updated>
        <summary type="html"><![CDATA[Languages differ in terms of the absence or presence of gender features, the
number of gender classes and whether and where gender features are explicitly
marked. These cross-linguistic differences can lead to ambiguities that are
difficult to resolve, especially for sentence-level MT systems. The
identification of ambiguity and its subsequent resolution is a challenging task
for which currently there aren't any specific resources or challenge sets
available. In this paper, we introduce gENder-IT, an English--Italian challenge
set focusing on the resolution of natural gender phenomena by providing
word-level gender tags on the English source side and multiple gender
alternative translations, where needed, on the Italian target side.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vanmassenhove_E/0/1/0/all/0/1"&gt;Eva Vanmassenhove&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Monti_J/0/1/0/all/0/1"&gt;Johanna Monti&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hate Speech Detection in Roman Urdu. (arXiv:2108.02830v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.02830</id>
        <link href="http://arxiv.org/abs/2108.02830"/>
        <updated>2021-08-09T00:49:26.085Z</updated>
        <summary type="html"><![CDATA[Hate speech is a specific type of controversial content that is widely
legislated as a crime that must be identified and blocked. However, due to the
sheer volume and velocity of the Twitter data stream, hate speech detection
cannot be performed manually. To address this issue, several studies have been
conducted for hate speech detection in European languages, whereas little
attention has been paid to low-resource South Asian languages, making the
social media vulnerable for millions of users. In particular, to the best of
our knowledge, no study has been conducted for hate speech detection in Roman
Urdu text, which is widely used in the sub-continent. In this study, we have
scrapped more than 90,000 tweets and manually parsed them to identify 5,000
Roman Urdu tweets. Subsequently, we have employed an iterative approach to
develop guidelines and used them for generating the Hate Speech Roman Urdu 2020
corpus. The tweets in the this corpus are classified at three levels:
Neutral-Hostile, Simple-Complex, and Offensive-Hate speech. As another
contribution, we have used five supervised learning techniques, including a
deep learning technique, to evaluate and compare their effectiveness for hate
speech detection. The results show that Logistic Regression outperformed all
other techniques, including deep learning techniques for the two levels of
classification, by achieved an F1 score of 0.906 for distinguishing between
Neutral-Hostile tweets, and 0.756 for distinguishing between Offensive-Hate
speech tweets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1"&gt;Moin Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shahzad_K/0/1/0/all/0/1"&gt;Khurram Shahzad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Malik_K/0/1/0/all/0/1"&gt;Kamran Malik&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Few-shot Unsupervised Domain Adaptation with Image-to-class Sparse Similarity Encoding. (arXiv:2108.02953v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02953</id>
        <link href="http://arxiv.org/abs/2108.02953"/>
        <updated>2021-08-09T00:49:26.032Z</updated>
        <summary type="html"><![CDATA[This paper investigates a valuable setting called few-shot unsupervised
domain adaptation (FS-UDA), which has not been sufficiently studied in the
literature. In this setting, the source domain data are labelled, but with
few-shot per category, while the target domain data are unlabelled. To address
the FS-UDA setting, we develop a general UDA model to solve the following two
key issues: the few-shot labeled data per category and the domain adaptation
between support and query sets. Our model is general in that once trained it
will be able to be applied to various FS-UDA tasks from the same source and
target domains. Inspired by the recent local descriptor based few-shot learning
(FSL), our general UDA model is fully built upon local descriptors (LDs) for
image classification and domain adaptation. By proposing a novel concept called
similarity patterns (SPs), our model not only effectively considers the spatial
relationship of LDs that was ignored in previous FSL methods, but also makes
the learned image similarity better serve the required domain alignment.
Specifically, we propose a novel IMage-to-class sparse Similarity Encoding
(IMSE) method. It learns SPs to extract the local discriminative information
for classification and meanwhile aligns the covariance matrix of the SPs for
domain adaptation. Also, domain adversarial training and multi-scale local
feature matching are performed upon LDs. Extensive experiments conducted on a
multi-domain benchmark dataset DomainNet demonstrates the state-of-the-art
performance of our IMSE for the novel setting of FS-UDA. In addition, for FSL,
our IMSE can also show better performance than most of recent FSL methods on
miniImageNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1"&gt;Shengqi Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1"&gt;Wanqi Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Lei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1"&gt;Luping Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1"&gt;Ming Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning with Noisy Labels for Robust Point Cloud Segmentation. (arXiv:2107.14230v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.14230</id>
        <link href="http://arxiv.org/abs/2107.14230"/>
        <updated>2021-08-06T01:58:38.210Z</updated>
        <summary type="html"><![CDATA[Point cloud segmentation is a fundamental task in 3D. Despite recent progress
on point cloud segmentation with the power of deep networks, current deep
learning methods based on the clean label assumptions may fail with noisy
labels. Yet, object class labels are often mislabeled in real-world point cloud
datasets. In this work, we take the lead in solving this issue by proposing a
novel Point Noise-Adaptive Learning (PNAL) framework. Compared to existing
noise-robust methods on image tasks, our PNAL is noise-rate blind, to cope with
the spatially variant noise rate problem specific to point clouds.
Specifically, we propose a novel point-wise confidence selection to obtain
reliable labels based on the historical predictions of each point. A novel
cluster-wise label correction is proposed with a voting strategy to generate
the best possible label taking the neighbor point correlations into
consideration. We conduct extensive experiments to demonstrate the
effectiveness of PNAL on both synthetic and real-world noisy datasets. In
particular, even with $60\%$ symmetric noisy labels, our proposed method
produces much better results than its baseline counterpart without PNAL and is
comparable to the ideal upper bound trained on a completely clean dataset.
Moreover, we fully re-labeled the validation set of a popular but noisy
real-world scene dataset ScanNetV2 to make it clean, for rigorous experiment
and future research. Our code and data are available at
\url{https://shuquanye.com/PNAL_website/}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ye_S/0/1/0/all/0/1"&gt;Shuquan Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1"&gt;Dongdong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1"&gt;Songfang Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liao_J/0/1/0/all/0/1"&gt;Jing Liao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predicting Unreliable Predictions by Shattering a Neural Network. (arXiv:2106.08365v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.08365</id>
        <link href="http://arxiv.org/abs/2106.08365"/>
        <updated>2021-08-06T00:51:48.178Z</updated>
        <summary type="html"><![CDATA[Piecewise linear neural networks can be split into subfunctions, each with
its own activation pattern, domain, and empirical error. Empirical error for
the full network can be written as an expectation over empirical error of
subfunctions. Constructing a generalization bound on subfunction empirical
error indicates that the more densely a subfunction is surrounded by training
samples in representation space, the more reliable its predictions are.
Further, it suggests that models with fewer activation regions generalize
better, and models that abstract knowledge to a greater degree generalize
better, all else equal. We propose not only a theoretical framework to reason
about subfunction error bounds but also a pragmatic way of approximately
evaluating it, which we apply to predicting which samples the network will not
successfully generalize to. We test our method on detection of
misclassification and out-of-distribution samples, finding that it performs
competitively in both cases. In short, some network activation patterns are
associated with higher reliability than others, and these can be identified
using subfunction error bounds.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ji_X/0/1/0/all/0/1"&gt;Xu Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pascanu_R/0/1/0/all/0/1"&gt;Razvan Pascanu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hjelm_D/0/1/0/all/0/1"&gt;Devon Hjelm&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vedaldi_A/0/1/0/all/0/1"&gt;Andrea Vedaldi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lakshminarayanan_B/0/1/0/all/0/1"&gt;Balaji Lakshminarayanan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1"&gt;Yoshua Bengio&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fuzzy Logic based Logical Query Answering on Knowledge Graph. (arXiv:2108.02390v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02390</id>
        <link href="http://arxiv.org/abs/2108.02390"/>
        <updated>2021-08-06T00:51:48.138Z</updated>
        <summary type="html"><![CDATA[Answering complex First-Order Logical (FOL) queries on large-scale incomplete
knowledge graphs (KGs) is an important yet challenging task. Recent advances
embed logical queries and KG entities in the vector space and conduct query
answering via dense similarity search. However, most of the designed logical
operators in existing works do not satisfy the axiomatic system of classical
logic. Moreover, these logical operators are parameterized so that they require
a large number of complex FOL queries as training data, which are often arduous
or even inaccessible to collect in most real-world KGs. In this paper, we
present FuzzQE, a fuzzy logic based query embedding framework for answering FOL
queries over KGs. FuzzQE follows fuzzy logic to define logical operators in a
principled and learning free manner. Extensive experiments on two benchmark
datasets demonstrate that FuzzQE achieves significantly better performance in
answering FOL queries compared to the state-of-the-art methods. In addition,
FuzzQE trained with only KG link prediction without any complex queries can
achieve comparable performance with the systems trained with all FOL queries.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xuelu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1"&gt;Ziniu Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1"&gt;Yizhou Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Detection of Lung Nodules in Chest Radiography Using Generative Adversarial Networks. (arXiv:2108.02233v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02233</id>
        <link href="http://arxiv.org/abs/2108.02233"/>
        <updated>2021-08-06T00:51:47.947Z</updated>
        <summary type="html"><![CDATA[Lung nodules are commonly missed in chest radiographs. We propose and
evaluate P-AnoGAN, an unsupervised anomaly detection approach for lung nodules
in radiographs. P-AnoGAN modifies the fast anomaly detection generative
adversarial network (f-AnoGAN) by utilizing a progressive GAN and a
convolutional encoder-decoder-encoder pipeline. Model training uses only
unlabelled healthy lung patches extracted from the Indiana University Chest
X-Ray Collection. External validation and testing are performed using healthy
and unhealthy patches extracted from the ChestX-ray14 and Japanese Society for
Radiological Technology datasets, respectively. Our model robustly identifies
patches containing lung nodules in external validation and test data with
ROC-AUC of 91.17% and 87.89%, respectively. These results show unsupervised
methods may be useful in challenging tasks such as lung nodule detection in
radiographs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bhatt_N/0/1/0/all/0/1"&gt;Nitish Bhatt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prados_D/0/1/0/all/0/1"&gt;David Ramon Prados&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hodzic_N/0/1/0/all/0/1"&gt;Nedim Hodzic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karanassios_C/0/1/0/all/0/1"&gt;Christos Karanassios&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tizhoosh_H/0/1/0/all/0/1"&gt;H.R. Tizhoosh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semi- and Self-Supervised Multi-View Fusion of 3D Microscopy Images using Generative Adversarial Networks. (arXiv:2108.02743v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02743</id>
        <link href="http://arxiv.org/abs/2108.02743"/>
        <updated>2021-08-06T00:51:47.898Z</updated>
        <summary type="html"><![CDATA[Recent developments in fluorescence microscopy allow capturing
high-resolution 3D images over time for living model organisms. To be able to
image even large specimens, techniques like multi-view light-sheet imaging
record different orientations at each time point that can then be fused into a
single high-quality volume. Based on measured point spread functions (PSF),
deconvolution and content fusion are able to largely revert the inevitable
degradation occurring during the imaging process. Classical multi-view
deconvolution and fusion methods mainly use iterative procedures and
content-based averaging. Lately, Convolutional Neural Networks (CNNs) have been
deployed to approach 3D single-view deconvolution microscopy, but the
multi-view case waits to be studied. We investigated the efficacy of CNN-based
multi-view deconvolution and fusion with two synthetic data sets that mimic
developing embryos and involve either two or four complementary 3D views.
Compared with classical state-of-the-art methods, the proposed semi- and
self-supervised models achieve competitive and superior deconvolution and
fusion quality in the two-view and quad-view cases, respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1"&gt;Canyu Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eschweiler_D/0/1/0/all/0/1"&gt;Dennis Eschweiler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stegmaier_J/0/1/0/all/0/1"&gt;Johannes Stegmaier&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph Self-Supervised Learning: A Survey. (arXiv:2103.00111v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.00111</id>
        <link href="http://arxiv.org/abs/2103.00111"/>
        <updated>2021-08-06T00:51:47.887Z</updated>
        <summary type="html"><![CDATA[Deep learning on graphs has attracted significant interests recently.
However, most of the works have focused on (semi-) supervised learning,
resulting in shortcomings including heavy label reliance, poor generalization,
and weak robustness. To address these issues, self-supervised learning (SSL),
which extracts informative knowledge through well-designed pretext tasks
without relying on manual labels, has become a promising and trending learning
paradigm for graph data. Different from SSL on other domains like computer
vision and natural language processing, SSL on graphs has an exclusive
background, design ideas, and taxonomies. Under the umbrella of graph
self-supervised learning, we present a timely and comprehensive review of the
existing approaches which employ SSL techniques for graph data. We construct a
unified framework that mathematically formalizes the paradigm of graph SSL.
According to the objectives of pretext tasks, we divide these approaches into
four categories: generation-based, auxiliary property-based, contrast-based,
and hybrid approaches. We further conclude the applications of graph SSL across
various research fields and summarize the commonly used datasets, evaluation
benchmark, performance comparison and open-source codes of graph SSL. Finally,
we discuss the remaining challenges and potential future directions in this
research field.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yixin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1"&gt;Shirui Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_M/0/1/0/all/0/1"&gt;Ming Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1"&gt;Chuan Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1"&gt;Feng Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1"&gt;Philip S. Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sketch Your Own GAN. (arXiv:2108.02774v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02774</id>
        <link href="http://arxiv.org/abs/2108.02774"/>
        <updated>2021-08-06T00:51:47.880Z</updated>
        <summary type="html"><![CDATA[Can a user create a deep generative model by sketching a single example?
Traditionally, creating a GAN model has required the collection of a
large-scale dataset of exemplars and specialized knowledge in deep learning. In
contrast, sketching is possibly the most universally accessible way to convey a
visual concept. In this work, we present a method, GAN Sketching, for rewriting
GANs with one or more sketches, to make GANs training easier for novice users.
In particular, we change the weights of an original GAN model according to user
sketches. We encourage the model's output to match the user sketches through a
cross-domain adversarial loss. Furthermore, we explore different regularization
methods to preserve the original model's diversity and image quality.
Experiments have shown that our method can mold GANs to match shapes and poses
specified by sketches while maintaining realism and diversity. Finally, we
demonstrate a few applications of the resulting GAN, including latent space
interpolation and image editing.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Sheng-Yu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bau_D/0/1/0/all/0/1"&gt;David Bau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jun-Yan Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Performer Identification From Symbolic Representation of Music Using Statistical Models. (arXiv:2108.02576v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2108.02576</id>
        <link href="http://arxiv.org/abs/2108.02576"/>
        <updated>2021-08-06T00:51:47.874Z</updated>
        <summary type="html"><![CDATA[Music Performers have their own idiosyncratic way of interpreting a musical
piece. A group of skilled performers playing the same piece of music would
likely to inject their unique artistic styles in their performances. The
variations of the tempo, timing, dynamics, articulation etc. from the actual
notated music are what make the performers unique in their performances. This
study presents a dataset consisting of four movements of Schubert's ``Sonata in
B-flat major, D.960" performed by nine virtuoso pianists individually. We
proposed and extracted a set of expressive features that are able to capture
the characteristics of an individual performer's style. We then present a
performer identification method based on the similarity of feature
distribution, given a set of piano performances. The identification is done
considering each feature individually as well as a fusion of the features.
Results show that the proposed method achieved a precision of 0.903 using
fusion features. Moreover, the onset time deviation feature shows promising
result when considered individually.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rafee_S/0/1/0/all/0/1"&gt;Syed Rifat Mahmud Rafee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fazekas_G/0/1/0/all/0/1"&gt;Gyorgy Fazekas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wiggins_G/0/1/0/all/0/1"&gt;Geraint A.~Wiggins&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MER-SDN: Machine Learning Framework for Traffic Aware Energy Efficient Routing in SDN. (arXiv:1909.08074v3 [cs.NI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1909.08074</id>
        <link href="http://arxiv.org/abs/1909.08074"/>
        <updated>2021-08-06T00:51:47.852Z</updated>
        <summary type="html"><![CDATA[Software Defined Networking (SDN) achieves programmability of a network
through separation of the control and data planes. It enables flexibility in
network management and control. Energy efficiency is one of the challenging
global problems which has both economic and environmental impact. A massive
amount of information is generated in the controller of an SDN based network.
Machine learning gives the ability to computers to progressively learn from
data without having to write specific instructions. In this work, we propose
MER-SDN: a machine learning framework for traffic-aware energy efficient
routing in SDN. Feature extraction, training, and testing are the three main
stages of the learning machine. Experiments are conducted on Mininet and POX
controller using real-world network topology and dynamic traffic traces from
SNDlib. Results show that our approach achieves more than 65\% feature size
reduction, more than 70% accuracy in parameter prediction of an energy
efficient heuristics algorithm, also our prediction refine heuristics converges
the predicted value to the optimal parameters values with up to 25X speedup as
compared to the brute force method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Assefa_B/0/1/0/all/0/1"&gt;Beakal Gizachew Assefa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ozkasap_O/0/1/0/all/0/1"&gt;Oznur Ozkasap&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Introduction to Normalizing Flows for Lattice Field Theory. (arXiv:2101.08176v2 [hep-lat] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.08176</id>
        <link href="http://arxiv.org/abs/2101.08176"/>
        <updated>2021-08-06T00:51:47.845Z</updated>
        <summary type="html"><![CDATA[This notebook tutorial demonstrates a method for sampling Boltzmann
distributions of lattice field theories using a class of machine learning
models known as normalizing flows. The ideas and approaches proposed in
arXiv:1904.12072, arXiv:2002.02428, and arXiv:2003.06413 are reviewed and a
concrete implementation of the framework is presented. We apply this framework
to a lattice scalar field theory and to U(1) gauge theory, explicitly encoding
gauge symmetries in the flow-based approach to the latter. This presentation is
intended to be interactive and working with the attached Jupyter notebook is
recommended.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/hep-lat/1/au:+Albergo_M/0/1/0/all/0/1"&gt;Michael S. Albergo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-lat/1/au:+Boyda_D/0/1/0/all/0/1"&gt;Denis Boyda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-lat/1/au:+Hackett_D/0/1/0/all/0/1"&gt;Daniel C. Hackett&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-lat/1/au:+Kanwar_G/0/1/0/all/0/1"&gt;Gurtej Kanwar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-lat/1/au:+Cranmer_K/0/1/0/all/0/1"&gt;Kyle Cranmer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-lat/1/au:+Racaniere_S/0/1/0/all/0/1"&gt;S&amp;#xe9;bastien Racani&amp;#xe8;re&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-lat/1/au:+Rezende_D/0/1/0/all/0/1"&gt;Danilo Jimenez Rezende&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-lat/1/au:+Shanahan_P/0/1/0/all/0/1"&gt;Phiala E. Shanahan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The AI Economist: Optimal Economic Policy Design via Two-level Deep Reinforcement Learning. (arXiv:2108.02755v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02755</id>
        <link href="http://arxiv.org/abs/2108.02755"/>
        <updated>2021-08-06T00:51:47.833Z</updated>
        <summary type="html"><![CDATA[AI and reinforcement learning (RL) have improved many areas, but are not yet
widely adopted in economic policy design, mechanism design, or economics at
large. At the same time, current economic methodology is limited by a lack of
counterfactual data, simplistic behavioral models, and limited opportunities to
experiment with policies and evaluate behavioral responses. Here we show that
machine-learning-based economic simulation is a powerful policy and mechanism
design framework to overcome these limitations. The AI Economist is a
two-level, deep RL framework that trains both agents and a social planner who
co-adapt, providing a tractable solution to the highly unstable and novel
two-level RL challenge. From a simple specification of an economy, we learn
rational agent behaviors that adapt to learned planner policies and vice versa.
We demonstrate the efficacy of the AI Economist on the problem of optimal
taxation. In simple one-step economies, the AI Economist recovers the optimal
tax policy of economic theory. In complex, dynamic economies, the AI Economist
substantially improves both utilitarian social welfare and the trade-off
between equality and productivity over baselines. It does so despite emergent
tax-gaming strategies, while accounting for agent interactions and behavioral
change more accurately than economic theory. These results demonstrate for the
first time that two-level, deep RL can be used for understanding and as a
complement to theory for economic design, unlocking a new computational
learning-based approach to understanding economic policy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1"&gt;Stephan Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Trott_A/0/1/0/all/0/1"&gt;Alexander Trott&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srinivasa_S/0/1/0/all/0/1"&gt;Sunil Srinivasa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parkes_D/0/1/0/all/0/1"&gt;David C. Parkes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Socher_R/0/1/0/all/0/1"&gt;Richard Socher&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Generative Adversarial Imitation Learning via Local Lipschitzness. (arXiv:2107.00116v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.00116</id>
        <link href="http://arxiv.org/abs/2107.00116"/>
        <updated>2021-08-06T00:51:47.826Z</updated>
        <summary type="html"><![CDATA[We explore methodologies to improve the robustness of generative adversarial
imitation learning (GAIL) algorithms to observation noise. Towards this
objective, we study the effect of local Lipschitzness of the discriminator and
the generator on the robustness of policies learned by GAIL. In many robotics
applications, the learned policies by GAIL typically suffer from a degraded
performance at test time since the observations from the environment might be
corrupted by noise. Hence, robustifying the learned policies against the
observation noise is of critical importance. To this end, we propose a
regularization method to induce local Lipschitzness in the generator and the
discriminator of adversarial imitation learning methods. We show that the
modified objective leads to learning significantly more robust policies.
Moreover, we demonstrate --- both theoretically and experimentally --- that
training a locally Lipschitz discriminator leads to a locally Lipschitz
generator, thereby improving the robustness of the resultant policy. We perform
extensive experiments on simulated robot locomotion environments from the
MuJoCo suite that demonstrate the proposed method learns policies that
significantly outperform the state-of-the-art generative adversarial imitation
learning algorithm when applied to test scenarios with noise-corrupted
observations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Memarian_F/0/1/0/all/0/1"&gt;Farzan Memarian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hashemi_A/0/1/0/all/0/1"&gt;Abolfazl Hashemi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niekum_S/0/1/0/all/0/1"&gt;Scott Niekum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Topcu_U/0/1/0/all/0/1"&gt;Ufuk Topcu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Locally Interpretable One-Class Anomaly Detection for Credit Card Fraud Detection. (arXiv:2108.02501v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02501</id>
        <link href="http://arxiv.org/abs/2108.02501"/>
        <updated>2021-08-06T00:51:47.820Z</updated>
        <summary type="html"><![CDATA[For the highly imbalanced credit card fraud detection problem, most existing
methods either use data augmentation methods or conventional machine learning
models, while neural network-based anomaly detection approaches are lacking.
Furthermore, few studies have employed AI interpretability tools to investigate
the feature importance of transaction data, which is crucial for the black-box
fraud detection module. Considering these two points together, we propose a
novel anomaly detection framework for credit card fraud detection as well as a
model-explaining module responsible for prediction explanations. The fraud
detection model is composed of two deep neural networks, which are trained in
an unsupervised and adversarial manner. Precisely, the generator is an
AutoEncoder aiming to reconstruct genuine transaction data, while the
discriminator is a fully-connected network for fraud detection. The explanation
module has three white-box explainers in charge of interpretations of the
AutoEncoder, discriminator, and the whole detection model, respectively.
Experimental results show the state-of-the-art performances of our fraud
detection model on the benchmark dataset compared with baselines. In addition,
prediction analyses by three explainers are presented, offering a clear
perspective on how each feature of an instance of interest contributes to the
final model output.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1"&gt;Tungyu Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Youting Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Method for Medical Data Analysis Using the LogNNet for Clinical Decision Support Systems and Edge Computing in Healthcare. (arXiv:2108.02428v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02428</id>
        <link href="http://arxiv.org/abs/2108.02428"/>
        <updated>2021-08-06T00:51:47.801Z</updated>
        <summary type="html"><![CDATA[The study presents a new method for analyzing medical data based on the
LogNNet neural network, which uses chaotic mappings to transform input
information. The technique calculates risk factors for the presence of a
disease in a patient according to a set of medical health indicators. The
LogNNet architecture allows the implementation of artificial intelligence on
medical pe-ripherals of the Internet of Things with low RAM resources, and the
development of edge computing in healthcare. The efficiency of LogNNet in
assessing perinatal risk is illustrated on cardiotocogram data of 2126 pregnant
women, obtained from the UC Irvine machine learning repository. The
classification accuracy reaches ~ 91%, with the ~ 3-10 kB of RAM used on the
Arduino microcontroller. In addition, examples for diagnosing COVID-19 are
provided, using LogNNet trained on a publicly available database from the
Israeli Ministry of Health. The service concept has been developed, which uses
the data of the express test for COVID-19 and reaches the classification
accuracy of ~ 95% with the ~ 0.6 kB of RAM used on Arduino microcontrollers. In
all examples, the model is tested using standard classification quality
metrics: Precision, Recall, and F1-measure. The study results can be used in
clinical decision support systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Velichko_A/0/1/0/all/0/1"&gt;Andrei Velichko&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sparse Communication via Mixed Distributions. (arXiv:2108.02658v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02658</id>
        <link href="http://arxiv.org/abs/2108.02658"/>
        <updated>2021-08-06T00:51:47.788Z</updated>
        <summary type="html"><![CDATA[Neural networks and other machine learning models compute continuous
representations, while humans communicate mostly through discrete symbols.
Reconciling these two forms of communication is desirable for generating
human-readable interpretations or learning discrete latent variable models,
while maintaining end-to-end differentiability. Some existing approaches (such
as the Gumbel-Softmax transformation) build continuous relaxations that are
discrete approximations in the zero-temperature limit, while others (such as
sparsemax transformations and the Hard Concrete distribution) produce
discrete/continuous hybrids. In this paper, we build rigorous theoretical
foundations for these hybrids, which we call "mixed random variables." Our
starting point is a new "direct sum" base measure defined on the face lattice
of the probability simplex. From this measure, we introduce new entropy and
Kullback-Leibler divergence functions that subsume the discrete and
differential cases and have interpretations in terms of code optimality. Our
framework suggests two strategies for representing and sampling mixed random
variables, an extrinsic ("sample-and-project") and an intrinsic one (based on
face stratification). We experiment with both approaches on an emergent
communication benchmark and on modeling MNIST and Fashion-MNIST data with
variational auto-encoders with mixed latent variables.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Farinhas_A/0/1/0/all/0/1"&gt;Ant&amp;#xf3;nio Farinhas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aziz_W/0/1/0/all/0/1"&gt;Wilker Aziz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niculae_V/0/1/0/all/0/1"&gt;Vlad Niculae&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martins_A/0/1/0/all/0/1"&gt;Andr&amp;#xe9; F. T. Martins&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spotify Danceability and Popularity Analysis using SAP. (arXiv:2108.02370v1 [cs.DC])]]></title>
        <id>http://arxiv.org/abs/2108.02370</id>
        <link href="http://arxiv.org/abs/2108.02370"/>
        <updated>2021-08-06T00:51:47.749Z</updated>
        <summary type="html"><![CDATA[Our analysis reviews and visualizes the audio features and popularity of
songs streamed on Spotify*. Our dataset, downloaded from Kaggle and originally
sourced from Spotify API, consists of multiple Excel files containing
information relevant to our visualization and regression analysis. The exercise
seeks to determine the connection between the popularity of the songs and the
danceability. Insights to be included and factored as part of our analysis
include song energy, valence, BPM, release date, and year.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ochi_V/0/1/0/all/0/1"&gt;Virginia Ochi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Estrada_R/0/1/0/all/0/1"&gt;Ricardo Estrada&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gaji_T/0/1/0/all/0/1"&gt;Teezal Gaji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gadea_W/0/1/0/all/0/1"&gt;Wendy Gadea&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duong_E/0/1/0/all/0/1"&gt;Emily Duong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bridging Few-Shot Learning and Adaptation: New Challenges of Support-Query Shift. (arXiv:2105.11804v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.11804</id>
        <link href="http://arxiv.org/abs/2105.11804"/>
        <updated>2021-08-06T00:51:47.743Z</updated>
        <summary type="html"><![CDATA[Few-Shot Learning (FSL) algorithms have made substantial progress in learning
novel concepts with just a handful of labelled data. To classify query
instances from novel classes encountered at test-time, they only require a
support set composed of a few labelled samples. FSL benchmarks commonly assume
that those queries come from the same distribution as instances in the support
set. However, in a realistic set-ting, data distribution is plausibly subject
to change, a situation referred to as Distribution Shift (DS). The present work
addresses the new and challenging problem of Few-Shot Learning under
Support/Query Shift (FSQS) i.e., when support and query instances are sampled
from related but different distributions. Our contributions are the following.
First, we release a testbed for FSQS, including datasets, relevant baselines
and a protocol for a rigorous and reproducible evaluation. Second, we observe
that well-established FSL algorithms unsurprisingly suffer from a considerable
drop in accuracy when facing FSQS, stressing the significance of our study.
Finally, we show that transductive algorithms can limit the inopportune effect
of DS. In particular, we study both the role of Batch-Normalization and Optimal
Transport (OT) in aligning distributions, bridging Unsupervised Domain
Adaptation with FSL. This results in a new method that efficiently combines OT
with the celebrated Prototypical Networks. We bring compelling experiments
demonstrating the advantage of our method. Our work opens an exciting line of
research by providing a testbed and strong baselines. Our code is available at
https://github.com/ebennequin/meta-domain-shift.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bennequin_E/0/1/0/all/0/1"&gt;Etienne Bennequin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bouvier_V/0/1/0/all/0/1"&gt;Victor Bouvier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tami_M/0/1/0/all/0/1"&gt;Myriam Tami&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Toubhans_A/0/1/0/all/0/1"&gt;Antoine Toubhans&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hudelot_C/0/1/0/all/0/1"&gt;C&amp;#xe9;line Hudelot&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Min-Max Complexity of Distributed Stochastic Convex Optimization with Intermittent Communication. (arXiv:2102.01583v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.01583</id>
        <link href="http://arxiv.org/abs/2102.01583"/>
        <updated>2021-08-06T00:51:47.735Z</updated>
        <summary type="html"><![CDATA[We resolve the min-max complexity of distributed stochastic convex
optimization (up to a log factor) in the intermittent communication setting,
where $M$ machines work in parallel over the course of $R$ rounds of
communication to optimize the objective, and during each round of
communication, each machine may sequentially compute $K$ stochastic gradient
estimates. We present a novel lower bound with a matching upper bound that
establishes an optimal algorithm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Woodworth_B/0/1/0/all/0/1"&gt;Blake Woodworth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bullins_B/0/1/0/all/0/1"&gt;Brian Bullins&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shamir_O/0/1/0/all/0/1"&gt;Ohad Shamir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srebro_N/0/1/0/all/0/1"&gt;Nathan Srebro&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Training independent subnetworks for robust prediction. (arXiv:2010.06610v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.06610</id>
        <link href="http://arxiv.org/abs/2010.06610"/>
        <updated>2021-08-06T00:51:47.717Z</updated>
        <summary type="html"><![CDATA[Recent approaches to efficiently ensemble neural networks have shown that
strong robustness and uncertainty performance can be achieved with a negligible
gain in parameters over the original network. However, these methods still
require multiple forward passes for prediction, leading to a significant
computational cost. In this work, we show a surprising result: the benefits of
using multiple predictions can be achieved `for free' under a single model's
forward pass. In particular, we show that, using a multi-input multi-output
(MIMO) configuration, one can utilize a single model's capacity to train
multiple subnetworks that independently learn the task at hand. By ensembling
the predictions made by the subnetworks, we improve model robustness without
increasing compute. We observe a significant improvement in negative
log-likelihood, accuracy, and calibration error on CIFAR10, CIFAR100, ImageNet,
and their out-of-distribution variants compared to previous methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Havasi_M/0/1/0/all/0/1"&gt;Marton Havasi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jenatton_R/0/1/0/all/0/1"&gt;Rodolphe Jenatton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fort_S/0/1/0/all/0/1"&gt;Stanislav Fort&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jeremiah Zhe Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Snoek_J/0/1/0/all/0/1"&gt;Jasper Snoek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lakshminarayanan_B/0/1/0/all/0/1"&gt;Balaji Lakshminarayanan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_A/0/1/0/all/0/1"&gt;Andrew M. Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tran_D/0/1/0/all/0/1"&gt;Dustin Tran&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Human strategic decision making in parametrized games. (arXiv:2104.14744v2 [cs.GT] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.14744</id>
        <link href="http://arxiv.org/abs/2104.14744"/>
        <updated>2021-08-06T00:51:47.711Z</updated>
        <summary type="html"><![CDATA[Many real-world games contain parameters which can affect payoffs, action
spaces, and information states. For fixed values of the parameters, the game
can be solved using standard algorithms. However, in many settings agents must
act without knowing the values of the parameters that will be encountered in
advance. Often the decisions must be made by a human under time and resource
constraints, and it is unrealistic to assume that a human can solve the game in
real time. We present a new framework that enables human decision makers to
make fast decisions without the aid of real-time solvers. We demonstrate
applicability to a variety of situations including settings with multiple
players and imperfect information.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ganzfried_S/0/1/0/all/0/1"&gt;Sam Ganzfried&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimal Transport for Unsupervised Restoration Learning. (arXiv:2108.02574v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2108.02574</id>
        <link href="http://arxiv.org/abs/2108.02574"/>
        <updated>2021-08-06T00:51:47.704Z</updated>
        <summary type="html"><![CDATA[Recently, much progress has been made in unsupervised restoration learning.
However, existing methods more or less rely on some assumptions on the signal
and/or degradation model, which limits their practical performance. How to
construct an optimal criterion for unsupervised restoration learning without
any prior knowledge on the degradation model is still an open question. Toward
answering this question, this work proposes a criterion for unsupervised
restoration learning based on the optimal transport theory. This criterion has
favorable properties, e.g., approximately maximal preservation of the
information of the signal, whilst achieving perceptual reconstruction.
Furthermore, though a relaxed unconstrained formulation is used in practical
implementation, we show that the relaxed formulation in theory has the same
solution as the original constrained formulation. Experiments on synthetic and
real-world data, including realistic photographic, microscopy, depth, and raw
depth images, demonstrate that the proposed method even compares favorably with
supervised methods, e.g., approaching the PSNR of supervised methods while
having better perceptual quality. Particularly, for spatially correlated noise
and realistic microscopy images, the proposed method not only achieves better
perceptual quality but also has higher PSNR than supervised methods. Besides,
it shows remarkable superiority in harsh practical conditions with complex
noise, e.g., raw depth images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wen_F/0/1/0/all/0/1"&gt;Fei Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yan_Z/0/1/0/all/0/1"&gt;Zeyu Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ying_R/0/1/0/all/0/1"&gt;Rendong Ying&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liu_P/0/1/0/all/0/1"&gt;Peilin Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dynamic Relevance Learning for Few-Shot Object Detection. (arXiv:2108.02235v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02235</id>
        <link href="http://arxiv.org/abs/2108.02235"/>
        <updated>2021-08-06T00:51:47.697Z</updated>
        <summary type="html"><![CDATA[Expensive bounding-box annotations have limited the development of object
detection task. Thus, it is necessary to focus on more challenging task of
few-shot object detection. It requires the detector to recognize objects of
novel classes with only a few training samples. Nowadays, many existing popular
methods based on meta-learning have achieved promising performance, such as
Meta R-CNN series. However, only a single category of support data is used as
the attention to guide the detecting of query images each time. Their relevance
to each other remains unexploited. Moreover, a lot of recent works treat the
support data and query images as independent branch without considering the
relationship between them. To address this issue, we propose a dynamic
relevance learning model, which utilizes the relationship between all support
images and Region of Interest (RoI) on the query images to construct a dynamic
graph convolutional network (GCN). By adjusting the prediction distribution of
the base detector using the output of this GCN, the proposed model can guide
the detector to improve the class representation implicitly. Comprehensive
experiments have been conducted on Pascal VOC and MS-COCO dataset. The proposed
model achieves the best overall performance, which shows its effectiveness of
learning more generalized features. Our code is available at
https://github.com/liuweijie19980216/DRL-for-FSOD.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Weijie Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang%2A_C/0/1/0/all/0/1"&gt;Chong Wang*&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Haohe Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1"&gt;Shenghao Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Song Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1"&gt;Xulun Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1"&gt;Jiafei Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Parallel Capsule Networks for Classification of White Blood Cells. (arXiv:2108.02644v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02644</id>
        <link href="http://arxiv.org/abs/2108.02644"/>
        <updated>2021-08-06T00:51:47.691Z</updated>
        <summary type="html"><![CDATA[Capsule Networks (CapsNets) is a machine learning architecture proposed to
overcome some of the shortcomings of convolutional neural networks (CNNs).
However, CapsNets have mainly outperformed CNNs in datasets where images are
small and/or the objects to identify have minimal background noise. In this
work, we present a new architecture, parallel CapsNets, which exploits the
concept of branching the network to isolate certain capsules, allowing each
branch to identify different entities. We applied our concept to the two
current types of CapsNet architectures, studying the performance for networks
with different layers of capsules. We tested our design in a public, highly
unbalanced dataset of acute myeloid leukaemia images (15 classes). Our
experiments showed that conventional CapsNets show similar performance than our
baseline CNN (ResNeXt-50) but depict instability problems. In contrast,
parallel CapsNets can outperform ResNeXt-50, is more stable, and shows better
rotational invariance than both, conventional CapsNets and ResNeXt-50.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vigueras_Guillen_J/0/1/0/all/0/1"&gt;Juan P. Vigueras-Guill&amp;#xe9;n&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Patra_A/0/1/0/all/0/1"&gt;Arijit Patra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Engkvist_O/0/1/0/all/0/1"&gt;Ola Engkvist&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seeliger_F/0/1/0/all/0/1"&gt;Frank Seeliger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VC-dimensions of nondeterministic finite automata for words of equal length. (arXiv:2001.02309v2 [cs.FL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2001.02309</id>
        <link href="http://arxiv.org/abs/2001.02309"/>
        <updated>2021-08-06T00:51:47.679Z</updated>
        <summary type="html"><![CDATA[Let $NFA_b(q)$ denote the set of languages accepted by nondeterministic
finite automata with $q$ states over an alphabet with $b$ letters. Let $B_n$
denote the set of words of length $n$. We give a quadratic lower bound on the
VC dimension of \[

NFA_2(q)\cap B_n = \{L\cap B_n \mid L \in NFA_2(q)\} \] as a function of $q$.

Next, the work of Gruber and Holzer (2007) gives an upper bound for the
nondeterministic state complexity of finite languages contained in $B_n$, which
we strengthen using our methods.

Finally, we give some theoretical and experimental results on the dependence
on $n$ of the VC dimension and testing dimension of $NFA_2(q)\cap B_n$.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kjos_Hanssen_B/0/1/0/all/0/1"&gt;Bj&amp;#xf8;rn Kjos-Hanssen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Felix_C/0/1/0/all/0/1"&gt;Clyde James Felix&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1"&gt;Sun Young Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lamb_E/0/1/0/all/0/1"&gt;Ethan Lamb&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Takahashi_D/0/1/0/all/0/1"&gt;Davin Takahashi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tikhonov Regularization of Circle-Valued Signals. (arXiv:2108.02602v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2108.02602</id>
        <link href="http://arxiv.org/abs/2108.02602"/>
        <updated>2021-08-06T00:51:47.662Z</updated>
        <summary type="html"><![CDATA[It is common to have to process signals or images whose values are cyclic and
can be represented as points on the complex circle, like wrapped phases,
angles, orientations, or color hues. We consider a Tikhonov-type regularization
model to smoothen or interpolate circle-valued signals defined on arbitrary
graphs. We propose a convex relaxation of this nonconvex problem as a
semidefinite program, and an efficient algorithm to solve it.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Condat_L/0/1/0/all/0/1"&gt;Laurent Condat&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CoReD: Generalizing Fake Media Detection with Continual Representation using Distillation. (arXiv:2107.02408v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.02408</id>
        <link href="http://arxiv.org/abs/2107.02408"/>
        <updated>2021-08-06T00:51:47.654Z</updated>
        <summary type="html"><![CDATA[Over the last few decades, artificial intelligence research has made
tremendous strides, but it still heavily relies on fixed datasets in stationary
environments. Continual learning is a growing field of research that examines
how AI systems can learn sequentially from a continuous stream of linked data
in the same way that biological systems do. Simultaneously, fake media such as
deepfakes and synthetic face images have emerged as significant to current
multimedia technologies. Recently, numerous method has been proposed which can
detect deepfakes with high accuracy. However, they suffer significantly due to
their reliance on fixed datasets in limited evaluation settings. Therefore, in
this work, we apply continuous learning to neural networks' learning dynamics,
emphasizing its potential to increase data efficiency significantly. We propose
Continual Representation using Distillation (CoReD) method that employs the
concept of Continual Learning (CL), Representation Learning (RL), and Knowledge
Distillation (KD). We design CoReD to perform sequential domain adaptation
tasks on new deepfake and GAN-generated synthetic face datasets, while
effectively minimizing the catastrophic forgetting in a teacher-student model
setting. Our extensive experimental results demonstrate that our method is
efficient at domain adaptation to detect low-quality deepfakes videos and
GAN-generated images from several datasets, outperforming the-state-of-art
baseline methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1"&gt;Minha Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tariq_S/0/1/0/all/0/1"&gt;Shahroz Tariq&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Woo_S/0/1/0/all/0/1"&gt;Simon S. Woo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rotaflip: A New CNN Layer for Regularization and Rotational Invariance in Medical Images. (arXiv:2108.02704v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02704</id>
        <link href="http://arxiv.org/abs/2108.02704"/>
        <updated>2021-08-06T00:51:47.648Z</updated>
        <summary type="html"><![CDATA[Regularization in convolutional neural networks (CNNs) is usually addressed
with dropout layers. However, dropout is sometimes detrimental in the
convolutional part of a CNN as it simply sets to zero a percentage of pixels in
the feature maps, adding unrepresentative examples during training. Here, we
propose a CNN layer that performs regularization by applying random rotations
of reflections to a small percentage of feature maps after every convolutional
layer. We prove how this concept is beneficial for images with orientational
symmetries, such as in medical images, as it provides a certain degree of
rotational invariance. We tested this method in two datasets, a patch-based set
of histopathology images (PatchCamelyon) to perform classification using a
generic DenseNet, and a set of specular microscopy images of the corneal
endothelium to perform segmentation using a tailored U-net, improving the
performance in both cases.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vigueras_Guillen_J/0/1/0/all/0/1"&gt;Juan P. Vigueras-Guill&amp;#xe9;n&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lasenby_J/0/1/0/all/0/1"&gt;Joan Lasenby&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seeliger_F/0/1/0/all/0/1"&gt;Frank Seeliger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Frequency Domain Image Translation: More Photo-realistic, Better Identity-preserving. (arXiv:2011.13611v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.13611</id>
        <link href="http://arxiv.org/abs/2011.13611"/>
        <updated>2021-08-06T00:51:47.641Z</updated>
        <summary type="html"><![CDATA[Image-to-image translation has been revolutionized with GAN-based methods.
However, existing methods lack the ability to preserve the identity of the
source domain. As a result, synthesized images can often over-adapt to the
reference domain, losing important structural characteristics and suffering
from suboptimal visual quality. To solve these challenges, we propose a novel
frequency domain image translation (FDIT) framework, exploiting frequency
information for enhancing the image generation process. Our key idea is to
decompose the image into low-frequency and high-frequency components, where the
high-frequency feature captures object structure akin to the identity. Our
training objective facilitates the preservation of frequency information in
both pixel space and Fourier spectral space. We broadly evaluate FDIT across
five large-scale datasets and multiple tasks including image translation and
GAN inversion. Extensive experiments and ablations show that FDIT effectively
preserves the identity of the source image, and produces photo-realistic
images. FDIT establishes state-of-the-art performance, reducing the average FID
score by 5.6% compared to the previous best method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cai_M/0/1/0/all/0/1"&gt;Mu Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Hong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1"&gt;Huijuan Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geng_Q/0/1/0/all/0/1"&gt;Qichuan Geng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yixuan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1"&gt;Gao Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Visual Domain Adaptation for Monocular Depth Estimation on Resource-Constrained Hardware. (arXiv:2108.02671v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02671</id>
        <link href="http://arxiv.org/abs/2108.02671"/>
        <updated>2021-08-06T00:51:47.635Z</updated>
        <summary type="html"><![CDATA[Real-world perception systems in many cases build on hardware with limited
resources to adhere to cost and power limitations of their carrying system.
Deploying deep neural networks on resource-constrained hardware became possible
with model compression techniques, as well as efficient and hardware-aware
architecture design. However, model adaptation is additionally required due to
the diverse operation environments. In this work, we address the problem of
training deep neural networks on resource-constrained hardware in the context
of visual domain adaptation. We select the task of monocular depth estimation
where our goal is to transform a pre-trained model to the target's domain data.
While the source domain includes labels, we assume an unlabelled target domain,
as it happens in real-world applications. Then, we present an adversarial
learning approach that is adapted for training on the device with limited
resources. Since visual domain adaptation, i.e. neural network training, has
not been previously explored for resource-constrained hardware, we present the
first feasibility study for image-based depth estimation. Our experiments show
that visual domain adaptation is relevant only for efficient network
architectures and training sets at the order of a few hundred samples. Models
and code are publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hornauer_J/0/1/0/all/0/1"&gt;Julia Hornauer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nalpantidis_L/0/1/0/all/0/1"&gt;Lazaros Nalpantidis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Belagiannis_V/0/1/0/all/0/1"&gt;Vasileios Belagiannis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Benchmarking machine learning models on multi-centre eICU critical care dataset. (arXiv:1910.00964v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1910.00964</id>
        <link href="http://arxiv.org/abs/1910.00964"/>
        <updated>2021-08-06T00:51:47.616Z</updated>
        <summary type="html"><![CDATA[Progress of machine learning in critical care has been difficult to track, in
part due to absence of public benchmarks. Other fields of research (such as
computer vision and natural language processing) have established various
competitions and public benchmarks. Recent availability of large clinical
datasets has enabled the possibility of establishing public benchmarks. Taking
advantage of this opportunity, we propose a public benchmark suite to address
four areas of critical care, namely mortality prediction, estimation of length
of stay, patient phenotyping and risk of decompensation. We define each task
and compare the performance of both clinical models as well as baseline and
deep learning models using eICU critical care dataset of around 73,000
patients. This is the first public benchmark on a multi-centre critical care
dataset, comparing the performance of clinical gold standard with our
predictive model. We also investigate the impact of numerical variables as well
as handling of categorical variables on each of the defined tasks. The source
code, detailing our methods and experiments is publicly available such that
anyone can replicate our results and build upon our work.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sheikhalishahi_S/0/1/0/all/0/1"&gt;Seyedmostafa Sheikhalishahi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balaraman_V/0/1/0/all/0/1"&gt;Vevake Balaraman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Osmani_V/0/1/0/all/0/1"&gt;Venet Osmani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reducing Unintended Bias of ML Models on Tabular and Textual Data. (arXiv:2108.02662v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02662</id>
        <link href="http://arxiv.org/abs/2108.02662"/>
        <updated>2021-08-06T00:51:47.605Z</updated>
        <summary type="html"><![CDATA[Unintended biases in machine learning (ML) models are among the major
concerns that must be addressed to maintain public trust in ML. In this paper,
we address process fairness of ML models that consists in reducing the
dependence of models on sensitive features, without compromising their
performance. We revisit the framework FixOut that is inspired in the approach
"fairness through unawareness" to build fairer models. We introduce several
improvements such as automating the choice of FixOut's parameters. Also, FixOut
was originally proposed to improve fairness of ML models on tabular data. We
also demonstrate the feasibility of FixOut's workflow for models on textual
data. We present several experimental results that illustrate the fact that
FixOut improves process fairness on different classification settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alves_G/0/1/0/all/0/1"&gt;Guilherme Alves&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Amblard_M/0/1/0/all/0/1"&gt;Maxime Amblard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bernier_F/0/1/0/all/0/1"&gt;Fabien Bernier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Couceiro_M/0/1/0/all/0/1"&gt;Miguel Couceiro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Napoli_A/0/1/0/all/0/1"&gt;Amedeo Napoli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Attentive Cross-modal Connections for Deep Multimodal Wearable-based Emotion Recognition. (arXiv:2108.02241v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02241</id>
        <link href="http://arxiv.org/abs/2108.02241"/>
        <updated>2021-08-06T00:51:47.598Z</updated>
        <summary type="html"><![CDATA[Classification of human emotions can play an essential role in the design and
improvement of human-machine systems. While individual biological signals such
as Electrocardiogram (ECG) and Electrodermal Activity (EDA) have been widely
used for emotion recognition with machine learning methods, multimodal
approaches generally fuse extracted features or final classification/regression
results to boost performance. To enhance multimodal learning, we present a
novel attentive cross-modal connection to share information between
convolutional neural networks responsible for learning individual modalities.
Specifically, these connections improve emotion classification by sharing
intermediate representations among EDA and ECG and apply attention weights to
the shared information, thus learning more effective multimodal embeddings. We
perform experiments on the WESAD dataset to identify the best configuration of
the proposed method for emotion classification. Our experiments show that the
proposed approach is capable of learning strong multimodal representations and
outperforms a number of baselines methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bhatti_A/0/1/0/all/0/1"&gt;Anubhav Bhatti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Behinaein_B/0/1/0/all/0/1"&gt;Behnam Behinaein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rodenburg_D/0/1/0/all/0/1"&gt;Dirk Rodenburg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hungler_P/0/1/0/all/0/1"&gt;Paul Hungler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Etemad_A/0/1/0/all/0/1"&gt;Ali Etemad&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Addressing Heterogeneity in Federated Learning for Autonomous Vehicles Connected to a Drone Orchestrator. (arXiv:2108.02712v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02712</id>
        <link href="http://arxiv.org/abs/2108.02712"/>
        <updated>2021-08-06T00:51:47.589Z</updated>
        <summary type="html"><![CDATA[In this paper we envision a federated learning (FL) scenario in service of
amending the performance of autonomous road vehicles, through a drone traffic
monitor (DTM), that also acts as an orchestrator. Expecting non-IID data
distribution, we focus on the issue of accelerating the learning of a
particular class of critical object (CO), that may harm the nominal operation
of an autonomous vehicle. This can be done through proper allocation of the
wireless resources for addressing learner and data heterogeneity. Thus, we
propose a reactive method for the allocation of wireless resources, that
happens dynamically each FL round, and is based on each learner's contribution
to the general model. In addition to this, we explore the use of static methods
that remain constant across all rounds. Since we expect partial work from each
learner, we use the FedProx FL algorithm, in the task of computer vision. For
testing, we construct a non-IID data distribution of the MNIST and FMNIST
datasets among four types of learners, in scenarios that represent the quickly
changing environment. The results show that proactive measures are effective
and versatile at improving system accuracy, and quickly learning the CO class
when underrepresented in the network. Furthermore, the experiments show a
tradeoff between FedProx intensity and resource allocation efforts.
Nonetheless, a well adjusted FedProx local optimizer allows for an even better
overall accuracy, particularly when using deeper neural network (NN)
implementations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Donevski_I/0/1/0/all/0/1"&gt;Igor Donevski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nielsen_J/0/1/0/all/0/1"&gt;Jimmy Jessen Nielsen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Popovski_P/0/1/0/all/0/1"&gt;Petar Popovski&lt;/a&gt;,</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Beyond No Regret: Instance-Dependent PAC Reinforcement Learning. (arXiv:2108.02717v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02717</id>
        <link href="http://arxiv.org/abs/2108.02717"/>
        <updated>2021-08-06T00:51:47.581Z</updated>
        <summary type="html"><![CDATA[The theory of reinforcement learning has focused on two fundamental problems:
achieving low regret, and identifying $\epsilon$-optimal policies. While a
simple reduction allows one to apply a low-regret algorithm to obtain an
$\epsilon$-optimal policy and achieve the worst-case optimal rate, it is
unknown whether low-regret algorithms can obtain the instance-optimal rate for
policy identification. We show that this is not possible -- there exists a
fundamental tradeoff between achieving low regret and identifying an
$\epsilon$-optimal policy at the instance-optimal rate.

Motivated by our negative finding, we propose a new measure of
instance-dependent sample complexity for PAC tabular reinforcement learning
which explicitly accounts for the attainable state visitation distributions in
the underlying MDP. We then propose and analyze a novel, planning-based
algorithm which attains this sample complexity -- yielding a complexity which
scales with the suboptimality gaps and the ``reachability'' of a state. We show
that our algorithm is nearly minimax optimal, and on several examples that our
instance-dependent sample complexity offers significant improvements over
worst-case bounds.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wagenmaker_A/0/1/0/all/0/1"&gt;Andrew Wagenmaker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Simchowitz_M/0/1/0/all/0/1"&gt;Max Simchowitz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jamieson_K/0/1/0/all/0/1"&gt;Kevin Jamieson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predicting Post-Concussion Syndrome Outcomes with Machine Learning. (arXiv:2108.02570v1 [q-bio.QM])]]></title>
        <id>http://arxiv.org/abs/2108.02570</id>
        <link href="http://arxiv.org/abs/2108.02570"/>
        <updated>2021-08-06T00:51:47.564Z</updated>
        <summary type="html"><![CDATA[In this paper, machine learning models are used to predict outcomes for
patients with persistent post-concussion syndrome (PCS). Patients had sustained
a concussion at an average of two to three months before the study. By
utilizing assessed data, the machine learning models aimed to predict whether
or not a patient would continue to have PCS after four to five months. The
random forest classifier achieved the highest performance with an 85% accuracy
and an area under the receiver operating characteristic curve (AUC) of 0.94.
Factors found to be predictive of PCS outcome were Post-Traumatic Stress
Disorder (PTSD), perceived injustice, self-rated prognosis, and symptom
severity post-injury. The results of this study demonstrate that machine
learning models can predict PCS outcomes with high accuracy. With further
research, machine learning models may be implemented in healthcare settings to
help patients with persistent PCS.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Kim_M/0/1/0/all/0/1"&gt;Minhong Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AutoLL: Automatic Linear Layout of Graphs based on Deep Neural Network. (arXiv:2108.02431v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2108.02431</id>
        <link href="http://arxiv.org/abs/2108.02431"/>
        <updated>2021-08-06T00:51:47.558Z</updated>
        <summary type="html"><![CDATA[Linear layouts are a graph visualization method that can be used to capture
an entry pattern in an adjacency matrix of a given graph. By reordering the
node indices of the original adjacency matrix, linear layouts provide knowledge
of latent graph structures. Conventional linear layout methods commonly aim to
find an optimal reordering solution based on predefined features of a given
matrix and loss function. However, prior knowledge of the appropriate features
to use or structural patterns in a given adjacency matrix is not always
available. In such a case, performing the reordering based on data-driven
feature extraction without assuming a specific structure in an adjacency matrix
is preferable. Recently, a neural-network-based matrix reordering method called
DeepTMR has been proposed to perform this function. However, it is limited to a
two-mode reordering (i.e., the rows and columns are reordered separately) and
it cannot be applied in the one-mode setting (i.e., the same node order is used
for reordering both rows and columns), owing to the characteristics of its
model architecture. In this study, we extend DeepTMR and propose a new one-mode
linear layout method referred to as AutoLL. We developed two types of neural
network models, AutoLL-D and AutoLL-U, for reordering directed and undirected
networks, respectively. To perform one-mode reordering, these AutoLL models
have specific encoder architectures, which extract node features from an
observed adjacency matrix. We conducted both qualitative and quantitative
evaluations of the proposed approach, and the experimental results demonstrate
its effectiveness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Watanabe_C/0/1/0/all/0/1"&gt;Chihiro Watanabe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Suzuki_T/0/1/0/all/0/1"&gt;Taiji Suzuki&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Off-Belief Learning. (arXiv:2103.04000v4 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.04000</id>
        <link href="http://arxiv.org/abs/2103.04000"/>
        <updated>2021-08-06T00:51:47.552Z</updated>
        <summary type="html"><![CDATA[The standard problem setting in Dec-POMDPs is self-play, where the goal is to
find a set of policies that play optimally together. Policies learned through
self-play may adopt arbitrary conventions and implicitly rely on multi-step
reasoning based on fragile assumptions about other agents' actions and thus
fail when paired with humans or independently trained agents at test time. To
address this, we present off-belief learning (OBL). At each timestep OBL agents
follow a policy $\pi_1$ that is optimized assuming past actions were taken by a
given, fixed policy ($\pi_0$), but assuming that future actions will be taken
by $\pi_1$. When $\pi_0$ is uniform random, OBL converges to an optimal policy
that does not rely on inferences based on other agents' behavior (an optimal
grounded policy). OBL can be iterated in a hierarchy, where the optimal policy
from one level becomes the input to the next, thereby introducing multi-level
cognitive reasoning in a controlled manner. Unlike existing approaches, which
may converge to any equilibrium policy, OBL converges to a unique policy,
making it suitable for zero-shot coordination (ZSC). OBL can be scaled to
high-dimensional settings with a fictitious transition mechanism and shows
strong performance in both a toy-setting and the benchmark human-AI & ZSC
problem Hanabi.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1"&gt;Hengyuan Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lerer_A/0/1/0/all/0/1"&gt;Adam Lerer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_B/0/1/0/all/0/1"&gt;Brandon Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1"&gt;David Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pineda_L/0/1/0/all/0/1"&gt;Luis Pineda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brown_N/0/1/0/all/0/1"&gt;Noam Brown&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Foerster_J/0/1/0/all/0/1"&gt;Jakob Foerster&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Shape Modeling with Spline Partitions. (arXiv:2108.02507v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2108.02507</id>
        <link href="http://arxiv.org/abs/2108.02507"/>
        <updated>2021-08-06T00:51:47.545Z</updated>
        <summary type="html"><![CDATA[Shape modelling (with methods that output shapes) is a new and important task
in Bayesian nonparametrics and bioinformatics. In this work, we focus on
Bayesian nonparametric methods for capturing shapes by partitioning a space
using curves. In related work, the classical Mondrian process is used to
partition spaces recursively with axis-aligned cuts, and is widely applied in
multi-dimensional and relational data. The Mondrian process outputs
hyper-rectangles. Recently, the random tessellation process was introduced as a
generalization of the Mondrian process, partitioning a domain with non-axis
aligned cuts in an arbitrary dimensional space, and outputting polytopes.
Motivated by these processes, in this work, we propose a novel parallelized
Bayesian nonparametric approach to partition a domain with curves, enabling
complex data-shapes to be acquired. We apply our method to HIV-1-infected human
macrophage image dataset, and also simulated datasets sets to illustrate our
approach. We compare to support vector machines, random forests and
state-of-the-art computer vision methods such as simple linear iterative
clustering super pixel image segmentation. We develop an R package that is
available at
\url{https://github.com/ShufeiGe/Shape-Modeling-with-Spline-Partitions}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Ge_S/0/1/0/all/0/1"&gt;Shufei Ge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shijia Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Elliott_L/0/1/0/all/0/1"&gt;Lloyd Elliott&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Empirical Study of the Collapsing Problem in Semi-Supervised 2D Human Pose Estimation. (arXiv:2011.12498v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.12498</id>
        <link href="http://arxiv.org/abs/2011.12498"/>
        <updated>2021-08-06T00:51:47.538Z</updated>
        <summary type="html"><![CDATA[Semi-supervised learning aims to boost the accuracy of a model by exploring
unlabeled images. The state-of-the-art methods are consistency-based which
learn about unlabeled images by encouraging the model to give consistent
predictions for images under different augmentations. However, when applied to
pose estimation, the methods degenerate and predict every pixel in unlabeled
images as background. This is because contradictory predictions are gradually
pushed to the background class due to highly imbalanced class distribution. But
this is not an issue in supervised learning because it has accurate labels.
This inspires us to stabilize the training by obtaining reliable pseudo labels.
Specifically, we learn two networks to mutually teach each other. In
particular, for each image, we compose an easy-hard pair by applying different
augmentations and feed them to both networks. The more reliable predictions on
easy images in each network are used to teach the other network to learn about
the corresponding hard images. The approach successfully avoids degeneration
and achieves promising results on public datasets. The source code will be
released.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_R/0/1/0/all/0/1"&gt;Rongchang Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chunyu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1"&gt;Wenjun Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yizhou Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SINGA-Easy: An Easy-to-Use Framework for MultiModal Analysis. (arXiv:2108.02572v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02572</id>
        <link href="http://arxiv.org/abs/2108.02572"/>
        <updated>2021-08-06T00:51:47.491Z</updated>
        <summary type="html"><![CDATA[Deep learning has achieved great success in a wide spectrum of multimedia
applications such as image classification, natural language processing and
multimodal data analysis. Recent years have seen the development of many deep
learning frameworks that provide a high-level programming interface for users
to design models, conduct training and deploy inference. However, it remains
challenging to build an efficient end-to-end multimedia application with most
existing frameworks. Specifically, in terms of usability, it is demanding for
non-experts to implement deep learning models, obtain the right settings for
the entire machine learning pipeline, manage models and datasets, and exploit
external data sources all together. Further, in terms of adaptability, elastic
computation solutions are much needed as the actual serving workload fluctuates
constantly, and scaling the hardware resources to handle the fluctuating
workload is typically infeasible. To address these challenges, we introduce
SINGA-Easy, a new deep learning framework that provides distributed
hyper-parameter tuning at the training stage, dynamic computational cost
control at the inference stage, and intuitive user interactions with multimedia
contents facilitated by model explanation. Our experiments on the training and
deployment of multi-modality data analysis applications show that the framework
is both usable and adaptable to dynamic inference loads. We implement
SINGA-Easy on top of Apache SINGA and demonstrate our system with the entire
machine learning life cycle.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xing_N/0/1/0/all/0/1"&gt;Naili Xing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yeung_S/0/1/0/all/0/1"&gt;Sai Ho Yeung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_C/0/1/0/all/0/1"&gt;Chenghao Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ng_T/0/1/0/all/0/1"&gt;Teck Khim Ng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1"&gt;Kaiyuan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_N/0/1/0/all/0/1"&gt;Nan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Meihui Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1"&gt;Gang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ooi_B/0/1/0/all/0/1"&gt;Beng Chin Ooi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-supervised optimization of random material microstructures in the small-data regime. (arXiv:2108.02606v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2108.02606</id>
        <link href="http://arxiv.org/abs/2108.02606"/>
        <updated>2021-08-06T00:51:47.485Z</updated>
        <summary type="html"><![CDATA[While the forward and backward modeling of the process-structure-property
chain has received a lot of attention from the materials community, fewer
efforts have taken into consideration uncertainties. Those arise from a
multitude of sources and their quantification and integration in the inversion
process are essential in meeting the materials design objectives. The first
contribution of this paper is a flexible, fully probabilistic formulation of
such optimization problems that accounts for the uncertainty in the
process-structure and structure-property linkages and enables the
identification of optimal, high-dimensional, process parameters. We employ a
probabilistic, data-driven surrogate for the structure-property link which
expedites computations and enables handling of non-differential objectives. We
couple this with a novel active learning strategy, i.e. a self-supervised
collection of data, which significantly improves accuracy while requiring small
amounts of training data. We demonstrate its efficacy in optimizing the
mechanical and thermal properties of two-phase, random media but envision its
applicability encompasses a wide variety of microstructure-sensitive design
problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Rixner_M/0/1/0/all/0/1"&gt;Maximilian Rixner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Koutsourelakis_P/0/1/0/all/0/1"&gt;Phaedon-Stelios Koutsourelakis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How to avoid machine learning pitfalls: a guide for academic researchers. (arXiv:2108.02497v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02497</id>
        <link href="http://arxiv.org/abs/2108.02497"/>
        <updated>2021-08-06T00:51:47.458Z</updated>
        <summary type="html"><![CDATA[This document gives a concise outline of some of the common mistakes that
occur when using machine learning techniques, and what can be done to avoid
them. It is intended primarily as a guide for research students, and focuses on
issues that are of particular concern within academic research, such as the
need to do rigorous comparisons and reach valid conclusions. It covers five
stages of the machine learning process: what to do before model building, how
to reliably build models, how to robustly evaluate models, how to compare
models fairly, and how to report results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lones_M/0/1/0/all/0/1"&gt;Michael A. Lones&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep multi-task mining Calabi-Yau four-folds. (arXiv:2108.02221v1 [hep-th])]]></title>
        <id>http://arxiv.org/abs/2108.02221</id>
        <link href="http://arxiv.org/abs/2108.02221"/>
        <updated>2021-08-06T00:51:47.441Z</updated>
        <summary type="html"><![CDATA[We continue earlier efforts in computing the dimensions of tangent space
cohomologies of Calabi-Yau manifolds using deep learning. In this paper, we
consider the dataset of all Calabi-Yau four-folds constructed as complete
intersections in products of projective spaces. Employing neural networks
inspired by state-of-the-art computer vision architectures, we improve earlier
benchmarks and demonstrate that all four non-trivial Hodge numbers can be
learned at the same time using a multi-task architecture. With 30% (80%)
training ratio, we reach an accuracy of 100% for $h^{(1,1)}$ and 97% for
$h^{(2,1)}$ (100% for both), 81% (96%) for $h^{(3,1)}$, and 49% (83%) for
$h^{(2,2)}$. Assuming that the Euler number is known, as it is easy to compute,
and taking into account the linear constraint arising from index computations,
we get 100% total accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/hep-th/1/au:+Erbin_H/0/1/0/all/0/1"&gt;Harold Erbin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-th/1/au:+Finotello_R/0/1/0/all/0/1"&gt;Riccardo Finotello&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-th/1/au:+Schneider_R/0/1/0/all/0/1"&gt;Robin Schneider&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-th/1/au:+Tamaazousti_M/0/1/0/all/0/1"&gt;Mohamed Tamaazousti&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HyperJump: Accelerating HyperBand via Risk Modelling. (arXiv:2108.02479v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02479</id>
        <link href="http://arxiv.org/abs/2108.02479"/>
        <updated>2021-08-06T00:51:47.433Z</updated>
        <summary type="html"><![CDATA[In the literature on hyper-parameter tuning, a number of recent solutions
rely on low-fidelity observations (e.g., training with sub-sampled datasets or
for short periods of time) to extrapolate good configurations to use when
performing full training. Among these, HyperBand is arguably one of the most
popular solutions, due to its efficiency and theoretically provable robustness.
In this work, we introduce HyperJump, a new approach that builds on HyperBand's
robust search strategy and complements it with novel model-based risk analysis
techniques that accelerate the search by jumping the evaluation of low risk
configurations, i.e., configurations that are likely to be discarded by
HyperBand. We evaluate HyperJump on a suite of hyper-parameter optimization
problems and show that it provides over one-order of magnitude speed-ups on a
variety of deep-learning and kernel-based learning problems when compared to
HyperBand as well as to a number of state of the art optimizers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mendes_P/0/1/0/all/0/1"&gt;Pedro Mendes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Casimiro_M/0/1/0/all/0/1"&gt;Maria Casimiro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Romano_P/0/1/0/all/0/1"&gt;Paolo Romano&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CRPS Learning. (arXiv:2102.00968v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.00968</id>
        <link href="http://arxiv.org/abs/2102.00968"/>
        <updated>2021-08-06T00:51:47.426Z</updated>
        <summary type="html"><![CDATA[Combination and aggregation techniques can significantly improve forecast
accuracy. This also holds for probabilistic forecasting methods where
predictive distributions are combined. There are several time-varying and
adaptive weighting schemes such as Bayesian model averaging (BMA). However, the
quality of different forecasts may vary not only over time but also within the
distribution. For example, some distribution forecasts may be more accurate in
the center of the distributions, while others are better at predicting the
tails. Therefore, we introduce a new weighting method that considers the
differences in performance over time and within the distribution. We discuss
pointwise combination algorithms based on aggregation across quantiles that
optimize with respect to the continuous ranked probability score (CRPS). After
analyzing the theoretical properties of pointwise CRPS learning, we discuss B-
and P-Spline-based estimation techniques for batch and online learning, based
on quantile regression and prediction with expert advice. We prove that the
proposed fully adaptive Bernstein online aggregation (BOA) method for pointwise
CRPS online learning has optimal convergence properties. They are confirmed in
simulations and a probabilistic forecasting study for European emission
allowance (EUA) prices.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Berrisch_J/0/1/0/all/0/1"&gt;Jonathan Berrisch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ziel_F/0/1/0/all/0/1"&gt;Florian Ziel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Behavioral Cloning for Autonomous Vehicles using End-to-End Imitation Learning. (arXiv:2010.04767v4 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.04767</id>
        <link href="http://arxiv.org/abs/2010.04767"/>
        <updated>2021-08-06T00:51:47.406Z</updated>
        <summary type="html"><![CDATA[In this work, we present a lightweight pipeline for robust behavioral cloning
of a human driver using end-to-end imitation learning. The proposed pipeline
was employed to train and deploy three distinct driving behavior models onto a
simulated vehicle. The training phase comprised of data collection, balancing,
augmentation, preprocessing and training a neural network, following which, the
trained model was deployed onto the ego vehicle to predict steering commands
based on the feed from an onboard camera. A novel coupled control law was
formulated to generate longitudinal control commands on-the-go based on the
predicted steering angle and other parameters such as actual speed of the ego
vehicle and the prescribed constraints for speed and steering. We analyzed
computational efficiency of the pipeline and evaluated robustness of the
trained models through exhaustive experimentation during the deployment phase.
We also compared our approach against state-of-the-art implementation in order
to comment on its validity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Samak_T/0/1/0/all/0/1"&gt;Tanmay Vilas Samak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Samak_C/0/1/0/all/0/1"&gt;Chinmay Vilas Samak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kandhasamy_S/0/1/0/all/0/1"&gt;Sivanathan Kandhasamy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Growing an architecture for a neural network. (arXiv:2108.02231v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02231</id>
        <link href="http://arxiv.org/abs/2108.02231"/>
        <updated>2021-08-06T00:51:47.399Z</updated>
        <summary type="html"><![CDATA[We propose a new kind of automatic architecture search algorithm. The
algorithm alternates pruning connections and adding neurons, and it is not
restricted to layered architectures only. Here architecture is an arbitrary
oriented graph with some weights (along with some biases and an activation
function), so there may be no layered structure in such a network. The
algorithm minimizes the complexity of staying within a given error. We
demonstrate our algorithm on the brightness prediction problem of the next
point through the previous points on an image. Our second test problem is the
approximation of the bivariate function defining the brightness of a black and
white image. Our optimized networks significantly outperform the standard
solution for neural network architectures in both cases.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khashin_S/0/1/0/all/0/1"&gt;Sergey Khashin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shemyakova_E/0/1/0/all/0/1"&gt;Ekaterina Shemyakova&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Redesigning Fully Convolutional DenseUNets for Large Histopathology Images. (arXiv:2108.02676v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02676</id>
        <link href="http://arxiv.org/abs/2108.02676"/>
        <updated>2021-08-06T00:51:47.393Z</updated>
        <summary type="html"><![CDATA[The automated segmentation of cancer tissue in histopathology images can help
clinicians to detect, diagnose, and analyze such disease. Different from other
natural images used in many convolutional networks for benchmark,
histopathology images can be extremely large, and the cancerous patterns can
reach beyond 1000 pixels. Therefore, the well-known networks in the literature
were never conceived to handle these peculiarities. In this work, we propose a
Fully Convolutional DenseUNet that is particularly designed to solve
histopathology problems. We evaluated our network in two public pathology
datasets published as challenges in the recent MICCAI 2019: binary segmentation
in colon cancer images (DigestPath2019), and multi-class segmentation in
prostate cancer images (Gleason2019), achieving similar and better results than
the winners of the challenges, respectively. Furthermore, we discussed some
good practices in the training setup to yield the best performance and the main
challenges in these histopathology datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vigueras_Guillen_J/0/1/0/all/0/1"&gt;Juan P. Vigueras-Guill&amp;#xe9;n&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lasenby_J/0/1/0/all/0/1"&gt;Joan Lasenby&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seeliger_F/0/1/0/all/0/1"&gt;Frank Seeliger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MixMicrobleed: Multi-stage detection and segmentation of cerebral microbleeds. (arXiv:2108.02482v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02482</id>
        <link href="http://arxiv.org/abs/2108.02482"/>
        <updated>2021-08-06T00:51:47.386Z</updated>
        <summary type="html"><![CDATA[Cerebral microbleeds are small, dark, round lesions that can be visualised on
T2*-weighted MRI or other sequences sensitive to susceptibility effects. In
this work, we propose a multi-stage approach to both microbleed detection and
segmentation. First, possible microbleed locations are detected with a Mask
R-CNN technique. Second, at each possible microbleed location, a simple U-Net
performs the final segmentation. This work used the 72 subjects as training
data provided by the "Where is VALDO?" challenge of MICCAI 2021.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sanguesa_M/0/1/0/all/0/1"&gt;Marta Girones Sanguesa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kutnar_D/0/1/0/all/0/1"&gt;Denis Kutnar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Velden_B/0/1/0/all/0/1"&gt;Bas H.M. van der Velden&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kuijf_H/0/1/0/all/0/1"&gt;Hugo J. Kuijf&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adapting to Function Difficulty and Growth Conditions in Private Optimization. (arXiv:2108.02391v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02391</id>
        <link href="http://arxiv.org/abs/2108.02391"/>
        <updated>2021-08-06T00:51:47.369Z</updated>
        <summary type="html"><![CDATA[We develop algorithms for private stochastic convex optimization that adapt
to the hardness of the specific function we wish to optimize. While previous
work provide worst-case bounds for arbitrary convex functions, it is often the
case that the function at hand belongs to a smaller class that enjoys faster
rates. Concretely, we show that for functions exhibiting $\kappa$-growth around
the optimum, i.e., $f(x) \ge f(x^*) + \lambda \kappa^{-1} \|x-x^*\|_2^\kappa$
for $\kappa > 1$, our algorithms improve upon the standard
${\sqrt{d}}/{n\varepsilon}$ privacy rate to the faster
$({\sqrt{d}}/{n\varepsilon})^{\tfrac{\kappa}{\kappa - 1}}$. Crucially, they
achieve these rates without knowledge of the growth constant $\kappa$ of the
function. Our algorithms build upon the inverse sensitivity mechanism, which
adapts to instance difficulty (Asi & Duchi, 2020), and recent localization
techniques in private optimization (Feldman et al., 2020). We complement our
algorithms with matching lower bounds for these function classes and
demonstrate that our adaptive algorithm is \emph{simultaneously} (minimax)
optimal over all $\kappa \ge 1+c$ whenever $c = \Theta(1)$.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Asi_H/0/1/0/all/0/1"&gt;Hilal Asi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Levy_D/0/1/0/all/0/1"&gt;Daniel Levy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duchi_J/0/1/0/all/0/1"&gt;John Duchi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Low Rank Promoting Prior for Unsupervised Contrastive Learning. (arXiv:2108.02696v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02696</id>
        <link href="http://arxiv.org/abs/2108.02696"/>
        <updated>2021-08-06T00:51:47.359Z</updated>
        <summary type="html"><![CDATA[Unsupervised learning is just at a tipping point where it could really take
off. Among these approaches, contrastive learning has seen tremendous progress
and led to state-of-the-art performance. In this paper, we construct a novel
probabilistic graphical model that effectively incorporates the low rank
promoting prior into the framework of contrastive learning, referred to as
LORAC. In contrast to the existing conventional self-supervised approaches that
only considers independent learning, our hypothesis explicitly requires that
all the samples belonging to the same instance class lie on the same subspace
with small dimension. This heuristic poses particular joint learning
constraints to reduce the degree of freedom of the problem during the search of
the optimal network parameterization. Most importantly, we argue that the low
rank prior employed here is not unique, and many different priors can be
invoked in a similar probabilistic way, corresponding to different hypotheses
about underlying truth behind the contrastive features. Empirical evidences
show that the proposed algorithm clearly surpasses the state-of-the-art
approaches on multiple benchmarks, including image classification, object
detection, instance segmentation and keypoint detection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1"&gt;Jingyang Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_Q/0/1/0/all/0/1"&gt;Qi Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1"&gt;Yingwei Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_T/0/1/0/all/0/1"&gt;Ting Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chao_H/0/1/0/all/0/1"&gt;Hongyang Chao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mei_T/0/1/0/all/0/1"&gt;Tao Mei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pseudo-Rehearsal for Continual Learning with Normalizing Flows. (arXiv:2007.02443v4 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.02443</id>
        <link href="http://arxiv.org/abs/2007.02443"/>
        <updated>2021-08-06T00:51:47.351Z</updated>
        <summary type="html"><![CDATA[Catastrophic forgetting (CF) happens whenever a neural network overwrites
past knowledge while being trained on new tasks. Common techniques to handle CF
include regularization of the weights (using, e.g., their importance on past
tasks), and rehearsal strategies, where the network is constantly re-trained on
past data. Generative models have also been applied for the latter, in order to
have endless sources of data. In this paper, we propose a novel method that
combines the strengths of regularization and generative-based rehearsal
approaches. Our generative model consists of a normalizing flow (NF), a
probabilistic and invertible neural network, trained on the internal embeddings
of the network. By keeping a single NF conditioned on the task, we show that
our memory overhead remains constant. In addition, exploiting the invertibility
of the NF, we propose a simple approach to regularize the network's embeddings
with respect to past tasks. We show that our method performs favorably with
respect to state-of-the-art approaches in the literature, with bounded
computational power and memory overheads.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Pomponi_J/0/1/0/all/0/1"&gt;Jary Pomponi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Scardapane_S/0/1/0/all/0/1"&gt;Simone Scardapane&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Uncini_A/0/1/0/all/0/1"&gt;Aurelio Uncini&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding collections of related datasets using dependent MMD coresets. (arXiv:2006.14621v2 [stat.ME] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.14621</id>
        <link href="http://arxiv.org/abs/2006.14621"/>
        <updated>2021-08-06T00:51:47.344Z</updated>
        <summary type="html"><![CDATA[Understanding how two datasets differ can help us determine whether one
dataset under-represents certain sub-populations, and provides insights into
how well models will generalize across datasets. Representative points selected
by a maximum mean discrepency (MMD) coreset can provide interpretable summaries
of a single dataset, but are not easily compared across datasets. In this paper
we introduce dependent MMD coresets, a data summarization method for
collections of datasets that facilitates comparison of distributions. We show
that dependent MMD coresets are useful for understanding multiple related
datasets and understanding model generalization between such datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Williamson_S/0/1/0/all/0/1"&gt;Sinead A. Williamson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Henderson_J/0/1/0/all/0/1"&gt;Jette Henderson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Layer-wise training convolutional neural networks with smaller filters for human activity recognition using wearable sensors. (arXiv:2005.03948v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.03948</id>
        <link href="http://arxiv.org/abs/2005.03948"/>
        <updated>2021-08-06T00:51:47.336Z</updated>
        <summary type="html"><![CDATA[Recently, convolutional neural networks (CNNs) have set latest
state-of-the-art on various human activity recognition (HAR) datasets. However,
deep CNNs often require more computing resources, which limits their
applications in embedded HAR. Although many successful methods have been
proposed to reduce memory and FLOPs of CNNs, they often involve special network
architectures designed for visual tasks, which are not suitable for deep HAR
tasks with time series sensor signals, due to remarkable discrepancy.
Therefore, it is necessary to develop lightweight deep models to perform HAR.
As filter is the basic unit in constructing CNNs, it deserves further research
whether re-designing smaller filters is applicable for deep HAR. In the paper,
inspired by the idea, we proposed a lightweight CNN using Lego filters for HAR.
A set of lower-dimensional filters is used as Lego bricks to be stacked for
conventional filters, which does not rely on any special network structure. The
local loss function is used to train model. To our knowledge, this is the first
paper that proposes lightweight CNN for HAR in ubiquitous and wearable
computing arena. The experiment results on five public HAR datasets, UCI-HAR
dataset, OPPORTUNITY dataset, UNIMIB-SHAR dataset, PAMAP2 dataset, and WISDM
dataset collected from either smartphones or multiple sensor nodes, indicate
that our novel Lego CNN with local loss can greatly reduce memory and
computation cost over CNN, while achieving higher accuracy. That is to say, the
proposed model is smaller, faster and more accurate. Finally, we evaluate the
actual performance on an Android smartphone.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1"&gt;Yin Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Teng_Q/0/1/0/all/0/1"&gt;Qi Teng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Min_F/0/1/0/all/0/1"&gt;Fuhong Min&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1"&gt;Jun He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hindsight Value Function for Variance Reduction in Stochastic Dynamic Environment. (arXiv:2107.12216v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.12216</id>
        <link href="http://arxiv.org/abs/2107.12216"/>
        <updated>2021-08-06T00:51:47.329Z</updated>
        <summary type="html"><![CDATA[Policy gradient methods are appealing in deep reinforcement learning but
suffer from high variance of gradient estimate. To reduce the variance, the
state value function is applied commonly. However, the effect of the state
value function becomes limited in stochastic dynamic environments, where the
unexpected state dynamics and rewards will increase the variance. In this
paper, we propose to replace the state value function with a novel hindsight
value function, which leverages the information from the future to reduce the
variance of the gradient estimate for stochastic dynamic environments.

Particularly, to obtain an ideally unbiased gradient estimate, we propose an
information-theoretic approach, which optimizes the embeddings of the future to
be independent of previous actions. In our experiments, we apply the proposed
hindsight value function in stochastic dynamic environments, including
discrete-action environments and continuous-action environments. Compared with
the standard state value function, the proposed hindsight value function
consistently reduces the variance, stabilizes the training, and improves the
eventual policy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1"&gt;Jiaming Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Rui Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xishan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1"&gt;Shaohui Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yi_Q/0/1/0/all/0/1"&gt;Qi Yi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_Z/0/1/0/all/0/1"&gt;Zidong Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1"&gt;Xing Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1"&gt;Qi Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yunji Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Residual Attention: A Simple but Effective Method for Multi-Label Recognition. (arXiv:2108.02456v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02456</id>
        <link href="http://arxiv.org/abs/2108.02456"/>
        <updated>2021-08-06T00:51:47.323Z</updated>
        <summary type="html"><![CDATA[Multi-label image recognition is a challenging computer vision task of
practical use. Progresses in this area, however, are often characterized by
complicated methods, heavy computations, and lack of intuitive explanations. To
effectively capture different spatial regions occupied by objects from
different categories, we propose an embarrassingly simple module, named
class-specific residual attention (CSRA). CSRA generates class-specific
features for every category by proposing a simple spatial attention score, and
then combines it with the class-agnostic average pooling feature. CSRA achieves
state-of-the-art results on multilabel recognition, and at the same time is
much simpler than them. Furthermore, with only 4 lines of code, CSRA also leads
to consistent improvement across many diverse pretrained models and datasets
without any extra training. CSRA is both easy to implement and light in
computations, which also enjoys intuitive explanations and visualizations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1"&gt;Ke Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1"&gt;Jianxin Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SAR: Scale-Aware Restoration Learning for 3D Tumor Segmentation. (arXiv:2010.06107v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.06107</id>
        <link href="http://arxiv.org/abs/2010.06107"/>
        <updated>2021-08-06T00:51:47.316Z</updated>
        <summary type="html"><![CDATA[Automatic and accurate tumor segmentation on medical images is in high demand
to assist physicians with diagnosis and treatment. However, it is difficult to
obtain massive amounts of annotated training data required by the deep-learning
models as the manual delineation process is often tedious and expertise
required. Although self-supervised learning (SSL) scheme has been widely
adopted to address this problem, most SSL methods focus only on global
structure information, ignoring the key distinguishing features of tumor
regions: local intensity variation and large size distribution. In this paper,
we propose Scale-Aware Restoration (SAR), a SSL method for 3D tumor
segmentation. Specifically, a novel proxy task, i.e. scale discrimination, is
formulated to pre-train the 3D neural network combined with the
self-restoration task. Thus, the pre-trained model learns multi-level local
representations through multi-scale inputs. Moreover, an adversarial learning
module is further introduced to learn modality invariant representations from
multiple unlabeled source datasets. We demonstrate the effectiveness of our
methods on two downstream tasks: i) Brain tumor segmentation, ii) Pancreas
tumor segmentation. Compared with the state-of-the-art 3D SSL methods, our
proposed approach can significantly improve the segmentation accuracy. Besides,
we analyze its advantages from multiple perspectives such as data efficiency,
performance, and convergence speed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiaoman Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1"&gt;Shixiang Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yuhang Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Ya Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yanfeng Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VoxelTrack: Multi-Person 3D Human Pose Estimation and Tracking in the Wild. (arXiv:2108.02452v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02452</id>
        <link href="http://arxiv.org/abs/2108.02452"/>
        <updated>2021-08-06T00:51:47.276Z</updated>
        <summary type="html"><![CDATA[We present VoxelTrack for multi-person 3D pose estimation and tracking from a
few cameras which are separated by wide baselines. It employs a multi-branch
network to jointly estimate 3D poses and re-identification (Re-ID) features for
all people in the environment. In contrast to previous efforts which require to
establish cross-view correspondence based on noisy 2D pose estimates, it
directly estimates and tracks 3D poses from a 3D voxel-based representation
constructed from multi-view images. We first discretize the 3D space by regular
voxels and compute a feature vector for each voxel by averaging the body joint
heatmaps that are inversely projected from all views. We estimate 3D poses from
the voxel representation by predicting whether each voxel contains a particular
body joint. Similarly, a Re-ID feature is computed for each voxel which is used
to track the estimated 3D poses over time. The main advantage of the approach
is that it avoids making any hard decisions based on individual images. The
approach can robustly estimate and track 3D poses even when people are severely
occluded in some cameras. It outperforms the state-of-the-art methods by a
large margin on three public datasets including Shelf, Campus and CMU Panoptic.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yifu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chunyu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xinggang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wenyu Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1"&gt;Wenjun Zeng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Occlusion-Robust Online Multi-Object Visual Tracking using a GM-PHD Filter with CNN-Based Re-Identification. (arXiv:1912.05949v6 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1912.05949</id>
        <link href="http://arxiv.org/abs/1912.05949"/>
        <updated>2021-08-06T00:51:47.265Z</updated>
        <summary type="html"><![CDATA[We propose a novel online multi-object visual tracker using a Gaussian
mixture Probability Hypothesis Density (GM-PHD) filter and deep appearance
learning. The GM-PHD filter has a linear complexity with the number of objects
and observations while estimating the states and cardinality of time-varying
number of objects, however, it is susceptible to miss-detections and does not
include the identity of objects. We use visual-spatio-temporal information
obtained from object bounding boxes and deeply learned appearance
representations to perform estimates-to-tracks data association for target
labeling as well as formulate an augmented likelihood and then integrate into
the update step of the GM-PHD filter. We also employ additional unassigned
tracks prediction after the data association step to overcome the
susceptibility of the GM-PHD filter towards miss-detections caused by
occlusion. Extensive evaluations on MOT16, MOT17 and HiEve benchmark datasets
show that our tracker significantly outperforms several state-of-the-art
trackers in terms of tracking accuracy and identification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Baisa_N/0/1/0/all/0/1"&gt;Nathanael L. Baisa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adaptive Normalized Representation Learning for Generalizable Face Anti-Spoofing. (arXiv:2108.02667v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02667</id>
        <link href="http://arxiv.org/abs/2108.02667"/>
        <updated>2021-08-06T00:51:47.250Z</updated>
        <summary type="html"><![CDATA[With various face presentation attacks arising under unseen scenarios, face
anti-spoofing (FAS) based on domain generalization (DG) has drawn growing
attention due to its robustness. Most existing methods utilize DG frameworks to
align the features to seek a compact and generalized feature space. However,
little attention has been paid to the feature extraction process for the FAS
task, especially the influence of normalization, which also has a great impact
on the generalization of the learned representation. To address this issue, we
propose a novel perspective of face anti-spoofing that focuses on the
normalization selection in the feature extraction process. Concretely, an
Adaptive Normalized Representation Learning (ANRL) framework is devised, which
adaptively selects feature normalization methods according to the inputs,
aiming to learn domain-agnostic and discriminative representation. Moreover, to
facilitate the representation learning, Dual Calibration Constraints are
designed, including Inter-Domain Compatible loss and Inter-Class Separable
loss, which provide a better optimization direction for generalizable
representation. Extensive experiments and visualizations are presented to
demonstrate the effectiveness of our method against the SOTA competitors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Shubao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1"&gt;Ke-Yue Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_T/0/1/0/all/0/1"&gt;Taiping Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bi_M/0/1/0/all/0/1"&gt;Mingwei Bi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1"&gt;Shouhong Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jilin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1"&gt;Feiyue Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1"&gt;Lizhuang Ma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Missingness Augmentation: A General Approach for Improving Generative Imputation Models. (arXiv:2108.02566v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02566</id>
        <link href="http://arxiv.org/abs/2108.02566"/>
        <updated>2021-08-06T00:51:47.242Z</updated>
        <summary type="html"><![CDATA[Despite tremendous progress in missing data imputation task, designing new
imputation models has become more and more cumbersome but the corresponding
gains are relatively small. Is there any simple but general approach that can
exploit the existing models to further improve the quality of the imputation?
In this article, we aim to respond to this concern and propose a novel general
data augmentation method called Missingness Augmentation (MA), which can be
applied in many existing generative imputation frameworks to further improve
the performance of these models. For MA, before each training epoch, we use the
outputs of the generator to expand the incomplete samples on the fly, and then
determine a special reconstruction loss for these augmented samples. This
reconstruction loss plus the original loss constitutes the final optimization
objective of the model. It is noteworthy that MA is very efficient and does not
need to change the structure of the original model. Experimental results
demonstrate that MA can significantly improve the performance of many recently
developed generative imputation models on a variety of datasets. Our code is
available at https://github.com/WYu-Feng/Missingness-Augmentation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yufeng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1"&gt;Dan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Cong Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1"&gt;Min Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[High dimensional Bayesian Optimization Algorithm for Complex System in Time Series. (arXiv:2108.02289v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02289</id>
        <link href="http://arxiv.org/abs/2108.02289"/>
        <updated>2021-08-06T00:51:47.223Z</updated>
        <summary type="html"><![CDATA[At present, high-dimensional global optimization problems with time-series
models have received much attention from engineering fields. Since it was
proposed, Bayesian optimization has quickly become a popular and promising
approach for solving global optimization problems. However, the standard
Bayesian optimization algorithm is insufficient to solving the global optimal
solution when the model is high-dimensional. Hence, this paper presents a novel
high dimensional Bayesian optimization algorithm by considering dimension
reduction and different dimension fill-in strategies. Most existing literature
about Bayesian optimization algorithms did not discuss the sampling strategies
to optimize the acquisition function. This study proposed a new sampling method
based on both the multi-armed bandit and random search methods while optimizing
the acquisition function. Besides, based on the time-dependent or
dimension-dependent characteristics of the model, the proposed algorithm can
reduce the dimension evenly. Then, five different dimension fill-in strategies
were discussed and compared in this study. Finally, to increase the final
accuracy of the optimal solution, the proposed algorithm adds a local search
based on a series of Adam-based steps at the final stage. Our computational
experiments demonstrated that the proposed Bayesian optimization algorithm
could achieve reasonable solutions with excellent performances for high
dimensional global optimization problems with a time-series optimal control
model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yuyang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bi_K/0/1/0/all/0/1"&gt;Kaiming Bi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1"&gt;Chih-Hang J. Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ben_Arieh_D/0/1/0/all/0/1"&gt;David Ben-Arieh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sinha_A/0/1/0/all/0/1"&gt;Ashesh Sinha&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Distribution-Free, Risk-Controlling Prediction Sets. (arXiv:2101.02703v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.02703</id>
        <link href="http://arxiv.org/abs/2101.02703"/>
        <updated>2021-08-06T00:51:47.208Z</updated>
        <summary type="html"><![CDATA[While improving prediction accuracy has been the focus of machine learning in
recent years, this alone does not suffice for reliable decision-making.
Deploying learning systems in consequential settings also requires calibrating
and communicating the uncertainty of predictions. To convey instance-wise
uncertainty for prediction tasks, we show how to generate set-valued
predictions from a black-box predictor that control the expected loss on future
test points at a user-specified level. Our approach provides explicit
finite-sample guarantees for any dataset by using a holdout set to calibrate
the size of the prediction sets. This framework enables simple,
distribution-free, rigorous error control for many tasks, and we demonstrate it
in five large-scale machine learning problems: (1) classification problems
where some mistakes are more costly than others; (2) multi-label
classification, where each observation has multiple associated labels; (3)
classification problems where the labels have a hierarchical structure; (4)
image segmentation, where we wish to predict a set of pixels containing an
object of interest; and (5) protein structure prediction. Lastly, we discuss
extensions to uncertainty quantification for ranking, metric learning and
distributionally robust learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bates_S/0/1/0/all/0/1"&gt;Stephen Bates&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Angelopoulos_A/0/1/0/all/0/1"&gt;Anastasios Angelopoulos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lei_L/0/1/0/all/0/1"&gt;Lihua Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Malik_J/0/1/0/all/0/1"&gt;Jitendra Malik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jordan_M/0/1/0/all/0/1"&gt;Michael I. Jordan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LSENet: Location and Seasonality Enhanced Network for Multi-Class Ocean Front Detection. (arXiv:2108.02455v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02455</id>
        <link href="http://arxiv.org/abs/2108.02455"/>
        <updated>2021-08-06T00:51:47.189Z</updated>
        <summary type="html"><![CDATA[Ocean fronts can cause the accumulation of nutrients and affect the
propagation of underwater sound, so high-precision ocean front detection is of
great significance to the marine fishery and national defense fields. However,
the current ocean front detection methods either have low detection accuracy or
most can only detect the occurrence of ocean front by binary classification,
rarely considering the differences of the characteristics of multiple ocean
fronts in different sea areas. In order to solve the above problems, we propose
a semantic segmentation network called location and seasonality enhanced
network (LSENet) for multi-class ocean fronts detection at pixel level. In this
network, we first design a channel supervision unit structure, which integrates
the seasonal characteristics of the ocean front itself and the contextual
information to improve the detection accuracy. We also introduce a location
attention mechanism to adaptively assign attention weights to the fronts
according to their frequently occurred sea area, which can further improve the
accuracy of multi-class ocean front detection. Compared with other semantic
segmentation methods and current representative ocean front detection method,
the experimental results demonstrate convincingly that our method is more
effective.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1"&gt;Cui Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1"&gt;Hao Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1"&gt;Junyu Dong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine Learning Classification Methods and Portfolio Allocation: An Examination of Market Efficiency. (arXiv:2108.02283v1 [q-fin.GN])]]></title>
        <id>http://arxiv.org/abs/2108.02283</id>
        <link href="http://arxiv.org/abs/2108.02283"/>
        <updated>2021-08-06T00:51:47.182Z</updated>
        <summary type="html"><![CDATA[We design a novel framework to examine market efficiency through
out-of-sample (OOS) predictability. We frame the asset pricing problem as a
machine learning classification problem and construct classification models to
predict return states. The prediction-based portfolios beat the market with
significant OOS economic gains. We measure prediction accuracies directly. For
each model, we introduce a novel application of binomial test to test the
accuracy of 3.34 million return state predictions. The tests show that our
models can extract useful contents from historical information to predict
future return states. We provide unique economic insights about OOS
predictability and machine learning models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-fin/1/au:+Bai_Y/0/1/0/all/0/1"&gt;Yang Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Pukthuanthong_K/0/1/0/all/0/1"&gt;Kuntara Pukthuanthong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Distilling the Knowledge from Conditional Normalizing Flows. (arXiv:2106.12699v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.12699</id>
        <link href="http://arxiv.org/abs/2106.12699"/>
        <updated>2021-08-06T00:51:47.176Z</updated>
        <summary type="html"><![CDATA[Normalizing flows are a powerful class of generative models demonstrating
strong performance in several speech and vision problems. In contrast to other
generative models, normalizing flows are latent variable models with tractable
likelihoods and allow for stable training. However, they have to be carefully
designed to represent invertible functions with efficient Jacobian determinant
calculation. In practice, these requirements lead to overparameterized and
sophisticated architectures that are inferior to alternative feed-forward
models in terms of inference time and memory consumption. In this work, we
investigate whether one can distill flow-based models into more efficient
alternatives. We provide a positive answer to this question by proposing a
simple distillation approach and demonstrating its effectiveness on
state-of-the-art conditional flow-based models for image super-resolution and
speech synthesis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Baranchuk_D/0/1/0/all/0/1"&gt;Dmitry Baranchuk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aliev_V/0/1/0/all/0/1"&gt;Vladimir Aliev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Babenko_A/0/1/0/all/0/1"&gt;Artem Babenko&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Computer-Aided Diagnosis System for Breast Pathology: A Deep Learning Approach with Model Interpretability from Pathological Perspective. (arXiv:2108.02656v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02656</id>
        <link href="http://arxiv.org/abs/2108.02656"/>
        <updated>2021-08-06T00:51:47.169Z</updated>
        <summary type="html"><![CDATA[Objective: We develop a computer-aided diagnosis (CAD) system using deep
learning approaches for lesion detection and classification on whole-slide
images (WSIs) with breast cancer. The deep features being distinguishing in
classification from the convolutional neural networks (CNN) are demonstrated in
this study to provide comprehensive interpretability for the proposed CAD
system using pathological knowledge. Methods: In the experiment, a total of 186
slides of WSIs were collected and classified into three categories:
Non-Carcinoma, Ductal Carcinoma in Situ (DCIS), and Invasive Ductal Carcinoma
(IDC). Instead of conducting pixel-wise classification into three classes
directly, we designed a hierarchical framework with the multi-view scheme that
performs lesion detection for region proposal at higher magnification first and
then conducts lesion classification at lower magnification for each detected
lesion. Results: The slide-level accuracy rate for three-category
classification reaches 90.8% (99/109) through 5-fold cross-validation and
achieves 94.8% (73/77) on the testing set. The experimental results show that
the morphological characteristics and co-occurrence properties learned by the
deep learning models for lesion classification are accordant with the clinical
rules in diagnosis. Conclusion: The pathological interpretability of the deep
features not only enhances the reliability of the proposed CAD system to gain
acceptance from medical specialists, but also facilitates the development of
deep learning frameworks for various tasks in pathology. Significance: This
paper presents a CAD system for pathological image analysis, which fills the
clinical requirements and can be accepted by medical specialists with providing
its interpretability from the pathological perspective.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1"&gt;Wei-Wen Hsu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yongfang Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hao_C/0/1/0/all/0/1"&gt;Chang Hao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1"&gt;Yu-Ling Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1"&gt;Xiang Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1"&gt;Yun Shao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xueli Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1"&gt;Tao He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tai_Y/0/1/0/all/0/1"&gt;Yanhong Tai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On spectral algorithms for community detection in stochastic blockmodel graphs with vertex covariates. (arXiv:2007.02156v3 [cs.SI] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2007.02156</id>
        <link href="http://arxiv.org/abs/2007.02156"/>
        <updated>2021-08-06T00:51:47.161Z</updated>
        <summary type="html"><![CDATA[In network inference applications, it is often desirable to detect community
structure, namely to cluster vertices into groups, or blocks, according to some
measure of similarity. Beyond mere adjacency matrices, many real networks also
involve vertex covariates that carry key information about underlying block
structure in graphs. To assess the effects of such covariates on block
recovery, we present a comparative analysis of two model-based spectral
algorithms for clustering vertices in stochastic blockmodel graphs with vertex
covariates. The first algorithm uses only the adjacency matrix, and directly
estimates the block assignments. The second algorithm incorporates both the
adjacency matrix and the vertex covariates into the estimation of block
assignments, and moreover quantifies the explicit impact of the vertex
covariates on the resulting estimate of the block assignments. We employ
Chernoff information to analytically compare the algorithms' performance and
derive the information-theoretic Chernoff ratio for certain models of interest.
Analytic results and simulations suggest that the second algorithm is often
preferred: we can often better estimate the induced block assignments by first
estimating the effect of vertex covariates. In addition, real data examples
also indicate that the second algorithm has the advantages of revealing
underlying block structure and taking observed vertex heterogeneity into
account in real applications. Our findings emphasize the importance of
distinguishing between observed and unobserved factors that can affect block
structure in graphs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mu_C/0/1/0/all/0/1"&gt;Cong Mu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mele_A/0/1/0/all/0/1"&gt;Angelo Mele&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hao_L/0/1/0/all/0/1"&gt;Lingxin Hao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cape_J/0/1/0/all/0/1"&gt;Joshua Cape&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Athreya_A/0/1/0/all/0/1"&gt;Avanti Athreya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Priebe_C/0/1/0/all/0/1"&gt;Carey E. Priebe&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluation of Audio-Visual Alignments in Visually Grounded Speech Models. (arXiv:2108.02562v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02562</id>
        <link href="http://arxiv.org/abs/2108.02562"/>
        <updated>2021-08-06T00:51:47.141Z</updated>
        <summary type="html"><![CDATA[Systems that can find correspondences between multiple modalities, such as
between speech and images, have great potential to solve different recognition
and data analysis tasks in an unsupervised manner. This work studies multimodal
learning in the context of visually grounded speech (VGS) models, and focuses
on their recently demonstrated capability to extract spatiotemporal alignments
between spoken words and the corresponding visual objects without ever been
explicitly trained for object localization or word recognition. As the main
contributions, we formalize the alignment problem in terms of an audiovisual
alignment tensor that is based on earlier VGS work, introduce systematic
metrics for evaluating model performance in aligning visual objects and spoken
words, and propose a new VGS model variant for the alignment task utilizing
cross-modal attention layer. We test our model and a previously proposed model
in the alignment task using SPEECH-COCO captions coupled with MSCOCO images. We
compare the alignment performance using our proposed evaluation metrics to the
semantic retrieval task commonly used to evaluate VGS models. We show that
cross-modal attention layer not only helps the model to achieve higher semantic
cross-modal retrieval performance, but also leads to substantial improvements
in the alignment performance between image object and spoken words.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khorrami_K/0/1/0/all/0/1"&gt;Khazar Khorrami&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rasanen_O/0/1/0/all/0/1"&gt;Okko R&amp;#xe4;s&amp;#xe4;nen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FMMformer: Efficient and Flexible Transformer via Decomposed Near-field and Far-field Attention. (arXiv:2108.02347v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02347</id>
        <link href="http://arxiv.org/abs/2108.02347"/>
        <updated>2021-08-06T00:51:47.134Z</updated>
        <summary type="html"><![CDATA[We propose FMMformers, a class of efficient and flexible transformers
inspired by the celebrated fast multipole method (FMM) for accelerating
interacting particle simulation. FMM decomposes particle-particle interaction
into near-field and far-field components and then performs direct and
coarse-grained computation, respectively. Similarly, FMMformers decompose the
attention into near-field and far-field attention, modeling the near-field
attention by a banded matrix and the far-field attention by a low-rank matrix.
Computing the attention matrix for FMMformers requires linear complexity in
computational time and memory footprint with respect to the sequence length. In
contrast, standard transformers suffer from quadratic complexity. We analyze
and validate the advantage of FMMformers over the standard transformer on the
Long Range Arena and language modeling benchmarks. FMMformers can even
outperform the standard transformer in terms of accuracy by a significant
margin. For instance, FMMformers achieve an average classification accuracy of
$60.74\%$ over the five Long Range Arena tasks, which is significantly better
than the standard transformer's average accuracy of $58.70\%$.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1"&gt;Tan M. Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suliafu_V/0/1/0/all/0/1"&gt;Vai Suliafu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Osher_S/0/1/0/all/0/1"&gt;Stanley J. Osher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Long Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1"&gt;Bao Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BOSS: Bidirectional One-Shot Synthesis of Adversarial Examples. (arXiv:2108.02756v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02756</id>
        <link href="http://arxiv.org/abs/2108.02756"/>
        <updated>2021-08-06T00:51:47.128Z</updated>
        <summary type="html"><![CDATA[The design of additive imperceptible perturbations to the inputs of deep
classifiers to maximize their misclassification rates is a central focus of
adversarial machine learning. An alternative approach is to synthesize
adversarial examples from scratch using GAN-like structures, albeit with the
use of large amounts of training data. By contrast, this paper considers
one-shot synthesis of adversarial examples; the inputs are synthesized from
scratch to induce arbitrary soft predictions at the output of pre-trained
models, while simultaneously maintaining high similarity to specified inputs.
To this end, we present a problem that encodes objectives on the distance
between the desired and output distributions of the trained model and the
similarity between such inputs and the synthesized examples. We prove that the
formulated problem is NP-complete. Then, we advance a generative approach to
the solution in which the adversarial examples are obtained as the output of a
generative network whose parameters are iteratively updated by optimizing
surrogate loss functions for the dual-objective. We demonstrate the generality
and versatility of the framework and approach proposed through applications to
the design of targeted adversarial attacks, generation of decision boundary
samples, and synthesis of low confidence classification inputs. The approach is
further extended to an ensemble of models with different soft output
specifications. The experimental results verify that the targeted and
confidence reduction attack methods developed perform on par with
state-of-the-art algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alkhouri_I/0/1/0/all/0/1"&gt;Ismail Alkhouri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Velasquez_A/0/1/0/all/0/1"&gt;Alvaro Velasquez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Atia_G/0/1/0/all/0/1"&gt;George Atia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Redesigning Fully Convolutional DenseUNets for Large Histopathology Images. (arXiv:2108.02676v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02676</id>
        <link href="http://arxiv.org/abs/2108.02676"/>
        <updated>2021-08-06T00:51:47.121Z</updated>
        <summary type="html"><![CDATA[The automated segmentation of cancer tissue in histopathology images can help
clinicians to detect, diagnose, and analyze such disease. Different from other
natural images used in many convolutional networks for benchmark,
histopathology images can be extremely large, and the cancerous patterns can
reach beyond 1000 pixels. Therefore, the well-known networks in the literature
were never conceived to handle these peculiarities. In this work, we propose a
Fully Convolutional DenseUNet that is particularly designed to solve
histopathology problems. We evaluated our network in two public pathology
datasets published as challenges in the recent MICCAI 2019: binary segmentation
in colon cancer images (DigestPath2019), and multi-class segmentation in
prostate cancer images (Gleason2019), achieving similar and better results than
the winners of the challenges, respectively. Furthermore, we discussed some
good practices in the training setup to yield the best performance and the main
challenges in these histopathology datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vigueras_Guillen_J/0/1/0/all/0/1"&gt;Juan P. Vigueras-Guill&amp;#xe9;n&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lasenby_J/0/1/0/all/0/1"&gt;Joan Lasenby&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seeliger_F/0/1/0/all/0/1"&gt;Frank Seeliger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Token Shift Transformer for Video Classification. (arXiv:2108.02432v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02432</id>
        <link href="http://arxiv.org/abs/2108.02432"/>
        <updated>2021-08-06T00:51:47.114Z</updated>
        <summary type="html"><![CDATA[Transformer achieves remarkable successes in understanding 1 and
2-dimensional signals (e.g., NLP and Image Content Understanding). As a
potential alternative to convolutional neural networks, it shares merits of
strong interpretability, high discriminative power on hyper-scale data, and
flexibility in processing varying length inputs. However, its encoders
naturally contain computational intensive operations such as pair-wise
self-attention, incurring heavy computational burden when being applied on the
complex 3-dimensional video signals.

This paper presents Token Shift Module (i.e., TokShift), a novel,
zero-parameter, zero-FLOPs operator, for modeling temporal relations within
each transformer encoder. Specifically, the TokShift barely temporally shifts
partial [Class] token features back-and-forth across adjacent frames. Then, we
densely plug the module into each encoder of a plain 2D vision transformer for
learning 3D video representation. It is worth noticing that our TokShift
transformer is a pure convolutional-free video transformer pilot with
computational efficiency for video understanding. Experiments on standard
benchmarks verify its robustness, effectiveness, and efficiency. Particularly,
with input clips of 8/12 frames, the TokShift transformer achieves SOTA
precision: 79.83%/80.40% on the Kinetics-400, 66.56% on EGTEA-Gaze+, and 96.80%
on UCF-101 datasets, comparable or better than existing SOTA convolutional
counterparts. Our code is open-sourced in:
https://github.com/VideoNetworks/TokShift-Transformer.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Hao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hao_Y/0/1/0/all/0/1"&gt;Yanbin Hao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ngo_C/0/1/0/all/0/1"&gt;Chong-Wah Ngo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Memory AMP. (arXiv:2012.10861v4 [cs.IT] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.10861</id>
        <link href="http://arxiv.org/abs/2012.10861"/>
        <updated>2021-08-06T00:51:47.077Z</updated>
        <summary type="html"><![CDATA[Approximate message passing (AMP) is a low-cost iterative
parameter-estimation technique for certain high-dimensional linear systems with
non-Gaussian distributions. However, AMP only applies to independent
identically distributed (IID) transform matrices, but may become unreliable
(e.g. perform poorly or even diverge) for other matrix ensembles, especially
for ill-conditioned ones. Orthogonal/vector AMP (OAMP/VAMP) was proposed for
general right-unitarily-invariant matrices to handle this difficulty. However,
the Bayes-optimal OAMP/VAMP requires a high-complexity linear minimum mean
square error (MMSE) estimator. This limits the application of OAMP/VAMP to
large-scale systems.

To solve the disadvantages of AMP and OAMP/VAMP, this paper proposes a memory
AMP (MAMP) framework under an orthogonality principle, which guarantees the
asymptotic IID Gaussianity of estimation errors in MAMP. We present an
orthogonalization procedure for the local memory estimators to realize the
required orthogonality for MAMP. Furthermore, we propose a Bayes-optimal MAMP
(BO-MAMP), in which a long-memory matched filter is proposed for interference
suppression. The complexity of BO-MAMP is comparable to AMP. A state evolution
is derived to asymptotically characterize the performance of BO-MAMP. Based on
state evolution, the relaxation parameters and damping vector in BO-MAMP are
optimized. For all right-unitarily-invariant matrices, the optimized BO-MAMP
converges to the high-complexity OAMP/VAMP, and thus is Bayes-optimal if it has
a unique fixed point. Finally, simulations are provided to verify the validity
and accuracy of the theoretical results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Lei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1"&gt;Shunqi Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kurkoski_B/0/1/0/all/0/1"&gt;Brian M. Kurkoski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Role-based lateral movement detection with unsupervised learning. (arXiv:2108.02713v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2108.02713</id>
        <link href="http://arxiv.org/abs/2108.02713"/>
        <updated>2021-08-06T00:51:47.013Z</updated>
        <summary type="html"><![CDATA[Adversarial lateral movement via compromised accounts remains difficult to
discover via traditional rule-based defenses because it generally lacks
explicit indicators of compromise. We propose a behavior-based, unsupervised
framework comprising two methods of lateral movement detection on enterprise
networks: one aimed at generic lateral movement via either exploit or
authenticated connections, and one targeting the specific techniques of process
injection and hijacking. The first method is based on the premise that the role
of a system---the functions it performs on the network---determines the roles
of the systems it should make connections with. The adversary meanwhile might
move between any systems whatever, possibly seeking out systems with unusual
roles that facilitate certain accesses. We use unsupervised learning to cluster
systems according to role and identify connections to systems with novel roles
as potentially malicious. The second method is based on the premise that the
temporal patterns of inter-system processes that facilitate these connections
depend on the roles of the systems involved. If a process is compromised by an
attacker, these normal patterns might be disrupted in discernible ways. We
apply frequent-itemset mining to process sequences to establish regular
patterns of communication between systems based on role, and identify rare
process sequences as signalling potentially malicious connections.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Powell_B/0/1/0/all/0/1"&gt;Brian A. Powell&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ImageNet-21K Pretraining for the Masses. (arXiv:2104.10972v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.10972</id>
        <link href="http://arxiv.org/abs/2104.10972"/>
        <updated>2021-08-06T00:51:47.007Z</updated>
        <summary type="html"><![CDATA[ImageNet-1K serves as the primary dataset for pretraining deep learning
models for computer vision tasks. ImageNet-21K dataset, which is bigger and
more diverse, is used less frequently for pretraining, mainly due to its
complexity, low accessibility, and underestimation of its added value. This
paper aims to close this gap, and make high-quality efficient pretraining on
ImageNet-21K available for everyone. Via a dedicated preprocessing stage,
utilization of WordNet hierarchical structure, and a novel training scheme
called semantic softmax, we show that various models significantly benefit from
ImageNet-21K pretraining on numerous datasets and tasks, including small
mobile-oriented models. We also show that we outperform previous ImageNet-21K
pretraining schemes for prominent new models like ViT and Mixer. Our proposed
pretraining pipeline is efficient, accessible, and leads to SoTA reproducible
results, from a publicly available dataset. The training code and pretrained
models are available at: https://github.com/Alibaba-MIIL/ImageNet21K]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ridnik_T/0/1/0/all/0/1"&gt;Tal Ridnik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ben_Baruch_E/0/1/0/all/0/1"&gt;Emanuel Ben-Baruch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Noy_A/0/1/0/all/0/1"&gt;Asaf Noy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zelnik_Manor_L/0/1/0/all/0/1"&gt;Lihi Zelnik-Manor&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bridging the Gap between Spatial and Spectral Domains: A Unified Framework for Graph Neural Networks. (arXiv:2107.10234v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.10234</id>
        <link href="http://arxiv.org/abs/2107.10234"/>
        <updated>2021-08-06T00:51:47.000Z</updated>
        <summary type="html"><![CDATA[Deep learning's performance has been extensively recognized recently. Graph
neural networks (GNNs) are designed to deal with graph-structural data that
classical deep learning does not easily manage. Since most GNNs were created
using distinct theories, direct comparisons are impossible. Prior research has
primarily concentrated on categorizing existing models, with little attention
paid to their intrinsic connections. The purpose of this study is to establish
a unified framework that integrates GNNs based on spectral graph and
approximation theory. The framework incorporates a strong integration between
spatial- and spectral-based GNNs while tightly associating approaches that
exist within each respective domain.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhiqian Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1"&gt;Fanglan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_T/0/1/0/all/0/1"&gt;Taoran Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_K/0/1/0/all/0/1"&gt;Kaiqun Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1"&gt;Liang Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1"&gt;Feng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1"&gt;Lingfei Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aggarwal_C/0/1/0/all/0/1"&gt;Charu Aggarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1"&gt;Chang-Tien Lu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rotaflip: A New CNN Layer for Regularization and Rotational Invariance in Medical Images. (arXiv:2108.02704v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02704</id>
        <link href="http://arxiv.org/abs/2108.02704"/>
        <updated>2021-08-06T00:51:46.986Z</updated>
        <summary type="html"><![CDATA[Regularization in convolutional neural networks (CNNs) is usually addressed
with dropout layers. However, dropout is sometimes detrimental in the
convolutional part of a CNN as it simply sets to zero a percentage of pixels in
the feature maps, adding unrepresentative examples during training. Here, we
propose a CNN layer that performs regularization by applying random rotations
of reflections to a small percentage of feature maps after every convolutional
layer. We prove how this concept is beneficial for images with orientational
symmetries, such as in medical images, as it provides a certain degree of
rotational invariance. We tested this method in two datasets, a patch-based set
of histopathology images (PatchCamelyon) to perform classification using a
generic DenseNet, and a set of specular microscopy images of the corneal
endothelium to perform segmentation using a tailored U-net, improving the
performance in both cases.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vigueras_Guillen_J/0/1/0/all/0/1"&gt;Juan P. Vigueras-Guill&amp;#xe9;n&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lasenby_J/0/1/0/all/0/1"&gt;Joan Lasenby&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seeliger_F/0/1/0/all/0/1"&gt;Frank Seeliger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Privately Learning Subspaces. (arXiv:2106.00001v2 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.00001</id>
        <link href="http://arxiv.org/abs/2106.00001"/>
        <updated>2021-08-06T00:51:46.961Z</updated>
        <summary type="html"><![CDATA[Private data analysis suffers a costly curse of dimensionality. However, the
data often has an underlying low-dimensional structure. For example, when
optimizing via gradient descent, the gradients often lie in or near a
low-dimensional subspace. If that low-dimensional structure can be identified,
then we can avoid paying (in terms of privacy or accuracy) for the high ambient
dimension.

We present differentially private algorithms that take input data sampled
from a low-dimensional linear subspace (possibly with a small amount of error)
and output that subspace (or an approximation to it). These algorithms can
serve as a pre-processing step for other procedures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Singhal_V/0/1/0/all/0/1"&gt;Vikrant Singhal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Steinke_T/0/1/0/all/0/1"&gt;Thomas Steinke&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Extending Neural P-frame Codecs for B-frame Coding. (arXiv:2104.00531v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.00531</id>
        <link href="http://arxiv.org/abs/2104.00531"/>
        <updated>2021-08-06T00:51:46.953Z</updated>
        <summary type="html"><![CDATA[While most neural video codecs address P-frame coding (predicting each frame
from past ones), in this paper we address B-frame compression (predicting
frames using both past and future reference frames). Our B-frame solution is
based on the existing P-frame methods. As a result, B-frame coding capability
can easily be added to an existing neural codec. The basic idea of our B-frame
coding method is to interpolate the two reference frames to generate a single
reference frame and then use it together with an existing P-frame codec to
encode the input B-frame. Our studies show that the interpolated frame is a
much better reference for the P-frame codec compared to using the previous
frame as is usually done. Our results show that using the proposed method with
an existing P-frame codec can lead to 28.5%saving in bit-rate on the UVG
dataset compared to the P-frame codec while generating the same video quality.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Pourreza_R/0/1/0/all/0/1"&gt;Reza Pourreza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cohen_T/0/1/0/all/0/1"&gt;Taco S Cohen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GIFAIR-FL: An Approach for Group and Individual Fairness in Federated Learning. (arXiv:2108.02741v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02741</id>
        <link href="http://arxiv.org/abs/2108.02741"/>
        <updated>2021-08-06T00:51:46.945Z</updated>
        <summary type="html"><![CDATA[In this paper we propose \texttt{GIFAIR-FL}: an approach that imposes group
and individual fairness to federated learning settings. By adding a
regularization term, our algorithm penalizes the spread in the loss of client
groups to drive the optimizer to fair solutions. Theoretically, we show
convergence in non-convex and strongly convex settings. Our convergence
guarantees hold for both $i.i.d.$ and non-$i.i.d.$ data. To demonstrate the
empirical performance of our algorithm, we apply our method on image
classification and text prediction tasks. Compared to existing algorithms, our
method shows improved fairness results while retaining superior or similar
prediction accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yue_X/0/1/0/all/0/1"&gt;Xubo Yue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nouiehed_M/0/1/0/all/0/1"&gt;Maher Nouiehed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kontar_R/0/1/0/all/0/1"&gt;Raed Al Kontar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lyapunov Robust Constrained-MDPs: Soft-Constrained Robustly Stable Policy Optimization under Model Uncertainty. (arXiv:2108.02701v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02701</id>
        <link href="http://arxiv.org/abs/2108.02701"/>
        <updated>2021-08-06T00:51:46.938Z</updated>
        <summary type="html"><![CDATA[Safety and robustness are two desired properties for any reinforcement
learning algorithm. CMDPs can handle additional safety constraints and RMDPs
can perform well under model uncertainties. In this paper, we propose to unite
these two frameworks resulting in robust constrained MDPs (RCMDPs). The
motivation is to develop a framework that can satisfy safety constraints while
also simultaneously offer robustness to model uncertainties. We develop the
RCMDP objective, derive gradient update formula to optimize this objective and
then propose policy gradient based algorithms. We also independently propose
Lyapunov based reward shaping for RCMDPs, yielding better stability and
convergence properties.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Russel_R/0/1/0/all/0/1"&gt;Reazul Hasan Russel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Benosman_M/0/1/0/all/0/1"&gt;Mouhacine Benosman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baar_J/0/1/0/all/0/1"&gt;Jeroen Van Baar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Corcodel_R/0/1/0/all/0/1"&gt;Radu Corcodel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Solving Schr\"odinger Bridges via Maximum Likelihood. (arXiv:2106.02081v4 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.02081</id>
        <link href="http://arxiv.org/abs/2106.02081"/>
        <updated>2021-08-06T00:51:46.931Z</updated>
        <summary type="html"><![CDATA[The Schr\"odinger bridge problem (SBP) finds the most likely stochastic
evolution between two probability distributions given a prior stochastic
evolution. As well as applications in the natural sciences, problems of this
kind have important applications in machine learning such as dataset alignment
and hypothesis testing. Whilst the theory behind this problem is relatively
mature, scalable numerical recipes to estimate the Schr\"odinger bridge remain
an active area of research. We prove an equivalence between the SBP and maximum
likelihood estimation enabling direct application of successful machine
learning techniques. We propose a numerical procedure to estimate SBPs using
Gaussian process and demonstrate the practical usage of our approach in
numerical simulations and experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Vargas_F/0/1/0/all/0/1"&gt;Francisco Vargas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Thodoroff_P/0/1/0/all/0/1"&gt;Pierre Thodoroff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lawrence_N/0/1/0/all/0/1"&gt;Neil D. Lawrence&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Lamacraft_A/0/1/0/all/0/1"&gt;Austen Lamacraft&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Test Score Algorithms for Budgeted Stochastic Utility Maximization. (arXiv:2012.15194v2 [cs.DS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.15194</id>
        <link href="http://arxiv.org/abs/2012.15194"/>
        <updated>2021-08-06T00:51:46.910Z</updated>
        <summary type="html"><![CDATA[Motivated by recent developments in designing algorithms based on individual
item scores for solving utility maximization problems, we study the framework
of using test scores, defined as a statistic of observed individual item
performance data, for solving the budgeted stochastic utility maximization
problem. We extend an existing scoring mechanism, namely the replication test
scores, to incorporate heterogeneous item costs as well as item values. We show
that a natural greedy algorithm that selects items solely based on their
replication test scores outputs solutions within a constant factor of the
optimum for a broad class of utility functions. Our algorithms and
approximation guarantees assume that test scores are noisy estimates of certain
expected values with respect to marginal distributions of individual item
values, thus making our algorithms practical and extending previous work that
assumes noiseless estimates. Moreover, we show how our algorithm can be adapted
to the setting where items arrive in a streaming fashion while maintaining the
same approximation guarantee. We present numerical results, using synthetic
data and data sets from the Academia.StackExchange Q&A forum, which show that
our test score algorithm can achieve competitiveness, and in some cases better
performance than a benchmark algorithm that requires access to a value oracle
to evaluate function values.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1"&gt;Dabeen Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vojnovic_M/0/1/0/all/0/1"&gt;Milan Vojnovic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yun_S/0/1/0/all/0/1"&gt;Se-Young Yun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Inserting Information Bottlenecks for Attribution in Transformers. (arXiv:2012.13838v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.13838</id>
        <link href="http://arxiv.org/abs/2012.13838"/>
        <updated>2021-08-06T00:51:46.903Z</updated>
        <summary type="html"><![CDATA[Pretrained transformers achieve the state of the art across tasks in natural
language processing, motivating researchers to investigate their inner
mechanisms. One common direction is to understand what features are important
for prediction. In this paper, we apply information bottlenecks to analyze the
attribution of each feature for prediction on a black-box model. We use BERT as
the example and evaluate our approach both quantitatively and qualitatively. We
show the effectiveness of our method in terms of attribution and the ability to
provide insight into how information flows through layers. We demonstrate that
our technique outperforms two competitive methods in degradation tests on four
datasets. Code is available at https://github.com/bazingagin/IBA.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1"&gt;Zhiying Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_R/0/1/0/all/0/1"&gt;Raphael Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xin_J/0/1/0/all/0/1"&gt;Ji Xin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1"&gt;Jimmy Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data Streaming and Traffic Gathering in Mesh-based NoC for Deep Neural Network Acceleration. (arXiv:2108.02569v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02569</id>
        <link href="http://arxiv.org/abs/2108.02569"/>
        <updated>2021-08-06T00:51:46.896Z</updated>
        <summary type="html"><![CDATA[The increasing popularity of deep neural network (DNN) applications demands
high computing power and efficient hardware accelerator architecture. DNN
accelerators use a large number of processing elements (PEs) and on-chip memory
for storing weights and other parameters. As the communication backbone of a
DNN accelerator, networks-on-chip (NoC) play an important role in supporting
various dataflow patterns and enabling processing with communication
parallelism in a DNN accelerator. However, the widely used mesh-based NoC
architectures inherently cannot support the efficient one-to-many and
many-to-one traffic largely existing in DNN workloads. In this paper, we
propose a modified mesh architecture with a one-way/two-way streaming bus to
speedup one-to-many (multicast) traffic, and the use of gather packets to
support many-to-one (gather) traffic. The analysis of the runtime latency of a
convolutional layer shows that the two-way streaming architecture achieves
better improvement than the one-way streaming architecture for an Output
Stationary (OS) dataflow architecture. The simulation results demonstrate that
the gather packets can help to reduce the runtime latency up to 1.8 times and
network power consumption up to 1.7 times, compared with the repetitive unicast
method on modified mesh architectures supporting two-way streaming.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tiwari_B/0/1/0/all/0/1"&gt;Binayak Tiwari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1"&gt;Mei Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaohang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1"&gt;Yingtao Jiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A variational Bayesian spatial interaction model for estimating revenue and demand at business facilities. (arXiv:2108.02594v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2108.02594</id>
        <link href="http://arxiv.org/abs/2108.02594"/>
        <updated>2021-08-06T00:51:46.890Z</updated>
        <summary type="html"><![CDATA[We study the problem of estimating potential revenue or demand at business
facilities and understanding its generating mechanism. This problem arises in
different fields such as operation research or urban science, and more
generally, it is crucial for businesses' planning and decision making. We
develop a Bayesian spatial interaction model, henceforth BSIM, which provides
probabilistic predictions about revenues generated by a particular business
location provided their features and the potential customers' characteristics
in a given region. BSIM explicitly accounts for the competition among the
competitive facilities through a probability value determined by evaluating a
store-specific Gaussian distribution at a given customer location. We propose a
scalable variational inference framework that, while being significantly faster
than competing Markov Chain Monte Carlo inference schemes, exhibits comparable
performances in terms of parameters identification and uncertainty
quantification. We demonstrate the benefits of BSIM in various synthetic
settings characterised by an increasing number of stores and customers.
Finally, we construct a real-world, large spatial dataset for pub activities in
London, UK, which includes over 1,500 pubs and 150,000 customer regions. We
demonstrate how BSIM outperforms competing approaches on this large dataset in
terms of prediction performances while providing results that are both
interpretable and consistent with related indicators observed for the London
region.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Perera_S/0/1/0/all/0/1"&gt;Shanaka Perera&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Aglietti_V/0/1/0/all/0/1"&gt;Virginia Aglietti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Damoulas_T/0/1/0/all/0/1"&gt;Theodoros Damoulas&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Grad-TTS: A Diffusion Probabilistic Model for Text-to-Speech. (arXiv:2105.06337v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.06337</id>
        <link href="http://arxiv.org/abs/2105.06337"/>
        <updated>2021-08-06T00:51:46.883Z</updated>
        <summary type="html"><![CDATA[Recently, denoising diffusion probabilistic models and generative score
matching have shown high potential in modelling complex data distributions
while stochastic calculus has provided a unified point of view on these
techniques allowing for flexible inference schemes. In this paper we introduce
Grad-TTS, a novel text-to-speech model with score-based decoder producing
mel-spectrograms by gradually transforming noise predicted by encoder and
aligned with text input by means of Monotonic Alignment Search. The framework
of stochastic differential equations helps us to generalize conventional
diffusion probabilistic models to the case of reconstructing data from noise
with different parameters and allows to make this reconstruction flexible by
explicitly controlling trade-off between sound quality and inference speed.
Subjective human evaluation shows that Grad-TTS is competitive with
state-of-the-art text-to-speech approaches in terms of Mean Opinion Score. We
will make the code publicly available shortly.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Popov_V/0/1/0/all/0/1"&gt;Vadim Popov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vovk_I/0/1/0/all/0/1"&gt;Ivan Vovk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gogoryan_V/0/1/0/all/0/1"&gt;Vladimir Gogoryan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sadekova_T/0/1/0/all/0/1"&gt;Tasnima Sadekova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kudinov_M/0/1/0/all/0/1"&gt;Mikhail Kudinov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Elect. (arXiv:2108.02768v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02768</id>
        <link href="http://arxiv.org/abs/2108.02768"/>
        <updated>2021-08-06T00:51:46.864Z</updated>
        <summary type="html"><![CDATA[Voting systems have a wide range of applications including recommender
systems, web search, product design and elections. Limited by the lack of
general-purpose analytical tools, it is difficult to hand-engineer desirable
voting rules for each use case. For this reason, it is appealing to
automatically discover voting rules geared towards each scenario. In this
paper, we show that set-input neural network architectures such as Set
Transformers, fully-connected graph networks and DeepSets are both
theoretically and empirically well-suited for learning voting rules. In
particular, we show that these network models can not only mimic a number of
existing voting rules to compelling accuracy --- both position-based (such as
Plurality and Borda) and comparison-based (such as Kemeny, Copeland and
Maximin) --- but also discover near-optimal voting rules that maximize
different social welfare functions. Furthermore, the learned voting rules
generalize well to different voter utility distributions and election sizes
unseen during training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Anil_C/0/1/0/all/0/1"&gt;Cem Anil&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bao_X/0/1/0/all/0/1"&gt;Xuchan Bao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Nonperturbative renormalization for the neural network-QFT correspondence. (arXiv:2108.01403v1 [hep-th] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2108.01403</id>
        <link href="http://arxiv.org/abs/2108.01403"/>
        <updated>2021-08-06T00:51:46.858Z</updated>
        <summary type="html"><![CDATA[In a recent work arXiv:2008.08601, Halverson, Maiti and Stoner proposed a
description of neural networks in terms of a Wilsonian effective field theory.
The infinite-width limit is mapped to a free field theory, while finite $N$
corrections are taken into account by interactions (non-Gaussian terms in the
action). In this paper, we study two related aspects of this correspondence.
First, we comment on the concepts of locality and power-counting in this
context. Indeed, these usual space-time notions may not hold for neural
networks (since inputs can be arbitrary), however, the renormalization group
provides natural notions of locality and scaling. Moreover, we comment on
several subtleties, for example, that data components may not have a
permutation symmetry: in that case, we argue that random tensor field theories
could provide a natural generalization. Second, we improve the perturbative
Wilsonian renormalization from arXiv:2008.08601 by providing an analysis in
terms of the nonperturbative renormalization group using the Wetterich-Morris
equation. An important difference with usual nonperturbative RG analysis is
that only the effective (IR) 2-point function is known, which requires setting
the problem with care. Our aim is to provide a useful formalism to investigate
neural networks behavior beyond the large-width limit (i.e.~far from Gaussian
limit) in a nonperturbative fashion. A major result of our analysis is that
changing the standard deviation of the neural network weight distribution can
be interpreted as a renormalization flow in the space of networks. We focus on
translations invariant kernels and provide preliminary numerical results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/hep-th/1/au:+Erbin_H/0/1/0/all/0/1"&gt;Harold Erbin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-th/1/au:+Lahoche_V/0/1/0/all/0/1"&gt;Vincent Lahoche&lt;/a&gt;, &lt;a href="http://arxiv.org/find/hep-th/1/au:+Samary_D/0/1/0/all/0/1"&gt;Dine Ousmane Samary&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Neural Networks and PIDE discretizations. (arXiv:2108.02430v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02430</id>
        <link href="http://arxiv.org/abs/2108.02430"/>
        <updated>2021-08-06T00:51:46.850Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose neural networks that tackle the problems of
stability and field-of-view of a Convolutional Neural Network (CNN). As an
alternative to increasing the network's depth or width to improve performance,
we propose integral-based spatially nonlocal operators which are related to
global weighted Laplacian, fractional Laplacian and inverse fractional
Laplacian operators that arise in several problems in the physical sciences.
The forward propagation of such networks is inspired by partial
integro-differential equations (PIDEs). We test the effectiveness of the
proposed neural architectures on benchmark image classification datasets and
semantic segmentation tasks in autonomous driving. Moreover, we investigate the
extra computational costs of these dense operators and the stability of forward
propagation of the proposed neural networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bohn_B/0/1/0/all/0/1"&gt;Bastian Bohn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Griebel_M/0/1/0/all/0/1"&gt;Michael Griebel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kannan_D/0/1/0/all/0/1"&gt;Dinesh Kannan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mean-Field Multi-Agent Reinforcement Learning: A Decentralized Network Approach. (arXiv:2108.02731v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02731</id>
        <link href="http://arxiv.org/abs/2108.02731"/>
        <updated>2021-08-06T00:51:46.844Z</updated>
        <summary type="html"><![CDATA[One of the challenges for multi-agent reinforcement learning (MARL) is
designing efficient learning algorithms for a large system in which each agent
has only limited or partial information of the entire system. In this system,
it is desirable to learn policies of a decentralized type. A recent and
promising paradigm to analyze such decentralized MARL is to take network
structures into consideration. While exciting progress has been made to analyze
decentralized MARL with the network of agents, often found in social networks
and team video games, little is known theoretically for decentralized MARL with
the network of states, frequently used for modeling self-driving vehicles,
ride-sharing, and data and traffic routing.

This paper proposes a framework called localized training and decentralized
execution to study MARL with network of states, with homogeneous (a.k.a.
mean-field type) agents. Localized training means that agents only need to
collect local information in their neighboring states during the training
phase; decentralized execution implies that, after the training stage, agents
can execute the learned decentralized policies, which only requires knowledge
of the agents' current states. The key idea is to utilize the homogeneity of
agents and regroup them according to their states, thus the formulation of a
networked Markov decision process with teams of agents, enabling the update of
the Q-function in a localized fashion. In order to design an efficient and
scalable reinforcement learning algorithm under such a framework, we adopt the
actor-critic approach with over-parameterized neural networks, and establish
the convergence and sample complexity for our algorithm, shown to be scalable
with respect to the size of both agents and states.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gu_H/0/1/0/all/0/1"&gt;Haotian Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1"&gt;Xin Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1"&gt;Xiaoli Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1"&gt;Renyuan Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TorchIO: A Python library for efficient loading, preprocessing, augmentation and patch-based sampling of medical images in deep learning. (arXiv:2003.04696v5 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.04696</id>
        <link href="http://arxiv.org/abs/2003.04696"/>
        <updated>2021-08-06T00:51:46.836Z</updated>
        <summary type="html"><![CDATA[Processing of medical images such as MRI or CT presents unique challenges
compared to RGB images typically used in computer vision. These include a lack
of labels for large datasets, high computational costs, and metadata to
describe the physical properties of voxels. Data augmentation is used to
artificially increase the size of the training datasets. Training with image
patches decreases the need for computational power. Spatial metadata needs to
be carefully taken into account in order to ensure a correct alignment of
volumes.

We present TorchIO, an open-source Python library to enable efficient
loading, preprocessing, augmentation and patch-based sampling of medical images
for deep learning. TorchIO follows the style of PyTorch and integrates standard
medical image processing libraries to efficiently process images during
training of neural networks. TorchIO transforms can be composed, reproduced,
traced and extended. We provide multiple generic preprocessing and augmentation
operations as well as simulation of MRI-specific artifacts.

Source code, comprehensive tutorials and extensive documentation for TorchIO
can be found at https://torchio.rtfd.io/. The package can be installed from the
Python Package Index running 'pip install torchio'. It includes a command-line
interface which allows users to apply transforms to image files without using
Python. Additionally, we provide a graphical interface within a TorchIO
extension in 3D Slicer to visualize the effects of transforms.

TorchIO was developed to help researchers standardize medical image
processing pipelines and allow them to focus on the deep learning experiments.
It encourages open science, as it supports reproducibility and is version
controlled so that the software can be cited precisely. Due to its modularity,
the library is compatible with other frameworks for deep learning with medical
images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Perez_Garcia_F/0/1/0/all/0/1"&gt;Fernando P&amp;#xe9;rez-Garc&amp;#xed;a&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sparks_R/0/1/0/all/0/1"&gt;Rachel Sparks&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ourselin_S/0/1/0/all/0/1"&gt;S&amp;#xe9;bastien Ourselin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ensemble Consensus-based Representation Deep Reinforcement Learning for Hybrid FSO/RF Communication Systems. (arXiv:2108.02551v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02551</id>
        <link href="http://arxiv.org/abs/2108.02551"/>
        <updated>2021-08-06T00:51:46.809Z</updated>
        <summary type="html"><![CDATA[Hybrid FSO/RF system requires an efficient FSO and RF link switching
mechanism to improve the system capacity by realizing the complementary
benefits of both the links. The dynamics of network conditions, such as fog,
dust, and sand storms compound the link switching problem and control
complexity. To address this problem, we initiate the study of deep
reinforcement learning (DRL) for link switching of hybrid FSO/RF systems.
Specifically, in this work, we focus on actor-critic called Actor/Critic-FSO/RF
and Deep-Q network (DQN) called DQN-FSO/RF for FSO/RF link switching under
atmospheric turbulences. To formulate the problem, we define the state, action,
and reward function of a hybrid FSO/RF system. DQN-FSO/RF frequently updates
the deployed policy that interacts with the environment in a hybrid FSO/RF
system, resulting in high switching costs. To overcome this, we lift this
problem to ensemble consensus-based representation learning for deep
reinforcement called DQNEnsemble-FSO/RF. The proposed novel DQNEnsemble-FSO/RF
DRL approach uses consensus learned features representations based on an
ensemble of asynchronous threads to update the deployed policy. Experimental
results corroborate that the proposed DQNEnsemble-FSO/RF's consensus-learned
features switching achieves better performance than Actor/Critic-FSO/RF,
DQN-FSO/RF, and MyOpic for FSO/RF link switching while keeping the switching
cost significantly low.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Henna_S/0/1/0/all/0/1"&gt;Shagufta Henna&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Entropic Out-of-Distribution Detection: Seamless Detection of Unknown Examples. (arXiv:2006.04005v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.04005</id>
        <link href="http://arxiv.org/abs/2006.04005"/>
        <updated>2021-08-06T00:51:46.799Z</updated>
        <summary type="html"><![CDATA[In this paper, we argue that the unsatisfactory out-of-distribution (OOD)
detection performance of neural networks is mainly due to the SoftMax loss
anisotropy and propensity to produce low entropy probability distributions in
disagreement with the principle of maximum entropy. Current out-of-distribution
(OOD) detection approaches usually do not directly fix the SoftMax loss
drawbacks, but rather build techniques to circumvent it. Unfortunately, those
methods usually produce undesired side effects (e.g., classification accuracy
drop, additional hyperparameters, slower inferences, and collecting extra
data). In the opposite direction, we propose replacing SoftMax loss with a
novel loss function that does not suffer from the mentioned weaknesses. The
proposed IsoMax loss is isotropic (exclusively distance-based) and provides
high entropy posterior probability distributions. Replacing the SoftMax loss by
IsoMax loss requires no model or training changes. Additionally, the models
trained with IsoMax loss produce as fast and energy-efficient inferences as
those trained using SoftMax loss. Moreover, no classification accuracy drop is
observed. The proposed method does not rely on outlier/background data,
hyperparameter tuning, temperature calibration, feature extraction, metric
learning, adversarial training, ensemble procedures, or generative models. Our
experiments showed that IsoMax loss works as a seamless SoftMax loss drop-in
replacement that significantly improves neural networks' OOD detection
performance. Hence, it may be used as a baseline OOD detection approach to be
combined with current or future OOD detection techniques to achieve even higher
results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Macedo_D/0/1/0/all/0/1"&gt;David Mac&amp;#xea;do&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_T/0/1/0/all/0/1"&gt;Tsang Ing Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zanchettin_C/0/1/0/all/0/1"&gt;Cleber Zanchettin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oliveira_A/0/1/0/all/0/1"&gt;Adriano L. I. Oliveira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ludermir_T/0/1/0/all/0/1"&gt;Teresa Ludermir&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Behavioral Cloning for Autonomous Vehicles using End-to-End Imitation Learning. (arXiv:2010.04767v4 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.04767</id>
        <link href="http://arxiv.org/abs/2010.04767"/>
        <updated>2021-08-06T00:51:46.783Z</updated>
        <summary type="html"><![CDATA[In this work, we present a lightweight pipeline for robust behavioral cloning
of a human driver using end-to-end imitation learning. The proposed pipeline
was employed to train and deploy three distinct driving behavior models onto a
simulated vehicle. The training phase comprised of data collection, balancing,
augmentation, preprocessing and training a neural network, following which, the
trained model was deployed onto the ego vehicle to predict steering commands
based on the feed from an onboard camera. A novel coupled control law was
formulated to generate longitudinal control commands on-the-go based on the
predicted steering angle and other parameters such as actual speed of the ego
vehicle and the prescribed constraints for speed and steering. We analyzed
computational efficiency of the pipeline and evaluated robustness of the
trained models through exhaustive experimentation during the deployment phase.
We also compared our approach against state-of-the-art implementation in order
to comment on its validity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Samak_T/0/1/0/all/0/1"&gt;Tanmay Vilas Samak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Samak_C/0/1/0/all/0/1"&gt;Chinmay Vilas Samak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kandhasamy_S/0/1/0/all/0/1"&gt;Sivanathan Kandhasamy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Compositional Abstraction Error and a Category of Causal Models. (arXiv:2103.15758v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.15758</id>
        <link href="http://arxiv.org/abs/2103.15758"/>
        <updated>2021-08-06T00:51:46.730Z</updated>
        <summary type="html"><![CDATA[Interventional causal models describe several joint distributions over some
variables used to describe a system, one for each intervention setting. They
provide a formal recipe for how to move between the different joint
distributions and make predictions about the variables upon intervening on the
system. Yet, it is difficult to formalise how we may change the underlying
variables used to describe the system, say moving from fine-grained to
coarse-grained variables. Here, we argue that compositionality is a desideratum
for such model transformations and the associated errors: When abstracting a
reference model M iteratively, first obtaining M' and then further simplifying
that to obtain M'', we expect the composite transformation from M to M'' to
exist and its error to be bounded by the errors incurred by each individual
transformation step. Category theory, the study of mathematical objects via
compositional transformations between them, offers a natural language to
develop our framework for model transformations and abstractions. We introduce
a category of finite interventional causal models and, leveraging theory of
enriched categories, prove the desired compositionality properties for our
framework.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Rischel_E/0/1/0/all/0/1"&gt;Eigil F. Rischel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Weichwald_S/0/1/0/all/0/1"&gt;Sebastian Weichwald&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring Sequence Feature Alignment for Domain Adaptive Detection Transformers. (arXiv:2107.12636v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.12636</id>
        <link href="http://arxiv.org/abs/2107.12636"/>
        <updated>2021-08-06T00:51:46.714Z</updated>
        <summary type="html"><![CDATA[Detection transformers have recently shown promising object detection results
and attracted increasing attention. However, how to develop effective domain
adaptation techniques to improve its cross-domain performance remains
unexplored and unclear. In this paper, we delve into this topic and empirically
find that direct feature distribution alignment on the CNN backbone only brings
limited improvements, as it does not guarantee domain-invariant sequence
features in the transformer for prediction. To address this issue, we propose a
novel Sequence Feature Alignment (SFA) method that is specially designed for
the adaptation of detection transformers. Technically, SFA consists of a domain
query-based feature alignment (DQFA) module and a token-wise feature alignment
(TDA) module. In DQFA, a novel domain query is used to aggregate and align
global context from the token sequence of both domains. DQFA reduces the domain
discrepancy in global feature representations and object relations when
deploying in the transformer encoder and decoder, respectively. Meanwhile, TDA
aligns token features in the sequence from both domains, which reduces the
domain gaps in local and instance-level feature representations in the
transformer encoder and decoder, respectively. Besides, a novel bipartite
matching consistency loss is proposed to enhance the feature discriminability
for robust object detection. Experiments on three challenging benchmarks show
that SFA outperforms state-of-the-art domain adaptive object detection methods.
Code has been made available at: https://github.com/encounter1997/SFA.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1"&gt;Yang Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_F/0/1/0/all/0/1"&gt;Fengxiang He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zha_Z/0/1/0/all/0/1"&gt;Zheng-Jun Zha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1"&gt;Yonggang Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1"&gt;Dacheng Tao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fixed-Budget Best-Arm Identification in Contextual Bandits: A Static-Adaptive Algorithm. (arXiv:2106.04763v6 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.04763</id>
        <link href="http://arxiv.org/abs/2106.04763"/>
        <updated>2021-08-06T00:51:46.693Z</updated>
        <summary type="html"><![CDATA[We study the problem of best-arm identification (BAI) in contextual bandits
in the fixed-budget setting. We propose a general successive elimination
algorithm that proceeds in stages and eliminates a fixed fraction of suboptimal
arms in each stage. This design takes advantage of the strengths of static and
adaptive allocations. We analyze the algorithm in linear models and obtain a
better error bound than prior work. We also apply it to generalized linear
models (GLMs) and bound its error. This is the first BAI algorithm for GLMs in
the fixed-budget setting. Our extensive numerical experiments show that our
algorithm outperforms the state of art.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Azizi_M/0/1/0/all/0/1"&gt;Mohammad Javad Azizi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kveton_B/0/1/0/all/0/1"&gt;Branislav Kveton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghavamzadeh_M/0/1/0/all/0/1"&gt;Mohammad Ghavamzadeh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep learning for inverse problems with unknown operator. (arXiv:2108.02744v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2108.02744</id>
        <link href="http://arxiv.org/abs/2108.02744"/>
        <updated>2021-08-06T00:51:46.686Z</updated>
        <summary type="html"><![CDATA[We consider ill-posed inverse problems where the forward operator $T$ is
unknown, and instead we have access to training data consisting of functions
$f_i$ and their noisy images $Tf_i$. This is a practically relevant and
challenging problem which current methods are able to solve only under strong
assumptions on the training set. Here we propose a new method that requires
minimal assumptions on the data, and prove reconstruction rates that depend on
the number of training points and the noise level. We show that, in the regime
of "many" training data, the method is minimax optimal. The proposed method
employs a type of convolutional neural networks (U-nets) and empirical risk
minimization in order to "fit" the unknown operator. In a nutshell, our
approach is based on two ideas: the first is to relate U-nets to multiscale
decompositions such as wavelets, thereby linking them to the existing theory,
and the second is to use the hierarchical structure of U-nets and the low
number of parameters of convolutional neural nets to prove entropy bounds that
are practically useful. A significant difference with the existing works on
neural networks in nonparametric statistics is that we use them to approximate
operators and not functions, which we argue is mathematically more natural and
technically more convenient.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Alamo_M/0/1/0/all/0/1"&gt;Miguel del Alamo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CoReD: Generalizing Fake Media Detection with Continual Representation using Distillation. (arXiv:2107.02408v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.02408</id>
        <link href="http://arxiv.org/abs/2107.02408"/>
        <updated>2021-08-06T00:51:46.679Z</updated>
        <summary type="html"><![CDATA[Over the last few decades, artificial intelligence research has made
tremendous strides, but it still heavily relies on fixed datasets in stationary
environments. Continual learning is a growing field of research that examines
how AI systems can learn sequentially from a continuous stream of linked data
in the same way that biological systems do. Simultaneously, fake media such as
deepfakes and synthetic face images have emerged as significant to current
multimedia technologies. Recently, numerous method has been proposed which can
detect deepfakes with high accuracy. However, they suffer significantly due to
their reliance on fixed datasets in limited evaluation settings. Therefore, in
this work, we apply continuous learning to neural networks' learning dynamics,
emphasizing its potential to increase data efficiency significantly. We propose
Continual Representation using Distillation (CoReD) method that employs the
concept of Continual Learning (CL), Representation Learning (RL), and Knowledge
Distillation (KD). We design CoReD to perform sequential domain adaptation
tasks on new deepfake and GAN-generated synthetic face datasets, while
effectively minimizing the catastrophic forgetting in a teacher-student model
setting. Our extensive experimental results demonstrate that our method is
efficient at domain adaptation to detect low-quality deepfakes videos and
GAN-generated images from several datasets, outperforming the-state-of-art
baseline methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1"&gt;Minha Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tariq_S/0/1/0/all/0/1"&gt;Shahroz Tariq&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Woo_S/0/1/0/all/0/1"&gt;Simon S. Woo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Advances in Trajectory Optimization for Space Vehicle Control. (arXiv:2108.02335v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2108.02335</id>
        <link href="http://arxiv.org/abs/2108.02335"/>
        <updated>2021-08-06T00:51:46.672Z</updated>
        <summary type="html"><![CDATA[Space mission design places a premium on cost and operational efficiency. The
search for new science and life beyond Earth calls for spacecraft that can
deliver scientific payloads to geologically rich yet hazardous landing sites.
At the same time, the last four decades of optimization research have put a
suite of powerful optimization tools at the fingertips of the controls
engineer. As we enter the new decade, optimization theory, algorithms, and
software tooling have reached a critical mass to start seeing serious
application in space vehicle guidance and control systems. This survey paper
provides a detailed overview of recent advances, successes, and promising
directions for optimization-based space vehicle control. The considered
applications include planetary landing, rendezvous and proximity operations,
small body landing, constrained reorientation, endo-atmospheric flight
including ascent and re-entry, and orbit transfer and injection. The primary
focus is on the last ten years of progress, which have seen a veritable rise in
the number of applications using three core technologies: lossless
convexification, sequential convex programming, and model predictive control.
The reader will come away with a well-rounded understanding of the
state-of-the-art in each space vehicle control application, and will be well
positioned to tackle important current open problems using convex optimization
as a core technology.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Malyuta_D/0/1/0/all/0/1"&gt;Danylo Malyuta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Yu_Y/0/1/0/all/0/1"&gt;Yue Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Elango_P/0/1/0/all/0/1"&gt;Purnanand Elango&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Acikmese_B/0/1/0/all/0/1"&gt;Behcet Acikmese&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Linearized Assignment Flows for Image Labeling. (arXiv:2108.02571v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02571</id>
        <link href="http://arxiv.org/abs/2108.02571"/>
        <updated>2021-08-06T00:51:46.665Z</updated>
        <summary type="html"><![CDATA[We introduce a novel algorithm for estimating optimal parameters of
linearized assignment flows for image labeling. An exact formula is derived for
the parameter gradient of any loss function that is constrained by the linear
system of ODEs determining the linearized assignment flow. We show how to
efficiently evaluate this formula using a Krylov subspace and a low-rank
approximation. This enables us to perform parameter learning by Riemannian
gradient descent in the parameter space, without the need to backpropagate
errors or to solve an adjoint equation, in less than 10 seconds for a
$512\times 512$ image using just about $0.5$ GB memory. Experiments demonstrate
that our method performs as good as highly-tuned machine learning software
using automatic differentiation. Unlike methods employing automatic
differentiation, our approach yields a low-dimensional representation of
internal parameters and their dynamics which helps to understand how networks
work and perform that realize assignment flows and generalizations thereof.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zeilmann_A/0/1/0/all/0/1"&gt;Alexander Zeilmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Petra_S/0/1/0/all/0/1"&gt;Stefania Petra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schnorr_C/0/1/0/all/0/1"&gt;Christoph Schn&amp;#xf6;rr&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust CUR Decomposition: Theory and Imaging Applications. (arXiv:2101.05231v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.05231</id>
        <link href="http://arxiv.org/abs/2101.05231"/>
        <updated>2021-08-06T00:51:46.647Z</updated>
        <summary type="html"><![CDATA[This paper considers the use of Robust PCA in a CUR decomposition framework
and applications thereof. Our main algorithms produce a robust version of
column-row factorizations of matrices $\mathbf{D}=\mathbf{L}+\mathbf{S}$ where
$\mathbf{L}$ is low-rank and $\mathbf{S}$ contains sparse outliers. These
methods yield interpretable factorizations at low computational cost, and
provide new CUR decompositions that are robust to sparse outliers, in contrast
to previous methods. We consider two key imaging applications of Robust PCA:
video foreground-background separation and face modeling. This paper examines
the qualitative behavior of our Robust CUR decompositions on the benchmark
videos and face datasets, and find that our method works as well as standard
Robust PCA while being significantly faster. Additionally, we consider hybrid
randomized and deterministic sampling methods which produce a compact CUR
decomposition of a given matrix, and apply this to video sequences to produce
canonical frames thereof.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1"&gt;HanQin Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hamm_K/0/1/0/all/0/1"&gt;Keaton Hamm&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1"&gt;Longxiu Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Needell_D/0/1/0/all/0/1"&gt;Deanna Needell&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Metric Transforms and Low Rank Matrices via Representation Theory of the Real Hyperrectangle. (arXiv:2011.11503v2 [cs.CG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.11503</id>
        <link href="http://arxiv.org/abs/2011.11503"/>
        <updated>2021-08-06T00:51:46.641Z</updated>
        <summary type="html"><![CDATA[In this paper, we develop a new technique which we call representation theory
of the real hyperrectangle, which describes how to compute the eigenvectors and
eigenvalues of certain matrices arising from hyperrectangles. We show that
these matrices arise naturally when analyzing a number of different algorithmic
tasks such as kernel methods, neural network training, natural language
processing, and the design of algorithms using the polynomial method. We then
use our new technique along with these connections to prove several new
structural results in these areas, including:

$\bullet$ A function is a positive definite Manhattan kernel if and only if
it is a completely monotone function. These kernels are widely used across
machine learning; one example is the Laplace kernel which is widely used in
machine learning for chemistry.

$\bullet$ A function transforms Manhattan distances to Manhattan distances if
and only if it is a Bernstein function. This completes the theory of Manhattan
to Manhattan metric transforms initiated by Assouad in 1980.

$\bullet$ A function applied entry-wise to any square matrix of rank $r$
always results in a matrix of rank $< 2^{r-1}$ if and only if it is a
polynomial of sufficiently low degree. This gives a converse to a key lemma
used by the polynomial method in algorithm design.

Our work includes a sophisticated combination of techniques from different
fields, including metric embeddings, the polynomial method, and group
representation theory.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alman_J/0/1/0/all/0/1"&gt;Josh Alman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chu_T/0/1/0/all/0/1"&gt;Timothy Chu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Miller_G/0/1/0/all/0/1"&gt;Gary Miller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Narayanan_S/0/1/0/all/0/1"&gt;Shyam Narayanan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sellke_M/0/1/0/all/0/1"&gt;Mark Sellke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1"&gt;Zhao Song&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pseudo-Rehearsal for Continual Learning with Normalizing Flows. (arXiv:2007.02443v4 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.02443</id>
        <link href="http://arxiv.org/abs/2007.02443"/>
        <updated>2021-08-06T00:51:46.634Z</updated>
        <summary type="html"><![CDATA[Catastrophic forgetting (CF) happens whenever a neural network overwrites
past knowledge while being trained on new tasks. Common techniques to handle CF
include regularization of the weights (using, e.g., their importance on past
tasks), and rehearsal strategies, where the network is constantly re-trained on
past data. Generative models have also been applied for the latter, in order to
have endless sources of data. In this paper, we propose a novel method that
combines the strengths of regularization and generative-based rehearsal
approaches. Our generative model consists of a normalizing flow (NF), a
probabilistic and invertible neural network, trained on the internal embeddings
of the network. By keeping a single NF conditioned on the task, we show that
our memory overhead remains constant. In addition, exploiting the invertibility
of the NF, we propose a simple approach to regularize the network's embeddings
with respect to past tasks. We show that our method performs favorably with
respect to state-of-the-art approaches in the literature, with bounded
computational power and memory overheads.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Pomponi_J/0/1/0/all/0/1"&gt;Jary Pomponi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Scardapane_S/0/1/0/all/0/1"&gt;Simone Scardapane&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Uncini_A/0/1/0/all/0/1"&gt;Aurelio Uncini&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving the Performance of a NoC-based CNN Accelerator with Gather Support. (arXiv:2108.02567v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02567</id>
        <link href="http://arxiv.org/abs/2108.02567"/>
        <updated>2021-08-06T00:51:46.627Z</updated>
        <summary type="html"><![CDATA[The increasing application of deep learning technology drives the need for an
efficient parallel computing architecture for Convolutional Neural Networks
(CNNs). A significant challenge faced when designing a many-core CNN
accelerator is to handle the data movement between the processing elements. The
CNN workload introduces many-to-one traffic in addition to one-to-one and
one-to-many traffic. As the de-facto standard for on-chip communication,
Network-on-Chip (NoC) can support various unicast and multicast traffic. For
many-to-one traffic, repetitive unicast is employed which is not an efficient
way. In this paper, we propose to use the gather packet on mesh-based NoCs
employing output stationary systolic array in support of many-to-one traffic.
The gather packet will collect the data from the intermediate nodes eventually
leading to the destination efficiently. This method is evaluated using the
traffic traces generated from the convolution layer of AlexNet and VGG-16 with
improvement in the latency and power than the repetitive unicast method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tiwari_B/0/1/0/all/0/1"&gt;Binayak Tiwari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1"&gt;Mei Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaohang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1"&gt;Yingtao Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muthukumar_V/0/1/0/all/0/1"&gt;Venkatesan Muthukumar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GuavaNet: A deep neural network architecture for automatic sensory evaluation to predict degree of acceptability for Guava by a consumer. (arXiv:2108.02563v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02563</id>
        <link href="http://arxiv.org/abs/2108.02563"/>
        <updated>2021-08-06T00:51:46.621Z</updated>
        <summary type="html"><![CDATA[This thesis is divided into two parts:Part I: Analysis of Fruits, Vegetables,
Cheese and Fish based on Image Processing using Computer Vision and Deep
Learning: A Review. It consists of a comprehensive review of image processing,
computer vision and deep learning techniques applied to carry out analysis of
fruits, vegetables, cheese and fish.This part also serves as a literature
review for Part II.Part II: GuavaNet: A deep neural network architecture for
automatic sensory evaluation to predict degree of acceptability for Guava by a
consumer. This part introduces to an end-to-end deep neural network
architecture that can predict the degree of acceptability by the consumer for a
guava based on sensory evaluation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mehra_V/0/1/0/all/0/1"&gt;Vipul Mehra&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bridging Few-Shot Learning and Adaptation: New Challenges of Support-Query Shift. (arXiv:2105.11804v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.11804</id>
        <link href="http://arxiv.org/abs/2105.11804"/>
        <updated>2021-08-06T00:51:46.615Z</updated>
        <summary type="html"><![CDATA[Few-Shot Learning (FSL) algorithms have made substantial progress in learning
novel concepts with just a handful of labelled data. To classify query
instances from novel classes encountered at test-time, they only require a
support set composed of a few labelled samples. FSL benchmarks commonly assume
that those queries come from the same distribution as instances in the support
set. However, in a realistic set-ting, data distribution is plausibly subject
to change, a situation referred to as Distribution Shift (DS). The present work
addresses the new and challenging problem of Few-Shot Learning under
Support/Query Shift (FSQS) i.e., when support and query instances are sampled
from related but different distributions. Our contributions are the following.
First, we release a testbed for FSQS, including datasets, relevant baselines
and a protocol for a rigorous and reproducible evaluation. Second, we observe
that well-established FSL algorithms unsurprisingly suffer from a considerable
drop in accuracy when facing FSQS, stressing the significance of our study.
Finally, we show that transductive algorithms can limit the inopportune effect
of DS. In particular, we study both the role of Batch-Normalization and Optimal
Transport (OT) in aligning distributions, bridging Unsupervised Domain
Adaptation with FSL. This results in a new method that efficiently combines OT
with the celebrated Prototypical Networks. We bring compelling experiments
demonstrating the advantage of our method. Our work opens an exciting line of
research by providing a testbed and strong baselines. Our code is available at
https://github.com/ebennequin/meta-domain-shift.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bennequin_E/0/1/0/all/0/1"&gt;Etienne Bennequin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bouvier_V/0/1/0/all/0/1"&gt;Victor Bouvier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tami_M/0/1/0/all/0/1"&gt;Myriam Tami&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Toubhans_A/0/1/0/all/0/1"&gt;Antoine Toubhans&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hudelot_C/0/1/0/all/0/1"&gt;C&amp;#xe9;line Hudelot&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VBridge: Connecting the Dots Between Features, Explanations, and Data for Healthcare Models. (arXiv:2108.02550v1 [cs.HC])]]></title>
        <id>http://arxiv.org/abs/2108.02550</id>
        <link href="http://arxiv.org/abs/2108.02550"/>
        <updated>2021-08-06T00:51:46.597Z</updated>
        <summary type="html"><![CDATA[Machine learning (ML) is increasingly applied to Electronic Health Records
(EHRs) to solve clinical prediction tasks. Although many ML models perform
promisingly, issues with model transparency and interpretability limit their
adoption in clinical practice. Directly using existing explainable ML
techniques in clinical settings can be challenging. Through literature surveys
and collaborations with six clinicians with an average of 17 years of clinical
experience, we identified three key challenges, including clinicians'
unfamiliarity with ML features, lack of contextual information, and the need
for cohort-level evidence. Following an iterative design process, we further
designed and developed VBridge, a visual analytics tool that seamlessly
incorporates ML explanations into clinicians' decision-making workflow. The
system includes a novel hierarchical display of contribution-based feature
explanations and enriched interactions that connect the dots between ML
features, explanations, and data. We demonstrated the effectiveness of VBridge
through two case studies and expert interviews with four clinicians, showing
that visually associating model explanations with patients' situational records
can help clinicians better interpret and use model predictions when making
clinician decisions. We further derived a list of design implications for
developing future explainable ML tools to support clinical decision-making.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_F/0/1/0/all/0/1"&gt;Furui Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1"&gt;Dongyu Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_F/0/1/0/all/0/1"&gt;Fan Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1"&gt;Yanna Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zytek_A/0/1/0/all/0/1"&gt;Alexandra Zytek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Haomin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qu_H/0/1/0/all/0/1"&gt;Huamin Qu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Veeramachaneni_K/0/1/0/all/0/1"&gt;Kalyan Veeramachaneni&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using Metamorphic Relations to Verify and Enhance Artcode Classification. (arXiv:2108.02694v1 [cs.SE])]]></title>
        <id>http://arxiv.org/abs/2108.02694</id>
        <link href="http://arxiv.org/abs/2108.02694"/>
        <updated>2021-08-06T00:51:46.590Z</updated>
        <summary type="html"><![CDATA[Software testing is often hindered where it is impossible or impractical to
determine the correctness of the behaviour or output of the software under test
(SUT), a situation known as the oracle problem. An example of an area facing
the oracle problem is automatic image classification, using machine learning to
classify an input image as one of a set of predefined classes. An approach to
software testing that alleviates the oracle problem is metamorphic testing
(MT). While traditional software testing examines the correctness of individual
test cases, MT instead examines the relations amongst multiple executions of
test cases and their outputs. These relations are called metamorphic relations
(MRs): if an MR is found to be violated, then a fault must exist in the SUT.
This paper examines the problem of classifying images containing visually
hidden markers called Artcodes, and applies MT to verify and enhance the
trained classifiers. This paper further examines two MRs, Separation and
Occlusion, and reports on their capability in verifying the image
classification using one-way analysis of variance (ANOVA) in conjunction with
three other statistical analysis methods: t-test (for unequal variances),
Kruskal-Wallis test, and Dunnett's test. In addition to our previously-studied
classifier, that used Random Forests, we introduce a new classifier that uses a
support vector machine, and present its MR-augmented version. Experimental
evaluations across a number of performance metrics show that the augmented
classifiers can achieve better performance than non-augmented classifiers. This
paper also analyses how the enhanced performance is obtained.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1"&gt;Liming Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Towey_D/0/1/0/all/0/1"&gt;Dave Towey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+French_A/0/1/0/all/0/1"&gt;Andrew French&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Benford_S/0/1/0/all/0/1"&gt;Steve Benford&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1"&gt;Zhi Quan Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1"&gt;Tsong Yueh Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Video Contrastive Learning with Global Context. (arXiv:2108.02722v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02722</id>
        <link href="http://arxiv.org/abs/2108.02722"/>
        <updated>2021-08-06T00:51:46.583Z</updated>
        <summary type="html"><![CDATA[Contrastive learning has revolutionized self-supervised image representation
learning field, and recently been adapted to video domain. One of the greatest
advantages of contrastive learning is that it allows us to flexibly define
powerful loss objectives as long as we can find a reasonable way to formulate
positive and negative samples to contrast. However, existing approaches rely
heavily on the short-range spatiotemporal salience to form clip-level
contrastive signals, thus limit themselves from using global context. In this
paper, we propose a new video-level contrastive learning method based on
segments to formulate positive pairs. Our formulation is able to capture global
context in a video, thus robust to temporal content change. We also incorporate
a temporal order regularization term to enforce the inherent sequential
structure of videos. Extensive experiments show that our video-level
contrastive learning framework (VCLR) is able to outperform previous
state-of-the-arts on five video datasets for downstream action classification,
action localization and video retrieval. Code is available at
https://github.com/amazon-research/video-contrastive-learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kuang_H/0/1/0/all/0/1"&gt;Haofei Kuang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yi Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xinyu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tighe_J/0/1/0/all/0/1"&gt;Joseph Tighe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schwertfeger_S/0/1/0/all/0/1"&gt;S&amp;#xf6;ren Schwertfeger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stachniss_C/0/1/0/all/0/1"&gt;Cyrill Stachniss&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1"&gt;Mu Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SLAMP: Stochastic Latent Appearance and Motion Prediction. (arXiv:2108.02760v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02760</id>
        <link href="http://arxiv.org/abs/2108.02760"/>
        <updated>2021-08-06T00:51:46.567Z</updated>
        <summary type="html"><![CDATA[Motion is an important cue for video prediction and often utilized by
separating video content into static and dynamic components. Most of the
previous work utilizing motion is deterministic but there are stochastic
methods that can model the inherent uncertainty of the future. Existing
stochastic models either do not reason about motion explicitly or make limiting
assumptions about the static part. In this paper, we reason about appearance
and motion in the video stochastically by predicting the future based on the
motion history. Explicit reasoning about motion without history already reaches
the performance of current stochastic models. The motion history further
improves the results by allowing to predict consistent dynamics several frames
into the future. Our model performs comparably to the state-of-the-art models
on the generic video prediction datasets, however, significantly outperforms
them on two challenging real-world autonomous driving datasets with complex
motion and dynamic background.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Akan_A/0/1/0/all/0/1"&gt;Adil Kaan Akan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Erdem_E/0/1/0/all/0/1"&gt;Erkut Erdem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Erdem_A/0/1/0/all/0/1"&gt;Aykut Erdem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guney_F/0/1/0/all/0/1"&gt;Fatma G&amp;#xfc;ney&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Hypothesis for the Aesthetic Appreciation in Neural Networks. (arXiv:2108.02646v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02646</id>
        <link href="http://arxiv.org/abs/2108.02646"/>
        <updated>2021-08-06T00:51:46.527Z</updated>
        <summary type="html"><![CDATA[This paper proposes a hypothesis for the aesthetic appreciation that
aesthetic images make a neural network strengthen salient concepts and discard
inessential concepts. In order to verify this hypothesis, we use multi-variate
interactions to represent salient concepts and inessential concepts contained
in images. Furthermore, we design a set of operations to revise images towards
more beautiful ones. In experiments, we find that the revised images are more
aesthetic than the original ones to some extent.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1"&gt;Xu Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1"&gt;Haotian Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_Z/0/1/0/all/0/1"&gt;Zhengyang Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Quanshi Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Low Rank Promoting Prior for Unsupervised Contrastive Learning. (arXiv:2108.02696v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02696</id>
        <link href="http://arxiv.org/abs/2108.02696"/>
        <updated>2021-08-06T00:51:46.521Z</updated>
        <summary type="html"><![CDATA[Unsupervised learning is just at a tipping point where it could really take
off. Among these approaches, contrastive learning has seen tremendous progress
and led to state-of-the-art performance. In this paper, we construct a novel
probabilistic graphical model that effectively incorporates the low rank
promoting prior into the framework of contrastive learning, referred to as
LORAC. In contrast to the existing conventional self-supervised approaches that
only considers independent learning, our hypothesis explicitly requires that
all the samples belonging to the same instance class lie on the same subspace
with small dimension. This heuristic poses particular joint learning
constraints to reduce the degree of freedom of the problem during the search of
the optimal network parameterization. Most importantly, we argue that the low
rank prior employed here is not unique, and many different priors can be
invoked in a similar probabilistic way, corresponding to different hypotheses
about underlying truth behind the contrastive features. Empirical evidences
show that the proposed algorithm clearly surpasses the state-of-the-art
approaches on multiple benchmarks, including image classification, object
detection, instance segmentation and keypoint detection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1"&gt;Jingyang Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_Q/0/1/0/all/0/1"&gt;Qi Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1"&gt;Yingwei Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_T/0/1/0/all/0/1"&gt;Ting Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chao_H/0/1/0/all/0/1"&gt;Hongyang Chao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mei_T/0/1/0/all/0/1"&gt;Tao Mei&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dependable Neural Networks Through Redundancy, A Comparison of Redundant Architectures. (arXiv:2108.02565v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02565</id>
        <link href="http://arxiv.org/abs/2108.02565"/>
        <updated>2021-08-06T00:51:46.514Z</updated>
        <summary type="html"><![CDATA[With edge-AI finding an increasing number of real-world applications,
especially in industry, the question of functionally safe applications using AI
has begun to be asked. In this body of work, we explore the issue of achieving
dependable operation of neural networks. We discuss the issue of dependability
in general implementation terms before examining lockstep solutions. We intuit
that it is not necessarily a given that two similar neural networks generate
results at precisely the same time and that synchronization between the
platforms will be required. We perform some preliminary measurements that may
support this intuition and introduce some work in implementing lockstep neural
network engines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Doran_H/0/1/0/all/0/1"&gt;Hans Dermot Doran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ielpo_G/0/1/0/all/0/1"&gt;Gianluca Ielpo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ganz_D/0/1/0/all/0/1"&gt;David Ganz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zapke_M/0/1/0/all/0/1"&gt;Michael Zapke&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Reinforcement Learning for Continuous Docking Control of Autonomous Underwater Vehicles: A Benchmarking Study. (arXiv:2108.02665v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2108.02665</id>
        <link href="http://arxiv.org/abs/2108.02665"/>
        <updated>2021-08-06T00:51:46.507Z</updated>
        <summary type="html"><![CDATA[Docking control of an autonomous underwater vehicle (AUV) is a task that is
integral to achieving persistent long term autonomy. This work explores the
application of state-of-the-art model-free deep reinforcement learning (DRL)
approaches to the task of AUV docking in the continuous domain. We provide a
detailed formulation of the reward function, utilized to successfully dock the
AUV onto a fixed docking platform. A major contribution that distinguishes our
work from the previous approaches is the usage of a physics simulator to define
and simulate the underwater environment as well as the DeepLeng AUV. We propose
a new reward function formulation for the docking task, incorporating several
components, that outperforms previous reward formulations. We evaluate proximal
policy optimization (PPO), twin delayed deep deterministic policy gradients
(TD3) and soft actor-critic (SAC) in combination with our reward function. Our
evaluation yielded results that conclusively show the TD3 agent to be most
efficient and consistent in terms of docking the AUV, over multiple evaluation
runs it achieved a 100% success rate and episode return of 10667.1 +- 688.8. We
also show how our reward function formulation improves over the state of the
art.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Patil_M/0/1/0/all/0/1"&gt;Mihir Patil&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wehbe_B/0/1/0/all/0/1"&gt;Bilal Wehbe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Valdenegro_Toro_M/0/1/0/all/0/1"&gt;Matias Valdenegro-Toro&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sinsy: A Deep Neural Network-Based Singing Voice Synthesis System. (arXiv:2108.02776v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2108.02776</id>
        <link href="http://arxiv.org/abs/2108.02776"/>
        <updated>2021-08-06T00:51:46.498Z</updated>
        <summary type="html"><![CDATA[This paper presents Sinsy, a deep neural network (DNN)-based singing voice
synthesis (SVS) system. In recent years, DNNs have been utilized in statistical
parametric SVS systems, and DNN-based SVS systems have demonstrated better
performance than conventional hidden Markov model-based ones. SVS systems are
required to synthesize a singing voice with pitch and timing that strictly
follow a given musical score. Additionally, singing expressions that are not
described on the musical score, such as vibrato and timing fluctuations, should
be reproduced. The proposed system is composed of four modules: a time-lag
model, a duration model, an acoustic model, and a vocoder, and singing voices
can be synthesized taking these characteristics of singing voices into account.
To better model a singing voice, the proposed system incorporates improved
approaches to modeling pitch and vibrato and better training criteria into the
acoustic model. In addition, we incorporated PeriodNet, a non-autoregressive
neural vocoder with robustness for the pitch, into our systems to generate a
high-fidelity singing voice waveform. Moreover, we propose automatic pitch
correction techniques for DNN-based SVS to synthesize singing voices with
correct pitch even if the training data has out-of-tune phrases. Experimental
results show our system can synthesize a singing voice with better timing, more
natural vibrato, and correct pitch, and it can achieve better mean opinion
scores in subjective evaluation tests.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Hono_Y/0/1/0/all/0/1"&gt;Yukiya Hono&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hashimoto_K/0/1/0/all/0/1"&gt;Kei Hashimoto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Oura_K/0/1/0/all/0/1"&gt;Keiichiro Oura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Nankaku_Y/0/1/0/all/0/1"&gt;Yoshihiko Nankaku&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tokuda_K/0/1/0/all/0/1"&gt;Keiichi Tokuda&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A New State-of-the-Art Transformers-Based Load Forecaster on the Smart Grid Domain. (arXiv:2108.02628v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02628</id>
        <link href="http://arxiv.org/abs/2108.02628"/>
        <updated>2021-08-06T00:51:46.475Z</updated>
        <summary type="html"><![CDATA[Meter-level load forecasting is crucial for efficient energy management and
power system planning for Smart Grids (SGs), in tasks associated with
regulation, dispatching, scheduling, and unit commitment of power grids.
Although a variety of algorithms have been proposed and applied on the field,
more accurate and robust models are still required: the overall utility cost of
operations in SGs increases 10 million currency units if the load forecasting
error increases 1%, and the mean absolute percentage error (MAPE) in
forecasting is still much higher than 1%. Transformers have become the new
state-of-the-art in a variety of tasks, including the ones in computer vision,
natural language processing and time series forecasting, surpassing alternative
neural models such as convolutional and recurrent neural networks. In this
letter, we present a new state-of-the-art Transformer-based algorithm for the
meter-level load forecasting task, which has surpassed the former
state-of-the-art, LSTM, and the traditional benchmark, vanilla RNN, in all
experiments by a margin of at least 13% in MAPE.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Novaes_A/0/1/0/all/0/1"&gt;Andre Luiz Farias Novaes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Araujo_R/0/1/0/all/0/1"&gt;Rui Alexandre de Matos Araujo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Figueiredo_J/0/1/0/all/0/1"&gt;Jose Figueiredo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pavanelli_L/0/1/0/all/0/1"&gt;Lucas Aguiar Pavanelli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Visual Domain Adaptation for Monocular Depth Estimation on Resource-Constrained Hardware. (arXiv:2108.02671v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02671</id>
        <link href="http://arxiv.org/abs/2108.02671"/>
        <updated>2021-08-06T00:51:46.467Z</updated>
        <summary type="html"><![CDATA[Real-world perception systems in many cases build on hardware with limited
resources to adhere to cost and power limitations of their carrying system.
Deploying deep neural networks on resource-constrained hardware became possible
with model compression techniques, as well as efficient and hardware-aware
architecture design. However, model adaptation is additionally required due to
the diverse operation environments. In this work, we address the problem of
training deep neural networks on resource-constrained hardware in the context
of visual domain adaptation. We select the task of monocular depth estimation
where our goal is to transform a pre-trained model to the target's domain data.
While the source domain includes labels, we assume an unlabelled target domain,
as it happens in real-world applications. Then, we present an adversarial
learning approach that is adapted for training on the device with limited
resources. Since visual domain adaptation, i.e. neural network training, has
not been previously explored for resource-constrained hardware, we present the
first feasibility study for image-based depth estimation. Our experiments show
that visual domain adaptation is relevant only for efficient network
architectures and training sets at the order of a few hundred samples. Models
and code are publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hornauer_J/0/1/0/all/0/1"&gt;Julia Hornauer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nalpantidis_L/0/1/0/all/0/1"&gt;Lazaros Nalpantidis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Belagiannis_V/0/1/0/all/0/1"&gt;Vasileios Belagiannis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeepScanner: a Robotic System for Automated 2D Object Dataset Collection with Annotations. (arXiv:2108.02555v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2108.02555</id>
        <link href="http://arxiv.org/abs/2108.02555"/>
        <updated>2021-08-06T00:51:46.445Z</updated>
        <summary type="html"><![CDATA[In the proposed study, we describe the possibility of automated dataset
collection using an articulated robot. The proposed technology reduces the
number of pixel errors on a polygonal dataset and the time spent on manual
labeling of 2D objects. The paper describes a novel automatic dataset
collection and annotation system, and compares the results of automated and
manual dataset labeling. Our approach increases the speed of data labeling
240-fold, and improves the accuracy compared to manual labeling 13-fold. We
also present a comparison of metrics for training a neural network on a
manually annotated and an automatically collected dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ilin_V/0/1/0/all/0/1"&gt;Valery Ilin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kalinov_I/0/1/0/all/0/1"&gt;Ivan Kalinov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karpyshev_P/0/1/0/all/0/1"&gt;Pavel Karpyshev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsetserukou_D/0/1/0/all/0/1"&gt;Dzmitry Tsetserukou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Robustness of Controlled Deep Reinforcement Learning for Slice Placement. (arXiv:2108.02505v1 [cs.NI])]]></title>
        <id>http://arxiv.org/abs/2108.02505</id>
        <link href="http://arxiv.org/abs/2108.02505"/>
        <updated>2021-08-06T00:51:46.419Z</updated>
        <summary type="html"><![CDATA[The evaluation of the impact of using Machine Learning in the management of
softwarized networks is considered in multiple research works. Beyond that, we
propose to evaluate the robustness of online learning for optimal network slice
placement. A major assumption to this study is to consider that slice request
arrivals are non-stationary. In this context, we simulate unpredictable network
load variations and compare two Deep Reinforcement Learning (DRL) algorithms: a
pure DRL-based algorithm and a heuristically controlled DRL as a hybrid
DRL-heuristic algorithm, to assess the impact of these unpredictable changes of
traffic load on the algorithms performance. We conduct extensive simulations of
a large-scale operator infrastructure. The evaluation results show that the
proposed hybrid DRL-heuristic approach is more robust and reliable in case of
unpredictable network load changes than pure DRL as it reduces the performance
degradation. These results are follow-ups for a series of recent research we
have performed showing that the proposed hybrid DRL-heuristic approach is
efficient and more adapted to real network scenarios than pure DRL.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Esteves_J/0/1/0/all/0/1"&gt;Jose Jurandir Alves Esteves&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boubendir_A/0/1/0/all/0/1"&gt;Amina Boubendir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guillemin_F/0/1/0/all/0/1"&gt;Fabrice Guillemin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sens_P/0/1/0/all/0/1"&gt;Pierre Sens&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Online Model-Free Reinforcement Learning for the Automatic Control of a Flexible Wing Aircraft. (arXiv:2108.02393v1 [eess.SY])]]></title>
        <id>http://arxiv.org/abs/2108.02393</id>
        <link href="http://arxiv.org/abs/2108.02393"/>
        <updated>2021-08-06T00:51:46.400Z</updated>
        <summary type="html"><![CDATA[The control problem of the flexible wing aircraft is challenging due to the
prevailing and high nonlinear deformations in the flexible wing system. This
urged for new control mechanisms that are robust to the real-time variations in
the wing's aerodynamics. An online control mechanism based on a value iteration
reinforcement learning process is developed for flexible wing aerial
structures. It employs a model-free control policy framework and a guaranteed
convergent adaptive learning architecture to solve the system's Bellman
optimality equation. A Riccati equation is derived and shown to be equivalent
to solving the underlying Bellman equation. The online reinforcement learning
solution is implemented using means of an adaptive-critic mechanism. The
controller is proven to be asymptotically stable in the Lyapunov sense. It is
assessed through computer simulations and its superior performance is
demonstrated on two scenarios under different operating conditions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Abouheaf_M/0/1/0/all/0/1"&gt;Mohammed Abouheaf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gueaieb_W/0/1/0/all/0/1"&gt;Wail Gueaieb&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lewis_F/0/1/0/all/0/1"&gt;Frank Lewis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DRL-based Slice Placement Under Non-Stationary Conditions. (arXiv:2108.02495v1 [cs.NI])]]></title>
        <id>http://arxiv.org/abs/2108.02495</id>
        <link href="http://arxiv.org/abs/2108.02495"/>
        <updated>2021-08-06T00:51:46.393Z</updated>
        <summary type="html"><![CDATA[We consider online learning for optimal network slice placement under the
assumption that slice requests arrive according to a non-stationary Poisson
process. We propose a framework based on Deep Reinforcement Learning (DRL)
combined with a heuristic to design algorithms. We specifically design two
pure-DRL algorithms and two families of hybrid DRL-heuristic algorithms. To
validate their performance, we perform extensive simulations in the context of
a large-scale operator infrastructure. The evaluation results show that the
proposed hybrid DRL-heuristic algorithms require three orders of magnitude of
learning episodes less than pure-DRL to achieve convergence. This result
indicates that the proposed hybrid DRL-heuristic approach is more reliable than
pure-DRL in a real non-stationary network scenario.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Esteves_J/0/1/0/all/0/1"&gt;Jose Jurandir Alves Esteves&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boubendir_A/0/1/0/all/0/1"&gt;Amina Boubendir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guillemin_F/0/1/0/all/0/1"&gt;Fabrice Guillemin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sens_P/0/1/0/all/0/1"&gt;Pierre Sens&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Hypothesis for the Aesthetic Appreciation in Neural Networks. (arXiv:2108.02646v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02646</id>
        <link href="http://arxiv.org/abs/2108.02646"/>
        <updated>2021-08-06T00:51:46.386Z</updated>
        <summary type="html"><![CDATA[This paper proposes a hypothesis for the aesthetic appreciation that
aesthetic images make a neural network strengthen salient concepts and discard
inessential concepts. In order to verify this hypothesis, we use multi-variate
interactions to represent salient concepts and inessential concepts contained
in images. Furthermore, we design a set of operations to revise images towards
more beautiful ones. In experiments, we find that the revised images are more
aesthetic than the original ones to some extent.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1"&gt;Xu Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1"&gt;Haotian Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_Z/0/1/0/all/0/1"&gt;Zhengyang Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Quanshi Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PI3NN: Prediction intervals from three independently trained neural networks. (arXiv:2108.02327v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02327</id>
        <link href="http://arxiv.org/abs/2108.02327"/>
        <updated>2021-08-06T00:51:46.379Z</updated>
        <summary type="html"><![CDATA[We propose a novel prediction interval method to learn prediction mean
values, lower and upper bounds of prediction intervals from three independently
trained neural networks only using the standard mean squared error (MSE) loss,
for uncertainty quantification in regression tasks. Our method requires no
distributional assumption on data, does not introduce unusual hyperparameters
to either the neural network models or the loss function. Moreover, our method
can effectively identify out-of-distribution samples and reasonably quantify
their uncertainty. Numerical experiments on benchmark regression problems show
that our method outperforms the state-of-the-art methods with respect to
predictive uncertainty quality, robustness, and identification of
out-of-distribution samples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Siyan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1"&gt;Pei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_D/0/1/0/all/0/1"&gt;Dan Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1"&gt;Guannan Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PSTN: Periodic Spatial-temporal Deep Neural Network for Traffic Condition Prediction. (arXiv:2108.02424v1 [stat.AP])]]></title>
        <id>http://arxiv.org/abs/2108.02424</id>
        <link href="http://arxiv.org/abs/2108.02424"/>
        <updated>2021-08-06T00:51:46.373Z</updated>
        <summary type="html"><![CDATA[Accurate forecasting of traffic conditions is critical for improving safety,
stability, and efficiency of a city transportation system. In reality, it is
challenging to produce accurate traffic forecasts due to the complex and
dynamic spatiotemporal correlations. Most existing works only consider partial
characteristics and features of traffic data, and result in unsatisfactory
performances on modeling and forecasting. In this paper, we propose a periodic
spatial-temporal deep neural network (PSTN) with three pivotal modules to
improve the forecasting performance of traffic conditions through a novel
integration of three types of information. First, the historical traffic
information is folded and fed into a module consisting of a graph convolutional
network and a temporal convolutional network. Second, the recent traffic
information together with the historical output passes through the second
module consisting of a graph convolutional network and a gated recurrent unit
framework. Finally, a multi-layer perceptron is applied to process the
auxiliary road attributes and output the final predictions. Experimental
results on two publicly accessible real-world urban traffic data sets show that
the proposed PSTN outperforms the state-of-the-art benchmarks by significant
margins for short-term traffic conditions forecasting]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Wang_T/0/1/0/all/0/1"&gt;Tiange Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zijun Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Tsui_K/0/1/0/all/0/1"&gt;Kwok-Leung Tsui&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Redatuming physical systems using symmetric autoencoders. (arXiv:2108.02537v1 [physics.comp-ph])]]></title>
        <id>http://arxiv.org/abs/2108.02537</id>
        <link href="http://arxiv.org/abs/2108.02537"/>
        <updated>2021-08-06T00:51:46.367Z</updated>
        <summary type="html"><![CDATA[This paper considers physical systems described by hidden states and
indirectly observed through repeated measurements corrupted by unmodeled
nuisance parameters. A network-based representation learns to disentangle the
coherent information (relative to the state) from the incoherent nuisance
information (relative to the sensing). Instead of physical models, the
representation uses symmetry and stochastic regularization to inform an
autoencoder architecture called SymAE. It enables redatuming, i.e., creating
virtual data instances where the nuisances are uniformized across measurements.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Bharadwaj_P/0/1/0/all/0/1"&gt;Pawan Bharadwaj&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Li_M/0/1/0/all/0/1"&gt;Matthew Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Demanet_L/0/1/0/all/0/1"&gt;Laurent Demanet&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Forecasting the outcome of spintronic experiments with Neural Ordinary Differential Equations. (arXiv:2108.02318v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02318</id>
        <link href="http://arxiv.org/abs/2108.02318"/>
        <updated>2021-08-06T00:51:46.348Z</updated>
        <summary type="html"><![CDATA[Deep learning has an increasing impact to assist research, allowing, for
example, the discovery of novel materials. Until now, however, these artificial
intelligence techniques have fallen short of discovering the full differential
equation of an experimental physical system. Here we show that a dynamical
neural network, trained on a minimal amount of data, can predict the behavior
of spintronic devices with high accuracy and an extremely efficient simulation
time, compared to the micromagnetic simulations that are usually employed to
model them. For this purpose, we re-frame the formalism of Neural Ordinary
Differential Equations (ODEs) to the constraints of spintronics: few measured
outputs, multiple inputs and internal parameters. We demonstrate with
Spin-Neural ODEs an acceleration factor over 200 compared to micromagnetic
simulations for a complex problem -- the simulation of a reservoir computer
made of magnetic skyrmions (20 minutes compared to three days). In a second
realization, we show that we can predict the noisy response of experimental
spintronic nano-oscillators to varying inputs after training Spin-Neural ODEs
on five milliseconds of their measured response to different excitations.
Spin-Neural ODE is a disruptive tool for developing spintronic applications in
complement to micromagnetic simulations, which are time-consuming and cannot
fit experiments when noise or imperfections are present. Spin-Neural ODE can
also be generalized to other electronic devices involving dynamics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xing Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Araujo_F/0/1/0/all/0/1"&gt;Flavio Abreu Araujo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Riou_M/0/1/0/all/0/1"&gt;Mathieu Riou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Torrejon_J/0/1/0/all/0/1"&gt;Jacob Torrejon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ravelosona_D/0/1/0/all/0/1"&gt;Dafin&amp;#xe9; Ravelosona&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kang_W/0/1/0/all/0/1"&gt;Wang Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1"&gt;Weisheng Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grollier_J/0/1/0/all/0/1"&gt;Julie Grollier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Querlioz_D/0/1/0/all/0/1"&gt;Damien Querlioz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-task Federated Edge Learning (MtFEEL) in Wireless Networks. (arXiv:2108.02517v1 [cs.IT])]]></title>
        <id>http://arxiv.org/abs/2108.02517</id>
        <link href="http://arxiv.org/abs/2108.02517"/>
        <updated>2021-08-06T00:51:46.337Z</updated>
        <summary type="html"><![CDATA[Federated Learning (FL) has evolved as a promising technique to handle
distributed machine learning across edge devices. A single neural network (NN)
that optimises a global objective is generally learned in most work in FL,
which could be suboptimal for edge devices. Although works finding a NN
personalised for edge device specific tasks exist, they lack generalisation
and/or convergence guarantees. In this paper, a novel communication efficient
FL algorithm for personalised learning in a wireless setting with guarantees is
presented. The algorithm relies on finding a ``better`` empirical estimate of
losses at each device, using a weighted average of the losses across different
devices. It is devised from a Probably Approximately Correct (PAC) bound on the
true loss in terms of the proposed empirical loss and is bounded by (i) the
Rademacher complexity, (ii) the discrepancy, (iii) and a penalty term. Using a
signed gradient feedback to find a personalised NN at each device, it is also
proven to converge in a Rayleigh flat fading (in the uplink) channel, at a rate
of the order max{1/SNR,1/sqrt(T)} Experimental results show that the proposed
algorithm outperforms locally trained devices as well as the conventionally
used FedAvg and FedSGD algorithms under practical SNR regimes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mahara_S/0/1/0/all/0/1"&gt;Sawan Singh Mahara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+M%2E_S/0/1/0/all/0/1"&gt;Shruti M.&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bharath_B/0/1/0/all/0/1"&gt;B. N. Bharath&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-supervising Action Recognition by Statistical Moment and Subspace Descriptors. (arXiv:2001.04627v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2001.04627</id>
        <link href="http://arxiv.org/abs/2001.04627"/>
        <updated>2021-08-06T00:51:46.237Z</updated>
        <summary type="html"><![CDATA[In this paper, we build on a concept of self-supervision by taking RGB frames
as input to learn to predict both action concepts and auxiliary descriptors
e.g., object descriptors. So-called hallucination streams are trained to
predict auxiliary cues, simultaneously fed into classification layers, and then
hallucinated at the testing stage to aid network. We design and hallucinate two
descriptors, one leveraging four popular object detectors applied to training
videos, and the other leveraging image- and video-level saliency detectors. The
first descriptor encodes the detector- and ImageNet-wise class prediction
scores, confidence scores, and spatial locations of bounding boxes and frame
indexes to capture the spatio-temporal distribution of features per video.
Another descriptor encodes spatio-angular gradient distributions of saliency
maps and intensity patterns. Inspired by the characteristic function of the
probability distribution, we capture four statistical moments on the above
intermediate descriptors. As numbers of coefficients in the mean, covariance,
coskewness and cokurtotsis grow linearly, quadratically, cubically and
quartically w.r.t. the dimension of feature vectors, we describe the covariance
matrix by its leading n' eigenvectors (so-called subspace) and we capture
skewness/kurtosis rather than costly coskewness/cokurtosis. We obtain state of
the art on five popular datasets such as Charades and EPIC-Kitchens.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Lei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koniusz_P/0/1/0/all/0/1"&gt;Piotr Koniusz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Regret Analysis of Learning-Based MPC with Partially-Unknown Cost Function. (arXiv:2108.02307v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2108.02307</id>
        <link href="http://arxiv.org/abs/2108.02307"/>
        <updated>2021-08-06T00:51:46.229Z</updated>
        <summary type="html"><![CDATA[The exploration/exploitation trade-off is an inherent challenge in
data-driven and adaptive control. Though this trade-off has been studied for
multi-armed bandits, reinforcement learning (RL) for finite Markov chains, and
RL for linear control systems; it is less well-studied for learning-based
control of nonlinear control systems. A significant theoretical challenge in
the nonlinear setting is that, unlike the linear case, there is no explicit
characterization of an optimal controller for a given set of cost and system
parameters. We propose in this paper the use of a finite-horizon oracle
controller with perfect knowledge of all system parameters as a reference for
optimal control actions. First, this allows us to propose a new regret notion
with respect to this oracle finite-horizon controller. Second, this allows us
to develop learning-based policies that we prove achieve low regret (i.e.,
square-root regret up to a log-squared factor) with respect to this oracle
finite-horizon controller. This policy is developed in the context of
learning-based model predictive control (LBMPC). We conduct a statistical
analysis to prove finite sample concentration bounds for the estimation step of
our policy, and then we perform a control-theoretic analysis using techniques
from MPC- and optimization-theory to show this policy ensures closed-loop
stability and achieves low regret. We conclude with numerical experiments on a
model of heating, ventilation, and air-conditioning (HVAC) systems that show
the low regret of our policy in a setting where the cost function is
partially-unknown to the controller.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Dogan_I/0/1/0/all/0/1"&gt;Ilgin Dogan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Shen_Z/0/1/0/all/0/1"&gt;Zuo-Jun Max Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Aswani_A/0/1/0/all/0/1"&gt;Anil Aswani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-phase Liver Tumor Segmentation with Spatial Aggregation and Uncertain Region Inpainting. (arXiv:2108.00911v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2108.00911</id>
        <link href="http://arxiv.org/abs/2108.00911"/>
        <updated>2021-08-06T00:51:46.221Z</updated>
        <summary type="html"><![CDATA[Multi-phase computed tomography (CT) images provide crucial complementary
information for accurate liver tumor segmentation (LiTS). State-of-the-art
multi-phase LiTS methods usually fused cross-phase features through
phase-weighted summation or channel-attention based concatenation. However,
these methods ignored the spatial (pixel-wise) relationships between different
phases, hence leading to insufficient feature integration. In addition, the
performance of existing methods remains subject to the uncertainty in
segmentation, which is particularly acute in tumor boundary regions. In this
work, we propose a novel LiTS method to adequately aggregate multi-phase
information and refine uncertain region segmentation. To this end, we introduce
a spatial aggregation module (SAM), which encourages per-pixel interactions
between different phases, to make full use of cross-phase information.
Moreover, we devise an uncertain region inpainting module (URIM) to refine
uncertain pixels using neighboring discriminative features. Experiments on an
in-house multi-phase CT dataset of focal liver lesions (MPCT-FLLs) demonstrate
that our method achieves promising liver tumor segmentation and outperforms
state-of-the-arts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yue Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Peng_C/0/1/0/all/0/1"&gt;Chengtao Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Peng_L/0/1/0/all/0/1"&gt;Liying Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Huang_H/0/1/0/all/0/1"&gt;Huimin Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tong_R/0/1/0/all/0/1"&gt;Ruofeng Tong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lin_L/0/1/0/all/0/1"&gt;Lanfen Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1"&gt;Jingsong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yen-Wei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chen_Q/0/1/0/all/0/1"&gt;Qingqing Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hu_H/0/1/0/all/0/1"&gt;Hongjie Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Peng_Z/0/1/0/all/0/1"&gt;Zhiyi Peng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generalization in Multimodal Language Learning from Simulation. (arXiv:2108.02319v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02319</id>
        <link href="http://arxiv.org/abs/2108.02319"/>
        <updated>2021-08-06T00:51:46.200Z</updated>
        <summary type="html"><![CDATA[Neural networks can be powerful function approximators, which are able to
model high-dimensional feature distributions from a subset of examples drawn
from the target distribution. Naturally, they perform well at generalizing
within the limits of their target function, but they often fail to generalize
outside of the explicitly learned feature space. It is therefore an open
research topic whether and how neural network-based architectures can be
deployed for systematic reasoning. Many studies have shown evidence for poor
generalization, but they often work with abstract data or are limited to
single-channel input. Humans, however, learn and interact through a combination
of multiple sensory modalities, and rarely rely on just one. To investigate
compositional generalization in a multimodal setting, we generate an extensible
dataset with multimodal input sequences from simulation. We investigate the
influence of the underlying training data distribution on compostional
generalization in a minimal LSTM-based network trained in a supervised, time
continuous setting. We find compositional generalization to fail in simple
setups while improving with the number of objects, actions, and particularly
with a lot of color overlaps between objects. Furthermore, multimodality
strongly improves compositional generalization in settings where a pure vision
model struggles to generalize.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Eisermann_A/0/1/0/all/0/1"&gt;Aaron Eisermann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jae Hee Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weber_C/0/1/0/all/0/1"&gt;Cornelius Weber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wermter_S/0/1/0/all/0/1"&gt;Stefan Wermter&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Stable neural networks: large-width asymptotics and convergence rates. (arXiv:2108.02316v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02316</id>
        <link href="http://arxiv.org/abs/2108.02316"/>
        <updated>2021-08-06T00:51:46.193Z</updated>
        <summary type="html"><![CDATA[In modern deep learning, there is a recent and growing literature on the
interplay between large-width asymptotics for deep Gaussian neural networks
(NNs), i.e. deep NNs with Gaussian-distributed weights, and classes of Gaussian
stochastic processes (SPs). Such an interplay has proved to be critical in
several contexts of practical interest, e.g. Bayesian inference under Gaussian
SP priors, kernel regression for infinite-wide deep NNs trained via gradient
descent, and information propagation within infinite-wide NNs. Motivated by
empirical analysis, showing the potential of replacing Gaussian distributions
with Stable distributions for the NN's weights, in this paper we investigate
large-width asymptotics for (fully connected) feed-forward deep Stable NNs,
i.e. deep NNs with Stable-distributed weights. First, we show that as the width
goes to infinity jointly over the NN's layers, a suitable rescaled deep Stable
NN converges weakly to a Stable SP whose distribution is characterized
recursively through the NN's layers. Because of the non-triangular NN's
structure, this is a non-standard asymptotic problem, to which we propose a
novel and self-contained inductive approach, which may be of independent
interest. Then, we establish sup-norm convergence rates of a deep Stable NN to
a Stable SP, quantifying the critical difference between the settings of
``joint growth" and ``sequential growth" of the width over the NN's layers. Our
work extends recent results on infinite-wide limits for deep Gaussian NNs to
the more general deep Stable NNs, providing the first result on convergence
rates for infinite-wide deep NNs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Favaro_S/0/1/0/all/0/1"&gt;Stefano Favaro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fortini_S/0/1/0/all/0/1"&gt;Sandra Fortini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peluchetti_S/0/1/0/all/0/1"&gt;Stefano Peluchetti&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Aspis: A Robust Detection System for Distributed Learning. (arXiv:2108.02416v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02416</id>
        <link href="http://arxiv.org/abs/2108.02416"/>
        <updated>2021-08-06T00:51:46.184Z</updated>
        <summary type="html"><![CDATA[State of the art machine learning models are routinely trained on large scale
distributed clusters. Crucially, such systems can be compromised when some of
the computing devices exhibit abnormal (Byzantine) behavior and return
arbitrary results to the parameter server (PS). This behavior may be attributed
to a plethora of reasons including system failures and orchestrated attacks.
Existing work suggests robust aggregation and/or computational redundancy to
alleviate the effect of distorted gradients. However, most of these schemes are
ineffective when an adversary knows the task assignment and can judiciously
choose the attacked workers to induce maximal damage. Our proposed method Aspis
assigns gradient computations to worker nodes using a subset-based assignment
which allows for multiple consistency checks on the behavior of a worker node.
Examination of the calculated gradients and post-processing (clique-finding in
an appropriately constructed graph) by the central node allows for efficient
detection and subsequent exclusion of adversaries from the training process. We
prove the Byzantine resilience and detection guarantees of Aspis under weak and
strong attacks and extensively evaluate the system on various large-scale
training scenarios. The main metric for our experiments is the test accuracy
for which we demonstrate significant improvement of about 30% compared to many
state-of-the-art approaches on the CIFAR-10 dataset. The corresponding
reduction of the fraction of corrupted gradients ranges from 16% to 98%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Konstantinidis_K/0/1/0/all/0/1"&gt;Konstantinos Konstantinidis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramamoorthy_A/0/1/0/all/0/1"&gt;Aditya Ramamoorthy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Active Reinforcement Learning over MDPs. (arXiv:2108.02323v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02323</id>
        <link href="http://arxiv.org/abs/2108.02323"/>
        <updated>2021-08-06T00:51:46.175Z</updated>
        <summary type="html"><![CDATA[The past decade has seen the rapid development of Reinforcement Learning,
which acquires impressive performance with numerous training resources.
However, one of the greatest challenges in RL is generalization efficiency
(i.e., generalization performance in a unit time). This paper proposes a
framework of Active Reinforcement Learning (ARL) over MDPs to improve
generalization efficiency in a limited resource by instance selection. Given a
number of instances, the algorithm chooses out valuable instances as training
sets while training the policy, thereby costing fewer resources. Unlike
existing approaches, we attempt to actively select and use training data rather
than train on all the given data, thereby costing fewer resources. Furthermore,
we introduce a general instance evaluation metrics and selection mechanism into
the framework. Experiments results reveal that the proposed framework with
Proximal Policy Optimization as policy optimizer can effectively improve
generalization efficiency than unselect-ed and unbiased selected methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1"&gt;Qi Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_P/0/1/0/all/0/1"&gt;Peng Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_K/0/1/0/all/0/1"&gt;Ke Tang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BEANNA: A Binary-Enabled Architecture for Neural Network Acceleration. (arXiv:2108.02313v1 [cs.AR])]]></title>
        <id>http://arxiv.org/abs/2108.02313</id>
        <link href="http://arxiv.org/abs/2108.02313"/>
        <updated>2021-08-06T00:51:46.168Z</updated>
        <summary type="html"><![CDATA[Modern hardware design trends have shifted towards specialized hardware
acceleration for computationally intensive tasks like machine learning and
computer vision. While these complex workloads can be accelerated by commercial
GPUs, domain-specific hardware is far more optimal when needing to meet the
stringent memory, throughput, and power constraints of mobile and embedded
devices. This paper proposes and evaluates a Binary-Enabled Architecture for
Neural Network Acceleration (BEANNA), a neural network hardware accelerator
capable of processing both floating point and binary network layers. Through
the use of a novel 16x16 systolic array based matrix multiplier with processing
elements that compute both floating point and binary multiply-adds, BEANNA
seamlessly switches between high precision floating point and binary neural
network layers. Running at a clock speed of 100MHz, BEANNA achieves a peak
throughput of 52.8 GigaOps/second when operating in high precision mode, and
820 GigaOps/second when operating in binary mode. Evaluation of BEANNA was
performed by comparing a hybrid network with floating point outer layers and
binary hidden layers to a network with only floating point layers. The hybrid
network accelerated using BEANNA achieved a 194% throughput increase, a 68%
memory usage decrease, and a 66% energy consumption decrease per inference, all
this at the cost of a mere 0.23% classification accuracy decrease on the MNIST
dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Terrill_C/0/1/0/all/0/1"&gt;Caleb Terrill&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chu_F/0/1/0/all/0/1"&gt;Fred Chu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SimpModeling: Sketching Implicit Field to Guide Mesh Modeling for 3D Animalmorphic Head Design. (arXiv:2108.02548v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02548</id>
        <link href="http://arxiv.org/abs/2108.02548"/>
        <updated>2021-08-06T00:51:46.148Z</updated>
        <summary type="html"><![CDATA[Head shapes play an important role in 3D character design. In this work, we
propose SimpModeling, a novel sketch-based system for helping users, especially
amateur users, easily model 3D animalmorphic heads - a prevalent kind of heads
in character design. Although sketching provides an easy way to depict desired
shapes, it is challenging to infer dense geometric information from sparse line
drawings. Recently, deepnet-based approaches have been taken to address this
challenge and try to produce rich geometric details from very few strokes.
However, while such methods reduce users' workload, they would cause less
controllability of target shapes. This is mainly due to the uncertainty of the
neural prediction. Our system tackles this issue and provides good
controllability from three aspects: 1) we separate coarse shape design and
geometric detail specification into two stages and respectively provide
different sketching means; 2) in coarse shape designing, sketches are used for
both shape inference and geometric constraints to determine global geometry,
and in geometric detail crafting, sketches are used for carving surface
details; 3) in both stages, we use the advanced implicit-based shape inference
methods, which have strong ability to handle the domain gap between freehand
sketches and synthetic ones used for training. Experimental results confirm the
effectiveness of our method and the usability of our interactive system. We
also contribute to a dataset of high-quality 3D animal heads, which are
manually created by artists.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1"&gt;Zhongjin Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jie Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1"&gt;Heming Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_D/0/1/0/all/0/1"&gt;Dong Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1"&gt;Xiaoguang Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1"&gt;Hongbo Fu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarial learning of cancer tissue representations. (arXiv:2108.02223v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02223</id>
        <link href="http://arxiv.org/abs/2108.02223"/>
        <updated>2021-08-06T00:51:46.030Z</updated>
        <summary type="html"><![CDATA[Deep learning based analysis of histopathology images shows promise in
advancing the understanding of tumor progression, tumor micro-environment, and
their underpinning biological processes. So far, these approaches have focused
on extracting information associated with annotations. In this work, we ask how
much information can be learned from the tissue architecture itself.

We present an adversarial learning model to extract feature representations
of cancer tissue, without the need for manual annotations. We show that these
representations are able to identify a variety of morphological characteristics
across three cancer types: Breast, colon, and lung. This is supported by 1) the
separation of morphologic characteristics in the latent space; 2) the ability
to classify tissue type with logistic regression using latent representations,
with an AUC of 0.97 and 85% accuracy, comparable to supervised deep models; 3)
the ability to predict the presence of tumor in Whole Slide Images (WSIs) using
multiple instance learning (MIL), achieving an AUC of 0.98 and 94% accuracy.

Our results show that our model captures distinct phenotypic characteristics
of real tissue samples, paving the way for further understanding of tumor
progression and tumor micro-environment, and ultimately refining
histopathological classification for diagnosis and treatment. The code and
pretrained models are available at:
https://github.com/AdalbertoCq/Adversarial-learning-of-cancer-tissue-representations]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Quiros_A/0/1/0/all/0/1"&gt;Adalberto Claudio Quiros&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Coudray_N/0/1/0/all/0/1"&gt;Nicolas Coudray&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yeaton_A/0/1/0/all/0/1"&gt;Anna Yeaton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sunhem_W/0/1/0/all/0/1"&gt;Wisuwat Sunhem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Murray_Smith_R/0/1/0/all/0/1"&gt;Roderick Murray-Smith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsirigos_A/0/1/0/all/0/1"&gt;Aristotelis Tsirigos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_K/0/1/0/all/0/1"&gt;Ke Yuan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transfer Learning of Deep Spatiotemporal Networks to Model Arbitrarily Long Videos of Seizures. (arXiv:2106.12014v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.12014</id>
        <link href="http://arxiv.org/abs/2106.12014"/>
        <updated>2021-08-06T00:51:46.021Z</updated>
        <summary type="html"><![CDATA[Detailed analysis of seizure semiology, the symptoms and signs which occur
during a seizure, is critical for management of epilepsy patients. Inter-rater
reliability using qualitative visual analysis is often poor for semiological
features. Therefore, automatic and quantitative analysis of video-recorded
seizures is needed for objective assessment.

We present GESTURES, a novel architecture combining convolutional neural
networks (CNNs) and recurrent neural networks (RNNs) to learn deep
representations of arbitrarily long videos of epileptic seizures.

We use a spatiotemporal CNN (STCNN) pre-trained on large human action
recognition (HAR) datasets to extract features from short snippets (approx. 0.5
s) sampled from seizure videos. We then train an RNN to learn seizure-level
representations from the sequence of features.

We curated a dataset of seizure videos from 68 patients and evaluated
GESTURES on its ability to classify seizures into focal onset seizures (FOSs)
(N = 106) vs. focal to bilateral tonic-clonic seizures (TCSs) (N = 77),
obtaining an accuracy of 98.9% using bidirectional long short-term memory
(BLSTM) units.

We demonstrate that an STCNN trained on a HAR dataset can be used in
combination with an RNN to accurately represent arbitrarily long videos of
seizures. GESTURES can provide accurate seizure classification by modeling
sequences of semiologies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Perez_Garcia_F/0/1/0/all/0/1"&gt;Fernando P&amp;#xe9;rez-Garc&amp;#xed;a&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Scott_C/0/1/0/all/0/1"&gt;Catherine Scott&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sparks_R/0/1/0/all/0/1"&gt;Rachel Sparks&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Diehl_B/0/1/0/all/0/1"&gt;Beate Diehl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ourselin_S/0/1/0/all/0/1"&gt;S&amp;#xe9;bastien Ourselin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Entropic Out-of-Distribution Detection: Seamless Detection of Unknown Examples. (arXiv:2006.04005v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.04005</id>
        <link href="http://arxiv.org/abs/2006.04005"/>
        <updated>2021-08-06T00:51:46.014Z</updated>
        <summary type="html"><![CDATA[In this paper, we argue that the unsatisfactory out-of-distribution (OOD)
detection performance of neural networks is mainly due to the SoftMax loss
anisotropy and propensity to produce low entropy probability distributions in
disagreement with the principle of maximum entropy. Current out-of-distribution
(OOD) detection approaches usually do not directly fix the SoftMax loss
drawbacks, but rather build techniques to circumvent it. Unfortunately, those
methods usually produce undesired side effects (e.g., classification accuracy
drop, additional hyperparameters, slower inferences, and collecting extra
data). In the opposite direction, we propose replacing SoftMax loss with a
novel loss function that does not suffer from the mentioned weaknesses. The
proposed IsoMax loss is isotropic (exclusively distance-based) and provides
high entropy posterior probability distributions. Replacing the SoftMax loss by
IsoMax loss requires no model or training changes. Additionally, the models
trained with IsoMax loss produce as fast and energy-efficient inferences as
those trained using SoftMax loss. Moreover, no classification accuracy drop is
observed. The proposed method does not rely on outlier/background data,
hyperparameter tuning, temperature calibration, feature extraction, metric
learning, adversarial training, ensemble procedures, or generative models. Our
experiments showed that IsoMax loss works as a seamless SoftMax loss drop-in
replacement that significantly improves neural networks' OOD detection
performance. Hence, it may be used as a baseline OOD detection approach to be
combined with current or future OOD detection techniques to achieve even higher
results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Macedo_D/0/1/0/all/0/1"&gt;David Mac&amp;#xea;do&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_T/0/1/0/all/0/1"&gt;Tsang Ing Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zanchettin_C/0/1/0/all/0/1"&gt;Cleber Zanchettin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oliveira_A/0/1/0/all/0/1"&gt;Adriano L. I. Oliveira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ludermir_T/0/1/0/all/0/1"&gt;Teresa Ludermir&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spartus: A 9.4 TOp/s FPGA-based LSTM Accelerator Exploiting Spatio-temporal Sparsity. (arXiv:2108.02297v1 [cs.AR])]]></title>
        <id>http://arxiv.org/abs/2108.02297</id>
        <link href="http://arxiv.org/abs/2108.02297"/>
        <updated>2021-08-06T00:51:46.007Z</updated>
        <summary type="html"><![CDATA[Long Short-Term Memory (LSTM) recurrent networks are frequently used for
tasks involving time sequential data such as speech recognition. However, it is
difficult to deploy these networks on hardware to achieve high throughput and
low latency because the fully-connected structure makes LSTM networks a
memory-bounded algorithm. Previous work in LSTM accelerators either exploited
weight spatial sparsity or temporal sparsity. In this paper, we present a new
accelerator called "Spartus" that exploits spatio-temporal sparsity to achieve
ultra-low latency inference. The spatial sparsity was induced using our
proposed pruning method called Column-Balanced Targeted Dropout (CBTD) that
leads to structured sparse weight matrices benefiting workload balance. It
achieved up to 96% weight sparsity with negligible accuracy difference for an
LSTM network trained on a TIMIT phone recognition task. To induce temporal
sparsity in LSTM, we create the DeltaLSTM by extending the previous DeltaGRU
method to the LSTM network. This combined sparsity saves on weight memory
access and associated arithmetic operations simultaneously. Spartus was
implemented on a Xilinx Zynq-7100 FPGA. The per-sample latency for a single
DeltaLSTM layer of 1024 neurons running on Spartus is 1 us. Spartus achieved
9.4 TOp/s effective batch-1 throughput and 1.1 TOp/J energy efficiency, which
are respectively 4X and 7X higher than the previous state-of-the-art.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1"&gt;Chang Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Delbruck_T/0/1/0/all/0/1"&gt;Tobi Delbruck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Shih-Chii Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transformer-Based Attention Networks for Continuous Pixel-Wise Prediction. (arXiv:2103.12091v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.12091</id>
        <link href="http://arxiv.org/abs/2103.12091"/>
        <updated>2021-08-06T00:51:45.988Z</updated>
        <summary type="html"><![CDATA[While convolutional neural networks have shown a tremendous impact on various
computer vision tasks, they generally demonstrate limitations in explicitly
modeling long-range dependencies due to the intrinsic locality of the
convolution operation. Initially designed for natural language processing
tasks, Transformers have emerged as alternative architectures with innate
global self-attention mechanisms to capture long-range dependencies. In this
paper, we propose TransDepth, an architecture that benefits from both
convolutional neural networks and transformers. To avoid the network losing its
ability to capture local-level details due to the adoption of transformers, we
propose a novel decoder that employs attention mechanisms based on gates.
Notably, this is the first paper that applies transformers to pixel-wise
prediction problems involving continuous labels (i.e., monocular depth
prediction and surface normal estimation). Extensive experiments demonstrate
that the proposed TransDepth achieves state-of-the-art performance on three
challenging datasets. Our code is available at:
https://github.com/ygjwd12345/TransDepth.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1"&gt;Guanglei Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1"&gt;Hao Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1"&gt;Mingli Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sebe_N/0/1/0/all/0/1"&gt;Nicu Sebe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ricci_E/0/1/0/all/0/1"&gt;Elisa Ricci&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring Sequence Feature Alignment for Domain Adaptive Detection Transformers. (arXiv:2107.12636v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.12636</id>
        <link href="http://arxiv.org/abs/2107.12636"/>
        <updated>2021-08-06T00:51:45.914Z</updated>
        <summary type="html"><![CDATA[Detection transformers have recently shown promising object detection results
and attracted increasing attention. However, how to develop effective domain
adaptation techniques to improve its cross-domain performance remains
unexplored and unclear. In this paper, we delve into this topic and empirically
find that direct feature distribution alignment on the CNN backbone only brings
limited improvements, as it does not guarantee domain-invariant sequence
features in the transformer for prediction. To address this issue, we propose a
novel Sequence Feature Alignment (SFA) method that is specially designed for
the adaptation of detection transformers. Technically, SFA consists of a domain
query-based feature alignment (DQFA) module and a token-wise feature alignment
(TDA) module. In DQFA, a novel domain query is used to aggregate and align
global context from the token sequence of both domains. DQFA reduces the domain
discrepancy in global feature representations and object relations when
deploying in the transformer encoder and decoder, respectively. Meanwhile, TDA
aligns token features in the sequence from both domains, which reduces the
domain gaps in local and instance-level feature representations in the
transformer encoder and decoder, respectively. Besides, a novel bipartite
matching consistency loss is proposed to enhance the feature discriminability
for robust object detection. Experiments on three challenging benchmarks show
that SFA outperforms state-of-the-art domain adaptive object detection methods.
Code has been made available at: https://github.com/encounter1997/SFA.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1"&gt;Yang Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_F/0/1/0/all/0/1"&gt;Fengxiang He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zha_Z/0/1/0/all/0/1"&gt;Zheng-Jun Zha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1"&gt;Yonggang Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1"&gt;Dacheng Tao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TorchIO: A Python library for efficient loading, preprocessing, augmentation and patch-based sampling of medical images in deep learning. (arXiv:2003.04696v5 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.04696</id>
        <link href="http://arxiv.org/abs/2003.04696"/>
        <updated>2021-08-06T00:51:45.884Z</updated>
        <summary type="html"><![CDATA[Processing of medical images such as MRI or CT presents unique challenges
compared to RGB images typically used in computer vision. These include a lack
of labels for large datasets, high computational costs, and metadata to
describe the physical properties of voxels. Data augmentation is used to
artificially increase the size of the training datasets. Training with image
patches decreases the need for computational power. Spatial metadata needs to
be carefully taken into account in order to ensure a correct alignment of
volumes.

We present TorchIO, an open-source Python library to enable efficient
loading, preprocessing, augmentation and patch-based sampling of medical images
for deep learning. TorchIO follows the style of PyTorch and integrates standard
medical image processing libraries to efficiently process images during
training of neural networks. TorchIO transforms can be composed, reproduced,
traced and extended. We provide multiple generic preprocessing and augmentation
operations as well as simulation of MRI-specific artifacts.

Source code, comprehensive tutorials and extensive documentation for TorchIO
can be found at https://torchio.rtfd.io/. The package can be installed from the
Python Package Index running 'pip install torchio'. It includes a command-line
interface which allows users to apply transforms to image files without using
Python. Additionally, we provide a graphical interface within a TorchIO
extension in 3D Slicer to visualize the effects of transforms.

TorchIO was developed to help researchers standardize medical image
processing pipelines and allow them to focus on the deep learning experiments.
It encourages open science, as it supports reproducibility and is version
controlled so that the software can be cited precisely. Due to its modularity,
the library is compatible with other frameworks for deep learning with medical
images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Perez_Garcia_F/0/1/0/all/0/1"&gt;Fernando P&amp;#xe9;rez-Garc&amp;#xed;a&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sparks_R/0/1/0/all/0/1"&gt;Rachel Sparks&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ourselin_S/0/1/0/all/0/1"&gt;S&amp;#xe9;bastien Ourselin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ImageNet-21K Pretraining for the Masses. (arXiv:2104.10972v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.10972</id>
        <link href="http://arxiv.org/abs/2104.10972"/>
        <updated>2021-08-06T00:51:45.876Z</updated>
        <summary type="html"><![CDATA[ImageNet-1K serves as the primary dataset for pretraining deep learning
models for computer vision tasks. ImageNet-21K dataset, which is bigger and
more diverse, is used less frequently for pretraining, mainly due to its
complexity, low accessibility, and underestimation of its added value. This
paper aims to close this gap, and make high-quality efficient pretraining on
ImageNet-21K available for everyone. Via a dedicated preprocessing stage,
utilization of WordNet hierarchical structure, and a novel training scheme
called semantic softmax, we show that various models significantly benefit from
ImageNet-21K pretraining on numerous datasets and tasks, including small
mobile-oriented models. We also show that we outperform previous ImageNet-21K
pretraining schemes for prominent new models like ViT and Mixer. Our proposed
pretraining pipeline is efficient, accessible, and leads to SoTA reproducible
results, from a publicly available dataset. The training code and pretrained
models are available at: https://github.com/Alibaba-MIIL/ImageNet21K]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ridnik_T/0/1/0/all/0/1"&gt;Tal Ridnik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ben_Baruch_E/0/1/0/all/0/1"&gt;Emanuel Ben-Baruch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Noy_A/0/1/0/all/0/1"&gt;Asaf Noy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zelnik_Manor_L/0/1/0/all/0/1"&gt;Lihi Zelnik-Manor&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Extending Neural P-frame Codecs for B-frame Coding. (arXiv:2104.00531v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.00531</id>
        <link href="http://arxiv.org/abs/2104.00531"/>
        <updated>2021-08-06T00:51:45.858Z</updated>
        <summary type="html"><![CDATA[While most neural video codecs address P-frame coding (predicting each frame
from past ones), in this paper we address B-frame compression (predicting
frames using both past and future reference frames). Our B-frame solution is
based on the existing P-frame methods. As a result, B-frame coding capability
can easily be added to an existing neural codec. The basic idea of our B-frame
coding method is to interpolate the two reference frames to generate a single
reference frame and then use it together with an existing P-frame codec to
encode the input B-frame. Our studies show that the interpolated frame is a
much better reference for the P-frame codec compared to using the previous
frame as is usually done. Our results show that using the proposed method with
an existing P-frame codec can lead to 28.5%saving in bit-rate on the UVG
dataset compared to the P-frame codec while generating the same video quality.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Pourreza_R/0/1/0/all/0/1"&gt;Reza Pourreza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cohen_T/0/1/0/all/0/1"&gt;Taco S Cohen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[XVFI: eXtreme Video Frame Interpolation. (arXiv:2103.16206v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.16206</id>
        <link href="http://arxiv.org/abs/2103.16206"/>
        <updated>2021-08-06T00:51:45.850Z</updated>
        <summary type="html"><![CDATA[In this paper, we firstly present a dataset (X4K1000FPS) of 4K videos of 1000
fps with the extreme motion to the research community for video frame
interpolation (VFI), and propose an extreme VFI network, called XVFI-Net, that
first handles the VFI for 4K videos with large motion. The XVFI-Net is based on
a recursive multi-scale shared structure that consists of two cascaded modules
for bidirectional optical flow learning between two input frames (BiOF-I) and
for bidirectional optical flow learning from target to input frames (BiOF-T).
The optical flows are stably approximated by a complementary flow reversal
(CFR) proposed in BiOF-T module. During inference, the BiOF-I module can start
at any scale of input while the BiOF-T module only operates at the original
input scale so that the inference can be accelerated while maintaining highly
accurate VFI performance. Extensive experimental results show that our XVFI-Net
can successfully capture the essential information of objects with extremely
large motions and complex textures while the state-of-the-art methods exhibit
poor performance. Furthermore, our XVFI-Net framework also performs comparably
on the previous lower resolution benchmark dataset, which shows a robustness of
our algorithm as well. All source codes, pre-trained models, and proposed
X4K1000FPS datasets are publicly available at
https://github.com/JihyongOh/XVFI.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sim_H/0/1/0/all/0/1"&gt;Hyeonjun Sim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1"&gt;Jihyong Oh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1"&gt;Munchurl Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Review on Deep Learning in UAV Remote Sensing. (arXiv:2101.10861v2 [cs.CV] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2101.10861</id>
        <link href="http://arxiv.org/abs/2101.10861"/>
        <updated>2021-08-06T00:51:45.844Z</updated>
        <summary type="html"><![CDATA[Deep Neural Networks (DNNs) learn representation from data with an impressive
capability, and brought important breakthroughs for processing images,
time-series, natural language, audio, video, and many others. In the remote
sensing field, surveys and literature revisions specifically involving DNNs
algorithms' applications have been conducted in an attempt to summarize the
amount of information produced in its subfields. Recently, Unmanned Aerial
Vehicles (UAV) based applications have dominated aerial sensing research.
However, a literature revision that combines both "deep learning" and "UAV
remote sensing" thematics has not yet been conducted. The motivation for our
work was to present a comprehensive review of the fundamentals of Deep Learning
(DL) applied in UAV-based imagery. We focused mainly on describing
classification and regression techniques used in recent applications with
UAV-acquired data. For that, a total of 232 papers published in international
scientific journal databases was examined. We gathered the published material
and evaluated their characteristics regarding application, sensor, and
technique used. We relate how DL presents promising results and has the
potential for processing tasks associated with UAV-based image data. Lastly, we
project future perspectives, commentating on prominent DL paths to be explored
in the UAV remote sensing field. Our revision consists of a friendly-approach
to introduce, commentate, and summarize the state-of-the-art in UAV-based image
applications with DNNs algorithms in diverse subfields of remote sensing,
grouping it in the environmental, urban, and agricultural contexts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Osco_L/0/1/0/all/0/1"&gt;Lucas Prado Osco&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Junior_J/0/1/0/all/0/1"&gt;Jos&amp;#xe9; Marcato Junior&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramos_A/0/1/0/all/0/1"&gt;Ana Paula Marques Ramos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jorge_L/0/1/0/all/0/1"&gt;L&amp;#xfa;cio Andr&amp;#xe9; de Castro Jorge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fatholahi_S/0/1/0/all/0/1"&gt;Sarah Narges Fatholahi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Silva_J/0/1/0/all/0/1"&gt;Jonathan de Andrade Silva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Matsubara_E/0/1/0/all/0/1"&gt;Edson Takashi Matsubara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pistori_H/0/1/0/all/0/1"&gt;Hemerson Pistori&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goncalves_W/0/1/0/all/0/1"&gt;Wesley Nunes Gon&amp;#xe7;alves&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jonathan Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semi- and Self-Supervised Multi-View Fusion of 3D Microscopy Images using Generative Adversarial Networks. (arXiv:2108.02743v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02743</id>
        <link href="http://arxiv.org/abs/2108.02743"/>
        <updated>2021-08-06T00:51:45.837Z</updated>
        <summary type="html"><![CDATA[Recent developments in fluorescence microscopy allow capturing
high-resolution 3D images over time for living model organisms. To be able to
image even large specimens, techniques like multi-view light-sheet imaging
record different orientations at each time point that can then be fused into a
single high-quality volume. Based on measured point spread functions (PSF),
deconvolution and content fusion are able to largely revert the inevitable
degradation occurring during the imaging process. Classical multi-view
deconvolution and fusion methods mainly use iterative procedures and
content-based averaging. Lately, Convolutional Neural Networks (CNNs) have been
deployed to approach 3D single-view deconvolution microscopy, but the
multi-view case waits to be studied. We investigated the efficacy of CNN-based
multi-view deconvolution and fusion with two synthetic data sets that mimic
developing embryos and involve either two or four complementary 3D views.
Compared with classical state-of-the-art methods, the proposed semi- and
self-supervised models achieve competitive and superior deconvolution and
fusion quality in the two-view and quad-view cases, respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1"&gt;Canyu Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eschweiler_D/0/1/0/all/0/1"&gt;Dennis Eschweiler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stegmaier_J/0/1/0/all/0/1"&gt;Johannes Stegmaier&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Structure First Detail Next: Image Inpainting with Pyramid Generator. (arXiv:2106.08905v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.08905</id>
        <link href="http://arxiv.org/abs/2106.08905"/>
        <updated>2021-08-06T00:51:45.826Z</updated>
        <summary type="html"><![CDATA[Recent deep generative models have achieved promising performance in image
inpainting. However, it is still very challenging for a neural network to
generate realistic image details and textures, due to its inherent spectral
bias. By our understanding of how artists work, we suggest to adopt a
`structure first detail next' workflow for image inpainting. To this end, we
propose to build a Pyramid Generator by stacking several sub-generators, where
lower-layer sub-generators focus on restoring image structures while the
higher-layer sub-generators emphasize image details. Given an input image, it
will be gradually restored by going through the entire pyramid in a bottom-up
fashion. Particularly, our approach has a learning scheme of progressively
increasing hole size, which allows it to restore large-hole images. In
addition, our method could fully exploit the benefits of learning with
high-resolution images, and hence is suitable for high-resolution image
inpainting. Extensive experimental results on benchmark datasets have validated
the effectiveness of our approach compared with state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qu_S/0/1/0/all/0/1"&gt;Shuyi Qu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niu_Z/0/1/0/all/0/1"&gt;Zhenxing Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1"&gt;Kaizhu Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jianke Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Protter_M/0/1/0/all/0/1"&gt;Matan Protter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zimerman_G/0/1/0/all/0/1"&gt;Gadi Zimerman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yinghui Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust CUR Decomposition: Theory and Imaging Applications. (arXiv:2101.05231v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.05231</id>
        <link href="http://arxiv.org/abs/2101.05231"/>
        <updated>2021-08-06T00:51:45.810Z</updated>
        <summary type="html"><![CDATA[This paper considers the use of Robust PCA in a CUR decomposition framework
and applications thereof. Our main algorithms produce a robust version of
column-row factorizations of matrices $\mathbf{D}=\mathbf{L}+\mathbf{S}$ where
$\mathbf{L}$ is low-rank and $\mathbf{S}$ contains sparse outliers. These
methods yield interpretable factorizations at low computational cost, and
provide new CUR decompositions that are robust to sparse outliers, in contrast
to previous methods. We consider two key imaging applications of Robust PCA:
video foreground-background separation and face modeling. This paper examines
the qualitative behavior of our Robust CUR decompositions on the benchmark
videos and face datasets, and find that our method works as well as standard
Robust PCA while being significantly faster. Additionally, we consider hybrid
randomized and deterministic sampling methods which produce a compact CUR
decomposition of a given matrix, and apply this to video sequences to produce
canonical frames thereof.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1"&gt;HanQin Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hamm_K/0/1/0/all/0/1"&gt;Keaton Hamm&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1"&gt;Longxiu Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Needell_D/0/1/0/all/0/1"&gt;Deanna Needell&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Distribution-Free, Risk-Controlling Prediction Sets. (arXiv:2101.02703v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.02703</id>
        <link href="http://arxiv.org/abs/2101.02703"/>
        <updated>2021-08-06T00:51:45.778Z</updated>
        <summary type="html"><![CDATA[While improving prediction accuracy has been the focus of machine learning in
recent years, this alone does not suffice for reliable decision-making.
Deploying learning systems in consequential settings also requires calibrating
and communicating the uncertainty of predictions. To convey instance-wise
uncertainty for prediction tasks, we show how to generate set-valued
predictions from a black-box predictor that control the expected loss on future
test points at a user-specified level. Our approach provides explicit
finite-sample guarantees for any dataset by using a holdout set to calibrate
the size of the prediction sets. This framework enables simple,
distribution-free, rigorous error control for many tasks, and we demonstrate it
in five large-scale machine learning problems: (1) classification problems
where some mistakes are more costly than others; (2) multi-label
classification, where each observation has multiple associated labels; (3)
classification problems where the labels have a hierarchical structure; (4)
image segmentation, where we wish to predict a set of pixels containing an
object of interest; and (5) protein structure prediction. Lastly, we discuss
extensions to uncertainty quantification for ranking, metric learning and
distributionally robust learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bates_S/0/1/0/all/0/1"&gt;Stephen Bates&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Angelopoulos_A/0/1/0/all/0/1"&gt;Anastasios Angelopoulos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lei_L/0/1/0/all/0/1"&gt;Lihua Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Malik_J/0/1/0/all/0/1"&gt;Jitendra Malik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jordan_M/0/1/0/all/0/1"&gt;Michael I. Jordan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Visual Pretraining with Contrastive Detection. (arXiv:2103.10957v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.10957</id>
        <link href="http://arxiv.org/abs/2103.10957"/>
        <updated>2021-08-06T00:51:45.764Z</updated>
        <summary type="html"><![CDATA[Self-supervised pretraining has been shown to yield powerful representations
for transfer learning. These performance gains come at a large computational
cost however, with state-of-the-art methods requiring an order of magnitude
more computation than supervised pretraining. We tackle this computational
bottleneck by introducing a new self-supervised objective, contrastive
detection, which tasks representations with identifying object-level features
across augmentations. This objective extracts a rich learning signal per image,
leading to state-of-the-art transfer accuracy on a variety of downstream tasks,
while requiring up to 10x less pretraining. In particular, our strongest
ImageNet-pretrained model performs on par with SEER, one of the largest
self-supervised systems to date, which uses 1000x more pretraining data.
Finally, our objective seamlessly handles pretraining on more complex images
such as those in COCO, closing the gap with supervised transfer learning from
COCO to PASCAL.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Henaff_O/0/1/0/all/0/1"&gt;Olivier J. H&amp;#xe9;naff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koppula_S/0/1/0/all/0/1"&gt;Skanda Koppula&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alayrac_J/0/1/0/all/0/1"&gt;Jean-Baptiste Alayrac&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oord_A/0/1/0/all/0/1"&gt;Aaron van den Oord&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vinyals_O/0/1/0/all/0/1"&gt;Oriol Vinyals&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carreira_J/0/1/0/all/0/1"&gt;Jo&amp;#xe3;o Carreira&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Instance Similarity Learning for Unsupervised Feature Representation. (arXiv:2108.02721v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02721</id>
        <link href="http://arxiv.org/abs/2108.02721"/>
        <updated>2021-08-06T00:51:45.757Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose an instance similarity learning (ISL) method for
unsupervised feature representation. Conventional methods assign close instance
pairs in the feature space with high similarity, which usually leads to wrong
pairwise relationship for large neighborhoods because the Euclidean distance
fails to depict the true semantic similarity on the feature manifold. On the
contrary, our method mines the feature manifold in an unsupervised manner,
through which the semantic similarity among instances is learned in order to
obtain discriminative representations. Specifically, we employ the Generative
Adversarial Networks (GAN) to mine the underlying feature manifold, where the
generated features are applied as the proxies to progressively explore the
feature manifold so that the semantic similarity among instances is acquired as
reliable pseudo supervision. Extensive experiments on image classification
demonstrate the superiority of our method compared with the state-of-the-art
methods. The code is available at https://github.com/ZiweiWangTHU/ISL.git.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Ziwei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yunsong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Ziyi Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1"&gt;Jiwen Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jie Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[UPDesc: Unsupervised Point Descriptor Learning for Robust Registration. (arXiv:2108.02740v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02740</id>
        <link href="http://arxiv.org/abs/2108.02740"/>
        <updated>2021-08-06T00:51:45.695Z</updated>
        <summary type="html"><![CDATA[In this work, we propose UPDesc, an unsupervised method to learn point
descriptors for robust point cloud registration. Our work builds upon a recent
supervised 3D CNN-based descriptor extraction framework, namely, 3DSmoothNet,
which leverages a voxel-based representation to parameterize the surrounding
geometry of interest points. Instead of using a predefined fixed-size local
support in voxelization, which potentially limits the access of richer local
geometry information, we propose to learn the support size in a data-driven
manner. To this end, we design a differentiable voxelization module that can
back-propagate gradients to the support size optimization. To optimize
descriptor similarity, the prior 3D CNN work and other supervised methods
require abundant correspondence labels or pose annotations of point clouds for
crafting metric learning losses. Differently, we show that unsupervised
learning of descriptor similarity can be achieved by performing geometric
registration in networks. Our learning objectives consider descriptor
similarity both across and within point clouds without supervision. Through
extensive experiments on point cloud registration benchmarks, we show that our
learned descriptors yield superior performance over existing unsupervised
methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1"&gt;Hongbo Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ovsjanikov_M/0/1/0/all/0/1"&gt;Maks Ovsjanikov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Parallel Capsule Networks for Classification of White Blood Cells. (arXiv:2108.02644v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02644</id>
        <link href="http://arxiv.org/abs/2108.02644"/>
        <updated>2021-08-06T00:51:45.676Z</updated>
        <summary type="html"><![CDATA[Capsule Networks (CapsNets) is a machine learning architecture proposed to
overcome some of the shortcomings of convolutional neural networks (CNNs).
However, CapsNets have mainly outperformed CNNs in datasets where images are
small and/or the objects to identify have minimal background noise. In this
work, we present a new architecture, parallel CapsNets, which exploits the
concept of branching the network to isolate certain capsules, allowing each
branch to identify different entities. We applied our concept to the two
current types of CapsNet architectures, studying the performance for networks
with different layers of capsules. We tested our design in a public, highly
unbalanced dataset of acute myeloid leukaemia images (15 classes). Our
experiments showed that conventional CapsNets show similar performance than our
baseline CNN (ResNeXt-50) but depict instability problems. In contrast,
parallel CapsNets can outperform ResNeXt-50, is more stable, and shows better
rotational invariance than both, conventional CapsNets and ResNeXt-50.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vigueras_Guillen_J/0/1/0/all/0/1"&gt;Juan P. Vigueras-Guill&amp;#xe9;n&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Patra_A/0/1/0/all/0/1"&gt;Arijit Patra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Engkvist_O/0/1/0/all/0/1"&gt;Ola Engkvist&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seeliger_F/0/1/0/all/0/1"&gt;Frank Seeliger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unifying Global-Local Representations in Salient Object Detection with Transformer. (arXiv:2108.02759v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02759</id>
        <link href="http://arxiv.org/abs/2108.02759"/>
        <updated>2021-08-06T00:51:45.669Z</updated>
        <summary type="html"><![CDATA[The fully convolutional network (FCN) has dominated salient object detection
for a long period. However, the locality of CNN requires the model deep enough
to have a global receptive field and such a deep model always leads to the loss
of local details. In this paper, we introduce a new attention-based encoder,
vision transformer, into salient object detection to ensure the globalization
of the representations from shallow to deep layers. With the global view in
very shallow layers, the transformer encoder preserves more local
representations to recover the spatial details in final saliency maps. Besides,
as each layer can capture a global view of its previous layer, adjacent layers
can implicitly maximize the representation differences and minimize the
redundant features, making that every output feature of transformer layers
contributes uniquely for final prediction. To decode features from the
transformer, we propose a simple yet effective deeply-transformed decoder. The
decoder densely decodes and upsamples the transformer features, generating the
final saliency map with less noise injection. Experimental results demonstrate
that our method significantly outperforms other FCN-based and transformer-based
methods in five benchmarks by a large margin, with an average of 12.17%
improvement in terms of Mean Absolute Error (MAE). Code will be available at
https://github.com/OliverRensu/GLSTR.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1"&gt;Sucheng Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_Q/0/1/0/all/0/1"&gt;Qiang Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_N/0/1/0/all/0/1"&gt;Nanxuan Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_G/0/1/0/all/0/1"&gt;Guoqiang Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1"&gt;Shengfeng He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SaRNet: A Dataset for Deep Learning Assisted Search and Rescue with Satellite Imagery. (arXiv:2107.12469v3 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.12469</id>
        <link href="http://arxiv.org/abs/2107.12469"/>
        <updated>2021-08-06T00:51:45.663Z</updated>
        <summary type="html"><![CDATA[Access to high resolution satellite imagery has dramatically increased in
recent years as several new constellations have entered service. High revisit
frequencies as well as improved resolution has widened the use cases of
satellite imagery to areas such as humanitarian relief and even Search and
Rescue (SaR). We propose a novel remote sensing object detection dataset for
deep learning assisted SaR. This dataset contains only small objects that have
been identified as potential targets as part of a live SaR response. We
evaluate the application of popular object detection models to this dataset as
a baseline to inform further research. We also propose a novel object detection
metric, specifically designed to be used in a deep learning assisted SaR
setting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Thoreau_M/0/1/0/all/0/1"&gt;Michael Thoreau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wilson_F/0/1/0/all/0/1"&gt;Frazer Wilson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Training independent subnetworks for robust prediction. (arXiv:2010.06610v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.06610</id>
        <link href="http://arxiv.org/abs/2010.06610"/>
        <updated>2021-08-06T00:51:45.656Z</updated>
        <summary type="html"><![CDATA[Recent approaches to efficiently ensemble neural networks have shown that
strong robustness and uncertainty performance can be achieved with a negligible
gain in parameters over the original network. However, these methods still
require multiple forward passes for prediction, leading to a significant
computational cost. In this work, we show a surprising result: the benefits of
using multiple predictions can be achieved `for free' under a single model's
forward pass. In particular, we show that, using a multi-input multi-output
(MIMO) configuration, one can utilize a single model's capacity to train
multiple subnetworks that independently learn the task at hand. By ensembling
the predictions made by the subnetworks, we improve model robustness without
increasing compute. We observe a significant improvement in negative
log-likelihood, accuracy, and calibration error on CIFAR10, CIFAR100, ImageNet,
and their out-of-distribution variants compared to previous methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Havasi_M/0/1/0/all/0/1"&gt;Marton Havasi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jenatton_R/0/1/0/all/0/1"&gt;Rodolphe Jenatton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fort_S/0/1/0/all/0/1"&gt;Stanislav Fort&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jeremiah Zhe Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Snoek_J/0/1/0/all/0/1"&gt;Jasper Snoek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lakshminarayanan_B/0/1/0/all/0/1"&gt;Balaji Lakshminarayanan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_A/0/1/0/all/0/1"&gt;Andrew M. Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tran_D/0/1/0/all/0/1"&gt;Dustin Tran&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VisualTextRank: Unsupervised Graph-based Content Extraction for Automating Ad Text to Image Search. (arXiv:2108.02725v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.02725</id>
        <link href="http://arxiv.org/abs/2108.02725"/>
        <updated>2021-08-06T00:51:45.649Z</updated>
        <summary type="html"><![CDATA[Numerous online stock image libraries offer high quality yet copyright free
images for use in marketing campaigns. To assist advertisers in navigating such
third party libraries, we study the problem of automatically fetching relevant
ad images given the ad text (via a short textual query for images). Motivated
by our observations in logged data on ad image search queries (given ad text),
we formulate a keyword extraction problem, where a keyword extracted from the
ad text (or its augmented version) serves as the ad image query. In this
context, we propose VisualTextRank: an unsupervised method to (i) augment input
ad text using semantically similar ads, and (ii) extract the image query from
the augmented ad text. VisualTextRank builds on prior work on graph based
context extraction (biased TextRank in particular) by leveraging both the text
and image of similar ads for better keyword extraction, and using advertiser
category specific biasing with sentence-BERT embeddings. Using data collected
from the Verizon Media Native (Yahoo Gemini) ad platform's stock image search
feature for onboarding advertisers, we demonstrate the superiority of
VisualTextRank compared to competitive keyword extraction baselines (including
an $11\%$ accuracy lift over biased TextRank). For the case when the stock
image library is restricted to English queries, we show the effectiveness of
VisualTextRank on multilingual ads (translated to English) while leveraging
semantically similar English ads. Online tests with a simplified version of
VisualTextRank led to a 28.7% increase in the usage of stock image search, and
a 41.6% increase in the advertiser onboarding rate in the Verizon Media Native
ad platform.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1"&gt;Shaunak Mishra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kuznetsov_M/0/1/0/all/0/1"&gt;Mikhail Kuznetsov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srivastava_G/0/1/0/all/0/1"&gt;Gaurav Srivastava&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sviridenko_M/0/1/0/all/0/1"&gt;Maxim Sviridenko&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Video Contrastive Learning with Global Context. (arXiv:2108.02722v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02722</id>
        <link href="http://arxiv.org/abs/2108.02722"/>
        <updated>2021-08-06T00:51:45.641Z</updated>
        <summary type="html"><![CDATA[Contrastive learning has revolutionized self-supervised image representation
learning field, and recently been adapted to video domain. One of the greatest
advantages of contrastive learning is that it allows us to flexibly define
powerful loss objectives as long as we can find a reasonable way to formulate
positive and negative samples to contrast. However, existing approaches rely
heavily on the short-range spatiotemporal salience to form clip-level
contrastive signals, thus limit themselves from using global context. In this
paper, we propose a new video-level contrastive learning method based on
segments to formulate positive pairs. Our formulation is able to capture global
context in a video, thus robust to temporal content change. We also incorporate
a temporal order regularization term to enforce the inherent sequential
structure of videos. Extensive experiments show that our video-level
contrastive learning framework (VCLR) is able to outperform previous
state-of-the-arts on five video datasets for downstream action classification,
action localization and video retrieval. Code is available at
https://github.com/amazon-research/video-contrastive-learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kuang_H/0/1/0/all/0/1"&gt;Haofei Kuang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yi Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xinyu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tighe_J/0/1/0/all/0/1"&gt;Joseph Tighe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schwertfeger_S/0/1/0/all/0/1"&gt;S&amp;#xf6;ren Schwertfeger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stachniss_C/0/1/0/all/0/1"&gt;Cyrill Stachniss&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1"&gt;Mu Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GuavaNet: A deep neural network architecture for automatic sensory evaluation to predict degree of acceptability for Guava by a consumer. (arXiv:2108.02563v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02563</id>
        <link href="http://arxiv.org/abs/2108.02563"/>
        <updated>2021-08-06T00:51:45.622Z</updated>
        <summary type="html"><![CDATA[This thesis is divided into two parts:Part I: Analysis of Fruits, Vegetables,
Cheese and Fish based on Image Processing using Computer Vision and Deep
Learning: A Review. It consists of a comprehensive review of image processing,
computer vision and deep learning techniques applied to carry out analysis of
fruits, vegetables, cheese and fish.This part also serves as a literature
review for Part II.Part II: GuavaNet: A deep neural network architecture for
automatic sensory evaluation to predict degree of acceptability for Guava by a
consumer. This part introduces to an end-to-end deep neural network
architecture that can predict the degree of acceptability by the consumer for a
guava based on sensory evaluation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mehra_V/0/1/0/all/0/1"&gt;Vipul Mehra&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NPMs: Neural Parametric Models for 3D Deformable Shapes. (arXiv:2104.00702v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.00702</id>
        <link href="http://arxiv.org/abs/2104.00702"/>
        <updated>2021-08-06T00:51:45.614Z</updated>
        <summary type="html"><![CDATA[Parametric 3D models have enabled a wide variety of tasks in computer
graphics and vision, such as modeling human bodies, faces, and hands. However,
the construction of these parametric models is often tedious, as it requires
heavy manual tweaking, and they struggle to represent additional complexity and
details such as wrinkles or clothing. To this end, we propose Neural Parametric
Models (NPMs), a novel, learned alternative to traditional, parametric 3D
models, which does not require hand-crafted, object-specific constraints. In
particular, we learn to disentangle 4D dynamics into latent-space
representations of shape and pose, leveraging the flexibility of recent
developments in learned implicit functions. Crucially, once learned, our neural
parametric models of shape and pose enable optimization over the learned spaces
to fit to new observations, similar to the fitting of a traditional parametric
model, e.g., SMPL. This enables NPMs to achieve a significantly more accurate
and detailed representation of observed deformable sequences. We show that NPMs
improve notably over both parametric and non-parametric state of the art in
reconstruction and tracking of monocular depth sequences of clothed humans and
hands. Latent-space interpolation as well as shape/pose transfer experiments
further demonstrate the usefulness of NPMs. Code is publicly available at
https://pablopalafox.github.io/npms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Palafox_P/0/1/0/all/0/1"&gt;Pablo Palafox&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bozic_A/0/1/0/all/0/1"&gt;Alja&amp;#x17e; Bo&amp;#x17e;i&amp;#x10d;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thies_J/0/1/0/all/0/1"&gt;Justus Thies&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niessner_M/0/1/0/all/0/1"&gt;Matthias Nie&amp;#xdf;ner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_A/0/1/0/all/0/1"&gt;Angela Dai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Object-Augmented RGB-D SLAM for Wide-Disparity Relocalisation. (arXiv:2108.02522v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02522</id>
        <link href="http://arxiv.org/abs/2108.02522"/>
        <updated>2021-08-06T00:51:45.607Z</updated>
        <summary type="html"><![CDATA[We propose a novel object-augmented RGB-D SLAM system that is capable of
constructing a consistent object map and performing relocalisation based on
centroids of objects in the map. The approach aims to overcome the view
dependence of appearance-based relocalisation methods using point features or
images. During the map construction, we use a pre-trained neural network to
detect objects and estimate 6D poses from RGB-D data. An incremental
probabilistic model is used to aggregate estimates over time to create the
object map. Then in relocalisation, we use the same network to extract
objects-of-interest in the `lost' frames. Pairwise geometric matching finds
correspondences between map and frame objects, and probabilistic absolute
orientation followed by application of iterative closest point to dense depth
maps and object centroids gives relocalisation. Results of experiments in
desktop environments demonstrate very high success rates even for frames with
widely different viewpoints from those used to construct the map, significantly
outperforming two appearance-based methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ming_Y/0/1/0/all/0/1"&gt;Yuhang Ming&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xingrui Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Calway_A/0/1/0/all/0/1"&gt;Andrew Calway&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Planning with Learned Dynamic Model for Unsupervised Point Cloud Registration. (arXiv:2108.02613v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02613</id>
        <link href="http://arxiv.org/abs/2108.02613"/>
        <updated>2021-08-06T00:51:45.599Z</updated>
        <summary type="html"><![CDATA[Point cloud registration is a fundamental problem in 3D computer vision. In
this paper, we cast point cloud registration into a planning problem in
reinforcement learning, which can seek the transformation between the source
and target point clouds through trial and error. By modeling the point cloud
registration process as a Markov decision process (MDP), we develop a latent
dynamic model of point clouds, consisting of a transformation network and
evaluation network. The transformation network aims to predict the new
transformed feature of the point cloud after performing a rigid transformation
(i.e., action) on it while the evaluation network aims to predict the alignment
precision between the transformed source point cloud and target point cloud as
the reward signal. Once the dynamic model of the point cloud is trained, we
employ the cross-entropy method (CEM) to iteratively update the planning policy
by maximizing the rewards in the point cloud registration process. Thus, the
optimal policy, i.e., the transformation between the source and target point
clouds, can be obtained via gradually narrowing the search space of the
transformation. Experimental results on ModelNet40 and 7Scene benchmark
datasets demonstrate that our method can yield good registration performance in
an unsupervised manner.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1"&gt;Haobo Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qian_J/0/1/0/all/0/1"&gt;Jianjun Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1"&gt;Jin Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jian Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fairness Properties of Face Recognition and Obfuscation Systems. (arXiv:2108.02707v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02707</id>
        <link href="http://arxiv.org/abs/2108.02707"/>
        <updated>2021-08-06T00:51:45.592Z</updated>
        <summary type="html"><![CDATA[The proliferation of automated facial recognition in various commercial and
government sectors has caused significant privacy concerns for individuals. A
recent and popular approach to address these privacy concerns is to employ
evasion attacks against the metric embedding networks powering facial
recognition systems. Face obfuscation systems generate imperceptible
perturbations, when added to an image, cause the facial recognition system to
misidentify the user. The key to these approaches is the generation of
perturbations using a pre-trained metric embedding network followed by their
application to an online system, whose model might be proprietary. This
dependence of face obfuscation on metric embedding networks, which are known to
be unfair in the context of facial recognition, surfaces the question of
demographic fairness -- \textit{are there demographic disparities in the
performance of face obfuscation systems?} To address this question, we perform
an analytical and empirical exploration of the performance of recent face
obfuscation systems that rely on deep embedding networks. We find that metric
embedding networks are demographically aware; they cluster faces in the
embedding space based on their demographic attributes. We observe that this
effect carries through to the face obfuscation systems: faces belonging to
minority groups incur reduced utility compared to those from majority groups.
For example, the disparity in average obfuscation success rate on the online
Face++ API can reach up to 20 percentage points. Further, for some demographic
groups, the average perturbation size increases by up to 17\% when choosing a
target identity belonging to a different demographic group versus the same
demographic group. Finally, we present a simple analytical model to provide
insights into these phenomena.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rosenberg_H/0/1/0/all/0/1"&gt;Harrison Rosenberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_B/0/1/0/all/0/1"&gt;Brian Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fawaz_K/0/1/0/all/0/1"&gt;Kassem Fawaz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jha_S/0/1/0/all/0/1"&gt;Somesh Jha&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Model-free Vehicle Tracking and State Estimation in Point Cloud Sequences. (arXiv:2103.06028v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.06028</id>
        <link href="http://arxiv.org/abs/2103.06028"/>
        <updated>2021-08-06T00:51:45.576Z</updated>
        <summary type="html"><![CDATA[Estimating the states of surrounding traffic participants stays at the core
of autonomous driving. In this paper, we study a novel setting of this problem:
model-free single-object tracking (SOT), which takes the object state in the
first frame as input, and jointly solves state estimation and tracking in
subsequent frames. The main purpose for this new setting is to break the strong
limitation of the popular "detection and tracking" scheme in multi-object
tracking. Moreover, we notice that shape completion by overlaying the point
clouds, which is a by-product of our proposed task, not only improves the
performance of state estimation but also has numerous applications. As no
benchmark for this task is available so far, we construct a new dataset
LiDAR-SOT and corresponding evaluation protocols based on the Waymo Open
dataset. We then propose an optimization-based algorithm called SOTracker
involving point cloud registration, vehicle shapes, correspondence, and motion
priors. Our quantitative and qualitative results prove the effectiveness of our
SOTracker and reveal the challenging cases for SOT in point clouds, including
the sparsity of LiDAR data, abrupt motion variation, etc. Finally, we also
explore how the proposed task and algorithm may benefit other autonomous
driving applications, including simulating LiDAR scans, generating motion data,
and annotating optical flow. The code and protocols for our benchmark and
algorithm are available at https://github.com/TuSimple/LiDAR_SOT/. A video
demonstration is at https://www.youtube.com/watch?v=BpHixKs91i8.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pang_Z/0/1/0/all/0/1"&gt;Ziqi Pang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhichao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1"&gt;Naiyan Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generalizable Mixed-Precision Quantization via Attribution Rank Preservation. (arXiv:2108.02720v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02720</id>
        <link href="http://arxiv.org/abs/2108.02720"/>
        <updated>2021-08-06T00:51:45.568Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a generalizable mixed-precision quantization (GMPQ)
method for efficient inference. Conventional methods require the consistency of
datasets for bitwidth search and model deployment to guarantee the policy
optimality, leading to heavy search cost on challenging largescale datasets in
realistic applications. On the contrary, our GMPQ searches the
mixed-quantization policy that can be generalized to largescale datasets with
only a small amount of data, so that the search cost is significantly reduced
without performance degradation. Specifically, we observe that locating network
attribution correctly is general ability for accurate visual analysis across
different data distribution. Therefore, despite of pursuing higher model
accuracy and complexity, we preserve attribution rank consistency between the
quantized models and their full-precision counterparts via efficient
capacity-aware attribution imitation for generalizable mixed-precision
quantization strategy search. Extensive experiments show that our method
obtains competitive accuracy-complexity trade-off compared with the
state-of-the-art mixed-precision networks in significantly reduced search cost.
The code is available at https://github.com/ZiweiWangTHU/GMPQ.git.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Ziwei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_H/0/1/0/all/0/1"&gt;Han Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1"&gt;Jiwen Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jie Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sketch Your Own GAN. (arXiv:2108.02774v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02774</id>
        <link href="http://arxiv.org/abs/2108.02774"/>
        <updated>2021-08-06T00:51:45.561Z</updated>
        <summary type="html"><![CDATA[Can a user create a deep generative model by sketching a single example?
Traditionally, creating a GAN model has required the collection of a
large-scale dataset of exemplars and specialized knowledge in deep learning. In
contrast, sketching is possibly the most universally accessible way to convey a
visual concept. In this work, we present a method, GAN Sketching, for rewriting
GANs with one or more sketches, to make GANs training easier for novice users.
In particular, we change the weights of an original GAN model according to user
sketches. We encourage the model's output to match the user sketches through a
cross-domain adversarial loss. Furthermore, we explore different regularization
methods to preserve the original model's diversity and image quality.
Experiments have shown that our method can mold GANs to match shapes and poses
specified by sketches while maintaining realism and diversity. Finally, we
demonstrate a few applications of the resulting GAN, including latent space
interpolation and image editing.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Sheng-Yu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bau_D/0/1/0/all/0/1"&gt;David Bau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jun-Yan Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Domain Adaptation for Object Detection via Cross-Domain Semi-Supervised Learning. (arXiv:1911.07158v5 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1911.07158</id>
        <link href="http://arxiv.org/abs/1911.07158"/>
        <updated>2021-08-06T00:51:45.555Z</updated>
        <summary type="html"><![CDATA[Current state-of-the-art object detectors can have significant performance
drop when deployed in the wild due to domain gaps with training data.
Unsupervised Domain Adaptation (UDA) is a promising approach to adapt models
for new domains/environments without any expensive label cost. However, without
ground truth labels, most prior works on UDA for object detection tasks can
only perform coarse image-level and/or feature-level adaptation by using
adversarial learning methods. In this work, we show that such adversarial-based
methods can only reduce the domain style gap, but cannot address the domain
content distribution gap that is shown to be important for object detectors. To
overcome this limitation, we propose the Cross-Domain Semi-Supervised Learning
(CDSSL) framework by leveraging high-quality pseudo labels to learn better
representations from the target domain directly. To enable SSL for cross-domain
object detection, we propose fine-grained domain transfer,
progressive-confidence-based label sharpening and imbalanced sampling strategy
to address two challenges: (i) non-identical distribution between source and
target domain data, (ii) error amplification/accumulation due to noisy pseudo
labeling on the target domain. Experiment results show that our proposed
approach consistently achieves new state-of-the-art performance (2.2% - 9.5%
better than prior best work on mAP) under various domain gap scenarios. The
code will be released.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1"&gt;Fuxun Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1"&gt;Di Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yinpeng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karianakis_N/0/1/0/all/0/1"&gt;Nikolaos Karianakis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_T/0/1/0/all/0/1"&gt;Tong Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1"&gt;Pei Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lymberopoulos_D/0/1/0/all/0/1"&gt;Dimitrios Lymberopoulos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1"&gt;Sidi Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1"&gt;Weisong Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xiang Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluation of Audio-Visual Alignments in Visually Grounded Speech Models. (arXiv:2108.02562v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02562</id>
        <link href="http://arxiv.org/abs/2108.02562"/>
        <updated>2021-08-06T00:51:45.531Z</updated>
        <summary type="html"><![CDATA[Systems that can find correspondences between multiple modalities, such as
between speech and images, have great potential to solve different recognition
and data analysis tasks in an unsupervised manner. This work studies multimodal
learning in the context of visually grounded speech (VGS) models, and focuses
on their recently demonstrated capability to extract spatiotemporal alignments
between spoken words and the corresponding visual objects without ever been
explicitly trained for object localization or word recognition. As the main
contributions, we formalize the alignment problem in terms of an audiovisual
alignment tensor that is based on earlier VGS work, introduce systematic
metrics for evaluating model performance in aligning visual objects and spoken
words, and propose a new VGS model variant for the alignment task utilizing
cross-modal attention layer. We test our model and a previously proposed model
in the alignment task using SPEECH-COCO captions coupled with MSCOCO images. We
compare the alignment performance using our proposed evaluation metrics to the
semantic retrieval task commonly used to evaluate VGS models. We show that
cross-modal attention layer not only helps the model to achieve higher semantic
cross-modal retrieval performance, but also leads to substantial improvements
in the alignment performance between image object and spoken words.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khorrami_K/0/1/0/all/0/1"&gt;Khazar Khorrami&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rasanen_O/0/1/0/all/0/1"&gt;Okko R&amp;#xe4;s&amp;#xe4;nen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Imperceptible Adversarial Examples by Spatial Chroma-Shift. (arXiv:2108.02502v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02502</id>
        <link href="http://arxiv.org/abs/2108.02502"/>
        <updated>2021-08-06T00:51:45.519Z</updated>
        <summary type="html"><![CDATA[Deep Neural Networks have been shown to be vulnerable to various kinds of
adversarial perturbations. In addition to widely studied additive noise based
perturbations, adversarial examples can also be created by applying a per pixel
spatial drift on input images. While spatial transformation based adversarial
examples look more natural to human observers due to absence of additive noise,
they still possess visible distortions caused by spatial transformations. Since
the human vision is more sensitive to the distortions in the luminance compared
to those in chrominance channels, which is one of the main ideas behind the
lossy visual multimedia compression standards, we propose a spatial
transformation based perturbation method to create adversarial examples by only
modifying the color components of an input image. While having competitive
fooling rates on CIFAR-10 and NIPS2017 Adversarial Learning Challenge datasets,
examples created with the proposed method have better scores with regards to
various perceptual quality metrics. Human visual perception studies validate
that the examples are more natural looking and often indistinguishable from
their original counterparts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aydin_A/0/1/0/all/0/1"&gt;Ayberk Aydin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sen_D/0/1/0/all/0/1"&gt;Deniz Sen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karli_B/0/1/0/all/0/1"&gt;Berat Tuna Karli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hanoglu_O/0/1/0/all/0/1"&gt;Oguz Hanoglu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Temizel_A/0/1/0/all/0/1"&gt;Alptekin Temizel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Object Wake-up: 3-D Object Reconstruction, Animation, and in-situ Rendering from a Single Image. (arXiv:2108.02708v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02708</id>
        <link href="http://arxiv.org/abs/2108.02708"/>
        <updated>2021-08-06T00:51:45.501Z</updated>
        <summary type="html"><![CDATA[Given a picture of a chair, could we extract the 3-D shape of the chair,
animate its plausible articulations and motions, and render in-situ in its
original image space? The above question prompts us to devise an automated
approach to extract and manipulate articulated objects in single images.
Comparing with previous efforts on object manipulation, our work goes beyond
2-D manipulation and focuses on articulable objects, thus introduces greater
flexibility for possible object deformations. The pipeline of our approach
starts by reconstructing and refining a 3-D mesh representation of the object
of interest from an input image; its control joints are predicted by exploiting
the semantic part segmentation information; the obtained object 3-D mesh is
then rigged \& animated by non-rigid deformation, and rendered to perform
in-situ motions in its original image space. Quantitative evaluations are
carried out on 3-D reconstruction from single images, an established task that
is related to our pipeline, where our results surpass those of the SOTAs by a
noticeable margin. Extensive visual results also demonstrate the applicability
of our approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zuo_X/0/1/0/all/0/1"&gt;Xinxin Zuo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Ji Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Sen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1"&gt;Zhenbo Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xinyu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ni_B/0/1/0/all/0/1"&gt;Bingbing Ni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_M/0/1/0/all/0/1"&gt;Minglun Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1"&gt;Li Cheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[UniCon: Unified Context Network for Robust Active Speaker Detection. (arXiv:2108.02607v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02607</id>
        <link href="http://arxiv.org/abs/2108.02607"/>
        <updated>2021-08-06T00:51:45.493Z</updated>
        <summary type="html"><![CDATA[We introduce a new efficient framework, the Unified Context Network (UniCon),
for robust active speaker detection (ASD). Traditional methods for ASD usually
operate on each candidate's pre-cropped face track separately and do not
sufficiently consider the relationships among the candidates. This potentially
limits performance, especially in challenging scenarios with low-resolution
faces, multiple candidates, etc. Our solution is a novel, unified framework
that focuses on jointly modeling multiple types of contextual information:
spatial context to indicate the position and scale of each candidate's face,
relational context to capture the visual relationships among the candidates and
contrast audio-visual affinities with each other, and temporal context to
aggregate long-term information and smooth out local uncertainties. Based on
such information, our model optimizes all candidates in a unified process for
robust and reliable ASD. A thorough ablation study is performed on several
challenging ASD benchmarks under different settings. In particular, our method
outperforms the state-of-the-art by a large margin of about 15% mean Average
Precision (mAP) absolute on two challenging subsets: one with three candidate
speakers, and the other with faces smaller than 64 pixels. Together, our UniCon
achieves 92.0% mAP on the AVA-ActiveSpeaker validation set, surpassing 90% for
the first time on this challenging dataset at the time of submission. Project
website: https://unicon-asd.github.io/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yuanhang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1"&gt;Susan Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1"&gt;Shuang Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Zhongqin Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shan_S/0/1/0/all/0/1"&gt;Shiguang Shan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xilin Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hierarchical Aggregation for 3D Instance Segmentation. (arXiv:2108.02350v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02350</id>
        <link href="http://arxiv.org/abs/2108.02350"/>
        <updated>2021-08-06T00:51:45.483Z</updated>
        <summary type="html"><![CDATA[Instance segmentation on point clouds is a fundamental task in 3D scene
perception. In this work, we propose a concise clustering-based framework named
HAIS, which makes full use of spatial relation of points and point sets.
Considering clustering-based methods may result in over-segmentation or
under-segmentation, we introduce the hierarchical aggregation to progressively
generate instance proposals, i.e., point aggregation for preliminarily
clustering points to sets and set aggregation for generating complete instances
from sets. Once the complete 3D instances are obtained, a sub-network of
intra-instance prediction is adopted for noisy points filtering and mask
quality scoring. HAIS is fast (only 410ms per frame) and does not require
non-maximum suppression. It ranks 1st on the ScanNet v2 benchmark, achieving
the highest 69.9% AP50 and surpassing previous state-of-the-art (SOTA) methods
by a large margin. Besides, the SOTA results on the S3DIS dataset validate the
good generalization ability. Code will be available at
https://github.com/hustvl/HAIS.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Shaoyu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fang_J/0/1/0/all/0/1"&gt;Jiemin Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Qian Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wenyu Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xinggang Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sentence-level Online Handwritten Chinese Character Recognition. (arXiv:2108.02561v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02561</id>
        <link href="http://arxiv.org/abs/2108.02561"/>
        <updated>2021-08-06T00:51:45.466Z</updated>
        <summary type="html"><![CDATA[Single online handwritten Chinese character recognition~(single OLHCCR) has
achieved prominent performance. However, in real application scenarios, users
always write multiple Chinese characters to form one complete sentence and the
contextual information within these characters holds the significant potential
to improve the accuracy, robustness and efficiency of sentence-level OLHCCR. In
this work, we first propose a simple and straightforward end-to-end network,
namely vanilla compositional network~(VCN) to tackle the sentence-level OLHCCR.
It couples convolutional neural network with sequence modeling architecture to
exploit the handwritten character's previous contextual information. Although
VCN performs much better than the state-of-the-art single OLHCCR model, it
exposes high fragility when confronting with not well written characters such
as sloppy writing, missing or broken strokes. To improve the robustness of
sentence-level OLHCCR, we further propose a novel deep spatial-temporal fusion
network~(DSTFN). It utilizes a pre-trained autoregresssive framework as the
backbone component, which projects each Chinese character into word embeddings,
and integrates the spatial glyph features of handwritten characters and their
contextual information multiple times at multi-layer fusion module. We also
construct a large-scale sentence-level handwriting dataset, named as CSOHD to
evaluate models. Extensive experiment results demonstrate that DSTFN achieves
the state-of-the-art performance, which presents strong robustness compared
with VCN and exiting single OLHCCR models. The in-depth empirical analysis and
case studies indicate that DSTFN can significantly improve the efficiency of
handwriting input, with the handwritten Chinese character with incomplete
strokes being recognized precisely.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yunxin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1"&gt;Qian Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1"&gt;Qingcai Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1"&gt;Lin Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_B/0/1/0/all/0/1"&gt;Baotian Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaolong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1"&gt;Yuxin Ding&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Colorectal Polyp Classification from White-light Colonoscopy Images via Domain Alignment. (arXiv:2108.02476v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02476</id>
        <link href="http://arxiv.org/abs/2108.02476"/>
        <updated>2021-08-06T00:51:45.458Z</updated>
        <summary type="html"><![CDATA[Differentiation of colorectal polyps is an important clinical examination. A
computer-aided diagnosis system is required to assist accurate diagnosis from
colonoscopy images. Most previous studies at-tempt to develop models for polyp
differentiation using Narrow-Band Imaging (NBI) or other enhanced images.
However, the wide range of these models' applications for clinical work has
been limited by the lagging of imaging techniques. Thus, we propose a novel
framework based on a teacher-student architecture for the accurate colorectal
polyp classification (CPC) through directly using white-light (WL) colonoscopy
images in the examination. In practice, during training, the auxiliary NBI
images are utilized to train a teacher network and guide the student network to
acquire richer feature representation from WL images. The feature transfer is
realized by domain alignment and contrastive learning. Eventually the final
student network has the ability to extract aligned features from only WL images
to facilitate the CPC task. Besides, we release the first public-available
paired CPC dataset containing WL-NBI pairs for the alignment training.
Quantitative and qualitative evaluation indicates that the proposed method
outperforms the previous methods in CPC, improving the accuracy by 5.6%with
very fast speed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1"&gt;Qin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Che_H/0/1/0/all/0/1"&gt;Hui Che&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_W/0/1/0/all/0/1"&gt;Weizhen Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiang_L/0/1/0/all/0/1"&gt;Li Xiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1"&gt;Guanbin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1"&gt;Shuguang Cui&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Comparison of Lossless Image Formats. (arXiv:2108.02557v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2108.02557</id>
        <link href="http://arxiv.org/abs/2108.02557"/>
        <updated>2021-08-06T00:51:45.451Z</updated>
        <summary type="html"><![CDATA[In recent years, a bag with image and video compression formats has been
torn. However, most of them are focused on lossy compression and only
marginally support the lossless mode. In this paper, I will focus on lossless
formats and the critical question: "Which one is the most efficient?" It turned
out that FLIF is currently the most efficient format for lossless image
compression. This finding is in contrast to that FLIF developers stopped its
development in favor of JPEG XL.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Barina_D/0/1/0/all/0/1"&gt;David Barina&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automatic Detection of Rail Components via A Deep Convolutional Transformer Network. (arXiv:2108.02423v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02423</id>
        <link href="http://arxiv.org/abs/2108.02423"/>
        <updated>2021-08-06T00:51:45.444Z</updated>
        <summary type="html"><![CDATA[Automatic detection of rail track and its fasteners via using continuously
collected railway images is important to maintenance as it can significantly
improve maintenance efficiency and better ensure system safety. Dominant
computer vision-based detection models typically rely on convolutional neural
networks that utilize local image features and cumbersome prior settings to
generate candidate boxes. In this paper, we propose a deep convolutional
transformer network based method to detect multi-class rail components
including the rail, clip, and bolt. We effectively synergize advantages of the
convolutional structure on extracting latent features from raw images as well
as advantages of transformers on selectively determining valuable latent
features to achieve an efficient and accurate performance on rail component
detections. Our proposed method simplifies the detection pipeline by
eliminating the need of prior settings, such as anchor box, aspect ratio,
default coordinates, and post-processing, such as the threshold for non-maximum
suppression; as well as allows users to trade off the quality and complexity of
the detector with limited training data. Results of a comprehensive
computational study show that our proposed method outperforms a set of existing
state-of-art approaches with large margins]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1"&gt;Tiange Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zijun Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1"&gt;Fangfang Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsui_K/0/1/0/all/0/1"&gt;Kwok-Leung Tsui&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spartus: A 9.4 TOp/s FPGA-based LSTM Accelerator Exploiting Spatio-temporal Sparsity. (arXiv:2108.02297v1 [cs.AR])]]></title>
        <id>http://arxiv.org/abs/2108.02297</id>
        <link href="http://arxiv.org/abs/2108.02297"/>
        <updated>2021-08-06T00:51:45.436Z</updated>
        <summary type="html"><![CDATA[Long Short-Term Memory (LSTM) recurrent networks are frequently used for
tasks involving time sequential data such as speech recognition. However, it is
difficult to deploy these networks on hardware to achieve high throughput and
low latency because the fully-connected structure makes LSTM networks a
memory-bounded algorithm. Previous work in LSTM accelerators either exploited
weight spatial sparsity or temporal sparsity. In this paper, we present a new
accelerator called "Spartus" that exploits spatio-temporal sparsity to achieve
ultra-low latency inference. The spatial sparsity was induced using our
proposed pruning method called Column-Balanced Targeted Dropout (CBTD) that
leads to structured sparse weight matrices benefiting workload balance. It
achieved up to 96% weight sparsity with negligible accuracy difference for an
LSTM network trained on a TIMIT phone recognition task. To induce temporal
sparsity in LSTM, we create the DeltaLSTM by extending the previous DeltaGRU
method to the LSTM network. This combined sparsity saves on weight memory
access and associated arithmetic operations simultaneously. Spartus was
implemented on a Xilinx Zynq-7100 FPGA. The per-sample latency for a single
DeltaLSTM layer of 1024 neurons running on Spartus is 1 us. Spartus achieved
9.4 TOp/s effective batch-1 throughput and 1.1 TOp/J energy efficiency, which
are respectively 4X and 7X higher than the previous state-of-the-art.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1"&gt;Chang Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Delbruck_T/0/1/0/all/0/1"&gt;Tobi Delbruck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Shih-Chii Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RCA-IUnet: A residual cross-spatial attention guided inception U-Net model for tumor segmentation in breast ultrasound imaging. (arXiv:2108.02508v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02508</id>
        <link href="http://arxiv.org/abs/2108.02508"/>
        <updated>2021-08-06T00:51:45.429Z</updated>
        <summary type="html"><![CDATA[The advancements in deep learning technologies have produced immense
contribution to biomedical image analysis applications. With breast cancer
being the common deadliest disease among women, early detection is the key
means to improve survivability. Medical imaging like ultrasound presents an
excellent visual representation of the functioning of the organs; however, for
any radiologist analysing such scans is challenging and time consuming which
delays the diagnosis process. Although various deep learning based approaches
are proposed that achieved promising results, the present article introduces an
efficient residual cross-spatial attention guided inception U-Net (RCA-IUnet)
model with minimal training parameters for tumor segmentation using breast
ultrasound imaging to further improve the segmentation performance of varying
tumor sizes. The RCA-IUnet model follows U-Net topology with residual inception
depth-wise separable convolution and hybrid pooling (max pooling and spectral
pooling) layers. In addition, cross-spatial attention filters are added to
suppress the irrelevant features and focus on the target structure. The
segmentation performance of the proposed model is validated on two publicly
available datasets using standard segmentation evaluation metrics, where it
outperformed the other state-of-the-art segmentation models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Punn_N/0/1/0/all/0/1"&gt;Narinder Singh Punn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1"&gt;Sonali Agarwal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Online Hashing with Similarity Learning. (arXiv:2108.02560v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02560</id>
        <link href="http://arxiv.org/abs/2108.02560"/>
        <updated>2021-08-06T00:51:45.421Z</updated>
        <summary type="html"><![CDATA[Online hashing methods usually learn the hash functions online, aiming to
efficiently adapt to the data variations in the streaming environment. However,
when the hash functions are updated, the binary codes for the whole database
have to be updated to be consistent with the hash functions, resulting in the
inefficiency in the online image retrieval process. In this paper, we propose a
novel online hashing framework without updating binary codes. In the proposed
framework, the hash functions are fixed and a parametric similarity function
for the binary codes is learnt online to adapt to the streaming data.
Specifically, a parametric similarity function that has a bilinear form is
adopted and a metric learning algorithm is proposed to learn the similarity
function online based on the characteristics of the hashing methods. The
experiments on two multi-label image datasets show that our method is
competitive or outperforms the state-of-the-art online hashing methods in terms
of both accuracy and efficiency for multi-label image retrieval.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Weng_Z/0/1/0/all/0/1"&gt;Zhenyu Weng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yuesheng Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring Structure Consistency for Deep Model Watermarking. (arXiv:2108.02360v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2108.02360</id>
        <link href="http://arxiv.org/abs/2108.02360"/>
        <updated>2021-08-06T00:51:45.414Z</updated>
        <summary type="html"><![CDATA[The intellectual property (IP) of Deep neural networks (DNNs) can be easily
``stolen'' by surrogate model attack. There has been significant progress in
solutions to protect the IP of DNN models in classification tasks. However,
little attention has been devoted to the protection of DNNs in image processing
tasks. By utilizing consistent invisible spatial watermarks, one recent work
first considered model watermarking for deep image processing networks and
demonstrated its efficacy in many downstream tasks. Nevertheless, it highly
depends on the hypothesis that the embedded watermarks in the network outputs
are consistent. When the attacker uses some common data augmentation attacks
(e.g., rotate, crop, and resize) during surrogate model training, it will
totally fail because the underlying watermark consistency is destroyed. To
mitigate this issue, we propose a new watermarking methodology, namely
``structure consistency'', based on which a new deep structure-aligned model
watermarking algorithm is designed. Specifically, the embedded watermarks are
designed to be aligned with physically consistent image structures, such as
edges or semantic regions. Experiments demonstrate that our method is much more
robust than the baseline method in resisting data augmentation attacks for
model IP protection. Besides that, we further test the generalization ability
and robustness of our method to a broader range of circumvention attacks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jie Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1"&gt;Dongdong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liao_J/0/1/0/all/0/1"&gt;Jing Liao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fang_H/0/1/0/all/0/1"&gt;Han Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1"&gt;Zehua Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Weiming Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hua_G/0/1/0/all/0/1"&gt;Gang Hua&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_N/0/1/0/all/0/1"&gt;Nenghai Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Poison Ink: Robust and Invisible Backdoor Attack. (arXiv:2108.02488v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2108.02488</id>
        <link href="http://arxiv.org/abs/2108.02488"/>
        <updated>2021-08-06T00:51:45.389Z</updated>
        <summary type="html"><![CDATA[Recent research shows deep neural networks are vulnerable to different types
of attacks, such as adversarial attack, data poisoning attack and backdoor
attack. Among them, backdoor attack is the most cunning one and can occur in
almost every stage of deep learning pipeline. Therefore, backdoor attack has
attracted lots of interests from both academia and industry. However, most
existing backdoor attack methods are either visible or fragile to some
effortless pre-processing such as common data transformations. To address these
limitations, we propose a robust and invisible backdoor attack called ``Poison
Ink''. Concretely, we first leverage the image structures as target poisoning
areas, and fill them with poison ink (information) to generate the trigger
pattern. As the image structure can keep its semantic meaning during the data
transformation, such trigger pattern is inherently robust to data
transformations. Then we leverage a deep injection network to embed such
trigger pattern into the cover image to achieve stealthiness. Compared to
existing popular backdoor attack methods, Poison Ink outperforms both in
stealthiness and robustness. Through extensive experiments, we demonstrate
Poison Ink is not only general to different datasets and network architectures,
but also flexible for different attack scenarios. Besides, it also has very
strong resistance against many state-of-the-art defense techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+zhang_J/0/1/0/all/0/1"&gt;Jie zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1"&gt;Dongdong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liao_J/0/1/0/all/0/1"&gt;Jing Liao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1"&gt;Qidong Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hua_G/0/1/0/all/0/1"&gt;Gang Hua&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Weiming Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_N/0/1/0/all/0/1"&gt;Nenghai Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[O2NA: An Object-Oriented Non-Autoregressive Approach for Controllable Video Captioning. (arXiv:2108.02359v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.02359</id>
        <link href="http://arxiv.org/abs/2108.02359"/>
        <updated>2021-08-06T00:51:45.370Z</updated>
        <summary type="html"><![CDATA[Video captioning combines video understanding and language generation.
Different from image captioning that describes a static image with details of
almost every object, video captioning usually considers a sequence of frames
and biases towards focused objects, e.g., the objects that stay in focus
regardless of the changing background. Therefore, detecting and properly
accommodating focused objects is critical in video captioning. To enforce the
description of focused objects and achieve controllable video captioning, we
propose an Object-Oriented Non-Autoregressive approach (O2NA), which performs
caption generation in three steps: 1) identify the focused objects and predict
their locations in the target caption; 2) generate the related attribute words
and relation words of these focused objects to form a draft caption; and 3)
combine video information to refine the draft caption to a fluent final
caption. Since the focused objects are generated and located ahead of other
words, it is difficult to apply the word-by-word autoregressive generation
process; instead, we adopt a non-autoregressive approach. The experiments on
two benchmark datasets, i.e., MSR-VTT and MSVD, demonstrate the effectiveness
of O2NA, which achieves results competitive with the state-of-the-arts but with
both higher diversity and higher inference speed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1"&gt;Fenglin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1"&gt;Xuancheng Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1"&gt;Xian Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1"&gt;Bang Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ge_S/0/1/0/all/0/1"&gt;Shen Ge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1"&gt;Xu Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimal Transport for Unsupervised Restoration Learning. (arXiv:2108.02574v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2108.02574</id>
        <link href="http://arxiv.org/abs/2108.02574"/>
        <updated>2021-08-06T00:51:45.355Z</updated>
        <summary type="html"><![CDATA[Recently, much progress has been made in unsupervised restoration learning.
However, existing methods more or less rely on some assumptions on the signal
and/or degradation model, which limits their practical performance. How to
construct an optimal criterion for unsupervised restoration learning without
any prior knowledge on the degradation model is still an open question. Toward
answering this question, this work proposes a criterion for unsupervised
restoration learning based on the optimal transport theory. This criterion has
favorable properties, e.g., approximately maximal preservation of the
information of the signal, whilst achieving perceptual reconstruction.
Furthermore, though a relaxed unconstrained formulation is used in practical
implementation, we show that the relaxed formulation in theory has the same
solution as the original constrained formulation. Experiments on synthetic and
real-world data, including realistic photographic, microscopy, depth, and raw
depth images, demonstrate that the proposed method even compares favorably with
supervised methods, e.g., approaching the PSNR of supervised methods while
having better perceptual quality. Particularly, for spatially correlated noise
and realistic microscopy images, the proposed method not only achieves better
perceptual quality but also has higher PSNR than supervised methods. Besides,
it shows remarkable superiority in harsh practical conditions with complex
noise, e.g., raw depth images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wen_F/0/1/0/all/0/1"&gt;Fei Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yan_Z/0/1/0/all/0/1"&gt;Zeyu Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ying_R/0/1/0/all/0/1"&gt;Rendong Ying&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liu_P/0/1/0/all/0/1"&gt;Peilin Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[M2IOSR: Maximal Mutual Information Open Set Recognition. (arXiv:2108.02373v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02373</id>
        <link href="http://arxiv.org/abs/2108.02373"/>
        <updated>2021-08-06T00:51:45.348Z</updated>
        <summary type="html"><![CDATA[In this work, we aim to address the challenging task of open set recognition
(OSR). Many recent OSR methods rely on auto-encoders to extract class-specific
features by a reconstruction strategy, requiring the network to restore the
input image on pixel-level. This strategy is commonly over-demanding for OSR
since class-specific features are generally contained in target objects, not in
all pixels. To address this shortcoming, here we discard the pixel-level
reconstruction strategy and pay more attention to improving the effectiveness
of class-specific feature extraction. We propose a mutual information-based
method with a streamlined architecture, Maximal Mutual Information Open Set
Recognition (M2IOSR). The proposed M2IOSR only uses an encoder to extract
class-specific features by maximizing the mutual information between the given
input and its latent features across multiple scales. Meanwhile, to further
reduce the open space risk, latent features are constrained to class
conditional Gaussian distributions by a KL-divergence loss function. In this
way, a strong function is learned to prevent the network from mapping different
observations to similar latent features and help the network extract
class-specific features with desired statistical characteristics. The proposed
method significantly improves the performance of baselines and achieves new
state-of-the-art results on several benchmarks consistently. Source codes are
uploaded in supplementary materials.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1"&gt;Xin Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_H/0/1/0/all/0/1"&gt;Henghui Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1"&gt;Guosheng Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ling_K/0/1/0/all/0/1"&gt;Keck-Voon Ling&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MS-KD: Multi-Organ Segmentation with Multiple Binary-Labeled Datasets. (arXiv:2108.02559v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02559</id>
        <link href="http://arxiv.org/abs/2108.02559"/>
        <updated>2021-08-06T00:51:45.325Z</updated>
        <summary type="html"><![CDATA[Annotating multiple organs in 3D medical images is time-consuming and costly.
Meanwhile, there exist many single-organ datasets with one specific organ
annotated. This paper investigates how to learn a multi-organ segmentation
model leveraging a set of binary-labeled datasets. A novel Multi-teacher
Single-student Knowledge Distillation (MS-KD) framework is proposed, where the
teacher models are pre-trained single-organ segmentation networks, and the
student model is a multi-organ segmentation network. Considering that each
teacher focuses on different organs, a region-based supervision method,
consisting of logits-wise supervision and feature-wise supervision, is
proposed. Each teacher supervises the student in two regions, the organ region
where the teacher is considered as an expert and the background region where
all teachers agree. Extensive experiments on three public single-organ datasets
and a multi-organ dataset have demonstrated the effectiveness of the proposed
MS-KD framework.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1"&gt;Shixiang Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yuhang Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiaoman Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Ya Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yanfeng Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MixLacune: Segmentation of lacunes of presumed vascular origin. (arXiv:2108.02483v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02483</id>
        <link href="http://arxiv.org/abs/2108.02483"/>
        <updated>2021-08-06T00:51:45.318Z</updated>
        <summary type="html"><![CDATA[Lacunes of presumed vascular origin are fluid-filled cavities of between 3 -
15 mm in diameter, visible on T1 and FLAIR brain MRI. Quantification of lacunes
relies on manual annotation or semi-automatic / interactive approaches; and
almost no automatic methods exist for this task. In this work, we present a
two-stage approach to segment lacunes of presumed vascular origin: (1)
detection with Mask R-CNN followed by (2) segmentation with a U-Net CNN. Data
originates from Task 3 of the "Where is VALDO?" challenge and consists of 40
training subjects. We report the mean DICE on the training set of 0.83 and on
the validation set of 0.84. Source code is available at:
https://github.com/hjkuijf/MixLacune . The docker container hjkuijf/mixlacune
can be pulled from https://hub.docker.com/r/hjkuijf/mixlacune .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kutnar_D/0/1/0/all/0/1"&gt;Denis Kutnar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Velden_B/0/1/0/all/0/1"&gt;Bas H.M. van der Velden&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sanguesa_M/0/1/0/all/0/1"&gt;Marta Girones Sanguesa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geerlings_M/0/1/0/all/0/1"&gt;Mirjam I. Geerlings&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Biesbroek_J/0/1/0/all/0/1"&gt;J. Matthijs Biesbroek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kuijf_H/0/1/0/all/0/1"&gt;Hugo J. Kuijf&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unifying Nonlocal Blocks for Neural Networks. (arXiv:2108.02451v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02451</id>
        <link href="http://arxiv.org/abs/2108.02451"/>
        <updated>2021-08-06T00:51:45.311Z</updated>
        <summary type="html"><![CDATA[The nonlocal-based blocks are designed for capturing long-range
spatial-temporal dependencies in computer vision tasks. Although having shown
excellent performance, they still lack the mechanism to encode the rich,
structured information among elements in an image or video. In this paper, to
theoretically analyze the property of these nonlocal-based blocks, we provide a
new perspective to interpret them, where we view them as a set of graph filters
generated on a fully-connected graph. Specifically, when choosing the Chebyshev
graph filter, a unified formulation can be derived for explaining and analyzing
the existing nonlocal-based blocks (e.g., nonlocal block, nonlocal stage,
double attention block). Furthermore, by concerning the property of spectral,
we propose an efficient and robust spectral nonlocal block, which can be more
robust and flexible to catch long-range dependencies when inserted into deep
neural networks than the existing nonlocal blocks. Experimental results
demonstrate the clear-cut improvements and practical applicabilities of our
method on image classification, action recognition, semantic segmentation, and
person re-identification tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1"&gt;Lei Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+She_Q/0/1/0/all/0/1"&gt;Qi She&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1"&gt;Duo Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1"&gt;Yanye Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kang_X/0/1/0/all/0/1"&gt;Xuejing Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1"&gt;Jie Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Changhu Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Webly Supervised Fine-Grained Recognition: Benchmark Datasets and An Approach. (arXiv:2108.02399v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02399</id>
        <link href="http://arxiv.org/abs/2108.02399"/>
        <updated>2021-08-06T00:51:45.264Z</updated>
        <summary type="html"><![CDATA[Learning from the web can ease the extreme dependence of deep learning on
large-scale manually labeled datasets. Especially for fine-grained recognition,
which targets at distinguishing subordinate categories, it will significantly
reduce the labeling costs by leveraging free web data. Despite its significant
practical and research value, the webly supervised fine-grained recognition
problem is not extensively studied in the computer vision community, largely
due to the lack of high-quality datasets. To fill this gap, in this paper we
construct two new benchmark webly supervised fine-grained datasets, termed
WebFG-496 and WebiNat-5089, respectively. In concretely, WebFG-496 consists of
three sub-datasets containing a total of 53,339 web training images with 200
species of birds (Web-bird), 100 types of aircrafts (Web-aircraft), and 196
models of cars (Web-car). For WebiNat-5089, it contains 5089 sub-categories and
more than 1.1 million web training images, which is the largest webly
supervised fine-grained dataset ever. As a minor contribution, we also propose
a novel webly supervised method (termed ``{Peer-learning}'') for benchmarking
these datasets.~Comprehensive experimental results and analyses on two new
benchmark datasets demonstrate that the proposed method achieves superior
performance over the competing baseline models and states-of-the-art. Our
benchmark datasets and the source codes of Peer-learning have been made
available at
{\url{https://github.com/NUST-Machine-Intelligence-Laboratory/weblyFG-dataset}}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1"&gt;Zeren Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1"&gt;Yazhou Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1"&gt;Xiu-Shen Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yongshun Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_F/0/1/0/all/0/1"&gt;Fumin Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1"&gt;Jianxin Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jian Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1"&gt;Heng-Tao Shen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MFuseNet: Robust Depth Estimation with Learned Multiscopic Fusion. (arXiv:2108.02448v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02448</id>
        <link href="http://arxiv.org/abs/2108.02448"/>
        <updated>2021-08-06T00:51:45.258Z</updated>
        <summary type="html"><![CDATA[We design a multiscopic vision system that utilizes a low-cost monocular RGB
camera to acquire accurate depth estimation. Unlike multi-view stereo with
images captured at unconstrained camera poses, the proposed system controls the
motion of a camera to capture a sequence of images in horizontally or
vertically aligned positions with the same parallax. In this system, we propose
a new heuristic method and a robust learning-based method to fuse multiple cost
volumes between the reference image and its surrounding images. To obtain
training data, we build a synthetic dataset with multiscopic images. The
experiments on the real-world Middlebury dataset and real robot demonstration
show that our multiscopic vision system outperforms traditional two-frame
stereo matching methods in depth estimation. Our code and dataset are available
at \url{https://sites.google.com/view/multiscopic]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_W/0/1/0/all/0/1"&gt;Weihao Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_R/0/1/0/all/0/1"&gt;Rui Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1"&gt;Michael Yu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1"&gt;Qifeng Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[IDM: An Intermediate Domain Module for Domain Adaptive Person Re-ID. (arXiv:2108.02413v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02413</id>
        <link href="http://arxiv.org/abs/2108.02413"/>
        <updated>2021-08-06T00:51:45.235Z</updated>
        <summary type="html"><![CDATA[Unsupervised domain adaptive person re-identification (UDA re-ID) aims at
transferring the labeled source domain's knowledge to improve the model's
discriminability on the unlabeled target domain. From a novel perspective, we
argue that the bridging between the source and target domains can be utilized
to tackle the UDA re-ID task, and we focus on explicitly modeling appropriate
intermediate domains to characterize this bridging. Specifically, we propose an
Intermediate Domain Module (IDM) to generate intermediate domains'
representations on-the-fly by mixing the source and target domains' hidden
representations using two domain factors. Based on the "shortest geodesic path"
definition, i.e., the intermediate domains along the shortest geodesic path
between the two extreme domains can play a better bridging role, we propose two
properties that these intermediate domains should satisfy. To ensure these two
properties to better characterize appropriate intermediate domains, we enforce
the bridge losses on intermediate domains' prediction space and feature space,
and enforce a diversity loss on the two domain factors. The bridge losses aim
at guiding the distribution of appropriate intermediate domains to keep the
right distance to the source and target domains. The diversity loss serves as a
regularization to prevent the generated intermediate domains from being
over-fitting to either of the source and target domains. Our proposed method
outperforms the state-of-the-arts by a large margin in all the common UDA re-ID
tasks, and the mAP gain is up to 7.7% on the challenging MSMT17 benchmark. Code
is available at https://github.com/SikaStar/IDM.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1"&gt;Yongxing Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jun Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1"&gt;Yifan Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tong_Z/0/1/0/all/0/1"&gt;Zekun Tong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duan_L/0/1/0/all/0/1"&gt;Ling-Yu Duan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Design and Construct Bridge without Blueprint. (arXiv:2108.02439v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2108.02439</id>
        <link href="http://arxiv.org/abs/2108.02439"/>
        <updated>2021-08-06T00:51:45.210Z</updated>
        <summary type="html"><![CDATA[Autonomous assembly has been a desired functionality of many intelligent
robot systems. We study a new challenging assembly task, designing and
constructing a bridge without a blueprint. In this task, the robot needs to
first design a feasible bridge architecture for arbitrarily wide cliffs and
then manipulate the blocks reliably to construct a stable bridge according to
the proposed design. In this paper, we propose a bi-level approach to tackle
this task. At the high level, the system learns a bridge blueprint policy in a
physical simulator using deep reinforcement learning and curriculum learning. A
policy is represented as an attention-based neural network with object-centric
input, which enables generalization to different numbers of blocks and cliff
widths. For low-level control, we implement a motion-planning-based policy for
real-robot motion control, which can be directly combined with a trained
blueprint policy for real-world bridge construction without tuning. In our
field study, our bi-level robot system demonstrates the capability of
manipulating blocks to construct a diverse set of bridges with different
architectures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yunfei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kong_T/0/1/0/all/0/1"&gt;Tao Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yifeng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yi Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Simultaneous Semantic and Collision Learning for 6-DoF Grasp Pose Estimation. (arXiv:2108.02425v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2108.02425</id>
        <link href="http://arxiv.org/abs/2108.02425"/>
        <updated>2021-08-06T00:51:45.203Z</updated>
        <summary type="html"><![CDATA[Grasping in cluttered scenes has always been a great challenge for robots,
due to the requirement of the ability to well understand the scene and object
information. Previous works usually assume that the geometry information of the
objects is available, or utilize a step-wise, multi-stage strategy to predict
the feasible 6-DoF grasp poses. In this work, we propose to formalize the 6-DoF
grasp pose estimation as a simultaneous multi-task learning problem. In a
unified framework, we jointly predict the feasible 6-DoF grasp poses, instance
semantic segmentation, and collision information. The whole framework is
jointly optimized and end-to-end differentiable. Our model is evaluated on
large-scale benchmarks as well as the real robot system. On the public dataset,
our method outperforms prior state-of-the-art methods by a large margin (+4.08
AP). We also demonstrate the implementation of our model on a real robotic
platform and show that the robot can accurately grasp target objects in
cluttered scenarios with a high success rate. Project link:
https://openbyterobotics.github.io/sscl]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yiming Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kong_T/0/1/0/all/0/1"&gt;Tao Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chu_R/0/1/0/all/0/1"&gt;Ruihang Chu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yifeng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1"&gt;Peng Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lei Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Video Abnormal Event Detection by Learning to Complete Visual Cloze Tests. (arXiv:2108.02356v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02356</id>
        <link href="http://arxiv.org/abs/2108.02356"/>
        <updated>2021-08-06T00:51:45.188Z</updated>
        <summary type="html"><![CDATA[Video abnormal event detection (VAD) is a vital semi-supervised task that
requires learning with only roughly labeled normal videos, as anomalies are
often practically unavailable. Although deep neural networks (DNNs) enable
great progress in VAD, existing solutions typically suffer from two issues: (1)
The precise and comprehensive localization of video events is ignored. (2) The
video semantics and temporal context are under-explored. To address those
issues, we are motivated by the prevalent cloze test in education and propose a
novel approach named visual cloze completion (VCC), which performs VAD by
learning to complete "visual cloze tests" (VCTs). Specifically, VCC first
localizes each video event and encloses it into a spatio-temporal cube (STC).
To achieve both precise and comprehensive localization, appearance and motion
are used as mutually complementary cues to mark the object region associated
with each video event. For each marked region, a normalized patch sequence is
extracted from temporally adjacent frames and stacked into the STC. By
comparing each patch and the patch sequence of a STC to a visual "word" and
"sentence" respectively, we can deliberately erase a certain "word" (patch) to
yield a VCT. DNNs are then trained to infer the erased patch by video
semantics, so as to complete the VCT. To fully exploit the temporal context,
each patch in STC is alternatively erased to create multiple VCTs, and the
erased patch's optical flow is also inferred to integrate richer motion clues.
Meanwhile, a new DNN architecture is designed as a model-level solution to
utilize video semantics and temporal context. Extensive experiments demonstrate
that VCC achieves state-of-the-art VAD performance. Our codes and results are
open at \url{https://github.com/yuguangnudt/VEC_VAD/tree/VCC}]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Siqi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_G/0/1/0/all/0/1"&gt;Guang Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1"&gt;Zhiping Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xinwang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_E/0/1/0/all/0/1"&gt;En Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1"&gt;Jianping Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liao_Q/0/1/0/all/0/1"&gt;Qing Liao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fast Convergence of DETR with Spatially Modulated Co-Attention. (arXiv:2108.02404v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02404</id>
        <link href="http://arxiv.org/abs/2108.02404"/>
        <updated>2021-08-06T00:51:45.179Z</updated>
        <summary type="html"><![CDATA[The recently proposed Detection Transformer (DETR) model successfully applies
Transformer to objects detection and achieves comparable performance with
two-stage object detection frameworks, such as Faster-RCNN. However, DETR
suffers from its slow convergence. Training DETR from scratch needs 500 epochs
to achieve a high accuracy. To accelerate its convergence, we propose a simple
yet effective scheme for improving the DETR framework, namely Spatially
Modulated Co-Attention (SMCA) mechanism. The core idea of SMCA is to conduct
location-aware co-attention in DETR by constraining co-attention responses to
be high near initially estimated bounding box locations. Our proposed SMCA
increases DETR's convergence speed by replacing the original co-attention
mechanism in the decoder while keeping other operations in DETR unchanged.
Furthermore, by integrating multi-head and scale-selection attention designs
into SMCA, our fully-fledged SMCA can achieve better performance compared to
DETR with a dilated convolution-based backbone (45.6 mAP at 108 epochs vs. 43.3
mAP at 500 epochs). We perform extensive ablation studies on COCO dataset to
validate SMCA. Code is released at https://github.com/gaopengcuhk/SMCA-DETR .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1"&gt;Peng Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_M/0/1/0/all/0/1"&gt;Minghang Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaogang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1"&gt;Jifeng Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Hongsheng Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dual-reference Training Data Acquisition and CNN Construction for Image Super-Resolution. (arXiv:2108.02348v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02348</id>
        <link href="http://arxiv.org/abs/2108.02348"/>
        <updated>2021-08-06T00:51:45.161Z</updated>
        <summary type="html"><![CDATA[For deep learning methods of image super-resolution, the most critical issue
is whether the paired low and high resolution images for training accurately
reflect the sampling process of real cameras. Low and high resolution
(LR$\sim$HR) image pairs synthesized by existing degradation models (\eg,
bicubic downsampling) deviate from those in reality; thus the super-resolution
CNN trained by these synthesized LR$\sim$HR image pairs does not perform well
when being applied to real images. In this paper, we propose a novel method to
capture a large set of realistic LR$\sim$HR image pairs using real cameras.The
data acquisition is carried out under controllable lab conditions with minimum
human intervention and at high throughput (about 500 image pairs per hour). The
high level of automation makes it easy to produce a set of real LR$\sim$HR
training image pairs for each camera. Our innovation is to shoot images
displayed on an ultra-high quality screen at different resolutions.There are
three distinctive advantages with our method that allow us to collect
high-quality training datasets for image super-resolution. First, as the LR and
HR images are taken of a 3D planar surface (the screen) the registration
problem fits exactly to a homography model. Second, we can display special
markers on the image margin to further improve the registration
precision.Third, the displayed digital image file can be exploited as a
reference to optimize the high frequency content of the restored image.
Experimental results show that training a super-resolution CNN by our
LR$\sim$HR dataset has superior restoration performance than training it by
existing datasets on real world images at the inference stage.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1"&gt;Yanhui Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shu_X/0/1/0/all/0/1"&gt;Xiao Shu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1"&gt;Xiaolin Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pan-Cancer Integrative Histology-Genomic Analysis via Interpretable Multimodal Deep Learning. (arXiv:2108.02278v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02278</id>
        <link href="http://arxiv.org/abs/2108.02278"/>
        <updated>2021-08-06T00:51:45.144Z</updated>
        <summary type="html"><![CDATA[The rapidly emerging field of deep learning-based computational pathology has
demonstrated promise in developing objective prognostic models from histology
whole slide images. However, most prognostic models are either based on
histology or genomics alone and do not address how histology and genomics can
be integrated to develop joint image-omic prognostic models. Additionally
identifying explainable morphological and molecular descriptors from these
models that govern such prognosis is of interest. We used multimodal deep
learning to integrate gigapixel whole slide pathology images, RNA-seq
abundance, copy number variation, and mutation data from 5,720 patients across
14 major cancer types. Our interpretable, weakly-supervised, multimodal deep
learning algorithm is able to fuse these heterogeneous modalities for
predicting outcomes and discover prognostic features from these modalities that
corroborate with poor and favorable outcomes via multimodal interpretability.
We compared our model with unimodal deep learning models trained on histology
slides and molecular profiles alone, and demonstrate performance increase in
risk stratification on 9 out of 14 cancers. In addition, we analyze morphologic
and molecular markers responsible for prognostic predictions across all cancer
types. All analyzed data, including morphological and molecular correlates of
patient prognosis across the 14 cancer types at a disease and patient level are
presented in an interactive open-access database
(this http URL) to allow for further exploration and
prognostic biomarker discovery. To validate that these model explanations are
prognostic, we further analyzed high attention morphological regions in WSIs,
which indicates that tumor-infiltrating lymphocyte presence corroborates with
favorable cancer prognosis on 9 out of 14 cancer types studied.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1"&gt;Richard J. Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_M/0/1/0/all/0/1"&gt;Ming Y. Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Williamson_D/0/1/0/all/0/1"&gt;Drew F. K. Williamson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1"&gt;Tiffany Y. Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lipkova_J/0/1/0/all/0/1"&gt;Jana Lipkova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shaban_M/0/1/0/all/0/1"&gt;Muhammad Shaban&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shady_M/0/1/0/all/0/1"&gt;Maha Shady&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Williams_M/0/1/0/all/0/1"&gt;Mane Williams&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joo_B/0/1/0/all/0/1"&gt;Bumjin Joo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Noor_Z/0/1/0/all/0/1"&gt;Zahra Noor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mahmood_F/0/1/0/all/0/1"&gt;Faisal Mahmood&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Structured Multi-modal Feature Embedding and Alignment for Image-Sentence Retrieval. (arXiv:2108.02417v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02417</id>
        <link href="http://arxiv.org/abs/2108.02417"/>
        <updated>2021-08-06T00:51:45.136Z</updated>
        <summary type="html"><![CDATA[The current state-of-the-art image-sentence retrieval methods implicitly
align the visual-textual fragments, like regions in images and words in
sentences, and adopt attention modules to highlight the relevance of
cross-modal semantic correspondences. However, the retrieval performance
remains unsatisfactory due to a lack of consistent representation in both
semantics and structural spaces. In this work, we propose to address the above
issue from two aspects: (i) constructing intrinsic structure (along with
relations) among the fragments of respective modalities, e.g., "dog $\to$ play
$\to$ ball" in semantic structure for an image, and (ii) seeking explicit
inter-modal structural and semantic correspondence between the visual and
textual modalities. In this paper, we propose a novel Structured Multi-modal
Feature Embedding and Alignment (SMFEA) model for image-sentence retrieval. In
order to jointly and explicitly learn the visual-textual embedding and the
cross-modal alignment, SMFEA creates a novel multi-modal structured module with
a shared context-aware referral tree. In particular, the relations of the
visual and textual fragments are modeled by constructing Visual Context-aware
Structured Tree encoder (VCS-Tree) and Textual Context-aware Structured Tree
encoder (TCS-Tree) with shared labels, from which visual and textual features
can be jointly learned and optimized. We utilize the multi-modal tree structure
to explicitly align the heterogeneous image-sentence data by maximizing the
semantic and structural similarity between corresponding inter-modal tree
nodes. Extensive experiments on Microsoft COCO and Flickr30K benchmarks
demonstrate the superiority of the proposed model in comparison to the
state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ge_X/0/1/0/all/0/1"&gt;Xuri Ge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1"&gt;Fuhai Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jose_J/0/1/0/all/0/1"&gt;Joemon M. Jose&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_Z/0/1/0/all/0/1"&gt;Zhilong Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Zhongqin Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiao Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Intelligent Railway Foreign Object Detection: A Semi-supervised Convolutional Autoencoder Based Method. (arXiv:2108.02421v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02421</id>
        <link href="http://arxiv.org/abs/2108.02421"/>
        <updated>2021-08-06T00:51:45.129Z</updated>
        <summary type="html"><![CDATA[Automated inspection and detection of foreign objects on railways is
important for rail transportation safety as it helps prevent potential
accidents and trains derailment. Most existing vision-based approaches focus on
the detection of frontal intrusion objects with prior labels, such as
categories and locations of the objects. In reality, foreign objects with
unknown categories can appear anytime on railway tracks. In this paper, we
develop a semi-supervised convolutional autoencoder based framework that only
requires railway track images without prior knowledge on the foreign objects in
the training process. It consists of three different modules, a bottleneck
feature generator as encoder, a photographic image generator as decoder, and a
reconstruction discriminator developed via adversarial learning. In the
proposed framework, the problem of detecting the presence, location, and shape
of foreign objects is addressed by comparing the input and reconstructed images
as well as setting thresholds based on reconstruction errors. The proposed
method is evaluated through comprehensive studies under different performance
criteria. The results show that the proposed method outperforms some well-known
benchmarking methods. The proposed framework is useful for data analytics via
the train Internet-of-Things (IoT) systems]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1"&gt;Tiange Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zijun Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1"&gt;Fangfang Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsui_K/0/1/0/all/0/1"&gt;Kwok-Leung Tsui&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dynamic Relevance Learning for Few-Shot Object Detection. (arXiv:2108.02235v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02235</id>
        <link href="http://arxiv.org/abs/2108.02235"/>
        <updated>2021-08-06T00:51:45.120Z</updated>
        <summary type="html"><![CDATA[Expensive bounding-box annotations have limited the development of object
detection task. Thus, it is necessary to focus on more challenging task of
few-shot object detection. It requires the detector to recognize objects of
novel classes with only a few training samples. Nowadays, many existing popular
methods based on meta-learning have achieved promising performance, such as
Meta R-CNN series. However, only a single category of support data is used as
the attention to guide the detecting of query images each time. Their relevance
to each other remains unexploited. Moreover, a lot of recent works treat the
support data and query images as independent branch without considering the
relationship between them. To address this issue, we propose a dynamic
relevance learning model, which utilizes the relationship between all support
images and Region of Interest (RoI) on the query images to construct a dynamic
graph convolutional network (GCN). By adjusting the prediction distribution of
the base detector using the output of this GCN, the proposed model can guide
the detector to improve the class representation implicitly. Comprehensive
experiments have been conducted on Pascal VOC and MS-COCO dataset. The proposed
model achieves the best overall performance, which shows its effectiveness of
learning more generalized features. Our code is available at
https://github.com/liuweijie19980216/DRL-for-FSOD.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Weijie Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang%2A_C/0/1/0/all/0/1"&gt;Chong Wang*&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Haohe Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1"&gt;Shenghao Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Song Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1"&gt;Xulun Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1"&gt;Jiafei Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Decoupled Transformer for Scalable Inference in Open-domain Question Answering. (arXiv:2108.02765v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.02765</id>
        <link href="http://arxiv.org/abs/2108.02765"/>
        <updated>2021-08-06T00:51:45.090Z</updated>
        <summary type="html"><![CDATA[Large transformer models, such as BERT, achieve state-of-the-art results in
machine reading comprehension (MRC) for open-domain question answering (QA).
However, transformers have a high computational cost for inference which makes
them hard to apply to online QA systems for applications like voice assistants.
To reduce computational cost and latency, we propose decoupling the transformer
MRC model into input-component and cross-component. The decoupling allows for
part of the representation computation to be performed offline and cached for
online use. To retain the decoupled transformer accuracy, we devised a
knowledge distillation objective from a standard transformer model. Moreover,
we introduce learned representation compression layers which help reduce by
four times the storage requirement for the cache. In experiments on the SQUAD
2.0 dataset, a decoupled transformer reduces the computational cost and latency
of open-domain MRC by 30-40% with only 1.2 points worse F1-score compared to a
standard transformer.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+ElFadeel_H/0/1/0/all/0/1"&gt;Haytham ElFadeel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peshterliev_S/0/1/0/all/0/1"&gt;Stan Peshterliev&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Boosting Few-shot Semantic Segmentation with Transformers. (arXiv:2108.02266v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02266</id>
        <link href="http://arxiv.org/abs/2108.02266"/>
        <updated>2021-08-06T00:51:45.083Z</updated>
        <summary type="html"><![CDATA[Due to the fact that fully supervised semantic segmentation methods require
sufficient fully-labeled data to work well and can not generalize to unseen
classes, few-shot segmentation has attracted lots of research attention.
Previous arts extract features from support and query images, which are
processed jointly before making predictions on query images. The whole process
is based on convolutional neural networks (CNN), leading to the problem that
only local information is used. In this paper, we propose a TRansformer-based
Few-shot Semantic segmentation method (TRFS). Specifically, our model consists
of two modules: Global Enhancement Module (GEM) and Local Enhancement Module
(LEM). GEM adopts transformer blocks to exploit global information, while LEM
utilizes conventional convolutions to exploit local information, across query
and support features. Both GEM and LEM are complementary, helping to learn
better feature representations for segmenting query images. Extensive
experiments on PASCAL-5i and COCO datasets show that our approach achieves new
state-of-the-art performance, demonstrating its effectiveness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_G/0/1/0/all/0/1"&gt;Guolei Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yun Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1"&gt;Jingyun Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1"&gt;Luc Van Gool&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Grad-TTS: A Diffusion Probabilistic Model for Text-to-Speech. (arXiv:2105.06337v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.06337</id>
        <link href="http://arxiv.org/abs/2105.06337"/>
        <updated>2021-08-06T00:51:45.076Z</updated>
        <summary type="html"><![CDATA[Recently, denoising diffusion probabilistic models and generative score
matching have shown high potential in modelling complex data distributions
while stochastic calculus has provided a unified point of view on these
techniques allowing for flexible inference schemes. In this paper we introduce
Grad-TTS, a novel text-to-speech model with score-based decoder producing
mel-spectrograms by gradually transforming noise predicted by encoder and
aligned with text input by means of Monotonic Alignment Search. The framework
of stochastic differential equations helps us to generalize conventional
diffusion probabilistic models to the case of reconstructing data from noise
with different parameters and allows to make this reconstruction flexible by
explicitly controlling trade-off between sound quality and inference speed.
Subjective human evaluation shows that Grad-TTS is competitive with
state-of-the-art text-to-speech approaches in terms of Mean Opinion Score. We
will make the code publicly available shortly.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Popov_V/0/1/0/all/0/1"&gt;Vadim Popov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vovk_I/0/1/0/all/0/1"&gt;Ivan Vovk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gogoryan_V/0/1/0/all/0/1"&gt;Vladimir Gogoryan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sadekova_T/0/1/0/all/0/1"&gt;Tasnima Sadekova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kudinov_M/0/1/0/all/0/1"&gt;Mikhail Kudinov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Finetuning Pretrained Transformers into Variational Autoencoders. (arXiv:2108.02446v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.02446</id>
        <link href="http://arxiv.org/abs/2108.02446"/>
        <updated>2021-08-06T00:51:45.066Z</updated>
        <summary type="html"><![CDATA[Text variational autoencoders (VAEs) are notorious for posterior collapse, a
phenomenon where the model's decoder learns to ignore signals from the encoder.
Because posterior collapse is known to be exacerbated by expressive decoders,
Transformers have seen limited adoption as components of text VAEs. Existing
studies that incorporate Transformers into text VAEs (Li et al., 2020; Fang et
al., 2021) mitigate posterior collapse using massive pretraining, a technique
unavailable to most of the research community without extensive computing
resources. We present a simple two-phase training scheme to convert a
sequence-to-sequence Transformer into a VAE with just finetuning. The resulting
language model is competitive with massively pretrained Transformer-based VAEs
in some internal metrics while falling short on others. To facilitate training
we comprehensively explore the impact of common posterior collapse alleviation
techniques in the literature. We release our code for reproducability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1"&gt;Seongmin Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jihwa Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[VisualTextRank: Unsupervised Graph-based Content Extraction for Automating Ad Text to Image Search. (arXiv:2108.02725v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.02725</id>
        <link href="http://arxiv.org/abs/2108.02725"/>
        <updated>2021-08-06T00:51:45.047Z</updated>
        <summary type="html"><![CDATA[Numerous online stock image libraries offer high quality yet copyright free
images for use in marketing campaigns. To assist advertisers in navigating such
third party libraries, we study the problem of automatically fetching relevant
ad images given the ad text (via a short textual query for images). Motivated
by our observations in logged data on ad image search queries (given ad text),
we formulate a keyword extraction problem, where a keyword extracted from the
ad text (or its augmented version) serves as the ad image query. In this
context, we propose VisualTextRank: an unsupervised method to (i) augment input
ad text using semantically similar ads, and (ii) extract the image query from
the augmented ad text. VisualTextRank builds on prior work on graph based
context extraction (biased TextRank in particular) by leveraging both the text
and image of similar ads for better keyword extraction, and using advertiser
category specific biasing with sentence-BERT embeddings. Using data collected
from the Verizon Media Native (Yahoo Gemini) ad platform's stock image search
feature for onboarding advertisers, we demonstrate the superiority of
VisualTextRank compared to competitive keyword extraction baselines (including
an $11\%$ accuracy lift over biased TextRank). For the case when the stock
image library is restricted to English queries, we show the effectiveness of
VisualTextRank on multilingual ads (translated to English) while leveraging
semantically similar English ads. Online tests with a simplified version of
VisualTextRank led to a 28.7% increase in the usage of stock image search, and
a 41.6% increase in the advertiser onboarding rate in the Verizon Media Native
ad platform.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1"&gt;Shaunak Mishra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kuznetsov_M/0/1/0/all/0/1"&gt;Mikhail Kuznetsov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srivastava_G/0/1/0/all/0/1"&gt;Gaurav Srivastava&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sviridenko_M/0/1/0/all/0/1"&gt;Maxim Sviridenko&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TransRefer3D: Entity-and-Relation Aware Transformer for Fine-Grained 3D Visual Grounding. (arXiv:2108.02388v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02388</id>
        <link href="http://arxiv.org/abs/2108.02388"/>
        <updated>2021-08-06T00:51:45.027Z</updated>
        <summary type="html"><![CDATA[Recently proposed fine-grained 3D visual grounding is an essential and
challenging task, whose goal is to identify the 3D object referred by a natural
language sentence from other distractive objects of the same category. Existing
works usually adopt dynamic graph networks to indirectly model the
intra/inter-modal interactions, making the model difficult to distinguish the
referred object from distractors due to the monolithic representations of
visual and linguistic contents. In this work, we exploit Transformer for its
natural suitability on permutation-invariant 3D point clouds data and propose a
TransRefer3D network to extract entity-and-relation aware multimodal context
among objects for more discriminative feature learning. Concretely, we devise
an Entity-aware Attention (EA) module and a Relation-aware Attention (RA)
module to conduct fine-grained cross-modal feature matching. Facilitated by
co-attention operation, our EA module matches visual entity features with
linguistic entity features while RA module matches pair-wise visual relation
features with linguistic relation features, respectively. We further integrate
EA and RA modules into an Entity-and-Relation aware Contextual Block (ERCB) and
stack several ERCBs to form our TransRefer3D for hierarchical multimodal
context modeling. Extensive experiments on both Nr3D and Sr3D datasets
demonstrate that our proposed model significantly outperforms existing
approaches by up to 10.6% and claims the new state-of-the-art. To the best of
our knowledge, this is the first work investigating Transformer architecture
for fine-grained 3D visual grounding task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+he_D/0/1/0/all/0/1"&gt;Dailan he&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yusheng Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1"&gt;Junyu Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hui_T/0/1/0/all/0/1"&gt;Tianrui Hui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1"&gt;Shaofei Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1"&gt;Aixi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Si Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[EENLP: Cross-lingual Eastern European NLP Index. (arXiv:2108.02605v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.02605</id>
        <link href="http://arxiv.org/abs/2108.02605"/>
        <updated>2021-08-06T00:51:45.020Z</updated>
        <summary type="html"><![CDATA[This report presents the results of the EENLP project, done as a part of EEML
2021 summer school.

It presents a broad index of NLP resources for Eastern European languages,
which, we hope, could be helpful for the NLP community; several new
hand-crafted cross-lingual datasets focused on Eastern European languages, and
a sketch evaluation of cross-lingual transfer learning abilities of several
modern multilingual Transformer-based models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tikhonov_A/0/1/0/all/0/1"&gt;Alexey Tikhonov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Malkhasov_A/0/1/0/all/0/1"&gt;Alex Malkhasov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Manoshin_A/0/1/0/all/0/1"&gt;Andrey Manoshin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dima_G/0/1/0/all/0/1"&gt;George Dima&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cserhati_R/0/1/0/all/0/1"&gt;R&amp;#xe9;ka Cserh&amp;#xe1;ti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Asif_M/0/1/0/all/0/1"&gt;Md.Sadek Hossain Asif&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sardi_M/0/1/0/all/0/1"&gt;Matt S&amp;#xe1;rdi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sentence-level Online Handwritten Chinese Character Recognition. (arXiv:2108.02561v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02561</id>
        <link href="http://arxiv.org/abs/2108.02561"/>
        <updated>2021-08-06T00:51:45.013Z</updated>
        <summary type="html"><![CDATA[Single online handwritten Chinese character recognition~(single OLHCCR) has
achieved prominent performance. However, in real application scenarios, users
always write multiple Chinese characters to form one complete sentence and the
contextual information within these characters holds the significant potential
to improve the accuracy, robustness and efficiency of sentence-level OLHCCR. In
this work, we first propose a simple and straightforward end-to-end network,
namely vanilla compositional network~(VCN) to tackle the sentence-level OLHCCR.
It couples convolutional neural network with sequence modeling architecture to
exploit the handwritten character's previous contextual information. Although
VCN performs much better than the state-of-the-art single OLHCCR model, it
exposes high fragility when confronting with not well written characters such
as sloppy writing, missing or broken strokes. To improve the robustness of
sentence-level OLHCCR, we further propose a novel deep spatial-temporal fusion
network~(DSTFN). It utilizes a pre-trained autoregresssive framework as the
backbone component, which projects each Chinese character into word embeddings,
and integrates the spatial glyph features of handwritten characters and their
contextual information multiple times at multi-layer fusion module. We also
construct a large-scale sentence-level handwriting dataset, named as CSOHD to
evaluate models. Extensive experiment results demonstrate that DSTFN achieves
the state-of-the-art performance, which presents strong robustness compared
with VCN and exiting single OLHCCR models. The in-depth empirical analysis and
case studies indicate that DSTFN can significantly improve the efficiency of
handwriting input, with the handwritten Chinese character with incomplete
strokes being recognized precisely.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yunxin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1"&gt;Qian Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1"&gt;Qingcai Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1"&gt;Lin Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_B/0/1/0/all/0/1"&gt;Baotian Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiaolong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1"&gt;Yuxin Ding&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Security and Privacy Enhanced Gait Authentication with Random Representation Learning and Digital Lockers. (arXiv:2108.02400v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02400</id>
        <link href="http://arxiv.org/abs/2108.02400"/>
        <updated>2021-08-06T00:51:45.006Z</updated>
        <summary type="html"><![CDATA[Gait data captured by inertial sensors have demonstrated promising results on
user authentication. However, most existing approaches stored the enrolled gait
pattern insecurely for matching with the validating pattern, thus, posed
critical security and privacy issues. In this study, we present a gait
cryptosystem that generates from gait data the random key for user
authentication, meanwhile, secures the gait pattern. First, we propose a
revocable and random binary string extraction method using a deep neural
network followed by feature-wise binarization. A novel loss function for
network optimization is also designed, to tackle not only the intrauser
stability but also the inter-user randomness. Second, we propose a new
biometric key generation scheme, namely Irreversible Error Correct and
Obfuscate (IECO), improved from the Error Correct and Obfuscate (ECO) scheme,
to securely generate from the binary string the random and irreversible key.
The model was evaluated with two benchmark datasets as OU-ISIR and whuGAIT. We
showed that our model could generate the key of 139 bits from 5-second data
sequence with zero False Acceptance Rate (FAR) and False Rejection Rate (FRR)
smaller than 5.441%. In addition, the security and user privacy analyses showed
that our model was secure against existing attacks on biometric template
protection, and fulfilled irreversibility and unlinkability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tran_L/0/1/0/all/0/1"&gt;Lam Tran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1"&gt;Thuc Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1"&gt;Hyunil Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_D/0/1/0/all/0/1"&gt;Deokjai Choi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring Out-of-Distribution Generalization in Text Classifiers Trained on Tobacco-3482 and RVL-CDIP. (arXiv:2108.02684v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.02684</id>
        <link href="http://arxiv.org/abs/2108.02684"/>
        <updated>2021-08-06T00:51:44.999Z</updated>
        <summary type="html"><![CDATA[To be robust enough for widespread adoption, document analysis systems
involving machine learning models must be able to respond correctly to inputs
that fall outside of the data distribution that was used to generate the data
on which the models were trained. This paper explores the ability of text
classifiers trained on standard document classification datasets to generalize
to out-of-distribution documents at inference time. We take the Tobacco-3482
and RVL-CDIP datasets as a starting point and generate new out-of-distribution
evaluation datasets in order to analyze the generalization performance of
models trained on these standard datasets. We find that models trained on the
smaller Tobacco-3482 dataset perform poorly on our new out-of-distribution
data, while text classification models trained on the larger RVL-CDIP exhibit
smaller performance drops.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Larson_S/0/1/0/all/0/1"&gt;Stefan Larson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_N/0/1/0/all/0/1"&gt;Navtej Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maheshwari_S/0/1/0/all/0/1"&gt;Saarthak Maheshwari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stewart_S/0/1/0/all/0/1"&gt;Shanti Stewart&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krishnaswamy_U/0/1/0/all/0/1"&gt;Uma Krishnaswamy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hybrid Reasoning Network for Video-based Commonsense Captioning. (arXiv:2108.02365v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02365</id>
        <link href="http://arxiv.org/abs/2108.02365"/>
        <updated>2021-08-06T00:51:44.981Z</updated>
        <summary type="html"><![CDATA[The task of video-based commonsense captioning aims to generate event-wise
captions and meanwhile provide multiple commonsense descriptions (e.g.,
attribute, effect and intention) about the underlying event in the video. Prior
works explore the commonsense captions by using separate networks for different
commonsense types, which is time-consuming and lacks mining the interaction of
different commonsense. In this paper, we propose a Hybrid Reasoning Network
(HybridNet) to endow the neural networks with the capability of semantic-level
reasoning and word-level reasoning. Firstly, we develop multi-commonsense
learning for semantic-level reasoning by jointly training different commonsense
types in a unified network, which encourages the interaction between the clues
of multiple commonsense descriptions, event-wise captions and videos. Then,
there are two steps to achieve the word-level reasoning: (1) a memory module
records the history predicted sequence from the previous generation processes;
(2) a memory-routed multi-head attention (MMHA) module updates the word-level
attention maps by incorporating the history information from the memory module
into the transformer decoder for word-level reasoning. Moreover, the multimodal
features are used to make full use of diverse knowledge for commonsense
reasoning. Experiments and abundant analysis on the large-scale
Video-to-Commonsense benchmark show that our HybridNet achieves
state-of-the-art performance compared with other methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1"&gt;Weijiang Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1"&gt;Jian Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_L/0/1/0/all/0/1"&gt;Lei Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1"&gt;Yuejian Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_N/0/1/0/all/0/1"&gt;Nong Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1"&gt;Nan Duan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Global and Local Texture Randomization for Synthetic-to-Real Semantic Segmentation. (arXiv:2108.02376v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02376</id>
        <link href="http://arxiv.org/abs/2108.02376"/>
        <updated>2021-08-06T00:51:44.972Z</updated>
        <summary type="html"><![CDATA[Semantic segmentation is a crucial image understanding task, where each pixel
of image is categorized into a corresponding label. Since the pixel-wise
labeling for ground-truth is tedious and labor intensive, in practical
applications, many works exploit the synthetic images to train the model for
real-word image semantic segmentation, i.e., Synthetic-to-Real Semantic
Segmentation (SRSS). However, Deep Convolutional Neural Networks (CNNs) trained
on the source synthetic data may not generalize well to the target real-world
data. In this work, we propose two simple yet effective texture randomization
mechanisms, Global Texture Randomization (GTR) and Local Texture Randomization
(LTR), for Domain Generalization based SRSS. GTR is proposed to randomize the
texture of source images into diverse unreal texture styles. It aims to
alleviate the reliance of the network on texture while promoting the learning
of the domain-invariant cues. In addition, we find the texture difference is
not always occurred in entire image and may only appear in some local areas.
Therefore, we further propose a LTR mechanism to generate diverse local regions
for partially stylizing the source images. Finally, we implement a
regularization of Consistency between GTR and LTR (CGL) aiming to harmonize the
two proposed mechanisms during training. Extensive experiments on five publicly
available datasets (i.e., GTA5, SYNTHIA, Cityscapes, BDDS and Mapillary) with
various SRSS settings (i.e., GTA5/SYNTHIA to Cityscapes/BDDS/Mapillary)
demonstrate that the proposed method is superior to the state-of-the-art
methods for domain generalization based SRSS.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Peng_D/0/1/0/all/0/1"&gt;Duo Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lei_Y/0/1/0/all/0/1"&gt;Yinjie Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Lingqiao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1"&gt;Pingping Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jun Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ACE: Ally Complementary Experts for Solving Long-Tailed Recognition in One-Shot. (arXiv:2108.02385v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02385</id>
        <link href="http://arxiv.org/abs/2108.02385"/>
        <updated>2021-08-06T00:51:44.960Z</updated>
        <summary type="html"><![CDATA[One-stage long-tailed recognition methods improve the overall performance in
a "seesaw" manner, i.e., either sacrifice the head's accuracy for better tail
classification or elevate the head's accuracy even higher but ignore the tail.
Existing algorithms bypass such trade-off by a multi-stage training process:
pre-training on imbalanced set and fine-tuning on balanced set. Though
achieving promising performance, not only are they sensitive to the
generalizability of the pre-trained model, but also not easily integrated into
other computer vision tasks like detection and segmentation, where pre-training
of classifiers solely is not applicable. In this paper, we propose a one-stage
long-tailed recognition scheme, ally complementary experts (ACE), where the
expert is the most knowledgeable specialist in a sub-set that dominates its
training, and is complementary to other experts in the less-seen categories
without being disturbed by what it has never seen. We design a
distribution-adaptive optimizer to adjust the learning pace of each expert to
avoid over-fitting. Without special bells and whistles, the vanilla ACE
outperforms the current one-stage SOTA method by 3-10% on CIFAR10-LT,
CIFAR100-LT, ImageNet-LT and iNaturalist datasets. It is also shown to be the
first one to break the "seesaw" trade-off by improving the accuracy of the
majority and minority categories simultaneously in only one stage. Code and
trained models are at https://github.com/jrcai/ACE.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1"&gt;Jiarui Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yizhou Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1"&gt;Jenq-Neng Hwang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understand me, if you refer to Aspect Knowledge: Knowledge-aware Gated Recurrent Memory Network. (arXiv:2108.02352v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.02352</id>
        <link href="http://arxiv.org/abs/2108.02352"/>
        <updated>2021-08-06T00:51:44.953Z</updated>
        <summary type="html"><![CDATA[Aspect-level sentiment classification (ASC) aims to predict the fine-grained
sentiment polarity towards a given aspect mentioned in a review. Despite recent
advances in ASC, enabling machines to preciously infer aspect sentiments is
still challenging. This paper tackles two challenges in ASC: (1) due to lack of
aspect knowledge, aspect representation derived in prior works is inadequate to
represent aspect's exact meaning and property information; (2) prior works only
capture either local syntactic information or global relational information,
thus missing either one of them leads to insufficient syntactic information. To
tackle these challenges, we propose a novel ASC model which not only end-to-end
embeds and leverages aspect knowledge but also marries the two kinds of
syntactic information and lets them compensate for each other. Our model
includes three key components: (1) a knowledge-aware gated recurrent memory
network recurrently integrates dynamically summarized aspect knowledge; (2) a
dual syntax graph network combines both kinds of syntactic information to
comprehensively capture sufficient syntactic information; (3) a knowledge
integrating gate re-enhances the final representation with further needed
aspect knowledge; (4) an aspect-to-context attention mechanism aggregates the
aspect-related semantics from all hidden states into the final representation.
Experimental results on several benchmark datasets demonstrate the
effectiveness of our model, which overpass previous state-of-the-art models by
large margins in terms of both Accuracy and Macro-F1.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xing_B/0/1/0/all/0/1"&gt;Bowen Xing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsang_I/0/1/0/all/0/1"&gt;Ivor W. Tsang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dual Graph Convolutional Networks with Transformer and Curriculum Learning for Image Captioning. (arXiv:2108.02366v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02366</id>
        <link href="http://arxiv.org/abs/2108.02366"/>
        <updated>2021-08-06T00:51:44.946Z</updated>
        <summary type="html"><![CDATA[Existing image captioning methods just focus on understanding the
relationship between objects or instances in a single image, without exploring
the contextual correlation existed among contextual image. In this paper, we
propose Dual Graph Convolutional Networks (Dual-GCN) with transformer and
curriculum learning for image captioning. In particular, we not only use an
object-level GCN to capture the object to object spatial relation within a
single image, but also adopt an image-level GCN to capture the feature
information provided by similar images. With the well-designed Dual-GCN, we can
make the linguistic transformer better understand the relationship between
different objects in a single image and make full use of similar images as
auxiliary information to generate a reasonable caption description for a single
image. Meanwhile, with a cross-review strategy introduced to determine
difficulty levels, we adopt curriculum learning as the training strategy to
increase the robustness and generalization of our proposed model. We conduct
extensive experiments on the large-scale MS COCO dataset, and the experimental
results powerfully demonstrate that our proposed method outperforms recent
state-of-the-art approaches. It achieves a BLEU-1 score of 82.2 and a BLEU-2
score of 67.6. Our source code is available at {\em
\color{magenta}{\url{https://github.com/Unbear430/DGCN-for-image-captioning}}}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1"&gt;Xinzhi Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Long_C/0/1/0/all/0/1"&gt;Chengjiang Long&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1"&gt;Wenju Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1"&gt;Chunxia Xiao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[dp-GAN : Alleviating Mode Collapse in GAN via Diversity Penalty Module. (arXiv:2108.02353v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02353</id>
        <link href="http://arxiv.org/abs/2108.02353"/>
        <updated>2021-08-06T00:51:44.896Z</updated>
        <summary type="html"><![CDATA[The vanilla GAN [5] suffers from mode collapse deeply, which usually
manifests as that the images generated by generators tend to have a high
similarity amongst them, even though their corresponding latent vectors have
been very different. In this paper, we introduce a pluggable block called
diversity penalty (dp) to alleviate mode collapse of GANs. It is used to reduce
the similarity of image pairs in feature space, i.e., if two latent vectors are
different, then we enforce the generator to generate two images with different
features. The normalized Gram Matrix is used to measure the similarity. We
compare the proposed method with Unrolled GAN [17], BourGAN [26], PacGAN [14],
VEEGAN [23] and ALI [4] on 2D synthetic dataset, and results show that our
proposed method can help GAN capture more modes of the data distribution.
Further, we apply this penalty term into image data augmentation on MNIST,
Fashion-MNIST and CIFAR-10, and the testing accuracy is improved by 0.24%,
1.34% and 0.52% compared with WGAN GP [6], respectively. Finally, we
quantitatively evaluate the proposed method with IS and FID on CelebA,
CIFAR-10, MNIST and Fashion-MNIST. Results show that our method gets much
higher IS and lower FID compared with some current GAN architectures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pei_S/0/1/0/all/0/1"&gt;Sen Pei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1"&gt;Richard Yi Da Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meng_G/0/1/0/all/0/1"&gt;Gaofeng Meng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Branch with Attention Network for Hand-Based Person Recognition. (arXiv:2108.02234v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02234</id>
        <link href="http://arxiv.org/abs/2108.02234"/>
        <updated>2021-08-06T00:51:44.857Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose a novel hand-based person recognition method for
the purpose of criminal investigations since the hand image is often the only
available information in cases of serious crime such as sexual abuse. Our
proposed method, Multi-Branch with Attention Network (MBA-Net), incorporates
both channel and spatial attention modules in branches in addition to a global
(without attention) branch to capture global structural information for
discriminative feature learning. The attention modules focus on the relevant
features of the hand image while suppressing the irrelevant backgrounds. In
order to overcome the weakness of the attention mechanisms, equivariant to
pixel shuffling, we integrate relative positional encodings into the spatial
attention module to capture the spatial positions of pixels. Extensive
evaluations on two large multi-ethnic and publicly available hand datasets
demonstrate that our proposed method achieves state-of-the-art performance,
surpassing the existing hand-based identification methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Baisa_N/0/1/0/all/0/1"&gt;Nathanael L. Baisa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Williams_B/0/1/0/all/0/1"&gt;Bryan Williams&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rahmani_H/0/1/0/all/0/1"&gt;Hossein Rahmani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Angelov_P/0/1/0/all/0/1"&gt;Plamen Angelov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Black_S/0/1/0/all/0/1"&gt;Sue Black&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Fourier single-pixel imaging with Gaussian random sampling. (arXiv:2108.02317v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2108.02317</id>
        <link href="http://arxiv.org/abs/2108.02317"/>
        <updated>2021-08-06T00:51:44.849Z</updated>
        <summary type="html"><![CDATA[Fourier single-pixel imaging (FSI) is a branch of single-pixel imaging
techniques. It uses Fourier basis patterns as structured patterns for spatial
information acquisition in the Fourier domain. However, the spatial resolution
of the image reconstructed by FSI mainly depends on the number of Fourier
coefficients sampled. The reconstruction of a high-resolution image typically
requires a number of Fourier coefficients to be sampled, and therefore takes a
long data acquisition time. Here we propose a new sampling strategy for FSI. It
allows FSI to reconstruct a clear and sharp image with a reduced number of
measurements. The core of the proposed sampling strategy is to perform a
variable density sampling in the Fourier space and, more importantly, the
density with respect to the importance of Fourier coefficients is subject to a
one-dimensional Gaussian function. Combined with compressive sensing, the
proposed sampling strategy enables better reconstruction quality than
conventional sampling strategies, especially when the sampling ratio is low. We
experimentally demonstrate compressive FSI combined with the proposed sampling
strategy is able to reconstruct a sharp and clear image of 256-by-256 pixels
with a sampling ratio of 10%. The proposed method enables fast single-pixel
imaging and provides a new approach for efficient spatial information
acquisition.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Qiu_Z/0/1/0/all/0/1"&gt;Ziheng Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Guo_X/0/1/0/all/0/1"&gt;Xinyi Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lu_T/0/1/0/all/0/1"&gt;Tianao Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Qi_P/0/1/0/all/0/1"&gt;Pan Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zibang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhong_J/0/1/0/all/0/1"&gt;Jingang Zhong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Impact of Machine Learning on 2D/3D Registration for Image-guided Interventions: A Systematic Review and Perspective. (arXiv:2108.02238v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02238</id>
        <link href="http://arxiv.org/abs/2108.02238"/>
        <updated>2021-08-06T00:51:44.842Z</updated>
        <summary type="html"><![CDATA[Image-based navigation is widely considered the next frontier of minimally
invasive surgery. It is believed that image-based navigation will increase the
access to reproducible, safe, and high-precision surgery as it may then be
performed at acceptable costs and effort. This is because image-based
techniques avoid the need of specialized equipment and seamlessly integrate
with contemporary workflows. Further, it is expected that image-based
navigation will play a major role in enabling mixed reality environments and
autonomous, robotic workflows. A critical component of image guidance is 2D/3D
registration, a technique to estimate the spatial relationships between 3D
structures, e.g., volumetric imagery or tool models, and 2D images thereof,
such as fluoroscopy or endoscopy. While image-based 2D/3D registration is a
mature technique, its transition from the bench to the bedside has been
restrained by well-known challenges, including brittleness of the optimization
objective, hyperparameter selection, and initialization, difficulties around
inconsistencies or multiple objects, and limited single-view performance. One
reason these challenges persist today is that analytical solutions are likely
inadequate considering the complexity, variability, and high-dimensionality of
generic 2D/3D registration problems. The recent advent of machine
learning-based approaches to imaging problems that, rather than specifying the
desired functional mapping, approximate it using highly expressive parametric
models holds promise for solving some of the notorious challenges in 2D/3D
registration. In this manuscript, we review the impact of machine learning on
2D/3D registration to systematically summarize the recent advances made by
introduction of this novel technology. Grounded in these insights, we then
offer our perspective on the most pressing needs, significant open problems,
and possible next steps.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Unberath_M/0/1/0/all/0/1"&gt;Mathias Unberath&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1"&gt;Cong Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1"&gt;Yicheng Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Judish_M/0/1/0/all/0/1"&gt;Max Judish&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Taylor_R/0/1/0/all/0/1"&gt;Russell H Taylor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Armand_M/0/1/0/all/0/1"&gt;Mehran Armand&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grupp_R/0/1/0/all/0/1"&gt;Robert Grupp&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Probing Classifiers: Promises, Shortcomings, and Advances. (arXiv:2102.12452v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.12452</id>
        <link href="http://arxiv.org/abs/2102.12452"/>
        <updated>2021-08-06T00:51:44.821Z</updated>
        <summary type="html"><![CDATA[Probing classifiers have emerged as one of the prominent methodologies for
interpreting and analyzing deep neural network models of natural language
processing. The basic idea is simple---a classifier is trained to predict some
linguistic property from a model's representations---and has been used to
examine a wide variety of models and properties. However, recent studies have
demonstrated various methodological weaknesses of this approach. This article
critically reviews the probing classifiers framework, highlighting their
promises, shortcomings, and advances.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Belinkov_Y/0/1/0/all/0/1"&gt;Yonatan Belinkov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Terabyte-scale supervised 3D training and benchmarking dataset of the mouse kidney. (arXiv:2108.02226v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02226</id>
        <link href="http://arxiv.org/abs/2108.02226"/>
        <updated>2021-08-06T00:51:44.814Z</updated>
        <summary type="html"><![CDATA[The performance of machine learning algorithms used for the segmentation of
3D biomedical images lags behind that of the algorithms employed in the
classification of 2D photos. This may be explained by the comparative lack of
high-volume, high-quality training datasets, which require state-of-the art
imaging facilities, domain experts for annotation and large computational and
personal resources to create. The HR-Kidney dataset presented in this work
bridges this gap by providing 1.7 TB of artefact-corrected synchrotron
radiation-based X-ray phase-contrast microtomography images of whole mouse
kidneys and validated segmentations of 33 729 glomeruli, which represents a 1-2
orders of magnitude increase over currently available biomedical datasets. The
dataset further contains the underlying raw data, classical segmentations of
renal vasculature and uriniferous tubules, as well as true 3D manual
annotations. By removing limits currently imposed by small training datasets,
the provided data open up the possibility for disruptions in machine learning
for biomedical image analysis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kuo_W/0/1/0/all/0/1"&gt;Willy Kuo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rossinelli_D/0/1/0/all/0/1"&gt;Diego Rossinelli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schulz_G/0/1/0/all/0/1"&gt;Georg Schulz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wenger_R/0/1/0/all/0/1"&gt;Roland H. Wenger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hieber_S/0/1/0/all/0/1"&gt;Simone Hieber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muller_B/0/1/0/all/0/1"&gt;Bert M&amp;#xfc;ller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kurtcuoglu_V/0/1/0/all/0/1"&gt;Vartan Kurtcuoglu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Shared Semantic Space for Speech-to-Text Translation. (arXiv:2105.03095v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.03095</id>
        <link href="http://arxiv.org/abs/2105.03095"/>
        <updated>2021-08-06T00:51:44.807Z</updated>
        <summary type="html"><![CDATA[Having numerous potential applications and great impact, end-to-end speech
translation (ST) has long been treated as an independent task, failing to fully
draw strength from the rapid advances of its sibling - text machine translation
(MT). With text and audio inputs represented differently, the modality gap has
rendered MT data and its end-to-end models incompatible with their ST
counterparts. In observation of this obstacle, we propose to bridge this
representation gap with Chimera. By projecting audio and text features to a
common semantic representation, Chimera unifies MT and ST tasks and boosts the
performance on ST benchmarks, MuST-C and Augmented Librispeech, to a new
state-of-the-art. Specifically, Chimera obtains 27.1 BLEU on MuST-C EN-DE,
improving the SOTA by a +1.9 BLEU margin. Further experimental analyses
demonstrate that the shared semantic space indeed conveys common knowledge
between these two tasks and thus paves a new way for augmenting training
resources across modalities. Code, data, and resources are available at
https://github.com/Glaciohound/Chimera-ST.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Han_C/0/1/0/all/0/1"&gt;Chi Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1"&gt;Mingxuan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1"&gt;Heng Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lei Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarial learning of cancer tissue representations. (arXiv:2108.02223v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02223</id>
        <link href="http://arxiv.org/abs/2108.02223"/>
        <updated>2021-08-06T00:51:44.800Z</updated>
        <summary type="html"><![CDATA[Deep learning based analysis of histopathology images shows promise in
advancing the understanding of tumor progression, tumor micro-environment, and
their underpinning biological processes. So far, these approaches have focused
on extracting information associated with annotations. In this work, we ask how
much information can be learned from the tissue architecture itself.

We present an adversarial learning model to extract feature representations
of cancer tissue, without the need for manual annotations. We show that these
representations are able to identify a variety of morphological characteristics
across three cancer types: Breast, colon, and lung. This is supported by 1) the
separation of morphologic characteristics in the latent space; 2) the ability
to classify tissue type with logistic regression using latent representations,
with an AUC of 0.97 and 85% accuracy, comparable to supervised deep models; 3)
the ability to predict the presence of tumor in Whole Slide Images (WSIs) using
multiple instance learning (MIL), achieving an AUC of 0.98 and 94% accuracy.

Our results show that our model captures distinct phenotypic characteristics
of real tissue samples, paving the way for further understanding of tumor
progression and tumor micro-environment, and ultimately refining
histopathological classification for diagnosis and treatment. The code and
pretrained models are available at:
https://github.com/AdalbertoCq/Adversarial-learning-of-cancer-tissue-representations]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Quiros_A/0/1/0/all/0/1"&gt;Adalberto Claudio Quiros&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Coudray_N/0/1/0/all/0/1"&gt;Nicolas Coudray&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yeaton_A/0/1/0/all/0/1"&gt;Anna Yeaton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sunhem_W/0/1/0/all/0/1"&gt;Wisuwat Sunhem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Murray_Smith_R/0/1/0/all/0/1"&gt;Roderick Murray-Smith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsirigos_A/0/1/0/all/0/1"&gt;Aristotelis Tsirigos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_K/0/1/0/all/0/1"&gt;Ke Yuan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Detection of Lung Nodules in Chest Radiography Using Generative Adversarial Networks. (arXiv:2108.02233v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02233</id>
        <link href="http://arxiv.org/abs/2108.02233"/>
        <updated>2021-08-06T00:51:44.792Z</updated>
        <summary type="html"><![CDATA[Lung nodules are commonly missed in chest radiographs. We propose and
evaluate P-AnoGAN, an unsupervised anomaly detection approach for lung nodules
in radiographs. P-AnoGAN modifies the fast anomaly detection generative
adversarial network (f-AnoGAN) by utilizing a progressive GAN and a
convolutional encoder-decoder-encoder pipeline. Model training uses only
unlabelled healthy lung patches extracted from the Indiana University Chest
X-Ray Collection. External validation and testing are performed using healthy
and unhealthy patches extracted from the ChestX-ray14 and Japanese Society for
Radiological Technology datasets, respectively. Our model robustly identifies
patches containing lung nodules in external validation and test data with
ROC-AUC of 91.17% and 87.89%, respectively. These results show unsupervised
methods may be useful in challenging tasks such as lung nodule detection in
radiographs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bhatt_N/0/1/0/all/0/1"&gt;Nitish Bhatt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prados_D/0/1/0/all/0/1"&gt;David Ramon Prados&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hodzic_N/0/1/0/all/0/1"&gt;Nedim Hodzic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karanassios_C/0/1/0/all/0/1"&gt;Christos Karanassios&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tizhoosh_H/0/1/0/all/0/1"&gt;H.R. Tizhoosh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improved Language Identification Through Cross-Lingual Self-Supervised Learning. (arXiv:2107.04082v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.04082</id>
        <link href="http://arxiv.org/abs/2107.04082"/>
        <updated>2021-08-06T00:51:44.765Z</updated>
        <summary type="html"><![CDATA[Language identification greatly impacts the success of downstream tasks such
as automatic speech recognition. Recently, self-supervised speech
representations learned by wav2vec 2.0 have been shown to be very effective for
a range of speech tasks. We extend previous self-supervised work on language
identification by experimenting with pre-trained models which were learned on
real-world unconstrained speech in multiple languages and not just on English.
We show that models pre-trained on many languages perform better and enable
language identification systems that require very little labeled data to
perform well. Results on a 25 languages setup show that with only 10 minutes of
labeled data per language, a cross-lingually pre-trained model can achieve over
93% accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tjandra_A/0/1/0/all/0/1"&gt;Andros Tjandra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choudhury_D/0/1/0/all/0/1"&gt;Diptanu Gon Choudhury&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1"&gt;Frank Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_K/0/1/0/all/0/1"&gt;Kritika Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Conneau_A/0/1/0/all/0/1"&gt;Alexis Conneau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baevski_A/0/1/0/all/0/1"&gt;Alexei Baevski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sela_A/0/1/0/all/0/1"&gt;Assaf Sela&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saraf_Y/0/1/0/all/0/1"&gt;Yatharth Saraf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Auli_M/0/1/0/all/0/1"&gt;Michael Auli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sinsy: A Deep Neural Network-Based Singing Voice Synthesis System. (arXiv:2108.02776v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2108.02776</id>
        <link href="http://arxiv.org/abs/2108.02776"/>
        <updated>2021-08-06T00:51:44.726Z</updated>
        <summary type="html"><![CDATA[This paper presents Sinsy, a deep neural network (DNN)-based singing voice
synthesis (SVS) system. In recent years, DNNs have been utilized in statistical
parametric SVS systems, and DNN-based SVS systems have demonstrated better
performance than conventional hidden Markov model-based ones. SVS systems are
required to synthesize a singing voice with pitch and timing that strictly
follow a given musical score. Additionally, singing expressions that are not
described on the musical score, such as vibrato and timing fluctuations, should
be reproduced. The proposed system is composed of four modules: a time-lag
model, a duration model, an acoustic model, and a vocoder, and singing voices
can be synthesized taking these characteristics of singing voices into account.
To better model a singing voice, the proposed system incorporates improved
approaches to modeling pitch and vibrato and better training criteria into the
acoustic model. In addition, we incorporated PeriodNet, a non-autoregressive
neural vocoder with robustness for the pitch, into our systems to generate a
high-fidelity singing voice waveform. Moreover, we propose automatic pitch
correction techniques for DNN-based SVS to synthesize singing voices with
correct pitch even if the training data has out-of-tune phrases. Experimental
results show our system can synthesize a singing voice with better timing, more
natural vibrato, and correct pitch, and it can achieve better mean opinion
scores in subjective evaluation tests.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Hono_Y/0/1/0/all/0/1"&gt;Yukiya Hono&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hashimoto_K/0/1/0/all/0/1"&gt;Kei Hashimoto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Oura_K/0/1/0/all/0/1"&gt;Keiichiro Oura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Nankaku_Y/0/1/0/all/0/1"&gt;Yoshihiko Nankaku&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tokuda_K/0/1/0/all/0/1"&gt;Keiichi Tokuda&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Recipes for Safety in Open-domain Chatbots. (arXiv:2010.07079v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.07079</id>
        <link href="http://arxiv.org/abs/2010.07079"/>
        <updated>2021-08-06T00:51:44.713Z</updated>
        <summary type="html"><![CDATA[Models trained on large unlabeled corpora of human interactions will learn
patterns and mimic behaviors therein, which include offensive or otherwise
toxic behavior and unwanted biases. We investigate a variety of methods to
mitigate these issues in the context of open-domain generative dialogue models.
We introduce a new human-and-model-in-the-loop framework for both training
safer models and for evaluating them, as well as a novel method to distill
safety considerations inside generative models without the use of an external
classifier at deployment time. We conduct experiments comparing these methods
and find our new techniques are (i) safer than existing models as measured by
automatic and human evaluations while (ii) maintaining usability metrics such
as engagingness relative to the state of the art. We then discuss the
limitations of this work by analyzing failure cases of our models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1"&gt;Jing Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ju_D/0/1/0/all/0/1"&gt;Da Ju&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1"&gt;Margaret Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boureau_Y/0/1/0/all/0/1"&gt;Y-Lan Boureau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weston_J/0/1/0/all/0/1"&gt;Jason Weston&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dinan_E/0/1/0/all/0/1"&gt;Emily Dinan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Coreference Resolution: Are the eliminated spans totally worthless?. (arXiv:2101.00737v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.00737</id>
        <link href="http://arxiv.org/abs/2101.00737"/>
        <updated>2021-08-06T00:51:44.672Z</updated>
        <summary type="html"><![CDATA[Various neural-based methods have been proposed so far for joint mention
detection and coreference resolution. However, existing works on coreference
resolution are mainly dependent on filtered mention representation, while other
spans are largely neglected. In this paper, we aim at increasing the
utilization rate of data and investigating whether those eliminated spans are
totally useless, or to what extent they can improve the performance of
coreference resolution. To achieve this, we propose a mention representation
refining strategy where spans highly related to mentions are well leveraged
using a pointer network for representation enhancing. Notably, we utilize an
additional loss term in this work to encourage the diversity between entity
clusters. Experimental results on the document-level CoNLL-2012 Shared Task
English dataset show that eliminated spans are indeed much effective and our
approach can achieve competitive results when compared with previous
state-of-the-art in coreference resolution.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1"&gt;Xin Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Longyin Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_G/0/1/0/all/0/1"&gt;Guodong Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[UniCon: Unified Context Network for Robust Active Speaker Detection. (arXiv:2108.02607v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02607</id>
        <link href="http://arxiv.org/abs/2108.02607"/>
        <updated>2021-08-06T00:51:44.662Z</updated>
        <summary type="html"><![CDATA[We introduce a new efficient framework, the Unified Context Network (UniCon),
for robust active speaker detection (ASD). Traditional methods for ASD usually
operate on each candidate's pre-cropped face track separately and do not
sufficiently consider the relationships among the candidates. This potentially
limits performance, especially in challenging scenarios with low-resolution
faces, multiple candidates, etc. Our solution is a novel, unified framework
that focuses on jointly modeling multiple types of contextual information:
spatial context to indicate the position and scale of each candidate's face,
relational context to capture the visual relationships among the candidates and
contrast audio-visual affinities with each other, and temporal context to
aggregate long-term information and smooth out local uncertainties. Based on
such information, our model optimizes all candidates in a unified process for
robust and reliable ASD. A thorough ablation study is performed on several
challenging ASD benchmarks under different settings. In particular, our method
outperforms the state-of-the-art by a large margin of about 15% mean Average
Precision (mAP) absolute on two challenging subsets: one with three candidate
speakers, and the other with faces smaller than 64 pixels. Together, our UniCon
achieves 92.0% mAP on the AVA-ActiveSpeaker validation set, surpassing 90% for
the first time on this challenging dataset at the time of submission. Project
website: https://unicon-asd.github.io/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yuanhang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1"&gt;Susan Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1"&gt;Shuang Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Zhongqin Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shan_S/0/1/0/all/0/1"&gt;Shiguang Shan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xilin Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Knowledge Distillation from BERT Transformer to Speech Transformer for Intent Classification. (arXiv:2108.02598v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.02598</id>
        <link href="http://arxiv.org/abs/2108.02598"/>
        <updated>2021-08-06T00:51:44.641Z</updated>
        <summary type="html"><![CDATA[End-to-end intent classification using speech has numerous advantages
compared to the conventional pipeline approach using automatic speech
recognition (ASR), followed by natural language processing modules. It attempts
to predict intent from speech without using an intermediate ASR module.
However, such end-to-end framework suffers from the unavailability of large
speech resources with higher acoustic variation in spoken language
understanding. In this work, we exploit the scope of the transformer
distillation method that is specifically designed for knowledge distillation
from a transformer based language model to a transformer based speech model. In
this regard, we leverage the reliable and widely used bidirectional encoder
representations from transformers (BERT) model as a language model and transfer
the knowledge to build an acoustic model for intent classification using the
speech. In particular, a multilevel transformer based teacher-student model is
designed, and knowledge distillation is performed across attention and hidden
sub-layers of different transformer layers of the student and teacher models.
We achieve an intent classification accuracy of 99.10% and 88.79% for Fluent
speech corpus and ATIS database, respectively. Further, the proposed method
demonstrates better performance and robustness in acoustically degraded
condition compared to the baseline method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1"&gt;Yidi Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_B/0/1/0/all/0/1"&gt;Bidisha Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Madhavi_M/0/1/0/all/0/1"&gt;Maulik Madhavi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Haizhou Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-clue reconstruction of sharing chains for social media images. (arXiv:2108.02515v1 [cs.MM])]]></title>
        <id>http://arxiv.org/abs/2108.02515</id>
        <link href="http://arxiv.org/abs/2108.02515"/>
        <updated>2021-08-06T00:51:44.632Z</updated>
        <summary type="html"><![CDATA[The amount of multimedia content shared everyday, combined with the level of
realism reached by recent fake-generating technologies, threatens to impair the
trustworthiness of online information sources. The process of uploading and
sharing data tends to hinder standard media forensic analyses, since multiple
re-sharing steps progressively hide the traces of past manipulations. At the
same time though, new traces are introduced by the platforms themselves,
enabling the reconstruction of the sharing history of digital objects, with
possible applications in information flow monitoring and source identification.
In this work, we propose a supervised framework for the reconstruction of image
sharing chains on social media platforms. The system is structured as a cascade
of backtracking blocks, each of them tracing back one step of the sharing chain
at a time. Blocks are designed as ensembles of classifiers trained to analyse
the input image independently from one another by leveraging different feature
representations that describe both content and container of the media object.
Individual decisions are then properly combined by a late fusion strategy.
Results highlight the advantages of employing multiple clues, which allow
accurately tracing back up to three steps along the sharing chain.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Verde_S/0/1/0/all/0/1"&gt;Sebastiano Verde&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pasquini_C/0/1/0/all/0/1"&gt;Cecilia Pasquini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lago_F/0/1/0/all/0/1"&gt;Federica Lago&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goller_A/0/1/0/all/0/1"&gt;Alessandro Goller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Natale_F/0/1/0/all/0/1"&gt;Francesco GB De Natale&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Piva_A/0/1/0/all/0/1"&gt;Alessandro Piva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boato_G/0/1/0/all/0/1"&gt;Giulia Boato&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MSTRE-Net: Multistreaming Acoustic Modeling for Automatic Lyrics Transcription. (arXiv:2108.02625v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2108.02625</id>
        <link href="http://arxiv.org/abs/2108.02625"/>
        <updated>2021-08-06T00:51:44.625Z</updated>
        <summary type="html"><![CDATA[This paper makes several contributions to automatic lyrics transcription
(ALT) research. Our main contribution is a novel variant of the Multistreaming
Time-Delay Neural Network (MTDNN) architecture, called MSTRE-Net, which
processes the temporal information using multiple streams in parallel with
varying resolutions keeping the network more compact, and thus with a faster
inference and an improved recognition rate than having identical TDNN streams.
In addition, two novel preprocessing steps prior to training the acoustic model
are proposed. First, we suggest using recordings from both monophonic and
polyphonic domains during training the acoustic model. Second, we tag
monophonic and polyphonic recordings with distinct labels for discriminating
non-vocal silence and music instances during alignment. Moreover, we present a
new test set with a considerably larger size and a higher musical variability
compared to the existing datasets used in ALT literature, while maintaining the
gender balance of the singers. Our best performing model sets the
state-of-the-art in lyrics transcription by a large margin. For
reproducibility, we publicly share the identifiers to retrieve the data used in
this paper.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Demirel_E/0/1/0/all/0/1"&gt;Emir Demirel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ahlback_S/0/1/0/all/0/1"&gt;Sven Ahlb&amp;#xe4;ck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dixon_S/0/1/0/all/0/1"&gt;Simon Dixon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automatic Detection of COVID-19 Vaccine Misinformation with Graph Link Prediction. (arXiv:2108.02314v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.02314</id>
        <link href="http://arxiv.org/abs/2108.02314"/>
        <updated>2021-08-06T00:51:44.610Z</updated>
        <summary type="html"><![CDATA[Enormous hope in the efficacy of vaccines became recently a successful
reality in the fight against the COVID-19 pandemic. However, vaccine hesitancy,
fueled by exposure to social media misinformation about COVID-19 vaccines
became a major hurdle. Therefore, it is essential to automatically detect where
misinformation about COVID-19 vaccines on social media is spread and what kind
of misinformation is discussed, such that inoculation interventions can be
delivered at the right time and in the right place, in addition to
interventions designed to address vaccine hesitancy. This paper is addressing
the first step in tackling hesitancy against COVID-19 vaccines, namely the
automatic detection of misinformation about the vaccines on Twitter, the social
media platform that has the highest volume of conversations about COVID-19 and
its vaccines. We present CoVaxLies, a new dataset of tweets judged relevant to
several misinformation targets about COVID-19 vaccines on which a novel method
of detecting misinformation was developed. Our method organizes CoVaxLies in a
Misinformation Knowledge Graph as it casts misinformation detection as a graph
link prediction problem. The misinformation detection method detailed in this
paper takes advantage of the link scoring functions provided by several
knowledge embedding methods. The experimental results demonstrate the
superiority of this method when compared with classification-based methods,
widely used currently.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Weinzierl_M/0/1/0/all/0/1"&gt;Maxwell A. Weinzierl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Harabagiu_S/0/1/0/all/0/1"&gt;Sanda M. Harabagiu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Inserting Information Bottlenecks for Attribution in Transformers. (arXiv:2012.13838v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.13838</id>
        <link href="http://arxiv.org/abs/2012.13838"/>
        <updated>2021-08-06T00:51:44.601Z</updated>
        <summary type="html"><![CDATA[Pretrained transformers achieve the state of the art across tasks in natural
language processing, motivating researchers to investigate their inner
mechanisms. One common direction is to understand what features are important
for prediction. In this paper, we apply information bottlenecks to analyze the
attribution of each feature for prediction on a black-box model. We use BERT as
the example and evaluate our approach both quantitatively and qualitatively. We
show the effectiveness of our method in terms of attribution and the ability to
provide insight into how information flows through layers. We demonstrate that
our technique outperforms two competitive methods in degradation tests on four
datasets. Code is available at https://github.com/bazingagin/IBA.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1"&gt;Zhiying Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_R/0/1/0/all/0/1"&gt;Raphael Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xin_J/0/1/0/all/0/1"&gt;Ji Xin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1"&gt;Jimmy Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evaluation of Audio-Visual Alignments in Visually Grounded Speech Models. (arXiv:2108.02562v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02562</id>
        <link href="http://arxiv.org/abs/2108.02562"/>
        <updated>2021-08-06T00:51:44.588Z</updated>
        <summary type="html"><![CDATA[Systems that can find correspondences between multiple modalities, such as
between speech and images, have great potential to solve different recognition
and data analysis tasks in an unsupervised manner. This work studies multimodal
learning in the context of visually grounded speech (VGS) models, and focuses
on their recently demonstrated capability to extract spatiotemporal alignments
between spoken words and the corresponding visual objects without ever been
explicitly trained for object localization or word recognition. As the main
contributions, we formalize the alignment problem in terms of an audiovisual
alignment tensor that is based on earlier VGS work, introduce systematic
metrics for evaluating model performance in aligning visual objects and spoken
words, and propose a new VGS model variant for the alignment task utilizing
cross-modal attention layer. We test our model and a previously proposed model
in the alignment task using SPEECH-COCO captions coupled with MSCOCO images. We
compare the alignment performance using our proposed evaluation metrics to the
semantic retrieval task commonly used to evaluate VGS models. We show that
cross-modal attention layer not only helps the model to achieve higher semantic
cross-modal retrieval performance, but also leads to substantial improvements
in the alignment performance between image object and spoken words.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khorrami_K/0/1/0/all/0/1"&gt;Khazar Khorrami&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rasanen_O/0/1/0/all/0/1"&gt;Okko R&amp;#xe4;s&amp;#xe4;nen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bambara Language Dataset for Sentiment Analysis. (arXiv:2108.02524v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.02524</id>
        <link href="http://arxiv.org/abs/2108.02524"/>
        <updated>2021-08-06T00:51:44.560Z</updated>
        <summary type="html"><![CDATA[For easier communication, posting, or commenting on each others posts, people
use their dialects. In Africa, various languages and dialects exist. However,
they are still underrepresented and not fully exploited for analytical studies
and research purposes. In order to perform approaches like Machine Learning and
Deep Learning, datasets are required. One of the African languages is Bambara,
used by citizens in different countries. However, no previous work on datasets
for this language was performed for Sentiment Analysis. In this paper, we
present the first common-crawl-based Bambara dialectal dataset dedicated for
Sentiment Analysis, available freely for Natural Language Processing research
purposes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Diallo_M/0/1/0/all/0/1"&gt;Mountaga Diallo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fourati_C/0/1/0/all/0/1"&gt;Chayma Fourati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Haddad_H/0/1/0/all/0/1"&gt;Hatem Haddad&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[WeChat Neural Machine Translation Systems for WMT21. (arXiv:2108.02401v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.02401</id>
        <link href="http://arxiv.org/abs/2108.02401"/>
        <updated>2021-08-06T00:51:44.470Z</updated>
        <summary type="html"><![CDATA[This paper introduces WeChat AI's participation in WMT 2021 shared news
translation task on English->Chinese, English->Japanese, Japanese->English and
English->German. Our systems are based on the Transformer (Vaswani et al.,
2017) with several novel and effective variants. In our experiments, we employ
data filtering, large-scale synthetic data generation (i.e., back-translation,
knowledge distillation, forward-translation, iterative in-domain knowledge
transfer), advanced finetuning approaches, and boosted Self-BLEU based model
ensemble. Our constrained systems achieve 36.9, 46.9, 27.8 and 31.3
case-sensitive BLEU scores on English->Chinese, English->Japanese,
Japanese->English and English->German, respectively. The BLEU scores of
English->Chinese, English->Japanese and Japanese->English are the highest among
all submissions, and that of English->German is the highest among all
constrained submissions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1"&gt;Xianfeng Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yijin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_E/0/1/0/all/0/1"&gt;Ernan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ran_Q/0/1/0/all/0/1"&gt;Qiu Ran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1"&gt;Fandong Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1"&gt;Peng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1"&gt;Jinan Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jie Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hybrid Reasoning Network for Video-based Commonsense Captioning. (arXiv:2108.02365v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02365</id>
        <link href="http://arxiv.org/abs/2108.02365"/>
        <updated>2021-08-06T00:51:44.457Z</updated>
        <summary type="html"><![CDATA[The task of video-based commonsense captioning aims to generate event-wise
captions and meanwhile provide multiple commonsense descriptions (e.g.,
attribute, effect and intention) about the underlying event in the video. Prior
works explore the commonsense captions by using separate networks for different
commonsense types, which is time-consuming and lacks mining the interaction of
different commonsense. In this paper, we propose a Hybrid Reasoning Network
(HybridNet) to endow the neural networks with the capability of semantic-level
reasoning and word-level reasoning. Firstly, we develop multi-commonsense
learning for semantic-level reasoning by jointly training different commonsense
types in a unified network, which encourages the interaction between the clues
of multiple commonsense descriptions, event-wise captions and videos. Then,
there are two steps to achieve the word-level reasoning: (1) a memory module
records the history predicted sequence from the previous generation processes;
(2) a memory-routed multi-head attention (MMHA) module updates the word-level
attention maps by incorporating the history information from the memory module
into the transformer decoder for word-level reasoning. Moreover, the multimodal
features are used to make full use of diverse knowledge for commonsense
reasoning. Experiments and abundant analysis on the large-scale
Video-to-Commonsense benchmark show that our HybridNet achieves
state-of-the-art performance compared with other methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1"&gt;Weijiang Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1"&gt;Jian Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_L/0/1/0/all/0/1"&gt;Lei Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1"&gt;Yuejian Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_N/0/1/0/all/0/1"&gt;Nong Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1"&gt;Nan Duan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Zipf Matrix Factorization : Matrix Factorization with Matthew Effect Reduction. (arXiv:2106.07347v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.07347</id>
        <link href="http://arxiv.org/abs/2106.07347"/>
        <updated>2021-08-06T00:51:44.435Z</updated>
        <summary type="html"><![CDATA[Recommender system recommends interesting items to users based on users' past
information history. Researchers have been paying attention to improvement of
algorithmic performance such as MAE and precision@K. Major techniques such as
matrix factorization and learning to rank are optimized based on such
evaluation metrics. However, the intrinsic Matthew Effect problem poses great
threat to the fairness of the recommender system, and the unfairness problem
cannot be resolved by optimization of traditional metrics. In this paper, we
propose a novel algorithm that incorporates Matthew Effect reduction with the
matrix factorization framework. We demonstrate that our approach can boost the
fairness of the algorithm and enhances performance evaluated by traditional
metrics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hao Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CoReD: Generalizing Fake Media Detection with Continual Representation using Distillation. (arXiv:2107.02408v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.02408</id>
        <link href="http://arxiv.org/abs/2107.02408"/>
        <updated>2021-08-06T00:51:44.416Z</updated>
        <summary type="html"><![CDATA[Over the last few decades, artificial intelligence research has made
tremendous strides, but it still heavily relies on fixed datasets in stationary
environments. Continual learning is a growing field of research that examines
how AI systems can learn sequentially from a continuous stream of linked data
in the same way that biological systems do. Simultaneously, fake media such as
deepfakes and synthetic face images have emerged as significant to current
multimedia technologies. Recently, numerous method has been proposed which can
detect deepfakes with high accuracy. However, they suffer significantly due to
their reliance on fixed datasets in limited evaluation settings. Therefore, in
this work, we apply continuous learning to neural networks' learning dynamics,
emphasizing its potential to increase data efficiency significantly. We propose
Continual Representation using Distillation (CoReD) method that employs the
concept of Continual Learning (CL), Representation Learning (RL), and Knowledge
Distillation (KD). We design CoReD to perform sequential domain adaptation
tasks on new deepfake and GAN-generated synthetic face datasets, while
effectively minimizing the catastrophic forgetting in a teacher-student model
setting. Our extensive experimental results demonstrate that our method is
efficient at domain adaptation to detect low-quality deepfakes videos and
GAN-generated images from several datasets, outperforming the-state-of-art
baseline methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1"&gt;Minha Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tariq_S/0/1/0/all/0/1"&gt;Shahroz Tariq&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Woo_S/0/1/0/all/0/1"&gt;Simon S. Woo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Empirical Study of UMLS Concept Extraction from Clinical Notes using Boolean Combination Ensembles. (arXiv:2108.02255v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.02255</id>
        <link href="http://arxiv.org/abs/2108.02255"/>
        <updated>2021-08-06T00:51:44.405Z</updated>
        <summary type="html"><![CDATA[Our objective in this study is to investigate the behavior of Boolean
operators on combining annotation output from multiple Natural Language
Processing (NLP) systems across multiple corpora and to assess how filtering by
aggregation of Unified Medical Language System (UMLS) Metathesaurus concepts
affects system performance for Named Entity Recognition (NER) of UMLS concepts.
We used three corpora annotated for UMLS concepts: 2010 i2b2 VA challenge set
(31,161 annotations), Multi-source Integrated Platform for Answering Clinical
Questions (MiPACQ) corpus (17,457 annotations including UMLS concept unique
identifiers), and Fairview Health Services corpus (44,530 annotations). Our
results showed that for UMLS concept matching, Boolean ensembling of the MiPACQ
corpus trended towards higher performance over individual systems. Use of an
approximate grid-search can help optimize the precision-recall tradeoff and can
provide a set of heuristics for choosing an optimal set of ensembles.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Silverman_G/0/1/0/all/0/1"&gt;Greg M. Silverman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Finzel_R/0/1/0/all/0/1"&gt;Raymond L. Finzel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heinz_M/0/1/0/all/0/1"&gt;Michael V. Heinz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vasilakes_J/0/1/0/all/0/1"&gt;Jake Vasilakes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Solinsky_J/0/1/0/all/0/1"&gt;Jacob C. Solinsky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McEwan_R/0/1/0/all/0/1"&gt;Reed McEwan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Knoll_B/0/1/0/all/0/1"&gt;Benjamin C. Knoll&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tignanelli_C/0/1/0/all/0/1"&gt;Christopher J. Tignanelli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Hongfang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1"&gt;Hua Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1"&gt;Xiaoqian Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Melton_G/0/1/0/all/0/1"&gt;Genevieve B. Melton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pakhomov_S/0/1/0/all/0/1"&gt;Serguei VS Pakhomov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Joint Geometry and Color Projection-based Point Cloud Quality Metric. (arXiv:2108.02481v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2108.02481</id>
        <link href="http://arxiv.org/abs/2108.02481"/>
        <updated>2021-08-06T00:51:44.395Z</updated>
        <summary type="html"><![CDATA[Point cloud coding solutions have been recently standardized to address the
needs of multiple application scenarios. The design and assessment of point
cloud coding methods require reliable objective quality metrics to evaluate the
level of degradation introduced by compression or any other type of processing.
Several point cloud objective quality metrics has been recently proposed to
reliable estimate human perceived quality, including the so-called
projection-based metrics. In this context, this paper proposes a joint geometry
and color projection-based point cloud objective quality metric which solves
the critical weakness of this type of quality metrics, i.e., the misalignment
between the reference and degraded projected images. Moreover, the proposed
point cloud quality metric exploits the best performing 2D quality metrics in
the literature to assess the quality of the projected images. The experimental
results show that the proposed projection-based quality metric offers the best
subjective-objective correlation performance in comparison with other metrics
in the literature. The Pearson correlation gains regarding D1-PSNR and D2-PSNR
metrics are 17% and 14.2 when data with all coding degradations is considered.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Javaheri_A/0/1/0/all/0/1"&gt;Alireza Javaheri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Brites_C/0/1/0/all/0/1"&gt;Catarina Brites&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Pereira_F/0/1/0/all/0/1"&gt;Fernando Pereira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ascenso_J/0/1/0/all/0/1"&gt;Jo&amp;#xe3;o Ascenso&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Transfer Learning with Pretrained Language Models through Adapters. (arXiv:2108.02340v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.02340</id>
        <link href="http://arxiv.org/abs/2108.02340"/>
        <updated>2021-08-06T00:51:44.369Z</updated>
        <summary type="html"><![CDATA[Transfer learning with large pretrained transformer-based language models
like BERT has become a dominating approach for most NLP tasks. Simply
fine-tuning those large language models on downstream tasks or combining it
with task-specific pretraining is often not robust. In particular, the
performance considerably varies as the random seed changes or the number of
pretraining and/or fine-tuning iterations varies, and the fine-tuned model is
vulnerable to adversarial attack. We propose a simple yet effective
adapter-based approach to mitigate these issues. Specifically, we insert small
bottleneck layers (i.e., adapter) within each layer of a pretrained model, then
fix the pretrained layers and train the adapter layers on the downstream task
data, with (1) task-specific unsupervised pretraining and then (2)
task-specific supervised training (e.g., classification, sequence labeling).
Our experiments demonstrate that such a training scheme leads to improved
stability and adversarial robustness in transfer learning to various downstream
tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Han_W/0/1/0/all/0/1"&gt;Wenjuan Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pang_B/0/1/0/all/0/1"&gt;Bo Pang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yingnian Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[O2NA: An Object-Oriented Non-Autoregressive Approach for Controllable Video Captioning. (arXiv:2108.02359v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.02359</id>
        <link href="http://arxiv.org/abs/2108.02359"/>
        <updated>2021-08-06T00:51:44.356Z</updated>
        <summary type="html"><![CDATA[Video captioning combines video understanding and language generation.
Different from image captioning that describes a static image with details of
almost every object, video captioning usually considers a sequence of frames
and biases towards focused objects, e.g., the objects that stay in focus
regardless of the changing background. Therefore, detecting and properly
accommodating focused objects is critical in video captioning. To enforce the
description of focused objects and achieve controllable video captioning, we
propose an Object-Oriented Non-Autoregressive approach (O2NA), which performs
caption generation in three steps: 1) identify the focused objects and predict
their locations in the target caption; 2) generate the related attribute words
and relation words of these focused objects to form a draft caption; and 3)
combine video information to refine the draft caption to a fluent final
caption. Since the focused objects are generated and located ahead of other
words, it is difficult to apply the word-by-word autoregressive generation
process; instead, we adopt a non-autoregressive approach. The experiments on
two benchmark datasets, i.e., MSR-VTT and MSVD, demonstrate the effectiveness
of O2NA, which achieves results competitive with the state-of-the-arts but with
both higher diversity and higher inference speed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1"&gt;Fenglin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1"&gt;Xuancheng Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1"&gt;Xian Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1"&gt;Bang Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ge_S/0/1/0/all/0/1"&gt;Shen Ge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1"&gt;Xu Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Step Critiquing User Interface for Recommender Systems. (arXiv:2107.06416v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.06416</id>
        <link href="http://arxiv.org/abs/2107.06416"/>
        <updated>2021-08-06T00:51:44.129Z</updated>
        <summary type="html"><![CDATA[Recommendations with personalized explanations have been shown to increase
user trust and perceived quality and help users make better decisions.
Moreover, such explanations allow users to provide feedback by critiquing them.
Several algorithms for recommender systems with multi-step critiquing have
therefore been developed. However, providing a user-friendly interface based on
personalized explanations and critiquing has not been addressed in the last
decade. In this paper, we introduce four different web interfaces (available
under https://lia.epfl.ch/critiquing/) helping users making decisions and
finding their ideal item. We have chosen the hotel recommendation domain as a
use case even though our approach is trivially adaptable for other domains.
Moreover, our system is model-agnostic (for both recommender systems and
critiquing models) allowing a great flexibility and further extensions. Our
interfaces are above all a useful tool to help research in recommendation with
critiquing. They allow to test such systems on a real use case and also to
highlight some limitations of these approaches to find solutions to overcome
them.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Petrescu_D/0/1/0/all/0/1"&gt;Diana Petrescu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Antognini_D/0/1/0/all/0/1"&gt;Diego Antognini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Faltings_B/0/1/0/all/0/1"&gt;Boi Faltings&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Performer Identification From Symbolic Representation of Music Using Statistical Models. (arXiv:2108.02576v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2108.02576</id>
        <link href="http://arxiv.org/abs/2108.02576"/>
        <updated>2021-08-06T00:51:44.089Z</updated>
        <summary type="html"><![CDATA[Music Performers have their own idiosyncratic way of interpreting a musical
piece. A group of skilled performers playing the same piece of music would
likely to inject their unique artistic styles in their performances. The
variations of the tempo, timing, dynamics, articulation etc. from the actual
notated music are what make the performers unique in their performances. This
study presents a dataset consisting of four movements of Schubert's ``Sonata in
B-flat major, D.960" performed by nine virtuoso pianists individually. We
proposed and extracted a set of expressive features that are able to capture
the characteristics of an individual performer's style. We then present a
performer identification method based on the similarity of feature
distribution, given a set of piano performances. The identification is done
considering each feature individually as well as a fusion of the features.
Results show that the proposed method achieved a precision of 0.903 using
fusion features. Moreover, the onset time deviation feature shows promising
result when considered individually.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rafee_S/0/1/0/all/0/1"&gt;Syed Rifat Mahmud Rafee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fazekas_G/0/1/0/all/0/1"&gt;Gyorgy Fazekas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wiggins_G/0/1/0/all/0/1"&gt;Geraint A.~Wiggins&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Time-aware Path Reasoning on Knowledge Graph for Recommendation. (arXiv:2108.02634v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2108.02634</id>
        <link href="http://arxiv.org/abs/2108.02634"/>
        <updated>2021-08-06T00:51:44.076Z</updated>
        <summary type="html"><![CDATA[Reasoning on knowledge graph (KG) has been studied for explainable
recommendation due to it's ability of providing explicit explanations. However,
current KG-based explainable recommendation methods unfortunately ignore the
temporal information (such as purchase time, recommend time, etc.), which may
result in unsuitable explanations. In this work, we propose a novel Time-aware
Path reasoning for Recommendation (TPRec for short) method, which leverages the
potential of temporal information to offer better recommendation with plausible
explanations. First, we present an efficient time-aware interaction relation
extraction component to construct collaborative knowledge graph with time-aware
interactions (TCKG for short), and then introduce a novel time-aware path
reasoning method for recommendation. We conduct extensive experiments on three
real-world datasets. The results demonstrate that the proposed TPRec could
successfully employ TCKG to achieve substantial gains and improve the quality
of explainable recommendation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yuyue Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jiawei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_W/0/1/0/all/0/1"&gt;Wei Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yashen Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1"&gt;Xiangnan He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_H/0/1/0/all/0/1"&gt;Haiyong Xie&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MSTRE-Net: Multistreaming Acoustic Modeling for Automatic Lyrics Transcription. (arXiv:2108.02625v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2108.02625</id>
        <link href="http://arxiv.org/abs/2108.02625"/>
        <updated>2021-08-06T00:51:44.047Z</updated>
        <summary type="html"><![CDATA[This paper makes several contributions to automatic lyrics transcription
(ALT) research. Our main contribution is a novel variant of the Multistreaming
Time-Delay Neural Network (MTDNN) architecture, called MSTRE-Net, which
processes the temporal information using multiple streams in parallel with
varying resolutions keeping the network more compact, and thus with a faster
inference and an improved recognition rate than having identical TDNN streams.
In addition, two novel preprocessing steps prior to training the acoustic model
are proposed. First, we suggest using recordings from both monophonic and
polyphonic domains during training the acoustic model. Second, we tag
monophonic and polyphonic recordings with distinct labels for discriminating
non-vocal silence and music instances during alignment. Moreover, we present a
new test set with a considerably larger size and a higher musical variability
compared to the existing datasets used in ALT literature, while maintaining the
gender balance of the singers. Our best performing model sets the
state-of-the-art in lyrics transcription by a large margin. For
reproducibility, we publicly share the identifiers to retrieve the data used in
this paper.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Demirel_E/0/1/0/all/0/1"&gt;Emir Demirel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ahlback_S/0/1/0/all/0/1"&gt;Sven Ahlb&amp;#xe4;ck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dixon_S/0/1/0/all/0/1"&gt;Simon Dixon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Itinerary-aware Personalized Deep Matching at Fliggy. (arXiv:2108.02343v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2108.02343</id>
        <link href="http://arxiv.org/abs/2108.02343"/>
        <updated>2021-08-06T00:51:44.001Z</updated>
        <summary type="html"><![CDATA[Matching items for a user from a travel item pool of large cardinality have
been the most important technology for increasing the business at Fliggy, one
of the most popular online travel platforms (OTPs) in China. There are three
major challenges facing OTPs: sparsity, diversity, and implicitness. In this
paper, we present a novel Fliggy ITinerary-aware deep matching NETwork (FitNET)
to address these three challenges. FitNET is designed based on the popular deep
matching network, which has been successfully employed in many industrial
recommendation systems, due to its effectiveness. The concept itinerary is
firstly proposed under the context of recommendation systems for OTPs, which is
defined as the list of unconsumed orders of a user. All orders in a user
itinerary are learned as a whole, based on which the implicit travel intention
of each user can be more accurately inferred. To alleviate the sparsity
problem, users' profiles are incorporated into FitNET. Meanwhile, a series of
itinerary-aware attention mechanisms that capture the vital interactions
between user's itinerary and other input categories are carefully designed.
These mechanisms are very helpful in inferring a user's travel intention or
preference, and handling the diversity in a user's need. Further, two training
objectives, i.e., prediction accuracy of user's travel intention and prediction
accuracy of user's click behavior, are utilized by FitNET, so that these two
objectives can be optimized simultaneously. An offline experiment on Fliggy
production dataset with over 0.27 million users and 1.55 million travel items,
and an online A/B test both show that FitNET effectively learns users' travel
intentions, preferences, and diverse needs, based on their itineraries and
gains superior performance compared with state-of-the-art methods. FitNET now
has been successfully deployed at Fliggy, serving major online traffic.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1"&gt;Jia Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Ziyi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zulong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lv_D/0/1/0/all/0/1"&gt;Detao Lv&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1"&gt;Yao Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Chuanfei Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Token Shift Transformer for Video Classification. (arXiv:2108.02432v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02432</id>
        <link href="http://arxiv.org/abs/2108.02432"/>
        <updated>2021-08-06T00:51:43.978Z</updated>
        <summary type="html"><![CDATA[Transformer achieves remarkable successes in understanding 1 and
2-dimensional signals (e.g., NLP and Image Content Understanding). As a
potential alternative to convolutional neural networks, it shares merits of
strong interpretability, high discriminative power on hyper-scale data, and
flexibility in processing varying length inputs. However, its encoders
naturally contain computational intensive operations such as pair-wise
self-attention, incurring heavy computational burden when being applied on the
complex 3-dimensional video signals.

This paper presents Token Shift Module (i.e., TokShift), a novel,
zero-parameter, zero-FLOPs operator, for modeling temporal relations within
each transformer encoder. Specifically, the TokShift barely temporally shifts
partial [Class] token features back-and-forth across adjacent frames. Then, we
densely plug the module into each encoder of a plain 2D vision transformer for
learning 3D video representation. It is worth noticing that our TokShift
transformer is a pure convolutional-free video transformer pilot with
computational efficiency for video understanding. Experiments on standard
benchmarks verify its robustness, effectiveness, and efficiency. Particularly,
with input clips of 8/12 frames, the TokShift transformer achieves SOTA
precision: 79.83%/80.40% on the Kinetics-400, 66.56% on EGTEA-Gaze+, and 96.80%
on UCF-101 datasets, comparable or better than existing SOTA convolutional
counterparts. Our code is open-sourced in:
https://github.com/VideoNetworks/TokShift-Transformer.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Hao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hao_Y/0/1/0/all/0/1"&gt;Yanbin Hao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ngo_C/0/1/0/all/0/1"&gt;Chong-Wah Ngo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LHRM: A LBS based Heterogeneous Relations Model for User Cold Start Recommendation in Online Travel Platform. (arXiv:2108.02344v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2108.02344</id>
        <link href="http://arxiv.org/abs/2108.02344"/>
        <updated>2021-08-06T00:51:43.933Z</updated>
        <summary type="html"><![CDATA[Most current recommender systems used the historical behaviour data of user
to predict user' preference. However, it is difficult to recommend items to new
users accurately. To alleviate this problem, existing user cold start methods
either apply deep learning to build a cross-domain recommender system or map
user attributes into the space of user behaviour. These methods are more
challenging when applied to online travel platform (e.g., Fliggy), because it
is hard to find a cross-domain that user has similar behaviour with travel
scenarios and the Location Based Services (LBS) information of users have not
been paid sufficient attention. In this work, we propose a LBS-based
Heterogeneous Relations Model (LHRM) for user cold start recommendation, which
utilizes user's LBS information and behaviour information in related domains
and user's behaviour information in travel platforms (e.g., Fliggy) to
construct the heterogeneous relations between users and items. Moreover, an
attention-based multi-layer perceptron is applied to extract latent factors of
users and items. Through this way, LHRM has better generalization performance
than existing methods. Experimental results on real data from Fliggy's offline
log illustrate the effectiveness of LHRM.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Ziyi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_W/0/1/0/all/0/1"&gt;Wendong Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zulong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1"&gt;Zhi Jiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Introducing a Family of Synthetic Datasets for Research on Bias in Machine Learning. (arXiv:2107.08928v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.08928</id>
        <link href="http://arxiv.org/abs/2107.08928"/>
        <updated>2021-08-05T01:56:21.727Z</updated>
        <summary type="html"><![CDATA[A significant impediment to progress in research on bias in machine learning
(ML) is the availability of relevant datasets. This situation is unlikely to
change much given the sensitivity of such data. For this reason, there is a
role for synthetic data in this research. In this short paper, we present one
such family of synthetic data sets. We provide an overview of the data,
describe how the level of bias can be varied, and present a simple example of
an experiment on the data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Blanzeisky_W/0/1/0/all/0/1"&gt;William Blanzeisky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cunningham_P/0/1/0/all/0/1"&gt;P&amp;#xe1;draig Cunningham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kennedy_K/0/1/0/all/0/1"&gt;Kenneth Kennedy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[COVID-19 Modeling: A Review. (arXiv:2104.12556v3 [cs.CY] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.12556</id>
        <link href="http://arxiv.org/abs/2104.12556"/>
        <updated>2021-08-05T01:56:21.720Z</updated>
        <summary type="html"><![CDATA[The SARS-CoV-2 virus and COVID-19 disease have posed unprecedented and
overwhelming demand, challenges and opportunities to domain, model and data
driven modeling. This paper provides a comprehensive review of the challenges,
tasks, methods, progress, gaps and opportunities in relation to modeling
COVID-19 problems, data and objectives. It constructs a research landscape of
COVID-19 modeling tasks and methods, and further categorizes, summarizes,
compares and discusses the related methods and progress of modeling COVID-19
epidemic transmission processes and dynamics, case identification and tracing,
infection diagnosis and medical treatments, non-pharmaceutical interventions
and their effects, drug and vaccine development, psychological, economic and
social influence and impact, and misinformation, etc. The modeling methods
involve mathematical and statistical models, domain-driven modeling by
epidemiological compartmental models, medical and biomedical analysis, AI and
data science in particular shallow and deep machine learning, simulation
modeling, social science methods, and hybrid modeling.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cao_L/0/1/0/all/0/1"&gt;Longbing Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1"&gt;Qing Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Differentially private training of neural networks with Langevin dynamics for calibrated predictive uncertainty. (arXiv:2107.04296v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.04296</id>
        <link href="http://arxiv.org/abs/2107.04296"/>
        <updated>2021-08-05T01:56:21.693Z</updated>
        <summary type="html"><![CDATA[We show that differentially private stochastic gradient descent (DP-SGD) can
yield poorly calibrated, overconfident deep learning models. This represents a
serious issue for safety-critical applications, e.g. in medical diagnosis. We
highlight and exploit parallels between stochastic gradient Langevin dynamics,
a scalable Bayesian inference technique for training deep neural networks, and
DP-SGD, in order to train differentially private, Bayesian neural networks with
minor adjustments to the original (DP-SGD) algorithm. Our approach provides
considerably more reliable uncertainty estimates than DP-SGD, as demonstrated
empirically by a reduction in expected calibration error (MNIST $\sim{5}$-fold,
Pediatric Pneumonia Dataset $\sim{2}$-fold).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Knolle_M/0/1/0/all/0/1"&gt;Moritz Knolle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ziller_A/0/1/0/all/0/1"&gt;Alexander Ziller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Usynin_D/0/1/0/all/0/1"&gt;Dmitrii Usynin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Braren_R/0/1/0/all/0/1"&gt;Rickmer Braren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Makowski_M/0/1/0/all/0/1"&gt;Marcus R. Makowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rueckert_D/0/1/0/all/0/1"&gt;Daniel Rueckert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kaissis_G/0/1/0/all/0/1"&gt;Georgios Kaissis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Beyond Offline Mapping: Learning Cross Lingual Word Embeddings through Context Anchoring. (arXiv:2012.15715v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.15715</id>
        <link href="http://arxiv.org/abs/2012.15715"/>
        <updated>2021-08-05T01:56:21.672Z</updated>
        <summary type="html"><![CDATA[Recent research on cross-lingual word embeddings has been dominated by
unsupervised mapping approaches that align monolingual embeddings. Such methods
critically rely on those embeddings having a similar structure, but it was
recently shown that the separate training in different languages causes
departures from this assumption. In this paper, we propose an alternative
approach that does not have this limitation, while requiring a weak seed
dictionary (e.g., a list of identical words) as the only form of supervision.
Rather than aligning two fixed embedding spaces, our method works by fixing the
target language embeddings, and learning a new set of embeddings for the source
language that are aligned with them. To that end, we use an extension of
skip-gram that leverages translated context words as anchor points, and
incorporates self-learning and iterative restarts to reduce the dependency on
the initial dictionary. Our approach outperforms conventional mapping methods
on bilingual lexicon induction, and obtains competitive results in the
downstream XNLI task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ormazabal_A/0/1/0/all/0/1"&gt;Aitor Ormazabal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Artetxe_M/0/1/0/all/0/1"&gt;Mikel Artetxe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soroa_A/0/1/0/all/0/1"&gt;Aitor Soroa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Labaka_G/0/1/0/all/0/1"&gt;Gorka Labaka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agirre_E/0/1/0/all/0/1"&gt;Eneko Agirre&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fast approximations of the Jeffreys divergence between univariate Gaussian mixture models via exponential polynomial densities. (arXiv:2107.05901v2 [cs.IT] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.05901</id>
        <link href="http://arxiv.org/abs/2107.05901"/>
        <updated>2021-08-05T01:56:21.665Z</updated>
        <summary type="html"><![CDATA[The Jeffreys divergence is a renown symmetrization of the statistical
Kullback-Leibler divergence which is often used in statistics, machine
learning, signal processing, and information sciences in general. Since the
Jeffreys divergence between the ubiquitous Gaussian Mixture Models are not
available in closed-form, many techniques with various pros and cons have been
proposed in the literature to either (i) estimate, (ii) approximate, or (iii)
lower and/or upper bound this divergence. In this work, we propose a simple yet
fast heuristic to approximate the Jeffreys divergence between two univariate
GMMs of arbitrary number of components. The heuristic relies on converting GMMs
into pairs of dually parameterized probability densities belonging to
exponential families. In particular, we consider Exponential-Polynomial
Densities, and design a goodness-of-fit criterion to measure the dissimilarity
between a GMM and a EPD which is a generalization of the Hyv\"arinen
divergence. This criterion allows one to select the orders of the EPDs to
approximate the GMMs. We demonstrate experimentally that the computational time
of our heuristic improves over the stochastic Monte Carlo estimation baseline
by several orders of magnitude while approximating reasonably well the Jeffreys
divergence, specially when the univariate mixtures have a small number of
modes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nielsen_F/0/1/0/all/0/1"&gt;Frank Nielsen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How Well do Feature Visualizations Support Causal Understanding of CNN Activations?. (arXiv:2106.12447v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.12447</id>
        <link href="http://arxiv.org/abs/2106.12447"/>
        <updated>2021-08-05T01:56:21.657Z</updated>
        <summary type="html"><![CDATA[One widely used approach towards understanding the inner workings of deep
convolutional neural networks is to visualize unit responses via activation
maximization. Feature visualizations via activation maximization are thought to
provide humans with precise information about the image features that cause a
unit to be activated. If this is indeed true, these synthetic images should
enable humans to predict the effect of an intervention, such as whether
occluding a certain patch of the image (say, a dog's head) changes a unit's
activation. Here, we test this hypothesis by asking humans to predict which of
two square occlusions causes a larger change to a unit's activation. Both a
large-scale crowdsourced experiment and measurements with experts show that on
average, the extremely activating feature visualizations by Olah et al. (2017)
indeed help humans on this task ($67 \pm 4\%$ accuracy; baseline performance
without any visualizations is $60 \pm 3\%$). However, they do not provide any
significant advantage over other visualizations (such as e.g. dataset samples),
which yield similar performance ($66 \pm 3\%$ to $67 \pm 3\%$ accuracy). Taken
together, we propose an objective psychophysical task to quantify the benefit
of unit-level interpretability methods for humans, and find no evidence that
feature visualizations provide humans with better "causal understanding" than
simple alternative visualizations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zimmermann_R/0/1/0/all/0/1"&gt;Roland S. Zimmermann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Borowski_J/0/1/0/all/0/1"&gt;Judy Borowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geirhos_R/0/1/0/all/0/1"&gt;Robert Geirhos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bethge_M/0/1/0/all/0/1"&gt;Matthias Bethge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wallis_T/0/1/0/all/0/1"&gt;Thomas S. A. Wallis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brendel_W/0/1/0/all/0/1"&gt;Wieland Brendel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Insta-RS: Instance-wise Randomized Smoothing for Improved Robustness and Accuracy. (arXiv:2103.04436v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.04436</id>
        <link href="http://arxiv.org/abs/2103.04436"/>
        <updated>2021-08-05T01:56:21.651Z</updated>
        <summary type="html"><![CDATA[Randomized smoothing (RS) is an effective and scalable technique for
constructing neural network classifiers that are certifiably robust to
adversarial perturbations. Most RS works focus on training a good base model
that boosts the certified robustness of the smoothed model. However, existing
RS techniques treat every data point the same, i.e., the variance of the
Gaussian noise used to form the smoothed model is preset and universal for all
training and test data. This preset and universal Gaussian noise variance is
suboptimal since different data points have different margins and the local
properties of the base model vary across the input examples. In this paper, we
examine the impact of customized handling of examples and propose Instance-wise
Randomized Smoothing (Insta-RS) -- a multiple-start search algorithm that
assigns customized Gaussian variances to test examples. We also design Insta-RS
Train -- a novel two-stage training algorithm that adaptively adjusts and
customizes the noise level of each training example for training a base model
that boosts the certified robustness of the instance-wise Gaussian smoothed
model. Through extensive experiments on CIFAR-10 and ImageNet, we show that our
method significantly enhances the average certified radius (ACR) as well as the
clean data accuracy compared to existing state-of-the-art provably robust
classifiers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Chen Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kong_K/0/1/0/all/0/1"&gt;Kezhi Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1"&gt;Peihong Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luque_J/0/1/0/all/0/1"&gt;Juan Luque&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1"&gt;Tom Goldstein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1"&gt;Furong Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Recommending Burgers based on Pizza Preferences: Addressing Data Sparsity with a Product of Experts. (arXiv:2104.12822v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.12822</id>
        <link href="http://arxiv.org/abs/2104.12822"/>
        <updated>2021-08-05T01:56:21.644Z</updated>
        <summary type="html"><![CDATA[In this paper, we describe a method to tackle data sparsity and create
recommendations in domains with limited knowledge about user preferences. We
expand the variational autoencoder collaborative filtering from a single-domain
to a multi-domain setting. The intuition is that user-item interactions in a
source domain can augment the recommendation quality in a target domain. The
intuition can be taken to its extreme, where, in a cross-domain setup, the user
history in a source domain is enough to generate high-quality recommendations
in a target one. We thus create a Product-of-Experts (POE) architecture for
recommendations that jointly models user-item interactions across multiple
domains. The method is resilient to missing data for one or more of the
domains, which is a situation often found in real life. We present results on
two widely-used datasets - Amazon and Yelp, which support the claim that
holistic user preference knowledge leads to better recommendations.
Surprisingly, we find that in some cases, a POE recommender that does not
access the target domain user representation can surpass a strong VAE
recommender baseline trained on the target domain.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Milenkoski_M/0/1/0/all/0/1"&gt;Martin Milenkoski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Antognini_D/0/1/0/all/0/1"&gt;Diego Antognini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Musat_C/0/1/0/all/0/1"&gt;Claudiu Musat&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MLPerf Tiny Benchmark. (arXiv:2106.07597v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.07597</id>
        <link href="http://arxiv.org/abs/2106.07597"/>
        <updated>2021-08-05T01:56:21.625Z</updated>
        <summary type="html"><![CDATA[Advancements in ultra-low-power tiny machine learning (TinyML) systems
promise to unlock an entirely new class of smart applications. However,
continued progress is limited by the lack of a widely accepted and easily
reproducible benchmark for these systems. To meet this need, we present MLPerf
Tiny, the first industry-standard benchmark suite for ultra-low-power tiny
machine learning systems. The benchmark suite is the collaborative effort of
more than 50 organizations from industry and academia and reflects the needs of
the community. MLPerf Tiny measures the accuracy, latency, and energy of
machine learning inference to properly evaluate the tradeoffs between systems.
Additionally, MLPerf Tiny implements a modular design that enables benchmark
submitters to show the benefits of their product, regardless of where it falls
on the ML deployment stack, in a fair and reproducible manner. The suite
features four benchmarks: keyword spotting, visual wake words, image
classification, and anomaly detection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Banbury_C/0/1/0/all/0/1"&gt;Colby Banbury&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reddi_V/0/1/0/all/0/1"&gt;Vijay Janapa Reddi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Torelli_P/0/1/0/all/0/1"&gt;Peter Torelli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Holleman_J/0/1/0/all/0/1"&gt;Jeremy Holleman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jeffries_N/0/1/0/all/0/1"&gt;Nat Jeffries&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kiraly_C/0/1/0/all/0/1"&gt;Csaba Kiraly&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Montino_P/0/1/0/all/0/1"&gt;Pietro Montino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kanter_D/0/1/0/all/0/1"&gt;David Kanter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ahmed_S/0/1/0/all/0/1"&gt;Sebastian Ahmed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pau_D/0/1/0/all/0/1"&gt;Danilo Pau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thakker_U/0/1/0/all/0/1"&gt;Urmish Thakker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Torrini_A/0/1/0/all/0/1"&gt;Antonio Torrini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Warden_P/0/1/0/all/0/1"&gt;Peter Warden&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cordaro_J/0/1/0/all/0/1"&gt;Jay Cordaro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guglielmo_G/0/1/0/all/0/1"&gt;Giuseppe Di Guglielmo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duarte_J/0/1/0/all/0/1"&gt;Javier Duarte&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gibellini_S/0/1/0/all/0/1"&gt;Stephen Gibellini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parekh_V/0/1/0/all/0/1"&gt;Videet Parekh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tran_H/0/1/0/all/0/1"&gt;Honson Tran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tran_N/0/1/0/all/0/1"&gt;Nhan Tran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wenxu_N/0/1/0/all/0/1"&gt;Niu Wenxu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xuesong_X/0/1/0/all/0/1"&gt;Xu Xuesong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Binary Matrix Factorisation and Completion via Integer Programming. (arXiv:2106.13434v2 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.13434</id>
        <link href="http://arxiv.org/abs/2106.13434"/>
        <updated>2021-08-05T01:56:21.618Z</updated>
        <summary type="html"><![CDATA[Binary matrix factorisation is an essential tool for identifying discrete
patterns in binary data. In this paper we consider the rank-k binary matrix
factorisation problem (k-BMF) under Boolean arithmetic: we are given an n x m
binary matrix X with possibly missing entries and need to find two binary
matrices A and B of dimension n x k and k x m respectively, which minimise the
distance between X and the Boolean product of A and B in the squared Frobenius
distance. We present a compact and two exponential size integer programs (IPs)
for k-BMF and show that the compact IP has a weak LP relaxation, while the
exponential size IPs have a stronger equivalent LP relaxation. We introduce a
new objective function, which differs from the traditional squared Frobenius
objective in attributing a weight to zero entries of the input matrix that is
proportional to the number of times the zero is erroneously covered in a rank-k
factorisation. For one of the exponential size IPs we describe a computational
approach based on column generation. Experimental results on synthetic and real
word datasets suggest that our integer programming approach is competitive
against available methods for k-BMF and provides accurate low-error
factorisations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Kovacs_R/0/1/0/all/0/1"&gt;Reka A. Kovacs&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Gunluk_O/0/1/0/all/0/1"&gt;Oktay Gunluk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Hauser_R/0/1/0/all/0/1"&gt;Raphael A. Hauser&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine Learning Characterization of Cancer Patients-Derived Extracellular Vesicles using Vibrational Spectroscopies. (arXiv:2107.10332v2 [q-bio.OT] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.10332</id>
        <link href="http://arxiv.org/abs/2107.10332"/>
        <updated>2021-08-05T01:56:21.612Z</updated>
        <summary type="html"><![CDATA[The early detection of cancer is a challenging problem in medicine. The blood
sera of cancer patients are enriched with heterogeneous secretory lipid bound
extracellular vesicles (EVs), which present a complex repertoire of information
and biomarkers, representing their cell of origin, that are being currently
studied in the field of liquid biopsy and cancer screening. Vibrational
spectroscopies provide non-invasive approaches for the assessment of structural
and biophysical properties in complex biological samples. In this study,
multiple Raman spectroscopy measurements were performed on the EVs extracted
from the blood sera of 9 patients consisting of four different cancer subtypes
(colorectal cancer, hepatocellular carcinoma, breast cancer and pancreatic
cancer) and five healthy patients (controls). FTIR(Fourier Transform Infrared)
spectroscopy measurements were performed as a complementary approach to Raman
analysis, on two of the four cancer subtypes.

The AdaBoost Random Forest Classifier, Decision Trees, and Support Vector
Machines (SVM) distinguished the baseline corrected Raman spectra of cancer EVs
from those of healthy controls (18 spectra) with a classification accuracy of
greater than 90% when reduced to a spectral frequency range of 1800 to 1940
inverse cm, and subjected to a 0.5 training/testing split. FTIR classification
accuracy on 14 spectra showed an 80% classification accuracy. Our findings
demonstrate that basic machine learning algorithms are powerful tools to
distinguish the complex vibrational spectra of cancer patient EVs from those of
healthy patients. These experimental methods hold promise as valid and
efficient liquid biopsy for machine intelligence-assisted early cancer
screening.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Uthamacumaran_A/0/1/0/all/0/1"&gt;Abicumaran Uthamacumaran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Elouatik_S/0/1/0/all/0/1"&gt;Samir Elouatik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Abdouh_M/0/1/0/all/0/1"&gt;Mohamed Abdouh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Berteau_Rainville_M/0/1/0/all/0/1"&gt;Michael Berteau-Rainville&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Gao_Z/0/1/0/all/0/1"&gt;Zhu- Hua Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Arena_G/0/1/0/all/0/1"&gt;Goffredo Arena&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Generalized Framework for Edge-preserving and Structure-preserving Image Smoothing. (arXiv:2107.07058v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.07058</id>
        <link href="http://arxiv.org/abs/2107.07058"/>
        <updated>2021-08-05T01:56:21.605Z</updated>
        <summary type="html"><![CDATA[Image smoothing is a fundamental procedure in applications of both computer
vision and graphics. The required smoothing properties can be different or even
contradictive among different tasks. Nevertheless, the inherent smoothing
nature of one smoothing operator is usually fixed and thus cannot meet the
various requirements of different applications. In this paper, we first
introduce the truncated Huber penalty function which shows strong flexibility
under different parameter settings. A generalized framework is then proposed
with the introduced truncated Huber penalty function. When combined with its
strong flexibility, our framework is able to achieve diverse smoothing natures
where contradictive smoothing behaviors can even be achieved. It can also yield
the smoothing behavior that can seldom be achieved by previous methods, and
superior performance is thus achieved in challenging cases. These together
enable our framework capable of a range of applications and able to outperform
the state-of-the-art approaches in several tasks, such as image detail
enhancement, clip-art compression artifacts removal, guided depth map
restoration, image texture removal, etc. In addition, an efficient numerical
solution is provided and its convergence is theoretically guaranteed even the
optimization framework is non-convex and non-smooth. A simple yet effective
approach is further proposed to reduce the computational cost of our method
while maintaining its performance. The effectiveness and superior performance
of our approach are validated through comprehensive experiments in a range of
applications. Our code is available at
https://github.com/wliusjtu/Generalized-Smoothing-Framework.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1"&gt;Pingping Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lei_Y/0/1/0/all/0/1"&gt;Yinjie Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1"&gt;Xiaolin Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jie Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ng_M/0/1/0/all/0/1"&gt;Michael Ng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How to Certify Machine Learning Based Safety-critical Systems? A Systematic Literature Review. (arXiv:2107.12045v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.12045</id>
        <link href="http://arxiv.org/abs/2107.12045"/>
        <updated>2021-08-05T01:56:21.587Z</updated>
        <summary type="html"><![CDATA[Context: Machine Learning (ML) has been at the heart of many innovations over
the past years. However, including it in so-called 'safety-critical' systems
such as automotive or aeronautic has proven to be very challenging, since the
shift in paradigm that ML brings completely changes traditional certification
approaches.

Objective: This paper aims to elucidate challenges related to the
certification of ML-based safety-critical systems, as well as the solutions
that are proposed in the literature to tackle them, answering the question 'How
to Certify Machine Learning Based Safety-critical Systems?'.

Method: We conduct a Systematic Literature Review (SLR) of research papers
published between 2015 to 2020, covering topics related to the certification of
ML systems. In total, we identified 217 papers covering topics considered to be
the main pillars of ML certification: Robustness, Uncertainty, Explainability,
Verification, Safe Reinforcement Learning, and Direct Certification. We
analyzed the main trends and problems of each sub-field and provided summaries
of the papers extracted.

Results: The SLR results highlighted the enthusiasm of the community for this
subject, as well as the lack of diversity in terms of datasets and type of
models. It also emphasized the need to further develop connections between
academia and industries to deepen the domain study. Finally, it also
illustrated the necessity to build connections between the above mention main
pillars that are for now mainly studied separately.

Conclusion: We highlighted current efforts deployed to enable the
certification of ML based software systems, and discuss some future research
directions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tambon_F/0/1/0/all/0/1"&gt;Florian Tambon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Laberge_G/0/1/0/all/0/1"&gt;Gabriel Laberge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+An_L/0/1/0/all/0/1"&gt;Le An&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nikanjam_A/0/1/0/all/0/1"&gt;Amin Nikanjam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mindom_P/0/1/0/all/0/1"&gt;Paulina Stevia Nouwou Mindom&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pequignot_Y/0/1/0/all/0/1"&gt;Yann Pequignot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khomh_F/0/1/0/all/0/1"&gt;Foutse Khomh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Antoniol_G/0/1/0/all/0/1"&gt;Giulio Antoniol&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Merlo_E/0/1/0/all/0/1"&gt;Ettore Merlo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Laviolette_F/0/1/0/all/0/1"&gt;Fran&amp;#xe7;ois Laviolette&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On Generalization of Graph Autoencoders with Adversarial Training. (arXiv:2107.02658v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.02658</id>
        <link href="http://arxiv.org/abs/2107.02658"/>
        <updated>2021-08-05T01:56:21.579Z</updated>
        <summary type="html"><![CDATA[Adversarial training is an approach for increasing model's resilience against
adversarial perturbations. Such approaches have been demonstrated to result in
models with feature representations that generalize better. However, limited
works have been done on adversarial training of models on graph data. In this
paper, we raise such a question { does adversarial training improve the
generalization of graph representations. We formulate L2 and L1 versions of
adversarial training in two powerful node embedding methods: graph autoencoder
(GAE) and variational graph autoencoder (VGAE). We conduct extensive
experiments on three main applications, i.e. link prediction, node clustering,
graph anomaly detection of GAE and VGAE, and demonstrate that both L2 and L1
adversarial training boost the generalization of GAE and VGAE.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1"&gt;Tianjin Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pei_Y/0/1/0/all/0/1"&gt;Yulong Pei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Menkovski_V/0/1/0/all/0/1"&gt;Vlado Menkovski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pechenizkiy_M/0/1/0/all/0/1"&gt;Mykola Pechenizkiy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An empirical evaluation of active inference in multi-armed bandits. (arXiv:2101.08699v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.08699</id>
        <link href="http://arxiv.org/abs/2101.08699"/>
        <updated>2021-08-05T01:56:21.562Z</updated>
        <summary type="html"><![CDATA[A key feature of sequential decision making under uncertainty is a need to
balance between exploiting--choosing the best action according to the current
knowledge, and exploring--obtaining information about values of other actions.
The multi-armed bandit problem, a classical task that captures this trade-off,
served as a vehicle in machine learning for developing bandit algorithms that
proved to be useful in numerous industrial applications. The active inference
framework, an approach to sequential decision making recently developed in
neuroscience for understanding human and animal behaviour, is distinguished by
its sophisticated strategy for resolving the exploration-exploitation
trade-off. This makes active inference an exciting alternative to already
established bandit algorithms. Here we derive an efficient and scalable
approximate active inference algorithm and compare it to two state-of-the-art
bandit algorithms: Bayesian upper confidence bound and optimistic Thompson
sampling. This comparison is done on two types of bandit problems: a stationary
and a dynamic switching bandit. Our empirical evaluation shows that the active
inference algorithm does not produce efficient long-term behaviour in
stationary bandits. However, in the more challenging switching bandit problem
active inference performs substantially better than the two state-of-the-art
bandit algorithms. The results open exciting venues for further research in
theoretical and applied machine learning, as well as lend additional
credibility to active inference as a general framework for studying human and
animal behaviour.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Markovic_D/0/1/0/all/0/1"&gt;Dimitrije Markovic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stojic_H/0/1/0/all/0/1"&gt;Hrvoje Stojic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schwoebel_S/0/1/0/all/0/1"&gt;Sarah Schwoebel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kiebel_S/0/1/0/all/0/1"&gt;Stefan J. Kiebel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Linking Sap Flow Measurements with Earth Observations. (arXiv:2108.01290v1 [stat.AP] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2108.01290</id>
        <link href="http://arxiv.org/abs/2108.01290"/>
        <updated>2021-08-05T01:56:21.555Z</updated>
        <summary type="html"><![CDATA[While single-tree transpiration is challenging to compare with earth
observation, canopy scale data are suitable for this purpose. To test the
potentialities of the second approach, we equipped the trees at two measurement
sites with sap flow sensors in spruce forests. The sites have contrasting
topography. The measurement period covered the months between June 2020 and
January 2021. To link plot scale transpiration with earth observations, we
utilized Sentinel-2 and local meteorological data. Within a machine learning
framework, we have tested the suitability of earth observations for modelling
canopy transpiration. The R2 of the cross-validated trained models at the
measurement sites was between 0.57 and 0.80. These results demonstrate the
relevance of Sentinel-2 data for the data-driven upscaling of ecosystem fluxes
from plot scale sap flow data. If applied to a broader network of sites and
climatic conditions, such an approach could offer unprecedented possibilities
for investigating our forests' resilience and resistance capacity to an
intensified hydrological cycle in the contest of a changing climate.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Tomelleri_E/0/1/0/all/0/1"&gt;Enrico Tomelleri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Tonon_G/0/1/0/all/0/1"&gt;Giustino Tonon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Auto-Pipeline: Synthesizing Complex Data Pipelines By-Target Using Reinforcement Learning and Search. (arXiv:2106.13861v2 [cs.DB] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.13861</id>
        <link href="http://arxiv.org/abs/2106.13861"/>
        <updated>2021-08-05T01:56:21.536Z</updated>
        <summary type="html"><![CDATA[Recent work has made significant progress in helping users to automate single
data preparation steps, such as string-transformations and table-manipulation
operators (e.g., Join, GroupBy, Pivot, etc.). We in this work propose to
automate multiple such steps end-to-end, by synthesizing complex data pipelines
with both string transformations and table-manipulation operators. We propose a
novel "by-target" paradigm that allows users to easily specify the desired
pipeline, which is a significant departure from the traditional by-example
paradigm. Using by-target, users would provide input tables (e.g., csv or json
files), and point us to a "target table" (e.g., an existing database table or
BI dashboard) to demonstrate how the output from the desired pipeline would
schematically "look like". While the problem is seemingly underspecified, our
unique insight is that implicit table constraints such as FDs and keys can be
exploited to significantly constrain the space to make the problem tractable.
We develop an Auto-Pipeline system that learns to synthesize pipelines using
reinforcement learning and search. Experiments on large numbers of real
pipelines crawled from GitHub suggest that Auto-Pipeline can successfully
synthesize 60-70% of these complex pipelines with up to 10 steps.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Junwen Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1"&gt;Yeye He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chaudhuri_S/0/1/0/all/0/1"&gt;Surajit Chaudhuri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ensemble of MRR and NDCG models for Visual Dialog. (arXiv:2104.07511v3 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.07511</id>
        <link href="http://arxiv.org/abs/2104.07511"/>
        <updated>2021-08-05T01:56:21.490Z</updated>
        <summary type="html"><![CDATA[Assessing an AI agent that can converse in human language and understand
visual content is challenging. Generation metrics, such as BLEU scores favor
correct syntax over semantics. Hence a discriminative approach is often used,
where an agent ranks a set of candidate options. The mean reciprocal rank (MRR)
metric evaluates the model performance by taking into account the rank of a
single human-derived answer. This approach, however, raises a new challenge:
the ambiguity and synonymy of answers, for instance, semantic equivalence
(e.g., `yeah' and `yes'). To address this, the normalized discounted cumulative
gain (NDCG) metric has been used to capture the relevance of all the correct
answers via dense annotations. However, the NDCG metric favors the usually
applicable uncertain answers such as `I don't know. Crafting a model that
excels on both MRR and NDCG metrics is challenging. Ideally, an AI agent should
answer a human-like reply and validate the correctness of any answer. To
address this issue, we describe a two-step non-parametric ranking approach that
can merge strong MRR and NDCG models. Using our approach, we manage to keep
most MRR state-of-the-art performance (70.41% vs. 71.24%) and the NDCG
state-of-the-art performance (72.16% vs. 75.35%). Moreover, our approach won
the recent Visual Dialog 2020 challenge. Source code is available at
https://github.com/idansc/mrr-ndcg.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schwartz_I/0/1/0/all/0/1"&gt;Idan Schwartz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Explaining COVID-19 and Thoracic Pathology Model Predictions by Identifying Informative Input Features. (arXiv:2104.00411v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.00411</id>
        <link href="http://arxiv.org/abs/2104.00411"/>
        <updated>2021-08-05T01:56:21.474Z</updated>
        <summary type="html"><![CDATA[Neural networks have demonstrated remarkable performance in classification
and regression tasks on chest X-rays. In order to establish trust in the
clinical routine, the networks' prediction mechanism needs to be interpretable.
One principal approach to interpretation is feature attribution. Feature
attribution methods identify the importance of input features for the output
prediction. Building on Information Bottleneck Attribution (IBA) method, for
each prediction we identify the chest X-ray regions that have high mutual
information with the network's output. Original IBA identifies input regions
that have sufficient predictive information. We propose Inverse IBA to identify
all informative regions. Thus all predictive cues for pathologies are
highlighted on the X-rays, a desirable property for chest X-ray diagnosis.
Moreover, we propose Regression IBA for explaining regression models. Using
Regression IBA we observe that a model trained on cumulative severity score
labels implicitly learns the severity of different X-ray regions. Finally, we
propose Multi-layer IBA to generate higher resolution and more detailed
attribution/saliency maps. We evaluate our methods using both human-centric
(ground-truth-based) interpretability metrics, and human-independent feature
importance metrics on NIH Chest X-ray8 and BrixIA datasets. The Code is
publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Khakzar_A/0/1/0/all/0/1"&gt;Ashkan Khakzar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mansour_W/0/1/0/all/0/1"&gt;Wejdene Mansour&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cai_Y/0/1/0/all/0/1"&gt;Yuezhi Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yawei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yucheng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kim_S/0/1/0/all/0/1"&gt;Seong Tae Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Navab_N/0/1/0/all/0/1"&gt;Nassir Navab&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Semantic Interpretation of Thoracic Disease and COVID-19 Diagnosis Models. (arXiv:2104.02481v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.02481</id>
        <link href="http://arxiv.org/abs/2104.02481"/>
        <updated>2021-08-05T01:56:21.451Z</updated>
        <summary type="html"><![CDATA[Convolutional neural networks are showing promise in the automatic diagnosis
of thoracic pathologies on chest x-rays. Their black-box nature has sparked
many recent works to explain the prediction via input feature attribution
methods (aka saliency methods). However, input feature attribution methods
merely identify the importance of input regions for the prediction and lack
semantic interpretation of model behavior. In this work, we first identify the
semantics associated with internal units (feature maps) of the network. We
proceed to investigate the following questions; Does a regression model that is
only trained with COVID-19 severity scores implicitly learn visual patterns
associated with thoracic pathologies? Does a network that is trained on weakly
labeled data (e.g. healthy, unhealthy) implicitly learn pathologies? Moreover,
we investigate the effect of pretraining and data imbalance on the
interpretability of learned features. In addition to the analysis, we propose
semantic attribution to semantically explain each prediction. We present our
findings using publicly available chest pathologies (CheXpert, NIH ChestX-ray8)
and COVID-19 datasets (BrixIA, and COVID-19 chest X-ray segmentation dataset).
The Code is publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Khakzar_A/0/1/0/all/0/1"&gt;Ashkan Khakzar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Musatian_S/0/1/0/all/0/1"&gt;Sabrina Musatian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Buchberger_J/0/1/0/all/0/1"&gt;Jonas Buchberger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Quiroz_I/0/1/0/all/0/1"&gt;Icxel Valeriano Quiroz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Pinger_N/0/1/0/all/0/1"&gt;Nikolaus Pinger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Baselizadeh_S/0/1/0/all/0/1"&gt;Soroosh Baselizadeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kim_S/0/1/0/all/0/1"&gt;Seong Tae Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Navab_N/0/1/0/all/0/1"&gt;Nassir Navab&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GANterfactual -- Counterfactual Explanations for Medical Non-Experts using Generative Adversarial Learning. (arXiv:2012.11905v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.11905</id>
        <link href="http://arxiv.org/abs/2012.11905"/>
        <updated>2021-08-05T01:56:21.431Z</updated>
        <summary type="html"><![CDATA[With the ongoing rise of machine learning, the need for methods for
explaining decisions made by artificial intelligence systems is becoming a more
and more important topic. Especially for image classification tasks, many
state-of-the-art tools to explain such classifiers rely on visual highlighting
of important areas of the input data. Contrary, counterfactual explanation
systems try to enable a counterfactual reasoning by modifying the input image
in a way such that the classifier would have made a different prediction. By
doing so, the users of counterfactual explanation systems are equipped with a
completely different kind of explanatory information. However, methods for
generating realistic counterfactual explanations for image classifiers are
still rare. Especially in medical contexts, where relevant information often
consists of textural and structural information, high-quality counterfactual
images have the potential to give meaningful insights into decision processes.
In this work, we present GANterfactual, an approach to generate such
counterfactual image explanations based on adversarial image-to-image
translation techniques. Additionally, we conduct a user study to evaluate our
approach in an exemplary medical use case. Our results show that, in the chosen
medical use-case, counterfactual explanations lead to significantly better
results regarding mental models, explanation satisfaction, trust, emotions, and
self-efficacy than two state-of-the-art systems that work with saliency maps,
namely LIME and LRP.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mertes_S/0/1/0/all/0/1"&gt;Silvan Mertes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huber_T/0/1/0/all/0/1"&gt;Tobias Huber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weitz_K/0/1/0/all/0/1"&gt;Katharina Weitz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heimerl_A/0/1/0/all/0/1"&gt;Alexander Heimerl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Andre_E/0/1/0/all/0/1"&gt;Elisabeth Andr&amp;#xe9;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Modeling Online Behavior in Recommender Systems: The Importance of Temporal Context. (arXiv:2009.08978v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.08978</id>
        <link href="http://arxiv.org/abs/2009.08978"/>
        <updated>2021-08-05T01:56:21.422Z</updated>
        <summary type="html"><![CDATA[Recommender systems research tends to evaluate model performance offline and
on randomly sampled targets, yet the same systems are later used to predict
user behavior sequentially from a fixed point in time. Simulating online
recommender system performance is notoriously difficult and the discrepancy
between online and offline behaviors is typically not accounted for in offline
evaluations. This disparity permits weaknesses to go unnoticed until the model
is deployed in a production setting. In this paper, we first demonstrate how
omitting temporal context when evaluating recommender system performance leads
to false confidence. To overcome this, we postulate that offline evaluation
protocols can only model real-life use-cases if they account for temporal
context. Next, we propose a training procedure to further embed the temporal
context in existing models: we introduce it in a multi-objective approach to
traditionally time-unaware recommender systems and confirm its advantage via
the proposed evaluation protocol. Finally, we validate that the Pareto Fronts
obtained with the added objective dominate those produced by state-of-the-art
models that are only optimized for accuracy on three real-world publicly
available datasets. The results show that including our temporal objective can
improve recall@20 by up to 20%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Filipovic_M/0/1/0/all/0/1"&gt;Milena Filipovic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mitrevski_B/0/1/0/all/0/1"&gt;Blagoj Mitrevski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Antognini_D/0/1/0/all/0/1"&gt;Diego Antognini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Glaude_E/0/1/0/all/0/1"&gt;Emma Lejal Glaude&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Faltings_B/0/1/0/all/0/1"&gt;Boi Faltings&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Musat_C/0/1/0/all/0/1"&gt;Claudiu Musat&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spacetime Neural Network for High Dimensional Quantum Dynamics. (arXiv:2108.02200v1 [cond-mat.dis-nn])]]></title>
        <id>http://arxiv.org/abs/2108.02200</id>
        <link href="http://arxiv.org/abs/2108.02200"/>
        <updated>2021-08-05T01:56:21.405Z</updated>
        <summary type="html"><![CDATA[We develop a spacetime neural network method with second order optimization
for solving quantum dynamics from the high dimensional Schr\"{o}dinger
equation. In contrast to the standard iterative first order optimization and
the time-dependent variational principle, our approach utilizes the implicit
mid-point method and generates the solution for all spatial and temporal values
simultaneously after optimization. We demonstrate the method in the
Schr\"{o}dinger equation with a self-normalized autoregressive spacetime neural
network construction. Future explorations for solving different high
dimensional differential equations are discussed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cond-mat/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jiangran Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhuo Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Luo_D/0/1/0/all/0/1"&gt;Di Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Zhao_Z/0/1/0/all/0/1"&gt;Zhizhen Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Hur_V/0/1/0/all/0/1"&gt;Vera Mikyoung Hur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Clark_B/0/1/0/all/0/1"&gt;Bryan K. Clark&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Safe Reinforcement Learning with Natural Language Constraints. (arXiv:2010.05150v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.05150</id>
        <link href="http://arxiv.org/abs/2010.05150"/>
        <updated>2021-08-05T01:56:21.399Z</updated>
        <summary type="html"><![CDATA[While safe reinforcement learning (RL) holds great promise for many practical
applications like robotics or autonomous cars, current approaches require
specifying constraints in mathematical form. Such specifications demand domain
expertise, limiting the adoption of safe RL. In this paper, we propose learning
to interpret natural language constraints for safe RL. To this end, we first
introduce HazardWorld, a new multi-task benchmark that requires an agent to
optimize reward while not violating constraints specified in free-form text. We
then develop an agent with a modular architecture that can interpret and adhere
to such textual constraints while learning new tasks. Our model consists of (1)
a constraint interpreter that encodes textual constraints into spatial and
temporal representations of forbidden states, and (2) a policy network that
uses these representations to produce a policy achieving minimal constraint
violations during training. Across different domains in HazardWorld, we show
that our method achieves higher rewards (up to11x) and fewer constraint
violations (by 1.8x) compared to existing approaches. However, in terms of
absolute performance, HazardWorld still poses significant challenges for agents
to learn efficiently, motivating the need for future work.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1"&gt;Tsung-Yen Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_M/0/1/0/all/0/1"&gt;Michael Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chow_Y/0/1/0/all/0/1"&gt;Yinlam Chow&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramadge_P/0/1/0/all/0/1"&gt;Peter J. Ramadge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Narasimhan_K/0/1/0/all/0/1"&gt;Karthik Narasimhan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Random Offset Block Embedding Array (ROBE) for CriteoTB Benchmark MLPerf DLRM Model : 1000$\times$ Compression and 2.7$\times$ Faster Inference. (arXiv:2108.02191v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2108.02191</id>
        <link href="http://arxiv.org/abs/2108.02191"/>
        <updated>2021-08-05T01:56:21.391Z</updated>
        <summary type="html"><![CDATA[Deep learning for recommendation data is the one of the most pervasive and
challenging AI workload in recent times. State-of-the-art recommendation models
are one of the largest models rivalling the likes of GPT-3 and Switch
Transformer. Challenges in deep learning recommendation models (DLRM) stem from
learning dense embeddings for each of the categorical values. These embedding
tables in industrial scale models can be as large as hundreds of terabytes.
Such large models lead to a plethora of engineering challenges, not to mention
prohibitive communication overheads, and slower training and inference times.
Of these, slower inference time directly impacts user experience. Model
compression for DLRM is gaining traction and the community has recently shown
impressive compression results. In this paper, we present Random Offset Block
Embedding Array (ROBE) as a low memory alternative to embedding tables which
provide orders of magnitude reduction in memory usage while maintaining
accuracy and boosting execution speed. ROBE is a simple fundamental approach in
improving both cache performance and the variance of randomized hashing, which
could be of independent interest in itself. We demonstrate that we can
successfully train DLRM models with same accuracy while using $1000 \times$
less memory. A $1000\times$ compressed model directly results in faster
inference without any engineering. In particular, we show that we can train
DLRM model using ROBE Array of size 100MB on a single GPU to achieve AUC of
0.8025 or higher as required by official MLPerf CriteoTB benchmark DLRM model
of 100GB while achieving about $2.7\times$ (170\%) improvement in inference
throughput.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Desai_A/0/1/0/all/0/1"&gt;Aditya Desai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chou_L/0/1/0/all/0/1"&gt;Li Chou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1"&gt;Anshumali Shrivastava&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generalized Matrix Factorization. (arXiv:2010.02469v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.02469</id>
        <link href="http://arxiv.org/abs/2010.02469"/>
        <updated>2021-08-05T01:56:21.384Z</updated>
        <summary type="html"><![CDATA[Unmeasured or latent variables are often the cause of correlations between
multivariate measurements and are studied in a variety of fields such as
psychology, ecology, and medicine. For Gaussian measurements, there are
classical tools such as factor analysis or principal component analysis with a
well-established theory and fast algorithms. Generalized Linear Latent Variable
models (GLLVM) generalize such factor models to non-Gaussian responses.
However, current algorithms for estimating model parameters in GLLVMs require
intensive computation and do not scale to large datasets with thousands of
observational units or responses. In this article, we propose a new approach
for fitting GLLVMs to such high-volume, high-dimensional datasets. We
approximate the likelihood using penalized quasi-likelihood and use a Newton
method and Fisher scoring to learn the model parameters. Our method greatly
reduces the computation time and can be easily parallelized, enabling
factorization at unprecedented scale using commodity hardware. We illustrate
application of our method on a dataset of 48,000 observational units with over
2,000 observed species in each unit, finding that most of the variability can
be explained with a handful of factors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kidzinski_L/0/1/0/all/0/1"&gt;&amp;#x141;ukasz Kidzi&amp;#x144;ski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hui_F/0/1/0/all/0/1"&gt;Francis K.C. Hui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Warton_D/0/1/0/all/0/1"&gt;David I. Warton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hastie_T/0/1/0/all/0/1"&gt;Trevor Hastie&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Modeling longitudinal data using matrix completion. (arXiv:1809.08771v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1809.08771</id>
        <link href="http://arxiv.org/abs/1809.08771"/>
        <updated>2021-08-05T01:56:21.372Z</updated>
        <summary type="html"><![CDATA[In clinical practice and biomedical research, measurements are often
collected sparsely and irregularly in time while the data acquisition is
expensive and inconvenient. Examples include measurements of spine bone mineral
density, cancer growth through mammography or biopsy, a progression of
defective vision, or assessment of gait in patients with neurological
disorders. Since the data collection is often costly and inconvenient,
estimation of progression from sparse observations is of great interest for
practitioners.

From the statistical standpoint, such data is often analyzed in the context
of a mixed-effect model where time is treated as both a fixed-effect
(population progression curve) and a random-effect (individual variability).
Alternatively, researchers analyze Gaussian processes or functional data where
observations are assumed to be drawn from a certain distribution of processes.
These models are flexible but rely on probabilistic assumptions, require very
careful implementation, specific to the given problem, and tend to be slow in
practice.

In this study, we propose an alternative elementary framework for analyzing
longitudinal data, relying on matrix completion. Our method yields estimates of
progression curves by iterative application of the Singular Value
Decomposition. Our framework covers multivariate longitudinal data, regression,
and can be easily extended to other settings. As it relies on existing tools
for matrix algebra it is efficient and easy to implement.

We apply our methods to understand trends of progression of motor impairment
in children with Cerebral Palsy. Our model approximates individual progression
curves and explains 30% of the variability. Low-rank representation of
progression trends enables identification of different progression trends in
subtypes of Cerebral Palsy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Kidzinski_L/0/1/0/all/0/1"&gt;&amp;#x141;ukasz Kidzi&amp;#x144;ski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Hastie_T/0/1/0/all/0/1"&gt;Trevor Hastie&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TERA: Self-Supervised Learning of Transformer Encoder Representation for Speech. (arXiv:2007.06028v3 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.06028</id>
        <link href="http://arxiv.org/abs/2007.06028"/>
        <updated>2021-08-05T01:56:21.362Z</updated>
        <summary type="html"><![CDATA[We introduce a self-supervised speech pre-training method called TERA, which
stands for Transformer Encoder Representations from Alteration. Recent
approaches often learn by using a single auxiliary task like contrastive
prediction, autoregressive prediction, or masked reconstruction. Unlike
previous methods, we use alteration along three orthogonal axes to pre-train
Transformer Encoders on a large amount of unlabeled speech. The model learns
through the reconstruction of acoustic frames from their altered counterpart,
where we use a stochastic policy to alter along various dimensions: time,
frequency, and magnitude. TERA can be used for speech representations
extraction or fine-tuning with downstream models. We evaluate TERA on several
downstream tasks, including phoneme classification, keyword spotting, speaker
recognition, and speech recognition. We present a large-scale comparison of
various self-supervised models. TERA achieves strong performance in the
comparison by improving upon surface features and outperforming previous
models. In our experiments, we study the effect of applying different
alteration techniques, pre-training on more data, and pre-training on various
features. We analyze different model sizes and find that smaller models are
strong representation learners than larger models, while larger models are more
effective for downstream fine-tuning than smaller models. Furthermore, we show
the proposed method is transferable to downstream datasets not used in
pre-training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Liu_A/0/1/0/all/0/1"&gt;Andy T. Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_S/0/1/0/all/0/1"&gt;Shang-Wen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lee_H/0/1/0/all/0/1"&gt;Hung-yi Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Reward Functions from Diverse Sources of Human Feedback: Optimally Integrating Demonstrations and Preferences. (arXiv:2006.14091v2 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.14091</id>
        <link href="http://arxiv.org/abs/2006.14091"/>
        <updated>2021-08-05T01:56:21.343Z</updated>
        <summary type="html"><![CDATA[Reward functions are a common way to specify the objective of a robot. As
designing reward functions can be extremely challenging, a more promising
approach is to directly learn reward functions from human teachers.
Importantly, data from human teachers can be collected either passively or
actively in a variety of forms: passive data sources include demonstrations,
(e.g., kinesthetic guidance), whereas preferences (e.g., comparative rankings)
are actively elicited. Prior research has independently applied reward learning
to these different data sources. However, there exist many domains where
multiple sources are complementary and expressive. Motivated by this general
problem, we present a framework to integrate multiple sources of information,
which are either passively or actively collected from human users. In
particular, we present an algorithm that first utilizes user demonstrations to
initialize a belief about the reward function, and then actively probes the
user with preference queries to zero-in on their true reward. This algorithm
not only enables us combine multiple data sources, but it also informs the
robot when it should leverage each type of information. Further, our approach
accounts for the human's ability to provide data: yielding user-friendly
preference queries which are also theoretically optimal. Our extensive
simulated experiments and user studies on a Fetch mobile manipulator
demonstrate the superiority and the usability of our integrated framework.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Biyik_E/0/1/0/all/0/1"&gt;Erdem B&amp;#x131;y&amp;#x131;k&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Losey_D/0/1/0/all/0/1"&gt;Dylan P. Losey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Palan_M/0/1/0/all/0/1"&gt;Malayandi Palan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Landolfi_N/0/1/0/all/0/1"&gt;Nicholas C. Landolfi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shevchuk_G/0/1/0/all/0/1"&gt;Gleb Shevchuk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sadigh_D/0/1/0/all/0/1"&gt;Dorsa Sadigh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Lightweight Music Texture Transfer System. (arXiv:1810.01248v3 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1810.01248</id>
        <link href="http://arxiv.org/abs/1810.01248"/>
        <updated>2021-08-05T01:56:21.336Z</updated>
        <summary type="html"><![CDATA[Deep learning researches on the transformation problems for image and text
have raised great attention. However, present methods for music feature
transfer using neural networks are far from practical application. In this
paper, we initiate a novel system for transferring the texture of music, and
release it as an open source project. Its core algorithm is composed of a
converter which represents sounds as texture spectra, a corresponding
reconstructor and a feed-forward transfer network. We evaluate this system from
multiple perspectives, and experimental results reveal that it achieves
convincing results in both sound effects and computational performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1"&gt;Xutan Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1"&gt;Zhi Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_F/0/1/0/all/0/1"&gt;Faqiang Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yidan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jianxin Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Potential of Using Vision Videos for CrowdRE: Video Comments as a Source of Feedback. (arXiv:2108.02076v1 [cs.SE])]]></title>
        <id>http://arxiv.org/abs/2108.02076</id>
        <link href="http://arxiv.org/abs/2108.02076"/>
        <updated>2021-08-05T01:56:21.329Z</updated>
        <summary type="html"><![CDATA[Vision videos are established for soliciting feedback and stimulating
discussions in requirements engineering (RE) practices, such as focus groups.
Different researchers motivated the transfer of these benefits into crowd-based
RE (CrowdRE) by using vision videos on social media platforms. So far, however,
little research explored the potential of using vision videos for CrowdRE in
detail. In this paper, we analyze and assess this potential, in particular,
focusing on video comments as a source of feedback. In a case study, we
analyzed 4505 comments on a vision video from YouTube. We found that the video
solicited 2770 comments from 2660 viewers in four days. This is more than 50%
of all comments the video received in four years. Even though only a certain
fraction of these comments are relevant to RE, the relevant comments address
typical intentions and topics of user feedback, such as feature request or
problem report. Besides the typical user feedback categories, we found more
than 300 comments that address the topic safety, which has not appeared in
previous analyses of user feedback. In an automated analysis, we compared the
performance of three machine learning algorithms on classifying the video
comments. Despite certain differences, the algorithms classified the video
comments well. Based on these findings, we conclude that the use of vision
videos for CrowdRE has a large potential. Despite the preliminary nature of the
case study, we are optimistic that vision videos can motivate stakeholders to
actively participate in a crowd and solicit numerous of video comments as a
valuable source of feedback.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Karras_O/0/1/0/all/0/1"&gt;Oliver Karras&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kristo_E/0/1/0/all/0/1"&gt;Eklekta Kristo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Klunder_J/0/1/0/all/0/1"&gt;Jil Kl&amp;#xfc;nder&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Relevance Attack on Detectors. (arXiv:2008.06822v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.06822</id>
        <link href="http://arxiv.org/abs/2008.06822"/>
        <updated>2021-08-05T01:56:21.322Z</updated>
        <summary type="html"><![CDATA[This paper focuses on high-transferable adversarial attacks on detectors,
which are hard to attack in a black-box manner, because of their
multiple-output characteristics and the diversity across architectures. To
pursue a high attack transferability, one plausible way is to find a common
property across detectors, which facilitates the discovery of common
weaknesses. We are the first to suggest that the relevance map from
interpreters for detectors is such a property. Based on it, we design a
Relevance Attack on Detectors (RAD), which achieves a state-of-the-art
transferability, exceeding existing results by above 20%. On MS COCO, the
detection mAPs for all 8 black-box architectures are more than halved and the
segmentation mAPs are also significantly influenced. Given the great
transferability of RAD, we generate the first adversarial dataset for object
detection and instance segmentation, i.e., Adversarial Objects in COntext
(AOCO), which helps to quickly evaluate and improve the robustness of
detectors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Sizhe Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_F/0/1/0/all/0/1"&gt;Fan He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1"&gt;Xiaolin Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1"&gt;Kun Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Physical Hard-Label Attacks on Deep Learning Visual Classification. (arXiv:2002.07088v4 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.07088</id>
        <link href="http://arxiv.org/abs/2002.07088"/>
        <updated>2021-08-05T01:56:21.306Z</updated>
        <summary type="html"><![CDATA[The physical, black-box hard-label setting is arguably the most realistic
threat model for cyber-physical vision systems. In this setting, the attacker
only has query access to the model and only receives the top-1 class label
without confidence information. Creating small physical stickers that are
robust to environmental variation is difficult in the discrete and
discontinuous hard-label space because the attack must both design a small
shape to perturb within and find robust noise to fill it with. Unfortunately,
we find that existing $\ell_2$ or $\ell_\infty$ minimizing hard-label attacks
do not easily extend to finding such robust physical perturbation attacks.
Thus, we propose GRAPHITE, the first algorithm for hard-label physical attacks
on computer vision models. We show that "survivability", an estimate of
physical variation robustness, can be used in new ways to generate small masks
and is a sufficiently smooth function to optimize with gradient-free
optimization. We use GRAPHITE to attack a traffic sign classifier and a
publicly-available Automatic License Plate Recognition (ALPR) tool using only
query access. We evaluate both tools in real-world field tests to measure its
physical-world robustness. We successfully cause a Stop sign to be
misclassified as a Speed Limit 30 km/hr sign in 95.7% of physical images and
cause errors in 75% of physical images for the ALPR tool.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Feng_R/0/1/0/all/0/1"&gt;Ryan Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jiefeng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fernandes_E/0/1/0/all/0/1"&gt;Earlence Fernandes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jha_S/0/1/0/all/0/1"&gt;Somesh Jha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prakash_A/0/1/0/all/0/1"&gt;Atul Prakash&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Under the Radar -- Auditing Fairness in ML for Humanitarian Mapping. (arXiv:2108.02137v1 [cs.CY])]]></title>
        <id>http://arxiv.org/abs/2108.02137</id>
        <link href="http://arxiv.org/abs/2108.02137"/>
        <updated>2021-08-05T01:56:21.296Z</updated>
        <summary type="html"><![CDATA[Humanitarian mapping from space with machine learning helps policy-makers to
timely and accurately identify people in need. However, recent concerns around
fairness and transparency of algorithmic decision-making are a significant
obstacle for applying these methods in practice. In this paper, we study if
humanitarian mapping approaches from space are prone to bias in their
predictions. We map village-level poverty and electricity rates in India based
on nighttime lights (NTLs) with linear regression and random forest and analyze
if the predictions systematically show prejudice against scheduled caste or
tribe communities. To achieve this, we design a causal approach to measure
counterfactual fairness based on propensity score matching. This allows to
compare villages within a community of interest to synthetic counterfactuals.
Our findings indicate that poverty is systematically overestimated and
electricity systematically underestimated for scheduled tribes in comparison to
a synthetic counterfactual group of villages. The effects have the opposite
direction for scheduled castes where poverty is underestimated and
electrification overestimated. These results are a warning sign for a variety
of applications in humanitarian mapping where fairness issues would compromise
policy goals.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kondmann_L/0/1/0/all/0/1"&gt;Lukas Kondmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1"&gt;Xiao Xiang Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semi-weakly Supervised Contrastive Representation Learning for Retinal Fundus Images. (arXiv:2108.02122v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02122</id>
        <link href="http://arxiv.org/abs/2108.02122"/>
        <updated>2021-08-05T01:56:21.289Z</updated>
        <summary type="html"><![CDATA[We explore the value of weak labels in learning transferable representations
for medical images. Compared to hand-labeled datasets, weak or inexact labels
can be acquired in large quantities at significantly lower cost and can provide
useful training signals for data-hungry models such as deep neural networks. We
consider weak labels in the form of pseudo-labels and propose a semi-weakly
supervised contrastive learning (SWCL) framework for representation learning
using semi-weakly annotated images. Specifically, we train a semi-supervised
model to propagate labels from a small dataset consisting of diverse
image-level annotations to a large unlabeled dataset. Using the propagated
labels, we generate a patch-level dataset for pretraining and formulate a
multi-label contrastive learning objective to capture position-specific
features encoded in each patch. We empirically validate the transfer learning
performance of SWCL on seven public retinal fundus datasets, covering three
disease classification tasks and two anatomical structure segmentation tasks.
Our experiment results suggest that, under very low data regime, large-scale
ImageNet pretraining on improved architecture remains a very strong baseline,
and recently proposed self-supervised methods falter in segmentation tasks,
possibly due to the strong invariant constraint imposed. Our method surpasses
all prior self-supervised methods and standard cross-entropy training, while
closing the gaps with ImageNet pretraining.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yap_B/0/1/0/all/0/1"&gt;Boon Peng Yap&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ng_B/0/1/0/all/0/1"&gt;Beng Koon Ng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Statistical Analysis of Wasserstein Distributionally Robust Estimators. (arXiv:2108.02120v1 [math.ST])]]></title>
        <id>http://arxiv.org/abs/2108.02120</id>
        <link href="http://arxiv.org/abs/2108.02120"/>
        <updated>2021-08-05T01:56:21.282Z</updated>
        <summary type="html"><![CDATA[We consider statistical methods which invoke a min-max distributionally
robust formulation to extract good out-of-sample performance in data-driven
optimization and learning problems. Acknowledging the distributional
uncertainty in learning from limited samples, the min-max formulations
introduce an adversarial inner player to explore unseen covariate data. The
resulting Distributionally Robust Optimization (DRO) formulations, which
include Wasserstein DRO formulations (our main focus), are specified using
optimal transportation phenomena. Upon describing how these
infinite-dimensional min-max problems can be approached via a
finite-dimensional dual reformulation, the tutorial moves into its main
component, namely, explaining a generic recipe for optimally selecting the size
of the adversary's budget. This is achieved by studying the limit behavior of
an optimal transport projection formulation arising from an inquiry on the
smallest confidence region that includes the unknown population risk minimizer.
Incidentally, this systematic prescription coincides with those in specific
examples in high-dimensional statistics and results in error bounds that are
free from the curse of dimensions. Equipped with this prescription, we present
a central limit theorem for the DRO estimator and provide a recipe for
constructing compatible confidence regions that are useful for uncertainty
quantification. The rest of the tutorial is devoted to insights into the nature
of the optimizers selected by the min-max formulations and additional
applications of optimal transport projections.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Blanchet_J/0/1/0/all/0/1"&gt;Jose Blanchet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Murthy_K/0/1/0/all/0/1"&gt;Karthyek Murthy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Nguyen_V/0/1/0/all/0/1"&gt;Viet Anh Nguyen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Discovering outliers in the Mars Express thermal power consumption patterns. (arXiv:2108.02067v1 [astro-ph.IM])]]></title>
        <id>http://arxiv.org/abs/2108.02067</id>
        <link href="http://arxiv.org/abs/2108.02067"/>
        <updated>2021-08-05T01:56:21.275Z</updated>
        <summary type="html"><![CDATA[The Mars Express (MEX) spacecraft has been orbiting Mars since 2004. The
operators need to constantly monitor its behavior and handle sporadic
deviations (outliers) from the expected patterns of measurements of quantities
that the satellite is sending to Earth. In this paper, we analyze the patterns
of the electrical power consumption of MEX's thermal subsystem, that maintains
the spacecraft's temperature at the desired level. The consumption is not
constant, but should be roughly periodic in the short term, with the period
that corresponds to one orbit around Mars. By using long short-term memory
neural networks, we show that the consumption pattern is more irregular than
expected, and successfully detect such irregularities, opening possibility for
automatic outlier detection on MEX in the future.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/astro-ph/1/au:+Petkovic_M/0/1/0/all/0/1"&gt;Matej Petkovi&amp;#x107;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Lucas_L/0/1/0/all/0/1"&gt;Luke Lucas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Stepisnik_T/0/1/0/all/0/1"&gt;Toma&amp;#x17e; Stepi&amp;#x161;nik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Panov_P/0/1/0/all/0/1"&gt;Pan&amp;#x10d;e Panov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Simidjievski_N/0/1/0/all/0/1"&gt;Nikola Simidjievski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Kocev_D/0/1/0/all/0/1"&gt;Dragi Kocev&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimal to-do list gamification. (arXiv:2008.05228v2 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.05228</id>
        <link href="http://arxiv.org/abs/2008.05228"/>
        <updated>2021-08-05T01:56:21.268Z</updated>
        <summary type="html"><![CDATA[What should I work on first? What can wait until later? Which projects should
I prioritize and which tasks are not worth my time? These are challenging
questions that many people face every day. People's intuitive strategy is to
prioritize their immediate experience over the long-term consequences. This
leads to procrastination and the neglect of important long-term projects in
favor of seemingly urgent tasks that are less important. Optimal gamification
strives to help people overcome these problems by incentivizing each task by a
number of points that communicates how valuable it is in the long-run.
Unfortunately, computing the optimal number of points with standard dynamic
programming methods quickly becomes intractable as the number of a person's
projects and the number of tasks required by each project increase. Here, we
introduce and evaluate a scalable method for identifying which tasks are most
important in the long run and incentivizing each task according to its
long-term value. Our method makes it possible to create to-do list gamification
apps that can handle the size and complexity of people's to-do lists in the
real world.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Stojcheski_J/0/1/0/all/0/1"&gt;Jugoslav Stojcheski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Felso_V/0/1/0/all/0/1"&gt;Valkyrie Felso&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lieder_F/0/1/0/all/0/1"&gt;Falk Lieder&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pervasive Hand Gesture Recognition for Smartphones using Non-audible Sound and Deep Learning. (arXiv:2108.02148v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2108.02148</id>
        <link href="http://arxiv.org/abs/2108.02148"/>
        <updated>2021-08-05T01:56:21.262Z</updated>
        <summary type="html"><![CDATA[Due to the mass advancement in ubiquitous technologies nowadays, new
pervasive methods have come into the practice to provide new innovative
features and stimulate the research on new human-computer interactions. This
paper presents a hand gesture recognition method that utilizes the smartphone's
built-in speakers and microphones. The proposed system emits an ultrasonic
sonar-based signal (inaudible sound) from the smartphone's stereo speakers,
which is then received by the smartphone's microphone and processed via a
Convolutional Neural Network (CNN) for Hand Gesture Recognition. Data
augmentation techniques are proposed to improve the detection accuracy and
three dual-channel input fusion methods are compared. The first method merges
the dual-channel audio as a single input spectrogram image. The second method
adopts early fusion by concatenating the dual-channel spectrograms. The third
method adopts late fusion by having two convectional input branches processing
each of the dual-channel spectrograms and then the outputs are merged by the
last layers. Our experimental results demonstrate a promising detection
accuracy for the six gestures presented in our publicly available dataset with
an accuracy of 93.58\% as a baseline.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ibrahim_A/0/1/0/all/0/1"&gt;Ahmed Ibrahim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+El_Refai_A/0/1/0/all/0/1"&gt;Ayman El-Refai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ahmed_S/0/1/0/all/0/1"&gt;Sara Ahmed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aboul_Ela_M/0/1/0/all/0/1"&gt;Mariam Aboul-Ela&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eraqi_H/0/1/0/all/0/1"&gt;Hesham M. Eraqi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moustafa_M/0/1/0/all/0/1"&gt;Mohamed Moustafa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ensemble Slice Sampling: Parallel, black-box and gradient-free inference for correlated & multimodal distributions. (arXiv:2002.06212v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.06212</id>
        <link href="http://arxiv.org/abs/2002.06212"/>
        <updated>2021-08-05T01:56:21.255Z</updated>
        <summary type="html"><![CDATA[Slice Sampling has emerged as a powerful Markov Chain Monte Carlo algorithm
that adapts to the characteristics of the target distribution with minimal
hand-tuning. However, Slice Sampling's performance is highly sensitive to the
user-specified initial length scale hyperparameter and the method generally
struggles with poorly scaled or strongly correlated distributions. This paper
introduces Ensemble Slice Sampling (ESS), a new class of algorithms that
bypasses such difficulties by adaptively tuning the initial length scale and
utilising an ensemble of parallel walkers in order to efficiently handle strong
correlations between parameters. These affine-invariant algorithms are trivial
to construct, require no hand-tuning, and can easily be implemented in parallel
computing environments. Empirical tests show that Ensemble Slice Sampling can
improve efficiency by more than an order of magnitude compared to conventional
MCMC methods on a broad range of highly correlated target distributions. In
cases of strongly multimodal target distributions, Ensemble Slice Sampling can
sample efficiently even in high dimensions. We argue that the parallel,
black-box and gradient-free nature of the method renders it ideal for use in
scientific fields such as physics, astrophysics and cosmology which are
dominated by a wide variety of computationally expensive and non-differentiable
models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Karamanis_M/0/1/0/all/0/1"&gt;Minas Karamanis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Beutler_F/0/1/0/all/0/1"&gt;Florian Beutler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning by Fixing: Solving Math Word Problems with Weak Supervision. (arXiv:2012.10582v2 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.10582</id>
        <link href="http://arxiv.org/abs/2012.10582"/>
        <updated>2021-08-05T01:56:21.236Z</updated>
        <summary type="html"><![CDATA[Previous neural solvers of math word problems (MWPs) are learned with full
supervision and fail to generate diverse solutions. In this paper, we address
this issue by introducing a \textit{weakly-supervised} paradigm for learning
MWPs. Our method only requires the annotations of the final answers and can
generate various solutions for a single problem. To boost weakly-supervised
learning, we propose a novel \textit{learning-by-fixing} (LBF) framework, which
corrects the misperceptions of the neural network via symbolic reasoning.
Specifically, for an incorrect solution tree generated by the neural network,
the \textit{fixing} mechanism propagates the error from the root node to the
leaf nodes and infers the most probable fix that can be executed to get the
desired answer. To generate more diverse solutions, \textit{tree
regularization} is applied to guide the efficient shrinkage and exploration of
the solution space, and a \textit{memory buffer} is designed to track and save
the discovered various fixes for each problem. Experimental results on the
Math23K dataset show the proposed LBF framework significantly outperforms
reinforcement learning baselines in weakly-supervised learning. Furthermore, it
achieves comparable top-1 and much better top-3/5 answer accuracies than
fully-supervised methods, demonstrating its strength in producing diverse
solutions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1"&gt;Yining Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1"&gt;Qing Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ciao_D/0/1/0/all/0/1"&gt;Daniel Ciao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1"&gt;Siyuan Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1"&gt;Song-Chun Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Aleatoric Uncertainty Quantification in Multi-Annotated Medical ImageSegmentation with Normalizing Flows. (arXiv:2108.02155v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02155</id>
        <link href="http://arxiv.org/abs/2108.02155"/>
        <updated>2021-08-05T01:56:21.216Z</updated>
        <summary type="html"><![CDATA[Quantifying uncertainty in medical image segmentation applications is
essential, as it is often connected to vital decision-making. Compelling
attempts have been made in quantifying the uncertainty in image segmentation
architectures, e.g. to learn a density segmentation model conditioned on the
input image. Typical work in this field restricts these learnt densities to be
strictly Gaussian. In this paper, we propose to use a more flexible approach by
introducing Normalizing Flows (NFs), which enables the learnt densities to be
more complex and facilitate more accurate modeling for uncertainty. We prove
this hypothesis by adopting the Probabilistic U-Net and augmenting the
posterior density with an NF, allowing it to be more expressive. Our
qualitative as well as quantitative (GED and IoU) evaluations on the
multi-annotated and single-annotated LIDC-IDRI and Kvasir-SEG segmentation
datasets, respectively, show a clear improvement. This is mostly apparent in
the quantification of aleatoric uncertainty and the increased predictive
performance of up to 14 percent. This result strongly indicates that a more
flexible density model should be seriously considered in architectures that
attempt to capture segmentation ambiguity through density modeling. The benefit
of this improved modeling will increase human confidence in annotation and
segmentation, and enable eager adoption of the technology in practice.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Valiuddin_M/0/1/0/all/0/1"&gt;M.M.A. Valiuddin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Viviers_C/0/1/0/all/0/1"&gt;C.G.A. Viviers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sloun_R/0/1/0/all/0/1"&gt;R.J.G. van Sloun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+With_P/0/1/0/all/0/1"&gt;P.H.N. de With&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sommen_F/0/1/0/all/0/1"&gt;F. van der Sommen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Making Contrastive Learning Robust to Shortcuts. (arXiv:2012.09962v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.09962</id>
        <link href="http://arxiv.org/abs/2012.09962"/>
        <updated>2021-08-05T01:56:21.191Z</updated>
        <summary type="html"><![CDATA[Contrastive learning is effective at learning useful representations without
supervision. Yet contrastive learning is susceptible to shortcuts -- i.e., it
may learn shortcut features irrelevant to the downstream task and discard
relevant information. Past work has addressed this limitation via handcrafted
data augmentations that eliminate the shortcut. However, handcrafted
augmentations are infeasible for data modalities that are not interpretable by
humans (e.g., radio signals). Further, even when the modality is interpretable
(e.g., RGB), sometimes eliminating the shortcut information may be undesirable.
For example, in multi-attribute classification, information related to one
attribute may act as a shortcut around other attributes. This paper presents
reconstructive contrastive learning (RCL), a framework for learning
unsupervised representations that are robust to shortcuts. The key idea is to
force the learned representation to reconstruct the input, which naturally
counters potential shortcuts. Extensive experiments verify that RCL is highly
robust to shortcuts and outperforms state-of-the-art contrastive learning
methods on both RGB and RF datasets for a variety of tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1"&gt;Tianhong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1"&gt;Lijie Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1"&gt;Yuan Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1"&gt;Hao He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1"&gt;Yonglong Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Katabi_D/0/1/0/all/0/1"&gt;Dina Katabi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feris_R/0/1/0/all/0/1"&gt;Rogerio Feris&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Label Gold Asymmetric Loss Correction with Single-Label Regulators. (arXiv:2108.02032v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02032</id>
        <link href="http://arxiv.org/abs/2108.02032"/>
        <updated>2021-08-05T01:56:21.184Z</updated>
        <summary type="html"><![CDATA[Multi-label learning is an emerging extension of the multi-class
classification where an image contains multiple labels. Not only acquiring a
clean and fully labeled dataset in multi-label learning is extremely expensive,
but also many of the actual labels are corrupted or missing due to the
automated or non-expert annotation techniques. Noisy label data decrease the
prediction performance drastically. In this paper, we propose a novel Gold
Asymmetric Loss Correction with Single-Label Regulators (GALC-SLR) that
operates robust against noisy labels. GALC-SLR estimates the noise confusion
matrix using single-label samples, then constructs an asymmetric loss
correction via estimated confusion matrix to avoid overfitting to the noisy
labels. Empirical results show that our method outperforms the state-of-the-art
original asymmetric loss multi-label classifier under all corruption levels,
showing mean average precision improvement up to 28.67% on a real world dataset
of MS-COCO, yielding a better generalization of the unseen data and increased
prediction performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pene_C/0/1/0/all/0/1"&gt;Cosmin Octavian Pene&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghiassi_A/0/1/0/all/0/1"&gt;Amirmasoud Ghiassi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Younesian_T/0/1/0/all/0/1"&gt;Taraneh Younesian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Birke_R/0/1/0/all/0/1"&gt;Robert Birke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Lydia Y.Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Targeted Attention Attack on Deep Learning Models in Road Sign Recognition. (arXiv:2010.04331v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.04331</id>
        <link href="http://arxiv.org/abs/2010.04331"/>
        <updated>2021-08-05T01:56:21.177Z</updated>
        <summary type="html"><![CDATA[Real world traffic sign recognition is an important step towards building
autonomous vehicles, most of which highly dependent on Deep Neural Networks
(DNNs). Recent studies demonstrated that DNNs are surprisingly susceptible to
adversarial examples. Many attack methods have been proposed to understand and
generate adversarial examples, such as gradient based attack, score based
attack, decision based attack, and transfer based attacks. However, most of
these algorithms are ineffective in real-world road sign attack, because (1)
iteratively learning perturbations for each frame is not realistic for a fast
moving car and (2) most optimization algorithms traverse all pixels equally
without considering their diverse contribution. To alleviate these problems,
this paper proposes the targeted attention attack (TAA) method for real world
road sign attack. Specifically, we have made the following contributions: (1)
we leverage the soft attention map to highlight those important pixels and skip
those zero-contributed areas - this also helps to generate natural
perturbations, (2) we design an efficient universal attack that optimizes a
single perturbation/noise based on a set of training images under the guidance
of the pre-trained attention map, (3) we design a simple objective function
that can be easily optimized, (4) we evaluate the effectiveness of TAA on real
world data sets. Experimental results validate that the TAA method improves the
attack successful rate (nearly 10%) and reduces the perturbation loss (about a
quarter) compared with the popular RP2 method. Additionally, our TAA also
provides good properties, e.g., transferability and generalization capability.
We provide code and data to ensure the reproducibility:
https://github.com/AdvAttack/RoadSignAttack.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xinghao Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Weifeng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shengli Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1"&gt;Dacheng Tao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The MIT Supercloud Dataset. (arXiv:2108.02037v1 [cs.DC])]]></title>
        <id>http://arxiv.org/abs/2108.02037</id>
        <link href="http://arxiv.org/abs/2108.02037"/>
        <updated>2021-08-05T01:56:21.170Z</updated>
        <summary type="html"><![CDATA[Artificial intelligence (AI) and Machine learning (ML) workloads are an
increasingly larger share of the compute workloads in traditional
High-Performance Computing (HPC) centers and commercial cloud systems. This has
led to changes in deployment approaches of HPC clusters and the commercial
cloud, as well as a new focus on approaches to optimized resource usage,
allocations and deployment of new AI frame- works, and capabilities such as
Jupyter notebooks to enable rapid prototyping and deployment. With these
changes, there is a need to better understand cluster/datacenter operations
with the goal of developing improved scheduling policies, identifying
inefficiencies in resource utilization, energy/power consumption, failure
prediction, and identifying policy violations. In this paper we introduce the
MIT Supercloud Dataset which aims to foster innovative AI/ML approaches to the
analysis of large scale HPC and datacenter/cloud operations. We provide
detailed monitoring logs from the MIT Supercloud system, which include CPU and
GPU usage by jobs, memory usage, file system logs, and physical monitoring
data. This paper discusses the details of the dataset, collection methodology,
data availability, and discusses potential challenge problems being developed
using this data. Datasets and future challenge announcements will be available
via https://dcc.mit.edu.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Samsi_S/0/1/0/all/0/1"&gt;Siddharth Samsi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weiss_M/0/1/0/all/0/1"&gt;Matthew L Weiss&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bestor_D/0/1/0/all/0/1"&gt;David Bestor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Baolin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jones_M/0/1/0/all/0/1"&gt;Michael Jones&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reuther_A/0/1/0/all/0/1"&gt;Albert Reuther&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Edelman_D/0/1/0/all/0/1"&gt;Daniel Edelman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arcand_W/0/1/0/all/0/1"&gt;William Arcand&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Byun_C/0/1/0/all/0/1"&gt;Chansup Byun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Holodnack_J/0/1/0/all/0/1"&gt;John Holodnack&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hubbell_M/0/1/0/all/0/1"&gt;Matthew Hubbell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kepner_J/0/1/0/all/0/1"&gt;Jeremy Kepner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Klein_A/0/1/0/all/0/1"&gt;Anna Klein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McDonald_J/0/1/0/all/0/1"&gt;Joseph McDonald&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Michaleas_A/0/1/0/all/0/1"&gt;Adam Michaleas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Michaleas_P/0/1/0/all/0/1"&gt;Peter Michaleas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Milechin_L/0/1/0/all/0/1"&gt;Lauren Milechin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mullen_J/0/1/0/all/0/1"&gt;Julia Mullen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yee_C/0/1/0/all/0/1"&gt;Charles Yee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Price_B/0/1/0/all/0/1"&gt;Benjamin Price&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prout_A/0/1/0/all/0/1"&gt;Andrew Prout&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rosa_A/0/1/0/all/0/1"&gt;Antonio Rosa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vanterpool_A/0/1/0/all/0/1"&gt;Allan Vanterpool&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McEvoy_L/0/1/0/all/0/1"&gt;Lindsey McEvoy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_A/0/1/0/all/0/1"&gt;Anson Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tiwari_D/0/1/0/all/0/1"&gt;Devesh Tiwari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gadepally_V/0/1/0/all/0/1"&gt;Vijay Gadepally&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Auto-encoder based Model for High-dimensional Imbalanced Industrial Data. (arXiv:2108.02083v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2108.02083</id>
        <link href="http://arxiv.org/abs/2108.02083"/>
        <updated>2021-08-05T01:56:21.163Z</updated>
        <summary type="html"><![CDATA[With the proliferation of IoT devices, the distributed control systems are
now capturing and processing more sensors at higher frequency than ever before.
These new data, due to their volume and novelty, cannot be effectively consumed
without the help of data-driven techniques. Deep learning is emerging as a
promising technique to analyze these data, particularly in soft sensor
modeling. The strong representational capabilities of complex data and the
flexibility it offers from an architectural perspective make it a topic of
active applied research in industrial settings. However, the successful
applications of deep learning in soft sensing are still not widely integrated
in factory control systems, because most of the research on soft sensing do not
have access to large scale industrial data which are varied, noisy and
incomplete. The results published in most research papers are therefore not
easily reproduced when applied to the variety of data in industrial settings.
Here we provide manufacturing data sets that are much larger and more complex
than public open soft sensor data. Moreover, the data sets are from Seagate
factories on active service with only necessary anonymization, so that they
reflect the complex and noisy nature of real-world data. We introduce a
variance weighted multi-headed auto-encoder classification model that fits well
into the high-dimensional and highly imbalanced data. Besides the use of
weighting or sampling methods to handle the highly imbalanced data, the model
also simultaneously predicts multiple outputs by exploiting output-supervised
representation learning and multi-task weighting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zhang1_C/0/1/0/all/0/1"&gt;Chao Zhang1&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bom_S/0/1/0/all/0/1"&gt;Sthitie Bom&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Convergence of gradient descent for learning linear neural networks. (arXiv:2108.02040v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02040</id>
        <link href="http://arxiv.org/abs/2108.02040"/>
        <updated>2021-08-05T01:56:21.142Z</updated>
        <summary type="html"><![CDATA[We study the convergence properties of gradient descent for training deep
linear neural networks, i.e., deep matrix factorizations, by extending a
previous analysis for the related gradient flow. We show that under suitable
conditions on the step sizes gradient descent converges to a critical point of
the loss function, i.e., the square loss in this article. Furthermore, we
demonstrate that for almost all initializations gradient descent converges to a
global minimum in the case of two layers. In the case of three or more layers
we show that gradient descent converges to a global minimum on the manifold
matrices of some fixed rank, where the rank cannot be determined a priori.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nguegnang_G/0/1/0/all/0/1"&gt;Gabin Maxime Nguegnang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rauhut_H/0/1/0/all/0/1"&gt;Holger Rauhut&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Terstiege_U/0/1/0/all/0/1"&gt;Ulrich Terstiege&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Policy Gradients Incorporating the Future. (arXiv:2108.02096v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02096</id>
        <link href="http://arxiv.org/abs/2108.02096"/>
        <updated>2021-08-05T01:56:21.136Z</updated>
        <summary type="html"><![CDATA[Reasoning about the future -- understanding how decisions in the present time
affect outcomes in the future -- is one of the central challenges for
reinforcement learning (RL), especially in highly-stochastic or partially
observable environments. While predicting the future directly is hard, in this
work we introduce a method that allows an agent to "look into the future"
without explicitly predicting it. Namely, we propose to allow an agent, during
its training on past experience, to observe what \emph{actually} happened in
the future at that time, while enforcing an information bottleneck to avoid the
agent overly relying on this privileged information. This gives our agent the
opportunity to utilize rich and useful information about the future trajectory
dynamics in addition to the present. Our method, Policy Gradients Incorporating
the Future (PGIF), is easy to implement and versatile, being applicable to
virtually any policy gradient algorithm. We apply our proposed method to a
number of off-the-shelf RL algorithms and show that PGIF is able to achieve
higher reward faster in a variety of online and offline RL domains, as well as
sparse-reward and partially observable environments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Venuto_D/0/1/0/all/0/1"&gt;David Venuto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lau_E/0/1/0/all/0/1"&gt;Elaine Lau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Precup_D/0/1/0/all/0/1"&gt;Doina Precup&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nachum_O/0/1/0/all/0/1"&gt;Ofir Nachum&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Parallelized Reverse Curriculum Generation. (arXiv:2108.02128v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02128</id>
        <link href="http://arxiv.org/abs/2108.02128"/>
        <updated>2021-08-05T01:56:21.130Z</updated>
        <summary type="html"><![CDATA[For reinforcement learning (RL), it is challenging for an agent to master a
task that requires a specific series of actions due to sparse rewards. To solve
this problem, reverse curriculum generation (RCG) provides a reverse expansion
approach that automatically generates a curriculum for the agent to learn. More
specifically, RCG adapts the initial state distribution from the neighborhood
of a goal to a distance as training proceeds. However, the initial state
distribution generated for each iteration might be biased, thus making the
policy overfit or slowing down the reverse expansion rate. While training RCG
for actor-critic (AC) based RL algorithms, this poor generalization and slow
convergence might be induced by the tight coupling between an AC pair.
Therefore, we propose a parallelized approach that simultaneously trains
multiple AC pairs and periodically exchanges their critics. We empirically
demonstrate that this proposed approach can improve RCG in performance and
convergence, and it can also be applied to other AC based RL algorithms with
adapted initial state distribution.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chiu_Z/0/1/0/all/0/1"&gt;Zih-Yun Chiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tuan_Y/0/1/0/all/0/1"&gt;Yi-Lin Tuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1"&gt;Hung-yi Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_L/0/1/0/all/0/1"&gt;Li-Chen Fu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Essential Features: Content-Adaptive Pixel Discretization to Improve Model Robustness to Adaptive Adversarial Attacks. (arXiv:2012.01699v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.01699</id>
        <link href="http://arxiv.org/abs/2012.01699"/>
        <updated>2021-08-05T01:56:21.122Z</updated>
        <summary type="html"><![CDATA[To remove the effects of adversarial perturbations, preprocessing defenses
such as pixel discretization are appealing due to their simplicity but have so
far been shown to be ineffective except on simple datasets such as MNIST,
leading to the belief that pixel discretization approaches are doomed to
failure as a defense technique. This paper revisits the pixel discretization
approaches. We hypothesize that the reason why existing approaches have failed
is that they have used a fixed codebook for the entire dataset. In particular,
we find that can lead to situations where images become more susceptible to
adversarial perturbations and also suffer significant loss of accuracy after
discretization. We propose a novel image preprocessing technique called
Essential Features that uses an adaptive codebook that is based on per-image
content and threat model. Essential Features adaptively selects a separable set
of color clusters for each image to reduce the color space while preserving the
pertinent features of the original image, maximizing both separability and
representation of colors. Additionally, to limit the adversary's ability to
influence the chosen color clusters, Essential Features takes advantage of
spatial correlation with an adaptive blur that moves pixels closer to their
original value without destroying original edge information. We design several
adaptive attacks and find that our approach is more robust than previous
baselines on $L_\infty$ and $L_2$ bounded attacks for several challenging
datasets including CIFAR-10, GTSRB, RESISC45, and ImageNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Feng_R/0/1/0/all/0/1"&gt;Ryan Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_W/0/1/0/all/0/1"&gt;Wu-chi Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prakash_A/0/1/0/all/0/1"&gt;Atul Prakash&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hyperparameter-free and Explainable Whole Graph Embedding. (arXiv:2108.02113v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02113</id>
        <link href="http://arxiv.org/abs/2108.02113"/>
        <updated>2021-08-05T01:56:21.115Z</updated>
        <summary type="html"><![CDATA[Many real-world complex systems can be described as graphs. For a large-scale
graph with low sparsity, a node's adjacency vector is a long and sparse
representation, limiting the practical utilization of existing machine learning
methods on nodal features. In practice, graph embedding (graph representation
learning) attempts to learn a lower-dimensional representation vector for each
node or the whole graph while maintaining the most basic information of graph.
Since various machine learning methods can efficiently process
lower-dimensional vectors, graph embedding has recently attracted a lot of
attention. However, most node embedding or whole graph embedding methods suffer
from the problem of having more sophisticated methodology, hyperparameter
optimization, and low explainability. This paper proposes a
hyperparameter-free, extensible, and explainable whole graph embedding method,
combining the DHC (Degree, H-index and Coreness) theorem and Shannon Entropy
(E), abbreviated as DHC-E. The new whole graph embedding scheme can obtain a
trade-off between the simplicity and the quality under some supervised
classification learning tasks, using molecular, social, and brain networks. In
addition, the proposed approach has a good performance in lower-dimensional
graph visualization. The new methodology is overall simple,
hyperparameter-free, extensible, and explainable for whole graph embedding with
promising potential for exploring graph classification, prediction, and
lower-dimensional graph visualization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1"&gt;Yue Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_L/0/1/0/all/0/1"&gt;Linyuan L&amp;#xfc;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1"&gt;Guanrong Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Binary Matrix Factorisation via Column Generation. (arXiv:2011.04457v3 [math.OC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.04457</id>
        <link href="http://arxiv.org/abs/2011.04457"/>
        <updated>2021-08-05T01:56:21.092Z</updated>
        <summary type="html"><![CDATA[Identifying discrete patterns in binary data is an important dimensionality
reduction tool in machine learning and data mining. In this paper, we consider
the problem of low-rank binary matrix factorisation (BMF) under Boolean
arithmetic. Due to the hardness of this problem, most previous attempts rely on
heuristic techniques. We formulate the problem as a mixed integer linear
program and use a large scale optimisation technique of column generation to
solve it without the need of heuristic pattern mining. Our approach focuses on
accuracy and on the provision of optimality guarantees. Experimental results on
real world datasets demonstrate that our proposed method is effective at
producing highly accurate factorisations and improves on the previously
available best known results for 15 out of 24 problem instances.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Kovacs_R/0/1/0/all/0/1"&gt;Reka A. Kovacs&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Gunluk_O/0/1/0/all/0/1"&gt;Oktay Gunluk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Hauser_R/0/1/0/all/0/1"&gt;Raphael A. Hauser&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Weaker Faithfulness Assumption based on Triple Interactions. (arXiv:2010.14265v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.14265</id>
        <link href="http://arxiv.org/abs/2010.14265"/>
        <updated>2021-08-05T01:56:21.085Z</updated>
        <summary type="html"><![CDATA[One of the core assumptions in causal discovery is the faithfulness
assumption, i.e., assuming that independencies found in the data are due to
separations in the true causal graph. This assumption can, however, be violated
in many ways, including xor connections, deterministic functions or cancelling
paths. In this work, we propose a weaker assumption that we call $2$-adjacency
faithfulness. In contrast to adjacency faithfulness, which assumes that there
is no conditional independence between each pair of variables that are
connected in the causal graph, we only require no conditional independence
between a node and a subset of its Markov blanket that can contain up to two
nodes. Equivalently, we adapt orientation faithfulness to this setting. We
further propose a sound orientation rule for causal discovery that applies
under weaker assumptions. As a proof of concept, we derive a modified Grow and
Shrink algorithm that recovers the Markov blanket of a target node and prove
its correctness under strictly weaker assumptions than the standard
faithfulness assumption.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Marx_A/0/1/0/all/0/1"&gt;Alexander Marx&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Gretton_A/0/1/0/all/0/1"&gt;Arthur Gretton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Mooij_J/0/1/0/all/0/1"&gt;Joris M. Mooij&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FedJAX: Federated learning simulation with JAX. (arXiv:2108.02117v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.02117</id>
        <link href="http://arxiv.org/abs/2108.02117"/>
        <updated>2021-08-05T01:56:21.063Z</updated>
        <summary type="html"><![CDATA[Federated learning is a machine learning technique that enables training
across decentralized data. Recently, federated learning has become an active
area of research due to the increased concerns over privacy and security. In
light of this, a variety of open source federated learning libraries have been
developed and released. We introduce FedJAX, a JAX-based open source library
for federated learning simulations that emphasizes ease-of-use in research.
With its simple primitives for implementing federated learning algorithms,
prepackaged datasets, models and algorithms, and fast simulation speed, FedJAX
aims to make developing and evaluating federated algorithms faster and easier
for researchers. Our benchmark results show that FedJAX can be used to train
models with federated averaging on the EMNIST dataset in a few minutes and the
Stack Overflow dataset in roughly an hour with standard hyperparmeters using
TPUs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ro_J/0/1/0/all/0/1"&gt;Jae Hun Ro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suresh_A/0/1/0/all/0/1"&gt;Ananda Theertha Suresh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1"&gt;Ke Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A purely data-driven framework for prediction, optimization, and control of networked processes: application to networked SIS epidemic model. (arXiv:2108.02005v1 [cs.SI])]]></title>
        <id>http://arxiv.org/abs/2108.02005</id>
        <link href="http://arxiv.org/abs/2108.02005"/>
        <updated>2021-08-05T01:56:21.055Z</updated>
        <summary type="html"><![CDATA[Networks are landmarks of many complex phenomena where interweaving
interactions between different agents transform simple local rule-sets into
nonlinear emergent behaviors. While some recent studies unveil associations
between the network structure and the underlying dynamical process, identifying
stochastic nonlinear dynamical processes continues to be an outstanding
problem. Here we develop a simple data-driven framework based on
operator-theoretic techniques to identify and control stochastic nonlinear
dynamics taking place over large-scale networks. The proposed approach requires
no prior knowledge of the network structure and identifies the underlying
dynamics solely using a collection of two-step snapshots of the states. This
data-driven system identification is achieved by using the Koopman operator to
find a low dimensional representation of the dynamical patterns that evolve
linearly. Further, we use the global linear Koopman model to solve critical
control problems by applying to model predictive control (MPC)--typically, a
challenging proposition when applied to large networks. We show that our
proposed approach tackles this by converting the original nonlinear programming
into a more tractable optimization problem that is both convex and with far
fewer variables.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tavasoli_A/0/1/0/all/0/1"&gt;Ali Tavasoli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Henry_T/0/1/0/all/0/1"&gt;Teague Henry&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shakeri_H/0/1/0/all/0/1"&gt;Heman Shakeri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Random Convolution Kernels with Multi-Scale Decomposition for Preterm EEG Inter-burst Detection. (arXiv:2108.02039v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2108.02039</id>
        <link href="http://arxiv.org/abs/2108.02039"/>
        <updated>2021-08-05T01:56:21.032Z</updated>
        <summary type="html"><![CDATA[Linear classifiers with random convolution kernels are computationally
efficient methods that need no design or domain knowledge. Unlike deep neural
networks, there is no need to hand-craft a network architecture; the kernels
are randomly generated and only the linear classifier needs training. A
recently proposed method, RandOm Convolutional KErnel Transforms (ROCKETs), has
shown high accuracy across a range of time-series data sets. Here we propose a
multi-scale version of this method, using both high- and low-frequency
components. We apply our methods to inter-burst detection in a cohort of
preterm EEG recorded from 36 neonates <30 weeks gestational age. Two features
from the convolution of 10,000 random kernels are combined using ridge
regression. The proposed multi-scale ROCKET method out-performs the method
without scale: median (interquartile range, IQR) Matthews correlation
coefficient (MCC) of 0.859 (0.815 to 0.874) for multi-scale versus 0.841 (0.807
to 0.865) without scale, p<0.001. The proposed method lags behind an existing
feature-based machine learning method developed with deep domain knowledge, but
is fast to train and can quickly set an initial baseline threshold of
performance for generic and biomedical time-series classification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Lundy_C/0/1/0/all/0/1"&gt;Christopher Lundy&lt;/a&gt; (1 and 2), &lt;a href="http://arxiv.org/find/eess/1/au:+OToole_J/0/1/0/all/0/1"&gt;John M. O&amp;#x27;Toole&lt;/a&gt; (1 and 2) ((1) Irish Centre for Maternal and Child Health Research (INFANT), University College Cork, Ireland, (2) Department of Paediatrics and Child Health, University College Cork, Ireland)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Personal Devices for Contact Tracing: Smartphones and Wearables to Fight Covid-19. (arXiv:2108.02008v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2108.02008</id>
        <link href="http://arxiv.org/abs/2108.02008"/>
        <updated>2021-08-05T01:56:20.993Z</updated>
        <summary type="html"><![CDATA[Digital contact tracing has emerged as a viable tool supplementing manual
contact tracing. To date, more than 100 contact tracing applications have been
published to slow down the spread of highly contagious Covid-19. Despite subtle
variabilities among these applications, all of them achieve contact tracing by
manipulating the following three components: a) use a personal device to
identify the user while designing a secure protocol to anonymize the user's
identity; b) leverage networking technologies to analyze and store the data; c)
exploit rich sensing features on the user device to detect the interaction
among users and thus estimate the exposure risk. This paper reviews the current
digital contact tracing based on these three components. We focus on two
personal devices that are intimate to the user: smartphones and wearables. We
discuss the centralized and decentralized networking approaches that use to
facilitate the data flow. Lastly, we investigate the sensing feature available
on smartphones and wearables to detect the proximity between any two users and
present experiments comparing the proximity sensing performance between these
two personal devices.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ng_P/0/1/0/all/0/1"&gt;Pai Chet Ng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Spachos_P/0/1/0/all/0/1"&gt;Petros Spachos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gregori_S/0/1/0/all/0/1"&gt;Stefano Gregori&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Plataniotis_K/0/1/0/all/0/1"&gt;Konstantinos Plataniotis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Exploitability of Audio Machine Learning Pipelines to Surreptitious Adversarial Examples. (arXiv:2108.02010v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2108.02010</id>
        <link href="http://arxiv.org/abs/2108.02010"/>
        <updated>2021-08-05T01:56:20.979Z</updated>
        <summary type="html"><![CDATA[Machine learning (ML) models are known to be vulnerable to adversarial
examples. Applications of ML to voice biometrics authentication are no
exception. Yet, the implications of audio adversarial examples on these
real-world systems remain poorly understood given that most research targets
limited defenders who can only listen to the audio samples. Conflating
detectability of an attack with human perceptibility, research has focused on
methods that aim to produce imperceptible adversarial examples which humans
cannot distinguish from the corresponding benign samples. We argue that this
perspective is coarse for two reasons: 1. Imperceptibility is impossible to
verify; it would require an experimental process that encompasses variations in
listener training, equipment, volume, ear sensitivity, types of background
noise etc, and 2. It disregards pipeline-based detection clues that realistic
defenders leverage. This results in adversarial examples that are ineffective
in the presence of knowledgeable defenders. Thus, an adversary only needs an
audio sample to be plausible to a human. We thus introduce surreptitious
adversarial examples, a new class of attacks that evades both human and
pipeline controls. In the white-box setting, we instantiate this class with a
joint, multi-stage optimization attack. Using an Amazon Mechanical Turk user
study, we show that this attack produces audio samples that are more
surreptitious than previous attacks that aim solely for imperceptibility.
Lastly we show that surreptitious adversarial examples are challenging to
develop in the black-box setting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Travers_A/0/1/0/all/0/1"&gt;Adelin Travers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Licollari_L/0/1/0/all/0/1"&gt;Lorna Licollari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1"&gt;Guanghan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chandrasekaran_V/0/1/0/all/0/1"&gt;Varun Chandrasekaran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dziedzic_A/0/1/0/all/0/1"&gt;Adam Dziedzic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lie_D/0/1/0/all/0/1"&gt;David Lie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Papernot_N/0/1/0/all/0/1"&gt;Nicolas Papernot&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Neural Network Approach to Estimate Early Worst-Case Execution Time. (arXiv:2108.02001v1 [cs.SE])]]></title>
        <id>http://arxiv.org/abs/2108.02001</id>
        <link href="http://arxiv.org/abs/2108.02001"/>
        <updated>2021-08-05T01:56:20.958Z</updated>
        <summary type="html"><![CDATA[Estimating Worst-Case Execution Time (WCET) is of utmost importance for
developing Cyber-Physical and Safety-Critical Systems. The system's scheduler
uses the estimated WCET to schedule each task of these systems, and failure may
lead to catastrophic events. It is thus imperative to build provably reliable
systems. WCET is available to us in the last stage of systems development when
the hardware is available and the application code is compiled on it. Different
methodologies measure the WCET, but none of them give early insights on WCET,
which is crucial for system development. If the system designers overestimate
WCET in the early stage, then it would lead to the overqualified system, which
will increase the cost of the final product, and if they underestimate WCET in
the early stage, then it would lead to financial loss as the system would not
perform as expected. This paper estimates early WCET using Deep Neural Networks
as an approximate predictor model for hardware architecture and compiler. This
model predicts the WCET based on the source code without compiling and running
on the hardware architecture. Our WCET prediction model is created using the
Pytorch framework. The resulting WCET is too erroneous to be used as an upper
bound on the WCET. However, getting these results in the early stages of system
development is an essential prerequisite for the system's dimensioning and
configuration of the hardware setup.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1"&gt;Vikash Kumar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robustness of convolutional neural networks to physiological ECG noise. (arXiv:2108.01995v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2108.01995</id>
        <link href="http://arxiv.org/abs/2108.01995"/>
        <updated>2021-08-05T01:56:20.951Z</updated>
        <summary type="html"><![CDATA[The electrocardiogram (ECG) is one of the most widespread diagnostic tools in
healthcare and supports the diagnosis of cardiovascular disorders. Deep
learning methods are a successful and popular technique to detect indications
of disorders from an ECG signal. However, there are open questions around the
robustness of these methods to various factors, including physiological ECG
noise. In this study we generate clean and noisy versions of an ECG dataset
before applying Symmetric Projection Attractor Reconstruction (SPAR) and
scalogram image transformations. A pretrained convolutional neural network is
trained using transfer learning to classify these image transforms. For the
clean ECG dataset, F1 scores for SPAR attractor and scalogram transforms were
0.70 and 0.79, respectively, and the scores decreased by less than 0.05 for the
noisy ECG datasets. Notably, when the network trained on clean data was used to
classify the noisy datasets, performance decreases of up to 0.18 in F1 scores
were seen. However, when the network trained on the noisy data was used to
classify the clean dataset, the performance decrease was less than 0.05. We
conclude that physiological ECG noise impacts classification using deep
learning methods and careful consideration should be given to the inclusion of
noisy ECG signals in the training data when developing supervised networks for
ECG classification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Venton_J/0/1/0/all/0/1"&gt;J. Venton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Harris_P/0/1/0/all/0/1"&gt;P. M. Harris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Sundar_A/0/1/0/all/0/1"&gt;A. Sundar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Smith_N/0/1/0/all/0/1"&gt;N. A. S. Smith&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Aston_P/0/1/0/all/0/1"&gt;P. J. Aston&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarial Energy Disaggregation for Non-intrusive Load Monitoring. (arXiv:2108.01998v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2108.01998</id>
        <link href="http://arxiv.org/abs/2108.01998"/>
        <updated>2021-08-05T01:56:20.944Z</updated>
        <summary type="html"><![CDATA[Energy disaggregation, also known as non-intrusive load monitoring (NILM),
challenges the problem of separating the whole-home electricity usage into
appliance-specific individual consumptions, which is a typical application of
data analysis. {NILM aims to help households understand how the energy is used
and consequently tell them how to effectively manage the energy, thus allowing
energy efficiency which is considered as one of the twin pillars of sustainable
energy policy (i.e., energy efficiency and renewable energy).} Although NILM is
unidentifiable, it is widely believed that the NILM problem can be addressed by
data science. Most of the existing approaches address the energy disaggregation
problem by conventional techniques such as sparse coding, non-negative matrix
factorization, and hidden Markov model. Recent advances reveal that deep neural
networks (DNNs) can get favorable performance for NILM since DNNs can
inherently learn the discriminative signatures of the different appliances. In
this paper, we propose a novel method named adversarial energy disaggregation
(AED) based on DNNs. We introduce the idea of adversarial learning into NILM,
which is new for the energy disaggregation task. Our method trains a generator
and multiple discriminators via an adversarial fashion. The proposed method not
only learns shard representations for different appliances, but captures the
specific multimode structures of each appliance. Extensive experiments on
real-world datasets verify that our method can achieve new state-of-the-art
performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Du_Z/0/1/0/all/0/1"&gt;Zhekai Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1"&gt;Jingjing Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhu_L/0/1/0/all/0/1"&gt;Lei Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lu_K/0/1/0/all/0/1"&gt;Ke Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shen_H/0/1/0/all/0/1"&gt;Heng Tao Shen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PDE-GCN: Novel Architectures for Graph Neural Networks Motivated by Partial Differential Equations. (arXiv:2108.01938v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.01938</id>
        <link href="http://arxiv.org/abs/2108.01938"/>
        <updated>2021-08-05T01:56:20.926Z</updated>
        <summary type="html"><![CDATA[Graph neural networks are increasingly becoming the go-to approach in various
fields such as computer vision, computational biology and chemistry, where data
are naturally explained by graphs. However, unlike traditional convolutional
neural networks, deep graph networks do not necessarily yield better
performance than shallow graph networks. This behavior usually stems from the
over-smoothing phenomenon. In this work, we propose a family of architectures
to control this behavior by design. Our networks are motivated by numerical
methods for solving Partial Differential Equations (PDEs) on manifolds, and as
such, their behavior can be explained by similar analysis. Moreover, as we
demonstrate using an extensive set of experiments, our PDE-motivated networks
can generalize and be effective for various types of problems from different
fields. Our architectures obtain better or on par with the current
state-of-the-art results for problems that are typically approached using
different architectures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Eliasof_M/0/1/0/all/0/1"&gt;Moshe Eliasof&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Haber_E/0/1/0/all/0/1"&gt;Eldad Haber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Treister_E/0/1/0/all/0/1"&gt;Eran Treister&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Secure and Privacy-Preserving Federated Learning via Co-Utility. (arXiv:2108.01913v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2108.01913</id>
        <link href="http://arxiv.org/abs/2108.01913"/>
        <updated>2021-08-05T01:56:20.919Z</updated>
        <summary type="html"><![CDATA[The decentralized nature of federated learning, that often leverages the
power of edge devices, makes it vulnerable to attacks against privacy and
security. The privacy risk for a peer is that the model update she computes on
her private data may, when sent to the model manager, leak information on those
private data. Even more obvious are security attacks, whereby one or several
malicious peers return wrong model updates in order to disrupt the learning
process and lead to a wrong model being learned. In this paper we build a
federated learning framework that offers privacy to the participating peers as
well as security against Byzantine and poisoning attacks. Our framework
consists of several protocols that provide strong privacy to the participating
peers via unlinkable anonymity and that are rationally sustainable based on the
co-utility property. In other words, no rational party is interested in
deviating from the proposed protocols. We leverage the notion of co-utility to
build a decentralized co-utile reputation management system that provides
incentives for parties to adhere to the protocols. Unlike privacy protection
via differential privacy, our approach preserves the values of model updates
and hence the accuracy of plain federated learning; unlike privacy protection
via update aggregation, our approach preserves the ability to detect bad model
updates while substantially reducing the computational overhead compared to
methods based on homomorphic encryption.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Domingo_Ferrer_J/0/1/0/all/0/1"&gt;Josep Domingo-Ferrer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Blanco_Justicia_A/0/1/0/all/0/1"&gt;Alberto Blanco-Justicia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Manjon_J/0/1/0/all/0/1"&gt;Jes&amp;#xfa;s Manj&amp;#xf3;n&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sanchez_D/0/1/0/all/0/1"&gt;David S&amp;#xe1;nchez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DuCN: Dual-children Network for Medical Diagnosis and Similar Case Recommendation towards COVID-19. (arXiv:2108.01997v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2108.01997</id>
        <link href="http://arxiv.org/abs/2108.01997"/>
        <updated>2021-08-05T01:56:20.913Z</updated>
        <summary type="html"><![CDATA[Early detection of the coronavirus disease 2019 (COVID-19) helps to treat
patients timely and increase the cure rate, thus further suppressing the spread
of the disease. In this study, we propose a novel deep learning based detection
and similar case recommendation network to help control the epidemic. Our
proposed network contains two stages: the first one is a lung region
segmentation step and is used to exclude irrelevant factors, and the second is
a detection and recommendation stage. Under this framework, in the second
stage, we develop a dual-children network (DuCN) based on a pre-trained
ResNet-18 to simultaneously realize the disease diagnosis and similar case
recommendation. Besides, we employ triplet loss and intrapulmonary distance
maps to assist the detection, which helps incorporate tiny differences between
two images and is conducive to improving the diagnostic accuracy. For each
confirmed COVID-19 case, we give similar cases to provide radiologists with
diagnosis and treatment references. We conduct experiments on a large publicly
available dataset (CC-CCII) and compare the proposed model with
state-of-the-art COVID-19 detection methods. The results show that our proposed
model achieves a promising clinical performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Peng_C/0/1/0/all/0/1"&gt;Chengtao Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Long_Y/0/1/0/all/0/1"&gt;Yunfei Long&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhu_S/0/1/0/all/0/1"&gt;Senhua Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tu_D/0/1/0/all/0/1"&gt;Dandan Tu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_B/0/1/0/all/0/1"&gt;Bin Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Graph Attention Network For Microwave Imaging of Brain Anomaly. (arXiv:2108.01965v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.01965</id>
        <link href="http://arxiv.org/abs/2108.01965"/>
        <updated>2021-08-05T01:56:20.906Z</updated>
        <summary type="html"><![CDATA[So far, numerous learned models have been pressed to use in microwave imaging
problems. These models however, are oblivious to the imaging geometry. It has
always been hard to bake the physical setup of the imaging array into the
structure of the network, resulting in a data-intensive models that are not
practical. This work put forward a graph formulation of the microwave imaging
array. The architectures proposed is made cognizant of the physical setup,
allowing it to incorporate the symmetries, resulting in a less data
requirements. Graph convolution and attention mechanism is deployed to handle
the cases of fully-connected graphs corresponding to multi-static arrays. The
graph-treatment of the problem is evaluated on experimental setup in context of
brain anomaly localization with microwave imaging.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Al_Saffar_A/0/1/0/all/0/1"&gt;A. Al-Saffar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_L/0/1/0/all/0/1"&gt;L. Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abbosh_A/0/1/0/all/0/1"&gt;A. Abbosh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lung Sound Classification Using Co-tuning and Stochastic Normalization. (arXiv:2108.01991v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2108.01991</id>
        <link href="http://arxiv.org/abs/2108.01991"/>
        <updated>2021-08-05T01:56:20.900Z</updated>
        <summary type="html"><![CDATA[In this paper, we use pre-trained ResNet models as backbone architectures for
classification of adventitious lung sounds and respiratory diseases. The
knowledge of the pre-trained model is transferred by using vanilla fine-tuning,
co-tuning, stochastic normalization and the combination of the co-tuning and
stochastic normalization techniques. Furthermore, data augmentation in both
time domain and time-frequency domain is used to account for the class
imbalance of the ICBHI and our multi-channel lung sound dataset. Additionally,
we apply spectrum correction to consider the variations of the recording device
properties on the ICBHI dataset. Empirically, our proposed systems mostly
outperform all state-of-the-art lung sound classification systems for the
adventitious lung sounds and respiratory diseases of both datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Nguyen_T/0/1/0/all/0/1"&gt;Truc Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Pernkopf_F/0/1/0/all/0/1"&gt;Franz Pernkopf&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MRCpy: A Library for Minimax Risk Classifiers. (arXiv:2108.01952v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2108.01952</id>
        <link href="http://arxiv.org/abs/2108.01952"/>
        <updated>2021-08-05T01:56:20.879Z</updated>
        <summary type="html"><![CDATA[Existing libraries for supervised classification implement techniques that
are based on empirical risk minimization and utilize surrogate losses. We
present MRCpy library that implements minimax risk classifiers (MRCs) that are
based on robust risk minimization and can utilize 0-1-loss. Such techniques
give rise to a manifold of classification methods that can provide tight bounds
on the expected loss. MRCpy provides a unified interface for different variants
of MRCs and follows the standards of popular Python libraries. The presented
library also provides implementation for popular techniques that can be seen as
MRCs such as L1-regularized logistic regression, zero-one adversarial, and
maximum entropy machines. In addition, MRCpy implements recent feature mappings
such as Fourier, ReLU, and threshold features. The library is designed with an
object-oriented approach that facilitates collaborators and users.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Bondugula_K/0/1/0/all/0/1"&gt;Kartheek Bondugula&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Mazuelas_S/0/1/0/all/0/1"&gt;Santiago Mazuelas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Perez_A/0/1/0/all/0/1"&gt;Aritz P&amp;#xe9;rez&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Nonconvex Factorization and Manifold Formulations are Almost Equivalent in Low-rank Matrix Optimization. (arXiv:2108.01772v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2108.01772</id>
        <link href="http://arxiv.org/abs/2108.01772"/>
        <updated>2021-08-05T01:56:20.872Z</updated>
        <summary type="html"><![CDATA[In this paper, we consider the geometric landscape connection of the widely
studied manifold and factorization formulations in low-rank positive
semidefinite (PSD) and general matrix optimization. We establish an equivalence
on the set of first-order stationary points (FOSPs) and second-order stationary
points (SOSPs) between the manifold and the factorization formulations. We
further give a sandwich inequality on the spectrum of Riemannian and Euclidean
Hessians at FOSPs, which can be used to transfer more geometric properties from
one formulation to another. Similarities and differences on the landscape
connection under the PSD case and the general case are discussed. To the best
of our knowledge, this is the first geometric landscape connection between the
manifold and the factorization formulations for handling rank constraints. In
the general low-rank matrix optimization, the landscape connection of two
factorization formulations (unregularized and regularized ones) is also
provided. By applying these geometric landscape connections, we are able to
solve unanswered questions in literature and establish stronger results in the
applications on geometric analysis of phase retrieval, well-conditioned
low-rank matrix optimization, and the role of regularization in factorization
arising from machine learning and signal processing.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Luo_Y/0/1/0/all/0/1"&gt;Yuetian Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Li_X/0/1/0/all/0/1"&gt;Xudong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Zhang_A/0/1/0/all/0/1"&gt;Anru R. Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using Interaction Data to Predict Engagement with Interactive Media. (arXiv:2108.01949v1 [cs.HC])]]></title>
        <id>http://arxiv.org/abs/2108.01949</id>
        <link href="http://arxiv.org/abs/2108.01949"/>
        <updated>2021-08-05T01:56:20.866Z</updated>
        <summary type="html"><![CDATA[Media is evolving from traditional linear narratives to personalised
experiences, where control over information (or how it is presented) is given
to individual audience members. Measuring and understanding audience engagement
with this media is important in at least two ways: (1) a post-hoc understanding
of how engaged audiences are with the content will help production teams learn
from experience and improve future productions; (2), this type of media has
potential for real-time measures of engagement to be used to enhance the user
experience by adapting content on-the-fly. Engagement is typically measured by
asking samples of users to self-report, which is time consuming and expensive.
In some domains, however, interaction data have been used to infer engagement.
Fortuitously, the nature of interactive media facilitates a much richer set of
interaction data than traditional media; our research aims to understand if
these data can be used to infer audience engagement. In this paper, we report a
study using data captured from audience interactions with an interactive TV
show to model and predict engagement. We find that temporal metrics, including
overall time spent on the experience and the interval between events, are
predictive of engagement. The results demonstrate that interaction data can be
used to infer users' engagement during and after an experience, and the
proposed techniques are relevant to better understand audience preference and
responses.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Carlton_J/0/1/0/all/0/1"&gt;Jonathan Carlton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brown_A/0/1/0/all/0/1"&gt;Andy Brown&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jay_C/0/1/0/all/0/1"&gt;Caroline Jay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Keane_J/0/1/0/all/0/1"&gt;John Keane&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Emergent Discrete Communication in SemanticSpaces. (arXiv:2108.01828v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.01828</id>
        <link href="http://arxiv.org/abs/2108.01828"/>
        <updated>2021-08-05T01:56:20.857Z</updated>
        <summary type="html"><![CDATA[Neural agents trained in reinforcement learning settings can learn to
communicate among themselves via discrete tokens, accomplishing as a team what
agents would be unable to do alone. However, the current standard of using
one-hot vectors as discrete communication tokens prevents agents from acquiring
more desirable aspects of communication such as zero-shot understanding.
Inspired by word embedding techniques from natural language processing, we
propose neural agent architectures that enables them to communicate via
discrete tokens derived from a learned, continuous space. We show in a decision
theoretic framework that our technique optimizes communication over a wide
range of scenarios, whereas one-hot tokens are only optimal under restrictive
assumptions. In self-play experiments, we validate that our trained agents
learn to cluster tokens in semantically-meaningful ways, allowing them
communicate in noisy environments where other techniques fail. Lastly, we
demonstrate both that agents using our method can effectively respond to novel
human communication and that humans can understand unlabeled emergent agent
communication, outperforming the use of one-hot communication.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tucker_M/0/1/0/all/0/1"&gt;Mycal Tucker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Huao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agrawal_S/0/1/0/all/0/1"&gt;Siddharth Agrawal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hughes_D/0/1/0/all/0/1"&gt;Dana Hughes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sycara_K/0/1/0/all/0/1"&gt;Katia Sycara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lewis_M/0/1/0/all/0/1"&gt;Michael Lewis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shah_J/0/1/0/all/0/1"&gt;Julie Shah&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How to Query Language Models?. (arXiv:2108.01928v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.01928</id>
        <link href="http://arxiv.org/abs/2108.01928"/>
        <updated>2021-08-05T01:56:20.849Z</updated>
        <summary type="html"><![CDATA[Large pre-trained language models (LMs) are capable of not only recovering
linguistic but also factual and commonsense knowledge. To access the knowledge
stored in mask-based LMs, we can use cloze-style questions and let the model
fill in the blank. The flexibility advantage over structured knowledge bases
comes with the drawback of finding the right query for a certain information
need. Inspired by human behavior to disambiguate a question, we propose to
query LMs by example. To clarify the ambivalent question "Who does Neuer play
for?", a successful strategy is to demonstrate the relation using another
subject, e.g., "Ronaldo plays for Portugal. Who does Neuer play for?". We apply
this approach of querying by example to the LAMA probe and obtain substantial
improvements of up to 37.8% for BERT-large on the T-REx data when providing
only 10 demonstrations--even outperforming a baseline that queries the model
with up to 40 paraphrases of the question. The examples are provided through
the model's context and thus require neither fine-tuning nor an additional
forward pass. This suggests that LMs contain more factual and commonsense
knowledge than previously assumed--if we query the model in the right way.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Adolphs_L/0/1/0/all/0/1"&gt;Leonard Adolphs&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dhuliawala_S/0/1/0/all/0/1"&gt;Shehzaad Dhuliawala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hofmann_T/0/1/0/all/0/1"&gt;Thomas Hofmann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Barrier Certificates: Towards Safe Reinforcement Learning with Zero Training-time Violations. (arXiv:2108.01846v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.01846</id>
        <link href="http://arxiv.org/abs/2108.01846"/>
        <updated>2021-08-05T01:56:20.821Z</updated>
        <summary type="html"><![CDATA[Training-time safety violations have been a major concern when we deploy
reinforcement learning algorithms in the real world. This paper explores the
possibility of safe RL algorithms with zero training-time safety violations in
the challenging setting where we are only given a safe but trivial-reward
initial policy without any prior knowledge of the dynamics model and additional
offline data. We propose an algorithm, Co-trained Barrier Certificate for Safe
RL (CRABS), which iteratively learns barrier certificates, dynamics models, and
policies. The barrier certificates, learned via adversarial training, ensure
the policy's safety assuming calibrated learned dynamics model. We also add a
regularization term to encourage larger certified regions to enable better
exploration. Empirical simulations show that zero safety violations are already
challenging for a suite of simple environments with only 2-4 dimensional state
space, especially if high-reward policies have to visit regions near the safety
boundary. Prior methods require hundreds of violations to achieve decent
rewards on these tasks, whereas our proposed algorithms incur zero violations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1"&gt;Yuping Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1"&gt;Tengyu Ma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Pragmatic Look at Deep Imitation Learning. (arXiv:2108.01867v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.01867</id>
        <link href="http://arxiv.org/abs/2108.01867"/>
        <updated>2021-08-05T01:56:20.813Z</updated>
        <summary type="html"><![CDATA[The introduction of the generative adversarial imitation learning (GAIL)
algorithm has spurred the development of scalable imitation learning approaches
using deep neural networks. The GAIL objective can be thought of as 1) matching
the expert policy's state distribution; 2) penalising the learned policy's
state distribution; and 3) maximising entropy. While theoretically motivated,
in practice GAIL can be difficult to apply, not least due to the instabilities
of adversarial training. In this paper, we take a pragmatic look at GAIL and
related imitation learning algorithms. We implement and automatically tune a
range of algorithms in a unified experimental setup, presenting a fair
evaluation between the competing methods. From our results, our primary
recommendation is to consider non-adversarial methods. Furthermore, we discuss
the common components of imitation learning objectives, and present promising
avenues for future research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Arulkumaran_K/0/1/0/all/0/1"&gt;Kai Arulkumaran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lillrank_D/0/1/0/all/0/1"&gt;Dan Ogawa Lillrank&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reconstructing a dynamical system and forecasting time series by self-consistent deep learning. (arXiv:2108.01862v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.01862</id>
        <link href="http://arxiv.org/abs/2108.01862"/>
        <updated>2021-08-05T01:56:20.807Z</updated>
        <summary type="html"><![CDATA[We introduce a self-consistent deep-learning framework which, for a noisy
deterministic time series, provides unsupervised filtering, state-space
reconstruction, identification of the underlying differential equations and
forecasting. Without a priori information on the signal, we embed the time
series in a state space, where deterministic structures, i.e. attractors, are
revealed. Under the assumption that the evolution of solution trajectories is
described by an unknown dynamical system, we filter out stochastic outliers.
The embedding function, the solution trajectories and the dynamical systems are
constructed using deep neural networks, respectively. By exploiting the
differentiability of the neural solution trajectory, the neural dynamical
system is defined locally at each time, mitigating the need for propagating
gradients through numerical solvers. On a chaotic time series masked by
additive Gaussian noise, we demonstrate the filtering ability and the
predictive power of the proposed framework.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhe Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guet_C/0/1/0/all/0/1"&gt;Claude Guet&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Task Agnostic Skills with Data-driven Guidance. (arXiv:2108.01869v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2108.01869</id>
        <link href="http://arxiv.org/abs/2108.01869"/>
        <updated>2021-08-05T01:56:20.801Z</updated>
        <summary type="html"><![CDATA[To increase autonomy in reinforcement learning, agents need to learn useful
behaviours without reliance on manually designed reward functions. To that end,
skill discovery methods have been used to learn the intrinsic options available
to an agent using task-agnostic objectives. However, without the guidance of
task-specific rewards, emergent behaviours are generally useless due to the
under-constrained problem of skill discovery in complex and high-dimensional
spaces. This paper proposes a framework for guiding the skill discovery towards
the subset of expert-visited states using a learned state projection. We apply
our method in various reinforcement learning (RL) tasks and show that such a
projection results in more useful behaviours.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Klemsdal_E/0/1/0/all/0/1"&gt;Even Klemsdal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Herland_S/0/1/0/all/0/1"&gt;Sverre Herland&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Murad_A/0/1/0/all/0/1"&gt;Abdulmajid Murad&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PARADISE: Exploiting Parallel Data for Multilingual Sequence-to-Sequence Pretraining. (arXiv:2108.01887v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.01887</id>
        <link href="http://arxiv.org/abs/2108.01887"/>
        <updated>2021-08-05T01:56:20.794Z</updated>
        <summary type="html"><![CDATA[Despite the success of multilingual sequence-to-sequence pretraining, most
existing approaches rely on monolingual corpora, and do not make use of the
strong cross-lingual signal contained in parallel data. In this paper, we
present PARADISE (PARAllel & Denoising Integration in SEquence-to-sequence
models), which extends the conventional denoising objective used to train these
models by (i) replacing words in the noised sequence according to a
multilingual dictionary, and (ii) predicting the reference translation
according to a parallel corpus instead of recovering the original sequence. Our
experiments on machine translation and cross-lingual natural language inference
show an average improvement of 2.0 BLEU points and 6.7 accuracy points from
integrating parallel data into pretraining, respectively, obtaining results
that are competitive with several popular models at a fraction of their
computational cost.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Reid_M/0/1/0/all/0/1"&gt;Machel Reid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Artetxe_M/0/1/0/all/0/1"&gt;Mikel Artetxe&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[High Performance Across Two Atari Paddle Games Using the Same Perceptual Control Architecture Without Training. (arXiv:2108.01895v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.01895</id>
        <link href="http://arxiv.org/abs/2108.01895"/>
        <updated>2021-08-05T01:56:20.785Z</updated>
        <summary type="html"><![CDATA[Deep reinforcement learning (DRL) requires large samples and a long training
time to operate optimally. Yet humans rarely require long periods training to
perform well on novel tasks, such as computer games, once they are provided
with an accurate program of instructions. We used perceptual control theory
(PCT) to construct a simple closed-loop model which requires no training
samples and training time within a video game study using the Arcade Learning
Environment (ALE). The model was programmed to parse inputs from the
environment into hierarchically organised perceptual signals, and it computed a
dynamic error signal by subtracting the incoming signal for each perceptual
variable from a reference signal to drive output signals to reduce this error.
We tested the same model across two different Atari paddle games Breakout and
Pong to achieve performance at least as high as DRL paradigms, and close to
good human performance. Our study shows that perceptual control models, based
on simple assumptions, can perform well without learning. We conclude by
specifying a parsimonious role of learning that may be more similar to
psychological functioning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gulrez_T/0/1/0/all/0/1"&gt;Tauseef Gulrez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mansell_W/0/1/0/all/0/1"&gt;Warren Mansell&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sparse Continuous Distributions and Fenchel-Young Losses. (arXiv:2108.01988v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.01988</id>
        <link href="http://arxiv.org/abs/2108.01988"/>
        <updated>2021-08-05T01:56:20.766Z</updated>
        <summary type="html"><![CDATA[Exponential families are widely used in machine learning; they include many
distributions in continuous and discrete domains (e.g., Gaussian, Dirichlet,
Poisson, and categorical distributions via the softmax transformation).
Distributions in each of these families have fixed support. In contrast, for
finite domains, there has been recent works on sparse alternatives to softmax
(e.g. sparsemax, $\alpha$-entmax, and fusedmax) and corresponding losses, which
have varying support.

This paper expands that line of work in several directions: first, it extends
$\Omega$-regularized prediction maps and Fenchel-Young losses to arbitrary
domains (possibly countably infinite or continuous). For linearly parametrized
families, we show that minimization of Fenchel-Young losses is equivalent to
moment matching of the statistics, generalizing a fundamental property of
exponential families. When $\Omega$ is a Tsallis negentropy with parameter
$\alpha$, we obtain "deformed exponential families," which include
$\alpha$-entmax and sparsemax ($\alpha$ = 2) as particular cases. For quadratic
energy functions in continuous domains, the resulting densities are
$\beta$-Gaussians, an instance of elliptical distributions that contain as
particular cases the Gaussian, biweight, triweight and Epanechnikov densities,
and for which we derive closed-form expressions for the variance, Tsallis
entropy, and Fenchel-Young loss. When $\Omega$ is a total variation or Sobolev
regularizer, we obtain a continuous version of the fusedmax. Finally, we
introduce continuous-domain attention mechanisms, deriving efficient gradient
backpropagation algorithms for $\alpha \in \{1, 4/3, 3/2, 2\}$. Using them, we
demonstrate our sparse continuous distributions for attention-based audio
classification and visual question answering, showing that they allow attending
to time intervals and compact regions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Martins_A/0/1/0/all/0/1"&gt;Andr&amp;#xe9; F. T. Martins&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Treviso_M/0/1/0/all/0/1"&gt;Marcos Treviso&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Farinhas_A/0/1/0/all/0/1"&gt;Ant&amp;#xf3;nio Farinhas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aguiar_P/0/1/0/all/0/1"&gt;Pedro M. Q. Aguiar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Figueiredo_M/0/1/0/all/0/1"&gt;M&amp;#xe1;rio A. T. Figueiredo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Blondel_M/0/1/0/all/0/1"&gt;Mathieu Blondel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niculae_V/0/1/0/all/0/1"&gt;Vlad Niculae&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generic Neural Architecture Search via Regression. (arXiv:2108.01899v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.01899</id>
        <link href="http://arxiv.org/abs/2108.01899"/>
        <updated>2021-08-05T01:56:20.754Z</updated>
        <summary type="html"><![CDATA[Most existing neural architecture search (NAS) algorithms are dedicated to
the downstream tasks, e.g., image classification in computer vision. However,
extensive experiments have shown that, prominent neural architectures, such as
ResNet in computer vision and LSTM in natural language processing, are
generally good at extracting patterns from the input data and perform well on
different downstream tasks. These observations inspire us to ask: Is it
necessary to use the performance of specific downstream tasks to evaluate and
search for good neural architectures? Can we perform NAS effectively and
efficiently while being agnostic to the downstream task? In this work, we
attempt to affirmatively answer the above two questions and improve the
state-of-the-art NAS solution by proposing a novel and generic NAS framework,
termed Generic NAS (GenNAS). GenNAS does not use task-specific labels but
instead adopts \textit{regression} on a set of manually designed synthetic
signal bases for architecture evaluation. Such a self-supervised regression
task can effectively evaluate the intrinsic power of an architecture to capture
and transform the input signal patterns, and allow more sufficient usage of
training samples. We then propose an automatic task search to optimize the
combination of synthetic signals using limited downstream-task-specific labels,
further improving the performance of GenNAS. We also thoroughly evaluate
GenNAS's generality and end-to-end NAS performance on all search spaces, which
outperforms almost all existing works with significant speedup.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yuhong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hao_C/0/1/0/all/0/1"&gt;Cong Hao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1"&gt;Pan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_J/0/1/0/all/0/1"&gt;Jinjun Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1"&gt;Deming Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Staged trees and asymmetry-labeled DAGs. (arXiv:2108.01994v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2108.01994</id>
        <link href="http://arxiv.org/abs/2108.01994"/>
        <updated>2021-08-05T01:56:20.748Z</updated>
        <summary type="html"><![CDATA[Bayesian networks are a widely-used class of probabilistic graphical models
capable of representing symmetric conditional independence between variables of
interest using the topology of the underlying graph. They can be seen as a
special case of the much more general class of models called staged trees,
which can represent any type of non-symmetric conditional independence. Here we
formalize the relationship between these two models and introduce a minimal
Bayesian network representation of the staged tree, which can be used to read
conditional independences in an intuitive way. Furthermore, we define a new
labeled graph, termed asymmetry-labeled directed acyclic graph, whose edges are
labeled to denote the type of dependence existing between any two random
variables. Various datasets are used to illustrate the methodology,
highlighting the need to construct models which more flexibly encode and
represent non-symmetric structures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Varando_G/0/1/0/all/0/1"&gt;Gherardo Varando&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Carli_F/0/1/0/all/0/1"&gt;Federico Carli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Leonelli_M/0/1/0/all/0/1"&gt;Manuele Leonelli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Approximating Attributed Incentive Salience In Large Scale Scenarios. A Representation Learning Approach Based on Artificial Neural Networks. (arXiv:2108.01724v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.01724</id>
        <link href="http://arxiv.org/abs/2108.01724"/>
        <updated>2021-08-05T01:56:20.739Z</updated>
        <summary type="html"><![CDATA[Incentive salience attribution can be understood as a psychobiological
process ascribing relevance to potentially rewarding objects and actions.
Despite being an important component of the motivational process guiding our
everyday behaviour its study in naturalistic contexts is not straightforward.
Here we propose a methodology based on artificial neural networks (ANNs) for
approximating latent states produced by this process in situations where large
volumes of behavioural data are available but no strict experimental control is
possible. Leveraging knowledge derived from theoretical and computational
accounts of incentive salience attribution we designed an ANN for estimating
duration and intensity of future interactions between individuals and a series
of video games in a large-scale ($N> 3 \times 10^6$) longitudinal dataset.
Through model comparison and inspection we show that our approach outperforms
competing ones while also generating a representation that well approximate
some of the functions of attributed incentive salience. We discuss our findings
with reference to the adopted theoretical and computational frameworks and
suggest how our methodology could be an initial step for estimating attributed
incentive salience in large scale behavioural studies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bonometti_V/0/1/0/all/0/1"&gt;Valerio Bonometti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ruiz_M/0/1/0/all/0/1"&gt;Mathieu J. Ruiz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Drachen_A/0/1/0/all/0/1"&gt;Anders Drachen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wade_A/0/1/0/all/0/1"&gt;Alex Wade&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Model-Based Opponent Modeling. (arXiv:2108.01843v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.01843</id>
        <link href="http://arxiv.org/abs/2108.01843"/>
        <updated>2021-08-05T01:56:20.730Z</updated>
        <summary type="html"><![CDATA[When one agent interacts with a multi-agent environment, it is challenging to
deal with various opponents unseen before. Modeling the behaviors, goals, or
beliefs of opponents could help the agent adjust its policy to adapt to
different opponents. In addition, it is also important to consider opponents
who are learning simultaneously or capable of reasoning. However, existing work
usually tackles only one of the aforementioned types of opponent. In this
paper, we propose model-based opponent modeling (MBOM), which employs the
environment model to adapt to all kinds of opponent. MBOM simulates the
recursive reasoning process in the environment model and imagines a set of
improving opponent policies. To effectively and accurately represent the
opponent policy, MBOM further mixes the imagined opponent policies according to
the similarity with the real behaviors of opponents. Empirically, we show that
MBOM achieves more effective adaptation than existing methods in competitive
and cooperative environments, respectively with different types of opponent,
i.e., fixed policy, na\"ive learner, and reasoning learner.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1"&gt;Xiaopeng Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1"&gt;Jiechuan Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1"&gt;Haobin Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1"&gt;Zongqing Lu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Effective Leaf Recognition Using Convolutional Neural Networks Based Features. (arXiv:2108.01808v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.01808</id>
        <link href="http://arxiv.org/abs/2108.01808"/>
        <updated>2021-08-05T01:56:20.712Z</updated>
        <summary type="html"><![CDATA[There is a warning light for the loss of plant habitats worldwide that
entails concerted efforts to conserve plant biodiversity. Thus, plant species
classification is of crucial importance to address this environmental
challenge. In recent years, there is a considerable increase in the number of
studies related to plant taxonomy. While some researchers try to improve their
recognition performance using novel approaches, others concentrate on
computational optimization for their framework. In addition, a few studies are
diving into feature extraction to gain significantly in terms of accuracy. In
this paper, we propose an effective method for the leaf recognition problem. In
our proposed approach, a leaf goes through some pre-processing to extract its
refined color image, vein image, xy-projection histogram, handcrafted shape,
texture features, and Fourier descriptors. These attributes are then
transformed into a better representation by neural network-based encoders
before a support vector machine (SVM) model is utilized to classify different
leaves. Overall, our approach performs a state-of-the-art result on the Flavia
leaf dataset, achieving the accuracy of 99.58\% on test sets under random
10-fold cross-validation and bypassing the previous methods. We also release
our codes\footnote{Scripts are available at
\url{https://github.com/dinhvietcuong1996/LeafRecognition}} for contributing to
the research community in the leaf classification problem.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Quach_B/0/1/0/all/0/1"&gt;Boi M. Quach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cuong_D/0/1/0/all/0/1"&gt;Dinh V. Cuong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pham_N/0/1/0/all/0/1"&gt;Nhung Pham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huynh_D/0/1/0/all/0/1"&gt;Dang Huynh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_B/0/1/0/all/0/1"&gt;Binh T. Nguyen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scalable Community Detection via Parallel Correlation Clustering. (arXiv:2108.01731v1 [cs.SI])]]></title>
        <id>http://arxiv.org/abs/2108.01731</id>
        <link href="http://arxiv.org/abs/2108.01731"/>
        <updated>2021-08-05T01:56:20.704Z</updated>
        <summary type="html"><![CDATA[Graph clustering and community detection are central problems in modern data
mining. The increasing need for analyzing billion-scale data calls for faster
and more scalable algorithms for these problems. There are certain trade-offs
between the quality and speed of such clustering algorithms. In this paper, we
design scalable algorithms that achieve high quality when evaluated based on
ground truth. We develop a generalized sequential and shared-memory parallel
framework based on the LambdaCC objective (introduced by Veldt et al.), which
encompasses modularity and correlation clustering. Our framework consists of
highly-optimized implementations that scale to large data sets of billions of
edges and that obtain high-quality clusters compared to ground-truth data, on
both unweighted and weighted graphs. Our empirical evaluation shows that this
framework improves the state-of-the-art trade-offs between speed and quality of
scalable community detection. For example, on a 30-core machine with two-way
hyper-threading, our implementations achieve orders of magnitude speedups over
other correlation clustering baselines, and up to 28.44x speedups over our own
sequential baselines while maintaining or improving quality.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1"&gt;Jessica Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dhulipala_L/0/1/0/all/0/1"&gt;Laxman Dhulipala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eisenstat_D/0/1/0/all/0/1"&gt;David Eisenstat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lacki_J/0/1/0/all/0/1"&gt;Jakub &amp;#x141;&amp;#x105;cki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mirrokni_V/0/1/0/all/0/1"&gt;Vahab Mirrokni&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Risk Conditioned Neural Motion Planning. (arXiv:2108.01851v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.01851</id>
        <link href="http://arxiv.org/abs/2108.01851"/>
        <updated>2021-08-05T01:56:20.698Z</updated>
        <summary type="html"><![CDATA[Risk-bounded motion planning is an important yet difficult problem for
safety-critical tasks. While existing mathematical programming methods offer
theoretical guarantees in the context of constrained Markov decision processes,
they either lack scalability in solving larger problems or produce conservative
plans. Recent advances in deep reinforcement learning improve scalability by
learning policy networks as function approximators. In this paper, we propose
an extension of soft actor critic model to estimate the execution risk of a
plan through a risk critic and produce risk-bounded policies efficiently by
adding an extra risk term in the loss function of the policy network. We define
the execution risk in an accurate form, as opposed to approximating it through
a summation of immediate risks at each time step that leads to conservative
plans. Our proposed model is conditioned on a continuous spectrum of risk
bounds, allowing the user to adjust the risk-averse level of the agent on the
fly. Through a set of experiments, we show the advantage of our model in terms
of both computational time and plan quality, compared to a state-of-the-art
mathematical programming baseline, and validate its performance in more
complicated scenarios, including nonlinear dynamics and larger state space.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1"&gt;Xin Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_M/0/1/0/all/0/1"&gt;Meng Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jasour_A/0/1/0/all/0/1"&gt;Ashkan Jasour&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rosman_G/0/1/0/all/0/1"&gt;Guy Rosman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Williams_B/0/1/0/all/0/1"&gt;Brian Williams&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Empirical Evaluation of End-to-End Polyphonic Optical Music Recognition. (arXiv:2108.01769v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.01769</id>
        <link href="http://arxiv.org/abs/2108.01769"/>
        <updated>2021-08-05T01:56:20.690Z</updated>
        <summary type="html"><![CDATA[Previous work has shown that neural architectures are able to perform optical
music recognition (OMR) on monophonic and homophonic music with high accuracy.
However, piano and orchestral scores frequently exhibit polyphonic passages,
which add a second dimension to the task. Monophonic and homophonic music can
be described as homorhythmic, or having a single musical rhythm. Polyphonic
music, on the other hand, can be seen as having multiple rhythmic sequences, or
voices, concurrently. We first introduce a workflow for creating large-scale
polyphonic datasets suitable for end-to-end recognition from sheet music
publicly available on the MuseScore forum. We then propose two novel
formulations for end-to-end polyphonic OMR -- one treating the problem as a
type of multi-task binary classification, and the other treating it as
multi-sequence detection. Building upon the encoder-decoder architecture and an
image encoder proposed in past work on end-to-end OMR, we propose two novel
decoder models -- FlagDecoder and RNNDecoder -- that correspond to the two
formulations. Finally, we compare the empirical performance of these end-to-end
approaches to polyphonic OMR and observe a new state-of-the-art performance
with our multi-sequence detection decoder, RNNDecoder.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Edirisooriya_S/0/1/0/all/0/1"&gt;Sachinda Edirisooriya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1"&gt;Hao-Wen Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McAuley_J/0/1/0/all/0/1"&gt;Julian McAuley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berg_Kirkpatrick_T/0/1/0/all/0/1"&gt;Taylor Berg-Kirkpatrick&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Offline Decentralized Multi-Agent Reinforcement Learning. (arXiv:2108.01832v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.01832</id>
        <link href="http://arxiv.org/abs/2108.01832"/>
        <updated>2021-08-05T01:56:20.684Z</updated>
        <summary type="html"><![CDATA[In many real-world multi-agent cooperative tasks, due to high cost and risk,
agents cannot interact with the environment and collect experiences during
learning, but have to learn from offline datasets. However, the transition
probabilities calculated from the dataset can be much different from the
transition probabilities induced by the learned policies of other agents,
creating large errors in value estimates. Moreover, the experience
distributions of agents' datasets may vary wildly due to diverse behavior
policies, causing large difference in value estimates between agents.
Consequently, agents will learn uncoordinated suboptimal policies. In this
paper, we propose MABCQ, which exploits value deviation and transition
normalization to modify the transition probabilities. Value deviation
optimistically increases the transition probabilities of high-value next
states, and transition normalization normalizes the biased transition
probabilities of next states. They together encourage agents to discover
potential optimal and coordinated policies. Mathematically, we prove the
convergence of Q-learning under the non-stationary transition probabilities
after modification. Empirically, we show that MABCQ greatly outperforms
baselines and reduces the difference in value estimates between agents.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1"&gt;Jiechuan Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1"&gt;Zongqing Lu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HTTP2vec: Embedding of HTTP Requests for Detection of Anomalous Traffic. (arXiv:2108.01763v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.01763</id>
        <link href="http://arxiv.org/abs/2108.01763"/>
        <updated>2021-08-05T01:56:20.678Z</updated>
        <summary type="html"><![CDATA[Hypertext transfer protocol (HTTP) is one of the most widely used protocols
on the Internet. As a consequence, most attacks (i.e., SQL injection, XSS) use
HTTP as the transport mechanism. Therefore, it is crucial to develop an
intelligent solution that would allow to effectively detect and filter out
anomalies in HTTP traffic. Currently, most of the anomaly detection systems are
either rule-based or trained using manually selected features. We propose
utilizing modern unsupervised language representation model for embedding HTTP
requests and then using it to classify anomalies in the traffic. The solution
is motivated by methods used in Natural Language Processing (NLP) such as
Doc2Vec which could potentially capture the true understanding of HTTP
messages, and therefore improve the efficiency of Intrusion Detection System.
In our work, we not only aim at generating a suitable embedding space, but also
at the interpretability of the proposed model. We decided to use the current
state-of-the-art RoBERTa, which, as far as we know, has never been used in a
similar problem. To verify how the solution would work in real word conditions,
we train the model using only legitimate traffic. We also try to explain the
results based on clusters that occur in the vectorized requests space and a
simple logistic regression classifier. We compared our approach with the
similar, previously proposed methods. We evaluate the feasibility of our method
on three different datasets: CSIC2010, CSE-CIC-IDS2018 and one that we prepared
ourselves. The results we show are comparable to others or better, and most
importantly - interpretable.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gniewkowski_M/0/1/0/all/0/1"&gt;Mateusz Gniewkowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maciejewski_H/0/1/0/all/0/1"&gt;Henryk Maciejewski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Surmacz_T/0/1/0/all/0/1"&gt;Tomasz R. Surmacz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Walentynowicz_W/0/1/0/all/0/1"&gt;Wiktor Walentynowicz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[I2V-GAN: Unpaired Infrared-to-Visible Video Translation. (arXiv:2108.00913v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2108.00913</id>
        <link href="http://arxiv.org/abs/2108.00913"/>
        <updated>2021-08-05T01:56:20.655Z</updated>
        <summary type="html"><![CDATA[Human vision is often adversely affected by complex environmental factors,
especially in night vision scenarios. Thus, infrared cameras are often
leveraged to help enhance the visual effects via detecting infrared radiation
in the surrounding environment, but the infrared videos are undesirable due to
the lack of detailed semantic information. In such a case, an effective
video-to-video translation method from the infrared domain to the visible light
counterpart is strongly needed by overcoming the intrinsic huge gap between
infrared and visible fields. To address this challenging problem, we propose an
infrared-to-visible (I2V) video translation method I2V-GAN to generate
fine-grained and spatial-temporal consistent visible light videos by given
unpaired infrared videos. Technically, our model capitalizes on three types of
constraints: 1)adversarial constraint to generate synthetic frames that are
similar to the real ones, 2)cyclic consistency with the introduced perceptual
loss for effective content conversion as well as style preservation, and
3)similarity constraints across and within domains to enhance the content and
motion consistency in both spatial and temporal spaces at a fine-grained level.
Furthermore, the current public available infrared and visible light datasets
are mainly used for object detection or tracking, and some are composed of
discontinuous images which are not suitable for video tasks. Thus, we provide a
new dataset for I2V video translation, which is named IRVI. Specifically, it
has 12 consecutive video clips of vehicle and monitoring scenes, and both
infrared and visible light videos could be apart into 24352 frames.
Comprehensive experiments validate that I2V-GAN is superior to the compared
SOTA methods in the translation of I2V videos with higher fluency and finer
semantic details. The code and IRVI dataset are available at
https://github.com/BIT-DA/I2V-GAN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shuang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1"&gt;Bingfeng Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1"&gt;Zhenjie Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1"&gt;Chi Harold Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1"&gt;Kai Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shuigen Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Study on Herd Behavior Using Sentiment Analysis in Online Social Network. (arXiv:2108.01728v1 [cs.SI])]]></title>
        <id>http://arxiv.org/abs/2108.01728</id>
        <link href="http://arxiv.org/abs/2108.01728"/>
        <updated>2021-08-05T01:56:20.648Z</updated>
        <summary type="html"><![CDATA[Social media platforms are thriving nowadays, so a huge volume of data is
produced. As it includes brief and clear statements, millions of people post
their thoughts on microblogging sites every day. This paper represents and
analyze the capacity of diverse strategies to volumetric, delicate, and social
networks to predict critical opinions from online social networking sites. In
the exploration of certain searching for relevant, the thoughts of people play
a crucial role. Social media becomes a good outlet since the last decades to
share the opinions globally. Sentiment analysis as well as opinion mining is a
tool that is used to extract the opinions or thoughts of the common public. An
occurrence in one place, be it economic, political, or social, may trigger
large-scale chain public reaction across many other sites in an increasingly
interconnected world. This study demonstrates the evaluation of sentiment
analysis techniques using social media contents and creating the association
between subjectivity with herd behavior and clustering coefficient as well as
tries to predict the election result (2021 election in West Bengal). This is an
implementation of sentiment analysis targeted at estimating the results of an
upcoming election by assessing the public's opinion across social media. This
paper also has a short discussion section on the usefulness of the idea in
other fields.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dutta_S/0/1/0/all/0/1"&gt;Suchandra Dutta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sarkar_D/0/1/0/all/0/1"&gt;Dhrubasish Sarkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1"&gt;Sohom Roy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kole_D/0/1/0/all/0/1"&gt;Dipak K. Kole&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jana_P/0/1/0/all/0/1"&gt;Premananda Jana&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Learning Chromatic and Clique Numbers of Graphs. (arXiv:2108.01810v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.01810</id>
        <link href="http://arxiv.org/abs/2108.01810"/>
        <updated>2021-08-05T01:56:20.642Z</updated>
        <summary type="html"><![CDATA[Deep neural networks have been applied to a wide range of problems across
different application domains with great success. Recently, research into
combinatorial optimization problems in particular has generated much interest
in the machine learning community. In this work, we develop deep learning
models to predict the chromatic number and maximum clique size of graphs, both
of which represent classical NP-complete combinatorial optimization problems
encountered in graph theory. The neural networks are trained using the most
basic representation of the graph, the adjacency matrix, as opposed to
undergoing complex domain-specific feature engineering. The experimental
results show that deep neural networks, and in particular convolutional neural
networks, obtain strong performance on this problem.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hulse_J/0/1/0/all/0/1"&gt;Jason Van Hulse&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Friedman_J/0/1/0/all/0/1"&gt;Joshua S. Friedman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Personalized Federated Learning with Clustering: Non-IID Heart Rate Variability Data Application. (arXiv:2108.01903v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.01903</id>
        <link href="http://arxiv.org/abs/2108.01903"/>
        <updated>2021-08-05T01:56:20.633Z</updated>
        <summary type="html"><![CDATA[While machine learning techniques are being applied to various fields for
their exceptional ability to find complex relations in large datasets, the
strengthening of regulations on data ownership and privacy is causing
increasing difficulty in its application to medical data. In light of this,
Federated Learning has recently been proposed as a solution to train on private
data without breach of confidentiality. This conservation of privacy is
particularly appealing in the field of healthcare, where patient data is highly
confidential. However, many studies have shown that its assumption of
Independent and Identically Distributed data is unrealistic for medical data.
In this paper, we propose Personalized Federated Cluster Models, a hierarchical
clustering-based FL process, to predict Major Depressive Disorder severity from
Heart Rate Variability. By allowing clients to receive more personalized model,
we address problems caused by non-IID data, showing an accuracy increase in
severity prediction. This increase in performance may be sufficient to use
Personalized Federated Cluster Models in many existing Federated Learning
scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yoo_J/0/1/0/all/0/1"&gt;Joo Hun Yoo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Son_H/0/1/0/all/0/1"&gt;Ha Min Son&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jeong_H/0/1/0/all/0/1"&gt;Hyejun Jeong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jang_E/0/1/0/all/0/1"&gt;Eun-Hye Jang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_A/0/1/0/all/0/1"&gt;Ah Young Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1"&gt;Han Young Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jeon_H/0/1/0/all/0/1"&gt;Hong Jin Jeon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chung_T/0/1/0/all/0/1"&gt;Tai-Myoung Chung&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Monte Carlo Tree Search for high precision manufacturing. (arXiv:2108.01789v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2108.01789</id>
        <link href="http://arxiv.org/abs/2108.01789"/>
        <updated>2021-08-05T01:56:20.600Z</updated>
        <summary type="html"><![CDATA[Monte Carlo Tree Search (MCTS) has shown its strength for a lot of
deterministic and stochastic examples, but literature lacks reports of
applications to real world industrial processes. Common reasons for this are
that there is no efficient simulator of the process available or there exist
problems in applying MCTS to the complex rules of the process. In this paper,
we apply MCTS for optimizing a high-precision manufacturing process that has
stochastic and partially observable outcomes. We make use of an
expert-knowledge-based simulator and adapt the MCTS default policy to deal with
the manufacturing process.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Weichert_D/0/1/0/all/0/1"&gt;Dorina Weichert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Horchler_F/0/1/0/all/0/1"&gt;Felix Horchler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kister_A/0/1/0/all/0/1"&gt;Alexander Kister&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Trost_M/0/1/0/all/0/1"&gt;Marcus Trost&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hartung_J/0/1/0/all/0/1"&gt;Johannes Hartung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Risse_S/0/1/0/all/0/1"&gt;Stefan Risse&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Neural Architecture Search with Performance Prediction. (arXiv:2108.01854v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.01854</id>
        <link href="http://arxiv.org/abs/2108.01854"/>
        <updated>2021-08-05T01:56:20.593Z</updated>
        <summary type="html"><![CDATA[Neural networks are powerful models that have a remarkable ability to extract
patterns that are too complex to be noticed by humans or other machine learning
models. Neural networks are the first class of models that can train end-to-end
systems with large learning capacities. However, we still have the difficult
challenge of designing the neural network, which requires human experience and
a long process of trial and error. As a solution, we can use a neural
architecture search to find the best network architecture for the task at hand.
Existing NAS algorithms generally evaluate the fitness of a new architecture by
fully training from scratch, resulting in the prohibitive computational cost,
even if operated on high-performance computers. In this paper, an end-to-end
offline performance predictor is proposed to accelerate the evaluation of
sampled architectures.

Index Terms- Learning Curve Prediction, Neural Architecture Search,
Reinforcement Learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alshubaily_I/0/1/0/all/0/1"&gt;Ibrahim Alshubaily&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Categorical EHR Imputation with Generative Adversarial Nets. (arXiv:2108.01701v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.01701</id>
        <link href="http://arxiv.org/abs/2108.01701"/>
        <updated>2021-08-05T01:56:20.586Z</updated>
        <summary type="html"><![CDATA[Electronic Health Records often suffer from missing data, which poses a major
problem in clinical practice and clinical studies. A novel approach for dealing
with missing data are Generative Adversarial Nets (GANs), which have been
generating huge research interest in image generation and transformation.
Recently, researchers have attempted to apply GANs to missing data generation
and imputation for EHR data: a major challenge here is the categorical nature
of the data. State-of-the-art solutions to the GAN-based generation of
categorical data involve either reinforcement learning, or learning a
bidirectional mapping between the categorical and the real latent feature
space, so that the GANs only need to generate real-valued features. However,
these methods are designed to generate complete feature vectors instead of
imputing only the subsets of missing features. In this paper we propose a
simple and yet effective approach that is based on previous work on GANs for
data imputation. We first motivate our solution by discussing the reason why
adversarial training often fails in case of categorical features. Then we
derive a novel way to re-code the categorical features to stabilize the
adversarial training. Based on experiments on two real-world EHR data with
multiple settings, we show that our imputation approach largely improves the
prediction accuracy, compared to more traditional data imputation approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yinchong Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Zhiilang Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tresp_V/0/1/0/all/0/1"&gt;Volker Tresp&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fasching_P/0/1/0/all/0/1"&gt;Peter A. Fasching&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Music Performance Assessment with Contrastive Learning. (arXiv:2108.01711v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2108.01711</id>
        <link href="http://arxiv.org/abs/2108.01711"/>
        <updated>2021-08-05T01:56:20.561Z</updated>
        <summary type="html"><![CDATA[Several automatic approaches for objective music performance assessment (MPA)
have been proposed in the past, however, existing systems are not yet capable
of reliably predicting ratings with the same accuracy as professional judges.
This study investigates contrastive learning as a potential method to improve
existing MPA systems. Contrastive learning is a widely used technique in
representation learning to learn a structured latent space capable of
separately clustering multiple classes. It has been shown to produce state of
the art results for image-based classification problems. We introduce a
weighted contrastive loss suitable for regression tasks applied to a
convolutional neural network and show that contrastive loss results in
performance gains in regression tasks for MPA. Our results show that
contrastive-based methods are able to match and exceed SoTA performance for MPA
regression tasks by creating better class clusters within the latent space of
the neural networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Seshadri_P/0/1/0/all/0/1"&gt;Pavan Seshadri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lerch_A/0/1/0/all/0/1"&gt;Alexander Lerch&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predicting Zip Code-Level Vaccine Hesitancy in US Metropolitan Areas Using Machine Learning Models on Public Tweets. (arXiv:2108.01699v1 [cs.SI])]]></title>
        <id>http://arxiv.org/abs/2108.01699</id>
        <link href="http://arxiv.org/abs/2108.01699"/>
        <updated>2021-08-05T01:56:20.555Z</updated>
        <summary type="html"><![CDATA[Although the recent rise and uptake of COVID-19 vaccines in the United States
has been encouraging, there continues to be significant vaccine hesitancy in
various geographic and demographic clusters of the adult population. Surveys,
such as the one conducted by Gallup over the past year, can be useful in
determining vaccine hesitancy, but can be expensive to conduct and do not
provide real-time data. At the same time, the advent of social media suggests
that it may be possible to get vaccine hesitancy signals at an aggregate level
(such as at the level of zip codes) by using machine learning models and
socioeconomic (and other) features from publicly available sources. It is an
open question at present whether such an endeavor is feasible, and how it
compares to baselines that only use constant priors. To our knowledge, a proper
methodology and evaluation results using real data has also not been presented.
In this article, we present such a methodology and experimental study, using
publicly available Twitter data collected over the last year. Our goal is not
to devise novel machine learning algorithms, but to evaluate existing and
established models in a comparative framework. We show that the best models
significantly outperform constant priors, and can be set up using open-source
tools.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Melotte_S/0/1/0/all/0/1"&gt;Sara Melotte&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kejriwal_M/0/1/0/all/0/1"&gt;Mayank Kejriwal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Factor Representation and Decision Making in Stock Markets Using Deep Reinforcement Learning. (arXiv:2108.01758v1 [q-fin.ST])]]></title>
        <id>http://arxiv.org/abs/2108.01758</id>
        <link href="http://arxiv.org/abs/2108.01758"/>
        <updated>2021-08-05T01:56:20.546Z</updated>
        <summary type="html"><![CDATA[Deep Reinforcement learning is a branch of unsupervised learning in which an
agent learns to act based on environment state in order to maximize its total
reward. Deep reinforcement learning provides good opportunity to model the
complexity of portfolio choice in high-dimensional and data-driven environment
by leveraging the powerful representation of deep neural networks. In this
paper, we build a portfolio management system using direct deep reinforcement
learning to make optimal portfolio choice periodically among S\&P500 underlying
stocks by learning a good factor representation (as input). The result shows
that an effective learning of market conditions and optimal portfolio
allocations can significantly outperform the average market.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-fin/1/au:+Dong_Z/0/1/0/all/0/1"&gt;Zhaolu Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Huang_S/0/1/0/all/0/1"&gt;Shan Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Ma_S/0/1/0/all/0/1"&gt;Simiao Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Qian_Y/0/1/0/all/0/1"&gt;Yining Qian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Conditional Directed Graph Convolution for 3D Human Pose Estimation. (arXiv:2107.07797v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.07797</id>
        <link href="http://arxiv.org/abs/2107.07797"/>
        <updated>2021-08-05T01:56:20.488Z</updated>
        <summary type="html"><![CDATA[Graph convolutional networks have significantly improved 3D human pose
estimation by representing the human skeleton as an undirected graph. However,
this representation fails to reflect the articulated characteristic of human
skeletons as the hierarchical orders among the joints are not explicitly
presented. In this paper, we propose to represent the human skeleton as a
directed graph with the joints as nodes and bones as edges that are directed
from parent joints to child joints. By so doing, the directions of edges can
explicitly reflect the hierarchical relationships among the nodes. Based on
this representation, we further propose a spatial-temporal conditional directed
graph convolution to leverage varying non-local dependence for different poses
by conditioning the graph topology on input poses. Altogether, we form a
U-shaped network, named U-shaped Conditional Directed Graph Convolutional
Network, for 3D human pose estimation from monocular videos. To evaluate the
effectiveness of our method, we conducted extensive experiments on two
challenging large-scale benchmarks: Human3.6M and MPI-INF-3DHP. Both
quantitative and qualitative results show that our method achieves top
performance. Also, ablation studies show that directed graphs can better
exploit the hierarchy of articulated human skeletons than undirected graphs,
and the conditional connections can yield adaptive graph topologies for
different poses.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1"&gt;Wenbo Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Changgong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhan_F/0/1/0/all/0/1"&gt;Fangneng Zhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wong_T/0/1/0/all/0/1"&gt;Tien-Tsin Wong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Generalized Framework for Edge-preserving and Structure-preserving Image Smoothing. (arXiv:2107.07058v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.07058</id>
        <link href="http://arxiv.org/abs/2107.07058"/>
        <updated>2021-08-05T01:56:20.481Z</updated>
        <summary type="html"><![CDATA[Image smoothing is a fundamental procedure in applications of both computer
vision and graphics. The required smoothing properties can be different or even
contradictive among different tasks. Nevertheless, the inherent smoothing
nature of one smoothing operator is usually fixed and thus cannot meet the
various requirements of different applications. In this paper, we first
introduce the truncated Huber penalty function which shows strong flexibility
under different parameter settings. A generalized framework is then proposed
with the introduced truncated Huber penalty function. When combined with its
strong flexibility, our framework is able to achieve diverse smoothing natures
where contradictive smoothing behaviors can even be achieved. It can also yield
the smoothing behavior that can seldom be achieved by previous methods, and
superior performance is thus achieved in challenging cases. These together
enable our framework capable of a range of applications and able to outperform
the state-of-the-art approaches in several tasks, such as image detail
enhancement, clip-art compression artifacts removal, guided depth map
restoration, image texture removal, etc. In addition, an efficient numerical
solution is provided and its convergence is theoretically guaranteed even the
optimization framework is non-convex and non-smooth. A simple yet effective
approach is further proposed to reduce the computational cost of our method
while maintaining its performance. The effectiveness and superior performance
of our approach are validated through comprehensive experiments in a range of
applications. Our code is available at
https://github.com/wliusjtu/Generalized-Smoothing-Framework.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1"&gt;Pingping Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lei_Y/0/1/0/all/0/1"&gt;Yinjie Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1"&gt;Xiaolin Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jie Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ng_M/0/1/0/all/0/1"&gt;Michael Ng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Real-Time Activity Recognition and Intention Recognition Using a Vision-based Embedded System. (arXiv:2107.12744v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.12744</id>
        <link href="http://arxiv.org/abs/2107.12744"/>
        <updated>2021-08-05T01:56:20.453Z</updated>
        <summary type="html"><![CDATA[With the rapid increase in digital technologies, most fields of study include
recognition of human activity and intention recognition, which are essential in
smart environments. In this study, we equipped the activity recognition system
with the ability to recognize intentions by affecting the pace of movement of
individuals in the representation of images. Using this technology in various
environments such as elevators and automatic doors will lead to identifying
those who intend to pass the automatic door from those who are passing by. This
system, if applied in elevators and automatic doors, will save energy and
increase efficiency. For this study, data preparation is applied to combine the
spatial and temporal features with the help of digital image processing
principles. Nevertheless, unlike previous studies, only one AlexNet neural
network is used instead of two-stream convolutional neural networks. Our
embedded system was implemented with an accuracy of 98.78% on our intention
recognition dataset. We also examined our data representation approach on other
datasets, including HMDB-51, KTH, and Weizmann, and obtained accuracy of
78.48%, 97.95%, and 100%, respectively. The image recognition and neural
network models were simulated and implemented using Xilinx simulators for the
Xilinx ZCU102 board. The operating frequency of this embedded system is 333
MHz, and it works in real-time with 120 frames per second (fps).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Darafsh_S/0/1/0/all/0/1"&gt;Sahar Darafsh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghidary_S/0/1/0/all/0/1"&gt;Saeed Shiry Ghidary&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zamani_M/0/1/0/all/0/1"&gt;Morteza Saheb Zamani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[WeClick: Weakly-Supervised Video Semantic Segmentation with Click Annotations. (arXiv:2107.03088v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.03088</id>
        <link href="http://arxiv.org/abs/2107.03088"/>
        <updated>2021-08-05T01:56:20.446Z</updated>
        <summary type="html"><![CDATA[Compared with tedious per-pixel mask annotating, it is much easier to
annotate data by clicks, which costs only several seconds for an image.
However, applying clicks to learn video semantic segmentation model has not
been explored before. In this work, we propose an effective weakly-supervised
video semantic segmentation pipeline with click annotations, called WeClick,
for saving laborious annotating effort by segmenting an instance of the
semantic class with only a single click. Since detailed semantic information is
not captured by clicks, directly training with click labels leads to poor
segmentation predictions. To mitigate this problem, we design a novel memory
flow knowledge distillation strategy to exploit temporal information (named
memory flow) in abundant unlabeled video frames, by distilling the neighboring
predictions to the target frame via estimated motion. Moreover, we adopt
vanilla knowledge distillation for model compression. In this case, WeClick
learns compact video semantic segmentation models with the low-cost click
annotations during the training phase yet achieves real-time and accurate
models during the inference period. Experimental results on Cityscapes and
Camvid show that WeClick outperforms the state-of-the-art methods, increases
performance by 10.24% mIoU than baseline, and achieves real-time execution.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1"&gt;Peidong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1"&gt;Zibin He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1"&gt;Xiyu Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1"&gt;Yong Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_S/0/1/0/all/0/1"&gt;Shutao Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_F/0/1/0/all/0/1"&gt;Feng Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_M/0/1/0/all/0/1"&gt;Maowei Hu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Brain Inspired Face Recognition: A Computational Framework. (arXiv:2105.07237v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.07237</id>
        <link href="http://arxiv.org/abs/2105.07237"/>
        <updated>2021-08-05T01:56:20.438Z</updated>
        <summary type="html"><![CDATA[This paper presents a new proposal of an efficient computational model of
face recognition which uses cues from the distributed face recognition
mechanism of the brain, and by gathering engineering equivalent of these cues
from existing literature. Three distinct and widely used features: Histogram of
Oriented Gradients (HOG), Local Binary Patterns (LBP), and Principal components
(PCs) extracted from target images are used in a manner which is simple, and
yet effective. The HOG and LBP features further undergo principal component
analysis for dimensionality reduction. Our model uses multi-layer perceptrons
(MLP) to classify these three features and fuse them at the decision level
using sum rule. A computational theory is first developed by using concepts
from the information processing mechanism of the brain. Extensive experiments
are carried out using ten publicly available datasets to validate our proposed
model's performance in recognizing faces with extreme variation of
illumination, pose angle, expression, and background. Results obtained are
extremely promising when compared with other face recognition algorithms
including CNN and deep learning-based methods. This highlights that simple
computational processes, if clubbed properly, can produce competing performance
with best algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chowdhury_P/0/1/0/all/0/1"&gt;Pinaki Roy Chowdhury&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wadhwa_A/0/1/0/all/0/1"&gt;Angad Wadhwa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kar_A/0/1/0/all/0/1"&gt;Antariksha Kar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tyagi_N/0/1/0/all/0/1"&gt;Nikhil Tyagi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ensemble of MRR and NDCG models for Visual Dialog. (arXiv:2104.07511v3 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.07511</id>
        <link href="http://arxiv.org/abs/2104.07511"/>
        <updated>2021-08-05T01:56:20.427Z</updated>
        <summary type="html"><![CDATA[Assessing an AI agent that can converse in human language and understand
visual content is challenging. Generation metrics, such as BLEU scores favor
correct syntax over semantics. Hence a discriminative approach is often used,
where an agent ranks a set of candidate options. The mean reciprocal rank (MRR)
metric evaluates the model performance by taking into account the rank of a
single human-derived answer. This approach, however, raises a new challenge:
the ambiguity and synonymy of answers, for instance, semantic equivalence
(e.g., `yeah' and `yes'). To address this, the normalized discounted cumulative
gain (NDCG) metric has been used to capture the relevance of all the correct
answers via dense annotations. However, the NDCG metric favors the usually
applicable uncertain answers such as `I don't know. Crafting a model that
excels on both MRR and NDCG metrics is challenging. Ideally, an AI agent should
answer a human-like reply and validate the correctness of any answer. To
address this issue, we describe a two-step non-parametric ranking approach that
can merge strong MRR and NDCG models. Using our approach, we manage to keep
most MRR state-of-the-art performance (70.41% vs. 71.24%) and the NDCG
state-of-the-art performance (72.16% vs. 75.35%). Moreover, our approach won
the recent Visual Dialog 2020 challenge. Source code is available at
https://github.com/idansc/mrr-ndcg.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schwartz_I/0/1/0/all/0/1"&gt;Idan Schwartz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[NeuLF: Efficient Novel View Synthesis with Neural 4D Light Field. (arXiv:2105.07112v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.07112</id>
        <link href="http://arxiv.org/abs/2105.07112"/>
        <updated>2021-08-05T01:56:20.420Z</updated>
        <summary type="html"><![CDATA[In this paper, we present an efficient and robust deep learning solution for
novel view synthesis of complex scenes. In our approach, a 3D scene is
represented as a light field, i.e., a set of rays, each of which has a
corresponding color when reaching the image plane. For efficient novel view
rendering, we adopt a 4D parameterization of the light field, where each ray is
characterized by a 4D parameter. We then formulate the light field as a 4D
function that maps 4D coordinates to corresponding color values. We train a
deep fully connected network to optimize this implicit function and memorize
the 3D scene. Then, the scene-specific model is used to synthesize novel views.
Different from previous light field approaches which require dense view
sampling to reliably render novel views, our method can render novel views by
sampling rays and querying the color for each ray from the network directly,
thus enabling high-quality light field rendering with a sparser set of training
images. Our method achieves state-of-the-art novel view synthesis results while
maintaining an interactive frame rate.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1"&gt;Celong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1"&gt;Junsong Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yi Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ASFM-Net: Asymmetrical Siamese Feature Matching Network for Point Completion. (arXiv:2104.09587v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.09587</id>
        <link href="http://arxiv.org/abs/2104.09587"/>
        <updated>2021-08-05T01:56:20.414Z</updated>
        <summary type="html"><![CDATA[We tackle the problem of object completion from point clouds and propose a
novel point cloud completion network employing an Asymmetrical Siamese Feature
Matching strategy, termed as ASFM-Net. Specifically, the Siamese auto-encoder
neural network is adopted to map the partial and complete input point cloud
into a shared latent space, which can capture detailed shape prior. Then we
design an iterative refinement unit to generate complete shapes with
fine-grained details by integrating prior information. Experiments are
conducted on the PCN dataset and the Completion3D benchmark, demonstrating the
state-of-the-art performance of the proposed ASFM-Net. Our method achieves the
1st place in the leaderboard of Completion3D and outperforms existing methods
with a large margin, about 12%. The codes and trained models are released
publicly at https://github.com/Yan-Xia/ASFM-Net.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1"&gt;Yaqi Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1"&gt;Yan Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1"&gt;Wei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_R/0/1/0/all/0/1"&gt;Rui Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_K/0/1/0/all/0/1"&gt;Kailang Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stilla_U/0/1/0/all/0/1"&gt;Uwe Stilla&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TS-CAM: Token Semantic Coupled Attention Map for Weakly Supervised Object Localization. (arXiv:2103.14862v5 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.14862</id>
        <link href="http://arxiv.org/abs/2103.14862"/>
        <updated>2021-08-05T01:56:20.407Z</updated>
        <summary type="html"><![CDATA[Weakly supervised object localization (WSOL) is a challenging problem when
given image category labels but requires to learn object localization models.
Optimizing a convolutional neural network (CNN) for classification tends to
activate local discriminative regions while ignoring complete object extent,
causing the partial activation issue. In this paper, we argue that partial
activation is caused by the intrinsic characteristics of CNN, where the
convolution operations produce local receptive fields and experience difficulty
to capture long-range feature dependency among pixels. We introduce the token
semantic coupled attention map (TS-CAM) to take full advantage of the
self-attention mechanism in visual transformer for long-range dependency
extraction. TS-CAM first splits an image into a sequence of patch tokens for
spatial embedding, which produce attention maps of long-range visual dependency
to avoid partial activation. TS-CAM then re-allocates category-related
semantics for patch tokens, enabling each of them to be aware of object
categories. TS-CAM finally couples the patch tokens with the semantic-agnostic
attention map to achieve semantic-aware localization. Experiments on the
ILSVRC/CUB-200-2011 datasets show that TS-CAM outperforms its CNN-CAM
counterparts by 7.1%/27.1% for WSOL, achieving state-of-the-art performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1"&gt;Wei Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wan_F/0/1/0/all/0/1"&gt;Fang Wan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1"&gt;Xingjia Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_Z/0/1/0/all/0/1"&gt;Zhiliang Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1"&gt;Qi Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1"&gt;Zhenjun Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1"&gt;Bolei Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1"&gt;Qixiang Ye&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Explaining COVID-19 and Thoracic Pathology Model Predictions by Identifying Informative Input Features. (arXiv:2104.00411v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.00411</id>
        <link href="http://arxiv.org/abs/2104.00411"/>
        <updated>2021-08-05T01:56:20.399Z</updated>
        <summary type="html"><![CDATA[Neural networks have demonstrated remarkable performance in classification
and regression tasks on chest X-rays. In order to establish trust in the
clinical routine, the networks' prediction mechanism needs to be interpretable.
One principal approach to interpretation is feature attribution. Feature
attribution methods identify the importance of input features for the output
prediction. Building on Information Bottleneck Attribution (IBA) method, for
each prediction we identify the chest X-ray regions that have high mutual
information with the network's output. Original IBA identifies input regions
that have sufficient predictive information. We propose Inverse IBA to identify
all informative regions. Thus all predictive cues for pathologies are
highlighted on the X-rays, a desirable property for chest X-ray diagnosis.
Moreover, we propose Regression IBA for explaining regression models. Using
Regression IBA we observe that a model trained on cumulative severity score
labels implicitly learns the severity of different X-ray regions. Finally, we
propose Multi-layer IBA to generate higher resolution and more detailed
attribution/saliency maps. We evaluate our methods using both human-centric
(ground-truth-based) interpretability metrics, and human-independent feature
importance metrics on NIH Chest X-ray8 and BrixIA datasets. The Code is
publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Khakzar_A/0/1/0/all/0/1"&gt;Ashkan Khakzar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mansour_W/0/1/0/all/0/1"&gt;Wejdene Mansour&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cai_Y/0/1/0/all/0/1"&gt;Yuezhi Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yawei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yucheng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kim_S/0/1/0/all/0/1"&gt;Seong Tae Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Navab_N/0/1/0/all/0/1"&gt;Nassir Navab&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How Well do Feature Visualizations Support Causal Understanding of CNN Activations?. (arXiv:2106.12447v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.12447</id>
        <link href="http://arxiv.org/abs/2106.12447"/>
        <updated>2021-08-05T01:56:20.379Z</updated>
        <summary type="html"><![CDATA[One widely used approach towards understanding the inner workings of deep
convolutional neural networks is to visualize unit responses via activation
maximization. Feature visualizations via activation maximization are thought to
provide humans with precise information about the image features that cause a
unit to be activated. If this is indeed true, these synthetic images should
enable humans to predict the effect of an intervention, such as whether
occluding a certain patch of the image (say, a dog's head) changes a unit's
activation. Here, we test this hypothesis by asking humans to predict which of
two square occlusions causes a larger change to a unit's activation. Both a
large-scale crowdsourced experiment and measurements with experts show that on
average, the extremely activating feature visualizations by Olah et al. (2017)
indeed help humans on this task ($67 \pm 4\%$ accuracy; baseline performance
without any visualizations is $60 \pm 3\%$). However, they do not provide any
significant advantage over other visualizations (such as e.g. dataset samples),
which yield similar performance ($66 \pm 3\%$ to $67 \pm 3\%$ accuracy). Taken
together, we propose an objective psychophysical task to quantify the benefit
of unit-level interpretability methods for humans, and find no evidence that
feature visualizations provide humans with better "causal understanding" than
simple alternative visualizations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zimmermann_R/0/1/0/all/0/1"&gt;Roland S. Zimmermann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Borowski_J/0/1/0/all/0/1"&gt;Judy Borowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geirhos_R/0/1/0/all/0/1"&gt;Robert Geirhos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bethge_M/0/1/0/all/0/1"&gt;Matthias Bethge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wallis_T/0/1/0/all/0/1"&gt;Thomas S. A. Wallis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brendel_W/0/1/0/all/0/1"&gt;Wieland Brendel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Registration-aided Domain Adaptation Network for 3D Point Cloud Based Place Recognition. (arXiv:2012.05018v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.05018</id>
        <link href="http://arxiv.org/abs/2012.05018"/>
        <updated>2021-08-05T01:56:20.358Z</updated>
        <summary type="html"><![CDATA[In the field of large-scale SLAM for autonomous driving and mobile robotics,
3D point cloud based place recognition has aroused significant research
interest due to its robustness to changing environments with drastic daytime
and weather variance. However, it is time-consuming and effort-costly to obtain
high-quality point cloud data for place recognition model training and ground
truth for registration in the real world. To this end, a novel
registration-aided 3D domain adaptation network for point cloud based place
recognition is proposed. A structure-aware registration network is introduced
to help to learn features with geometric information and a 6-DoFs pose between
two point clouds with partial overlap can be estimated. The model is trained
through a synthetic virtual LiDAR dataset through GTA-V with diverse weather
and daytime conditions and domain adaptation is implemented to the real-world
domain by aligning the global features. Our results outperform state-of-the-art
3D place recognition baselines or achieve comparable on the real-world Oxford
RobotCar dataset with the visualization of registration on the virtual dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qiao_Z/0/1/0/all/0/1"&gt;Zhijian Qiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1"&gt;Hanjiang Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1"&gt;Weiang Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Siyuan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhe Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hesheng Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Making Contrastive Learning Robust to Shortcuts. (arXiv:2012.09962v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.09962</id>
        <link href="http://arxiv.org/abs/2012.09962"/>
        <updated>2021-08-05T01:56:20.332Z</updated>
        <summary type="html"><![CDATA[Contrastive learning is effective at learning useful representations without
supervision. Yet contrastive learning is susceptible to shortcuts -- i.e., it
may learn shortcut features irrelevant to the downstream task and discard
relevant information. Past work has addressed this limitation via handcrafted
data augmentations that eliminate the shortcut. However, handcrafted
augmentations are infeasible for data modalities that are not interpretable by
humans (e.g., radio signals). Further, even when the modality is interpretable
(e.g., RGB), sometimes eliminating the shortcut information may be undesirable.
For example, in multi-attribute classification, information related to one
attribute may act as a shortcut around other attributes. This paper presents
reconstructive contrastive learning (RCL), a framework for learning
unsupervised representations that are robust to shortcuts. The key idea is to
force the learned representation to reconstruct the input, which naturally
counters potential shortcuts. Extensive experiments verify that RCL is highly
robust to shortcuts and outperforms state-of-the-art contrastive learning
methods on both RGB and RF datasets for a variety of tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1"&gt;Tianhong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1"&gt;Lijie Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1"&gt;Yuan Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1"&gt;Hao He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1"&gt;Yonglong Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Katabi_D/0/1/0/all/0/1"&gt;Dina Katabi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feris_R/0/1/0/all/0/1"&gt;Rogerio Feris&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fast Batch Nuclear-norm Maximization and Minimization for Robust Domain Adaptation. (arXiv:2107.06154v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.06154</id>
        <link href="http://arxiv.org/abs/2107.06154"/>
        <updated>2021-08-05T01:56:20.322Z</updated>
        <summary type="html"><![CDATA[Due to the domain discrepancy in visual domain adaptation, the performance of
source model degrades when bumping into the high data density near decision
boundary in target domain. A common solution is to minimize the Shannon Entropy
to push the decision boundary away from the high density area. However, entropy
minimization also leads to severe reduction of prediction diversity, and
unfortunately brings harm to the domain adaptation. In this paper, we
investigate the prediction discriminability and diversity by studying the
structure of the classification output matrix of a randomly selected data
batch. We find by theoretical analysis that the prediction discriminability and
diversity could be separately measured by the Frobenius-norm and rank of the
batch output matrix. The nuclear-norm is an upperbound of the former, and a
convex approximation of the latter. Accordingly, we propose Batch Nuclear-norm
Maximization and Minimization, which performs nuclear-norm maximization on the
target output matrix to enhance the target prediction ability, and nuclear-norm
minimization on the source batch output matrix to increase applicability of the
source domain knowledge. We further approximate the nuclear-norm by
L_{1,2}-norm, and design multi-batch optimization for stable solution on large
number of categories. The fast approximation method achieves O(n^2)
computational complexity and better convergence property. Experiments show that
our method could boost the adaptation accuracy and robustness under three
typical domain adaptation scenarios. The code is available at
https://github.com/cuishuhao/BNM.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1"&gt;Shuhao Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shuhui Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhuo_J/0/1/0/all/0/1"&gt;Junbao Zhuo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Liang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1"&gt;Qingming Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1"&gt;Qi Tian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeepChange: A Long-Term Person Re-Identification Benchmark. (arXiv:2105.14685v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14685</id>
        <link href="http://arxiv.org/abs/2105.14685"/>
        <updated>2021-08-05T01:56:20.315Z</updated>
        <summary type="html"><![CDATA[Existing person re-identification (Re-ID) works mostly consider a short-term
search problem assuming unchanged clothes and personal appearance. However, in
real-world we often dress differently across locations, time, dates, seasons,
weather, and events. As a result, the existing methods are unsuitable for
long-term person Re-ID with clothes change involved. Whilst there are several
recent long-term Re-ID attempts, a large realistic dataset with clothes change
is lacking and indispensable for enabling extensive study as already
experienced in short-term Re-ID setting. In this work, we contribute a large,
realistic long-term person identification benchmark. It consists of 178K
bounding boxes from 1.1K person identities, collected and constructed over 12
months. Unique characteristics of this dataset include: (1) Natural/native
personal appearance (e.g., clothes and hair style) variations: The
clothes-change and dressing styles all are highly diverse, with the reappearing
gap in time ranging from minutes, hours, and days to weeks, months, seasons,
and years. (2) Diverse walks of life: Persons across a wide range of ages and
professions appear in different weather conditions (e.g., sunny, cloudy, windy,
rainy, snowy, extremely cold) and events (e.g., working, leisure, daily
activities). (3) Rich camera setups: The raw videos were recorded by 17 outdoor
security cameras with various resolutions operating in a real-world
surveillance system for a wide and dense block. (4) Largest scale: It covers
the largest number of (17) cameras, (1, 121) identities, and (178, 407)
bounding boxes, as compared to alternative datasets. Our dataset and benchmark
codes are available on https://github.com/PengBoXiangShang/deepchange.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1"&gt;Peng Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1"&gt;Xiatian Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Differentially private training of neural networks with Langevin dynamics for calibrated predictive uncertainty. (arXiv:2107.04296v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.04296</id>
        <link href="http://arxiv.org/abs/2107.04296"/>
        <updated>2021-08-05T01:56:20.307Z</updated>
        <summary type="html"><![CDATA[We show that differentially private stochastic gradient descent (DP-SGD) can
yield poorly calibrated, overconfident deep learning models. This represents a
serious issue for safety-critical applications, e.g. in medical diagnosis. We
highlight and exploit parallels between stochastic gradient Langevin dynamics,
a scalable Bayesian inference technique for training deep neural networks, and
DP-SGD, in order to train differentially private, Bayesian neural networks with
minor adjustments to the original (DP-SGD) algorithm. Our approach provides
considerably more reliable uncertainty estimates than DP-SGD, as demonstrated
empirically by a reduction in expected calibration error (MNIST $\sim{5}$-fold,
Pediatric Pneumonia Dataset $\sim{2}$-fold).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Knolle_M/0/1/0/all/0/1"&gt;Moritz Knolle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ziller_A/0/1/0/all/0/1"&gt;Alexander Ziller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Usynin_D/0/1/0/all/0/1"&gt;Dmitrii Usynin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Braren_R/0/1/0/all/0/1"&gt;Rickmer Braren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Makowski_M/0/1/0/all/0/1"&gt;Marcus R. Makowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rueckert_D/0/1/0/all/0/1"&gt;Daniel Rueckert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kaissis_G/0/1/0/all/0/1"&gt;Georgios Kaissis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[EDN: Salient Object Detection via Extremely-Downsampled Network. (arXiv:2012.13093v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.13093</id>
        <link href="http://arxiv.org/abs/2012.13093"/>
        <updated>2021-08-05T01:56:20.299Z</updated>
        <summary type="html"><![CDATA[Recent progress on salient object detection (SOD) mainly benefits from
multi-scale learning, where the high-level and low-level features collaborate
in locating salient objects and discovering fine details, respectively.
However, most efforts are devoted to low-level feature learning by fusing
multi-scale features or enhancing boundary representations. High-level
features, which although have long proven effective for many other tasks, yet
have been barely studied for SOD. In this paper, we tap into this gap and show
that enhancing high-level features is essential for SOD as well. To this end,
we introduce an Extremely-Downsampled Network (EDN), which employs an extreme
downsampling technique to effectively learn a global view of the whole image,
leading to accurate salient object localization. To accomplish better
multi-level feature fusion, we construct the Scale-Correlated Pyramid
Convolution (SCPC) to build an elegant decoder for recovering object details
from the above extreme downsampling. Extensive experiments demonstrate that EDN
achieves state-of-the-art performance with real-time speed. Our efficient
EDN-Lite also achieves competitive performance with a speed of 316fps. Hence,
this work is expected to spark some new thinking in SOD. Full training and
testing code will be available at https://github.com/yuhuan-wu/EDN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yu-Huan Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yun Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Le Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1"&gt;Ming-Ming Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_B/0/1/0/all/0/1"&gt;Bo Ren&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Semantic Interpretation of Thoracic Disease and COVID-19 Diagnosis Models. (arXiv:2104.02481v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.02481</id>
        <link href="http://arxiv.org/abs/2104.02481"/>
        <updated>2021-08-05T01:56:20.280Z</updated>
        <summary type="html"><![CDATA[Convolutional neural networks are showing promise in the automatic diagnosis
of thoracic pathologies on chest x-rays. Their black-box nature has sparked
many recent works to explain the prediction via input feature attribution
methods (aka saliency methods). However, input feature attribution methods
merely identify the importance of input regions for the prediction and lack
semantic interpretation of model behavior. In this work, we first identify the
semantics associated with internal units (feature maps) of the network. We
proceed to investigate the following questions; Does a regression model that is
only trained with COVID-19 severity scores implicitly learn visual patterns
associated with thoracic pathologies? Does a network that is trained on weakly
labeled data (e.g. healthy, unhealthy) implicitly learn pathologies? Moreover,
we investigate the effect of pretraining and data imbalance on the
interpretability of learned features. In addition to the analysis, we propose
semantic attribution to semantically explain each prediction. We present our
findings using publicly available chest pathologies (CheXpert, NIH ChestX-ray8)
and COVID-19 datasets (BrixIA, and COVID-19 chest X-ray segmentation dataset).
The Code is publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Khakzar_A/0/1/0/all/0/1"&gt;Ashkan Khakzar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Musatian_S/0/1/0/all/0/1"&gt;Sabrina Musatian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Buchberger_J/0/1/0/all/0/1"&gt;Jonas Buchberger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Quiroz_I/0/1/0/all/0/1"&gt;Icxel Valeriano Quiroz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Pinger_N/0/1/0/all/0/1"&gt;Nikolaus Pinger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Baselizadeh_S/0/1/0/all/0/1"&gt;Soroosh Baselizadeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kim_S/0/1/0/all/0/1"&gt;Seong Tae Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Navab_N/0/1/0/all/0/1"&gt;Nassir Navab&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Recent Ice Trends in Swiss Mountain Lakes: 20-year Analysis of MODIS Imagery. (arXiv:2103.12434v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.12434</id>
        <link href="http://arxiv.org/abs/2103.12434"/>
        <updated>2021-08-05T01:56:20.273Z</updated>
        <summary type="html"><![CDATA[Depleting lake ice can serve as an indicator for climate change, just like
sea level rise or glacial retreat. Several Lake Ice Phenological (LIP) events
serve as sentinels to understand the regional and global climate change. Hence,
it is useful to monitor long-term lake freezing and thawing patterns. In this
paper we report a case study for the Oberengadin region of Switzerland, where
there are several small- and medium-sized mountain lakes. We observe the LIP
events, such as freeze-up, break-up and ice cover duration, across two decades
(2000-2020) from optical satellite images. We analyse time-series of MODIS
imagery by estimating spatially resolved maps of lake ice for these Alpine
lakes with supervised machine learning (and additionally cross-check with VIIRS
data when available). To train the classifier we rely on reference data
annotated manually based on webcam images. From the ice maps we derive
long-term LIP trends. Since the webcam data is only available for two winters,
we also validate our results against the operational MODIS and VIIRS snow
products. We find a change in complete freeze duration of -0.76 and -0.89 days
per annum for lakes Sils and Silvaplana, respectively. Furthermore, we observe
plausible correlations of the LIP trends with climate data measured at nearby
meteorological stations. We notice that mean winter air temperature has
negative correlation with the freeze duration and break-up events, and positive
correlation with the freeze-up events. Additionally, we observe strong negative
correlation of sunshine during the winter months with the freeze duration and
break-up events.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tom_M/0/1/0/all/0/1"&gt;Manu Tom&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1"&gt;Tianyu Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baltsavias_E/0/1/0/all/0/1"&gt;Emmanuel Baltsavias&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schindler_K/0/1/0/all/0/1"&gt;Konrad Schindler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Relevance Attack on Detectors. (arXiv:2008.06822v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.06822</id>
        <link href="http://arxiv.org/abs/2008.06822"/>
        <updated>2021-08-05T01:56:20.267Z</updated>
        <summary type="html"><![CDATA[This paper focuses on high-transferable adversarial attacks on detectors,
which are hard to attack in a black-box manner, because of their
multiple-output characteristics and the diversity across architectures. To
pursue a high attack transferability, one plausible way is to find a common
property across detectors, which facilitates the discovery of common
weaknesses. We are the first to suggest that the relevance map from
interpreters for detectors is such a property. Based on it, we design a
Relevance Attack on Detectors (RAD), which achieves a state-of-the-art
transferability, exceeding existing results by above 20%. On MS COCO, the
detection mAPs for all 8 black-box architectures are more than halved and the
segmentation mAPs are also significantly influenced. Given the great
transferability of RAD, we generate the first adversarial dataset for object
detection and instance segmentation, i.e., Adversarial Objects in COntext
(AOCO), which helps to quickly evaluate and improve the robustness of
detectors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Sizhe Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_F/0/1/0/all/0/1"&gt;Fan He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1"&gt;Xiaolin Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1"&gt;Kun Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Enhancing Self-supervised Video Representation Learning via Multi-level Feature Optimization. (arXiv:2108.02183v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02183</id>
        <link href="http://arxiv.org/abs/2108.02183"/>
        <updated>2021-08-05T01:56:20.260Z</updated>
        <summary type="html"><![CDATA[The crux of self-supervised video representation learning is to build general
features from unlabeled videos. However, most recent works have mainly focused
on high-level semantics and neglected lower-level representations and their
temporal relationship which are crucial for general video understanding. To
address these challenges, this paper proposes a multi-level feature
optimization framework to improve the generalization and temporal modeling
ability of learned video representations. Concretely, high-level features
obtained from naive and prototypical contrastive learning are utilized to build
distribution graphs, guiding the process of low-level and mid-level feature
learning. We also devise a simple temporal modeling module from multi-level
features to enhance motion pattern learning. Experiments demonstrate that
multi-level feature optimization with the graph constraint and temporal
modeling can greatly improve the representation ability in video understanding.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qian_R/0/1/0/all/0/1"&gt;Rui Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yuxi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Huabin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+See_J/0/1/0/all/0/1"&gt;John See&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1"&gt;Shuangrui Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xian Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1"&gt;Dian Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1"&gt;Weiyao Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Targeted Attention Attack on Deep Learning Models in Road Sign Recognition. (arXiv:2010.04331v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.04331</id>
        <link href="http://arxiv.org/abs/2010.04331"/>
        <updated>2021-08-05T01:56:20.253Z</updated>
        <summary type="html"><![CDATA[Real world traffic sign recognition is an important step towards building
autonomous vehicles, most of which highly dependent on Deep Neural Networks
(DNNs). Recent studies demonstrated that DNNs are surprisingly susceptible to
adversarial examples. Many attack methods have been proposed to understand and
generate adversarial examples, such as gradient based attack, score based
attack, decision based attack, and transfer based attacks. However, most of
these algorithms are ineffective in real-world road sign attack, because (1)
iteratively learning perturbations for each frame is not realistic for a fast
moving car and (2) most optimization algorithms traverse all pixels equally
without considering their diverse contribution. To alleviate these problems,
this paper proposes the targeted attention attack (TAA) method for real world
road sign attack. Specifically, we have made the following contributions: (1)
we leverage the soft attention map to highlight those important pixels and skip
those zero-contributed areas - this also helps to generate natural
perturbations, (2) we design an efficient universal attack that optimizes a
single perturbation/noise based on a set of training images under the guidance
of the pre-trained attention map, (3) we design a simple objective function
that can be easily optimized, (4) we evaluate the effectiveness of TAA on real
world data sets. Experimental results validate that the TAA method improves the
attack successful rate (nearly 10%) and reduces the perturbation loss (about a
quarter) compared with the popular RP2 method. Additionally, our TAA also
provides good properties, e.g., transferability and generalization capability.
We provide code and data to ensure the reproducibility:
https://github.com/AdvAttack/RoadSignAttack.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xinghao Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Weifeng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shengli Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1"&gt;Dacheng Tao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GANterfactual -- Counterfactual Explanations for Medical Non-Experts using Generative Adversarial Learning. (arXiv:2012.11905v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.11905</id>
        <link href="http://arxiv.org/abs/2012.11905"/>
        <updated>2021-08-05T01:56:20.246Z</updated>
        <summary type="html"><![CDATA[With the ongoing rise of machine learning, the need for methods for
explaining decisions made by artificial intelligence systems is becoming a more
and more important topic. Especially for image classification tasks, many
state-of-the-art tools to explain such classifiers rely on visual highlighting
of important areas of the input data. Contrary, counterfactual explanation
systems try to enable a counterfactual reasoning by modifying the input image
in a way such that the classifier would have made a different prediction. By
doing so, the users of counterfactual explanation systems are equipped with a
completely different kind of explanatory information. However, methods for
generating realistic counterfactual explanations for image classifiers are
still rare. Especially in medical contexts, where relevant information often
consists of textural and structural information, high-quality counterfactual
images have the potential to give meaningful insights into decision processes.
In this work, we present GANterfactual, an approach to generate such
counterfactual image explanations based on adversarial image-to-image
translation techniques. Additionally, we conduct a user study to evaluate our
approach in an exemplary medical use case. Our results show that, in the chosen
medical use-case, counterfactual explanations lead to significantly better
results regarding mental models, explanation satisfaction, trust, emotions, and
self-efficacy than two state-of-the-art systems that work with saliency maps,
namely LIME and LRP.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mertes_S/0/1/0/all/0/1"&gt;Silvan Mertes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huber_T/0/1/0/all/0/1"&gt;Tobias Huber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weitz_K/0/1/0/all/0/1"&gt;Katharina Weitz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heimerl_A/0/1/0/all/0/1"&gt;Alexander Heimerl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Andre_E/0/1/0/all/0/1"&gt;Elisabeth Andr&amp;#xe9;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Recursive Fusion and Deformable Spatiotemporal Attention for Video Compression Artifact Reduction. (arXiv:2108.02110v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2108.02110</id>
        <link href="http://arxiv.org/abs/2108.02110"/>
        <updated>2021-08-05T01:56:20.227Z</updated>
        <summary type="html"><![CDATA[A number of deep learning based algorithms have been proposed to recover
high-quality videos from low-quality compressed ones. Among them, some restore
the missing details of each frame via exploring the spatiotemporal information
of neighboring frames. However, these methods usually suffer from a narrow
temporal scope, thus may miss some useful details from some frames outside the
neighboring ones. In this paper, to boost artifact removal, on the one hand, we
propose a Recursive Fusion (RF) module to model the temporal dependency within
a long temporal range. Specifically, RF utilizes both the current reference
frames and the preceding hidden state to conduct better spatiotemporal
compensation. On the other hand, we design an efficient and effective
Deformable Spatiotemporal Attention (DSTA) module such that the model can pay
more effort on restoring the artifact-rich areas like the boundary area of a
moving object. Extensive experiments show that our method outperforms the
existing ones on the MFQE 2.0 dataset in terms of both fidelity and perceptual
effect. Code is available at https://github.com/zhaominyiz/RFDA-PyTorch.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zhao_M/0/1/0/all/0/1"&gt;Minyi Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yi Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhou_S/0/1/0/all/0/1"&gt;Shuigeng Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Deep Image Denoising via Class Specific Convolution. (arXiv:2103.01624v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.01624</id>
        <link href="http://arxiv.org/abs/2103.01624"/>
        <updated>2021-08-05T01:56:20.220Z</updated>
        <summary type="html"><![CDATA[Deep neural networks have been widely used in image denoising during the past
few years. Even though they achieve great success on this problem, they are
computationally inefficient which makes them inappropriate to be implemented in
mobile devices. In this paper, we propose an efficient deep neural network for
image denoising based on pixel-wise classification. Despite using a
computationally efficient network cannot effectively remove the noises from any
content, it is still capable to denoise from a specific type of pattern or
texture. The proposed method follows such a divide and conquer scheme. We first
use an efficient U-net to pixel-wisely classify pixels in the noisy image based
on the local gradient statistics. Then we replace part of the convolution
layers in existing denoising networks by the proposed Class Specific
Convolution layers (CSConv) which use different weights for different classes
of pixels. Quantitative and qualitative evaluations on public datasets
demonstrate that the proposed method can reduce the computational costs without
sacrificing the performance compared to state-of-the-art algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Xu_L/0/1/0/all/0/1"&gt;Lu Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jiawei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cheng_X/0/1/0/all/0/1"&gt;Xuanye Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_F/0/1/0/all/0/1"&gt;Feng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wei_X/0/1/0/all/0/1"&gt;Xing Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ren_J/0/1/0/all/0/1"&gt;Jimmy Ren&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Residual Recurrent CRNN for End-to-End Optical Music Recognition on Monophonic Scores. (arXiv:2010.13418v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.13418</id>
        <link href="http://arxiv.org/abs/2010.13418"/>
        <updated>2021-08-05T01:56:20.213Z</updated>
        <summary type="html"><![CDATA[One of the challenges of the Optical Music Recognition task is to transcript
the symbols of the camera-captured images into digital music notations.
Previous end-to-end model which was developed as a Convolutional Recurrent
Neural Network does not explore sufficient contextual information from full
scales and there is still a large room for improvement. We propose an
innovative framework that combines a block of Residual Recurrent Convolutional
Neural Network with a recurrent Encoder-Decoder network to map a sequence of
monophonic music symbols corresponding to the notations present in the image.
The Residual Recurrent Convolutional block can improve the ability of the model
to enrich the context information. The experiment results are benchmarked
against a publicly available dataset called CAMERA-PRIMUS, which demonstrates
that our approach surpass the state-of-the-art end-to-end method using
Convolutional Recurrent Neural Network.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1"&gt;Aozhi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lipei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mei_Y/0/1/0/all/0/1"&gt;Yaqi Mei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1"&gt;Baoqiang Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1"&gt;Zifeng Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1"&gt;Zhaohua Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1"&gt;Jing Xiao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Calibrated Adversarial Refinement for Stochastic Semantic Segmentation. (arXiv:2006.13144v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.13144</id>
        <link href="http://arxiv.org/abs/2006.13144"/>
        <updated>2021-08-05T01:56:20.203Z</updated>
        <summary type="html"><![CDATA[In semantic segmentation tasks, input images can often have more than one
plausible interpretation, thus allowing for multiple valid labels. To capture
such ambiguities, recent work has explored the use of probabilistic networks
that can learn a distribution over predictions. However, these do not
necessarily represent the empirical distribution accurately. In this work, we
present a strategy for learning a calibrated predictive distribution over
semantic maps, where the probability associated with each prediction reflects
its ground truth correctness likelihood. To this end, we propose a novel
two-stage, cascaded approach for calibrated adversarial refinement: (i) a
standard segmentation network is trained with categorical cross entropy to
predict a pixelwise probability distribution over semantic classes and (ii) an
adversarially trained stochastic network is used to model the inter-pixel
correlations to refine the output of the first network into coherent samples.
Importantly, to calibrate the refinement network and prevent mode collapse, the
expectation of the samples in the second stage is matched to the probabilities
predicted in the first. We demonstrate the versatility and robustness of the
approach by achieving state-of-the-art results on the multigrader LIDC dataset
and on a modified Cityscapes dataset with injected ambiguities. In addition, we
show that the core design can be adapted to other tasks requiring learning a
calibrated predictive distribution by experimenting on a toy regression
dataset. We provide an open source implementation of our method at
https://github.com/EliasKassapis/CARSSS.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kassapis_E/0/1/0/all/0/1"&gt;Elias Kassapis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dikov_G/0/1/0/all/0/1"&gt;Georgi Dikov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_D/0/1/0/all/0/1"&gt;Deepak K. Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nugteren_C/0/1/0/all/0/1"&gt;Cedric Nugteren&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Global Adaptive Filtering Layer for Computer Vision. (arXiv:2010.01177v4 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.01177</id>
        <link href="http://arxiv.org/abs/2010.01177"/>
        <updated>2021-08-05T01:56:20.195Z</updated>
        <summary type="html"><![CDATA[We devise a universal adaptive neural layer to "learn" optimal frequency
filter for each image together with the weights of the base neural network that
performs some computer vision task. The proposed approach takes the source
image in the spatial domain, automatically selects the best frequencies from
the frequency domain, and transmits the inverse-transform image to the main
neural network. Remarkably, such a simple add-on layer dramatically improves
the performance of the main network regardless of its design. We observe that
the light networks gain a noticeable boost in the performance metrics; whereas,
the training of the heavy ones converges faster when our adaptive layer is
allowed to "learn" alongside the main architecture. We validate the idea in
four classical computer vision tasks: classification, segmentation, denoising,
and erasing, considering popular natural and medical data benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Shipitsin_V/0/1/0/all/0/1"&gt;Viktor Shipitsin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bespalov_I/0/1/0/all/0/1"&gt;Iaroslav Bespalov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dylov_D/0/1/0/all/0/1"&gt;Dmitry V. Dylov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Aleatoric Uncertainty Quantification in Multi-Annotated Medical ImageSegmentation with Normalizing Flows. (arXiv:2108.02155v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02155</id>
        <link href="http://arxiv.org/abs/2108.02155"/>
        <updated>2021-08-05T01:56:20.173Z</updated>
        <summary type="html"><![CDATA[Quantifying uncertainty in medical image segmentation applications is
essential, as it is often connected to vital decision-making. Compelling
attempts have been made in quantifying the uncertainty in image segmentation
architectures, e.g. to learn a density segmentation model conditioned on the
input image. Typical work in this field restricts these learnt densities to be
strictly Gaussian. In this paper, we propose to use a more flexible approach by
introducing Normalizing Flows (NFs), which enables the learnt densities to be
more complex and facilitate more accurate modeling for uncertainty. We prove
this hypothesis by adopting the Probabilistic U-Net and augmenting the
posterior density with an NF, allowing it to be more expressive. Our
qualitative as well as quantitative (GED and IoU) evaluations on the
multi-annotated and single-annotated LIDC-IDRI and Kvasir-SEG segmentation
datasets, respectively, show a clear improvement. This is mostly apparent in
the quantification of aleatoric uncertainty and the increased predictive
performance of up to 14 percent. This result strongly indicates that a more
flexible density model should be seriously considered in architectures that
attempt to capture segmentation ambiguity through density modeling. The benefit
of this improved modeling will increase human confidence in annotation and
segmentation, and enable eager adoption of the technology in practice.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Valiuddin_M/0/1/0/all/0/1"&gt;M.M.A. Valiuddin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Viviers_C/0/1/0/all/0/1"&gt;C.G.A. Viviers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sloun_R/0/1/0/all/0/1"&gt;R.J.G. van Sloun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+With_P/0/1/0/all/0/1"&gt;P.H.N. de With&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sommen_F/0/1/0/all/0/1"&gt;F. van der Sommen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Physical Hard-Label Attacks on Deep Learning Visual Classification. (arXiv:2002.07088v4 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.07088</id>
        <link href="http://arxiv.org/abs/2002.07088"/>
        <updated>2021-08-05T01:56:20.164Z</updated>
        <summary type="html"><![CDATA[The physical, black-box hard-label setting is arguably the most realistic
threat model for cyber-physical vision systems. In this setting, the attacker
only has query access to the model and only receives the top-1 class label
without confidence information. Creating small physical stickers that are
robust to environmental variation is difficult in the discrete and
discontinuous hard-label space because the attack must both design a small
shape to perturb within and find robust noise to fill it with. Unfortunately,
we find that existing $\ell_2$ or $\ell_\infty$ minimizing hard-label attacks
do not easily extend to finding such robust physical perturbation attacks.
Thus, we propose GRAPHITE, the first algorithm for hard-label physical attacks
on computer vision models. We show that "survivability", an estimate of
physical variation robustness, can be used in new ways to generate small masks
and is a sufficiently smooth function to optimize with gradient-free
optimization. We use GRAPHITE to attack a traffic sign classifier and a
publicly-available Automatic License Plate Recognition (ALPR) tool using only
query access. We evaluate both tools in real-world field tests to measure its
physical-world robustness. We successfully cause a Stop sign to be
misclassified as a Speed Limit 30 km/hr sign in 95.7% of physical images and
cause errors in 75% of physical images for the ALPR tool.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Feng_R/0/1/0/all/0/1"&gt;Ryan Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jiefeng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fernandes_E/0/1/0/all/0/1"&gt;Earlence Fernandes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jha_S/0/1/0/all/0/1"&gt;Somesh Jha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prakash_A/0/1/0/all/0/1"&gt;Atul Prakash&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Imbalanced Image Classification with Complement Cross Entropy. (arXiv:2009.02189v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.02189</id>
        <link href="http://arxiv.org/abs/2009.02189"/>
        <updated>2021-08-05T01:56:20.156Z</updated>
        <summary type="html"><![CDATA[Recently, deep learning models have achieved great success in computer vision
applications, relying on large-scale class-balanced datasets. However,
imbalanced class distributions still limit the wide applicability of these
models due to degradation in performance. To solve this problem, in this paper,
we concentrate on the study of cross entropy which mostly ignores output scores
on incorrect classes. This work discovers that neutralizing predicted
probabilities on incorrect classes improves the prediction accuracy for
imbalanced image classification. This paper proposes a simple but effective
loss named complement cross entropy based on this finding. The proposed loss
makes the ground truth class overwhelm the other classes in terms of softmax
probability, by neutralizing probabilities of incorrect classes, without
additional training procedures. Along with it, this loss facilitates the models
to learn key information especially from samples on minority classes. It
ensures more accurate and robust classification results on imbalanced
distributions. Extensive experiments on imbalanced datasets demonstrate the
effectiveness of the proposed method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1"&gt;Yechan Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1"&gt;Younkwan Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jeon_M/0/1/0/all/0/1"&gt;Moongu Jeon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CLAWS: Clustering Assisted Weakly Supervised Learning with Normalcy Suppression for Anomalous Event Detection. (arXiv:2011.12077v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.12077</id>
        <link href="http://arxiv.org/abs/2011.12077"/>
        <updated>2021-08-05T01:56:20.148Z</updated>
        <summary type="html"><![CDATA[Learning to detect real-world anomalous events through video-level labels is
a challenging task due to the rare occurrence of anomalies as well as noise in
the labels. In this work, we propose a weakly supervised anomaly detection
method which has manifold contributions including1) a random batch based
training procedure to reduce inter-batch correlation, 2) a normalcy suppression
mechanism to minimize anomaly scores of the normal regions of a video by taking
into account the overall information available in one training batch, and 3) a
clustering distance based loss to contribute towards mitigating the label noise
and to produce better anomaly representations by encouraging our model to
generate distinct normal and anomalous clusters. The proposed method
obtains83.03% and 89.67% frame-level AUC performance on the UCF Crime and
ShanghaiTech datasets respectively, demonstrating its superiority over the
existing state-of-the-art algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zaheer_M/0/1/0/all/0/1"&gt;Muhammad Zaigham Zaheer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mahmood_A/0/1/0/all/0/1"&gt;Arif Mahmood&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Astrid_M/0/1/0/all/0/1"&gt;Marcella Astrid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1"&gt;Seung-Ik Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Essential Features: Content-Adaptive Pixel Discretization to Improve Model Robustness to Adaptive Adversarial Attacks. (arXiv:2012.01699v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.01699</id>
        <link href="http://arxiv.org/abs/2012.01699"/>
        <updated>2021-08-05T01:56:20.140Z</updated>
        <summary type="html"><![CDATA[To remove the effects of adversarial perturbations, preprocessing defenses
such as pixel discretization are appealing due to their simplicity but have so
far been shown to be ineffective except on simple datasets such as MNIST,
leading to the belief that pixel discretization approaches are doomed to
failure as a defense technique. This paper revisits the pixel discretization
approaches. We hypothesize that the reason why existing approaches have failed
is that they have used a fixed codebook for the entire dataset. In particular,
we find that can lead to situations where images become more susceptible to
adversarial perturbations and also suffer significant loss of accuracy after
discretization. We propose a novel image preprocessing technique called
Essential Features that uses an adaptive codebook that is based on per-image
content and threat model. Essential Features adaptively selects a separable set
of color clusters for each image to reduce the color space while preserving the
pertinent features of the original image, maximizing both separability and
representation of colors. Additionally, to limit the adversary's ability to
influence the chosen color clusters, Essential Features takes advantage of
spatial correlation with an adaptive blur that moves pixels closer to their
original value without destroying original edge information. We design several
adaptive attacks and find that our approach is more robust than previous
baselines on $L_\infty$ and $L_2$ bounded attacks for several challenging
datasets including CIFAR-10, GTSRB, RESISC45, and ImageNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Feng_R/0/1/0/all/0/1"&gt;Ryan Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_W/0/1/0/all/0/1"&gt;Wu-chi Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prakash_A/0/1/0/all/0/1"&gt;Atul Prakash&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to generate shape from global-local spectra. (arXiv:2108.02161v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02161</id>
        <link href="http://arxiv.org/abs/2108.02161"/>
        <updated>2021-08-05T01:56:20.119Z</updated>
        <summary type="html"><![CDATA[In this work, we present a new learning-based pipeline for the generation of
3D shapes. We build our method on top of recent advances on the so called
shape-from-spectrum paradigm, which aims at recovering the full 3D geometric
structure of an object only from the eigenvalues of its Laplacian operator. In
designing our learning strategy, we consider the spectrum as a natural and
ready to use representation to encode variability of the shapes. Therefore, we
propose a simple decoder-only architecture that directly maps spectra to 3D
embeddings; in particular, we combine information from global and local
spectra, the latter being obtained from localized variants of the manifold
Laplacian. This combination captures the relations between the full shape and
its local parts, leading to more accurate generation of geometric details and
an improved semantic control in shape synthesis and novel editing applications.
Our results confirm the improvement of the proposed approach in comparison to
existing and alternative methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pegoraro_M/0/1/0/all/0/1"&gt;Marco Pegoraro&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Marin_R/0/1/0/all/0/1"&gt;Riccardo Marin&lt;/a&gt; (2), &lt;a href="http://arxiv.org/find/cs/1/au:+Castellani_U/0/1/0/all/0/1"&gt;Umberto Castellani&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Melzi_S/0/1/0/all/0/1"&gt;Simone Melzi&lt;/a&gt; (2), &lt;a href="http://arxiv.org/find/cs/1/au:+Rodola_E/0/1/0/all/0/1"&gt;Emanuele Rodol&amp;#xe0;&lt;/a&gt; (2) ((1) University of Verona, (2) Sapienza University of Rome)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Point Discriminative Learning for Unsupervised Representation Learning on 3D Point Clouds. (arXiv:2108.02104v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02104</id>
        <link href="http://arxiv.org/abs/2108.02104"/>
        <updated>2021-08-05T01:56:20.111Z</updated>
        <summary type="html"><![CDATA[Recently deep learning has achieved significant progress on point cloud
analysis tasks. Learning good representations is of vital importance to these
tasks. Most current methods rely on massive labelled data for training. We here
propose a point discriminative learning method for unsupervised representation
learning on 3D point clouds, which can learn local and global geometry
features. We achieve this by imposing a novel point discrimination loss on the
middle level and global level point features produced in the backbone network.
This point discrimination loss enforces the features to be consistent with
points belonging to the shape surface and inconsistent with randomly sampled
noisy points. Our method is simple in design, which works by adding an extra
adaptation module and a point consistency module for unsupervised training of
the encoder in the backbone network. Once trained, these two modules can be
discarded during supervised training of the classifier or decoder for
down-stream tasks. We conduct extensive experiments on 3D object
classification, 3D part segmentation and shape reconstruction in various
unsupervised and transfer settings. Both quantitative and qualitative results
show that our method learns powerful representations and achieves new
state-of-the-art performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1"&gt;Fayao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1"&gt;Guosheng Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Foo_C/0/1/0/all/0/1"&gt;Chuan-Sheng Foo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Situational Fusion of Visual Representation for Visual Navigation. (arXiv:1908.09073v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1908.09073</id>
        <link href="http://arxiv.org/abs/1908.09073"/>
        <updated>2021-08-05T01:56:20.104Z</updated>
        <summary type="html"><![CDATA[A complex visual navigation task puts an agent in different situations which
call for a diverse range of visual perception abilities. For example, to "go to
the nearest chair", the agent might need to identify a chair in a living room
using semantics, follow along a hallway using vanishing point cues, and avoid
obstacles using depth. Therefore, utilizing the appropriate visual perception
abilities based on a situational understanding of the visual environment can
empower these navigation models in unseen visual environments. We propose to
train an agent to fuse a large set of visual representations that correspond to
diverse visual perception abilities. To fully utilize each representation, we
develop an action-level representation fusion scheme, which predicts an action
candidate from each representation and adaptively consolidate these action
candidates into the final action. Furthermore, we employ a data-driven
inter-task affinity regularization to reduce redundancies and improve
generalization. Our approach leads to a significantly improved performance in
novel environments over ImageNet-pretrained baseline and other fusion methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shen_B/0/1/0/all/0/1"&gt;Bokui Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1"&gt;Danfei Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yuke Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guibas_L/0/1/0/all/0/1"&gt;Leonidas J. Guibas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1"&gt;Li Fei-Fei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Savarese_S/0/1/0/all/0/1"&gt;Silvio Savarese&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Coherent Visual Storytelling with Ordered Image Attention. (arXiv:2108.02180v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02180</id>
        <link href="http://arxiv.org/abs/2108.02180"/>
        <updated>2021-08-05T01:56:20.097Z</updated>
        <summary type="html"><![CDATA[We address the problem of visual storytelling, i.e., generating a story for a
given sequence of images. While each sentence of the story should describe a
corresponding image, a coherent story also needs to be consistent and relate to
both future and past images. To achieve this we develop ordered image attention
(OIA). OIA models interactions between the sentence-corresponding image and
important regions in other images of the sequence. To highlight the important
objects, a message-passing-like algorithm collects representations of those
objects in an order-aware manner. To generate the story's sentences, we then
highlight important image attention vectors with an Image-Sentence Attention
(ISA). Further, to alleviate common linguistic mistakes like repetitiveness, we
introduce an adaptive prior. The obtained results improve the METEOR score on
the VIST dataset by 1%. In addition, an extensive human study verifies
coherency improvements and shows that OIA and ISA generated stories are more
focused, shareable, and image-grounded.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Braude_T/0/1/0/all/0/1"&gt;Tom Braude&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schwartz_I/0/1/0/all/0/1"&gt;Idan Schwartz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schwing_A/0/1/0/all/0/1"&gt;Alexander Schwing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shamir_A/0/1/0/all/0/1"&gt;Ariel Shamir&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pervasive Hand Gesture Recognition for Smartphones using Non-audible Sound and Deep Learning. (arXiv:2108.02148v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2108.02148</id>
        <link href="http://arxiv.org/abs/2108.02148"/>
        <updated>2021-08-05T01:56:20.088Z</updated>
        <summary type="html"><![CDATA[Due to the mass advancement in ubiquitous technologies nowadays, new
pervasive methods have come into the practice to provide new innovative
features and stimulate the research on new human-computer interactions. This
paper presents a hand gesture recognition method that utilizes the smartphone's
built-in speakers and microphones. The proposed system emits an ultrasonic
sonar-based signal (inaudible sound) from the smartphone's stereo speakers,
which is then received by the smartphone's microphone and processed via a
Convolutional Neural Network (CNN) for Hand Gesture Recognition. Data
augmentation techniques are proposed to improve the detection accuracy and
three dual-channel input fusion methods are compared. The first method merges
the dual-channel audio as a single input spectrogram image. The second method
adopts early fusion by concatenating the dual-channel spectrograms. The third
method adopts late fusion by having two convectional input branches processing
each of the dual-channel spectrograms and then the outputs are merged by the
last layers. Our experimental results demonstrate a promising detection
accuracy for the six gestures presented in our publicly available dataset with
an accuracy of 93.58\% as a baseline.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ibrahim_A/0/1/0/all/0/1"&gt;Ahmed Ibrahim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+El_Refai_A/0/1/0/all/0/1"&gt;Ayman El-Refai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ahmed_S/0/1/0/all/0/1"&gt;Sara Ahmed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aboul_Ela_M/0/1/0/all/0/1"&gt;Mariam Aboul-Ela&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eraqi_H/0/1/0/all/0/1"&gt;Hesham M. Eraqi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moustafa_M/0/1/0/all/0/1"&gt;Mohamed Moustafa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Portrait Lighting Enhancement with 3D Guidance. (arXiv:2108.02121v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02121</id>
        <link href="http://arxiv.org/abs/2108.02121"/>
        <updated>2021-08-05T01:56:20.080Z</updated>
        <summary type="html"><![CDATA[Despite recent breakthroughs in deep learning methods for image lighting
enhancement, they are inferior when applied to portraits because 3D facial
information is ignored in their models. To address this, we present a novel
deep learning framework for portrait lighting enhancement based on 3D facial
guidance. Our framework consists of two stages. In the first stage, corrected
lighting parameters are predicted by a network from the input bad lighting
image, with the assistance of a 3D morphable model and a differentiable
renderer. Given the predicted lighting parameter, the differentiable renderer
renders a face image with corrected shading and texture, which serves as the 3D
guidance for learning image lighting enhancement in the second stage. To better
exploit the long-range correlations between the input and the guidance, in the
second stage, we design an image-to-image translation network with a novel
transformer architecture, which automatically produces a lighting-enhanced
result. Experimental results on the FFHQ dataset and in-the-wild images show
that the proposed method outperforms state-of-the-art methods in terms of both
quantitative metrics and visual quality. We will publish our dataset along with
more results on https://cassiepython.github.io/egsr/index.html.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Han_F/0/1/0/all/0/1"&gt;Fangzhou Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Can Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_H/0/1/0/all/0/1"&gt;Hao Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liao_J/0/1/0/all/0/1"&gt;Jing Liao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Physics-based Noise Modeling for Extreme Low-light Photography. (arXiv:2108.02158v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2108.02158</id>
        <link href="http://arxiv.org/abs/2108.02158"/>
        <updated>2021-08-05T01:56:20.061Z</updated>
        <summary type="html"><![CDATA[Enhancing the visibility in extreme low-light environments is a challenging
task. Under nearly lightless condition, existing image denoising methods could
easily break down due to significantly low SNR. In this paper, we
systematically study the noise statistics in the imaging pipeline of CMOS
photosensors, and formulate a comprehensive noise model that can accurately
characterize the real noise structures. Our novel model considers the noise
sources caused by digital camera electronics which are largely overlooked by
existing methods yet have significant influence on raw measurement in the dark.
It provides a way to decouple the intricate noise structure into different
statistical distributions with physical interpretations. Moreover, our noise
model can be used to synthesize realistic training data for learning-based
low-light denoising algorithms. In this regard, although promising results have
been shown recently with deep convolutional neural networks, the success
heavily depends on abundant noisy clean image pairs for training, which are
tremendously difficult to obtain in practice. Generalizing their trained models
to images from new devices is also problematic. Extensive experiments on
multiple low-light denoising datasets -- including a newly collected one in
this work covering various devices -- show that a deep neural network trained
with our proposed noise formation model can reach surprisingly-high accuracy.
The results are on par with or sometimes even outperform training with paired
real data, opening a new door to real-world extreme low-light photography.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Wei_K/0/1/0/all/0/1"&gt;Kaixuan Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Fu_Y/0/1/0/all/0/1"&gt;Ying Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zheng_Y/0/1/0/all/0/1"&gt;Yinqiang Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yang_J/0/1/0/all/0/1"&gt;Jiaolong Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semi-weakly Supervised Contrastive Representation Learning for Retinal Fundus Images. (arXiv:2108.02122v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02122</id>
        <link href="http://arxiv.org/abs/2108.02122"/>
        <updated>2021-08-05T01:56:20.054Z</updated>
        <summary type="html"><![CDATA[We explore the value of weak labels in learning transferable representations
for medical images. Compared to hand-labeled datasets, weak or inexact labels
can be acquired in large quantities at significantly lower cost and can provide
useful training signals for data-hungry models such as deep neural networks. We
consider weak labels in the form of pseudo-labels and propose a semi-weakly
supervised contrastive learning (SWCL) framework for representation learning
using semi-weakly annotated images. Specifically, we train a semi-supervised
model to propagate labels from a small dataset consisting of diverse
image-level annotations to a large unlabeled dataset. Using the propagated
labels, we generate a patch-level dataset for pretraining and formulate a
multi-label contrastive learning objective to capture position-specific
features encoded in each patch. We empirically validate the transfer learning
performance of SWCL on seven public retinal fundus datasets, covering three
disease classification tasks and two anatomical structure segmentation tasks.
Our experiment results suggest that, under very low data regime, large-scale
ImageNet pretraining on improved architecture remains a very strong baseline,
and recently proposed self-supervised methods falter in segmentation tasks,
possibly due to the strong invariant constraint imposed. Our method surpasses
all prior self-supervised methods and standard cross-entropy training, while
closing the gaps with ImageNet pretraining.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yap_B/0/1/0/all/0/1"&gt;Boon Peng Yap&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ng_B/0/1/0/all/0/1"&gt;Beng Koon Ng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[OncoNet: Weakly Supervised Siamese Network to automate cancer treatment response assessment between longitudinal FDG PET/CT examinations. (arXiv:2108.02016v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2108.02016</id>
        <link href="http://arxiv.org/abs/2108.02016"/>
        <updated>2021-08-05T01:56:20.040Z</updated>
        <summary type="html"><![CDATA[FDG PET/CT imaging is a resource intensive examination critical for managing
malignant disease and is particularly important for longitudinal assessment
during therapy. Approaches to automate longtudinal analysis present many
challenges including lack of available longitudinal datasets, managing complex
large multimodal imaging examinations, and need for detailed annotations for
traditional supervised machine learning. In this work we develop OncoNet, novel
machine learning algorithm that assesses treatment response from a 1,954 pairs
of sequential FDG PET/CT exams through weak supervision using the standard
uptake values (SUVmax) in associated radiology reports. OncoNet demonstrates an
AUROC of 0.86 and 0.84 on internal and external institution test sets
respectively for determination of change between scans while also showing
strong agreement to clinical scoring systems with a kappa score of 0.8. We also
curated a dataset of 1,954 paired FDG PET/CT exams designed for response
assessment for the broader machine learning in healthcare research community.
Automated assessment of radiographic response from FDG PET/CT with OncoNet
could provide clinicians with a valuable tool to rapidly and consistently
interpret change over time in longitudinal multi-modal imaging exams.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Joshi_A/0/1/0/all/0/1"&gt;Anirudh Joshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Eyuboglu_S/0/1/0/all/0/1"&gt;Sabri Eyuboglu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Huang_S/0/1/0/all/0/1"&gt;Shih-Cheng Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dunnmon_J/0/1/0/all/0/1"&gt;Jared Dunnmon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Soin_A/0/1/0/all/0/1"&gt;Arjun Soin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Davidzon_G/0/1/0/all/0/1"&gt;Guido Davidzon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chaudhari_A/0/1/0/all/0/1"&gt;Akshay Chaudhari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lungren_M/0/1/0/all/0/1"&gt;Matthew P Lungren&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimizing Latency for Online Video CaptioningUsing Audio-Visual Transformers. (arXiv:2108.02147v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02147</id>
        <link href="http://arxiv.org/abs/2108.02147"/>
        <updated>2021-08-05T01:56:20.031Z</updated>
        <summary type="html"><![CDATA[Video captioning is an essential technology to understand scenes and describe
events in natural language. To apply it to real-time monitoring, a system needs
not only to describe events accurately but also to produce the captions as soon
as possible. Low-latency captioning is needed to realize such functionality,
but this research area for online video captioning has not been pursued yet.
This paper proposes a novel approach to optimize each caption's output timing
based on a trade-off between latency and caption quality. An audio-visual
Trans-former is trained to generate ground-truth captions using only a small
portion of all video frames, and to mimic outputs of a pre-trained Transformer
to which all the frames are given. A CNN-based timing detector is also trained
to detect a proper output timing, where the captions generated by the two
Trans-formers become sufficiently close to each other. With the jointly trained
Transformer and timing detector, a caption can be generated in the early stages
of an event-triggered video clip, as soon as an event happens or when it can be
forecasted. Experiments with the ActivityNet Captions dataset show that our
approach achieves 94% of the caption quality of the upper bound given by the
pre-trained Transformer using the entire video clips, using only 28% of frames
from the beginning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hori_C/0/1/0/all/0/1"&gt;Chiori Hori&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hori_T/0/1/0/all/0/1"&gt;Takaaki Hori&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roux_J/0/1/0/all/0/1"&gt;Jonathan Le Roux&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Question-controlled Text-aware Image Captioning. (arXiv:2108.02059v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02059</id>
        <link href="http://arxiv.org/abs/2108.02059"/>
        <updated>2021-08-05T01:56:20.020Z</updated>
        <summary type="html"><![CDATA[For an image with multiple scene texts, different people may be interested in
different text information. Current text-aware image captioning models are not
able to generate distinctive captions according to various information needs.
To explore how to generate personalized text-aware captions, we define a new
challenging task, namely Question-controlled Text-aware Image Captioning
(Qc-TextCap). With questions as control signals, this task requires models to
understand questions, find related scene texts and describe them together with
objects fluently in human language. Based on two existing text-aware captioning
datasets, we automatically construct two datasets, ControlTextCaps and
ControlVizWiz to support the task. We propose a novel Geometry and Question
Aware Model (GQAM). GQAM first applies a Geometry-informed Visual Encoder to
fuse region-level object features and region-level scene text features with
considering spatial relationships. Then, we design a Question-guided Encoder to
select the most relevant visual features for each question. Finally, GQAM
generates a personalized text-aware caption with a Multimodal Decoder. Our
model achieves better captioning performance and question answering ability
than carefully designed baselines on both two datasets. With questions as
control signals, our model generates more informative and diverse captions than
the state-of-the-art text-aware captioning model. Our code and datasets are
publicly available at https://github.com/HAWLYQ/Qc-TextCap.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_A/0/1/0/all/0/1"&gt;Anwen Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Shizhe Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_Q/0/1/0/all/0/1"&gt;Qin Jin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Label Gold Asymmetric Loss Correction with Single-Label Regulators. (arXiv:2108.02032v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02032</id>
        <link href="http://arxiv.org/abs/2108.02032"/>
        <updated>2021-08-05T01:56:20.002Z</updated>
        <summary type="html"><![CDATA[Multi-label learning is an emerging extension of the multi-class
classification where an image contains multiple labels. Not only acquiring a
clean and fully labeled dataset in multi-label learning is extremely expensive,
but also many of the actual labels are corrupted or missing due to the
automated or non-expert annotation techniques. Noisy label data decrease the
prediction performance drastically. In this paper, we propose a novel Gold
Asymmetric Loss Correction with Single-Label Regulators (GALC-SLR) that
operates robust against noisy labels. GALC-SLR estimates the noise confusion
matrix using single-label samples, then constructs an asymmetric loss
correction via estimated confusion matrix to avoid overfitting to the noisy
labels. Empirical results show that our method outperforms the state-of-the-art
original asymmetric loss multi-label classifier under all corruption levels,
showing mean average precision improvement up to 28.67% on a real world dataset
of MS-COCO, yielding a better generalization of the unseen data and increased
prediction performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pene_C/0/1/0/all/0/1"&gt;Cosmin Octavian Pene&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghiassi_A/0/1/0/all/0/1"&gt;Amirmasoud Ghiassi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Younesian_T/0/1/0/all/0/1"&gt;Taraneh Younesian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Birke_R/0/1/0/all/0/1"&gt;Robert Birke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Lydia Y.Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MRI to PET Cross-Modality Translation using Globally and Locally Aware GAN (GLA-GAN) for Multi-Modal Diagnosis of Alzheimer's Disease. (arXiv:2108.02160v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2108.02160</id>
        <link href="http://arxiv.org/abs/2108.02160"/>
        <updated>2021-08-05T01:56:19.995Z</updated>
        <summary type="html"><![CDATA[Medical imaging datasets are inherently high dimensional with large
variability and low sample sizes that limit the effectiveness of deep learning
algorithms. Recently, generative adversarial networks (GANs) with the ability
to synthesize realist images have shown great potential as an alternative to
standard data augmentation techniques. Our work focuses on cross-modality
synthesis of fluorodeoxyglucose~(FDG) Positron Emission Tomography~(PET) scans
from structural Magnetic Resonance~(MR) images using generative models to
facilitate multi-modal diagnosis of Alzheimer's disease (AD). Specifically, we
propose a novel end-to-end, globally and locally aware image-to-image
translation GAN (GLA-GAN) with a multi-path architecture that enforces both
global structural integrity and fidelity to local details. We further
supplement the standard adversarial loss with voxel-level intensity,
multi-scale structural similarity (MS-SSIM) and region-of-interest (ROI) based
loss components that reduce reconstruction error, enforce structural
consistency at different scales and perceive variation in regional sensitivity
to AD respectively. Experimental results demonstrate that our GLA-GAN not only
generates synthesized FDG-PET scans with enhanced image quality but also
superior clinical utility in improving AD diagnosis compared to
state-of-the-art models. Finally, we attempt to interpret some of the internal
units of the GAN that are closely related to this specific cross-modality
generation task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Sikka_A/0/1/0/all/0/1"&gt;Apoorva Sikka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Skand/0/1/0/all/0/1"&gt;Skand&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Virk_J/0/1/0/all/0/1"&gt;Jitender Singh Virk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bathula_D/0/1/0/all/0/1"&gt;Deepti R. Bathula&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Online Knowledge Distillation for Efficient Pose Estimation. (arXiv:2108.02092v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02092</id>
        <link href="http://arxiv.org/abs/2108.02092"/>
        <updated>2021-08-05T01:56:19.988Z</updated>
        <summary type="html"><![CDATA[Existing state-of-the-art human pose estimation methods require heavy
computational resources for accurate predictions. One promising technique to
obtain an accurate yet lightweight pose estimator is knowledge distillation,
which distills the pose knowledge from a powerful teacher model to a
less-parameterized student model. However, existing pose distillation works
rely on a heavy pre-trained estimator to perform knowledge transfer and require
a complex two-stage learning procedure. In this work, we investigate a novel
Online Knowledge Distillation framework by distilling Human Pose structure
knowledge in a one-stage manner to guarantee the distillation efficiency,
termed OKDHP. Specifically, OKDHP trains a single multi-branch network and
acquires the predicted heatmaps from each, which are then assembled by a
Feature Aggregation Unit (FAU) as the target heatmaps to teach each branch in
reverse. Instead of simply averaging the heatmaps, FAU which consists of
multiple parallel transformations with different receptive fields, leverages
the multi-scale information, thus obtains target heatmaps with higher-quality.
Specifically, the pixel-wise Kullback-Leibler (KL) divergence is utilized to
minimize the discrepancy between the target heatmaps and the predicted ones,
which enables the student network to learn the implicit keypoint relationship.
Besides, an unbalanced OKDHP scheme is introduced to customize the student
networks with different compression rates. The effectiveness of our approach is
demonstrated by extensive experiments on two common benchmark datasets, MPII
and COCO.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zheng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1"&gt;Jingwen Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_M/0/1/0/all/0/1"&gt;Mingli Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Ying Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_Z/0/1/0/all/0/1"&gt;Zhigeng Pan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Free Lunch for Co-Saliency Detection: Context Adjustment. (arXiv:2108.02093v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02093</id>
        <link href="http://arxiv.org/abs/2108.02093"/>
        <updated>2021-08-05T01:56:19.968Z</updated>
        <summary type="html"><![CDATA[We unveil a long-standing problem in the prevailing co-saliency detection
systems: there is indeed inconsistency between training and testing.
Constructing a high-quality co-saliency detection dataset involves
time-consuming and labor-intensive pixel-level labeling, which has forced most
recent works to rely instead on semantic segmentation or saliency detection
datasets for training. However, the lack of proper co-saliency and the absence
of multiple foreground objects in these datasets can lead to spurious
variations and inherent biases learned by models. To tackle this, we introduce
the idea of counterfactual training through context adjustment, and propose a
"cost-free" group-cut-paste (GCP) procedure to leverage images from
off-the-shelf saliency detection datasets and synthesize new samples. Following
GCP, we collect a novel dataset called Context Adjustment Training. The two
variants of our dataset, i.e., CAT and CAT+, consist of 16,750 and 33,500
images, respectively. All images are automatically annotated with high-quality
masks. As a side-product, object categories, as well as edge information, are
also provided to facilitate other related works. Extensive experiments with
state-of-the-art models are conducted to demonstrate the superiority of our
dataset. We hope that the scale, diversity, and quality of CAT/CAT+ can benefit
researchers in this area and beyond. The dataset and benchmark toolkit will be
accessible through our project page.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kong_L/0/1/0/all/0/1"&gt;Lingdong Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ganesh_P/0/1/0/all/0/1"&gt;Prakhar Ganesh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1"&gt;Tan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Junhao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yao Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Le Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Online unsupervised Learning for domain shift in COVID-19 CT scan datasets. (arXiv:2108.02002v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2108.02002</id>
        <link href="http://arxiv.org/abs/2108.02002"/>
        <updated>2021-08-05T01:56:19.956Z</updated>
        <summary type="html"><![CDATA[Neural networks often require large amounts of expert annotated data to
train. When changes are made in the process of medical imaging, trained
networks may not perform as well, and obtaining large amounts of expert
annotations for each change in the imaging process can be time consuming and
expensive. Online unsupervised learning is a method that has been proposed to
deal with situations where there is a domain shift in incoming data, and a lack
of annotations. The aim of this study is to see whether online unsupervised
learning can help COVID-19 CT scan classification models adjust to slight
domain shifts, when there are no annotations available for the new data. A
total of six experiments are performed using three test datasets with differing
amounts of domain shift. These experiments compare the performance of the
online unsupervised learning strategy to a baseline, as well as comparing how
the strategy performs on different domain shifts. Code for online unsupervised
learning can be found at this link:
https://github.com/Mewtwo/online-unsupervised-learning]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Ewen_N/0/1/0/all/0/1"&gt;Nicolas Ewen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Khan_N/0/1/0/all/0/1"&gt;Naimul Khan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adversarial Energy Disaggregation for Non-intrusive Load Monitoring. (arXiv:2108.01998v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2108.01998</id>
        <link href="http://arxiv.org/abs/2108.01998"/>
        <updated>2021-08-05T01:56:19.938Z</updated>
        <summary type="html"><![CDATA[Energy disaggregation, also known as non-intrusive load monitoring (NILM),
challenges the problem of separating the whole-home electricity usage into
appliance-specific individual consumptions, which is a typical application of
data analysis. {NILM aims to help households understand how the energy is used
and consequently tell them how to effectively manage the energy, thus allowing
energy efficiency which is considered as one of the twin pillars of sustainable
energy policy (i.e., energy efficiency and renewable energy).} Although NILM is
unidentifiable, it is widely believed that the NILM problem can be addressed by
data science. Most of the existing approaches address the energy disaggregation
problem by conventional techniques such as sparse coding, non-negative matrix
factorization, and hidden Markov model. Recent advances reveal that deep neural
networks (DNNs) can get favorable performance for NILM since DNNs can
inherently learn the discriminative signatures of the different appliances. In
this paper, we propose a novel method named adversarial energy disaggregation
(AED) based on DNNs. We introduce the idea of adversarial learning into NILM,
which is new for the energy disaggregation task. Our method trains a generator
and multiple discriminators via an adversarial fashion. The proposed method not
only learns shard representations for different appliances, but captures the
specific multimode structures of each appliance. Extensive experiments on
real-world datasets verify that our method can achieve new state-of-the-art
performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Du_Z/0/1/0/all/0/1"&gt;Zhekai Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1"&gt;Jingjing Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhu_L/0/1/0/all/0/1"&gt;Lei Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lu_K/0/1/0/all/0/1"&gt;Ke Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shen_H/0/1/0/all/0/1"&gt;Heng Tao Shen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Compatible Embeddings. (arXiv:2108.01958v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.01958</id>
        <link href="http://arxiv.org/abs/2108.01958"/>
        <updated>2021-08-05T01:56:19.928Z</updated>
        <summary type="html"><![CDATA[Achieving backward compatibility when rolling out new models can highly
reduce costs or even bypass feature re-encoding of existing gallery images for
in-production visual retrieval systems. Previous related works usually leverage
losses used in knowledge distillation which can cause performance degradations
or not guarantee compatibility. To address these issues, we propose a general
framework called Learning Compatible Embeddings (LCE) which is applicable for
both cross model compatibility and compatible training in
direct/forward/backward manners. Our compatibility is achieved by aligning
class centers between models directly or via a transformation, and restricting
more compact intra-class distributions for the new model. Experiments are
conducted in extensive scenarios such as changes of training dataset, loss
functions, network architectures as well as feature dimensions, and demonstrate
that LCE efficiently enables model compatibility with marginal sacrifices of
accuracies. The code will be available at https://github.com/IrvingMeng/LCE.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Meng_Q/0/1/0/all/0/1"&gt;Qiang Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chixiang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1"&gt;Xiaoqiang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_F/0/1/0/all/0/1"&gt;Feng Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Signature Verification using Geometrical Features and Artificial Neural Network Classifier. (arXiv:2108.02029v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02029</id>
        <link href="http://arxiv.org/abs/2108.02029"/>
        <updated>2021-08-05T01:56:19.918Z</updated>
        <summary type="html"><![CDATA[Signature verification has been one of the major researched areas in the
field of computer vision. Many financial and legal organizations use signature
verification as access control and authentication. Signature images are not
rich in texture; however, they have much vital geometrical information. Through
this work, we have proposed a signature verification methodology that is simple
yet effective. The technique presented in this paper harnesses the geometrical
features of a signature image like center, isolated points, connected
components, etc., and with the power of Artificial Neural Network (ANN)
classifier, classifies the signature image based on their geometrical features.
Publicly available dataset MCYT, BHSig260 (contains the image of two regional
languages Bengali and Hindi) has been used in this paper to test the
effectiveness of the proposed method. We have received a lower Equal Error Rate
(EER) on MCYT 100 dataset and higher accuracy on the BHSig260 dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1"&gt;Anamika Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1"&gt;Satish Kumar Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_K/0/1/0/all/0/1"&gt;Krishna Pratap Singh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ICECAP: Information Concentrated Entity-aware Image Captioning. (arXiv:2108.02050v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02050</id>
        <link href="http://arxiv.org/abs/2108.02050"/>
        <updated>2021-08-05T01:56:19.912Z</updated>
        <summary type="html"><![CDATA[Most current image captioning systems focus on describing general image
content, and lack background knowledge to deeply understand the image, such as
exact named entities or concrete events. In this work, we focus on the
entity-aware news image captioning task which aims to generate informative
captions by leveraging the associated news articles to provide background
knowledge about the target image. However, due to the length of news articles,
previous works only employ news articles at the coarse article or sentence
level, which are not fine-grained enough to refine relevant events and choose
named entities accurately. To overcome these limitations, we propose an
Information Concentrated Entity-aware news image CAPtioning (ICECAP) model,
which progressively concentrates on relevant textual information within the
corresponding news article from the sentence level to the word level. Our model
first creates coarse concentration on relevant sentences using a cross-modality
retrieval model and then generates captions by further concentrating on
relevant words within the sentences. Extensive experiments on both BreakingNews
and GoodNews datasets demonstrate the effectiveness of our proposed method,
which outperforms other state-of-the-arts. The code of ICECAP is publicly
available at https://github.com/HAWLYQ/ICECAP.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_A/0/1/0/all/0/1"&gt;Anwen Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Shizhe Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_Q/0/1/0/all/0/1"&gt;Qin Jin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Human-In-The-Loop Document Layout Analysis. (arXiv:2108.02095v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02095</id>
        <link href="http://arxiv.org/abs/2108.02095"/>
        <updated>2021-08-05T01:56:19.905Z</updated>
        <summary type="html"><![CDATA[Document layout analysis (DLA) aims to divide a document image into different
types of regions. DLA plays an important role in the document content
understanding and information extraction systems. Exploring a method that can
use less data for effective training contributes to the development of DLA. We
consider a Human-in-the-loop (HITL) collaborative intelligence in the DLA. Our
approach was inspired by the fact that the HITL push the model to learn from
the unknown problems by adding a small amount of data based on knowledge. The
HITL select key samples by using confidence. However, using confidence to find
key samples is not suitable for DLA tasks. We propose the Key Samples Selection
(KSS) method to find key samples in high-level tasks (semantic segmentation)
more accurately through agent collaboration, effectively reducing costs. Once
selected, these key samples are passed to human beings for active labeling,
then the model will be updated with the labeled samples. Hence, we revisited
the learning system from reinforcement learning and designed a sample-based
agent update strategy, which effectively improves the agent's ability to accept
new samples. It achieves significant improvement results in two benchmarks
(DSSE-200 (from 77.1% to 86.3%) and CS-150 (from 88.0% to 95.6%)) by using 10%
of labeled data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1"&gt;Xingjiao Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1"&gt;Tianlong Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1"&gt;Qin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1"&gt;Liang He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Skeleton Cloud Colorization for Unsupervised 3D Action Representation Learning. (arXiv:2108.01959v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.01959</id>
        <link href="http://arxiv.org/abs/2108.01959"/>
        <updated>2021-08-05T01:56:19.885Z</updated>
        <summary type="html"><![CDATA[Skeleton-based human action recognition has attracted increasing attention in
recent years. However, most of the existing works focus on supervised learning
which requiring a large number of annotated action sequences that are often
expensive to collect. We investigate unsupervised representation learning for
skeleton action recognition, and design a novel skeleton cloud colorization
technique that is capable of learning skeleton representations from unlabeled
skeleton sequence data. Specifically, we represent a skeleton action sequence
as a 3D skeleton cloud and colorize each point in the cloud according to its
temporal and spatial orders in the original (unannotated) skeleton sequence.
Leveraging the colorized skeleton point cloud, we design an auto-encoder
framework that can learn spatial-temporal features from the artificial color
labels of skeleton joints effectively. We evaluate our skeleton cloud
colorization approach with action classifiers trained under different
configurations, including unsupervised, semi-supervised and fully-supervised
settings. Extensive experiments on NTU RGB+D and NW-UCLA datasets show that the
proposed method outperforms existing unsupervised and semi-supervised 3D action
recognition methods by large margins, and it achieves competitive performance
in supervised 3D action recognition as well.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1"&gt;Siyuan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jun Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1"&gt;Shijian Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Er_M/0/1/0/all/0/1"&gt;Meng Hwa Er&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kot_A/0/1/0/all/0/1"&gt;Alex C. Kot&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DuCN: Dual-children Network for Medical Diagnosis and Similar Case Recommendation towards COVID-19. (arXiv:2108.01997v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2108.01997</id>
        <link href="http://arxiv.org/abs/2108.01997"/>
        <updated>2021-08-05T01:56:19.860Z</updated>
        <summary type="html"><![CDATA[Early detection of the coronavirus disease 2019 (COVID-19) helps to treat
patients timely and increase the cure rate, thus further suppressing the spread
of the disease. In this study, we propose a novel deep learning based detection
and similar case recommendation network to help control the epidemic. Our
proposed network contains two stages: the first one is a lung region
segmentation step and is used to exclude irrelevant factors, and the second is
a detection and recommendation stage. Under this framework, in the second
stage, we develop a dual-children network (DuCN) based on a pre-trained
ResNet-18 to simultaneously realize the disease diagnosis and similar case
recommendation. Besides, we employ triplet loss and intrapulmonary distance
maps to assist the detection, which helps incorporate tiny differences between
two images and is conducive to improving the diagnostic accuracy. For each
confirmed COVID-19 case, we give similar cases to provide radiologists with
diagnosis and treatment references. We conduct experiments on a large publicly
available dataset (CC-CCII) and compare the proposed model with
state-of-the-art COVID-19 detection methods. The results show that our proposed
model achieves a promising clinical performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Peng_C/0/1/0/all/0/1"&gt;Chengtao Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Long_Y/0/1/0/all/0/1"&gt;Yunfei Long&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhu_S/0/1/0/all/0/1"&gt;Senhua Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tu_D/0/1/0/all/0/1"&gt;Dandan Tu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_B/0/1/0/all/0/1"&gt;Bin Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Supervised Learning of Depth and Ego-Motion from Video by Alternative Training and Geometric Constraints from 3D to 2D. (arXiv:2108.01980v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.01980</id>
        <link href="http://arxiv.org/abs/2108.01980"/>
        <updated>2021-08-05T01:56:19.853Z</updated>
        <summary type="html"><![CDATA[Self-supervised learning of depth and ego-motion from unlabeled monocular
video has acquired promising results and drawn extensive attention. Most
existing methods jointly train the depth and pose networks by photometric
consistency of adjacent frames based on the principle of structure-from-motion
(SFM). However, the coupling relationship of the depth and pose networks
seriously influences the learning performance, and the re-projection relations
is sensitive to scale ambiguity, especially for pose learning. In this paper,
we aim to improve the depth-pose learning performance without the auxiliary
tasks and address the above issues by alternative training each task and
incorporating the epipolar geometric constraints into the Iterative Closest
Point (ICP) based point clouds match process. Distinct from jointly training
the depth and pose networks, our key idea is to better utilize the mutual
dependency of these two tasks by alternatively training each network with
respective losses while fixing the other. We also design a log-scale 3D
structural consistency loss to put more emphasis on the smaller depth values
during training. To makes the optimization easier, we further incorporate the
epipolar geometry into the ICP based learning process for pose learning.
Extensive experiments on various benchmarks datasets indicate the superiority
of our algorithm over the state-of-the-art self-supervised methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fang_J/0/1/0/all/0/1"&gt;Jiaojiao Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1"&gt;Guizhong Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cross-modality Discrepant Interaction Network for RGB-D Salient Object Detection. (arXiv:2108.01971v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.01971</id>
        <link href="http://arxiv.org/abs/2108.01971"/>
        <updated>2021-08-05T01:56:19.846Z</updated>
        <summary type="html"><![CDATA[The popularity and promotion of depth maps have brought new vigor and
vitality into salient object detection (SOD), and a mass of RGB-D SOD
algorithms have been proposed, mainly concentrating on how to better integrate
cross-modality features from RGB image and depth map. For the cross-modality
interaction in feature encoder, existing methods either indiscriminately treat
RGB and depth modalities, or only habitually utilize depth cues as auxiliary
information of the RGB branch. Different from them, we reconsider the status of
two modalities and propose a novel Cross-modality Discrepant Interaction
Network (CDINet) for RGB-D SOD, which differentially models the dependence of
two modalities according to the feature representations of different layers. To
this end, two components are designed to implement the effective cross-modality
interaction: 1) the RGB-induced Detail Enhancement (RDE) module leverages RGB
modality to enhance the details of the depth features in low-level encoder
stage. 2) the Depth-induced Semantic Enhancement (DSE) module transfers the
object positioning and internal consistency of depth features to the RGB branch
in high-level encoder stage. Furthermore, we also design a Dense Decoding
Reconstruction (DDR) structure, which constructs a semantic block by combining
multi-level encoder features to upgrade the skip connection in the feature
decoding. Extensive experiments on five benchmark datasets demonstrate that our
network outperforms $15$ state-of-the-art methods both quantitatively and
qualitatively. Our code is publicly available at:
https://rmcong.github.io/proj_CDINet.html.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chen Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cong_R/0/1/0/all/0/1"&gt;Runmin Cong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Q/0/1/0/all/0/1"&gt;Qinwei Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1"&gt;Lin Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1"&gt;Feng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yao Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kwong_S/0/1/0/all/0/1"&gt;Sam Kwong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sensing Anomalies like Humans: A Hominine Framework to Detect Abnormal Events from Unlabeled Videos. (arXiv:2108.01975v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.01975</id>
        <link href="http://arxiv.org/abs/2108.01975"/>
        <updated>2021-08-05T01:56:19.839Z</updated>
        <summary type="html"><![CDATA[Video anomaly detection (VAD) has constantly been a vital topic in video
analysis. As anomalies are often rare, it is typically addressed under a
semi-supervised setup, which requires a training set with pure normal videos.
To avoid exhausted manual labeling, we are inspired by how humans sense
anomalies and propose a hominine framework that enables both unsupervised and
end-to-end VAD. The framework is based on two key observations: 1) Human
perception is usually local, i.e. focusing on local foreground and its context
when sensing anomalies. Thus, we propose to impose locality-awareness by
localizing foreground with generic knowledge, and a region localization
strategy is designed to exploit local context. 2) Frequently-occurred events
will mould humans' definition of normality, which motivates us to devise a
surrogate training paradigm. It trains a deep neural network (DNN) to learn a
surrogate task with unlabeled videos, and frequently-occurred events will play
a dominant role in "moulding" the DNN. In this way, a training loss gap will
automatically manifest rarely-seen novel events as anomalies. For
implementation, we explore various surrogate tasks as well as both classic and
emerging DNN models. Extensive evaluations on commonly-used VAD benchmarks
justify the framework's applicability to different surrogate tasks or DNN
models, and demonstrate its astonishing effectiveness: It not only outperforms
existing unsupervised solutions by a wide margin (8% to 10% AUROC gain), but
also achieves comparable or even superior performance to state-of-the-art
semi-supervised counterparts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Siqi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_G/0/1/0/all/0/1"&gt;Guang Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1"&gt;Zhiping Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_E/0/1/0/all/0/1"&gt;En Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xinwang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1"&gt;Jianping Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1"&gt;Chengzhang Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FPB: Feature Pyramid Branch for Person Re-Identification. (arXiv:2108.01901v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.01901</id>
        <link href="http://arxiv.org/abs/2108.01901"/>
        <updated>2021-08-05T01:56:19.829Z</updated>
        <summary type="html"><![CDATA[High performance person Re-Identification (Re-ID) requires the model to focus
on both global silhouette and local details of pedestrian. To extract such more
representative features, an effective way is to exploit deep models with
multiple branches. However, most multi-branch based methods implemented by
duplication of part backbone structure normally lead to severe increase of
computational cost. In this paper, we propose a lightweight Feature Pyramid
Branch (FPB) to extract features from different layers of networks and
aggregate them in a bidirectional pyramid structure. Cooperated by attention
modules and our proposed cross orthogonality regularization, FPB significantly
prompts the performance of backbone network by only introducing less than 1.5M
extra parameters. Extensive experimental results on standard benchmark datasets
demonstrate that our proposed FPB based model outperforms state-of-the-art
methods with obvious margin as well as much less model complexity. FPB borrows
the idea of the Feature Pyramid Network (FPN) from prevailing object detection
methods. To our best knowledge, it is the first successful application of
similar structure in person Re-ID tasks, which empirically proves that pyramid
network as affiliated branch could be a potential structure in related feature
embedding models. The source code is publicly available at
https://github.com/anocodetest1/FPB.git.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Suofei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1"&gt;Zirui Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1"&gt;Xiofu Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1"&gt;Kun Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1"&gt;Quan Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kang_B/0/1/0/all/0/1"&gt;Bin Kang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Effective Leaf Recognition Using Convolutional Neural Networks Based Features. (arXiv:2108.01808v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.01808</id>
        <link href="http://arxiv.org/abs/2108.01808"/>
        <updated>2021-08-05T01:56:19.807Z</updated>
        <summary type="html"><![CDATA[There is a warning light for the loss of plant habitats worldwide that
entails concerted efforts to conserve plant biodiversity. Thus, plant species
classification is of crucial importance to address this environmental
challenge. In recent years, there is a considerable increase in the number of
studies related to plant taxonomy. While some researchers try to improve their
recognition performance using novel approaches, others concentrate on
computational optimization for their framework. In addition, a few studies are
diving into feature extraction to gain significantly in terms of accuracy. In
this paper, we propose an effective method for the leaf recognition problem. In
our proposed approach, a leaf goes through some pre-processing to extract its
refined color image, vein image, xy-projection histogram, handcrafted shape,
texture features, and Fourier descriptors. These attributes are then
transformed into a better representation by neural network-based encoders
before a support vector machine (SVM) model is utilized to classify different
leaves. Overall, our approach performs a state-of-the-art result on the Flavia
leaf dataset, achieving the accuracy of 99.58\% on test sets under random
10-fold cross-validation and bypassing the previous methods. We also release
our codes\footnote{Scripts are available at
\url{https://github.com/dinhvietcuong1996/LeafRecognition}} for contributing to
the research community in the leaf classification problem.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Quach_B/0/1/0/all/0/1"&gt;Boi M. Quach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cuong_D/0/1/0/all/0/1"&gt;Dinh V. Cuong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pham_N/0/1/0/all/0/1"&gt;Nhung Pham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huynh_D/0/1/0/all/0/1"&gt;Dang Huynh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_B/0/1/0/all/0/1"&gt;Binh T. Nguyen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Curriculum learning for language modeling. (arXiv:2108.02170v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.02170</id>
        <link href="http://arxiv.org/abs/2108.02170"/>
        <updated>2021-08-05T01:56:19.793Z</updated>
        <summary type="html"><![CDATA[Language Models like ELMo and BERT have provided robust representations of
natural language, which serve as the language understanding component for a
diverse range of downstream tasks.Curriculum learning is a method that employs
a structured training regime instead, which has been leveraged in computer
vision and machine translation to improve model training speed and model
performance. While language models have proven transformational for the natural
language processing community, these models have proven expensive,
energy-intensive, and challenging to train. In this work, we explore the effect
of curriculum learning on language model pretraining using various
linguistically motivated curricula and evaluate transfer performance on the
GLUE Benchmark. Despite a broad variety of training methodologies and
experiments we do not find compelling evidence that curriculum learning methods
improve language model training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Campos_D/0/1/0/all/0/1"&gt;Daniel Campos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Robustness of Domain Adaption to Adversarial Attacks. (arXiv:2108.01807v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.01807</id>
        <link href="http://arxiv.org/abs/2108.01807"/>
        <updated>2021-08-05T01:56:19.783Z</updated>
        <summary type="html"><![CDATA[State-of-the-art deep neural networks (DNNs) have been proved to have
excellent performance on unsupervised domain adaption (UDA). However, recent
work shows that DNNs perform poorly when being attacked by adversarial samples,
where these attacks are implemented by simply adding small disturbances to the
original images. Although plenty of work has focused on this, as far as we
know, there is no systematic research on the robustness of unsupervised domain
adaption model. Hence, we discuss the robustness of unsupervised domain
adaption against adversarial attacking for the first time. We benchmark various
settings of adversarial attack and defense in domain adaption, and propose a
cross domain attack method based on pseudo label. Most importantly, we analyze
the impact of different datasets, models, attack methods and defense methods.
Directly, our work proves the limited robustness of unsupervised domain
adaptation model, and we hope our work may facilitate the community to pay more
attention to improve the robustness of the model against attacking.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Liyuan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1"&gt;Yuhang Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lei Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automatic hemisphere segmentation in rodent MRI with lesions. (arXiv:2108.01941v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2108.01941</id>
        <link href="http://arxiv.org/abs/2108.01941"/>
        <updated>2021-08-05T01:56:19.773Z</updated>
        <summary type="html"><![CDATA[We present MedicDeepLabv3+, a convolutional neural network that is the first
completely automatic method to segment brain hemispheres in magnetic resonance
(MR) images of rodents with lesions. MedicDeepLabv3+ improves the
state-of-the-art DeepLabv3+ with an advanced decoder, incorporating spatial
attention layers and additional skip connections that, as we show in our
experiments, lead to more precise segmentations. MedicDeepLabv3+ requires no MR
image preprocessing, such as bias-field correction or registration to a
template, produces segmentations in less than a second, and its GPU memory
requirements can be adjusted based on the available resources. Using a large
dataset of 723 MR rat brain images, we evaluated our MedicDeepLabv3+, two
state-of-the-art convolutional neural networks (DeepLabv3+, UNet) and three
approaches that were specifically designed for skull-stripping rodent MR images
(Demon, RATS and RBET). In our experiments, MedicDeepLabv3+ outperformed the
other methods, yielding an average Dice coefficient of 0.952 and 0.944 in the
brain and contralateral hemisphere regions. Additionally, we show that despite
limiting the GPU memory and the training data to only three images, our
MedicDeepLabv3+ also provided satisfactory segmentations. In conclusion, our
method, publicly available at https://github.com/jmlipman/MedicDeepLabv3Plus,
yielded excellent results in multiple scenarios, demonstrating its capability
to reduce human workload in rodent neuroimaging studies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Valverde_J/0/1/0/all/0/1"&gt;Juan Miguel Valverde&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shatillo_A/0/1/0/all/0/1"&gt;Artem Shatillo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Feo_R/0/1/0/all/0/1"&gt;Riccardo de Feo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tohka_J/0/1/0/all/0/1"&gt;Jussi Tohka&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PDE-GCN: Novel Architectures for Graph Neural Networks Motivated by Partial Differential Equations. (arXiv:2108.01938v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.01938</id>
        <link href="http://arxiv.org/abs/2108.01938"/>
        <updated>2021-08-05T01:56:19.753Z</updated>
        <summary type="html"><![CDATA[Graph neural networks are increasingly becoming the go-to approach in various
fields such as computer vision, computational biology and chemistry, where data
are naturally explained by graphs. However, unlike traditional convolutional
neural networks, deep graph networks do not necessarily yield better
performance than shallow graph networks. This behavior usually stems from the
over-smoothing phenomenon. In this work, we propose a family of architectures
to control this behavior by design. Our networks are motivated by numerical
methods for solving Partial Differential Equations (PDEs) on manifolds, and as
such, their behavior can be explained by similar analysis. Moreover, as we
demonstrate using an extensive set of experiments, our PDE-motivated networks
can generalize and be effective for various types of problems from different
fields. Our architectures obtain better or on par with the current
state-of-the-art results for problems that are typically approached using
different architectures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Eliasof_M/0/1/0/all/0/1"&gt;Moshe Eliasof&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Haber_E/0/1/0/all/0/1"&gt;Eldad Haber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Treister_E/0/1/0/all/0/1"&gt;Eran Treister&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Video Similarity and Alignment Learning on Partial Video Copy Detection. (arXiv:2108.01817v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.01817</id>
        <link href="http://arxiv.org/abs/2108.01817"/>
        <updated>2021-08-05T01:56:19.744Z</updated>
        <summary type="html"><![CDATA[Existing video copy detection methods generally measure video similarity
based on spatial similarities between key frames, neglecting the latent
similarity in temporal dimension, so that the video similarity is biased
towards spatial information. There are methods modeling unified video
similarity in an end-to-end way, but losing detailed partial alignment
information, which causes the incapability of copy segments localization. To
address the above issues, we propose the Video Similarity and Alignment
Learning (VSAL) approach, which jointly models spatial similarity, temporal
similarity and partial alignment. To mitigate the spatial similarity bias, we
model the temporal similarity as the mask map predicted from frame-level
spatial similarity, where each element indicates the probability of frame pair
lying right on the partial alignments. To further localize partial copies, the
step map is learned from the spatial similarity where the elements indicate
extending directions of the current partial alignments on the spatial-temporal
similarity map. Obtained from the mask map, the start points extend out into
partial optimal alignments following instructions of the step map. With the
similarity and alignment learning strategy, VSAL achieves the state-of-the-art
F1-score on VCDB core dataset. Furthermore, we construct a new benchmark of
partial video copy detection and localization by adding new segment-level
annotations for FIVR-200k dataset, where VSAL also achieves the best
performance, verifying its effectiveness in more challenging situations. Our
project is publicly available at https://pvcd-vsal.github.io/vsal/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1"&gt;Zhen Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1"&gt;Xiangteng He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_M/0/1/0/all/0/1"&gt;Mingqian Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lv_Y/0/1/0/all/0/1"&gt;Yiliang Lv&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generic Neural Architecture Search via Regression. (arXiv:2108.01899v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.01899</id>
        <link href="http://arxiv.org/abs/2108.01899"/>
        <updated>2021-08-05T01:56:19.736Z</updated>
        <summary type="html"><![CDATA[Most existing neural architecture search (NAS) algorithms are dedicated to
the downstream tasks, e.g., image classification in computer vision. However,
extensive experiments have shown that, prominent neural architectures, such as
ResNet in computer vision and LSTM in natural language processing, are
generally good at extracting patterns from the input data and perform well on
different downstream tasks. These observations inspire us to ask: Is it
necessary to use the performance of specific downstream tasks to evaluate and
search for good neural architectures? Can we perform NAS effectively and
efficiently while being agnostic to the downstream task? In this work, we
attempt to affirmatively answer the above two questions and improve the
state-of-the-art NAS solution by proposing a novel and generic NAS framework,
termed Generic NAS (GenNAS). GenNAS does not use task-specific labels but
instead adopts \textit{regression} on a set of manually designed synthetic
signal bases for architecture evaluation. Such a self-supervised regression
task can effectively evaluate the intrinsic power of an architecture to capture
and transform the input signal patterns, and allow more sufficient usage of
training samples. We then propose an automatic task search to optimize the
combination of synthetic signals using limited downstream-task-specific labels,
further improving the performance of GenNAS. We also thoroughly evaluate
GenNAS's generality and end-to-end NAS performance on all search spaces, which
outperforms almost all existing works with significant speedup.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yuhong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hao_C/0/1/0/all/0/1"&gt;Cong Hao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1"&gt;Pan Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_J/0/1/0/all/0/1"&gt;Jinjun Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1"&gt;Deming Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Operator-Splitting Method for the Gaussian Curvature Regularization Model with Applications in Surface Smoothing and Imaging. (arXiv:2108.01914v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.01914</id>
        <link href="http://arxiv.org/abs/2108.01914"/>
        <updated>2021-08-05T01:56:19.720Z</updated>
        <summary type="html"><![CDATA[Gaussian curvature is an important geometric property of surfaces, which has
been used broadly in mathematical modeling. Due to the full nonlinearity of the
Gaussian curvature, efficient numerical methods for models based on it are
uncommon in literature. In this article, we propose an operator-splitting
method for a general Gaussian curvature model. In our method, we decouple the
full nonlinearity of Gaussian curvature from differential operators by
introducing two matrix- and vector-valued functions. The optimization problem
is then converted into the search for the steady state solution of a time
dependent PDE system. The above PDE system is well-suited to time
discretization by operator splitting, the sub-problems encountered at each
fractional step having either a closed form solution or being solvable by
efficient algorithms. The proposed method is not sensitive to the choice of
parameters, its efficiency and performances being demonstrated via systematic
experiments on surface smoothing and image denoising.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Hao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tai_X/0/1/0/all/0/1"&gt;Xue-Cheng Tai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Glowinski_R/0/1/0/all/0/1"&gt;Roland Glowinski&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Internal Video Inpainting by Implicit Long-range Propagation. (arXiv:2108.01912v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.01912</id>
        <link href="http://arxiv.org/abs/2108.01912"/>
        <updated>2021-08-05T01:56:19.713Z</updated>
        <summary type="html"><![CDATA[We propose a novel framework for video inpainting by adopting an internal
learning strategy. Unlike previous methods that use optical flow for
cross-frame context propagation to inpaint unknown regions, we show that this
can be achieved implicitly by fitting a convolutional neural network to the
known region. Moreover, to handle challenging sequences with ambiguous
backgrounds or long-term occlusion, we design two regularization terms to
preserve high-frequency details and long-term temporal consistency. Extensive
experiments on the DAVIS dataset demonstrate that the proposed method achieves
state-of-the-art inpainting quality quantitatively and qualitatively. We
further extend the proposed method to another challenging task: learning to
remove an object from a video giving a single object mask in only one frame in
a 4K video.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ouyang_H/0/1/0/all/0/1"&gt;Hao Ouyang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1"&gt;Tengfei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1"&gt;Qifeng Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A universal detector of CNN-generated images using properties of checkerboard artifacts in the frequency domain. (arXiv:2108.01892v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.01892</id>
        <link href="http://arxiv.org/abs/2108.01892"/>
        <updated>2021-08-05T01:56:19.705Z</updated>
        <summary type="html"><![CDATA[We propose a novel universal detector for detecting images generated by using
CNNs. In this paper, properties of checkerboard artifacts in CNN-generated
images are considered, and the spectrum of images is enhanced in accordance
with the properties. Next, a classifier is trained by using the enhanced
spectrums to judge a query image to be a CNN-generated ones or not. In
addition, an ensemble of the proposed detector with emphasized spectrums and a
conventional detector is proposed to improve the performance of these methods.
In an experiment, the proposed ensemble is demonstrated to outperform a
state-of-the-art method under some conditions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tanaka_M/0/1/0/all/0/1"&gt;Miki Tanaka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shiota_S/0/1/0/all/0/1"&gt;Sayaka Shiota&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kiya_H/0/1/0/all/0/1"&gt;Hitoshi Kiya&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Emergent Discrete Communication in SemanticSpaces. (arXiv:2108.01828v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.01828</id>
        <link href="http://arxiv.org/abs/2108.01828"/>
        <updated>2021-08-05T01:56:19.685Z</updated>
        <summary type="html"><![CDATA[Neural agents trained in reinforcement learning settings can learn to
communicate among themselves via discrete tokens, accomplishing as a team what
agents would be unable to do alone. However, the current standard of using
one-hot vectors as discrete communication tokens prevents agents from acquiring
more desirable aspects of communication such as zero-shot understanding.
Inspired by word embedding techniques from natural language processing, we
propose neural agent architectures that enables them to communicate via
discrete tokens derived from a learned, continuous space. We show in a decision
theoretic framework that our technique optimizes communication over a wide
range of scenarios, whereas one-hot tokens are only optimal under restrictive
assumptions. In self-play experiments, we validate that our trained agents
learn to cluster tokens in semantically-meaningful ways, allowing them
communicate in noisy environments where other techniques fail. Lastly, we
demonstrate both that agents using our method can effectively respond to novel
human communication and that humans can understand unlabeled emergent agent
communication, outperforming the use of one-hot communication.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tucker_M/0/1/0/all/0/1"&gt;Mycal Tucker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Huao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agrawal_S/0/1/0/all/0/1"&gt;Siddharth Agrawal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hughes_D/0/1/0/all/0/1"&gt;Dana Hughes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sycara_K/0/1/0/all/0/1"&gt;Katia Sycara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lewis_M/0/1/0/all/0/1"&gt;Michael Lewis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shah_J/0/1/0/all/0/1"&gt;Julie Shah&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Combining Attention with Flow for Person Image Synthesis. (arXiv:2108.01823v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.01823</id>
        <link href="http://arxiv.org/abs/2108.01823"/>
        <updated>2021-08-05T01:56:19.678Z</updated>
        <summary type="html"><![CDATA[Pose-guided person image synthesis aims to synthesize person images by
transforming reference images into target poses. In this paper, we observe that
the commonly used spatial transformation blocks have complementary advantages.
We propose a novel model by combining the attention operation with the
flow-based operation. Our model not only takes the advantage of the attention
operation to generate accurate target structures but also uses the flow-based
operation to sample realistic source textures. Both objective and subjective
experiments demonstrate the superiority of our model. Meanwhile, comprehensive
ablation studies verify our hypotheses and show the efficacy of the proposed
modules. Besides, additional experiments on the portrait image editing task
demonstrate the versatility of the proposed combination.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1"&gt;Yurui Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yubo Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1"&gt;Thomas H. Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1"&gt;Shan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1"&gt;Ge Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploiting BERT For Multimodal Target SentimentClassification Through Input Space Translation. (arXiv:2108.01682v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.01682</id>
        <link href="http://arxiv.org/abs/2108.01682"/>
        <updated>2021-08-05T01:56:19.671Z</updated>
        <summary type="html"><![CDATA[Multimodal target/aspect sentiment classification combines multimodal
sentiment analysis and aspect/target sentiment classification. The goal of the
task is to combine vision and language to understand the sentiment towards a
target entity in a sentence. Twitter is an ideal setting for the task because
it is inherently multimodal, highly emotional, and affects real world events.
However, multimodal tweets are short and accompanied by complex, possibly
irrelevant images. We introduce a two-stream model that translates images in
input space using an object-aware transformer followed by a single-pass
non-autoregressive text generation approach. We then leverage the translation
to construct an auxiliary sentence that provides multimodal information to a
language model. Our approach increases the amount of text available to the
language model and distills the object-level information in complex images. We
achieve state-of-the-art performance on two multimodal Twitter datasets without
modifying the internals of the language model to accept multimodal data,
demonstrating the effectiveness of our translation. In addition, we explain a
failure mode of a popular approach for aspect sentiment analysis when applied
to tweets. Our code is available at
\textcolor{blue}{\url{https://github.com/codezakh/exploiting-BERT-thru-translation}}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khan_Z/0/1/0/all/0/1"&gt;Zaid Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1"&gt;Yun Fu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Knowledgeable Prompt-tuning: Incorporating Knowledge into Prompt Verbalizer for Text Classification. (arXiv:2108.02035v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.02035</id>
        <link href="http://arxiv.org/abs/2108.02035"/>
        <updated>2021-08-05T01:56:19.664Z</updated>
        <summary type="html"><![CDATA[Tuning pre-trained language models (PLMs) with task-specific prompts has been
a promising approach for text classification. Particularly, previous studies
suggest that prompt-tuning has remarkable superiority in the low-data scenario
over the generic fine-tuning methods with extra classifiers. The core idea of
prompt-tuning is to insert text pieces, i.e., template, to the input and
transform a classification problem into a masked language modeling problem,
where a crucial step is to construct a projection, i.e., verbalizer, between a
label space and a label word space. A verbalizer is usually handcrafted or
searched by gradient descent, which may lack coverage and bring considerable
bias and high variances to the results. In this work, we focus on incorporating
external knowledge into the verbalizer, forming a knowledgeable prompt-tuning
(KPT), to improve and stabilize prompt-tuning. Specifically, we expand the
label word space of the verbalizer using external knowledge bases (KBs) and
refine the expanded label word space with the PLM itself before predicting with
the expanded label word space. Extensive experiments on zero and few-shot text
classification tasks demonstrate the effectiveness of knowledgeable
prompt-tuning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1"&gt;Shengding Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_N/0/1/0/all/0/1"&gt;Ning Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Huadong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhiyuan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Juanzi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1"&gt;Maosong Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Domain Adaptation for Retinal Vessel Segmentation with Adversarial Learning and Transfer Normalization. (arXiv:2108.01821v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2108.01821</id>
        <link href="http://arxiv.org/abs/2108.01821"/>
        <updated>2021-08-05T01:56:19.649Z</updated>
        <summary type="html"><![CDATA[Retinal vessel segmentation plays a key role in computer-aided screening,
diagnosis, and treatment of various cardiovascular and ophthalmic diseases.
Recently, deep learning-based retinal vessel segmentation algorithms have
achieved remarkable performance. However, due to the domain shift problem, the
performance of these algorithms often degrades when they are applied to new
data that is different from the training data. Manually labeling new data for
each test domain is often a time-consuming and laborious task. In this work, we
explore unsupervised domain adaptation in retinal vessel segmentation by using
entropy-based adversarial learning and transfer normalization layer to train a
segmentation network, which generalizes well across domains and requires no
annotation of the target domain. Specifically, first, an entropy-based
adversarial learning strategy is developed to reduce the distribution
discrepancy between the source and target domains while also achieving the
objective of entropy minimization on the target domain. In addition, a new
transfer normalization layer is proposed to further boost the transferability
of the deep network. It normalizes the features of each domain separately to
compensate for the domain distribution gap. Besides, it also adaptively selects
those feature channels that are more transferable between domains, thus further
enhancing the generalization performance of the network. We conducted extensive
experiments on three regular fundus image datasets and an ultra-widefield
fundus image dataset, and the results show that our approach yields significant
performance gains compared to other state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Feng_W/0/1/0/all/0/1"&gt;Wei Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ju_L/0/1/0/all/0/1"&gt;Lie Ju&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_L/0/1/0/all/0/1"&gt;Lin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Song_K/0/1/0/all/0/1"&gt;Kaimin Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhao_X/0/1/0/all/0/1"&gt;Xin Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tao_Q/0/1/0/all/0/1"&gt;Qingyi Tao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ge_Z/0/1/0/all/0/1"&gt;Zongyuan Ge&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ensemble of MRR and NDCG models for Visual Dialog. (arXiv:2104.07511v3 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.07511</id>
        <link href="http://arxiv.org/abs/2104.07511"/>
        <updated>2021-08-05T01:56:19.642Z</updated>
        <summary type="html"><![CDATA[Assessing an AI agent that can converse in human language and understand
visual content is challenging. Generation metrics, such as BLEU scores favor
correct syntax over semantics. Hence a discriminative approach is often used,
where an agent ranks a set of candidate options. The mean reciprocal rank (MRR)
metric evaluates the model performance by taking into account the rank of a
single human-derived answer. This approach, however, raises a new challenge:
the ambiguity and synonymy of answers, for instance, semantic equivalence
(e.g., `yeah' and `yes'). To address this, the normalized discounted cumulative
gain (NDCG) metric has been used to capture the relevance of all the correct
answers via dense annotations. However, the NDCG metric favors the usually
applicable uncertain answers such as `I don't know. Crafting a model that
excels on both MRR and NDCG metrics is challenging. Ideally, an AI agent should
answer a human-like reply and validate the correctness of any answer. To
address this issue, we describe a two-step non-parametric ranking approach that
can merge strong MRR and NDCG models. Using our approach, we manage to keep
most MRR state-of-the-art performance (70.41% vs. 71.24%) and the NDCG
state-of-the-art performance (72.16% vs. 75.35%). Moreover, our approach won
the recent Visual Dialog 2020 challenge. Source code is available at
https://github.com/idansc/mrr-ndcg.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schwartz_I/0/1/0/all/0/1"&gt;Idan Schwartz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Empirical Evaluation of End-to-End Polyphonic Optical Music Recognition. (arXiv:2108.01769v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.01769</id>
        <link href="http://arxiv.org/abs/2108.01769"/>
        <updated>2021-08-05T01:56:19.634Z</updated>
        <summary type="html"><![CDATA[Previous work has shown that neural architectures are able to perform optical
music recognition (OMR) on monophonic and homophonic music with high accuracy.
However, piano and orchestral scores frequently exhibit polyphonic passages,
which add a second dimension to the task. Monophonic and homophonic music can
be described as homorhythmic, or having a single musical rhythm. Polyphonic
music, on the other hand, can be seen as having multiple rhythmic sequences, or
voices, concurrently. We first introduce a workflow for creating large-scale
polyphonic datasets suitable for end-to-end recognition from sheet music
publicly available on the MuseScore forum. We then propose two novel
formulations for end-to-end polyphonic OMR -- one treating the problem as a
type of multi-task binary classification, and the other treating it as
multi-sequence detection. Building upon the encoder-decoder architecture and an
image encoder proposed in past work on end-to-end OMR, we propose two novel
decoder models -- FlagDecoder and RNNDecoder -- that correspond to the two
formulations. Finally, we compare the empirical performance of these end-to-end
approaches to polyphonic OMR and observe a new state-of-the-art performance
with our multi-sequence detection decoder, RNNDecoder.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Edirisooriya_S/0/1/0/all/0/1"&gt;Sachinda Edirisooriya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1"&gt;Hao-Wen Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McAuley_J/0/1/0/all/0/1"&gt;Julian McAuley&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berg_Kirkpatrick_T/0/1/0/all/0/1"&gt;Taylor Berg-Kirkpatrick&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Sentence-Level Relation Extraction through Curriculum Learning. (arXiv:2107.09332v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.09332</id>
        <link href="http://arxiv.org/abs/2107.09332"/>
        <updated>2021-08-05T01:56:19.624Z</updated>
        <summary type="html"><![CDATA[Sentence-level relation extraction mainly aims to classify the relation
between two entities in a sentence. The sentence-level relation extraction
corpus often contains data that are difficult for the model to infer or noise
data. In this paper, we propose a curriculum learning-based relation extraction
model that splits data by difficulty and utilizes them for learning. In the
experiments with the representative sentence-level relation extraction
datasets, TACRED and Re-TACRED, the proposed method obtained an F1-score of
75.0% and 91.4% respectively, which are the state-of-the-art performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1"&gt;Seongsik Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1"&gt;Harksoo Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning by Fixing: Solving Math Word Problems with Weak Supervision. (arXiv:2012.10582v2 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.10582</id>
        <link href="http://arxiv.org/abs/2012.10582"/>
        <updated>2021-08-05T01:56:19.617Z</updated>
        <summary type="html"><![CDATA[Previous neural solvers of math word problems (MWPs) are learned with full
supervision and fail to generate diverse solutions. In this paper, we address
this issue by introducing a \textit{weakly-supervised} paradigm for learning
MWPs. Our method only requires the annotations of the final answers and can
generate various solutions for a single problem. To boost weakly-supervised
learning, we propose a novel \textit{learning-by-fixing} (LBF) framework, which
corrects the misperceptions of the neural network via symbolic reasoning.
Specifically, for an incorrect solution tree generated by the neural network,
the \textit{fixing} mechanism propagates the error from the root node to the
leaf nodes and infers the most probable fix that can be executed to get the
desired answer. To generate more diverse solutions, \textit{tree
regularization} is applied to guide the efficient shrinkage and exploration of
the solution space, and a \textit{memory buffer} is designed to track and save
the discovered various fixes for each problem. Experimental results on the
Math23K dataset show the proposed LBF framework significantly outperforms
reinforcement learning baselines in weakly-supervised learning. Furthermore, it
achieves comparable top-1 and much better top-3/5 answer accuracies than
fully-supervised methods, demonstrating its strength in producing diverse
solutions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1"&gt;Yining Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1"&gt;Qing Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ciao_D/0/1/0/all/0/1"&gt;Daniel Ciao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1"&gt;Siyuan Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1"&gt;Song-Chun Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Beyond Offline Mapping: Learning Cross Lingual Word Embeddings through Context Anchoring. (arXiv:2012.15715v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.15715</id>
        <link href="http://arxiv.org/abs/2012.15715"/>
        <updated>2021-08-05T01:56:19.610Z</updated>
        <summary type="html"><![CDATA[Recent research on cross-lingual word embeddings has been dominated by
unsupervised mapping approaches that align monolingual embeddings. Such methods
critically rely on those embeddings having a similar structure, but it was
recently shown that the separate training in different languages causes
departures from this assumption. In this paper, we propose an alternative
approach that does not have this limitation, while requiring a weak seed
dictionary (e.g., a list of identical words) as the only form of supervision.
Rather than aligning two fixed embedding spaces, our method works by fixing the
target language embeddings, and learning a new set of embeddings for the source
language that are aligned with them. To that end, we use an extension of
skip-gram that leverages translated context words as anchor points, and
incorporates self-learning and iterative restarts to reduce the dependency on
the initial dictionary. Our approach outperforms conventional mapping methods
on bilingual lexicon induction, and obtains competitive results in the
downstream XNLI task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ormazabal_A/0/1/0/all/0/1"&gt;Aitor Ormazabal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Artetxe_M/0/1/0/all/0/1"&gt;Mikel Artetxe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soroa_A/0/1/0/all/0/1"&gt;Aitor Soroa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Labaka_G/0/1/0/all/0/1"&gt;Gorka Labaka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agirre_E/0/1/0/all/0/1"&gt;Eneko Agirre&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Scene Decoration from a Single Photograph. (arXiv:2108.01806v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.01806</id>
        <link href="http://arxiv.org/abs/2108.01806"/>
        <updated>2021-08-05T01:56:19.602Z</updated>
        <summary type="html"><![CDATA[Furnishing and rendering an indoor scene is a common but tedious task for
interior design: an artist needs to observe the space, create a conceptual
design, build a 3D model, and perform rendering. In this paper, we introduce a
new problem of domain-specific image synthesis using generative modeling,
namely neural scene decoration. Given a photograph of an empty indoor space, we
aim to synthesize a new image of the same space that is fully furnished and
decorated. Neural scene decoration can be applied in practice to efficiently
generate conceptual but realistic interior designs, bypassing the traditional
multi-step and time-consuming pipeline. Our attempt to neural scene decoration
in this paper is a generative adversarial neural network that takes the input
photograph and directly produce the image of the desired furnishing and
decorations. Our network contains a novel image generator that transforms an
initial point-based object layout into a realistic photograph. We demonstrate
the performance of our proposed method by showing that it outperforms the
baselines built upon previous works on image translations both qualitatively
and quantitatively. Our user study further validates the plausibility and
aesthetics in the generated designs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pang_H/0/1/0/all/0/1"&gt;Hong-Wing Pang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yingshu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hua_B/0/1/0/all/0/1"&gt;Binh-Son Hua&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yeung_S/0/1/0/all/0/1"&gt;Sai-Kit Yeung&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ensemble of MRR and NDCG models for Visual Dialog. (arXiv:2104.07511v3 [cs.AI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.07511</id>
        <link href="http://arxiv.org/abs/2104.07511"/>
        <updated>2021-08-05T01:56:19.592Z</updated>
        <summary type="html"><![CDATA[Assessing an AI agent that can converse in human language and understand
visual content is challenging. Generation metrics, such as BLEU scores favor
correct syntax over semantics. Hence a discriminative approach is often used,
where an agent ranks a set of candidate options. The mean reciprocal rank (MRR)
metric evaluates the model performance by taking into account the rank of a
single human-derived answer. This approach, however, raises a new challenge:
the ambiguity and synonymy of answers, for instance, semantic equivalence
(e.g., `yeah' and `yes'). To address this, the normalized discounted cumulative
gain (NDCG) metric has been used to capture the relevance of all the correct
answers via dense annotations. However, the NDCG metric favors the usually
applicable uncertain answers such as `I don't know. Crafting a model that
excels on both MRR and NDCG metrics is challenging. Ideally, an AI agent should
answer a human-like reply and validate the correctness of any answer. To
address this issue, we describe a two-step non-parametric ranking approach that
can merge strong MRR and NDCG models. Using our approach, we manage to keep
most MRR state-of-the-art performance (70.41% vs. 71.24%) and the NDCG
state-of-the-art performance (72.16% vs. 75.35%). Moreover, our approach won
the recent Visual Dialog 2020 challenge. Source code is available at
https://github.com/idansc/mrr-ndcg.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schwartz_I/0/1/0/all/0/1"&gt;Idan Schwartz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Coherent Visual Storytelling with Ordered Image Attention. (arXiv:2108.02180v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02180</id>
        <link href="http://arxiv.org/abs/2108.02180"/>
        <updated>2021-08-05T01:56:19.584Z</updated>
        <summary type="html"><![CDATA[We address the problem of visual storytelling, i.e., generating a story for a
given sequence of images. While each sentence of the story should describe a
corresponding image, a coherent story also needs to be consistent and relate to
both future and past images. To achieve this we develop ordered image attention
(OIA). OIA models interactions between the sentence-corresponding image and
important regions in other images of the sequence. To highlight the important
objects, a message-passing-like algorithm collects representations of those
objects in an order-aware manner. To generate the story's sentences, we then
highlight important image attention vectors with an Image-Sentence Attention
(ISA). Further, to alleviate common linguistic mistakes like repetitiveness, we
introduce an adaptive prior. The obtained results improve the METEOR score on
the VIST dataset by 1%. In addition, an extensive human study verifies
coherency improvements and shows that OIA and ISA generated stories are more
focused, shareable, and image-grounded.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Braude_T/0/1/0/all/0/1"&gt;Tom Braude&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schwartz_I/0/1/0/all/0/1"&gt;Idan Schwartz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schwing_A/0/1/0/all/0/1"&gt;Alexander Schwing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shamir_A/0/1/0/all/0/1"&gt;Ariel Shamir&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ReadOnce Transformers: Reusable Representations of Text for Transformers. (arXiv:2010.12854v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.12854</id>
        <link href="http://arxiv.org/abs/2010.12854"/>
        <updated>2021-08-05T01:56:19.538Z</updated>
        <summary type="html"><![CDATA[We present ReadOnce Transformers, an approach to convert a transformer-based
model into one that can build an information-capturing, task-independent, and
compressed representation of text. The resulting representation is reusable
across different examples and tasks, thereby requiring a document shared across
many examples or tasks to only be \emph{read once}. This leads to faster
training and evaluation of models. Additionally, we extend standard
text-to-text transformer models to Representation+Text-to-text models, and
evaluate on multiple downstream tasks: multi-hop QA, abstractive QA, and
long-document summarization. Our one-time computed representation results in a
2x-5x speedup compared to standard text-to-text models, while the compression
also allows existing language models to handle longer documents without the
need for designing new pre-trained models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1"&gt;Shih-Ting Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sabharwal_A/0/1/0/all/0/1"&gt;Ashish Sabharwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khot_T/0/1/0/all/0/1"&gt;Tushar Khot&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TERA: Self-Supervised Learning of Transformer Encoder Representation for Speech. (arXiv:2007.06028v3 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.06028</id>
        <link href="http://arxiv.org/abs/2007.06028"/>
        <updated>2021-08-05T01:56:19.521Z</updated>
        <summary type="html"><![CDATA[We introduce a self-supervised speech pre-training method called TERA, which
stands for Transformer Encoder Representations from Alteration. Recent
approaches often learn by using a single auxiliary task like contrastive
prediction, autoregressive prediction, or masked reconstruction. Unlike
previous methods, we use alteration along three orthogonal axes to pre-train
Transformer Encoders on a large amount of unlabeled speech. The model learns
through the reconstruction of acoustic frames from their altered counterpart,
where we use a stochastic policy to alter along various dimensions: time,
frequency, and magnitude. TERA can be used for speech representations
extraction or fine-tuning with downstream models. We evaluate TERA on several
downstream tasks, including phoneme classification, keyword spotting, speaker
recognition, and speech recognition. We present a large-scale comparison of
various self-supervised models. TERA achieves strong performance in the
comparison by improving upon surface features and outperforming previous
models. In our experiments, we study the effect of applying different
alteration techniques, pre-training on more data, and pre-training on various
features. We analyze different model sizes and find that smaller models are
strong representation learners than larger models, while larger models are more
effective for downstream fine-tuning than smaller models. Furthermore, we show
the proposed method is transferable to downstream datasets not used in
pre-training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Liu_A/0/1/0/all/0/1"&gt;Andy T. Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_S/0/1/0/all/0/1"&gt;Shang-Wen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lee_H/0/1/0/all/0/1"&gt;Hung-yi Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Round Parsing-based Multiword Rules for Scientific OpenIE. (arXiv:2108.02074v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.02074</id>
        <link href="http://arxiv.org/abs/2108.02074"/>
        <updated>2021-08-05T01:56:19.502Z</updated>
        <summary type="html"><![CDATA[Information extraction (IE) in scientific literature has facilitated many
down-stream tasks. OpenIE, which does not require any relation schema but
identifies a relational phrase to describe the relationship between a subject
and an object, is being a trending topic of IE in sciences. The subjects,
objects, and relations are often multiword expressions, which brings challenges
for methods to identify the boundaries of the expressions given very limited or
even no training data. In this work, we present a set of rules for extracting
structured information based on dependency parsing that can be applied to any
scientific dataset requiring no expert's annotation. Results on novel datasets
show the effectiveness of the proposed method. We discuss negative results as
well.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kuebler_J/0/1/0/all/0/1"&gt;Joseph Kuebler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tong_L/0/1/0/all/0/1"&gt;Lingbo Tong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1"&gt;Meng Jiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Safe Reinforcement Learning with Natural Language Constraints. (arXiv:2010.05150v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.05150</id>
        <link href="http://arxiv.org/abs/2010.05150"/>
        <updated>2021-08-05T01:56:19.494Z</updated>
        <summary type="html"><![CDATA[While safe reinforcement learning (RL) holds great promise for many practical
applications like robotics or autonomous cars, current approaches require
specifying constraints in mathematical form. Such specifications demand domain
expertise, limiting the adoption of safe RL. In this paper, we propose learning
to interpret natural language constraints for safe RL. To this end, we first
introduce HazardWorld, a new multi-task benchmark that requires an agent to
optimize reward while not violating constraints specified in free-form text. We
then develop an agent with a modular architecture that can interpret and adhere
to such textual constraints while learning new tasks. Our model consists of (1)
a constraint interpreter that encodes textual constraints into spatial and
temporal representations of forbidden states, and (2) a policy network that
uses these representations to produce a policy achieving minimal constraint
violations during training. Across different domains in HazardWorld, we show
that our method achieves higher rewards (up to11x) and fewer constraint
violations (by 1.8x) compared to existing approaches. However, in terms of
absolute performance, HazardWorld still poses significant challenges for agents
to learn efficiently, motivating the need for future work.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1"&gt;Tsung-Yen Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_M/0/1/0/all/0/1"&gt;Michael Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chow_Y/0/1/0/all/0/1"&gt;Yinlam Chow&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramadge_P/0/1/0/all/0/1"&gt;Peter J. Ramadge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Narasimhan_K/0/1/0/all/0/1"&gt;Karthik Narasimhan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AMU-EURANOVA at CASE 2021 Task 1: Assessing the stability of multilingual BERT. (arXiv:2106.14625v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.14625</id>
        <link href="http://arxiv.org/abs/2106.14625"/>
        <updated>2021-08-05T01:56:19.485Z</updated>
        <summary type="html"><![CDATA[This paper explains our participation in task 1 of the CASE 2021 shared task.
This task is about multilingual event extraction from news. We focused on
sub-task 4, event information extraction. This sub-task has a small training
dataset and we fine-tuned a multilingual BERT to solve this sub-task. We
studied the instability problem on the dataset and tried to mitigate it.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bouscarrat_L/0/1/0/all/0/1"&gt;L&amp;#xe9;o Bouscarrat&lt;/a&gt; (LIS, TALEP, QARMA), &lt;a href="http://arxiv.org/find/cs/1/au:+Bonnefoy_A/0/1/0/all/0/1"&gt;Antoine Bonnefoy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Capponi_C/0/1/0/all/0/1"&gt;C&amp;#xe9;cile Capponi&lt;/a&gt; (LIS, QARMA), &lt;a href="http://arxiv.org/find/cs/1/au:+Ramisch_C/0/1/0/all/0/1"&gt;Carlos Ramisch&lt;/a&gt; (LIS, TALEP)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transfer Learning for Pose Estimation of Illustrated Characters. (arXiv:2108.01819v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.01819</id>
        <link href="http://arxiv.org/abs/2108.01819"/>
        <updated>2021-08-05T01:56:19.461Z</updated>
        <summary type="html"><![CDATA[Human pose information is a critical component in many downstream image
processing tasks, such as activity recognition and motion tracking. Likewise, a
pose estimator for the illustrated character domain would provide a valuable
prior for assistive content creation tasks, such as reference pose retrieval
and automatic character animation. But while modern data-driven techniques have
substantially improved pose estimation performance on natural images, little
work has been done for illustrations. In our work, we bridge this domain gap by
efficiently transfer-learning from both domain-specific and task-specific
source models. Additionally, we upgrade and expand an existing illustrated pose
estimation dataset, and introduce two new datasets for classification and
segmentation subtasks. We then apply the resultant state-of-the-art character
pose estimator to solve the novel task of pose-guided illustration retrieval.
All data, models, and code will be made publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Shuhong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zwicker_M/0/1/0/all/0/1"&gt;Matthias Zwicker&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Specialize and Fuse: Pyramidal Output Representation for Semantic Segmentation. (arXiv:2108.01866v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.01866</id>
        <link href="http://arxiv.org/abs/2108.01866"/>
        <updated>2021-08-05T01:56:19.453Z</updated>
        <summary type="html"><![CDATA[We present a novel pyramidal output representation to ensure parsimony with
our "specialize and fuse" process for semantic segmentation. A pyramidal
"output" representation consists of coarse-to-fine levels, where each level is
"specialize" in a different class distribution (e.g., more stuff than things
classes at coarser levels). Two types of pyramidal outputs (i.e., unity and
semantic pyramid) are "fused" into the final semantic output, where the unity
pyramid indicates unity-cells (i.e., all pixels in such cell share the same
semantic label). The process ensures parsimony by predicting a relatively small
number of labels for unity-cells (e.g., a large cell of grass) to build the
final semantic output. In addition to the "output" representation, we design a
coarse-to-fine contextual module to aggregate the "features" representation
from different levels. We validate the effectiveness of each key module in our
method through comprehensive ablation studies. Finally, our approach achieves
state-of-the-art performance on three widely-used semantic segmentation
datasets -- ADE20K, COCO-Stuff, and Pascal-Context.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hsiao_C/0/1/0/all/0/1"&gt;Chi-Wei Hsiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1"&gt;Cheng Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Hwann-Tzong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1"&gt;Min Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Weakly Supervised Foreground Learning for Weakly Supervised Localization and Detection. (arXiv:2108.01785v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.01785</id>
        <link href="http://arxiv.org/abs/2108.01785"/>
        <updated>2021-08-05T01:56:19.446Z</updated>
        <summary type="html"><![CDATA[Modern deep learning models require large amounts of accurately annotated
data, which is often difficult to satisfy. Hence, weakly supervised tasks,
including weakly supervised object localization~(WSOL) and detection~(WSOD),
have recently received attention in the computer vision community. In this
paper, we motivate and propose the weakly supervised foreground learning (WSFL)
task by showing that both WSOL and WSOD can be greatly improved if groundtruth
foreground masks are available. More importantly, we propose a complete WSFL
pipeline with low computational cost, which generates pseudo boxes, learns
foreground masks, and does not need any localization annotations. With the help
of foreground masks predicted by our WSFL model, we achieve 72.97% correct
localization accuracy on CUB for WSOL, and 55.7% mean average precision on
VOC07 for WSOD, thereby establish new state-of-the-art for both tasks. Our WSFL
model also shows excellent transfer ability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chen-Lin Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1"&gt;Jianxin Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Vision Transformer with Progressive Sampling. (arXiv:2108.01684v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.01684</id>
        <link href="http://arxiv.org/abs/2108.01684"/>
        <updated>2021-08-05T01:56:19.439Z</updated>
        <summary type="html"><![CDATA[Transformers with powerful global relation modeling abilities have been
introduced to fundamental computer vision tasks recently. As a typical example,
the Vision Transformer (ViT) directly applies a pure transformer architecture
on image classification, by simply splitting images into tokens with a fixed
length, and employing transformers to learn relations between these tokens.
However, such naive tokenization could destruct object structures, assign grids
to uninterested regions such as background, and introduce interference signals.
To mitigate the above issues, in this paper, we propose an iterative and
progressive sampling strategy to locate discriminative regions. At each
iteration, embeddings of the current sampling step are fed into a transformer
encoder layer, and a group of sampling offsets is predicted to update the
sampling locations for the next step. The progressive sampling is
differentiable. When combined with the Vision Transformer, the obtained PS-ViT
network can adaptively learn where to look. The proposed PS-ViT is both
effective and efficient. When trained from scratch on ImageNet, PS-ViT performs
3.8% higher than the vanilla ViT in terms of top-1 accuracy with about
$4\times$ fewer parameters and $10\times$ fewer FLOPs. Code is available at
https://github.com/yuexy/PS-ViT.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yue_X/0/1/0/all/0/1"&gt;Xiaoyu Yue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1"&gt;Shuyang Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kuang_Z/0/1/0/all/0/1"&gt;Zhanghui Kuang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_M/0/1/0/all/0/1"&gt;Meng Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1"&gt;Philip Torr&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Wayne Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1"&gt;Dahua Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Solo-learn: A Library of Self-supervised Methods for Visual Representation Learning. (arXiv:2108.01775v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.01775</id>
        <link href="http://arxiv.org/abs/2108.01775"/>
        <updated>2021-08-05T01:56:19.430Z</updated>
        <summary type="html"><![CDATA[This paper presents solo-learn, a library of self-supervised methods for
visual representation learning. Implemented in Python, using Pytorch and
Pytorch lightning, the library fits both research and industry needs by
featuring distributed training pipelines with mixed-precision, faster data
loading via Nvidia DALI, online linear evaluation for better prototyping, and
many additional training tricks. Our goal is to provide an easy-to-use library
comprising a large amount of Self-supervised Learning (SSL) methods, that can
be easily extended and fine-tuned by the community. solo-learn opens up avenues
for exploiting large-budget SSL solutions on inexpensive smaller
infrastructures and seeks to democratize SSL by making it accessible to all.
The source code is available at https://github.com/vturrisi/solo-learn.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Costa_V/0/1/0/all/0/1"&gt;Victor G. Turrisi da Costa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fini_E/0/1/0/all/0/1"&gt;Enrico Fini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nabi_M/0/1/0/all/0/1"&gt;Moin Nabi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sebe_N/0/1/0/all/0/1"&gt;Nicu Sebe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ricci_E/0/1/0/all/0/1"&gt;Elisa Ricci&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Armour: Generalizable Compact Self-Attention for Vision Transformers. (arXiv:2108.01778v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.01778</id>
        <link href="http://arxiv.org/abs/2108.01778"/>
        <updated>2021-08-05T01:56:19.411Z</updated>
        <summary type="html"><![CDATA[Attention-based transformer networks have demonstrated promising potential as
their applications extend from natural language processing to vision. However,
despite the recent improvements, such as sub-quadratic attention approximation
and various training enhancements, the compact vision transformers to date
using the regular attention still fall short in comparison with its convnet
counterparts, in terms of \textit{accuracy,} \textit{model size}, \textit{and}
\textit{throughput}. This paper introduces a compact self-attention mechanism
that is fundamental and highly generalizable. The proposed method reduces
redundancy and improves efficiency on top of the existing attention
optimizations. We show its drop-in applicability for both the regular attention
mechanism and some most recent variants in vision transformers. As a result, we
produced smaller and faster models with the same or better accuracies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Meng_L/0/1/0/all/0/1"&gt;Lingchuan Meng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Acyclic and Cyclic Reversing Computations in Petri Nets. (arXiv:2108.02167v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.02167</id>
        <link href="http://arxiv.org/abs/2108.02167"/>
        <updated>2021-08-05T01:56:19.404Z</updated>
        <summary type="html"><![CDATA[Reversible computations constitute an unconventional form of computing where
any sequence of performed operations can be undone by executing in reverse
order at any point during a computation. It has been attracting increasing
attention as it provides opportunities for low-power computation, being at the
same time essential or eligible in various applications. In recent work, we
have proposed a structural way of translating Reversing Petri Nets (RPNs) - a
type of Petri nets that embeds reversible computation, to bounded Coloured
Petri Nets (CPNs) - an extension of traditional Petri Nets, where tokens carry
data values. Three reversing semantics are possible in RPNs: backtracking
(reversing of the lately executed action), causal reversing (action can be
reversed only when all its effects have been undone) and out of causal
reversing (any previously performed action can be reversed). In this paper, we
extend the RPN to CPN translation with formal proofs of correctness. Moreover,
the possibility of introduction of cycles to RPNs is discussed. We analyze
which type of cycles could be allowed in RPNs to ensure consistency with the
current semantics. It emerged that the most interesting case related to cycles
in RPNs occurs in causal semantics, where various interpretations of dependency
result in different net's behaviour during reversing. Three definitions of
dependence are presented and discussed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Barylska_K/0/1/0/all/0/1"&gt;Kamila Barylska&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gogolinska_A/0/1/0/all/0/1"&gt;Anna Gogoli&amp;#x144;ska&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Biologically Plausible Parser. (arXiv:2108.02189v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.02189</id>
        <link href="http://arxiv.org/abs/2108.02189"/>
        <updated>2021-08-05T01:56:19.396Z</updated>
        <summary type="html"><![CDATA[We describe a parser of English effectuated by biologically plausible neurons
and synapses, and implemented through the Assembly Calculus, a recently
proposed computational framework for cognitive function. We demonstrate that
this device is capable of correctly parsing reasonably nontrivial sentences.
While our experiments entail rather simple sentences in English, our results
suggest that the parser can be extended beyond what we have implemented, to
several directions encompassing much of language. For example, we present a
simple Russian version of the parser, and discuss how to handle recursion,
embedding, and polysemy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mitropolsky_D/0/1/0/all/0/1"&gt;Daniel Mitropolsky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Collins_M/0/1/0/all/0/1"&gt;Michael J. Collins&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Papadimitriou_C/0/1/0/all/0/1"&gt;Christos H. Papadimitriou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Lightweight Music Texture Transfer System. (arXiv:1810.01248v3 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1810.01248</id>
        <link href="http://arxiv.org/abs/1810.01248"/>
        <updated>2021-08-05T01:56:19.261Z</updated>
        <summary type="html"><![CDATA[Deep learning researches on the transformation problems for image and text
have raised great attention. However, present methods for music feature
transfer using neural networks are far from practical application. In this
paper, we initiate a novel system for transferring the texture of music, and
release it as an open source project. Its core algorithm is composed of a
converter which represents sounds as texture spectra, a corresponding
reconstructor and a feed-forward transfer network. We evaluate this system from
multiple perspectives, and experimental results reveal that it achieves
convincing results in both sound effects and computational performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1"&gt;Xutan Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1"&gt;Zhi Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_F/0/1/0/all/0/1"&gt;Faqiang Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yidan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jianxin Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimizing Latency for Online Video CaptioningUsing Audio-Visual Transformers. (arXiv:2108.02147v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02147</id>
        <link href="http://arxiv.org/abs/2108.02147"/>
        <updated>2021-08-05T01:56:19.253Z</updated>
        <summary type="html"><![CDATA[Video captioning is an essential technology to understand scenes and describe
events in natural language. To apply it to real-time monitoring, a system needs
not only to describe events accurately but also to produce the captions as soon
as possible. Low-latency captioning is needed to realize such functionality,
but this research area for online video captioning has not been pursued yet.
This paper proposes a novel approach to optimize each caption's output timing
based on a trade-off between latency and caption quality. An audio-visual
Trans-former is trained to generate ground-truth captions using only a small
portion of all video frames, and to mimic outputs of a pre-trained Transformer
to which all the frames are given. A CNN-based timing detector is also trained
to detect a proper output timing, where the captions generated by the two
Trans-formers become sufficiently close to each other. With the jointly trained
Transformer and timing detector, a caption can be generated in the early stages
of an event-triggered video clip, as soon as an event happens or when it can be
forecasted. Experiments with the ActivityNet Captions dataset show that our
approach achieves 94% of the caption quality of the upper bound given by the
pre-trained Transformer using the entire video clips, using only 28% of frames
from the beginning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hori_C/0/1/0/all/0/1"&gt;Chiori Hori&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hori_T/0/1/0/all/0/1"&gt;Takaaki Hori&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roux_J/0/1/0/all/0/1"&gt;Jonathan Le Roux&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Random Offset Block Embedding Array (ROBE) for CriteoTB Benchmark MLPerf DLRM Model : 1000$\times$ Compression and 2.7$\times$ Faster Inference. (arXiv:2108.02191v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2108.02191</id>
        <link href="http://arxiv.org/abs/2108.02191"/>
        <updated>2021-08-05T01:56:19.246Z</updated>
        <summary type="html"><![CDATA[Deep learning for recommendation data is the one of the most pervasive and
challenging AI workload in recent times. State-of-the-art recommendation models
are one of the largest models rivalling the likes of GPT-3 and Switch
Transformer. Challenges in deep learning recommendation models (DLRM) stem from
learning dense embeddings for each of the categorical values. These embedding
tables in industrial scale models can be as large as hundreds of terabytes.
Such large models lead to a plethora of engineering challenges, not to mention
prohibitive communication overheads, and slower training and inference times.
Of these, slower inference time directly impacts user experience. Model
compression for DLRM is gaining traction and the community has recently shown
impressive compression results. In this paper, we present Random Offset Block
Embedding Array (ROBE) as a low memory alternative to embedding tables which
provide orders of magnitude reduction in memory usage while maintaining
accuracy and boosting execution speed. ROBE is a simple fundamental approach in
improving both cache performance and the variance of randomized hashing, which
could be of independent interest in itself. We demonstrate that we can
successfully train DLRM models with same accuracy while using $1000 \times$
less memory. A $1000\times$ compressed model directly results in faster
inference without any engineering. In particular, we show that we can train
DLRM model using ROBE Array of size 100MB on a single GPU to achieve AUC of
0.8025 or higher as required by official MLPerf CriteoTB benchmark DLRM model
of 100GB while achieving about $2.7\times$ (170\%) improvement in inference
throughput.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Desai_A/0/1/0/all/0/1"&gt;Aditya Desai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chou_L/0/1/0/all/0/1"&gt;Li Chou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1"&gt;Anshumali Shrivastava&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Modeling Online Behavior in Recommender Systems: The Importance of Temporal Context. (arXiv:2009.08978v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.08978</id>
        <link href="http://arxiv.org/abs/2009.08978"/>
        <updated>2021-08-05T01:56:19.213Z</updated>
        <summary type="html"><![CDATA[Recommender systems research tends to evaluate model performance offline and
on randomly sampled targets, yet the same systems are later used to predict
user behavior sequentially from a fixed point in time. Simulating online
recommender system performance is notoriously difficult and the discrepancy
between online and offline behaviors is typically not accounted for in offline
evaluations. This disparity permits weaknesses to go unnoticed until the model
is deployed in a production setting. In this paper, we first demonstrate how
omitting temporal context when evaluating recommender system performance leads
to false confidence. To overcome this, we postulate that offline evaluation
protocols can only model real-life use-cases if they account for temporal
context. Next, we propose a training procedure to further embed the temporal
context in existing models: we introduce it in a multi-objective approach to
traditionally time-unaware recommender systems and confirm its advantage via
the proposed evaluation protocol. Finally, we validate that the Pareto Fronts
obtained with the added objective dominate those produced by state-of-the-art
models that are only optimized for accuracy on three real-world publicly
available datasets. The results show that including our temporal objective can
improve recall@20 by up to 20%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Filipovic_M/0/1/0/all/0/1"&gt;Milena Filipovic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mitrevski_B/0/1/0/all/0/1"&gt;Blagoj Mitrevski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Antognini_D/0/1/0/all/0/1"&gt;Diego Antognini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Glaude_E/0/1/0/all/0/1"&gt;Emma Lejal Glaude&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Faltings_B/0/1/0/all/0/1"&gt;Boi Faltings&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Musat_C/0/1/0/all/0/1"&gt;Claudiu Musat&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How to Query Language Models?. (arXiv:2108.01928v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.01928</id>
        <link href="http://arxiv.org/abs/2108.01928"/>
        <updated>2021-08-05T01:56:19.202Z</updated>
        <summary type="html"><![CDATA[Large pre-trained language models (LMs) are capable of not only recovering
linguistic but also factual and commonsense knowledge. To access the knowledge
stored in mask-based LMs, we can use cloze-style questions and let the model
fill in the blank. The flexibility advantage over structured knowledge bases
comes with the drawback of finding the right query for a certain information
need. Inspired by human behavior to disambiguate a question, we propose to
query LMs by example. To clarify the ambivalent question "Who does Neuer play
for?", a successful strategy is to demonstrate the relation using another
subject, e.g., "Ronaldo plays for Portugal. Who does Neuer play for?". We apply
this approach of querying by example to the LAMA probe and obtain substantial
improvements of up to 37.8% for BERT-large on the T-REx data when providing
only 10 demonstrations--even outperforming a baseline that queries the model
with up to 40 paraphrases of the question. The examples are provided through
the model's context and thus require neither fine-tuning nor an additional
forward pass. This suggests that LMs contain more factual and commonsense
knowledge than previously assumed--if we query the model in the right way.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Adolphs_L/0/1/0/all/0/1"&gt;Leonard Adolphs&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dhuliawala_S/0/1/0/all/0/1"&gt;Shehzaad Dhuliawala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hofmann_T/0/1/0/all/0/1"&gt;Thomas Hofmann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How to Query Language Models?. (arXiv:2108.01928v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.01928</id>
        <link href="http://arxiv.org/abs/2108.01928"/>
        <updated>2021-08-05T01:56:19.194Z</updated>
        <summary type="html"><![CDATA[Large pre-trained language models (LMs) are capable of not only recovering
linguistic but also factual and commonsense knowledge. To access the knowledge
stored in mask-based LMs, we can use cloze-style questions and let the model
fill in the blank. The flexibility advantage over structured knowledge bases
comes with the drawback of finding the right query for a certain information
need. Inspired by human behavior to disambiguate a question, we propose to
query LMs by example. To clarify the ambivalent question "Who does Neuer play
for?", a successful strategy is to demonstrate the relation using another
subject, e.g., "Ronaldo plays for Portugal. Who does Neuer play for?". We apply
this approach of querying by example to the LAMA probe and obtain substantial
improvements of up to 37.8% for BERT-large on the T-REx data when providing
only 10 demonstrations--even outperforming a baseline that queries the model
with up to 40 paraphrases of the question. The examples are provided through
the model's context and thus require neither fine-tuning nor an additional
forward pass. This suggests that LMs contain more factual and commonsense
knowledge than previously assumed--if we query the model in the right way.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Adolphs_L/0/1/0/all/0/1"&gt;Leonard Adolphs&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dhuliawala_S/0/1/0/all/0/1"&gt;Shehzaad Dhuliawala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hofmann_T/0/1/0/all/0/1"&gt;Thomas Hofmann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Summary Explorer: Visualizing the State of the Art in Text Summarization. (arXiv:2108.01879v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.01879</id>
        <link href="http://arxiv.org/abs/2108.01879"/>
        <updated>2021-08-05T01:56:19.186Z</updated>
        <summary type="html"><![CDATA[This paper introduces Summary Explorer, a new tool to support the manual
inspection of text summarization systems by compiling the outputs of
55~state-of-the-art single document summarization approaches on three benchmark
datasets, and visually exploring them during a qualitative assessment. The
underlying design of the tool considers three well-known summary quality
criteria (coverage, faithfulness, and position bias), encapsulated in a guided
assessment based on tailored visualizations. The tool complements existing
approaches for locally debugging summarization models and improves upon them.
The tool is available at https://tldr.webis.de/]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Syed_S/0/1/0/all/0/1"&gt;Shahbaz Syed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yousef_T/0/1/0/all/0/1"&gt;Tariq Yousef&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Al_Khatib_K/0/1/0/all/0/1"&gt;Khalid Al-Khatib&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Janicke_S/0/1/0/all/0/1"&gt;Stefan J&amp;#xe4;nicke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Potthast_M/0/1/0/all/0/1"&gt;Martin Potthast&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Recommending Burgers based on Pizza Preferences: Addressing Data Sparsity with a Product of Experts. (arXiv:2104.12822v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.12822</id>
        <link href="http://arxiv.org/abs/2104.12822"/>
        <updated>2021-08-05T01:56:19.168Z</updated>
        <summary type="html"><![CDATA[In this paper, we describe a method to tackle data sparsity and create
recommendations in domains with limited knowledge about user preferences. We
expand the variational autoencoder collaborative filtering from a single-domain
to a multi-domain setting. The intuition is that user-item interactions in a
source domain can augment the recommendation quality in a target domain. The
intuition can be taken to its extreme, where, in a cross-domain setup, the user
history in a source domain is enough to generate high-quality recommendations
in a target one. We thus create a Product-of-Experts (POE) architecture for
recommendations that jointly models user-item interactions across multiple
domains. The method is resilient to missing data for one or more of the
domains, which is a situation often found in real life. We present results on
two widely-used datasets - Amazon and Yelp, which support the claim that
holistic user preference knowledge leads to better recommendations.
Surprisingly, we find that in some cases, a POE recommender that does not
access the target domain user representation can surpass a strong VAE
recommender baseline trained on the target domain.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Milenkoski_M/0/1/0/all/0/1"&gt;Martin Milenkoski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Antognini_D/0/1/0/all/0/1"&gt;Diego Antognini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Musat_C/0/1/0/all/0/1"&gt;Claudiu Musat&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predicting Music Relistening Behavior Using the ACT-R Framework. (arXiv:2108.02138v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2108.02138</id>
        <link href="http://arxiv.org/abs/2108.02138"/>
        <updated>2021-08-05T01:56:19.159Z</updated>
        <summary type="html"><![CDATA[Providing suitable recommendations is of vital importance to improve the user
satisfaction of music recommender systems. Here, users often listen to the same
track repeatedly and appreciate recommendations of the same song multiple
times. Thus, accounting for users' relistening behavior is critical for music
recommender systems. In this paper, we describe a psychology-informed approach
to model and predict music relistening behavior that is inspired by studies in
music psychology, which relate music preferences to human memory. We adopt a
well-established psychological theory of human cognition that models the
operations of human memory, i.e., Adaptive Control of Thought-Rational (ACT-R).
In contrast to prior work, which uses only the base-level component of ACT-R,
we utilize five components of ACT-R, i.e., base-level, spreading, partial
matching, valuation, and noise, to investigate the effect of five factors on
music relistening behavior: (i) recency and frequency of prior exposure to
tracks, (ii) co-occurrence of tracks, (iii) the similarity between tracks, (iv)
familiarity with tracks, and (v) randomness in behavior. On a dataset of 1.7
million listening events from Last.fm, we evaluate the performance of our
approach by sequentially predicting the next track(s) in user sessions. We find
that recency and frequency of prior exposure to tracks is an effective
predictor of relistening behavior. Besides, considering the co-occurrence of
tracks and familiarity with tracks further improves performance in terms of
R-precision. We hope that our work inspires future research on the merits of
considering cognitive aspects of memory retrieval to model and predict complex
user behavior.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Reiter_Haas_M/0/1/0/all/0/1"&gt;Markus Reiter-Haas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parada_Cabaleiro_E/0/1/0/all/0/1"&gt;Emilia Parada-Cabaleiro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schedl_M/0/1/0/all/0/1"&gt;Markus Schedl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Motamedi_E/0/1/0/all/0/1"&gt;Elham Motamedi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tkalcic_M/0/1/0/all/0/1"&gt;Marko Tkalcic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lex_E/0/1/0/all/0/1"&gt;Elisabeth Lex&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using Interaction Data to Predict Engagement with Interactive Media. (arXiv:2108.01949v1 [cs.HC])]]></title>
        <id>http://arxiv.org/abs/2108.01949</id>
        <link href="http://arxiv.org/abs/2108.01949"/>
        <updated>2021-08-05T01:56:19.146Z</updated>
        <summary type="html"><![CDATA[Media is evolving from traditional linear narratives to personalised
experiences, where control over information (or how it is presented) is given
to individual audience members. Measuring and understanding audience engagement
with this media is important in at least two ways: (1) a post-hoc understanding
of how engaged audiences are with the content will help production teams learn
from experience and improve future productions; (2), this type of media has
potential for real-time measures of engagement to be used to enhance the user
experience by adapting content on-the-fly. Engagement is typically measured by
asking samples of users to self-report, which is time consuming and expensive.
In some domains, however, interaction data have been used to infer engagement.
Fortuitously, the nature of interactive media facilitates a much richer set of
interaction data than traditional media; our research aims to understand if
these data can be used to infer audience engagement. In this paper, we report a
study using data captured from audience interactions with an interactive TV
show to model and predict engagement. We find that temporal metrics, including
overall time spent on the experience and the interval between events, are
predictive of engagement. The results demonstrate that interaction data can be
used to infer users' engagement during and after an experience, and the
proposed techniques are relevant to better understand audience preference and
responses.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Carlton_J/0/1/0/all/0/1"&gt;Jonathan Carlton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brown_A/0/1/0/all/0/1"&gt;Andy Brown&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jay_C/0/1/0/all/0/1"&gt;Caroline Jay&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Keane_J/0/1/0/all/0/1"&gt;John Keane&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adaptive Multi-modal Fusion Hashing via Hadamard Matrix. (arXiv:2009.12148v4 [cs.MM] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.12148</id>
        <link href="http://arxiv.org/abs/2009.12148"/>
        <updated>2021-08-05T01:56:19.133Z</updated>
        <summary type="html"><![CDATA[Hashing plays an important role in information retrieval, due to its low
storage and high speed of processing. Among the techniques available in the
literature, multi-modal hashing, which can encode heterogeneous multi-modal
features into compact hash codes, has received particular attention. Most of
the existing multi-modal hashing methods adopt the fixed weighting factors to
fuse multiple modalities for any query data, which cannot capture the variation
of different queries. Besides, many methods introduce hyper-parameters to
balance many regularization terms that make the optimization harder. Meanwhile,
it is time-consuming and labor-intensive to set proper parameter values. The
limitations may significantly hinder their promotion in real applications. In
this paper, we propose a simple, yet effective method that is inspired by the
Hadamard matrix. The proposed method captures the multi-modal feature
information in an adaptive manner and preserves the discriminative semantic
information in the hash codes. Our framework is flexible and involves a very
few hyper-parameters. Extensive experimental results show the method is
effective and achieves superior performance compared to state-of-the-art
algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1"&gt;Jun Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1"&gt;Donglin Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shu_Z/0/1/0/all/0/1"&gt;Zhenqiu Shu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1"&gt;Feng Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PARADISE: Exploiting Parallel Data for Multilingual Sequence-to-Sequence Pretraining. (arXiv:2108.01887v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.01887</id>
        <link href="http://arxiv.org/abs/2108.01887"/>
        <updated>2021-08-05T01:56:19.125Z</updated>
        <summary type="html"><![CDATA[Despite the success of multilingual sequence-to-sequence pretraining, most
existing approaches rely on monolingual corpora, and do not make use of the
strong cross-lingual signal contained in parallel data. In this paper, we
present PARADISE (PARAllel & Denoising Integration in SEquence-to-sequence
models), which extends the conventional denoising objective used to train these
models by (i) replacing words in the noised sequence according to a
multilingual dictionary, and (ii) predicting the reference translation
according to a parallel corpus instead of recovering the original sequence. Our
experiments on machine translation and cross-lingual natural language inference
show an average improvement of 2.0 BLEU points and 6.7 accuracy points from
integrating parallel data into pretraining, respectively, obtaining results
that are competitive with several popular models at a fraction of their
computational cost.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Reid_M/0/1/0/all/0/1"&gt;Machel Reid&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Artetxe_M/0/1/0/all/0/1"&gt;Mikel Artetxe&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An analytical study of content and contexts of keywords on physics. (arXiv:2108.01915v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2108.01915</id>
        <link href="http://arxiv.org/abs/2108.01915"/>
        <updated>2021-08-05T01:56:19.116Z</updated>
        <summary type="html"><![CDATA[This paper analysed author-assigned and title keywords into constituent words
collected from 769 articles published in the journal Low Temperature Physics
since the year 2006 to 2010. The total number of distinct keywords over the
said time span has been found as 1155, which have been analyzed into 869
numbers of single words having total frequency of occurrence of 2287. The
single words obtained from keywords have been categorized in four broad
classes, viz. eponymous word, form word, acronym and semantic word. A semantic
word bears several contexts and thus may be considered as relevant in several
other subject areas. These probable relevant subject areas have been found with
the aid of two popular online reference tools. The semantic words are further
categorized in twelve classes according to their contexts. Some parameters have
been defined on the basis of associations among the words and formation of
keywords in consequence, i.e. Word Association Density, Word Association
Coefficient and Keyword Formation Density. The values of these parameters have
been observed for different word categories. The statistics of word association
tending keyword formation would be known from this study. The allied subject
domains also become predictable from this study.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dutta_B/0/1/0/all/0/1"&gt;Bidyarthi Dutta&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Question-controlled Text-aware Image Captioning. (arXiv:2108.02059v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02059</id>
        <link href="http://arxiv.org/abs/2108.02059"/>
        <updated>2021-08-05T01:56:19.076Z</updated>
        <summary type="html"><![CDATA[For an image with multiple scene texts, different people may be interested in
different text information. Current text-aware image captioning models are not
able to generate distinctive captions according to various information needs.
To explore how to generate personalized text-aware captions, we define a new
challenging task, namely Question-controlled Text-aware Image Captioning
(Qc-TextCap). With questions as control signals, this task requires models to
understand questions, find related scene texts and describe them together with
objects fluently in human language. Based on two existing text-aware captioning
datasets, we automatically construct two datasets, ControlTextCaps and
ControlVizWiz to support the task. We propose a novel Geometry and Question
Aware Model (GQAM). GQAM first applies a Geometry-informed Visual Encoder to
fuse region-level object features and region-level scene text features with
considering spatial relationships. Then, we design a Question-guided Encoder to
select the most relevant visual features for each question. Finally, GQAM
generates a personalized text-aware caption with a Multimodal Decoder. Our
model achieves better captioning performance and question answering ability
than carefully designed baselines on both two datasets. With questions as
control signals, our model generates more informative and diverse captions than
the state-of-the-art text-aware captioning model. Our code and datasets are
publicly available at https://github.com/HAWLYQ/Qc-TextCap.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_A/0/1/0/all/0/1"&gt;Anwen Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Shizhe Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_Q/0/1/0/all/0/1"&gt;Qin Jin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[WeClick: Weakly-Supervised Video Semantic Segmentation with Click Annotations. (arXiv:2107.03088v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.03088</id>
        <link href="http://arxiv.org/abs/2107.03088"/>
        <updated>2021-08-05T01:56:19.065Z</updated>
        <summary type="html"><![CDATA[Compared with tedious per-pixel mask annotating, it is much easier to
annotate data by clicks, which costs only several seconds for an image.
However, applying clicks to learn video semantic segmentation model has not
been explored before. In this work, we propose an effective weakly-supervised
video semantic segmentation pipeline with click annotations, called WeClick,
for saving laborious annotating effort by segmenting an instance of the
semantic class with only a single click. Since detailed semantic information is
not captured by clicks, directly training with click labels leads to poor
segmentation predictions. To mitigate this problem, we design a novel memory
flow knowledge distillation strategy to exploit temporal information (named
memory flow) in abundant unlabeled video frames, by distilling the neighboring
predictions to the target frame via estimated motion. Moreover, we adopt
vanilla knowledge distillation for model compression. In this case, WeClick
learns compact video semantic segmentation models with the low-cost click
annotations during the training phase yet achieves real-time and accurate
models during the inference period. Experimental results on Cityscapes and
Camvid show that WeClick outperforms the state-of-the-art methods, increases
performance by 10.24% mIoU than baseline, and achieves real-time execution.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1"&gt;Peidong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1"&gt;Zibin He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1"&gt;Xiyu Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1"&gt;Yong Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xia_S/0/1/0/all/0/1"&gt;Shutao Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_F/0/1/0/all/0/1"&gt;Feng Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_M/0/1/0/all/0/1"&gt;Maowei Hu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Q-Pain: A Question Answering Dataset to Measure Social Bias in Pain Management. (arXiv:2108.01764v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.01764</id>
        <link href="http://arxiv.org/abs/2108.01764"/>
        <updated>2021-08-05T01:56:19.054Z</updated>
        <summary type="html"><![CDATA[Recent advances in Natural Language Processing (NLP), and specifically
automated Question Answering (QA) systems, have demonstrated both impressive
linguistic fluency and a pernicious tendency to reflect social biases. In this
study, we introduce Q-Pain, a dataset for assessing bias in medical QA in the
context of pain management, one of the most challenging forms of clinical
decision-making. Along with the dataset, we propose a new, rigorous framework,
including a sample experimental design, to measure the potential biases present
when making treatment decisions. We demonstrate its use by assessing two
reference Question-Answering systems, GPT-2 and GPT-3, and find statistically
significant differences in treatment between intersectional race-gender
subgroups, thus reaffirming the risks posed by AI in medical settings, and the
need for datasets like ours to ensure safety before medical AI applications are
deployed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Loge_C/0/1/0/all/0/1"&gt;C&amp;#xe9;cile Log&amp;#xe9;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ross_E/0/1/0/all/0/1"&gt;Emily Ross&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dadey_D/0/1/0/all/0/1"&gt;David Yaw Amoah Dadey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1"&gt;Saahil Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saporta_A/0/1/0/all/0/1"&gt;Adriel Saporta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ng_A/0/1/0/all/0/1"&gt;Andrew Y. Ng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rajpurkar_P/0/1/0/all/0/1"&gt;Pranav Rajpurkar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Distinction between ASR Errors and Speech Disfluencies with Feature Space Interpolation. (arXiv:2108.01812v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.01812</id>
        <link href="http://arxiv.org/abs/2108.01812"/>
        <updated>2021-08-05T01:56:19.038Z</updated>
        <summary type="html"><![CDATA[Fine-tuning pretrained language models (LMs) is a popular approach to
automatic speech recognition (ASR) error detection during post-processing.
While error detection systems often take advantage of statistical language
archetypes captured by LMs, at times the pretrained knowledge can hinder error
detection performance. For instance, presence of speech disfluencies might
confuse the post-processing system into tagging disfluent but accurate
transcriptions as ASR errors. Such confusion occurs because both error
detection and disfluency detection tasks attempt to identify tokens at
statistically unlikely positions. This paper proposes a scheme to improve
existing LM-based ASR error detection systems, both in terms of detection
scores and resilience to such distracting auxiliary tasks. Our approach adopts
the popular mixup method in text feature space and can be utilized with any
black-box ASR output. To demonstrate the effectiveness of our method, we
conduct post-processing experiments with both traditional and end-to-end ASR
systems (both for English and Korean languages) with 5 different speech
corpora. We find that our method improves both ASR error detection F 1 scores
and reduces the number of correctly transcribed disfluencies wrongly detected
as ASR errors. Finally, we suggest methods to utilize resulting LMs directly in
semi-supervised ASR training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1"&gt;Seongmin Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shin_D/0/1/0/all/0/1"&gt;Dongchan Shin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paik_S/0/1/0/all/0/1"&gt;Sangyoun Paik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_S/0/1/0/all/0/1"&gt;Subong Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kazakova_A/0/1/0/all/0/1"&gt;Alena Kazakova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jihwa Lee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Controlled Text Generation as Continuous Optimization with Multiple Constraints. (arXiv:2108.01850v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.01850</id>
        <link href="http://arxiv.org/abs/2108.01850"/>
        <updated>2021-08-05T01:56:19.015Z</updated>
        <summary type="html"><![CDATA[As large-scale language model pretraining pushes the state-of-the-art in text
generation, recent work has turned to controlling attributes of the text such
models generate. While modifying the pretrained models via fine-tuning remains
the popular approach, it incurs a significant computational cost and can be
infeasible due to lack of appropriate data. As an alternative, we propose
MuCoCO -- a flexible and modular algorithm for controllable inference from
pretrained models. We formulate the decoding process as an optimization problem
which allows for multiple attributes we aim to control to be easily
incorporated as differentiable constraints to the optimization. By relaxing
this discrete optimization to a continuous one, we make use of Lagrangian
multipliers and gradient-descent based techniques to generate the desired text.
We evaluate our approach on controllable machine translation and style transfer
with multiple sentence-level attributes and observe significant improvements
over baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1"&gt;Sachin Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Malmi_E/0/1/0/all/0/1"&gt;Eric Malmi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Severyn_A/0/1/0/all/0/1"&gt;Aliaksei Severyn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsvetkov_Y/0/1/0/all/0/1"&gt;Yulia Tsvetkov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ICECAP: Information Concentrated Entity-aware Image Captioning. (arXiv:2108.02050v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.02050</id>
        <link href="http://arxiv.org/abs/2108.02050"/>
        <updated>2021-08-05T01:56:19.003Z</updated>
        <summary type="html"><![CDATA[Most current image captioning systems focus on describing general image
content, and lack background knowledge to deeply understand the image, such as
exact named entities or concrete events. In this work, we focus on the
entity-aware news image captioning task which aims to generate informative
captions by leveraging the associated news articles to provide background
knowledge about the target image. However, due to the length of news articles,
previous works only employ news articles at the coarse article or sentence
level, which are not fine-grained enough to refine relevant events and choose
named entities accurately. To overcome these limitations, we propose an
Information Concentrated Entity-aware news image CAPtioning (ICECAP) model,
which progressively concentrates on relevant textual information within the
corresponding news article from the sentence level to the word level. Our model
first creates coarse concentration on relevant sentences using a cross-modality
retrieval model and then generates captions by further concentrating on
relevant words within the sentences. Extensive experiments on both BreakingNews
and GoodNews datasets demonstrate the effectiveness of our proposed method,
which outperforms other state-of-the-arts. The code of ICECAP is publicly
available at https://github.com/HAWLYQ/ICECAP.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_A/0/1/0/all/0/1"&gt;Anwen Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Shizhe Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_Q/0/1/0/all/0/1"&gt;Qin Jin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quality Evaluation of the Low-Resource Synthetically Generated Code-Mixed Hinglish Text. (arXiv:2108.01861v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.01861</id>
        <link href="http://arxiv.org/abs/2108.01861"/>
        <updated>2021-08-05T01:56:18.987Z</updated>
        <summary type="html"><![CDATA[In this shared task, we seek the participating teams to investigate the
factors influencing the quality of the code-mixed text generation systems. We
synthetically generate code-mixed Hinglish sentences using two distinct
approaches and employ human annotators to rate the generation quality. We
propose two subtasks, quality rating prediction and annotators' disagreement
prediction of the synthetic Hinglish dataset. The proposed subtasks will put
forward the reasoning and explanation of the factors influencing the quality
and human perception of the code-mixed text.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Srivastava_V/0/1/0/all/0/1"&gt;Vivek Srivastava&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1"&gt;Mayank Singh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploiting BERT For Multimodal Target SentimentClassification Through Input Space Translation. (arXiv:2108.01682v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.01682</id>
        <link href="http://arxiv.org/abs/2108.01682"/>
        <updated>2021-08-05T01:56:18.976Z</updated>
        <summary type="html"><![CDATA[Multimodal target/aspect sentiment classification combines multimodal
sentiment analysis and aspect/target sentiment classification. The goal of the
task is to combine vision and language to understand the sentiment towards a
target entity in a sentence. Twitter is an ideal setting for the task because
it is inherently multimodal, highly emotional, and affects real world events.
However, multimodal tweets are short and accompanied by complex, possibly
irrelevant images. We introduce a two-stream model that translates images in
input space using an object-aware transformer followed by a single-pass
non-autoregressive text generation approach. We then leverage the translation
to construct an auxiliary sentence that provides multimodal information to a
language model. Our approach increases the amount of text available to the
language model and distills the object-level information in complex images. We
achieve state-of-the-art performance on two multimodal Twitter datasets without
modifying the internals of the language model to accept multimodal data,
demonstrating the effectiveness of our translation. In addition, we explain a
failure mode of a popular approach for aspect sentiment analysis when applied
to tweets. Our code is available at
\textcolor{blue}{\url{https://github.com/codezakh/exploiting-BERT-thru-translation}}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khan_Z/0/1/0/all/0/1"&gt;Zaid Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1"&gt;Yun Fu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Counterfactual Generation for Fair Hate Speech Detection. (arXiv:2108.01721v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.01721</id>
        <link href="http://arxiv.org/abs/2108.01721"/>
        <updated>2021-08-05T01:56:18.964Z</updated>
        <summary type="html"><![CDATA[Bias mitigation approaches reduce models' dependence on sensitive features of
data, such as social group tokens (SGTs), resulting in equal predictions across
the sensitive features. In hate speech detection, however, equalizing model
predictions may ignore important differences among targeted social groups, as
hate speech can contain stereotypical language specific to each SGT. Here, to
take the specific language about each SGT into account, we rely on
counterfactual fairness and equalize predictions among counterfactuals,
generated by changing the SGTs. Our method evaluates the similarity in sentence
likelihoods (via pre-trained language models) among counterfactuals, to treat
SGTs equally only within interchangeable contexts. By applying logit pairing to
equalize outcomes on the restricted set of counterfactuals for each instance,
we improve fairness metrics while preserving model performance on hate speech
detection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Davani_A/0/1/0/all/0/1"&gt;Aida Mostafazadeh Davani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Omrani_A/0/1/0/all/0/1"&gt;Ali Omrani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kennedy_B/0/1/0/all/0/1"&gt;Brendan Kennedy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Atari_M/0/1/0/all/0/1"&gt;Mohammad Atari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1"&gt;Xiang Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dehghani_M/0/1/0/all/0/1"&gt;Morteza Dehghani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TextCNN with Attention for Text Classification. (arXiv:2108.01921v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.01921</id>
        <link href="http://arxiv.org/abs/2108.01921"/>
        <updated>2021-08-05T01:56:18.943Z</updated>
        <summary type="html"><![CDATA[The vast majority of textual content is unstructured, making automated
classification an important task for many applications. The goal of text
classification is to automatically classify text documents into one or more
predefined categories. Recently proposed simple architectures for text
classification such as Convolutional Neural Networks for Sentence
Classification by Kim, Yoon showed promising results. In this paper, we propose
incorporating an attention mechanism into the network to boost its performance,
we also propose WordRank for vocabulary selection to reduce the network
embedding parameters and speed up training with minimum accuracy loss. By
adopting the proposed ideas TextCNN accuracy on 20News increased from 94.79 to
96.88, moreover, the number of parameters for the embedding layer can be
reduced substantially with little accuracy loss by using WordRank. By using
WordRank for vocabulary selection we can reduce the number of parameters by
more than 5x from 7.9M to 1.5M, and the accuracy will only decrease by 1.2%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alshubaily_I/0/1/0/all/0/1"&gt;Ibrahim Alshubaily&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dyn-ASR: Compact, Multilingual Speech Recognition via Spoken Language and Accent Identification. (arXiv:2108.02034v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.02034</id>
        <link href="http://arxiv.org/abs/2108.02034"/>
        <updated>2021-08-05T01:56:18.924Z</updated>
        <summary type="html"><![CDATA[Running automatic speech recognition (ASR) on edge devices is non-trivial due
to resource constraints, especially in scenarios that require supporting
multiple languages. We propose a new approach to enable multilingual speech
recognition on edge devices. This approach uses both language identification
and accent identification to select one of multiple monolingual ASR models
on-the-fly, each fine-tuned for a particular accent. Initial results for both
recognition performance and resource usage are promising with our approach
using less than 1/12th of the memory consumed by other solutions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ghangam_S/0/1/0/all/0/1"&gt;Sangeeta Ghangam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Whitenack_D/0/1/0/all/0/1"&gt;Daniel Whitenack&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nemecek_J/0/1/0/all/0/1"&gt;Joshua Nemecek&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What's Wrong with the Bottom-up Methods in Arbitrary-shape Scene Text Detection. (arXiv:2108.01809v1 [cs.MM])]]></title>
        <id>http://arxiv.org/abs/2108.01809</id>
        <link href="http://arxiv.org/abs/2108.01809"/>
        <updated>2021-08-05T01:56:18.908Z</updated>
        <summary type="html"><![CDATA[The latest trend in the bottom-up perspective for arbitrary-shape scene text
detection is to reason the links between text segments using Graph
Convolutional Network (GCN). Notwithstanding, the performance of the best
performing bottom-up method is still inferior to that of the best performing
top-down method even with the help of GCN. We argue that this is not mainly
caused by the limited feature capturing ability of the text proposal backbone
or GCN, but by their failure to make a full use of visual-relational features
for suppressing false detection, as well as the sub-optimal route-finding
mechanism used for grouping text segments. In this paper, we revitalize the
classic text detection frameworks by aggregating the visual-relational features
of text with two effective false positive/negative suppression mechanisms.
First, dense overlapping text segments depicting the `characterness' and
`streamline' of text are generated for further relational reasoning and weakly
supervised segment classification. Here, relational graph features are used for
suppressing false positives/negatives. Then, to fuse the relational features
with visual features, a Location-Aware Transfer (LAT) module is designed to
transfer text's relational features into visual compatible features with a Fuse
Decoding (FD) module to enhance the representation of text regions for the
second step suppression. Finally, a novel multiple-text-map-aware
contour-approximation strategy is developed, instead of the widely-used
route-finding process. Experiments conducted on five benchmark datasets, i.e.,
CTW1500, Total-Text, ICDAR2015, MSRA-TD500, and MLT2017 demonstrate that our
method outperforms the state-of-the-art performance when being embedded in a
classic text detection framework, which revitalises the superb strength of the
bottom-up methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Chengpei Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jia_W/0/1/0/all/0/1"&gt;Wenjing Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_T/0/1/0/all/0/1"&gt;Tingcheng Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1"&gt;Ruomei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yuan-fang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1"&gt;Xiangjian He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Low-complexity Scaling Methods for DCT-II Approximations. (arXiv:2108.02119v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2108.02119</id>
        <link href="http://arxiv.org/abs/2108.02119"/>
        <updated>2021-08-05T01:56:18.869Z</updated>
        <summary type="html"><![CDATA[This paper introduces a collection of scaling methods for generating
$2N$-point DCT-II approximations based on $N$-point low-complexity
transformations. Such scaling is based on the Hou recursive matrix
factorization of the exact $2N$-point DCT-II matrix. Encompassing the widely
employed Jridi-Alfalou-Meher scaling method, the proposed techniques are shown
to produce DCT-II approximations that outperform the transforms resulting from
the JAM scaling method according to total error energy and mean squared error.
Orthogonality conditions are derived and an extensive error analysis based on
statistical simulation demonstrates the good performance of the introduced
scaling methods. A hardware implementation is also provided demonstrating the
competitiveness of the proposed methods when compared to the JAM scaling
method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Coelho_D/0/1/0/all/0/1"&gt;D. F. G. Coelho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cintra_R/0/1/0/all/0/1"&gt;R. J. Cintra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Madanayake_A/0/1/0/all/0/1"&gt;A. Madanayake&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Perera_S/0/1/0/all/0/1"&gt;S. Perera&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Binary Neural Network Operation from 233 K to 398 K via Gate Stack and Bias Optimization of Ferroelectric FinFET Synapses. (arXiv:2103.03111v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.03111</id>
        <link href="http://arxiv.org/abs/2103.03111"/>
        <updated>2021-08-04T01:59:24.276Z</updated>
        <summary type="html"><![CDATA[A synergistic approach for optimizing devices, circuits, and neural network
architectures was used to abate junction-temperature-change-induced performance
degradation of a Fe-FinFET-based artificial neural network. We demonstrated
that the digital nature of the binarized neural network, with the "0" state
programmed deep in the subthreshold and the "1" state in strong inversion, is
crucial for robust DNN inference. The performance of a purely software-based
binary neural network (BNN), with 96.1% accuracy for Modified National
Institute of Standards and Technology (MNIST) handwritten digit recognition,
was used as a baseline. The Fe-FinFET-based BNN (including device-to-device
variation at 300 K) achieved 95.7% inference accuracy on the MNIST dataset.
Although substantial inference accuracy degradation with temperature change was
observed in a nonbinary neural network, the BNN with optimized Fe-FinFETs as
synaptic devices had excellent resistance to temperature change effects and
maintained a minimum inference accuracy of 95.2% within a temperature range of
-233K to 398K after gate stack and bias optimization. However, reprogramming to
adjust device conductance was necessary for temperatures higher than 398K.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+De_S/0/1/0/all/0/1"&gt;Sourav De&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Le_H/0/1/0/all/0/1"&gt;Hoang-Hiep Le&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_B/0/1/0/all/0/1"&gt;Bo-Han Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baig_M/0/1/0/all/0/1"&gt;Md. Aftab Baig&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sung_P/0/1/0/all/0/1"&gt;Po-Jung Sung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_C/0/1/0/all/0/1"&gt;Chung Jun Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1"&gt;Yao-Jen Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_D/0/1/0/all/0/1"&gt;Darsen D. Lu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neograd: Near-Ideal Gradient Descent. (arXiv:2010.07873v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.07873</id>
        <link href="http://arxiv.org/abs/2010.07873"/>
        <updated>2021-08-04T01:59:24.269Z</updated>
        <summary type="html"><![CDATA[The purpose of this paper is to improve upon existing variants of gradient
descent by solving two problems: (1) removing (or reducing) the plateau that
occurs while minimizing the cost function, (2) continually adjusting the
learning rate to an "ideal" value. The approach taken is to approximately solve
for the learning rate as a function of a trust metric. When this technique is
hybridized with momentum, it creates an especially effective gradient descent
variant, called NeogradM. It is shown to outperform Adam on several test
problems, and can easily reach cost function values that are smaller by a
factor of $10^8$, for example.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zimmer_M/0/1/0/all/0/1"&gt;Michael F. Zimmer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Curvature-based Feature Selection with Application in Classifying Electronic Health Records. (arXiv:2101.03581v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.03581</id>
        <link href="http://arxiv.org/abs/2101.03581"/>
        <updated>2021-08-04T01:59:24.262Z</updated>
        <summary type="html"><![CDATA[Disruptive technologies provides unparalleled opportunities to contribute to
the identifications of many aspects in pervasive healthcare, from the adoption
of the Internet of Things through to Machine Learning (ML) techniques. As a
powerful tool, ML has been widely applied in patient-centric healthcare
solutions. To further improve the quality of patient care, Electronic Health
Records (EHRs) are widely applied in healthcare facilities nowadays. Due to the
inherent heterogeneity, unbalanced, incompleteness, and high-dimensional nature
of EHRs, it is a challenging task to employ machine learning algorithms to
analyse such EHRs for prediction and diagnostics within the scope of precision
medicine. Dimensionality reduction is an efficient data preprocessing technique
for the analysis of high dimensional data that reduces the number of features
while improving the performance of the data analysis, e.g. classification. In
this paper, we propose an efficient curvature-based feature selection method
for supporting more precise diagnosis. The proposed method is a filter-based
feature selection method, which directly utilises the Menger Curvature for
ranking all the attributes in the given data set. We evaluate the performance
of our method against conventional PCA and recent ones including BPCM, GSAM,
WCNN, BLS II, VIBES, 2L-MJFA, RFGA, and VAF. Our method achieves
state-of-the-art performance on four benchmark healthcare data sets including
CCRFDS, BCCDS, BTDS, and DRDDS with impressive 24.73% and 13.93% improvements
respectively on BTDS and CCRFDS, 7.97% improvement on BCCDS, and 3.63%
improvement on DRDDS. Our CFS source code is publicly available at
https://github.com/zhemingzuo/CFS.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zuo_Z/0/1/0/all/0/1"&gt;Zheming Zuo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jie Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1"&gt;Han Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moubayed_N/0/1/0/all/0/1"&gt;Noura Al Moubayed&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Large-Scale Differentially Private BERT. (arXiv:2108.01624v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.01624</id>
        <link href="http://arxiv.org/abs/2108.01624"/>
        <updated>2021-08-04T01:59:24.255Z</updated>
        <summary type="html"><![CDATA[In this work, we study the large-scale pretraining of BERT-Large with
differentially private SGD (DP-SGD). We show that combined with a careful
implementation, scaling up the batch size to millions (i.e., mega-batches)
improves the utility of the DP-SGD step for BERT; we also enhance its
efficiency by using an increasing batch size schedule. Our implementation
builds on the recent work of [SVK20], who demonstrated that the overhead of a
DP-SGD step is minimized with effective use of JAX [BFH+18, FJL18] primitives
in conjunction with the XLA compiler [XLA17]. Our implementation achieves a
masked language model accuracy of 60.5% at a batch size of 2M, for $\epsilon =
5.36$. To put this number in perspective, non-private BERT models achieve an
accuracy of $\sim$70%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Anil_R/0/1/0/all/0/1"&gt;Rohan Anil&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghazi_B/0/1/0/all/0/1"&gt;Badih Ghazi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_V/0/1/0/all/0/1"&gt;Vineet Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_R/0/1/0/all/0/1"&gt;Ravi Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Manurangsi_P/0/1/0/all/0/1"&gt;Pasin Manurangsi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Model Complexity of Deep Learning: A Survey. (arXiv:2103.05127v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.05127</id>
        <link href="http://arxiv.org/abs/2103.05127"/>
        <updated>2021-08-04T01:59:24.249Z</updated>
        <summary type="html"><![CDATA[Model complexity is a fundamental problem in deep learning. In this paper we
conduct a systematic overview of the latest studies on model complexity in deep
learning. Model complexity of deep learning can be categorized into expressive
capacity and effective model complexity. We review the existing studies on
those two categories along four important factors, including model framework,
model size, optimization process and data complexity. We also discuss the
applications of deep learning model complexity including understanding model
generalization, model optimization, and model selection and design. We conclude
by proposing several interesting future directions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1"&gt;Xia Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chu_L/0/1/0/all/0/1"&gt;Lingyang Chu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pei_J/0/1/0/all/0/1"&gt;Jian Pei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Weiqing Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bian_J/0/1/0/all/0/1"&gt;Jiang Bian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Geometry of Linear Convolutional Networks. (arXiv:2108.01538v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.01538</id>
        <link href="http://arxiv.org/abs/2108.01538"/>
        <updated>2021-08-04T01:59:24.231Z</updated>
        <summary type="html"><![CDATA[We study the family of functions that are represented by a linear
convolutional neural network (LCN). These functions form a semi-algebraic
subset of the set of linear maps from input space to output space. In contrast,
the families of functions represented by fully-connected linear networks form
algebraic sets. We observe that the functions represented by LCNs can be
identified with polynomials that admit certain factorizations, and we use this
perspective to describe the impact of the network's architecture on the
geometry of the resulting function space. We further study the optimization of
an objective function over an LCN, analyzing critical points in function space
and in parameter space, and describing dynamical invariants for gradient
descent. Overall, our theory predicts that the optimized parameters of an LCN
will often correspond to repeated filters across layers, or filters that can be
decomposed as repeated filters. We also conduct numerical and symbolic
experiments that illustrate our results and present an in-depth analysis of the
landscape for small architectures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kohn_K/0/1/0/all/0/1"&gt;Kathl&amp;#xe9;n Kohn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Merkh_T/0/1/0/all/0/1"&gt;Thomas Merkh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Montufar_G/0/1/0/all/0/1"&gt;Guido Mont&amp;#xfa;far&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Trager_M/0/1/0/all/0/1"&gt;Matthew Trager&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Double-Dot Network for Antipodal Grasp Detection. (arXiv:2108.01527v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2108.01527</id>
        <link href="http://arxiv.org/abs/2108.01527"/>
        <updated>2021-08-04T01:59:24.220Z</updated>
        <summary type="html"><![CDATA[This paper proposes a new deep learning approach to antipodal grasp
detection, named Double-Dot Network (DD-Net). It follows the recent anchor-free
object detection framework, which does not depend on empirically pre-set
anchors and thus allows more generalized and flexible prediction on unseen
objects. Specifically, unlike the widely used 5-dimensional rectangle, the
gripper configuration is defined as a pair of fingertips. An effective CNN
architecture is introduced to localize such fingertips, and with the help of
auxiliary centers for refinement, it accurately and robustly infers grasp
candidates. Additionally, we design a specialized loss function to measure the
quality of grasps, and in contrast to the IoU scores of bounding boxes adopted
in object detection, it is more consistent to the grasp detection task. Both
the simulation and robotic experiments are executed and state of the art
accuracies are achieved, showing that DD-Net is superior to the counterparts in
handling unseen objects.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1"&gt;Yangtao Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_B/0/1/0/all/0/1"&gt;Boyang Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1"&gt;Di Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bottleneck Transformers for Visual Recognition. (arXiv:2101.11605v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.11605</id>
        <link href="http://arxiv.org/abs/2101.11605"/>
        <updated>2021-08-04T01:59:24.212Z</updated>
        <summary type="html"><![CDATA[We present BoTNet, a conceptually simple yet powerful backbone architecture
that incorporates self-attention for multiple computer vision tasks including
image classification, object detection and instance segmentation. By just
replacing the spatial convolutions with global self-attention in the final
three bottleneck blocks of a ResNet and no other changes, our approach improves
upon the baselines significantly on instance segmentation and object detection
while also reducing the parameters, with minimal overhead in latency. Through
the design of BoTNet, we also point out how ResNet bottleneck blocks with
self-attention can be viewed as Transformer blocks. Without any bells and
whistles, BoTNet achieves 44.4% Mask AP and 49.7% Box AP on the COCO Instance
Segmentation benchmark using the Mask R-CNN framework; surpassing the previous
best published single model and single scale results of ResNeSt evaluated on
the COCO validation set. Finally, we present a simple adaptation of the BoTNet
design for image classification, resulting in models that achieve a strong
performance of 84.7% top-1 accuracy on the ImageNet benchmark while being up to
1.64x faster in compute time than the popular EfficientNet models on TPU-v3
hardware. We hope our simple and effective approach will serve as a strong
baseline for future research in self-attention models for vision]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Srinivas_A/0/1/0/all/0/1"&gt;Aravind Srinivas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1"&gt;Tsung-Yi Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parmar_N/0/1/0/all/0/1"&gt;Niki Parmar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shlens_J/0/1/0/all/0/1"&gt;Jonathon Shlens&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1"&gt;Pieter Abbeel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vaswani_A/0/1/0/all/0/1"&gt;Ashish Vaswani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Infinite-dimensional Folded-in-time Deep Neural Networks. (arXiv:2101.02966v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.02966</id>
        <link href="http://arxiv.org/abs/2101.02966"/>
        <updated>2021-08-04T01:59:24.197Z</updated>
        <summary type="html"><![CDATA[The method recently introduced in arXiv:2011.10115 realizes a deep neural
network with just a single nonlinear element and delayed feedback. It is
applicable for the description of physically implemented neural networks. In
this work, we present an infinite-dimensional generalization, which allows for
a more rigorous mathematical analysis and a higher flexibility in choosing the
weight functions. Precisely speaking, the weights are described by Lebesgue
integrable functions instead of step functions. We also provide a functional
back-propagation algorithm, which enables gradient descent training of the
weights. In addition, with a slight modification, our concept realizes
recurrent neural networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Stelzer_F/0/1/0/all/0/1"&gt;Florian Stelzer&lt;/a&gt; (1, 2 and 3), &lt;a href="http://arxiv.org/find/cs/1/au:+Yanchuk_S/0/1/0/all/0/1"&gt;Serhiy Yanchuk&lt;/a&gt; (1) ((1) Institute of Mathematics, Technische Universit&amp;#xe4;t Berlin, Germany, (2) Department of Mathematics, Humboldt-Universit&amp;#xe4;t zu Berlin, Germany, (3) Institute of Computer Science, University of Tartu, Estonia)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Global Data Science Project for COVID-19. (arXiv:2006.05573v2 [cs.SI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.05573</id>
        <link href="http://arxiv.org/abs/2006.05573"/>
        <updated>2021-08-04T01:59:24.174Z</updated>
        <summary type="html"><![CDATA[This paper aims at providing the summary of the Global Data Science Project
(GDSC) for COVID-19. as on May 31 2020. COVID-19 has largely impacted on our
societies through both direct and indirect effects transmitted by the policy
measures to counter the spread of viruses. We quantitatively analysed the
multifaceted impacts of the COVID-19 pandemic on our societies including
people's mobility, health, and social behaviour changes. People's mobility has
changed significantly due to the implementation of travel restriction and
quarantine measurements. Indeed, the physical distance has widened at
international (cross-border), national and regional level. At international
level, due to the travel restrictions, the number of international flights has
plunged overall at around 88 percent during March. In particular, the number of
flights connecting Europe dropped drastically in mid of March after the United
States announced travel restrictions to Europe and the EU and participating
countries agreed to close borders, at 84 percent decline compared to March
10th. Similarly, we examined the impacts of quarantine measures in the major
city: Tokyo (Japan), New York City (the United States), and Barcelona (Spain).
Within all three cities, we found the significant decline in traffic volume. We
also identified the increased concern for mental health through the analysis of
posts on social networking services such as Twitter and Instagram. Notably, in
the beginning of April 2020, the number of post with #depression on Instagram
doubled, which might reflect the rise in mental health awareness among
Instagram users. Besides, we identified the changes in a wide range of people's
social behaviors, as well as economic impacts through the analysis of Instagram
data and primary survey data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Suzumura_T/0/1/0/all/0/1"&gt;Toyotaro Suzumura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Garcia_Gasulla_D/0/1/0/all/0/1"&gt;Dario Garcia-Gasulla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Napagao_S/0/1/0/all/0/1"&gt;Sergio Alvarez Napagao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_I/0/1/0/all/0/1"&gt;Irene Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maruyama_H/0/1/0/all/0/1"&gt;Hiroshi Maruyama&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kanezashi_H/0/1/0/all/0/1"&gt;Hiroki Kanezashi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Perez_Arnal_R/0/1/0/all/0/1"&gt;Raquel P&amp;#x27;erez-Arnal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Miyoshi_K/0/1/0/all/0/1"&gt;Kunihiko Miyoshi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ishii_E/0/1/0/all/0/1"&gt;Euma Ishii&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suzuki_K/0/1/0/all/0/1"&gt;Keita Suzuki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shiba_S/0/1/0/all/0/1"&gt;Sayaka Shiba&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kurokawa_M/0/1/0/all/0/1"&gt;Mariko Kurokawa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kanzawa_Y/0/1/0/all/0/1"&gt;Yuta Kanzawa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nakagawa_N/0/1/0/all/0/1"&gt;Naomi Nakagawa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hanai_M/0/1/0/all/0/1"&gt;Masatoshi Hanai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yixin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1"&gt;Tianxiao Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Back to the Future: Unsupervised Backprop-based Decoding for Counterfactual and Abductive Commonsense Reasoning. (arXiv:2010.05906v4 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.05906</id>
        <link href="http://arxiv.org/abs/2010.05906"/>
        <updated>2021-08-04T01:59:24.167Z</updated>
        <summary type="html"><![CDATA[Abductive and counterfactual reasoning, core abilities of everyday human
cognition, require reasoning about what might have happened at time t, while
conditioning on multiple contexts from the relative past and future. However,
simultaneous incorporation of past and future contexts using generative
language models (LMs) can be challenging, as they are trained either to
condition only on the past context or to perform narrowly scoped
text-infilling. In this paper, we propose DeLorean, a new unsupervised decoding
algorithm that can flexibly incorporate both the past and future contexts using
only off-the-shelf, left-to-right language models and no supervision. The key
intuition of our algorithm is incorporating the future through
back-propagation, during which, we only update the internal representation of
the output while fixing the model parameters. By alternating between forward
and backward propagation, DeLorean can decode the output representation that
reflects both the left and right contexts. We demonstrate that our approach is
general and applicable to two nonmonotonic reasoning tasks: abductive text
generation and counterfactual story revision, where DeLorean outperforms a
range of unsupervised and some supervised methods, based on automatic and human
evaluation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qin_L/0/1/0/all/0/1"&gt;Lianhui Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shwartz_V/0/1/0/all/0/1"&gt;Vered Shwartz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+West_P/0/1/0/all/0/1"&gt;Peter West&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhagavatula_C/0/1/0/all/0/1"&gt;Chandra Bhagavatula&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1"&gt;Jena Hwang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bras_R/0/1/0/all/0/1"&gt;Ronan Le Bras&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bosselut_A/0/1/0/all/0/1"&gt;Antoine Bosselut&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1"&gt;Yejin Choi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Compressed Sensing MRI with Deep Generative Priors. (arXiv:2108.01368v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.01368</id>
        <link href="http://arxiv.org/abs/2108.01368"/>
        <updated>2021-08-04T01:59:24.158Z</updated>
        <summary type="html"><![CDATA[The CSGM framework (Bora-Jalal-Price-Dimakis'17) has shown that deep
generative priors can be powerful tools for solving inverse problems. However,
to date this framework has been empirically successful only on certain datasets
(for example, human faces and MNIST digits), and it is known to perform poorly
on out-of-distribution samples. In this paper, we present the first successful
application of the CSGM framework on clinical MRI data. We train a generative
prior on brain scans from the fastMRI dataset, and show that posterior sampling
via Langevin dynamics achieves high quality reconstructions. Furthermore, our
experiments and theory show that posterior sampling is robust to changes in the
ground-truth distribution and measurement process. Our code and models are
available at: \url{https://github.com/utcsilab/csgm-mri-langevin}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jalal_A/0/1/0/all/0/1"&gt;Ajil Jalal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arvinte_M/0/1/0/all/0/1"&gt;Marius Arvinte&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Daras_G/0/1/0/all/0/1"&gt;Giannis Daras&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Price_E/0/1/0/all/0/1"&gt;Eric Price&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dimakis_A/0/1/0/all/0/1"&gt;Alexandros G. Dimakis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tamir_J/0/1/0/all/0/1"&gt;Jonathan I. Tamir&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Nonlinear MPC for Offset-Free Tracking of systems learned by GRU Neural Networks. (arXiv:2103.02383v3 [eess.SY] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.02383</id>
        <link href="http://arxiv.org/abs/2103.02383"/>
        <updated>2021-08-04T01:59:24.032Z</updated>
        <summary type="html"><![CDATA[The use of Recurrent Neural Networks (RNNs) for system identification has
recently gathered increasing attention, thanks to their black-box modeling
capabilities.Albeit RNNs have been fruitfully adopted in many applications,
only few works are devoted to provide rigorous theoretical foundations that
justify their use for control purposes. The aim of this paper is to describe
how stable Gated Recurrent Units (GRUs), a particular RNN architecture, can be
trained and employed in a Nonlinear MPC framework to perform offset-free
tracking of constant references with guaranteed closed-loop stability. The
proposed approach is tested on a pH neutralization process benchmark, showing
remarkable performances.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Bonassi_F/0/1/0/all/0/1"&gt;Fabio Bonassi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Silva_C/0/1/0/all/0/1"&gt;Caio Fabio Oliveira da Silva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Scattolini_R/0/1/0/all/0/1"&gt;Riccardo Scattolini&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spectral Graph Convolutional Networks WithLifting-based Adaptive Graph Wavelets. (arXiv:2108.01660v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.01660</id>
        <link href="http://arxiv.org/abs/2108.01660"/>
        <updated>2021-08-04T01:59:24.014Z</updated>
        <summary type="html"><![CDATA[Spectral graph convolutional networks (SGCNs) have been attracting increasing
attention in graph representation learning partly due to their interpretability
through the prism of the established graph signal processing framework.
However, existing SGCNs are limited in implementing graph convolutions with
rigid transforms that could not adapt to signals residing on graphs and tasks
at hand. In this paper, we propose a novel class of spectral graph
convolutional networks that implement graph convolutions with adaptive graph
wavelets. Specifically, the adaptive graph wavelets are learned with neural
network-parameterized lifting structures, where structure-aware attention-based
lifting operations are developed to jointly consider graph structures and node
features. We propose to lift based on diffusion wavelets to alleviate the
structural information loss induced by partitioning non-bipartite graphs. By
design, the locality and sparsity of the resulting wavelet transform as well as
the scalability of the lifting structure for large and varying-size graphs are
guaranteed. We further derive a soft-thresholding filtering operation by
learning sparse graph representations in terms of the learned wavelets, which
improves the scalability and interpretablity, and yield a localized, efficient
and scalable spectral graph convolution. To ensure that the learned graph
representations are invariant to node permutations, a layer is employed at the
input of the networks to reorder the nodes according to their local topology
information. We evaluate the proposed networks in both node-level and
graph-level representation learning tasks on benchmark citation and
bioinformatics graph datasets. Extensive experiments demonstrate the
superiority of the proposed networks over existing SGCNs in terms of accuracy,
efficiency and scalability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1"&gt;Mingxing Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1"&gt;Wenrui Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chenglin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1"&gt;Junni Zou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1"&gt;Hongkai Xiong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frossard_P/0/1/0/all/0/1"&gt;Pascal Frossard&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Generative Utility of Cyclic Conditionals. (arXiv:2106.15962v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.15962</id>
        <link href="http://arxiv.org/abs/2106.15962"/>
        <updated>2021-08-04T01:59:23.979Z</updated>
        <summary type="html"><![CDATA[We study whether and how can we model a joint distribution $p(x,z)$ using two
conditional models $p(x|z)$ and $q(z|x)$ that form a cycle. This is motivated
by the observation that deep generative models, in addition to a likelihood
model $p(x|z)$, often also use an inference model $q(z|x)$ for data
representation, but they rely on a usually uninformative prior distribution
$p(z)$ to define a joint distribution, which may render problems like posterior
collapse and manifold mismatch. To explore the possibility to model a joint
distribution using only $p(x|z)$ and $q(z|x)$, we study their compatibility and
determinacy, corresponding to the existence and uniqueness of a joint
distribution whose conditional distributions coincide with them. We develop a
general theory for novel and operable equivalence criteria for compatibility,
and sufficient conditions for determinacy. Based on the theory, we propose the
CyGen framework for cyclic-conditional generative modeling, including methods
to enforce compatibility and use the determined distribution to fit and
generate data. With the prior constraint removed, CyGen better fits data and
captures more representative features, supported by experiments showing better
generation and downstream classification performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1"&gt;Chang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1"&gt;Haoyue Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1"&gt;Tao Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jintao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1"&gt;Tie-Yan Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DistDGL: Distributed Graph Neural Network Training for Billion-Scale Graphs. (arXiv:2010.05337v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.05337</id>
        <link href="http://arxiv.org/abs/2010.05337"/>
        <updated>2021-08-04T01:59:23.965Z</updated>
        <summary type="html"><![CDATA[Graph neural networks (GNN) have shown great success in learning from
graph-structured data. They are widely used in various applications, such as
recommendation, fraud detection, and search. In these domains, the graphs are
typically large, containing hundreds of millions of nodes and several billions
of edges. To tackle this challenge, we develop DistDGL, a system for training
GNNs in a mini-batch fashion on a cluster of machines. DistDGL is based on the
Deep Graph Library (DGL), a popular GNN development framework. DistDGL
distributes the graph and its associated data (initial features and embeddings)
across the machines and uses this distribution to derive a computational
decomposition by following an owner-compute rule. DistDGL follows a synchronous
training approach and allows ego-networks forming the mini-batches to include
non-local nodes. To minimize the overheads associated with distributed
computations, DistDGL uses a high-quality and light-weight min-cut graph
partitioning algorithm along with multiple balancing constraints. This allows
it to reduce communication overheads and statically balance the computations.
It further reduces the communication by replicating halo nodes and by using
sparse embedding updates. The combination of these design choices allows
DistDGL to train high-quality models while achieving high parallel efficiency
and memory scalability. We demonstrate our optimizations on both inductive and
transductive GNN models. Our results show that DistDGL achieves linear speedup
without compromising model accuracy and requires only 13 seconds to complete a
training epoch for a graph with 100 million nodes and 3 billion edges on a
cluster with 16 machines. DistDGL is now publicly available as part of
DGL:https://github.com/dmlc/dgl/tree/master/python/dgl/distributed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_D/0/1/0/all/0/1"&gt;Da Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1"&gt;Chao Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1"&gt;Minjie Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jinjing Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_Q/0/1/0/all/0/1"&gt;Qidong Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1"&gt;Xiang Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gan_Q/0/1/0/all/0/1"&gt;Quan Gan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zheng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karypis_G/0/1/0/all/0/1"&gt;George Karypis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Devil is in the GAN: Defending Deep Generative Models Against Backdoor Attacks. (arXiv:2108.01644v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2108.01644</id>
        <link href="http://arxiv.org/abs/2108.01644"/>
        <updated>2021-08-04T01:59:23.956Z</updated>
        <summary type="html"><![CDATA[Deep Generative Models (DGMs) allow users to synthesize data from complex,
high-dimensional manifolds. Industry applications of DGMs include data
augmentation to boost performance of (semi-)supervised machine learning, or to
mitigate fairness or privacy concerns. Large-scale DGMs are notoriously hard to
train, requiring expert skills, large amounts of data and extensive
computational resources. Thus, it can be expected that many enterprises will
resort to sourcing pre-trained DGMs from potentially unverified third parties,
e.g.~open source model repositories.

As we show in this paper, such a deployment scenario poses a new attack
surface, which allows adversaries to potentially undermine the integrity of
entire machine learning development pipelines in a victim organization.
Specifically, we describe novel training-time attacks resulting in corrupted
DGMs that synthesize regular data under normal operations and designated target
outputs for inputs sampled from a trigger distribution. Depending on the
control that the adversary has over the random number generation, this imposes
various degrees of risk that harmful data may enter the machine learning
development pipelines, potentially causing material or reputational damage to
the victim organization.

Our attacks are based on adversarial loss functions that combine the dual
objectives of attack stealth and fidelity. We show its effectiveness for a
variety of DGM architectures (Generative Adversarial Networks (GANs),
Variational Autoencoders (VAEs)) and data domains (images, audio). Our
experiments show that - even for large-scale industry-grade DGMs - our attack
can be mounted with only modest computational efforts. We also investigate the
effectiveness of different defensive approaches (based on static/dynamic model
and output inspections) and prescribe a practical defense strategy that paves
the way for safe usage of DGMs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rawat_A/0/1/0/all/0/1"&gt;Ambrish Rawat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Levacher_K/0/1/0/all/0/1"&gt;Killian Levacher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sinn_M/0/1/0/all/0/1"&gt;Mathieu Sinn&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multilevel Knowledge Transfer for Cross-Domain Object Detection. (arXiv:2108.00977v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2108.00977</id>
        <link href="http://arxiv.org/abs/2108.00977"/>
        <updated>2021-08-04T01:59:23.943Z</updated>
        <summary type="html"><![CDATA[Domain shift is a well known problem where a model trained on a particular
domain (source) does not perform well when exposed to samples from a different
domain (target). Unsupervised methods that can adapt to domain shift are highly
desirable as they allow effective utilization of the source data without
requiring additional annotated training data from the target. Practically,
obtaining sufficient amount of annotated data from the target domain can be
both infeasible and extremely expensive. In this work, we address the domain
shift problem for the object detection task. Our approach relies on gradually
removing the domain shift between the source and the target domains. The key
ingredients to our approach are -- (a) mapping the source to the target domain
on pixel-level; (b) training a teacher network on the mapped source and the
unannotated target domain using adversarial feature alignment; and (c) finally
training a student network using the pseudo-labels obtained from the teacher.
Experimentally, when tested on challenging scenarios involving domain shift, we
consistently obtain significantly large performance gains over various recent
state of the art approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Csaba_B/0/1/0/all/0/1"&gt;Botos Csaba&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1"&gt;Xiaojuan Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chaudhry_A/0/1/0/all/0/1"&gt;Arslan Chaudhry&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dokania_P/0/1/0/all/0/1"&gt;Puneet Dokania&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1"&gt;Philip Torr&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Debiased Off-Policy Evaluation for Recommendation Systems. (arXiv:2002.08536v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.08536</id>
        <link href="http://arxiv.org/abs/2002.08536"/>
        <updated>2021-08-04T01:59:23.938Z</updated>
        <summary type="html"><![CDATA[Efficient methods to evaluate new algorithms are critical for improving
interactive bandit and reinforcement learning systems such as recommendation
systems. A/B tests are reliable, but are time- and money-consuming, and entail
a risk of failure. In this paper, we develop an alternative method, which
predicts the performance of algorithms given historical data that may have been
generated by a different algorithm. Our estimator has the property that its
prediction converges in probability to the true performance of a counterfactual
algorithm at a rate of $\sqrt{N}$, as the sample size $N$ increases. We also
show a correct way to estimate the variance of our prediction, thus allowing
the analyst to quantify the uncertainty in the prediction. These properties
hold even when the analyst does not know which among a large number of
potentially important state variables are actually important. We validate our
method by a simulation experiment about reinforcement learning. We finally
apply it to improve advertisement design by a major advertisement company. We
find that our method produces smaller mean squared errors than state-of-the-art
methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Narita_Y/0/1/0/all/0/1"&gt;Yusuke Narita&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yasui_S/0/1/0/all/0/1"&gt;Shota Yasui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yata_K/0/1/0/all/0/1"&gt;Kohei Yata&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cross-Modal Analysis of Human Detection for Robotics: An Industrial Case Study. (arXiv:2108.01495v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2108.01495</id>
        <link href="http://arxiv.org/abs/2108.01495"/>
        <updated>2021-08-04T01:59:23.918Z</updated>
        <summary type="html"><![CDATA[Advances in sensing and learning algorithms have led to increasingly mature
solutions for human detection by robots, particularly in selected use-cases
such as pedestrian detection for self-driving cars or close-range person
detection in consumer settings. Despite this progress, the simple question
"which sensor-algorithm combination is best suited for a person detection task
at hand?" remains hard to answer. In this paper, we tackle this issue by
conducting a systematic cross-modal analysis of sensor-algorithm combinations
typically used in robotics. We compare the performance of state-of-the-art
person detectors for 2D range data, 3D lidar, and RGB-D data as well as
selected combinations thereof in a challenging industrial use-case.

We further address the related problems of data scarcity in the industrial
target domain, and that recent research on human detection in 3D point clouds
has mostly focused on autonomous driving scenarios. To leverage these
methodological advances for robotics applications, we utilize a simple, yet
effective multi-sensor transfer learning strategy by extending a strong
image-based RGB-D detector to provide cross-modal supervision for lidar
detectors in the form of weak 3D bounding box labels.

Our results show a large variance among the different approaches in terms of
detection performance, generalization, frame rates and computational
requirements. As our use-case contains difficulties representative for a wide
range of service robot applications, we believe that these results point to
relevant open challenges for further research and provide valuable support to
practitioners for the design of their robot system.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Linder_T/0/1/0/all/0/1"&gt;Timm Linder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vaskevicius_N/0/1/0/all/0/1"&gt;Narunas Vaskevicius&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schirmer_R/0/1/0/all/0/1"&gt;Robert Schirmer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arras_K/0/1/0/all/0/1"&gt;Kai O. Arras&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Causal Relationships from Conditional Moment Conditions by Importance Weighting. (arXiv:2108.01312v1 [econ.EM])]]></title>
        <id>http://arxiv.org/abs/2108.01312</id>
        <link href="http://arxiv.org/abs/2108.01312"/>
        <updated>2021-08-04T01:59:23.911Z</updated>
        <summary type="html"><![CDATA[We consider learning causal relationships under conditional moment
conditions. Unlike causal inference under unconditional moment conditions,
conditional moment conditions pose serious challenges for causal inference,
especially in complex, high-dimensional settings. To address this issue, we
propose a method that transforms conditional moment conditions to unconditional
moment conditions through importance weighting using the conditional density
ratio. Then, using this transformation, we propose a method that successfully
approximates conditional moment conditions. Our proposed approach allows us to
employ methods for estimating causal parameters from unconditional moment
conditions, such as generalized method of moments, adequately in a
straightforward manner. In experiments, we confirm that our proposed method
performs well compared to existing methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/econ/1/au:+Kato_M/0/1/0/all/0/1"&gt;Masahiro Kato&lt;/a&gt;, &lt;a href="http://arxiv.org/find/econ/1/au:+Kakehi_H/0/1/0/all/0/1"&gt;Haruo Kakehi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/econ/1/au:+McAlinn_K/0/1/0/all/0/1"&gt;Kenichiro McAlinn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/econ/1/au:+Yasui_S/0/1/0/all/0/1"&gt;Shota Yasui&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Disentanglement Learning for Variational Autoencoders Applied to Audio-Visual Speech Enhancement. (arXiv:2105.08970v2 [eess.AS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.08970</id>
        <link href="http://arxiv.org/abs/2105.08970"/>
        <updated>2021-08-04T01:59:23.905Z</updated>
        <summary type="html"><![CDATA[Recently, the standard variational autoencoder has been successfully used to
learn a probabilistic prior over speech signals, which is then used to perform
speech enhancement. Variational autoencoders have then been conditioned on a
label describing a high-level speech attribute (e.g. speech activity) that
allows for a more explicit control of speech generation. However, the label is
not guaranteed to be disentangled from the other latent variables, which
results in limited performance improvements compared to the standard
variational autoencoder. In this work, we propose to use an adversarial
training scheme for variational autoencoders to disentangle the label from the
other latent variables. At training, we use a discriminator that competes with
the encoder of the variational autoencoder. Simultaneously, we also use an
additional encoder that estimates the label for the decoder of the variational
autoencoder, which proves to be crucial to learn disentanglement. We show the
benefit of the proposed disentanglement learning when a voice activity label,
estimated from visual data, is used for speech enhancement.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Carbajal_G/0/1/0/all/0/1"&gt;Guillaume Carbajal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Richter_J/0/1/0/all/0/1"&gt;Julius Richter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gerkmann_T/0/1/0/all/0/1"&gt;Timo Gerkmann&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Recursive Least Squares Based Refinement Network for the Rollout Trajectory Prediction Methods. (arXiv:2102.10859v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.10859</id>
        <link href="http://arxiv.org/abs/2102.10859"/>
        <updated>2021-08-04T01:59:23.899Z</updated>
        <summary type="html"><![CDATA[Trajectory prediction plays a pivotal role in the field of intelligent
vehicles. It currently suffers from several challenges,e.g., accumulative error
in rollout process and weak adaptability in various scenarios. This paper
proposes a parametric-learning recursive least squares (RLS) estimation based
on deep neural network for trajectory prediction. We design a flexible plug-in
module which can be readily implanted into rollout approaches. Goal points are
proposed to capture the long-term prediction stability from the global
perspective. We carried experiments out on the NGSIM dataset. The promising
results indicate that our method could improve rollout trajectory prediction
methods effectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xue_Q/0/1/0/all/0/1"&gt;Qifan Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xuanpeng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Weigong Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Greedy Gradient Ensemble for Robust Visual Question Answering. (arXiv:2107.12651v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.12651</id>
        <link href="http://arxiv.org/abs/2107.12651"/>
        <updated>2021-08-04T01:59:23.893Z</updated>
        <summary type="html"><![CDATA[Language bias is a critical issue in Visual Question Answering (VQA), where
models often exploit dataset biases for the final decision without considering
the image information. As a result, they suffer from performance drop on
out-of-distribution data and inadequate visual explanation. Based on
experimental analysis for existing robust VQA methods, we stress the language
bias in VQA that comes from two aspects, i.e., distribution bias and shortcut
bias. We further propose a new de-bias framework, Greedy Gradient Ensemble
(GGE), which combines multiple biased models for unbiased base model learning.
With the greedy strategy, GGE forces the biased models to over-fit the biased
data distribution in priority, thus makes the base model pay more attention to
examples that are hard to solve by biased models. The experiments demonstrate
that our method makes better use of visual information and achieves
state-of-the-art performance on diagnosing dataset VQA-CP without using extra
annotations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1"&gt;Xinzhe Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shuhui Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_C/0/1/0/all/0/1"&gt;Chi Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1"&gt;Qingming Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1"&gt;Qi Tian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Augmented World Models Facilitate Zero-Shot Dynamics Generalization From a Single Offline Environment. (arXiv:2104.05632v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.05632</id>
        <link href="http://arxiv.org/abs/2104.05632"/>
        <updated>2021-08-04T01:59:23.873Z</updated>
        <summary type="html"><![CDATA[Reinforcement learning from large-scale offline datasets provides us with the
ability to learn policies without potentially unsafe or impractical
exploration. Significant progress has been made in the past few years in
dealing with the challenge of correcting for differing behavior between the
data collection and learned policies. However, little attention has been paid
to potentially changing dynamics when transferring a policy to the online
setting, where performance can be up to 90% reduced for existing methods. In
this paper we address this problem with Augmented World Models (AugWM). We
augment a learned dynamics model with simple transformations that seek to
capture potential changes in physical properties of the robot, leading to more
robust policies. We not only train our policy in this new setting, but also
provide it with the sampled augmentation as a context, allowing it to adapt to
changes in the environment. At test time we learn the context in a
self-supervised fashion by approximating the augmentation which corresponds to
the new environment. We rigorously evaluate our approach on over 100 different
changed dynamics settings, and show that this simple approach can significantly
improve the zero-shot generalization of a recent state-of-the-art baseline,
often achieving successful policies where the baseline fails.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ball_P/0/1/0/all/0/1"&gt;Philip J. Ball&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1"&gt;Cong Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parker_Holder_J/0/1/0/all/0/1"&gt;Jack Parker-Holder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roberts_S/0/1/0/all/0/1"&gt;Stephen Roberts&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Uniform Sampling over Episode Difficulty. (arXiv:2108.01662v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.01662</id>
        <link href="http://arxiv.org/abs/2108.01662"/>
        <updated>2021-08-04T01:59:23.852Z</updated>
        <summary type="html"><![CDATA[Episodic training is a core ingredient of few-shot learning to train models
on tasks with limited labelled data. Despite its success, episodic training
remains largely understudied, prompting us to ask the question: what is the
best way to sample episodes? In this paper, we first propose a method to
approximate episode sampling distributions based on their difficulty. Building
on this method, we perform an extensive analysis and find that sampling
uniformly over episode difficulty outperforms other sampling schemes, including
curriculum and easy-/hard-mining. As the proposed sampling method is algorithm
agnostic, we can leverage these insights to improve few-shot learning
accuracies across many episodic training algorithms. We demonstrate the
efficacy of our method across popular few-shot learning datasets, algorithms,
network architectures, and protocols.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Arnold_S/0/1/0/all/0/1"&gt;S&amp;#xe9;bastien M. R. Arnold&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dhillon_G/0/1/0/all/0/1"&gt;Guneet S. Dhillon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ravichandran_A/0/1/0/all/0/1"&gt;Avinash Ravichandran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1"&gt;Stefano Soatto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Beyond Pointwise Submodularity: Non-Monotone Adaptive Submodular Maximization subject to Knapsack and $k$-System Constraints. (arXiv:2104.04853v2 [cs.DS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.04853</id>
        <link href="http://arxiv.org/abs/2104.04853"/>
        <updated>2021-08-04T01:59:23.830Z</updated>
        <summary type="html"><![CDATA[In this paper, we study the non-monotone adaptive submodular maximization
problem subject to a knapsack and a $k$-system constraints. The input of our
problem is a set of items, where each item has a particular state drawn from a
known prior distribution. However, the state of an item is initially unknown,
one must select an item in order to reveal the state of that item. There is a
utility function which is defined over items and states. Our objective is to
sequentially select a group of items to maximize the expected utility. Although
the cardinality-constrained non-monotone adaptive submodular maximization has
been well studied in the literature, whether there exists a constant
approximation solution for the knapsack-constrained or $k$-system constrained
adaptive submodular maximization problem remains an open problem. It fact, it
has only been settled given the additional assumption of pointwise
submodularity. In this paper, we remove the common assumption on pointwise
submodularity and propose the first constant approximation solutions for both
cases. Inspired by two recent studies on non-monotone adaptive submodular
maximization, we develop a sampling-based randomized algorithm that achieves a
$\frac{1}{10}$ approximation for the case of a knapsack constraint and that
achieves a $\frac{1}{2k+4}$ approximation ratio for the case of a $k$-system
constraint.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1"&gt;Shaojie Tang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[JCS: An Explainable COVID-19 Diagnosis System by Joint Classification and Segmentation. (arXiv:2004.07054v3 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.07054</id>
        <link href="http://arxiv.org/abs/2004.07054"/>
        <updated>2021-08-04T01:59:23.823Z</updated>
        <summary type="html"><![CDATA[Recently, the coronavirus disease 2019 (COVID-19) has caused a pandemic
disease in over 200 countries, influencing billions of humans. To control the
infection, identifying and separating the infected people is the most crucial
step. The main diagnostic tool is the Reverse Transcription Polymerase Chain
Reaction (RT-PCR) test. Still, the sensitivity of the RT-PCR test is not high
enough to effectively prevent the pandemic. The chest CT scan test provides a
valuable complementary tool to the RT-PCR test, and it can identify the
patients in the early-stage with high sensitivity. However, the chest CT scan
test is usually time-consuming, requiring about 21.5 minutes per case. This
paper develops a novel Joint Classification and Segmentation (JCS) system to
perform real-time and explainable COVID-19 chest CT diagnosis. To train our JCS
system, we construct a large scale COVID-19 Classification and Segmentation
(COVID-CS) dataset, with 144,167 chest CT images of 400 COVID-19 patients and
350 uninfected cases. 3,855 chest CT images of 200 patients are annotated with
fine-grained pixel-level labels of opacifications, which are increased
attenuation of the lung parenchyma. We also have annotated lesion counts,
opacification areas, and locations and thus benefit various diagnosis aspects.
Extensive experiments demonstrate that the proposed JCS diagnosis system is
very efficient for COVID-19 classification and segmentation. It obtains an
average sensitivity of 95.0% and a specificity of 93.0% on the classification
test set, and 78.5% Dice score on the segmentation test set of our COVID-CS
dataset. The COVID-CS dataset and code are available at
https://github.com/yuhuan-wu/JCS.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yu-Huan Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gao_S/0/1/0/all/0/1"&gt;Shang-Hua Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mei_J/0/1/0/all/0/1"&gt;Jie Mei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xu_J/0/1/0/all/0/1"&gt;Jun Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Fan_D/0/1/0/all/0/1"&gt;Deng-Ping Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Rong-Guo Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cheng_M/0/1/0/all/0/1"&gt;Ming-Ming Cheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Communication-Efficient Accurate Statistical Estimation. (arXiv:1906.04870v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1906.04870</id>
        <link href="http://arxiv.org/abs/1906.04870"/>
        <updated>2021-08-04T01:59:23.817Z</updated>
        <summary type="html"><![CDATA[When the data are stored in a distributed manner, direct application of
traditional statistical inference procedures is often prohibitive due to
communication cost and privacy concerns. This paper develops and investigates
two Communication-Efficient Accurate Statistical Estimators (CEASE),
implemented through iterative algorithms for distributed optimization. In each
iteration, node machines carry out computation in parallel and communicate with
the central processor, which then broadcasts aggregated information to node
machines for new updates. The algorithms adapt to the similarity among loss
functions on node machines, and converge rapidly when each node machine has
large enough sample size. Moreover, they do not require good initialization and
enjoy linear converge guarantees under general conditions. The contraction rate
of optimization errors is presented explicitly, with dependence on the local
sample size unveiled. In addition, the improved statistical accuracy per
iteration is derived. By regarding the proposed method as a multi-step
statistical estimator, we show that statistical efficiency can be achieved in
finite steps in typical statistical applications. In addition, we give the
conditions under which the one-step CEASE estimator is statistically efficient.
Extensive numerical experiments on both synthetic and real data validate the
theoretical results and demonstrate the superior performance of our algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Fan_J/0/1/0/all/0/1"&gt;Jianqing Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Guo_Y/0/1/0/all/0/1"&gt;Yongyi Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wang_K/0/1/0/all/0/1"&gt;Kaizheng Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Variational Bayes on Manifolds. (arXiv:1908.03097v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1908.03097</id>
        <link href="http://arxiv.org/abs/1908.03097"/>
        <updated>2021-08-04T01:59:23.797Z</updated>
        <summary type="html"><![CDATA[Variational Bayes (VB) has become a widely-used tool for Bayesian inference
in statistics and machine learning. Nonetheless, the development of the
existing VB algorithms is so far generally restricted to the case where the
variational parameter space is Euclidean, which hinders the potential broad
application of VB methods. This paper extends the scope of VB to the case where
the variational parameter space is a Riemannian manifold. We develop an
efficient manifold-based VB algorithm that exploits both the geometric
structure of the constraint parameter space and the information geometry of the
manifold of VB approximating probability distributions. Our algorithm is
provably convergent and achieves a convergence rate of order $\mathcal
O(1/\sqrt{T})$ and $\mathcal O(1/T^{2-2\epsilon})$ for a non-convex evidence
lower bound function and a strongly retraction-convex evidence lower bound
function, respectively. We develop in particular two manifold VB algorithms,
Manifold Gaussian VB and Manifold Neural Net VB, and demonstrate through
numerical experiments that the proposed algorithms are stable, less sensitive
to initialization and compares favourably to existing VB methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tran_M/0/1/0/all/0/1"&gt;Minh-Ngoc Tran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1"&gt;Dang H. Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1"&gt;Duy Nguyen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Domain Generalization via Gradient Surgery. (arXiv:2108.01621v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.01621</id>
        <link href="http://arxiv.org/abs/2108.01621"/>
        <updated>2021-08-04T01:59:23.787Z</updated>
        <summary type="html"><![CDATA[In real-life applications, machine learning models often face scenarios where
there is a change in data distribution between training and test domains. When
the aim is to make predictions on distributions different from those seen at
training, we incur in a domain generalization problem. Methods to address this
issue learn a model using data from multiple source domains, and then apply
this model to the unseen target domain. Our hypothesis is that when training
with multiple domains, conflicting gradients within each mini-batch contain
information specific to the individual domains which is irrelevant to the
others, including the test domain. If left untouched, such disagreement may
degrade generalization performance. In this work, we characterize the
conflicting gradients emerging in domain shift scenarios and devise novel
gradient agreement strategies based on gradient surgery to alleviate their
effect. We validate our approach in image classification tasks with three
multi-domain datasets, showing the value of the proposed agreement strategy in
enhancing the generalization capability of deep learning models in domain shift
scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mansilla_L/0/1/0/all/0/1"&gt;Lucas Mansilla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Echeveste_R/0/1/0/all/0/1"&gt;Rodrigo Echeveste&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Milone_D/0/1/0/all/0/1"&gt;Diego H. Milone&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ferrante_E/0/1/0/all/0/1"&gt;Enzo Ferrante&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Survey on Assessing the Generalization Envelope of Deep Neural Networks: Predictive Uncertainty, Out-of-distribution and Adversarial Samples. (arXiv:2008.09381v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.09381</id>
        <link href="http://arxiv.org/abs/2008.09381"/>
        <updated>2021-08-04T01:59:23.779Z</updated>
        <summary type="html"><![CDATA[Deep Neural Networks (DNNs) achieve state-of-the-art performance on numerous
applications. However, it is difficult to tell beforehand if a DNN receiving an
input will deliver the correct output since their decision criteria are usually
nontransparent. A DNN delivers the correct output if the input is within the
area enclosed by its generalization envelope. In this case, the information
contained in the input sample is processed reasonably by the network. It is of
large practical importance to assess at inference time if a DNN generalizes
correctly. Currently, the approaches to achieve this goal are investigated in
different problem set-ups rather independently from one another, leading to
three main research and literature fields: predictive uncertainty,
out-of-distribution detection and adversarial example detection. This survey
connects the three fields within the larger framework of investigating the
generalization performance of machine learning methods and in particular DNNs.
We underline the common ground, point at the most promising approaches and give
a structured overview of the methods that provide at inference time means to
establish if the current input is within the generalization envelope of a DNN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lust_J/0/1/0/all/0/1"&gt;Julia Lust&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Condurache_A/0/1/0/all/0/1"&gt;Alexandru Paul Condurache&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dopant Network Processing Units: Towards Efficient Neural-network Emulators with High-capacity Nanoelectronic Nodes. (arXiv:2007.12371v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.12371</id>
        <link href="http://arxiv.org/abs/2007.12371"/>
        <updated>2021-08-04T01:59:23.738Z</updated>
        <summary type="html"><![CDATA[The rapidly growing computational demands of deep neural networks require
novel hardware designs. Recently, tunable nanoelectronic devices were developed
based on hopping electrons through a network of dopant atoms in silicon. These
"Dopant Network Processing Units" (DNPUs) are highly energy-efficient and have
potentially very high throughput. By adapting the control voltages applied to
its terminals, a single DNPU can solve a variety of linearly non-separable
classification problems. However, using a single device has limitations due to
the implicit single-node architecture. This paper presents a promising novel
approach to neural information processing by introducing DNPUs as high-capacity
neurons and moving from a single to a multi-neuron framework. By implementing
and testing a small multi-DNPU classifier in hardware, we show that
feed-forward DNPU networks improve the performance of a single DNPU from 77% to
94% test accuracy on a binary classification task with concentric classes on a
plane. Furthermore, motivated by the integration of DNPUs with memristor
arrays, we study the potential of using DNPUs in combination with linear
layers. We show by simulation that a single-layer MNIST classifier with only 10
DNPUs achieves over 96% test accuracy. Our results pave the road towards
hardware neural-network emulators that offer atomic-scale information
processing with low latency and energy consumption.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ruiz_Euler_H/0/1/0/all/0/1"&gt;Hans-Christian Ruiz-Euler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alegre_Ibarra_U/0/1/0/all/0/1"&gt;Unai Alegre-Ibarra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ven_B/0/1/0/all/0/1"&gt;Bram van de Ven&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Broersma_H/0/1/0/all/0/1"&gt;Hajo Broersma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bobbert_P/0/1/0/all/0/1"&gt;Peter A. Bobbert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wiel_W/0/1/0/all/0/1"&gt;Wilfred G. van der Wiel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Structure-preserving Support Tensor Train Machine. (arXiv:2002.05079v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.05079</id>
        <link href="http://arxiv.org/abs/2002.05079"/>
        <updated>2021-08-04T01:59:23.706Z</updated>
        <summary type="html"><![CDATA[An increasing amount of collected data are high-dimensional multi-way arrays
(tensors), and it is crucial for efficient learning algorithms to exploit this
tensorial structure as much as possible. The ever-present curse of
dimensionality for high dimensional data and the loss of structure when
vectorizing the data motivates the use of tailored low-rank tensor
classification methods. In the presence of small amounts of training data,
kernel methods offer an attractive choice as they provide the possibility for a
nonlinear decision boundary. We develop the Tensor Train Multi-way Multi-level
Kernel (TT-MMK), which combines the simplicity of the Canonical Polyadic
decomposition, the classification power of the Dual Structure-preserving
Support Vector Machine, and the reliability of the Tensor Train (TT)
approximation. We show by experiments that the TT-MMK method is usually more
reliable computationally, less sensitive to tuning parameters, and gives higher
prediction accuracy in the SVM classification when benchmarked against other
state-of-the-art techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kour_K/0/1/0/all/0/1"&gt;Kirandeep Kour&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dolgov_S/0/1/0/all/0/1"&gt;Sergey Dolgov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stoll_M/0/1/0/all/0/1"&gt;Martin Stoll&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Benner_P/0/1/0/all/0/1"&gt;Peter Benner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sequential Weakly Labeled Multi-Activity Localization and Recognition on Wearable Sensors using Recurrent Attention Networks. (arXiv:2004.05768v3 [eess.SP] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.05768</id>
        <link href="http://arxiv.org/abs/2004.05768"/>
        <updated>2021-08-04T01:59:23.678Z</updated>
        <summary type="html"><![CDATA[With the popularity and development of the wearable devices such as
smartphones, human activity recognition (HAR) based on sensors has become as a
key research area in human computer interaction and ubiquitous computing. The
emergence of deep learning leads to a recent shift in the research of HAR,
which requires massive strictly labeled data. In comparison with video data,
activity data recorded from accelerometer or gyroscope is often more difficult
to interpret and segment. Recently, several attention mechanisms are proposed
to handle the weakly labeled human activity data, which do not require accurate
data annotation. However, these attention-based models can only handle the
weakly labeled dataset whose sample includes one target activity, as a result
it limits efficiency and practicality. In the paper, we propose a recurrent
attention networks (RAN) to handle sequential weakly labeled multi-activity
recognition and location tasks. The model can repeatedly perform steps of
attention on multiple activities of one sample and each step is corresponding
to the current focused activity. The effectiveness of the RAN model is
validated on a collected sequential weakly labeled multi-activity dataset and
the other two public datasets. The experiment results show that our RAN model
can simultaneously infer multi-activity types from the coarse-grained
sequential weak labels and determine specific locations of every target
activity with only knowledge of which types of activities contained in the long
sequence. It will greatly reduce the burden of manual labeling. The code of our
work is available at https://github.com/KennCoder7/RAN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Wang_K/0/1/0/all/0/1"&gt;Kun Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+He_J/0/1/0/all/0/1"&gt;Jun He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lei Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[From augmented microscopy to the topological transformer: a new approach in cell image analysis for Alzheimer's research. (arXiv:2108.01625v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2108.01625</id>
        <link href="http://arxiv.org/abs/2108.01625"/>
        <updated>2021-08-04T01:59:23.459Z</updated>
        <summary type="html"><![CDATA[Cell image analysis is crucial in Alzheimer's research to detect the presence
of A$\beta$ protein inhibiting cell function. Deep learning speeds up the
process by making only low-level data sufficient for fruitful inspection. We
first found Unet is most suitable in augmented microscopy by comparing
performance in multi-class semantics segmentation. We develop the augmented
microscopy method to capture nuclei in a brightfield image and the transformer
using Unet model to convert an input image into a sequence of topological
information. The performance regarding Intersection-over-Union is consistent
concerning the choice of image preprocessing and ground-truth generation.
Training model with data of a specific cell type demonstrates transfer learning
applies to some extent.

The topological transformer aims to extract persistence silhouettes or
landscape signatures containing geometric information of a given image of
cells. This feature extraction facilitates studying an image as a collection of
one-dimensional data, substantially reducing computational costs. Using the
transformer, we attempt grouping cell images by their cell type relying solely
on topological features. Performances of the transformers followed by SVM,
XGBoost, LGBM, and simple convolutional neural network classifiers are inferior
to the conventional image classification. However, since this research
initiates a new perspective in biomedical research by combining deep learning
and topology for image analysis, we speculate follow-up investigation will
reinforce our genuine regime.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Jung_W/0/1/0/all/0/1"&gt;Wooseok Jung&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Semantic Segmentation by Contrasting Object Mask Proposals. (arXiv:2102.06191v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.06191</id>
        <link href="http://arxiv.org/abs/2102.06191"/>
        <updated>2021-08-04T01:59:23.452Z</updated>
        <summary type="html"><![CDATA[Being able to learn dense semantic representations of images without
supervision is an important problem in computer vision. However, despite its
significance, this problem remains rather unexplored, with a few exceptions
that considered unsupervised semantic segmentation on small-scale datasets with
a narrow visual domain. In this paper, we make a first attempt to tackle the
problem on datasets that have been traditionally utilized for the supervised
case. To achieve this, we introduce a two-step framework that adopts a
predetermined mid-level prior in a contrastive optimization objective to learn
pixel embeddings. This marks a large deviation from existing works that relied
on proxy tasks or end-to-end clustering. Additionally, we argue about the
importance of having a prior that contains information about objects, or their
parts, and discuss several possibilities to obtain such a prior in an
unsupervised manner.

Experimental evaluation shows that our method comes with key advantages over
existing works. First, the learned pixel embeddings can be directly clustered
in semantic groups using K-Means on PASCAL. Under the fully unsupervised
setting, there is no precedent in solving the semantic segmentation task on
such a challenging benchmark. Second, our representations can improve over
strong baselines when transferred to new datasets, e.g. COCO and DAVIS. The
code is available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gansbeke_W/0/1/0/all/0/1"&gt;Wouter Van Gansbeke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vandenhende_S/0/1/0/all/0/1"&gt;Simon Vandenhende&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Georgoulis_S/0/1/0/all/0/1"&gt;Stamatios Georgoulis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1"&gt;Luc Van Gool&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Creation and Detection of German Voice Deepfakes. (arXiv:2108.01469v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2108.01469</id>
        <link href="http://arxiv.org/abs/2108.01469"/>
        <updated>2021-08-04T01:59:23.445Z</updated>
        <summary type="html"><![CDATA[Synthesizing voice with the help of machine learning techniques has made
rapid progress over the last years [1] and first high profile fraud cases have
been recently reported [2]. Given the current increase in using conferencing
tools for online teaching, we question just how easy (i.e. needed data,
hardware, skill set) it would be to create a convincing voice fake. We analyse
how much training data a participant (e.g. a student) would actually need to
fake another participants voice (e.g. a professor). We provide an analysis of
the existing state of the art in creating voice deep fakes, as well as offer
detailed technical guidance and evidence of just how much effort is needed to
copy a voice. A user study with more than 100 participants shows how difficult
it is to identify real and fake voice (on avg. only 37 percent can distinguish
between real and fake voice of a professor). With a focus on German language
and an online teaching environment we discuss the societal implications as well
as demonstrate how to use machine learning techniques to possibly detect such
fakes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Barnekow_V/0/1/0/all/0/1"&gt;Vanessa Barnekow&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Binder_D/0/1/0/all/0/1"&gt;Dominik Binder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kromrey_N/0/1/0/all/0/1"&gt;Niclas Kromrey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Munaretto_P/0/1/0/all/0/1"&gt;Pascal Munaretto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Schaad_A/0/1/0/all/0/1"&gt;Andreas Schaad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Schmieder_F/0/1/0/all/0/1"&gt;Felix Schmieder&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Grounding Representation Similarity with Statistical Testing. (arXiv:2108.01661v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.01661</id>
        <link href="http://arxiv.org/abs/2108.01661"/>
        <updated>2021-08-04T01:59:23.439Z</updated>
        <summary type="html"><![CDATA[To understand neural network behavior, recent works quantitatively compare
different networks' learned representations using canonical correlation
analysis (CCA), centered kernel alignment (CKA), and other dissimilarity
measures. Unfortunately, these widely used measures often disagree on
fundamental observations, such as whether deep networks differing only in
random initialization learn similar representations. These disagreements raise
the question: which, if any, of these dissimilarity measures should we believe?
We provide a framework to ground this question through a concrete test:
measures should have sensitivity to changes that affect functional behavior,
and specificity against changes that do not. We quantify this through a variety
of functional behaviors including probing accuracy and robustness to
distribution shift, and examine changes such as varying random initialization
and deleting principal components. We find that current metrics exhibit
different weaknesses, note that a classical baseline performs surprisingly
well, and highlight settings where all metrics appear to fail, thus providing a
challenge set for further improvement.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ding_F/0/1/0/all/0/1"&gt;Frances Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Denain_J/0/1/0/all/0/1"&gt;Jean-Stanislas Denain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Steinhardt_J/0/1/0/all/0/1"&gt;Jacob Steinhardt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Inference via Sparse Coding in a Hierarchical Vision Model. (arXiv:2108.01548v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.01548</id>
        <link href="http://arxiv.org/abs/2108.01548"/>
        <updated>2021-08-04T01:59:23.422Z</updated>
        <summary type="html"><![CDATA[Sparse coding has been incorporated in models of the visual cortex for its
computational advantages and connection to biology. But how the level of
sparsity contributes to performance on visual tasks is not well understood. In
this work, sparse coding has been integrated into an existing hierarchical V2
model (Hosoya and Hyv\"arinen, 2015), but replacing the Independent Component
Analysis (ICA) with an explicit sparse coding in which the degree of sparsity
can be controlled. After training, the sparse coding basis functions with a
higher degree of sparsity resembled qualitatively different structures, such as
curves and corners. The contributions of the models were assessed with image
classification tasks, including object classification, and tasks associated
with mid-level vision including figure-ground classification, texture
classification, and angle prediction between two line stimuli. In addition, the
models were assessed in comparison to a texture sensitivity measure that has
been reported in V2 (Freeman et al., 2013), and a deleted-region inference
task. The results from the experiments show that while sparse coding performed
worse than ICA at classifying images, only sparse coding was able to better
match the texture sensitivity level of V2 and infer deleted image regions, both
by increasing the degree of sparsity in sparse coding. Higher degrees of
sparsity allowed for inference over larger deleted image regions. The mechanism
that allows for this inference capability in sparse coding is described here.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bowren_J/0/1/0/all/0/1"&gt;Joshua Bowren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sanchez_Giraldo_L/0/1/0/all/0/1"&gt;Luis Sanchez-Giraldo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schwartz_O/0/1/0/all/0/1"&gt;Odelia Schwartz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Non-local Graph Convolutional Network for joint Activity Recognition and Motion Prediction. (arXiv:2108.01518v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.01518</id>
        <link href="http://arxiv.org/abs/2108.01518"/>
        <updated>2021-08-04T01:59:23.414Z</updated>
        <summary type="html"><![CDATA[3D skeleton-based motion prediction and activity recognition are two
interwoven tasks in human behaviour analysis. In this work, we propose a motion
context modeling methodology that provides a new way to combine the advantages
of both graph convolutional neural networks and recurrent neural networks for
joint human motion prediction and activity recognition. Our approach is based
on using an LSTM encoder-decoder and a non-local feature extraction attention
mechanism to model the spatial correlation of human skeleton data and temporal
correlation among motion frames. The proposed network can easily include two
output branches, one for Activity Recognition and one for Future Motion
Prediction, which can be jointly trained for enhanced performance. Experimental
results on Human 3.6M, CMU Mocap and NTU RGB-D datasets show that our proposed
approach provides the best prediction capability among baseline LSTM-based
methods, while achieving comparable performance to other state-of-the-art
methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1"&gt;Dianhao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vien_N/0/1/0/all/0/1"&gt;Ngo Anh Vien&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Van_M/0/1/0/all/0/1"&gt;Mien Van&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McLoone_S/0/1/0/all/0/1"&gt;Sean McLoone&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Quantum Neural Networks: Concepts, Applications, and Challenges. (arXiv:2108.01468v1 [quant-ph])]]></title>
        <id>http://arxiv.org/abs/2108.01468</id>
        <link href="http://arxiv.org/abs/2108.01468"/>
        <updated>2021-08-04T01:59:23.406Z</updated>
        <summary type="html"><![CDATA[Quantum deep learning is a research field for the use of quantum computing
techniques for training deep neural networks. The research topics and
directions of deep learning and quantum computing have been separated for long
time, however by discovering that quantum circuits can act like artificial
neural networks, quantum deep learning research is widely adopted. This paper
explains the backgrounds and basic principles of quantum deep learning and also
introduces major achievements. After that, this paper discusses the challenges
of quantum deep learning research in multiple perspectives. Lastly, this paper
presents various future research directions and application fields of quantum
deep learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Kwak_Y/0/1/0/all/0/1"&gt;Yunseok Kwak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Yun_W/0/1/0/all/0/1"&gt;Won Joon Yun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Jung_S/0/1/0/all/0/1"&gt;Soyi Jung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Kim_J/0/1/0/all/0/1"&gt;Joongheon Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Near-optimal Algorithms for Explainable k-Medians and k-Means. (arXiv:2107.00798v2 [cs.DS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.00798</id>
        <link href="http://arxiv.org/abs/2107.00798"/>
        <updated>2021-08-04T01:59:23.400Z</updated>
        <summary type="html"><![CDATA[We consider the problem of explainable $k$-medians and $k$-means introduced
by Dasgupta, Frost, Moshkovitz, and Rashtchian~(ICML 2020). In this problem,
our goal is to find a threshold decision tree that partitions data into $k$
clusters and minimizes the $k$-medians or $k$-means objective. The obtained
clustering is easy to interpret because every decision node of a threshold tree
splits data based on a single feature into two groups. We propose a new
algorithm for this problem which is $\tilde O(\log k)$ competitive with
$k$-medians with $\ell_1$ norm and $\tilde O(k)$ competitive with $k$-means.
This is an improvement over the previous guarantees of $O(k)$ and $O(k^2)$ by
Dasgupta et al (2020). We also provide a new algorithm which is $O(\log^{3/2}
k)$ competitive for $k$-medians with $\ell_2$ norm. Our first algorithm is
near-optimal: Dasgupta et al (2020) showed a lower bound of $\Omega(\log k)$
for $k$-medians; in this work, we prove a lower bound of $\tilde\Omega(k)$ for
$k$-means. We also provide a lower bound of $\Omega(\log k)$ for $k$-medians
with $\ell_2$ norm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Makarychev_K/0/1/0/all/0/1"&gt;Konstantin Makarychev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shan_L/0/1/0/all/0/1"&gt;Liren Shan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fast Estimation Method for the Stability of Ensemble Feature Selectors. (arXiv:2108.01485v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.01485</id>
        <link href="http://arxiv.org/abs/2108.01485"/>
        <updated>2021-08-04T01:59:23.394Z</updated>
        <summary type="html"><![CDATA[It is preferred that feature selectors be \textit{stable} for better
interpretabity and robust prediction. Ensembling is known to be effective for
improving the stability of feature selectors. Since ensembling is
time-consuming, it is desirable to reduce the computational cost to estimate
the stability of the ensemble feature selectors. We propose a simulator of a
feature selector, and apply it to a fast estimation of the stability of
ensemble feature selectors. To the best of our knowledge, this is the first
study that estimates the stability of ensemble feature selectors and reduces
the computation time theoretically and empirically.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Onda_R/0/1/0/all/0/1"&gt;Rina Onda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1"&gt;Zhengyan Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kotera_M/0/1/0/all/0/1"&gt;Masaaki Kotera&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oono_K/0/1/0/all/0/1"&gt;Kenta Oono&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sequence Adaptation via Reinforcement Learning in Recommender Systems. (arXiv:2108.01442v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2108.01442</id>
        <link href="http://arxiv.org/abs/2108.01442"/>
        <updated>2021-08-04T01:59:23.378Z</updated>
        <summary type="html"><![CDATA[Accounting for the fact that users have different sequential patterns, the
main drawback of state-of-the-art recommendation strategies is that a fixed
sequence length of user-item interactions is required as input to train the
models. This might limit the recommendation accuracy, as in practice users
follow different trends on the sequential recommendations. Hence, baseline
strategies might ignore important sequential interactions or add noise to the
models with redundant interactions, depending on the variety of users'
sequential behaviours. To overcome this problem, in this study we propose the
SAR model, which not only learns the sequential patterns but also adjusts the
sequence length of user-item interactions in a personalized manner. We first
design an actor-critic framework, where the RL agent tries to compute the
optimal sequence length as an action, given the user's state representation at
a certain time step. In addition, we optimize a joint loss function to align
the accuracy of the sequential recommendations with the expected cumulative
rewards of the critic network, while at the same time we adapt the sequence
length with the actor network in a personalized manner. Our experimental
evaluation on four real-world datasets demonstrates the superiority of our
proposed model over several baseline approaches. Finally, we make our
implementation publicly available at https://github.com/stefanosantaris/sar.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Antaris_S/0/1/0/all/0/1"&gt;Stefanos Antaris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rafailidis_D/0/1/0/all/0/1"&gt;Dimitrios Rafailidis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Double-Dot Network for Antipodal Grasp Detection. (arXiv:2108.01527v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2108.01527</id>
        <link href="http://arxiv.org/abs/2108.01527"/>
        <updated>2021-08-04T01:59:23.371Z</updated>
        <summary type="html"><![CDATA[This paper proposes a new deep learning approach to antipodal grasp
detection, named Double-Dot Network (DD-Net). It follows the recent anchor-free
object detection framework, which does not depend on empirically pre-set
anchors and thus allows more generalized and flexible prediction on unseen
objects. Specifically, unlike the widely used 5-dimensional rectangle, the
gripper configuration is defined as a pair of fingertips. An effective CNN
architecture is introduced to localize such fingertips, and with the help of
auxiliary centers for refinement, it accurately and robustly infers grasp
candidates. Additionally, we design a specialized loss function to measure the
quality of grasps, and in contrast to the IoU scores of bounding boxes adopted
in object detection, it is more consistent to the grasp detection task. Both
the simulation and robotic experiments are executed and state of the art
accuracies are achieved, showing that DD-Net is superior to the counterparts in
handling unseen objects.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1"&gt;Yangtao Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_B/0/1/0/all/0/1"&gt;Boyang Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1"&gt;Di Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Hinge-Loss based Codebook Transfer for Cross-Domain Recommendation with Nonoverlapping Data. (arXiv:2108.01473v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2108.01473</id>
        <link href="http://arxiv.org/abs/2108.01473"/>
        <updated>2021-08-04T01:59:23.365Z</updated>
        <summary type="html"><![CDATA[Recommender systems(RS), especially collaborative filtering(CF) based RS, has
been playing an important role in many e-commerce applications. As the
information being searched over the internet is rapidly increasing, users often
face the difficulty of finding items of his/her own interest and RS often
provides help in such tasks. Recent studies show that, as the item space
increases, and the number of items rated by the users become very less, issues
like sparsity arise. To mitigate the sparsity problem, transfer learning
techniques are being used wherein the data from dense domain(source) is
considered in order to predict the missing entries in the sparse
domain(target). In this paper, we propose a transfer learning approach for
cross-domain recommendation when both domains have no overlap of users and
items. In our approach the transferring of knowledge from source to target
domain is done in a novel way. We make use of co-clustering technique to obtain
the codebook (cluster-level rating pattern) of source domain. By making use of
hinge loss function we transfer the learnt codebook of the source domain to
target. The use of hinge loss as a loss function is novel and has not been
tried before in transfer learning. We demonstrate that our technique improves
the approximation of the target matrix on benchmark datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Veeramachaneni_S/0/1/0/all/0/1"&gt;Sowmini Devi Veeramachaneni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pujari_A/0/1/0/all/0/1"&gt;Arun K Pujari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Padmanabhan_V/0/1/0/all/0/1"&gt;Vineet Padmanabhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1"&gt;Vikas Kumar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Inverse Reinforcement Learning Based Stochastic Driver Behavior Learning. (arXiv:2107.06344v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.06344</id>
        <link href="http://arxiv.org/abs/2107.06344"/>
        <updated>2021-08-04T01:59:23.347Z</updated>
        <summary type="html"><![CDATA[Drivers have unique and rich driving behaviors when operating vehicles in
traffic. This paper presents a novel driver behavior learning approach that
captures the uniqueness and richness of human driver behavior in realistic
driving scenarios. A stochastic inverse reinforcement learning (SIRL) approach
is proposed to learn a distribution of cost function, which represents the
richness of the human driver behavior with a given set of driver-specific
demonstrations. Evaluations are conducted on the realistic driving data
collected from the 3D driver-in-the-loop driving simulation. The results show
that the learned stochastic driver model is capable of expressing the richness
of the human driving strategies under different realistic driving scenarios.
Compared to the deterministic baseline driver behavior model, the results
reveal that the proposed stochastic driver behavior model can better replicate
the driver's unique and rich driving strategies in a variety of traffic
conditions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ozkan_M/0/1/0/all/0/1"&gt;Mehmet Fatih Ozkan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rocque_A/0/1/0/all/0/1"&gt;Abishek Joseph Rocque&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1"&gt;Yao Ma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[I3CL:Intra- and Inter-Instance Collaborative Learning for Arbitrary-shaped Scene Text Detection. (arXiv:2108.01343v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.01343</id>
        <link href="http://arxiv.org/abs/2108.01343"/>
        <updated>2021-08-04T01:59:23.341Z</updated>
        <summary type="html"><![CDATA[Existing methods for arbitrary-shaped text detection in natural scenes face
two critical issues, i.e., 1) fracture detections at the gaps in a text
instance; and 2) inaccurate detections of arbitrary-shaped text instances with
diverse background context. To address these issues, we propose a novel method
named Intra- and Inter-Instance Collaborative Learning (I3CL). Specifically, to
address the first issue, we design an effective convolutional module with
multiple receptive fields, which is able to collaboratively learn better
character and gap feature representations at local and long ranges inside a
text instance. To address the second issue, we devise an instance-based
transformer module to exploit the dependencies between different text instances
and a pixel-based transformer module to exploit the global context from the
shared background, which are able to collaboratively learn more discriminative
text feature representations. In this way, I3CL can effectively exploit the
intra- and inter-instance dependencies together in a unified end-to-end
trainable framework. Experimental results show that the proposed I3CL sets new
state-of-the-art performances on three challenging public benchmarks, i.e., an
F-measure of 76.4% on ICDAR2019-ArT, 86.2% on Total-Text, and 85.8% on
CTW-1500. Besides, I3CL with ResNeSt-101 backbone ranked 1st place on the
ICDAR2019-ArT leaderboard. The source code will be made publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1"&gt;Jian Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Juhua Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_B/0/1/0/all/0/1"&gt;Bo Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1"&gt;Dacheng Tao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Three-Dimensional Swarming Using Cyclic Stochastic Optimization. (arXiv:2010.05328v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.05328</id>
        <link href="http://arxiv.org/abs/2010.05328"/>
        <updated>2021-08-04T01:59:23.327Z</updated>
        <summary type="html"><![CDATA[In this paper we simulate an ensemble of cooperating, mobile sensing agents
that implement the cyclic stochastic optimization (CSO) algorithm in an attempt
to survey and track multiple targets. In the CSO algorithm proposed, each agent
uses its sensed measurements, its shared information, and its predictions of
others' future motion to decide on its next action. This decision is selected
to minimize a loss function that decreases as the uncertainty in the targets'
state estimates decreases. Only noisy measurements of this loss function are
available to each agent, and in this study, each agent attempts to minimize
this function by calculating its stochastic gradient. This paper examines, via
simulation-based experiments, the implications and applicability of CSO
convergence in three dimensions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Botts_C/0/1/0/all/0/1"&gt;Carsten H. Botts&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sequoia: A Software Framework to Unify Continual Learning Research. (arXiv:2108.01005v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2108.01005</id>
        <link href="http://arxiv.org/abs/2108.01005"/>
        <updated>2021-08-04T01:59:23.319Z</updated>
        <summary type="html"><![CDATA[The field of Continual Learning (CL) seeks to develop algorithms that
accumulate knowledge and skills over time through interaction with
non-stationary environments and data distributions. Measuring progress in CL
can be difficult because a plethora of evaluation procedures (ettings) and
algorithmic solutions (methods) have emerged, each with their own potentially
disjoint set of assumptions about the CL problem. In this work, we view each
setting as a set of assumptions. We then create a tree-shaped hierarchy of the
research settings in CL, in which more general settings become the parents of
those with more restrictive assumptions. This makes it possible to use
inheritance to share and reuse research, as developing a method for a given
setting also makes it directly applicable onto any of its children. We
instantiate this idea as a publicly available software framework called
Sequoia, which features a variety of settings from both the Continual
Supervised Learning (CSL) and Continual Reinforcement Learning (CRL) domains.
Sequoia also includes a growing suite of methods which are easy to extend and
customize, in addition to more specialized methods from third-party libraries.
We hope that this new paradigm and its first implementation can serve as a
foundation for the unification and acceleration of research in CL. You can help
us grow the tree by visiting www.github.com/lebrice/Sequoia.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Normandin_F/0/1/0/all/0/1"&gt;Fabrice Normandin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Golemo_F/0/1/0/all/0/1"&gt;Florian Golemo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ostapenko_O/0/1/0/all/0/1"&gt;Oleksiy Ostapenko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rodriguez_P/0/1/0/all/0/1"&gt;Pau Rodriguez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Riemer_M/0/1/0/all/0/1"&gt;Matthew D Riemer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hurtado_J/0/1/0/all/0/1"&gt;Julio Hurtado&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khetarpal_K/0/1/0/all/0/1"&gt;Khimya Khetarpal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1"&gt;Dominic Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lindeborg_R/0/1/0/all/0/1"&gt;Ryan Lindeborg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lesort_T/0/1/0/all/0/1"&gt;Timoth&amp;#xe9;e Lesort&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Charlin_L/0/1/0/all/0/1"&gt;Laurent Charlin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rish_I/0/1/0/all/0/1"&gt;Irina Rish&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Caccia_M/0/1/0/all/0/1"&gt;Massimo Caccia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Risk Adversarial Learning System for Connected and Autonomous Vehicle Charging. (arXiv:2108.01466v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2108.01466</id>
        <link href="http://arxiv.org/abs/2108.01466"/>
        <updated>2021-08-04T01:59:23.289Z</updated>
        <summary type="html"><![CDATA[In this paper, the design of a rational decision support system (RDSS) for a
connected and autonomous vehicle charging infrastructure (CAV-CI) is studied.
In the considered CAV-CI, the distribution system operator (DSO) deploys
electric vehicle supply equipment (EVSE) to provide an EV charging facility for
human-driven connected vehicles (CVs) and autonomous vehicles (AVs). The
charging request by the human-driven EV becomes irrational when it demands more
energy and charging period than its actual need. Therefore, the scheduling
policy of each EVSE must be adaptively accumulated the irrational charging
request to satisfy the charging demand of both CVs and AVs. To tackle this, we
formulate an RDSS problem for the DSO, where the objective is to maximize the
charging capacity utilization by satisfying the laxity risk of the DSO. Thus,
we devise a rational reward maximization problem to adapt the irrational
behavior by CVs in a data-informed manner. We propose a novel risk adversarial
multi-agent learning system (RAMALS) for CAV-CI to solve the formulated RDSS
problem. In RAMALS, the DSO acts as a centralized risk adversarial agent (RAA)
for informing the laxity risk to each EVSE. Subsequently, each EVSE plays the
role of a self-learner agent to adaptively schedule its own EV sessions by
coping advice from RAA. Experiment results show that the proposed RAMALS
affords around 46.6% improvement in charging rate, about 28.6% improvement in
the EVSE's active charging time and at least 33.3% more energy utilization, as
compared to a currently deployed ACN EVSE system, and other baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Munir_M/0/1/0/all/0/1"&gt;Md. Shirajum Munir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1"&gt;Ki Tae Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thar_K/0/1/0/all/0/1"&gt;Kyi Thar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niyato_D/0/1/0/all/0/1"&gt;Dusit Niyato&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hong_C/0/1/0/all/0/1"&gt;Choong Seon Hong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FEBR: Expert-Based Recommendation Framework for beneficial and personalized content. (arXiv:2108.01455v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2108.01455</id>
        <link href="http://arxiv.org/abs/2108.01455"/>
        <updated>2021-08-04T01:59:23.271Z</updated>
        <summary type="html"><![CDATA[So far, most research on recommender systems focused on maintaining long-term
user engagement and satisfaction, by promoting relevant and personalized
content. However, it is still very challenging to evaluate the quality and the
reliability of this content. In this paper, we propose FEBR (Expert-Based
Recommendation Framework), an apprenticeship learning framework to assess the
quality of the recommended content on online platforms. The framework exploits
the demonstrated trajectories of an expert (assumed to be reliable) in a
recommendation evaluation environment, to recover an unknown utility function.
This function is used to learn an optimal policy describing the expert's
behavior, which is then used in the framework to provide high-quality and
personalized recommendations. We evaluate the performance of our solution
through a user interest simulation environment (using RecSim). We simulate
interactions under the aforementioned expert policy for videos recommendation,
and compare its efficiency with standard recommendation methods. The results
show that our approach provides a significant gain in terms of content quality,
evaluated by experts and watched by users, while maintaining almost the same
watch time as the baseline approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lechiakh_M/0/1/0/all/0/1"&gt;Mohamed Lechiakh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maurer_A/0/1/0/all/0/1"&gt;Alexandre Maurer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using Query Expansion in Manifold Ranking for Query-Oriented Multi-Document Summarization. (arXiv:2108.01441v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2108.01441</id>
        <link href="http://arxiv.org/abs/2108.01441"/>
        <updated>2021-08-04T01:59:23.261Z</updated>
        <summary type="html"><![CDATA[Manifold ranking has been successfully applied in query-oriented
multi-document summarization. It not only makes use of the relationships among
the sentences, but also the relationships between the given query and the
sentences. However, the information of original query is often insufficient. So
we present a query expansion method, which is combined in the manifold ranking
to resolve this problem. Our method not only utilizes the information of the
query term itself and the knowledge base WordNet to expand it by synonyms, but
also uses the information of the document set itself to expand the query in
various ways (mean expansion, variance expansion and TextRank expansion).
Compared with the previous query expansion methods, our method combines
multiple query expansion methods to better represent query information, and at
the same time, it makes a useful attempt on manifold ranking. In addition, we
use the degree of word overlap and the proximity between words to calculate the
similarity between sentences. We performed experiments on the datasets of DUC
2006 and DUC2007, and the evaluation results show that the proposed query
expansion method can significantly improve the system performance and make our
system comparable to the state-of-the-art systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jia_Q/0/1/0/all/0/1"&gt;Quanye Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1"&gt;Rui Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1"&gt;Jianying Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Reinforcement Learning Based Networked Control with Network Delays for Signal Temporal Logic Specifications. (arXiv:2108.01317v1 [eess.SY])]]></title>
        <id>http://arxiv.org/abs/2108.01317</id>
        <link href="http://arxiv.org/abs/2108.01317"/>
        <updated>2021-08-04T01:59:23.254Z</updated>
        <summary type="html"><![CDATA[We present a novel deep reinforcement learning (DRL)-based design of a
networked controller with network delays for signal temporal logic (STL)
specifications. We consider the case in which both the system dynamics and
network delays are unknown. Because the satisfaction of an STL formula is based
not only on the current state but also on the behavior of the system, we
propose an extension of the Markov decision process (MDP), which is called a
$\tau\delta$-MDP, such that we can evaluate the satisfaction of the STL formula
under the network delays using the $\tau\delta$-MDP. Thereafter, we construct
deep neural networks based on the $\tau\delta$-MDP and propose a learning
algorithm. Through simulations, we also demonstrate the learning performance of
the proposed algorithm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Ikemoto_J/0/1/0/all/0/1"&gt;Junya Ikemoto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ushio_T/0/1/0/all/0/1"&gt;Toshimitsu Ushio&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Is Disentanglement enough? On Latent Representations for Controllable Music Generation. (arXiv:2108.01450v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2108.01450</id>
        <link href="http://arxiv.org/abs/2108.01450"/>
        <updated>2021-08-04T01:59:23.236Z</updated>
        <summary type="html"><![CDATA[Improving controllability or the ability to manipulate one or more attributes
of the generated data has become a topic of interest in the context of deep
generative models of music. Recent attempts in this direction have relied on
learning disentangled representations from data such that the underlying
factors of variation are well separated. In this paper, we focus on the
relationship between disentanglement and controllability by conducting a
systematic study using different supervised disentanglement learning algorithms
based on the Variational Auto-Encoder (VAE) architecture. Our experiments show
that a high degree of disentanglement can be achieved by using different forms
of supervision to train a strong discriminative encoder. However, in the
absence of a strong generative decoder, disentanglement does not necessarily
imply controllability. The structure of the latent space with respect to the
VAE-decoder plays an important role in boosting the ability of a generative
model to manipulate different attributes. To this end, we also propose methods
and metrics to help evaluate the quality of a latent space with respect to the
afforded degree of controllability.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pati_A/0/1/0/all/0/1"&gt;Ashis Pati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lerch_A/0/1/0/all/0/1"&gt;Alexander Lerch&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Electrical peak demand forecasting- A review. (arXiv:2108.01393v1 [eess.SY])]]></title>
        <id>http://arxiv.org/abs/2108.01393</id>
        <link href="http://arxiv.org/abs/2108.01393"/>
        <updated>2021-08-04T01:59:23.222Z</updated>
        <summary type="html"><![CDATA[The power system is undergoing rapid evolution with the roll-out of advanced
metering infrastructure and local energy applications (e.g. electric vehicles)
as well as the increasing penetration of intermittent renewable energy at both
transmission and distribution level, which characterizes the peak load demand
with stronger randomness and less predictability and therefore poses a threat
to the power grid security. Since storing large quantities of electricity to
satisfy load demand is neither economically nor environmentally friendly,
effective peak demand management strategies and reliable peak load forecast
methods become essential for optimizing the power system operations. To this
end, this paper provides a timely and comprehensive overview of peak load
demand forecast methods in the literature. To our best knowledge, this is the
first comprehensive review on such topic. In this paper we first give a precise
and unified problem definition of peak load demand forecast. Second, 139 papers
on peak load forecast methods were systematically reviewed where methods were
classified into different stages based on the timeline. Thirdly, a comparative
analysis of peak load forecast methods are summarized and different optimizing
methods to improve the forecast performance are discussed. The paper ends with
a comprehensive summary of the reviewed papers and a discussion of potential
future research directions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Dai_S/0/1/0/all/0/1"&gt;Shuang Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Meng_F/0/1/0/all/0/1"&gt;Fanlin Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Dai_H/0/1/0/all/0/1"&gt;Hongsheng Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_Q/0/1/0/all/0/1"&gt;Qian Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xizhong Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GalaxAI: Machine learning toolbox for interpretable analysis of spacecraft telemetry data. (arXiv:2108.01407v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.01407</id>
        <link href="http://arxiv.org/abs/2108.01407"/>
        <updated>2021-08-04T01:59:23.206Z</updated>
        <summary type="html"><![CDATA[We present GalaxAI - a versatile machine learning toolbox for efficient and
interpretable end-to-end analysis of spacecraft telemetry data. GalaxAI employs
various machine learning algorithms for multivariate time series analyses,
classification, regression and structured output prediction, capable of
handling high-throughput heterogeneous data. These methods allow for the
construction of robust and accurate predictive models, that are in turn applied
to different tasks of spacecraft monitoring and operations planning. More
importantly, besides the accurate building of models, GalaxAI implements a
visualisation layer, providing mission specialists and operators with a full,
detailed and interpretable view of the data analysis process. We show the
utility and versatility of GalaxAI on two use-cases concerning two different
spacecraft: i) analysis and planning of Mars Express thermal power consumption
and ii) predicting of INTEGRAL's crossings through Van Allen belts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kostovska_A/0/1/0/all/0/1"&gt;Ana Kostovska&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Petkovicc_M/0/1/0/all/0/1"&gt;Matej Petkovic&amp;#x107;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stepisnik_T/0/1/0/all/0/1"&gt;Toma&amp;#x17e; Stepi&amp;#x161;nik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lucas_L/0/1/0/all/0/1"&gt;Luke Lucas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Finn_T/0/1/0/all/0/1"&gt;Timothy Finn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martinez_Heras_J/0/1/0/all/0/1"&gt;Jos&amp;#xe9; Mart&amp;#xed;nez-Heras&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Panov_P/0/1/0/all/0/1"&gt;Pan&amp;#x10d;e Panov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dzeroski_S/0/1/0/all/0/1"&gt;Sa&amp;#x161;o D&amp;#x17e;eroski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Donati_A/0/1/0/all/0/1"&gt;Alessandro Donati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Simidjievski_N/0/1/0/all/0/1"&gt;Nikola Simidjievski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kocev_D/0/1/0/all/0/1"&gt;Dragi Kocev&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Robust Compressed Sensing MRI with Deep Generative Priors. (arXiv:2108.01368v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.01368</id>
        <link href="http://arxiv.org/abs/2108.01368"/>
        <updated>2021-08-04T01:59:23.191Z</updated>
        <summary type="html"><![CDATA[The CSGM framework (Bora-Jalal-Price-Dimakis'17) has shown that deep
generative priors can be powerful tools for solving inverse problems. However,
to date this framework has been empirically successful only on certain datasets
(for example, human faces and MNIST digits), and it is known to perform poorly
on out-of-distribution samples. In this paper, we present the first successful
application of the CSGM framework on clinical MRI data. We train a generative
prior on brain scans from the fastMRI dataset, and show that posterior sampling
via Langevin dynamics achieves high quality reconstructions. Furthermore, our
experiments and theory show that posterior sampling is robust to changes in the
ground-truth distribution and measurement process. Our code and models are
available at: \url{https://github.com/utcsilab/csgm-mri-langevin}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jalal_A/0/1/0/all/0/1"&gt;Ajil Jalal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arvinte_M/0/1/0/all/0/1"&gt;Marius Arvinte&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Daras_G/0/1/0/all/0/1"&gt;Giannis Daras&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Price_E/0/1/0/all/0/1"&gt;Eric Price&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dimakis_A/0/1/0/all/0/1"&gt;Alexandros G. Dimakis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tamir_J/0/1/0/all/0/1"&gt;Jonathan I. Tamir&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RAIN: Reinforced Hybrid Attention Inference Network for Motion Forecasting. (arXiv:2108.01316v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.01316</id>
        <link href="http://arxiv.org/abs/2108.01316"/>
        <updated>2021-08-04T01:59:23.185Z</updated>
        <summary type="html"><![CDATA[Motion forecasting plays a significant role in various domains (e.g.,
autonomous driving, human-robot interaction), which aims to predict future
motion sequences given a set of historical observations. However, the observed
elements may be of different levels of importance. Some information may be
irrelevant or even distracting to the forecasting in certain situations. To
address this issue, we propose a generic motion forecasting framework (named
RAIN) with dynamic key information selection and ranking based on a hybrid
attention mechanism. The general framework is instantiated to handle
multi-agent trajectory prediction and human motion forecasting tasks,
respectively. In the former task, the model learns to recognize the relations
between agents with a graph representation and to determine their relative
significance. In the latter task, the model learns to capture the temporal
proximity and dependency in long-term human motions. We also propose an
effective double-stage training pipeline with an alternating training strategy
to optimize the parameters in different modules of the framework. We validate
the framework on both synthetic simulations and motion forecasting benchmarks
in different domains, demonstrating that our method not only achieves
state-of-the-art forecasting performance, but also provides interpretable and
reasonable hybrid attention weights.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jiachen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1"&gt;Fan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1"&gt;Hengbo Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Malla_S/0/1/0/all/0/1"&gt;Srikanth Malla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tomizuka_M/0/1/0/all/0/1"&gt;Masayoshi Tomizuka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_C/0/1/0/all/0/1"&gt;Chiho Choi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Classifying action correctness in physical rehabilitation exercises. (arXiv:2108.01375v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.01375</id>
        <link href="http://arxiv.org/abs/2108.01375"/>
        <updated>2021-08-04T01:59:23.167Z</updated>
        <summary type="html"><![CDATA[The work in this paper focuses on the role of machine learning in assessing
the correctness of a human motion or action. This task proves to be more
challenging than the gesture and action recognition ones. We will demonstrate,
through a set of experiments on a recent dataset, that machine learning
algorithms can produce good results for certain actions, but can also fall into
the trap of classifying an incorrect execution of an action as a correct
execution of another action.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Miron_A/0/1/0/all/0/1"&gt;Alina Miron&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grosan_C/0/1/0/all/0/1"&gt;Crina Grosan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Accelerating the Convergence of Human-in-the-Loop Reinforcement Learning with Counterfactual Explanations. (arXiv:2108.01358v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2108.01358</id>
        <link href="http://arxiv.org/abs/2108.01358"/>
        <updated>2021-08-04T01:59:23.152Z</updated>
        <summary type="html"><![CDATA[The capability to interactively learn from human feedback would enable robots
in new social settings. For example, novice users could train service robots in
new tasks naturally and interactively. Human-in-the-loop Reinforcement Learning
(HRL) addresses this issue by combining human feedback and reinforcement
learning (RL) techniques. State-of-the-art interactive learning techniques
suffer from slow convergence, thus leading to a frustrating experience for the
human. This work approaches this problem by extending the existing TAMER
Framework with the possibility to enhance human feedback with two different
types of counterfactual explanations. We demonstrate our extensions' success in
improving the convergence, especially in the crucial early phases of the
training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Karalus_J/0/1/0/all/0/1"&gt;Jakob Karalus&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lindner_F/0/1/0/all/0/1"&gt;Felix Lindner&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ARIANN: Low-Interaction Privacy-Preserving Deep Learning via Function Secret Sharing. (arXiv:2006.04593v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.04593</id>
        <link href="http://arxiv.org/abs/2006.04593"/>
        <updated>2021-08-04T01:59:23.076Z</updated>
        <summary type="html"><![CDATA[We propose AriaNN, a low-interaction privacy-preserving framework for private
neural network training and inference on sensitive data. Our semi-honest
2-party computation protocol leverages function secret sharing, a recent
lightweight cryptographic protocol that allows us to achieve an efficient
online phase. We design optimized primitives for the building blocks of neural
networks such as ReLU, MaxPool and BatchNorm. For instance, we perform private
comparison for ReLU operations with a single message of the size of the input
during the online phase, and with preprocessing keys close to 4X smaller than
previous work. Last, we propose an extension to support n-party private
federated learning. We implement our framework as an extensible system on top
of PyTorch that leverages CPU and GPU hardware acceleration for cryptographic
and machine learning operations. We evaluate our end-to-end system for private
inference and training on standard neural networks such as AlexNet, VGG16 or
ResNet18 between distant servers. We show that computation rather than
communication is the main bottleneck and that using GPUs together with reduced
key size is a promising solution to overcome this barrier.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ryffel_T/0/1/0/all/0/1"&gt;Th&amp;#xe9;o Ryffel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tholoniat_P/0/1/0/all/0/1"&gt;Pierre Tholoniat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pointcheval_D/0/1/0/all/0/1"&gt;David Pointcheval&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bach_F/0/1/0/all/0/1"&gt;Francis Bach&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automatic classification of eclipsing binary stars using deep learning methods. (arXiv:2108.01640v1 [astro-ph.SR])]]></title>
        <id>http://arxiv.org/abs/2108.01640</id>
        <link href="http://arxiv.org/abs/2108.01640"/>
        <updated>2021-08-04T01:59:23.049Z</updated>
        <summary type="html"><![CDATA[In the last couple of decades, tremendous progress has been achieved in
developing robotic telescopes and, as a result, sky surveys (both terrestrial
and space) have become the source of a substantial amount of new observational
data. These data contain a lot of information about binary stars, hidden in
their light curves. With the huge amount of astronomical data gathered, it is
not reasonable to expect all the data to be manually processed and analyzed.
Therefore, in this paper, we focus on the automatic classification of eclipsing
binary stars using deep learning methods. Our classifier provides a tool for
the categorization of light curves of binary stars into two classes: detached
and over-contact. We used the ELISa software to obtain synthetic data, which we
then used for the training of the classifier. For evaluation purposes, we
collected 100 light curves of observed binary stars, in order to evaluate a
number of classifiers. We evaluated semi-detached eclipsing binary stars as
detached. The best-performing classifier combines bidirectional Long Short-Term
Memory (LSTM) and a one-dimensional convolutional neural network, which
achieved 98% accuracy on the evaluation set. Omitting semi-detached eclipsing
binary stars, we could obtain 100% accuracy in classification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/astro-ph/1/au:+Cokina_M/0/1/0/all/0/1"&gt;Michal &amp;#x10c;okina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Maslej_Kresnakova_V/0/1/0/all/0/1"&gt;Viera Maslej-Kre&amp;#x161;&amp;#x148;&amp;#xe1;kov&amp;#xe1;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Butka_P/0/1/0/all/0/1"&gt;Peter Butka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Parimucha_S/0/1/0/all/0/1"&gt;&amp;#x160;tefan Parimucha&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Calibration for Scalable Beamforming in FDD Massive MIMO with Implicit Channel Estimation. (arXiv:2108.01529v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2108.01529</id>
        <link href="http://arxiv.org/abs/2108.01529"/>
        <updated>2021-08-04T01:59:23.040Z</updated>
        <summary type="html"><![CDATA[Channel estimation and beamforming play critical roles in frequency-division
duplexing (FDD) massive multiple-input multiple-output (MIMO) systems. However,
these two modules have been treated as two stand-alone components, which makes
it difficult to achieve a global system optimality. In this paper, we propose a
deep learning-based approach that directly optimizes the beamformers at the
base station according to the received uplink pilots, thereby, bypassing the
explicit channel estimation. Different from the existing fully data-driven
approach where all the modules are replaced by deep neural networks (DNNs), a
neural calibration method is proposed to improve the scalability of the
end-to-end design. In particular, the backbone of conventional time-efficient
algorithms, i.e., the least-squares (LS) channel estimator and the zero-forcing
(ZF) beamformer, is preserved and DNNs are leveraged to calibrate their inputs
for better performance. The permutation equivariance property of the formulated
resource allocation problem is then identified to design a low-complexity
neural network architecture. Simulation results will show the superiority of
the proposed neural calibration method over benchmark schemes in terms of both
the spectral efficiency and scalability in large-scale wireless networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Ma_Y/0/1/0/all/0/1"&gt;Yifan Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shen_Y/0/1/0/all/0/1"&gt;Yifei Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yu_X/0/1/0/all/0/1"&gt;Xianghao Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jun Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Song_S/0/1/0/all/0/1"&gt;S.H. Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Letaief_K/0/1/0/all/0/1"&gt;Khaled B. Letaief&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Task Agnostic Metrics for Reservoir Computing. (arXiv:2108.01512v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.01512</id>
        <link href="http://arxiv.org/abs/2108.01512"/>
        <updated>2021-08-04T01:59:23.002Z</updated>
        <summary type="html"><![CDATA[Physical reservoir computing is a computational paradigm that enables
temporal pattern recognition to be performed directly in physical matter. By
exciting non-linear dynamical systems and linearly classifying their changes in
state, we can create highly energy-efficient devices capable of solving machine
learning tasks without the need to build a modular system consisting of
millions of neurons interconnected by synapses. The chosen dynamical system
must have three desirable properties: non-linearity, complexity, and fading
memory to act as an effective reservoir. We present task agnostic quantitative
measures for each of these three requirements and exemplify them for two
reservoirs: an echo state network and a simulated magnetic skyrmion-based
reservoir. We show that, in general, systems with lower damping reach higher
values in all three performance metrics. Whilst for input signal strength,
there is a natural trade-off between memory capacity and non-linearity of the
reservoir's behaviour. In contrast to typical task-dependent reservoir
computing benchmarks, these metrics can be evaluated in parallel from a single
input signal, drastically speeding up the parameter search to design efficient
and high-performance reservoirs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Love_J/0/1/0/all/0/1"&gt;Jake Love&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mulkers_J/0/1/0/all/0/1"&gt;Jeroen Mulkers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bourianoff_G/0/1/0/all/0/1"&gt;George Bourianoff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leliaert_J/0/1/0/all/0/1"&gt;Jonathan Leliaert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Everschor_Sitte_K/0/1/0/all/0/1"&gt;Karin Everschor-Sitte&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SphereFace2: Binary Classification is All You Need for Deep Face Recognition. (arXiv:2108.01513v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.01513</id>
        <link href="http://arxiv.org/abs/2108.01513"/>
        <updated>2021-08-04T01:59:22.940Z</updated>
        <summary type="html"><![CDATA[State-of-the-art deep face recognition methods are mostly trained with a
softmax-based multi-class classification framework. Despite being popular and
effective, these methods still have a few shortcomings that limit empirical
performance. In this paper, we first identify the discrepancy between training
and evaluation in the existing multi-class classification framework and then
discuss the potential limitations caused by the "competitive" nature of softmax
normalization. Motivated by these limitations, we propose a novel binary
classification training framework, termed SphereFace2. In contrast to existing
methods, SphereFace2 circumvents the softmax normalization, as well as the
corresponding closed-set assumption. This effectively bridges the gap between
training and evaluation, enabling the representations to be improved
individually by each binary classification task. Besides designing a specific
well-performing loss function, we summarize a few general principles for this
"one-vs-all" binary classification framework so that it can outperform current
competitive methods. We conduct comprehensive experiments on popular benchmarks
to demonstrate that SphereFace2 can consistently outperform current
state-of-the-art deep face recognition methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1"&gt;Yandong Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Weiyang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weller_A/0/1/0/all/0/1"&gt;Adrian Weller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raj_B/0/1/0/all/0/1"&gt;Bhiksha Raj&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1"&gt;Rita Singh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Controlled Deep Reinforcement Learning for Optimized Slice Placement. (arXiv:2108.01544v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.01544</id>
        <link href="http://arxiv.org/abs/2108.01544"/>
        <updated>2021-08-04T01:59:22.929Z</updated>
        <summary type="html"><![CDATA[We present a hybrid ML-heuristic approach that we name "Heuristically
Assisted Deep Reinforcement Learning (HA-DRL)" to solve the problem of Network
Slice Placement Optimization. The proposed approach leverages recent works on
Deep Reinforcement Learning (DRL) for slice placement and Virtual Network
Embedding (VNE) and uses a heuristic function to optimize the exploration of
the action space by giving priority to reliable actions indicated by an
efficient heuristic algorithm. The evaluation results show that the proposed
HA-DRL algorithm can accelerate the learning of an efficient slice placement
policy improving slice acceptance ratio when compared with state-of-the-art
approaches that are based only on reinforcement learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Esteves_J/0/1/0/all/0/1"&gt;Jose Jurandir Alves Esteves&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boubendir_A/0/1/0/all/0/1"&gt;Amina Boubendir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guillemin_F/0/1/0/all/0/1"&gt;Fabrice Guillemin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sens_P/0/1/0/all/0/1"&gt;Pierre Sens&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Memorize, Factorize, or be Na\"ive: Learning Optimal Feature Interaction Methods for CTR Prediction. (arXiv:2108.01265v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.01265</id>
        <link href="http://arxiv.org/abs/2108.01265"/>
        <updated>2021-08-04T01:59:22.914Z</updated>
        <summary type="html"><![CDATA[Click-through rate prediction is one of the core tasks in commercial
recommender systems. It aims to predict the probability of a user clicking a
particular item given user and item features. As feature interactions bring in
non-linearity, they are widely adopted to improve the performance of CTR
prediction models. Therefore, effectively modelling feature interactions has
attracted much attention in both the research and industry field. The current
approaches can generally be categorized into three classes: (1) na\"ive
methods, which do not model feature interactions and only use original
features; (2) memorized methods, which memorize feature interactions by
explicitly viewing them as new features and assigning trainable embeddings; (3)
factorized methods, which learn latent vectors for original features and
implicitly model feature interactions through factorization functions. Studies
have shown that modelling feature interactions by one of these methods alone
are suboptimal due to the unique characteristics of different feature
interactions. To address this issue, we first propose a general framework
called OptInter which finds the most suitable modelling method for each feature
interaction. Different state-of-the-art deep CTR models can be viewed as
instances of OptInter. To realize the functionality of OptInter, we also
introduce a learning algorithm that automatically searches for the optimal
modelling method. We conduct extensive experiments on four large datasets. Our
experiments show that OptInter improves the best performed state-of-the-art
baseline deep CTR models by up to 2.21%. Compared to the memorized method,
which also outperforms baselines, we reduce up to 91% parameters. In addition,
we conduct several ablation studies to investigate the influence of different
components of OptInter. Finally, we provide interpretable discussions on the
results of OptInter.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lyu_F/0/1/0/all/0/1"&gt;Fuyuan Lyu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1"&gt;Xing Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1"&gt;Huifeng Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_R/0/1/0/all/0/1"&gt;Ruiming Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1"&gt;Xiuqiang He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Rui Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xue Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Where do Models go Wrong? Parameter-Space Saliency Maps for Explainability. (arXiv:2108.01335v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.01335</id>
        <link href="http://arxiv.org/abs/2108.01335"/>
        <updated>2021-08-04T01:59:22.899Z</updated>
        <summary type="html"><![CDATA[Conventional saliency maps highlight input features to which neural network
predictions are highly sensitive. We take a different approach to saliency, in
which we identify and analyze the network parameters, rather than inputs, which
are responsible for erroneous decisions. We find that samples which cause
similar parameters to malfunction are semantically similar. We also show that
pruning the most salient parameters for a wrongly classified sample often
improves model behavior. Furthermore, fine-tuning a small number of the most
salient parameters on a single sample results in error correction on other
samples that are misclassified for similar reasons. Based on our parameter
saliency method, we also introduce an input-space saliency technique that
reveals how image features cause specific network components to malfunction.
Further, we rigorously validate the meaningfulness of our saliency maps on both
the dataset and case-study levels.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Levin_R/0/1/0/all/0/1"&gt;Roman Levin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shu_M/0/1/0/all/0/1"&gt;Manli Shu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Borgnia_E/0/1/0/all/0/1"&gt;Eitan Borgnia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1"&gt;Furong Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldblum_M/0/1/0/all/0/1"&gt;Micah Goldblum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1"&gt;Tom Goldstein&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Your fairness may vary: Group fairness of pretrained language models in toxic text classification. (arXiv:2108.01250v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.01250</id>
        <link href="http://arxiv.org/abs/2108.01250"/>
        <updated>2021-08-04T01:59:22.880Z</updated>
        <summary type="html"><![CDATA[We study the performance-fairness trade-off in more than a dozen fine-tuned
LMs for toxic text classification. We empirically show that no blanket
statement can be made with respect to the bias of large versus regular versus
compressed models. Moreover, we find that focusing on fairness-agnostic
performance metrics can lead to models with varied fairness characteristics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Baldini_I/0/1/0/all/0/1"&gt;Ioana Baldini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_D/0/1/0/all/0/1"&gt;Dennis Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramamurthy_K/0/1/0/all/0/1"&gt;Karthikeyan Natesan Ramamurthy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yurochkin_M/0/1/0/all/0/1"&gt;Mikhail Yurochkin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1"&gt;Moninder Singh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Average-reward model-free reinforcement learning: a systematic review and literature mapping. (arXiv:2010.08920v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.08920</id>
        <link href="http://arxiv.org/abs/2010.08920"/>
        <updated>2021-08-04T01:59:22.872Z</updated>
        <summary type="html"><![CDATA[Reinforcement learning is important part of artificial intelligence. In this
paper, we review model-free reinforcement learning that utilizes the average
reward optimality criterion in the infinite horizon setting. Motivated by the
solo survey by Mahadevan (1996a), we provide an updated review of work in this
area and extend it to cover policy-iteration and function approximation methods
(in addition to the value-iteration and tabular counterparts). We present a
comprehensive literature mapping. We also identify and discuss opportunities
for future work.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dewanto_V/0/1/0/all/0/1"&gt;Vektor Dewanto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dunn_G/0/1/0/all/0/1"&gt;George Dunn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eshragh_A/0/1/0/all/0/1"&gt;Ali Eshragh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gallagher_M/0/1/0/all/0/1"&gt;Marcus Gallagher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roosta_F/0/1/0/all/0/1"&gt;Fred Roosta&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[From augmented microscopy to the topological transformer: a new approach in cell image analysis for Alzheimer's research. (arXiv:2108.01625v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2108.01625</id>
        <link href="http://arxiv.org/abs/2108.01625"/>
        <updated>2021-08-04T01:59:22.866Z</updated>
        <summary type="html"><![CDATA[Cell image analysis is crucial in Alzheimer's research to detect the presence
of A$\beta$ protein inhibiting cell function. Deep learning speeds up the
process by making only low-level data sufficient for fruitful inspection. We
first found Unet is most suitable in augmented microscopy by comparing
performance in multi-class semantics segmentation. We develop the augmented
microscopy method to capture nuclei in a brightfield image and the transformer
using Unet model to convert an input image into a sequence of topological
information. The performance regarding Intersection-over-Union is consistent
concerning the choice of image preprocessing and ground-truth generation.
Training model with data of a specific cell type demonstrates transfer learning
applies to some extent.

The topological transformer aims to extract persistence silhouettes or
landscape signatures containing geometric information of a given image of
cells. This feature extraction facilitates studying an image as a collection of
one-dimensional data, substantially reducing computational costs. Using the
transformer, we attempt grouping cell images by their cell type relying solely
on topological features. Performances of the transformers followed by SVM,
XGBoost, LGBM, and simple convolutional neural network classifiers are inferior
to the conventional image classification. However, since this research
initiates a new perspective in biomedical research by combining deep learning
and topology for image analysis, we speculate follow-up investigation will
reinforce our genuine regime.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Jung_W/0/1/0/all/0/1"&gt;Wooseok Jung&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Can a single neuron learn quantiles?. (arXiv:2106.03702v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.03702</id>
        <link href="http://arxiv.org/abs/2106.03702"/>
        <updated>2021-08-04T01:59:22.860Z</updated>
        <summary type="html"><![CDATA[A novel non-parametric quantile estimation method for continuous random
variables is introduced, based on a minimal neural network architecture
consisting of a single unit. Its advantage over estimations from ranking the
order statistics is shown, specifically for small sample size. In a regression
context, the method can be used to quantify predictive uncertainty under the
split conformal prediction setting, where prediction intervals are estimated
from the residuals of a pre-trained model on a held-out validation set to
quantify the uncertainty in future predictions. Benchmarking experiments
demonstrate that the method is competitive in quality and coverage with
state-of-the-art solutions, with the added benefit of being more
computationally efficient.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Solano_Carrillo_E/0/1/0/all/0/1"&gt;Edgardo Solano-Carrillo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A probabilistic database approach to autoencoder-based data cleaning. (arXiv:2106.09764v2 [cs.DB] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.09764</id>
        <link href="http://arxiv.org/abs/2106.09764"/>
        <updated>2021-08-04T01:59:22.853Z</updated>
        <summary type="html"><![CDATA[Data quality problems are a large threat in data science. In this paper, we
propose a data-cleaning autoencoder capable of near-automatic data quality
improvement. It learns the structure and dependencies in the data and uses it
as evidence to identify and correct doubtful values. We apply a probabilistic
database approach to represent weak and strong evidence for attribute value
repairs. A theoretical framework is provided, and experiments show that it can
remove significant amounts of noise (i.e., data quality problems) from
categorical and numeric probabilistic data. Our method does not require clean
data. We do, however, show that manually cleaning a small fraction of the data
significantly improves performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mauritz_R/0/1/0/all/0/1"&gt;R.R. Mauritz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nijweide_F/0/1/0/all/0/1"&gt;F.P.J. Nijweide&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goseling_J/0/1/0/all/0/1"&gt;J. Goseling&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Keulen_M/0/1/0/all/0/1"&gt;M. van Keulen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Counting People by Estimating People Flows. (arXiv:2012.00452v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.00452</id>
        <link href="http://arxiv.org/abs/2012.00452"/>
        <updated>2021-08-04T01:59:22.847Z</updated>
        <summary type="html"><![CDATA[Modern methods for counting people in crowded scenes rely on deep networks to
estimate people densities in individual images. As such, only very few take
advantage of temporal consistency in video sequences, and those that do only
impose weak smoothness constraints across consecutive frames. In this paper, we
advocate estimating people flows across image locations between consecutive
images and inferring the people densities from these flows instead of directly
regressing them. This enables us to impose much stronger constraints encoding
the conservation of the number of people. As a result, it significantly boosts
performance without requiring a more complex architecture. Furthermore, it
allows us to exploit the correlation between people flow and optical flow to
further improve the results. We also show that leveraging people conservation
constraints in both a spatial and temporal manner makes it possible to train a
deep crowd counting model in an active learning setting with much fewer
annotations. This significantly reduces the annotation cost while still leading
to similar performance to the full supervision case.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Weizhe Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salzmann_M/0/1/0/all/0/1"&gt;Mathieu Salzmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fua_P/0/1/0/all/0/1"&gt;Pascal Fua&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Visualizing Data using GTSNE. (arXiv:2108.01301v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.01301</id>
        <link href="http://arxiv.org/abs/2108.01301"/>
        <updated>2021-08-04T01:59:22.829Z</updated>
        <summary type="html"><![CDATA[We present a new method GTSNE to visualize high-dimensional data points in
the two dimensional map. The technique is a variation of t-SNE that produces
better visualizations by capturing both the local neighborhood structure and
the macro structure in the data. This is particularly important for
high-dimensional data that lie on continuous low-dimensional manifolds. We
illustrate the performance of GTSNE on a wide variety of datasets and compare
it the state of art methods, including t-SNE and UMAP. The visualizations
produced by GTSNE are better than those produced by the other techniques on
almost all of the datasets on the macro structure preservation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1"&gt;Songting Shi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MBDP: A Model-based Approach to Achieve both Robustness and Sample Efficiency via Double Dropout Planning. (arXiv:2108.01295v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.01295</id>
        <link href="http://arxiv.org/abs/2108.01295"/>
        <updated>2021-08-04T01:59:22.814Z</updated>
        <summary type="html"><![CDATA[Model-based reinforcement learning is a widely accepted solution for solving
excessive sample demands. However, the predictions of the dynamics models are
often not accurate enough, and the resulting bias may incur catastrophic
decisions due to insufficient robustness. Therefore, it is highly desired to
investigate how to improve the robustness of model-based RL algorithms while
maintaining high sampling efficiency. In this paper, we propose Model-Based
Double-dropout Planning (MBDP) to balance robustness and efficiency. MBDP
consists of two kinds of dropout mechanisms, where the rollout-dropout aims to
improve the robustness with a small cost of sample efficiency, while the
model-dropout is designed to compensate for the lost efficiency at a slight
expense of robustness. By combining them in a complementary way, MBDP provides
a flexible control mechanism to meet different demands of robustness and
efficiency by tuning two corresponding dropout ratios. The effectiveness of
MBDP is demonstrated both theoretically and experimentally.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Wanpeng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1"&gt;Xi Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1"&gt;Yao Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1"&gt;Mingzhe Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_D/0/1/0/all/0/1"&gt;Dijun Luo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Process Mining Model to Predict Mortality in Paralytic Ileus Patients. (arXiv:2108.01267v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.01267</id>
        <link href="http://arxiv.org/abs/2108.01267"/>
        <updated>2021-08-04T01:59:22.808Z</updated>
        <summary type="html"><![CDATA[Paralytic Ileus (PI) patients are at high risk of death when admitted to the
Intensive care unit (ICU), with mortality as high as 40\%. There is minimal
research concerning PI patient mortality prediction. There is a need for more
accurate prediction modeling for ICU patients diagnosed with PI. This paper
demonstrates performance improvements in predicting the mortality of ICU
patients diagnosed with PI after 24 hours of being admitted. The proposed
framework, PMPI(Process Mining Model to predict mortality of PI patients), is a
modification of the work used for prediction of in-hospital mortality for ICU
patients with diabetes. PMPI demonstrates similar if not better performance
with an Area under the ROC Curve (AUC) score of 0.82 compared to the best
results of the existing literature. PMPI uses patient medical history, the time
related to the events, and demographic information for prediction. The PMPI
prediction framework has the potential to help medical teams in making better
decisions for treatment and care for ICU patients with PI to increase their
life expectancy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pishgar_M/0/1/0/all/0/1"&gt;Maryam Pishgar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Razo_M/0/1/0/all/0/1"&gt;Martha Razo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Theis_J/0/1/0/all/0/1"&gt;Julian Theis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Darabi_H/0/1/0/all/0/1"&gt;Houshang Darabi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TENSILE: A Tensor granularity dynamic GPU memory scheduling method towards multiple dynamic workloads system. (arXiv:2105.13336v3 [cs.DC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.13336</id>
        <link href="http://arxiv.org/abs/2105.13336"/>
        <updated>2021-08-04T01:59:22.795Z</updated>
        <summary type="html"><![CDATA[Recently, deep learning has been an area of intense researching. However, as
a kind of computing intensive task, deep learning highly relies on the scale of
GPU memory, which is usually prohibitive and scarce. Although there are some
extensive works have been proposed for dynamic GPU memory management, they are
hard to be applied to systems with multiple dynamic workloads, such as
in-database machine learning system.

In this paper, we demonstrated TENSILE, a method of managing GPU memory in
tensor granularity to reduce the GPU memory peak, with taking the multiple
dynamic workloads into consideration. As far as we know, TENSILE is the first
method which is designed to manage multiple workloads' GPU memory using. We
implement TENSILE on a deep learning framework built by ourselves, and
evaluated its performance. The experiment results show that TENSILE can save
more GPU memory with less extra time overhead than prior works in both single
and multiple dynamic workloads scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1"&gt;Kaixin Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hongzhi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1"&gt;Tongxin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1"&gt;Han Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zou_S/0/1/0/all/0/1"&gt;Songling Zou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_J/0/1/0/all/0/1"&gt;Jiye Qiu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-type Disentanglement without Adversarial Training. (arXiv:2012.08883v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.08883</id>
        <link href="http://arxiv.org/abs/2012.08883"/>
        <updated>2021-08-04T01:59:22.730Z</updated>
        <summary type="html"><![CDATA[Controlling the style of natural language by disentangling the latent space
is an important step towards interpretable machine learning. After the latent
space is disentangled, the style of a sentence can be transformed by tuning the
style representation without affecting other features of the sentence. Previous
works usually use adversarial training to guarantee that disentangled vectors
do not affect each other. However, adversarial methods are difficult to train.
Especially when there are multiple features (e.g., sentiment, or tense, which
we call style types in this paper), each feature requires a separate
discriminator for extracting a disentangled style vector corresponding to that
feature. In this paper, we propose a unified distribution-controlling method,
which provides each specific style value (the value of style types, e.g.,
positive sentiment, or past tense) with a unique representation. This method
contributes a solid theoretical basis to avoid adversarial training in
multi-type disentanglement. We also propose multiple loss functions to achieve
a style-content disentanglement as well as a disentanglement among multiple
style types. In addition, we observe that if two different style types always
have some specific style values that occur together in the dataset, they will
affect each other when transferring the style values. We call this phenomenon
training bias, and we propose a loss function to alleviate such training bias
while disentangling multiple types. We conduct experiments on two datasets
(Yelp service reviews and Amazon product reviews) to evaluate the
style-disentangling effect and the unsupervised style transfer performance on
two style types: sentiment and tense. The experimental results show the
effectiveness of our model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sha_L/0/1/0/all/0/1"&gt;Lei Sha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lukasiewicz_T/0/1/0/all/0/1"&gt;Thomas Lukasiewicz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ItNet: iterative neural networks with small graphs for accurate, efficient and anytime semantic segmentation. (arXiv:2101.08685v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.08685</id>
        <link href="http://arxiv.org/abs/2101.08685"/>
        <updated>2021-08-04T01:59:22.724Z</updated>
        <summary type="html"><![CDATA[Deep neural networks have usually to be compressed and accelerated for their
usage in low-power, e.g. mobile, devices. Recently, massively-parallel hardware
accelerators were developed that offer high throughput and low latency at low
power by utilizing in-memory computation. However, to exploit these benefits
the computational graph of a neural network has to fit into the in-computation
memory of these hardware systems that is usually rather limited in size. In
this study, we introduce a class of network models that have a small memory
footprint in terms of their computational graphs. To this end, the graph is
designed to contain loops by iteratively executing a single network building
block. Furthermore, the trade-off between accuracy and latency of these
so-called iterative neural networks is improved by adding multiple intermediate
outputs during both training and inference. We show state-of-the-art results
for semantic segmentation on the CamVid and Cityscapes datasets that are
especially demanding in terms of computational resources. In ablation studies,
the improvement of network training by intermediate network outputs as well as
the trade-off between weight sharing over iterations and the network size are
investigated.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pfeil_T/0/1/0/all/0/1"&gt;Thomas Pfeil&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[STAN: Synthetic Network Traffic Generation with Generative Neural Models. (arXiv:2009.12740v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.12740</id>
        <link href="http://arxiv.org/abs/2009.12740"/>
        <updated>2021-08-04T01:59:22.695Z</updated>
        <summary type="html"><![CDATA[Deep learning models have achieved great success in recent years but progress
in some domains like cybersecurity is stymied due to a paucity of realistic
datasets. Organizations are reluctant to share such data, even internally, due
to privacy reasons. An alternative is to use synthetically generated data but
existing methods are limited in their ability to capture complex dependency
structures, between attributes and across time. This paper presents STAN
(Synthetic network Traffic generation with Autoregressive Neural models), a
tool to generate realistic synthetic network traffic datasets for subsequent
downstream applications. Our novel neural architecture captures both temporal
dependencies and dependence between attributes at any given time. It integrates
convolutional neural layers with mixture density neural layers and softmax
layers, and models both continuous and discrete variables. We evaluate the
performance of STAN in terms of the quality of data generated, by training it
on both a simulated dataset and a real network traffic data set. Finally, to
answer the question - can real network traffic data be substituted with
synthetic data to train models of comparable accuracy? We train two anomaly
detection models based on self-supervision. The results show only a small
decline in the accuracy of models trained solely on synthetic data. While
current results are encouraging in terms of quality of data generated and
absence of any obvious data leakage from training data, in the future we plan
to further validate this fact by conducting privacy attacks on the generated
data. Other future work includes validating capture of long term dependencies
and making model training]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1"&gt;Shengzhe Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marwah_M/0/1/0/all/0/1"&gt;Manish Marwah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arlitt_M/0/1/0/all/0/1"&gt;Martin Arlitt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramakrishnan_N/0/1/0/all/0/1"&gt;Naren Ramakrishnan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Numerical Solution of Stiff Ordinary Differential Equations with Random Projection Neural Networks. (arXiv:2108.01584v1 [math.NA])]]></title>
        <id>http://arxiv.org/abs/2108.01584</id>
        <link href="http://arxiv.org/abs/2108.01584"/>
        <updated>2021-08-04T01:59:22.659Z</updated>
        <summary type="html"><![CDATA[We propose a numerical scheme based on Random Projection Neural Networks
(RPNN) for the solution of Ordinary Differential Equations (ODEs) with a focus
on stiff problems. In particular, we use an Extreme Learning Machine, a
single-hidden layer Feedforward Neural Network with Radial Basis Functions
which widths are uniformly distributed random variables, while the values of
the weights between the input and the hidden layer are set equal to one. The
numerical solution is obtained by constructing a system of nonlinear algebraic
equations, which is solved with respect to the output weights using the
Gauss-Newton method. For our illustrations, we apply the proposed machine
learning approach to solve two benchmark stiff problems, namely the Rober and
the van der Pol ones (the latter with large values of the stiffness parameter),
and we perform a comparison with well-established methods such as the adaptive
Runge-Kutta method based on the Dormand-Prince pair, and a variable-step
variable-order multistep solver based on numerical differentiation formulas, as
implemented in the \texttt{ode45} and \texttt{ode15s} MATLAB functions,
respectively. We show that our proposed scheme yields good numerical
approximation accuracy without being affected by the stiffness, thus
outperforming in same cases the \texttt{ode45} and \texttt{ode15s} functions.
Importantly, upon training using a fixed number of collocation points, the
proposed scheme approximates the solution in the whole domain in contrast to
the classical time integration methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Galaris_E/0/1/0/all/0/1"&gt;Evangelos Galaris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Calabro_F/0/1/0/all/0/1"&gt;Francesco Calabr&amp;#xf2;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Serafino_D/0/1/0/all/0/1"&gt;Daniela di Serafino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Siettos_C/0/1/0/all/0/1"&gt;Constantinos Siettos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adaptively Optimize Content Recommendation Using Multi Armed Bandit Algorithms in E-commerce. (arXiv:2108.01440v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2108.01440</id>
        <link href="http://arxiv.org/abs/2108.01440"/>
        <updated>2021-08-04T01:59:22.635Z</updated>
        <summary type="html"><![CDATA[E-commerce sites strive to provide users the most timely relevant information
in order to reduce shopping frictions and increase customer satisfaction. Multi
armed bandit models (MAB) as a type of adaptive optimization algorithms provide
possible approaches for such purposes. In this paper, we analyze using three
classic MAB algorithms, epsilon-greedy, Thompson sampling (TS), and upper
confidence bound 1 (UCB1) for dynamic content recommendations, and walk through
the process of developing these algorithms internally to solve a real world
e-commerce use case. First, we analyze the three MAB algorithms using simulated
purchasing datasets with non-stationary reward distributions to simulate the
possible time-varying customer preferences, where the traffic allocation
dynamics and the accumulative rewards of different algorithms are studied.
Second, we compare the accumulative rewards of the three MAB algorithms with
more than 1,000 trials using actual historical A/B test datasets. We find that
the larger difference between the success rates of competing recommendations
the more accumulative rewards the MAB algorithms can achieve. In addition, we
find that TS shows the highest average accumulative rewards under different
testing scenarios. Third, we develop a batch-updated MAB algorithm to overcome
the delayed reward issue in e-commerce and enable an online content
optimization on our App homepage. For a state-of-the-art comparison, a real A/B
test among our batch-updated MAB algorithm, a third-party MAB solution, and the
default business logic are conducted. The result shows that our batch-updated
MAB algorithm outperforms the counterparts and achieves 6.13% relative
click-through rate (CTR) increase and 16.1% relative conversion rate (CVR)
increase compared to the default experience, and 2.9% relative CTR increase and
1.4% relative CVR increase compared to the external MAB service.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xiang_D/0/1/0/all/0/1"&gt;Ding Xiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+West_B/0/1/0/all/0/1"&gt;Becky West&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jiaqi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_X/0/1/0/all/0/1"&gt;Xiquan Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1"&gt;Jinzhou Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Noise-Resistant Deep Metric Learning with Probabilistic Instance Filtering. (arXiv:2108.01431v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.01431</id>
        <link href="http://arxiv.org/abs/2108.01431"/>
        <updated>2021-08-04T01:59:22.629Z</updated>
        <summary type="html"><![CDATA[Noisy labels are commonly found in real-world data, which cause performance
degradation of deep neural networks. Cleaning data manually is labour-intensive
and time-consuming. Previous research mostly focuses on enhancing
classification models against noisy labels, while the robustness of deep metric
learning (DML) against noisy labels remains less well-explored. In this paper,
we bridge this important gap by proposing Probabilistic Ranking-based Instance
Selection with Memory (PRISM) approach for DML. PRISM calculates the
probability of a label being clean, and filters out potentially noisy samples.
Specifically, we propose three methods to calculate this probability: 1)
Average Similarity Method (AvgSim), which calculates the average similarity
between potentially noisy data and clean data; 2) Proxy Similarity Method
(ProxySim), which replaces the centers maintained by AvgSim with the proxies
trained by proxy-based method; and 3) von Mises-Fisher Distribution Similarity
(vMF-Sim), which estimates a von Mises-Fisher distribution for each data class.
With such a design, the proposed approach can deal with challenging DML
situations in which the majority of the samples are noisy. Extensive
experiments on both synthetic and real-world noisy dataset show that the
proposed approach achieves up to 8.37% higher Precision@1 compared with the
best performing state-of-the-art baseline approaches, within reasonable
training time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1"&gt;Chang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1"&gt;Han Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Boyang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1"&gt;Zhiqi Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1"&gt;Zhanning Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_P/0/1/0/all/0/1"&gt;Peiran Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1"&gt;Xuansong Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1"&gt;Lizhen Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Miao_C/0/1/0/all/0/1"&gt;Chunyan Miao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Solving Fashion Recommendation -- The Farfetch Challenge. (arXiv:2108.01314v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.01314</id>
        <link href="http://arxiv.org/abs/2108.01314"/>
        <updated>2021-08-04T01:59:22.567Z</updated>
        <summary type="html"><![CDATA[Recommendation engines are integral to the modern e-commerce experience, both
for the seller and the end user. Accurate recommendations lead to higher
revenue and better user experience. In this paper, we are presenting our
solution to ECML PKDD Farfetch Fashion Recommendation Challenge.The goal of
this challenge is to maximize the chances of a click when the users are
presented with set of fashion items. We have approached this problem as a
binary classification problem. Our winning solution utilizes Catboost as the
classifier and Bayesian Optimization for hyper parameter tuning. Our baseline
model achieved MRR of 0.5153 on the validation set. Bayesian optimization of
hyper parameters improved the MRR to 0.5240 on the validation set. Our final
submission on the test set achieved a MRR of 0.5257.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pathak_M/0/1/0/all/0/1"&gt;Manish Pathak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1"&gt;Aditya Jain&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-objective Recurrent Neural Networks Optimization for the Edge -- a Quantization-based Approach. (arXiv:2108.01192v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.01192</id>
        <link href="http://arxiv.org/abs/2108.01192"/>
        <updated>2021-08-04T01:59:22.560Z</updated>
        <summary type="html"><![CDATA[The compression of deep learning models is of fundamental importance in
deploying such models to edge devices. Incorporating hardware model and
application constraints during compression maximizes the benefits but makes it
specifically designed for one case. Therefore, the compression needs to be
automated. Searching for the optimal compression method parameters is
considered an optimization problem. This article introduces a Multi-Objective
Hardware-Aware Quantization (MOHAQ) method, which considers both hardware
efficiency and inference error as objectives for mixed-precision quantization.
The proposed method makes the evaluation of candidate solutions in a large
search space feasible by relying on two steps. First, post-training
quantization is applied for fast solution evaluation. Second, we propose a
search technique named "beacon-based search" to retrain selected solutions only
in the search space and use them as beacons to know the effect of retraining on
other solutions. To evaluate the optimization potential, we chose a speech
recognition model using the TIMIT dataset. The model is based on Simple
Recurrent Unit (SRU) due to its considerable speedup over other recurrent
units. We applied our method to run on two platforms: SiLago and Bitfusion.
Experimental evaluations showed that SRU can be compressed up to 8x by
post-training quantization without any significant increase in the error and up
to 12x with only a 1.5 percentage point increase in error. On SiLago, the
inference-only search found solutions that achieve 80\% and 64\% of the maximum
possible speedup and energy saving, respectively, with a 0.5 percentage point
increase in the error. On Bitfusion, with a constraint of a small SRAM size,
beacon-based search reduced the error gain of inference-only search by 4
percentage points and increased the possible reached speedup to be 47x compared
to the Bitfusion baseline.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rezk_N/0/1/0/all/0/1"&gt;Nesma M. Rezk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nordstrom_T/0/1/0/all/0/1"&gt;Tomas Nordstr&amp;#xf6;m&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stathis_D/0/1/0/all/0/1"&gt;Dimitrios Stathis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ul_Abdin_Z/0/1/0/all/0/1"&gt;Zain Ul-Abdin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aksoy_E/0/1/0/all/0/1"&gt;Eren Erdal Aksoy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hemani_A/0/1/0/all/0/1"&gt;Ahmed Hemani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AdvRush: Searching for Adversarially Robust Neural Architectures. (arXiv:2108.01289v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.01289</id>
        <link href="http://arxiv.org/abs/2108.01289"/>
        <updated>2021-08-04T01:59:22.554Z</updated>
        <summary type="html"><![CDATA[Deep neural networks continue to awe the world with their remarkable
performance. Their predictions, however, are prone to be corrupted by
adversarial examples that are imperceptible to humans. Current efforts to
improve the robustness of neural networks against adversarial examples are
focused on developing robust training methods, which update the weights of a
neural network in a more robust direction. In this work, we take a step beyond
training of the weight parameters and consider the problem of designing an
adversarially robust neural architecture with high intrinsic robustness. We
propose AdvRush, a novel adversarial robustness-aware neural architecture
search algorithm, based upon a finding that independent of the training method,
the intrinsic robustness of a neural network can be represented with the
smoothness of its input loss landscape. Through a regularizer that favors a
candidate architecture with a smoother input loss landscape, AdvRush
successfully discovers an adversarially robust neural architecture. Along with
a comprehensive theoretical motivation for AdvRush, we conduct an extensive
amount of experiments to demonstrate the efficacy of AdvRush on various
benchmark datasets. Notably, on CIFAR-10, AdvRush achieves 55.91% robust
accuracy under FGSM attack after standard training and 50.04% robust accuracy
under AutoAttack after 7-step PGD adversarial training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mok_J/0/1/0/all/0/1"&gt;Jisoo Mok&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Na_B/0/1/0/all/0/1"&gt;Byunggook Na&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choe_H/0/1/0/all/0/1"&gt;Hyeokjun Choe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1"&gt;Sungroh Yoon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Variational Actor-Critic Algorithms. (arXiv:2108.01215v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.01215</id>
        <link href="http://arxiv.org/abs/2108.01215"/>
        <updated>2021-08-04T01:59:22.542Z</updated>
        <summary type="html"><![CDATA[We introduce a class of variational actor-critic algorithms based on a
variational formulation over both the value function and the policy. The
objective function of the variational formulation consists of two parts: one
for maximizing the value function and the other for minimizing the Bellman
residual. Besides the vanilla gradient descent with both the value function and
the policy updates, we propose two variants, the clipping method and the
flipping method, in order to speed up the convergence. We also prove that, when
the prefactor of the Bellman residual is sufficiently large, the fixed point of
the algorithm is close to the optimal policy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yuhua Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ying_L/0/1/0/all/0/1"&gt;Lexing Ying&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PyEuroVoc: A Tool for Multilingual Legal Document Classification with EuroVoc Descriptors. (arXiv:2108.01139v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.01139</id>
        <link href="http://arxiv.org/abs/2108.01139"/>
        <updated>2021-08-04T01:59:22.532Z</updated>
        <summary type="html"><![CDATA[EuroVoc is a multilingual thesaurus that was built for organizing the
legislative documentary of the European Union institutions. It contains
thousands of categories at different levels of specificity and its descriptors
are targeted by legal texts in almost thirty languages. In this work we propose
a unified framework for EuroVoc classification on 22 languages by fine-tuning
modern Transformer-based pretrained language models. We study extensively the
performance of our trained models and show that they significantly improve the
results obtained by a similar tool - JEX - on the same dataset. The code and
the fine-tuned models were open sourced, together with a programmatic interface
that eases the process of loading the weights of a trained model and of
classifying a new document.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Avram_A/0/1/0/all/0/1"&gt;Andrei-Marius Avram&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pais_V/0/1/0/all/0/1"&gt;Vasile Pais&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tufis_D/0/1/0/all/0/1"&gt;Dan Tufis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pre-trained Models for Sonar Images. (arXiv:2108.01111v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.01111</id>
        <link href="http://arxiv.org/abs/2108.01111"/>
        <updated>2021-08-04T01:59:22.513Z</updated>
        <summary type="html"><![CDATA[Machine learning and neural networks are now ubiquitous in sonar perception,
but it lags behind the computer vision field due to the lack of data and
pre-trained models specifically for sonar images. In this paper we present the
Marine Debris Turntable dataset and produce pre-trained neural networks trained
on this dataset, meant to fill the gap of missing pre-trained models for sonar
images. We train Resnet 20, MobileNets, DenseNet121, SqueezeNet, MiniXception,
and an Autoencoder, over several input image sizes, from 32 x 32 to 96 x 96, on
the Marine Debris turntable dataset. We evaluate these models using transfer
learning for low-shot classification in the Marine Debris Watertank and another
dataset captured using a Gemini 720i sonar. Our results show that in both
datasets the pre-trained models produce good features that allow good
classification accuracy with low samples (10-30 samples per class). The Gemini
dataset validates that the features transfer to other kinds of sonar sensors.
We expect that the community benefits from the public release of our
pre-trained models and the turntable dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Valdenegro_Toro_M/0/1/0/all/0/1"&gt;Matias Valdenegro-Toro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Preciado_Grijalva_A/0/1/0/all/0/1"&gt;Alan Preciado-Grijalva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wehbe_B/0/1/0/all/0/1"&gt;Bilal Wehbe&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automatic airway segmentation from Computed Tomography using robust and efficient 3-D convolutional neural networks. (arXiv:2103.16328v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.16328</id>
        <link href="http://arxiv.org/abs/2103.16328"/>
        <updated>2021-08-04T01:59:22.507Z</updated>
        <summary type="html"><![CDATA[This paper presents a fully automatic and end-to-end optimised airway
segmentation method for thoracic computed tomography, based on the U-Net
architecture. We use a simple and low-memory 3D U-Net as backbone, which allows
the method to process large 3D image patches, often comprising full lungs, in a
single pass through the network. This makes the method simple, robust and
efficient. We validated the proposed method on three datasets with very
different characteristics and various airway abnormalities: i) a dataset of
pediatric patients including subjects with cystic fibrosis, ii) a subset of the
Danish Lung Cancer Screening Trial, including subjects with chronic obstructive
pulmonary disease, and iii) the EXACT'09 public dataset. We compared our method
with other state-of-the-art airway segmentation methods, including relevant
learning-based methods in the literature evaluated on the EXACT'09 data. We
show that our method can extract highly complete airway trees with few false
positive errors, on scans from both healthy and diseased subjects, and also
that the method generalizes well across different datasets. On the EXACT'09
test set, our method achieved the second highest sensitivity score among all
methods that reported good specificity.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Garcia_Uceda_A/0/1/0/all/0/1"&gt;A. Garcia-Uceda&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Selvan_R/0/1/0/all/0/1"&gt;R. Selvan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Saghir_Z/0/1/0/all/0/1"&gt;Z. Saghir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tiddens_H/0/1/0/all/0/1"&gt;H.A.W.M. Tiddens&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bruijne_M/0/1/0/all/0/1"&gt;M. de Bruijne&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TransPose: Keypoint Localization via Transformer. (arXiv:2012.14214v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.14214</id>
        <link href="http://arxiv.org/abs/2012.14214"/>
        <updated>2021-08-04T01:59:22.493Z</updated>
        <summary type="html"><![CDATA[While CNN-based models have made remarkable progress on human pose
estimation, what spatial dependencies they capture to localize keypoints
remains unclear. In this work, we propose a model called \textbf{TransPose},
which introduces Transformer for human pose estimation. The attention layers
built in Transformer enable our model to capture long-range relationships
efficiently and also can reveal what dependencies the predicted keypoints rely
on. To predict keypoint heatmaps, the last attention layer acts as an
aggregator, which collects contributions from image clues and forms maximum
positions of keypoints. Such a heatmap-based localization approach via
Transformer conforms to the principle of Activation
Maximization~\cite{erhan2009visualizing}. And the revealed dependencies are
image-specific and fine-grained, which also can provide evidence of how the
model handles special cases, e.g., occlusion. The experiments show that
TransPose achieves 75.8 AP and 75.0 AP on COCO validation and test-dev sets,
while being more lightweight and faster than mainstream CNN architectures. The
TransPose model also transfers very well on MPII benchmark, achieving superior
performance on the test set when fine-tuned with small training costs. Code and
pre-trained models are publicly
available\footnote{\url{https://github.com/yangsenius/TransPose}}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1"&gt;Sen Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Quan_Z/0/1/0/all/0/1"&gt;Zhibin Quan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nie_M/0/1/0/all/0/1"&gt;Mu Nie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1"&gt;Wankou Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Auxiliary Tasks and Exploration Enable ObjectNav. (arXiv:2104.04112v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.04112</id>
        <link href="http://arxiv.org/abs/2104.04112"/>
        <updated>2021-08-04T01:59:22.486Z</updated>
        <summary type="html"><![CDATA[ObjectGoal Navigation (ObjectNav) is an embodied task wherein agents are to
navigate to an object instance in an unseen environment. Prior works have shown
that end-to-end ObjectNav agents that use vanilla visual and recurrent modules,
e.g. a CNN+RNN, perform poorly due to overfitting and sample inefficiency. This
has motivated current state-of-the-art methods to mix analytic and learned
components and operate on explicit spatial maps of the environment. We instead
re-enable a generic learned agent by adding auxiliary learning tasks and an
exploration reward. Our agents achieve 24.5% success and 8.1% SPL, a 37% and 8%
relative improvement over prior state-of-the-art, respectively, on the Habitat
ObjectNav Challenge. From our analysis, we propose that agents will act to
simplify their visual inputs so as to smooth their RNN dynamics, and that
auxiliary tasks reduce overfitting by minimizing effective RNN dimensionality;
i.e. a performant ObjectNav agent that must maintain coherent plans over long
horizons does so by learning smooth, low-dimensional recurrent dynamics. Site:
https://joel99.github.io/objectnav/]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1"&gt;Joel Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Batra_D/0/1/0/all/0/1"&gt;Dhruv Batra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Das_A/0/1/0/all/0/1"&gt;Abhishek Das&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wijmans_E/0/1/0/all/0/1"&gt;Erik Wijmans&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SABER: Data-Driven Motion Planner for Autonomously Navigating Heterogeneous Robots. (arXiv:2108.01262v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2108.01262</id>
        <link href="http://arxiv.org/abs/2108.01262"/>
        <updated>2021-08-04T01:59:22.440Z</updated>
        <summary type="html"><![CDATA[We present an end-to-end online motion planning framework that uses a
data-driven approach to navigate a heterogeneous robot team towards a global
goal while avoiding obstacles in uncertain environments. First, we use
stochastic model predictive control (SMPC) to calculate control inputs that
satisfy robot dynamics, and consider uncertainty during obstacle avoidance with
chance constraints. Second, recurrent neural networks are used to provide a
quick estimate of future state uncertainty considered in the SMPC finite-time
horizon solution, which are trained on uncertainty outputs of various
simultaneous localization and mapping algorithms. When two or more robots are
in communication range, these uncertainties are then updated using a
distributed Kalman filtering approach. Lastly, a Deep Q-learning agent is
employed to serve as a high-level path planner, providing the SMPC with target
positions that move the robots towards a desired global goal. Our complete
methods are demonstrated on a ground and aerial robot simultaneously (code
available at: https://github.com/AlexS28/SABER).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schperberg_A/0/1/0/all/0/1"&gt;Alexander Schperberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsuei_S/0/1/0/all/0/1"&gt;Stephanie Tsuei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1"&gt;Stefano Soatto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hong_D/0/1/0/all/0/1"&gt;Dennis Hong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Survey on Assessing the Generalization Envelope of Deep Neural Networks: Predictive Uncertainty, Out-of-distribution and Adversarial Samples. (arXiv:2008.09381v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.09381</id>
        <link href="http://arxiv.org/abs/2008.09381"/>
        <updated>2021-08-04T01:59:22.413Z</updated>
        <summary type="html"><![CDATA[Deep Neural Networks (DNNs) achieve state-of-the-art performance on numerous
applications. However, it is difficult to tell beforehand if a DNN receiving an
input will deliver the correct output since their decision criteria are usually
nontransparent. A DNN delivers the correct output if the input is within the
area enclosed by its generalization envelope. In this case, the information
contained in the input sample is processed reasonably by the network. It is of
large practical importance to assess at inference time if a DNN generalizes
correctly. Currently, the approaches to achieve this goal are investigated in
different problem set-ups rather independently from one another, leading to
three main research and literature fields: predictive uncertainty,
out-of-distribution detection and adversarial example detection. This survey
connects the three fields within the larger framework of investigating the
generalization performance of machine learning methods and in particular DNNs.
We underline the common ground, point at the most promising approaches and give
a structured overview of the methods that provide at inference time means to
establish if the current input is within the generalization envelope of a DNN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lust_J/0/1/0/all/0/1"&gt;Julia Lust&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Condurache_A/0/1/0/all/0/1"&gt;Alexandru Paul Condurache&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Toward Spatially Unbiased Generative Models. (arXiv:2108.01285v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.01285</id>
        <link href="http://arxiv.org/abs/2108.01285"/>
        <updated>2021-08-04T01:59:22.407Z</updated>
        <summary type="html"><![CDATA[Recent image generation models show remarkable generation performance.
However, they mirror strong location preference in datasets, which we call
spatial bias. Therefore, generators render poor samples at unseen locations and
scales. We argue that the generators rely on their implicit positional encoding
to render spatial content. From our observations, the generator's implicit
positional encoding is translation-variant, making the generator spatially
biased. To address this issue, we propose injecting explicit positional
encoding at each scale of the generator. By learning the spatially unbiased
generator, we facilitate the robust use of generators in multiple tasks, such
as GAN inversion, multi-scale generation, generation of arbitrary sizes and
aspect ratios. Furthermore, we show that our method can also be applied to
denoising diffusion probabilistic models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1"&gt;Jooyoung Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jungbeom Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jeong_Y/0/1/0/all/0/1"&gt;Yonghyun Jeong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1"&gt;Sungroh Yoon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[OVERT: An Algorithm for Safety Verification of Neural Network Control Policies for Nonlinear Systems. (arXiv:2108.01220v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.01220</id>
        <link href="http://arxiv.org/abs/2108.01220"/>
        <updated>2021-08-04T01:59:22.401Z</updated>
        <summary type="html"><![CDATA[Deep learning methods can be used to produce control policies, but certifying
their safety is challenging. The resulting networks are nonlinear and often
very large. In response to this challenge, we present OVERT: a sound algorithm
for safety verification of nonlinear discrete-time closed loop dynamical
systems with neural network control policies. The novelty of OVERT lies in
combining ideas from the classical formal methods literature with ideas from
the newer neural network verification literature. The central concept of OVERT
is to abstract nonlinear functions with a set of optimally tight piecewise
linear bounds. Such piecewise linear bounds are designed for seamless
integration into ReLU neural network verification tools. OVERT can be used to
prove bounded-time safety properties by either computing reachable sets or
solving feasibility queries directly. We demonstrate various examples of safety
verification for several classical benchmark examples. OVERT compares favorably
to existing methods both in computation time and in tightness of the reachable
set.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sidrane_C/0/1/0/all/0/1"&gt;Chelsea Sidrane&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maleki_A/0/1/0/all/0/1"&gt;Amir Maleki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Irfan_A/0/1/0/all/0/1"&gt;Ahmed Irfan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kochenderfer_M/0/1/0/all/0/1"&gt;Mykel J. Kochenderfer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Elastic Architecture Search for Diverse Tasks with Different Resources. (arXiv:2108.01224v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.01224</id>
        <link href="http://arxiv.org/abs/2108.01224"/>
        <updated>2021-08-04T01:59:22.395Z</updated>
        <summary type="html"><![CDATA[We study a new challenging problem of efficient deployment for diverse tasks
with different resources, where the resource constraint and task of interest
corresponding to a group of classes are dynamically specified at testing time.
Previous NAS approaches seek to design architectures for all classes
simultaneously, which may not be optimal for some individual tasks. A
straightforward solution is to search an architecture from scratch for each
deployment scenario, which however is computation-intensive and impractical. To
address this, we present a novel and general framework, called Elastic
Architecture Search (EAS), permitting instant specializations at runtime for
diverse tasks with various resource constraints. To this end, we first propose
to effectively train the over-parameterized network via a task dropout strategy
to disentangle the tasks during training. In this way, the resulting model is
robust to the subsequent task dropping at inference time. Based on the
well-trained over-parameterized network, we then propose an efficient
architecture generator to obtain optimal architectures within a single forward
pass. Experiments on two image classification datasets show that EAS is able to
find more compact networks with better performance while remarkably being
orders of magnitude faster than state-of-the-art NAS methods. For example, our
proposed EAS finds compact architectures within 0.1 second for 50 deployment
scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jing Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhuang_B/0/1/0/all/0/1"&gt;Bohan Zhuang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1"&gt;Mingkui Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xu Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Phung_D/0/1/0/all/0/1"&gt;Dinh Phung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yuanqing Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1"&gt;Jianfei Cai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generalized Source-free Domain Adaptation. (arXiv:2108.01614v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.01614</id>
        <link href="http://arxiv.org/abs/2108.01614"/>
        <updated>2021-08-04T01:59:22.388Z</updated>
        <summary type="html"><![CDATA[Domain adaptation (DA) aims to transfer the knowledge learned from a source
domain to an unlabeled target domain. Some recent works tackle source-free
domain adaptation (SFDA) where only a source pre-trained model is available for
adaptation to the target domain. However, those methods do not consider keeping
source performance which is of high practical value in real world applications.
In this paper, we propose a new domain adaptation paradigm called Generalized
Source-free Domain Adaptation (G-SFDA), where the learned model needs to
perform well on both the target and source domains, with only access to current
unlabeled target data during adaptation. First, we propose local structure
clustering (LSC), aiming to cluster the target features with its semantically
similar neighbors, which successfully adapts the model to the target domain in
the absence of source data. Second, we propose sparse domain attention (SDA),
it produces a binary domain specific attention to activate different feature
channels for different domains, meanwhile the domain attention will be utilized
to regularize the gradient during adaptation to keep source information. In the
experiments, for target performance our method is on par with or better than
existing DA and SFDA methods, specifically it achieves state-of-the-art
performance (85.4%) on VisDA, and our method works well for all domains after
adapting to single or multiple target domains. Code is available in
https://github.com/Albert0147/G-SFDA.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1"&gt;Shiqi Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yaxing Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weijer_J/0/1/0/all/0/1"&gt;Joost van de Weijer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Herranz_L/0/1/0/all/0/1"&gt;Luis Herranz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jui_S/0/1/0/all/0/1"&gt;Shangling Jui&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Waveform Selection for Radar Tracking in Target Channels With Memory via Universal Learning. (arXiv:2108.01181v1 [cs.IT])]]></title>
        <id>http://arxiv.org/abs/2108.01181</id>
        <link href="http://arxiv.org/abs/2108.01181"/>
        <updated>2021-08-04T01:59:22.371Z</updated>
        <summary type="html"><![CDATA[In tracking radar, the sensing environment often varies significantly over a
track duration due to the target's trajectory and dynamic interference.
Adapting the radar's waveform using partial information about the state of the
scene has been shown to provide performance benefits in many practical
scenarios. Moreover, radar measurements generally exhibit strong temporal
correlation, allowing memory-based learning algorithms to effectively learn
waveform selection strategies. This work examines a radar system which builds a
compressed model of the radar-environment interface in the form of a
context-tree. The radar uses this context tree-based model to select waveforms
in a signal-dependent target channel, which may respond adversarially to the
radar's strategy. This approach is guaranteed to asymptotically converge to the
average-cost optimal policy for any stationary target channel that can be
represented as a Markov process of order U < $\infty$, where the constant U is
unknown to the radar. The proposed approach is tested in a simulation study,
and is shown to provide tracking performance improvements over two
state-of-the-art waveform selection schemes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Thornton_C/0/1/0/all/0/1"&gt;Charles E. Thornton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Buehrer_R/0/1/0/all/0/1"&gt;R. Michael Buehrer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martone_A/0/1/0/all/0/1"&gt;Anthony F. Martone&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Representation learning for neural population activity with Neural Data Transformers. (arXiv:2108.01210v1 [q-bio.NC])]]></title>
        <id>http://arxiv.org/abs/2108.01210</id>
        <link href="http://arxiv.org/abs/2108.01210"/>
        <updated>2021-08-04T01:59:22.365Z</updated>
        <summary type="html"><![CDATA[Neural population activity is theorized to reflect an underlying dynamical
structure. This structure can be accurately captured using state space models
with explicit dynamics, such as those based on recurrent neural networks
(RNNs). However, using recurrence to explicitly model dynamics necessitates
sequential processing of data, slowing real-time applications such as
brain-computer interfaces. Here we introduce the Neural Data Transformer (NDT),
a non-recurrent alternative. We test the NDT's ability to capture autonomous
dynamical systems by applying it to synthetic datasets with known dynamics and
data from monkey motor cortex during a reaching task well-modeled by RNNs. The
NDT models these datasets as well as state-of-the-art recurrent models.
Further, its non-recurrence enables 3.9ms inference, well within the loop time
of real-time applications and more than 6 times faster than recurrent baselines
on the monkey reaching dataset. These results suggest that an explicit dynamics
model is not necessary to model autonomous neural population dynamics. Code:
https://github.com/snel-repo/neural-data-transformers]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Ye_J/0/1/0/all/0/1"&gt;Joel Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Pandarinath_C/0/1/0/all/0/1"&gt;Chethan Pandarinath&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Computing the Newton-step faster than Hessian accumulation. (arXiv:2108.01219v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2108.01219</id>
        <link href="http://arxiv.org/abs/2108.01219"/>
        <updated>2021-08-04T01:59:22.359Z</updated>
        <summary type="html"><![CDATA[Computing the Newton-step of a generic function with $N$ decision variables
takes $O(N^3)$ flops. In this paper, we show that given the computational graph
of the function, this bound can be reduced to $O(m\tau^3)$, where $\tau, m$ are
the width and size of a tree-decomposition of the graph. The proposed algorithm
generalizes nonlinear optimal-control methods based on LQR to general
optimization problems and provides non-trivial gains in iteration-complexity
even in cases where the Hessian is dense.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Srinivasan_A/0/1/0/all/0/1"&gt;Akshay Srinivasan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Todorov_E/0/1/0/all/0/1"&gt;Emanuel Todorov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hybrid Quantum-Classical Neural Network for Incident Detection. (arXiv:2108.01127v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.01127</id>
        <link href="http://arxiv.org/abs/2108.01127"/>
        <updated>2021-08-04T01:59:22.353Z</updated>
        <summary type="html"><![CDATA[The efficiency and reliability of real-time incident detection models
directly impact the affected corridors' traffic safety and operational
conditions. The recent emergence of cloud-based quantum computing
infrastructure and innovations in noisy intermediate-scale quantum devices have
revealed a new era of quantum-enhanced algorithms that can be leveraged to
improve real-time incident detection accuracy. In this research, a hybrid
machine learning model, which includes classical and quantum machine learning
(ML) models, is developed to identify incidents using the connected vehicle
(CV) data. The incident detection performance of the hybrid model is evaluated
against baseline classical ML models. The framework is evaluated using data
from a microsimulation tool for different incident scenarios. The results
indicate that a hybrid neural network containing a 4-qubit quantum layer
outperforms all other baseline models when there is a lack of training data. We
have created three datasets; DS-1 with sufficient training data, and DS-2 and
DS-3 with insufficient training data. The hybrid model achieves a recall of
98.9%, 98.3%, and 96.6% for DS-1, DS-2, and DS-3, respectively. For DS-2 and
DS-3, the average improvement in F2-score (measures model's performance to
correctly identify incidents) achieved by the hybrid model is 1.9% and 7.8%,
respectively, compared to the classical models. It shows that with insufficient
data, which may be common for CVs, the hybrid ML model will perform better than
the classical models. With the continuing improvements of quantum computing
infrastructure, the quantum ML models could be a promising alternative for
CV-related applications when the available data is insufficient.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khan_Z/0/1/0/all/0/1"&gt;Zadid Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1"&gt;Sakib Mahmud Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tine_J/0/1/0/all/0/1"&gt;Jean Michel Tine&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Comert_A/0/1/0/all/0/1"&gt;Ayse Turhan Comert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rice_D/0/1/0/all/0/1"&gt;Diamon Rice&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Comert_G/0/1/0/all/0/1"&gt;Gurcan Comert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Michalaka_D/0/1/0/all/0/1"&gt;Dimitra Michalaka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mwakalonge_J/0/1/0/all/0/1"&gt;Judith Mwakalonge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Majumdar_R/0/1/0/all/0/1"&gt;Reek Majumdar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chowdhury_M/0/1/0/all/0/1"&gt;Mashrur Chowdhury&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Artificial Neural Network Pruning to Extract Knowledge. (arXiv:2005.06284v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.06284</id>
        <link href="http://arxiv.org/abs/2005.06284"/>
        <updated>2021-08-04T01:59:22.341Z</updated>
        <summary type="html"><![CDATA[Artificial Neural Networks (NN) are widely used for solving complex problems
from medical diagnostics to face recognition. Despite notable successes, the
main disadvantages of NN are also well known: the risk of overfitting, lack of
explainability (inability to extract algorithms from trained NN), and high
consumption of computing resources. Determining the appropriate specific NN
structure for each problem can help overcome these difficulties: Too poor NN
cannot be successfully trained, but too rich NN gives unexplainable results and
may have a high chance of overfitting. Reducing precision of NN parameters
simplifies the implementation of these NN, saves computing resources, and makes
the NN skills more transparent. This paper lists the basic NN simplification
problems and controlled pruning procedures to solve these problems. All the
described pruning procedures can be implemented in one framework. The developed
procedures, in particular, find the optimal structure of NN for each task,
measure the influence of each input signal and NN parameter, and provide a
detailed verbal description of the algorithms and skills of NN. The described
methods are illustrated by a simple example: the generation of explicit
algorithms for predicting the results of the US presidential election.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mirkes_E/0/1/0/all/0/1"&gt;Evgeny M Mirkes&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Correcting Arabic Soft Spelling Mistakes using BiLSTM-based Machine Learning. (arXiv:2108.01141v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.01141</id>
        <link href="http://arxiv.org/abs/2108.01141"/>
        <updated>2021-08-04T01:59:22.303Z</updated>
        <summary type="html"><![CDATA[Soft spelling errors are a class of spelling mistakes that is widespread
among native Arabic speakers and foreign learners alike. Some of these errors
are typographical in nature. They occur due to orthographic variations of some
Arabic letters and the complex rules that dictate their correct usage. Many
people forgo these rules, and given the identical phonetic sounds, they often
confuse such letters. In this paper, we propose a bidirectional long short-term
memory network that corrects this class of errors. We develop, train, evaluate,
and compare a set of BiLSTM networks. We approach the spelling correction
problem at the character level. We handle Arabic texts from both classical and
modern standard Arabic. We treat the problem as a one-to-one sequence
transcription problem. Since the soft Arabic errors class encompasses omission
and addition mistakes, to preserve the one-to-one sequence transcription, we
propose a simple low-resource yet effective technique that maintains the
one-to-one sequencing and avoids using a costly encoder-decoder architecture.
We train the BiLSTM models to correct the spelling mistakes using transformed
input and stochastic error injection approaches. We recommend a configuration
that has two BiLSTM layers, uses the dropout regularization, and is trained
using the latter training approach with error injection rate of 40%. The best
model corrects 96.4% of the injected errors and achieves a low character error
rate of 1.28% on a real test set of soft spelling mistakes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Abandah_G/0/1/0/all/0/1"&gt;Gheith A. Abandah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suyyagh_A/0/1/0/all/0/1"&gt;Ashraf Suyyagh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khedher_M/0/1/0/all/0/1"&gt;Mohammed Z. Khedher&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficacy of Statistical and Artificial Intelligence-based False Information Cyberattack Detection Models for Connected Vehicles. (arXiv:2108.01124v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2108.01124</id>
        <link href="http://arxiv.org/abs/2108.01124"/>
        <updated>2021-08-04T01:59:22.297Z</updated>
        <summary type="html"><![CDATA[Connected vehicles (CVs), because of the external connectivity with other CVs
and connected infrastructure, are vulnerable to cyberattacks that can instantly
compromise the safety of the vehicle itself and other connected vehicles and
roadway infrastructure. One such cyberattack is the false information attack,
where an external attacker injects inaccurate information into the connected
vehicles and eventually can cause catastrophic consequences by compromising
safety-critical applications like the forward collision warning. The occurrence
and target of such attack events can be very dynamic, making real-time and
near-real-time detection challenging. Change point models, can be used for
real-time anomaly detection caused by the false information attack. In this
paper, we have evaluated three change point-based statistical models;
Expectation Maximization, Cumulative Summation, and Bayesian Online Change
Point Algorithms for cyberattack detection in the CV data. Also, data-driven
artificial intelligence (AI) models, which can be used to detect known and
unknown underlying patterns in the dataset, have the potential of detecting a
real-time anomaly in the CV data. We have used six AI models to detect false
information attacks and compared the performance for detecting the attacks with
our developed change point models. Our study shows that change points models
performed better in real-time false information attack detection compared to
the performance of the AI models. Change point models having the advantage of
no training requirements can be a feasible and computationally efficient
alternative to AI models for false information attack detection in connected
vehicles.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1"&gt;Sakib Mahmud Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Comert_G/0/1/0/all/0/1"&gt;Gurcan Comert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chowdhury_M/0/1/0/all/0/1"&gt;Mashrur Chowdhury&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pure Exploration in Multi-armed Bandits with Graph Side Information. (arXiv:2108.01152v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.01152</id>
        <link href="http://arxiv.org/abs/2108.01152"/>
        <updated>2021-08-04T01:59:22.277Z</updated>
        <summary type="html"><![CDATA[We study pure exploration in multi-armed bandits with graph side-information.
In particular, we consider the best arm (and near-best arm) identification
problem in the fixed confidence setting under the assumption that the arm
rewards are smooth with respect to a given arbitrary graph. This captures a
range of real world pure-exploration scenarios where one often has information
about the similarity of the options or actions under consideration. We propose
a novel algorithm GRUB (GRaph based UcB) for this problem and provide a
theoretical characterization of its performance that elicits the benefit of the
graph-side information. We complement our theory with experimental results that
show that capitalizing on available graph side information yields significant
improvements over pure exploration methods that are unable to use this
information.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Thaker_P/0/1/0/all/0/1"&gt;Parth K.Thaker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rao_N/0/1/0/all/0/1"&gt;Nikhil Rao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Malu_M/0/1/0/all/0/1"&gt;Mohit Malu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dasarathy_G/0/1/0/all/0/1"&gt;Gautam Dasarathy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning-based Preference Prediction for Constrained Multi-Criteria Path-Planning. (arXiv:2108.01080v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2108.01080</id>
        <link href="http://arxiv.org/abs/2108.01080"/>
        <updated>2021-08-04T01:59:22.270Z</updated>
        <summary type="html"><![CDATA[Learning-based methods are increasingly popular for search algorithms in
single-criterion optimization problems. In contrast, for multiple-criteria
optimization there are significantly fewer approaches despite the existence of
numerous applications. Constrained path-planning for Autonomous Ground Vehicles
(AGV) is one such application, where an AGV is typically deployed in disaster
relief or search and rescue applications in off-road environments. The agent
can be faced with the following dilemma : optimize a source-destination path
according to a known criterion and an uncertain criterion under operational
constraints. The known criterion is associated to the cost of the path,
representing the distance. The uncertain criterion represents the feasibility
of driving through the path without requiring human intervention. It depends on
various external parameters such as the physics of the vehicle, the state of
the explored terrains or weather conditions. In this work, we leverage
knowledge acquired through offline simulations by training a neural network
model to predict the uncertain criterion. We integrate this model inside a
path-planner which can solve problems online. Finally, we conduct experiments
on realistic AGV scenarios which illustrate that the proposed framework
requires human intervention less frequently, trading for a limited increase in
the path distance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Osanlou_K/0/1/0/all/0/1"&gt;Kevin Osanlou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guettier_C/0/1/0/all/0/1"&gt;Christophe Guettier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bursuc_A/0/1/0/all/0/1"&gt;Andrei Bursuc&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cazenave_T/0/1/0/all/0/1"&gt;Tristan Cazenave&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jacopin_E/0/1/0/all/0/1"&gt;Eric Jacopin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The RareDis corpus: a corpus annotated with rare diseases, their signs and symptoms. (arXiv:2108.01204v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.01204</id>
        <link href="http://arxiv.org/abs/2108.01204"/>
        <updated>2021-08-04T01:59:22.264Z</updated>
        <summary type="html"><![CDATA[The RareDis corpus contains more than 5,000 rare diseases and almost 6,000
clinical manifestations are annotated. Moreover, the Inter Annotator Agreement
evaluation shows a relatively high agreement (F1-measure equal to 83.5% under
exact match criteria for the entities and equal to 81.3% for the relations).
Based on these results, this corpus is of high quality, supposing a significant
step for the field since there is a scarcity of available corpus annotated with
rare diseases. This could open the door to further NLP applications, which
would facilitate the diagnosis and treatment of these rare diseases and,
therefore, would improve dramatically the quality of life of these patients.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Martinez_deMiguel_C/0/1/0/all/0/1"&gt;Claudia Mart&amp;#xed;nez-deMiguel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Segura_Bedmar_I/0/1/0/all/0/1"&gt;Isabel Segura-Bedmar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chacon_Solano_E/0/1/0/all/0/1"&gt;Esteban Chac&amp;#xf3;n-Solano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guerrero_Aspizua_S/0/1/0/all/0/1"&gt;Sara Guerrero-Aspizua&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hybrid Classical-Quantum Deep Learning Models for Autonomous Vehicle Traffic Image Classification Under Adversarial Attack. (arXiv:2108.01125v1 [quant-ph])]]></title>
        <id>http://arxiv.org/abs/2108.01125</id>
        <link href="http://arxiv.org/abs/2108.01125"/>
        <updated>2021-08-04T01:59:22.257Z</updated>
        <summary type="html"><![CDATA[Image classification must work for autonomous vehicles (AV) operating on
public roads, and actions performed based on image misclassification can have
serious consequences. Traffic sign images can be misclassified by an
adversarial attack on machine learning models used by AVs for traffic sign
recognition. To make classification models resilient against adversarial
attacks, we used a hybrid deep-learning model with both the quantum and
classical layers. Our goal is to study the hybrid deep-learning architecture
for classical-quantum transfer learning models to support the current era of
intermediate-scale quantum technology. We have evaluated the impacts of various
white box adversarial attacks on these hybrid models. The classical part of
hybrid models includes a convolution network from the pre-trained Resnet18
model, which extracts informative features from a high dimensional LISA traffic
sign image dataset. The output from the classical processor is processed
further through the quantum layer, which is composed of various quantum gates
and provides support to various quantum mechanical features like entanglement
and superposition. We have tested multiple combinations of quantum circuits to
provide better classification accuracy with decreasing training data and found
better resiliency for our hybrid classical-quantum deep learning model during
attacks compared to the classical-only machine learning models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Majumder_R/0/1/0/all/0/1"&gt;Reek Majumder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Khan_S/0/1/0/all/0/1"&gt;Sakib Mahmud Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Ahmed_F/0/1/0/all/0/1"&gt;Fahim Ahmed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Khan_Z/0/1/0/all/0/1"&gt;Zadid Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Ngeni_F/0/1/0/all/0/1"&gt;Frank Ngeni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Comert_G/0/1/0/all/0/1"&gt;Gurcan Comert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Mwakalonge_J/0/1/0/all/0/1"&gt;Judith Mwakalonge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Michalaka_D/0/1/0/all/0/1"&gt;Dimitra Michalaka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Chowdhury_M/0/1/0/all/0/1"&gt;Mashrur Chowdhury&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Nearest Neighborhood-Based Deep Clustering for Source Data-absent Unsupervised Domain Adaptation. (arXiv:2107.12585v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.12585</id>
        <link href="http://arxiv.org/abs/2107.12585"/>
        <updated>2021-08-04T01:59:22.238Z</updated>
        <summary type="html"><![CDATA[In the classic setting of unsupervised domain adaptation (UDA), the labeled
source data are available in the training phase. However, in many real-world
scenarios, owing to some reasons such as privacy protection and information
security, the source data is inaccessible, and only a model trained on the
source domain is available. This paper proposes a novel deep clustering method
for this challenging task. Aiming at the dynamical clustering at feature-level,
we introduce extra constraints hidden in the geometric structure between data
to assist the process. Concretely, we propose a geometry-based constraint,
named semantic consistency on the nearest neighborhood (SCNNH), and use it to
encourage robust clustering. To reach this goal, we construct the nearest
neighborhood for every target data and take it as the fundamental clustering
unit by building our objective on the geometry. Also, we develop a more
SCNNH-compliant structure with an additional semantic credibility constraint,
named semantic hyper-nearest neighborhood (SHNNH). After that, we extend our
method to this new geometry. Extensive experiments on three challenging UDA
datasets indicate that our method achieves state-of-the-art results. The
proposed method has significant improvement on all datasets (as we adopt SHNNH,
the average accuracy increases by over 3.0% on the large-scaled dataset). Code
is available at https://github.com/tntek/N2DCX.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1"&gt;Song Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1"&gt;Zhiyuan Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hendrich_N/0/1/0/all/0/1"&gt;Norman Hendrich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_F/0/1/0/all/0/1"&gt;Fanyu Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ge_S/0/1/0/all/0/1"&gt;Shuzhi Sam Ge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Changshui Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jianwei Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Shift-Robust GNNs: Overcoming the Limitations of Localized Graph Training data. (arXiv:2108.01099v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.01099</id>
        <link href="http://arxiv.org/abs/2108.01099"/>
        <updated>2021-08-04T01:59:22.206Z</updated>
        <summary type="html"><![CDATA[There has been a recent surge of interest in designing Graph Neural Networks
(GNNs) for semi-supervised learning tasks. Unfortunately this work has assumed
that the nodes labeled for use in training were selected uniformly at random
(i.e. are an IID sample). However in many real world scenarios gathering labels
for graph nodes is both expensive and inherently biased -- so this assumption
can not be met. GNNs can suffer poor generalization when this occurs, by
overfitting to superfluous regularities present in the training data. In this
work we present a method, Shift-Robust GNN (SR-GNN), designed to account for
distributional differences between biased training data and the graph's true
inference distribution. SR-GNN adapts GNN models for the presence of
distributional shifts between the nodes which have had labels provided for
training and the rest of the dataset. We illustrate the effectiveness of SR-GNN
in a variety of experiments with biased training datasets on common GNN
benchmark datasets for semi-supervised learning, where we see that SR-GNN
outperforms other GNN baselines by accuracy, eliminating at least (~40%) of the
negative effects introduced by biased training data. On the largest dataset we
consider, ogb-arxiv, we observe an 2% absolute improvement over the baseline
and reduce 30% of the negative effects.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1"&gt;Qi Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ponomareva_N/0/1/0/all/0/1"&gt;Natalia Ponomareva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1"&gt;Jiawei Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Perozzi_B/0/1/0/all/0/1"&gt;Bryan Perozzi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Greedy Gradient Ensemble for Robust Visual Question Answering. (arXiv:2107.12651v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.12651</id>
        <link href="http://arxiv.org/abs/2107.12651"/>
        <updated>2021-08-04T01:59:22.200Z</updated>
        <summary type="html"><![CDATA[Language bias is a critical issue in Visual Question Answering (VQA), where
models often exploit dataset biases for the final decision without considering
the image information. As a result, they suffer from performance drop on
out-of-distribution data and inadequate visual explanation. Based on
experimental analysis for existing robust VQA methods, we stress the language
bias in VQA that comes from two aspects, i.e., distribution bias and shortcut
bias. We further propose a new de-bias framework, Greedy Gradient Ensemble
(GGE), which combines multiple biased models for unbiased base model learning.
With the greedy strategy, GGE forces the biased models to over-fit the biased
data distribution in priority, thus makes the base model pay more attention to
examples that are hard to solve by biased models. The experiments demonstrate
that our method makes better use of visual information and achieves
state-of-the-art performance on diagnosing dataset VQA-CP without using extra
annotations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1"&gt;Xinzhe Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shuhui Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_C/0/1/0/all/0/1"&gt;Chi Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1"&gt;Qingming Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1"&gt;Qi Tian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Batch Normalization Preconditioning for Neural Network Training. (arXiv:2108.01110v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.01110</id>
        <link href="http://arxiv.org/abs/2108.01110"/>
        <updated>2021-08-04T01:59:22.179Z</updated>
        <summary type="html"><![CDATA[Batch normalization (BN) is a popular and ubiquitous method in deep learning
that has been shown to decrease training time and improve generalization
performance of neural networks. Despite its success, BN is not theoretically
well understood. It is not suitable for use with very small mini-batch sizes or
online learning. In this paper, we propose a new method called Batch
Normalization Preconditioning (BNP). Instead of applying normalization
explicitly through a batch normalization layer as is done in BN, BNP applies
normalization by conditioning the parameter gradients directly during training.
This is designed to improve the Hessian matrix of the loss function and hence
convergence during training. One benefit is that BNP is not constrained on the
mini-batch size and works in the online learning setting. Furthermore, its
connection to BN provides theoretical insights on how BN improves training and
how BN is applied to special architectures such as convolutional neural
networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lange_S/0/1/0/all/0/1"&gt;Susanna Lange&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Helfrich_K/0/1/0/all/0/1"&gt;Kyle Helfrich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1"&gt;Qiang Ye&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Comparing Machine Learning based Segmentation Models on Jet Fire Radiation Zones. (arXiv:2107.03461v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.03461</id>
        <link href="http://arxiv.org/abs/2107.03461"/>
        <updated>2021-08-04T01:59:22.163Z</updated>
        <summary type="html"><![CDATA[Risk assessment is relevant in any workplace, however there is a degree of
unpredictability when dealing with flammable or hazardous materials so that
detection of fire accidents by itself may not be enough. An example of this is
the impingement of jet fires, where the heat fluxes of the flame could reach
nearby equipment and dramatically increase the probability of a domino effect
with catastrophic results. Because of this, the characterization of such fire
accidents is important from a risk management point of view. One such
characterization would be the segmentation of different radiation zones within
the flame, so this paper presents an exploratory research regarding several
traditional computer vision and Deep Learning segmentation approaches to solve
this specific problem. A data set of propane jet fires is used to train and
evaluate the different approaches and given the difference in the distribution
of the zones and background of the images, different loss functions, that seek
to alleviate data imbalance, are also explored. Additionally, different metrics
are correlated to a manual ranking performed by experts to make an evaluation
that closely resembles the expert's criteria. The Hausdorff Distance and
Adjusted Random Index were the metrics with the highest correlation and the
best results were obtained from the UNet architecture with a Weighted
Cross-Entropy Loss. These results can be used in future research to extract
more geometric information from the segmentation masks or could even be
implemented on other types of fire accidents.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Perez_Guerrero_C/0/1/0/all/0/1"&gt;Carmina P&amp;#xe9;rez-Guerrero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Palacios_A/0/1/0/all/0/1"&gt;Adriana Palacios&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ochoa_Ruiz_G/0/1/0/all/0/1"&gt;Gilberto Ochoa-Ruiz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mata_C/0/1/0/all/0/1"&gt;Christian Mata&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gonzalez_Mendoza_M/0/1/0/all/0/1"&gt;Miguel Gonzalez-Mendoza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Falcon_Morales_L/0/1/0/all/0/1"&gt;Luis Eduardo Falc&amp;#xf3;n-Morales&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generating Master Faces for Dictionary Attacks with a Network-Assisted Latent Space Evolution. (arXiv:2108.01077v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2108.01077</id>
        <link href="http://arxiv.org/abs/2108.01077"/>
        <updated>2021-08-04T01:59:22.140Z</updated>
        <summary type="html"><![CDATA[A master face is a face image that passes face-based identity-authentication
for a large portion of the population. These faces can be used to impersonate,
with a high probability of success, any user, without having access to any user
information. We optimize these faces, by using an evolutionary algorithm in the
latent embedding space of the StyleGAN face generator. Multiple evolutionary
strategies are compared, and we propose a novel approach that employs a neural
network in order to direct the search in the direction of promising samples,
without adding fitness evaluations. The results we present demonstrate that it
is possible to obtain a high coverage of the population (over 40%) with less
than 10 master faces, for three leading deep face recognition systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shmelkin_R/0/1/0/all/0/1"&gt;Ron Shmelkin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Friedlander_T/0/1/0/all/0/1"&gt;Tomer Friedlander&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wolf_L/0/1/0/all/0/1"&gt;Lior Wolf&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring Deep Learning Methods for Real-Time Surgical Instrument Segmentation in Laparoscopy. (arXiv:2107.02319v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.02319</id>
        <link href="http://arxiv.org/abs/2107.02319"/>
        <updated>2021-08-04T01:59:22.134Z</updated>
        <summary type="html"><![CDATA[Minimally invasive surgery is a surgical intervention used to examine the
organs inside the abdomen and has been widely used due to its effectiveness
over open surgery. Due to the hardware improvements such as high definition
cameras, this procedure has significantly improved and new software methods
have demonstrated potential for computer-assisted procedures. However, there
exists challenges and requirements to improve detection and tracking of the
position of the instruments during these surgical procedures. To this end, we
evaluate and compare some popular deep learning methods that can be explored
for the automated segmentation of surgical instruments in laparoscopy, an
important step towards tool tracking. Our experimental results exhibit that the
Dual decoder attention network (DDANet) produces a superior result compared to
other recent deep learning methods. DDANet yields a Dice coefficient of 0.8739
and mean intersection-over-union of 0.8183 for the Robust Medical Instrument
Segmentation (ROBUST-MIS) Challenge 2019 dataset, at a real-time speed of
101.36 frames-per-second that is critical for such procedures.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Jha_D/0/1/0/all/0/1"&gt;Debesh Jha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ali_S/0/1/0/all/0/1"&gt;Sharib Ali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tomar_N/0/1/0/all/0/1"&gt;Nikhil Kumar Tomar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Riegler_M/0/1/0/all/0/1"&gt;Michael A. Riegler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Johansen_D/0/1/0/all/0/1"&gt;Dag Johansen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Johansen_H/0/1/0/all/0/1"&gt;H&amp;#xe5;vard D. Johansen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Halvorsen_P/0/1/0/all/0/1"&gt;P&amp;#xe5;l Halvorsen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Metodos de Agrupamentos em dois Estagios. (arXiv:2108.01123v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.01123</id>
        <link href="http://arxiv.org/abs/2108.01123"/>
        <updated>2021-08-04T01:59:22.124Z</updated>
        <summary type="html"><![CDATA[This work investigates the use of two-stage clustering methods. Four
techniques were proposed: SOMK, SOMAK, ASCAK and SOINAK. SOMK is composed of a
SOM (Self-Organizing Maps) followed by the K-means algorithm, SOMAK is a
combination of SOM followed by the Ant K-means (AK) algorithm, ASCAK is
composed by the ASCA (Ant System-based Clustering Algorithm) and AK algorithms,
SOINAK is composed by the Self-Organizing Incremental Neural Network (SOINN)
and AK. SOINAK presented a better performance among the four proposed
techniques when applied to pattern recognition problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Souza_J/0/1/0/all/0/1"&gt;Jefferson Souza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ludermir_T/0/1/0/all/0/1"&gt;Teresa Ludermir&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Zero-Shot Domain Adaptation in CT Segmentation by Filtered Back Projection Augmentation. (arXiv:2107.08543v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.08543</id>
        <link href="http://arxiv.org/abs/2107.08543"/>
        <updated>2021-08-04T01:59:22.111Z</updated>
        <summary type="html"><![CDATA[Domain shift is one of the most salient challenges in medical computer
vision. Due to immense variability in scanners' parameters and imaging
protocols, even images obtained from the same person and the same scanner could
differ significantly. We address variability in computed tomography (CT) images
caused by different convolution kernels used in the reconstruction process, the
critical domain shift factor in CT. The choice of a convolution kernel affects
pixels' granularity, image smoothness, and noise level. We analyze a dataset of
paired CT images, where smooth and sharp images were reconstructed from the
same sinograms with different kernels, thus providing identical anatomy but
different style. Though identical predictions are desired, we show that the
consistency, measured as the average Dice between predictions on pairs, is just
0.54. We propose Filtered Back-Projection Augmentation (FBPAug), a simple and
surprisingly efficient approach to augment CT images in sinogram space
emulating reconstruction with different kernels. We apply the proposed method
in a zero-shot domain adaptation setup and show that the consistency boosts
from 0.54 to 0.92 outperforming other augmentation approaches. Neither specific
preparation of source domain data nor target domain data is required, so our
publicly released FBPAug can be used as a plug-and-play module for zero-shot
domain adaptation in any CT-based task.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Saparov_T/0/1/0/all/0/1"&gt;Talgat Saparov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kurmukov_A/0/1/0/all/0/1"&gt;Anvar Kurmukov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Shirokih_B/0/1/0/all/0/1"&gt;Boris Shirokih&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Belyaev_M/0/1/0/all/0/1"&gt;Mikhail Belyaev&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multilevel Knowledge Transfer for Cross-Domain Object Detection. (arXiv:2108.00977v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2108.00977</id>
        <link href="http://arxiv.org/abs/2108.00977"/>
        <updated>2021-08-04T01:59:22.088Z</updated>
        <summary type="html"><![CDATA[Domain shift is a well known problem where a model trained on a particular
domain (source) does not perform well when exposed to samples from a different
domain (target). Unsupervised methods that can adapt to domain shift are highly
desirable as they allow effective utilization of the source data without
requiring additional annotated training data from the target. Practically,
obtaining sufficient amount of annotated data from the target domain can be
both infeasible and extremely expensive. In this work, we address the domain
shift problem for the object detection task. Our approach relies on gradually
removing the domain shift between the source and the target domains. The key
ingredients to our approach are -- (a) mapping the source to the target domain
on pixel-level; (b) training a teacher network on the mapped source and the
unannotated target domain using adversarial feature alignment; and (c) finally
training a student network using the pseudo-labels obtained from the teacher.
Experimentally, when tested on challenging scenarios involving domain shift, we
consistently obtain significantly large performance gains over various recent
state of the art approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Csaba_B/0/1/0/all/0/1"&gt;Botos Csaba&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1"&gt;Xiaojuan Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chaudhry_A/0/1/0/all/0/1"&gt;Arslan Chaudhry&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dokania_P/0/1/0/all/0/1"&gt;Puneet Dokania&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1"&gt;Philip Torr&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CanvasVAE: Learning to Generate Vector Graphic Documents. (arXiv:2108.01249v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.01249</id>
        <link href="http://arxiv.org/abs/2108.01249"/>
        <updated>2021-08-04T01:59:22.059Z</updated>
        <summary type="html"><![CDATA[Vector graphic documents present visual elements in a resolution free,
compact format and are often seen in creative applications. In this work, we
attempt to learn a generative model of vector graphic documents. We define
vector graphic documents by a multi-modal set of attributes associated to a
canvas and a sequence of visual elements such as shapes, images, or texts, and
train variational auto-encoders to learn the representation of the documents.
We collect a new dataset of design templates from an online service that
features complete document structure including occluded elements. In
experiments, we show that our model, named CanvasVAE, constitutes a strong
baseline for generative modeling of vector graphic documents.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yamaguchi_K/0/1/0/all/0/1"&gt;Kota Yamaguchi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Content Disentanglement for Semantically Consistent Synthetic-to-Real Domain Adaptation. (arXiv:2105.08704v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.08704</id>
        <link href="http://arxiv.org/abs/2105.08704"/>
        <updated>2021-08-04T01:59:22.000Z</updated>
        <summary type="html"><![CDATA[Synthetic data generation is an appealing approach to generate novel traffic
scenarios in autonomous driving. However, deep learning perception algorithms
trained solely on synthetic data encounter serious performance drops when they
are tested on real data. Such performance drops are commonly attributed to the
domain gap between real and synthetic data. Domain adaptation methods that have
been applied to mitigate the aforementioned domain gap achieve visually
appealing results, but usually introduce semantic inconsistencies into the
translated samples. In this work, we propose a novel, unsupervised, end-to-end
domain adaptation network architecture that enables semantically consistent
\textit{sim2real} image transfer. Our method performs content disentanglement
by employing shared content encoder and fixed style code.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Keser_M/0/1/0/all/0/1"&gt;Mert Keser&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Savkin_A/0/1/0/all/0/1"&gt;Artem Savkin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tombari_F/0/1/0/all/0/1"&gt;Federico Tombari&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[You Better Look Twice: a new perspective for designing accurate detectors with reduced computations. (arXiv:2107.10050v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.10050</id>
        <link href="http://arxiv.org/abs/2107.10050"/>
        <updated>2021-08-04T01:59:21.974Z</updated>
        <summary type="html"><![CDATA[General object detectors use powerful backbones that uniformly extract
features from images for enabling detection of a vast amount of object types.
However, utilization of such backbones in object detection applications
developed for specific object types can unnecessarily over-process an extensive
amount of background. In addition, they are agnostic to object scales, thus
redundantly process all image regions at the same resolution. In this work we
introduce BLT-net, a new low-computation two-stage object detection
architecture designed to process images with a significant amount of background
and objects of variate scales. BLT-net reduces computations by separating
objects from background using a very lite first-stage. BLT-net then efficiently
merges obtained proposals to further decrease processed background and then
dynamically reduces their resolution to minimize computations. Resulting image
proposals are then processed in the second-stage by a highly accurate model. We
demonstrate our architecture on the pedestrian detection problem, where objects
are of different sizes, images are of high resolution and object detection is
required to run in real-time. We show that our design reduces computations by a
factor of x4-x7 on the Citypersons and Caltech datasets with respect to leading
pedestrian detectors, on account of a small accuracy degradation. This method
can be applied on other object detection applications in scenes with a
considerable amount of background and variate object sizes to reduce
computations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dana_A/0/1/0/all/0/1"&gt;Alexandra Dana&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shutman_M/0/1/0/all/0/1"&gt;Maor Shutman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Perlitz_Y/0/1/0/all/0/1"&gt;Yotam Perlitz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vitek_R/0/1/0/all/0/1"&gt;Ran Vitek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peleg_T/0/1/0/all/0/1"&gt;Tomer Peleg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jevnisek_R/0/1/0/all/0/1"&gt;Roy J Jevnisek&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[What s in My LiDAR Odometry Toolbox?. (arXiv:2103.09708v2 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.09708</id>
        <link href="http://arxiv.org/abs/2103.09708"/>
        <updated>2021-08-04T01:59:21.967Z</updated>
        <summary type="html"><![CDATA[With the democratization of 3D LiDAR sensors, precise LiDAR odometries and
SLAM are in high demand. New methods regularly appear, proposing solutions
ranging from small variations in classical algorithms to radically new
paradigms based on deep learning. Yet it is often difficult to compare these
methods, notably due to the few datasets on which the methods can be evaluated
and compared. Furthermore, their weaknesses are rarely examined, often letting
the user discover the hard way whether a method would be appropriate for a use
case. In this paper, we review and organize the main 3D LiDAR odometries into
distinct categories. We implemented several approaches (geometric based, deep
learning based, and hybrid methods) to conduct an in-depth analysis of their
strengths and weaknesses on multiple datasets, guiding the reader through the
different LiDAR odometries available. Implementation of the methods has been
made publicly available at https://github.com/Kitware/pyLiDAR-SLAM.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dellenbach_P/0/1/0/all/0/1"&gt;Pierre Dellenbach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deschaud_J/0/1/0/all/0/1"&gt;Jean-Emmanuel Deschaud&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jacquet_B/0/1/0/all/0/1"&gt;Bastien Jacquet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goulette_F/0/1/0/all/0/1"&gt;Fran&amp;#xe7;ois Goulette&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CORSAIR: Convolutional Object Retrieval and Symmetry-AIded Registration. (arXiv:2103.06911v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.06911</id>
        <link href="http://arxiv.org/abs/2103.06911"/>
        <updated>2021-08-04T01:59:21.941Z</updated>
        <summary type="html"><![CDATA[This paper considers online object-level mapping using partial point-cloud
observations obtained online in an unknown environment. We develop and approach
for fully Convolutional Object Retrieval and Symmetry-AIded Registration
(CORSAIR). Our model extends the Fully Convolutional Geometric Features model
to learn a global object-shape embedding in addition to local point-wise
features from the point-cloud observations. The global feature is used to
retrieve a similar object from a category database, and the local features are
used for robust pose registration between the observed and the retrieved
object. Our formulation also leverages symmetries, present in the object
shapes, to obtain promising local-feature pairs from different symmetry classes
for matching. We present results from synthetic and real-world datasets with
different object categories to verify the robustness of our method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1"&gt;Tianyu Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_Q/0/1/0/all/0/1"&gt;Qiaojun Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jadhav_S/0/1/0/all/0/1"&gt;Sai Jadhav&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Atanasov_N/0/1/0/all/0/1"&gt;Nikolay Atanasov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Event Camera Based Real-Time Detection and Tracking of Indoor Ground Robots. (arXiv:2102.11916v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.11916</id>
        <link href="http://arxiv.org/abs/2102.11916"/>
        <updated>2021-08-04T01:59:21.933Z</updated>
        <summary type="html"><![CDATA[This paper presents a real-time method to detect and track multiple mobile
ground robots using event cameras. The method uses density-based spatial
clustering of applications with noise (DBSCAN) to detect the robots and a
single k-dimensional ($k - d$) tree to accurately keep track of them as they
move in an indoor arena. Robust detections and tracks are maintained in the
face of event camera noise and lack of events (due to robots moving slowly or
stopping). An off-the-shelf RGB camera-based tracking system was used to
provide ground truth. Experiments including up to 4 robots are performed to
study the effect of i) varying DBSCAN parameters, ii) the event accumulation
time, iii) the number of robots in the arena, iv) the speed of the robots, and
v) variation in ambient light conditions on the detection and tracking
performance. The experimental results showed 100% detection and tracking
fidelity in the face of event camera noise and robots stopping for tests
involving up to 3 robots (and upwards of 93% for 4 robots). When the lighting
conditions were varied, a graceful degradation in detection and tracking
fidelity was observed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Patel_H/0/1/0/all/0/1"&gt;Himanshu Patel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iaboni_C/0/1/0/all/0/1"&gt;Craig Iaboni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lobo_D/0/1/0/all/0/1"&gt;Deepan Lobo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1"&gt;Ji-won Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abichandani_P/0/1/0/all/0/1"&gt;Pramod Abichandani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multispectral Vineyard Segmentation: A Deep Learning approach. (arXiv:2108.01200v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.01200</id>
        <link href="http://arxiv.org/abs/2108.01200"/>
        <updated>2021-08-04T01:59:21.912Z</updated>
        <summary type="html"><![CDATA[Digital agriculture has evolved significantly over the last few years due to
the technological developments in automation and computational intelligence
applied to the agricultural sector, including vineyards which are a relevant
crop in the Mediterranean region. In this paper, a study of semantic
segmentation for vine detection in real-world vineyards is presented by
exploring state-of-the-art deep segmentation networks and conventional
unsupervised methods. Camera data was collected on vineyards using an Unmanned
Aerial System (UAS) equipped with a dual imaging sensor payload, namely a
high-resolution color camera and a five-band multispectral and thermal camera.
Extensive experiments of the segmentation networks and unsupervised methods
have been performed on multimodal datasets representing three distinct
vineyards located in the central region of Portugal. The reported results
indicate that the best segmentation performances are obtained with deep
networks, while traditional (non-deep) approaches using the NIR band shown
competitive results. The results also show that multimodality slightly improves
the performance of vine segmentation but the NIR spectrum alone generally is
sufficient on most of the datasets. The code and dataset are publicly available
on \url{https://github.com/Cybonic/DL_vineyard_segmentation_study.git]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Barros_T/0/1/0/all/0/1"&gt;T. Barros&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Conde_P/0/1/0/all/0/1"&gt;P. Conde&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goncalves_G/0/1/0/all/0/1"&gt;G. Gon&amp;#xe7;alves&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Premebida_C/0/1/0/all/0/1"&gt;C. Premebida&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Monteiro_M/0/1/0/all/0/1"&gt;M. Monteiro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ferreira_C/0/1/0/all/0/1"&gt;C.S.S. Ferreira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nunes_U/0/1/0/all/0/1"&gt;U.J. Nunes&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LADMM-Net: An Unrolled Deep Network For Spectral Image Fusion From Compressive Data. (arXiv:2103.00940v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.00940</id>
        <link href="http://arxiv.org/abs/2103.00940"/>
        <updated>2021-08-04T01:59:21.904Z</updated>
        <summary type="html"><![CDATA[Image fusion aims at estimating a high-resolution spectral image from a
low-spatial-resolution hyperspectral image and a low-spectral-resolution
multispectral image. In this regard, compressive spectral imaging (CSI) has
emerged as an acquisition framework that captures the relevant information of
spectral images using a reduced number of measurements. Recently, various image
fusion methods from CSI measurements have been proposed. However, these methods
exhibit high running times and face the challenging task of choosing
sparsity-inducing bases. In this paper, a deep network under the algorithm
unrolling approach is proposed for fusing spectral images from compressive
measurements. This architecture, dubbed LADMM-Net, casts each iteration of a
linearized version of the alternating direction method of multipliers into a
processing layer whose concatenation deploys a deep network. The linearized
approach enables obtaining fusion estimates without resorting to costly matrix
inversions. Furthermore, this approach exploits the benefits of learnable
transforms to estimate the image details included in both the auxiliary
variable and the Lagrange multiplier. Finally, the performance of the proposed
technique is evaluated on two spectral image databases and one dataset captured
at the laboratory. Extensive simulations show that the proposed method
outperforms the state-of-the-art approaches that fuse spectral images from
compressive measurements.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Ramirez_J/0/1/0/all/0/1"&gt;Juan Marcos Ram&amp;#xed;rez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Torre_J/0/1/0/all/0/1"&gt;Jos&amp;#xe9; Ignacio Mart&amp;#xed;nez Torre&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Fuentes_H/0/1/0/all/0/1"&gt;Henry Arguello Fuentes&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Semantic Segmentation by Contrasting Object Mask Proposals. (arXiv:2102.06191v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.06191</id>
        <link href="http://arxiv.org/abs/2102.06191"/>
        <updated>2021-08-04T01:59:21.897Z</updated>
        <summary type="html"><![CDATA[Being able to learn dense semantic representations of images without
supervision is an important problem in computer vision. However, despite its
significance, this problem remains rather unexplored, with a few exceptions
that considered unsupervised semantic segmentation on small-scale datasets with
a narrow visual domain. In this paper, we make a first attempt to tackle the
problem on datasets that have been traditionally utilized for the supervised
case. To achieve this, we introduce a two-step framework that adopts a
predetermined mid-level prior in a contrastive optimization objective to learn
pixel embeddings. This marks a large deviation from existing works that relied
on proxy tasks or end-to-end clustering. Additionally, we argue about the
importance of having a prior that contains information about objects, or their
parts, and discuss several possibilities to obtain such a prior in an
unsupervised manner.

Experimental evaluation shows that our method comes with key advantages over
existing works. First, the learned pixel embeddings can be directly clustered
in semantic groups using K-Means on PASCAL. Under the fully unsupervised
setting, there is no precedent in solving the semantic segmentation task on
such a challenging benchmark. Second, our representations can improve over
strong baselines when transferred to new datasets, e.g. COCO and DAVIS. The
code is available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gansbeke_W/0/1/0/all/0/1"&gt;Wouter Van Gansbeke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vandenhende_S/0/1/0/all/0/1"&gt;Simon Vandenhende&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Georgoulis_S/0/1/0/all/0/1"&gt;Stamatios Georgoulis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1"&gt;Luc Van Gool&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Families In Wild Multimedia (FIW MM): A Multi-Modal Database for Recognizing Kinship. (arXiv:2007.14509v5 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.14509</id>
        <link href="http://arxiv.org/abs/2007.14509"/>
        <updated>2021-08-04T01:59:21.874Z</updated>
        <summary type="html"><![CDATA[Kinship, a soft biometric detectable in media, is fundamental for a myriad of
use-cases. Despite the difficulty of detecting kinship, annual data challenges
using still-images have consistently improved performances and attracted new
researchers. Now, systems reach performance levels unforeseeable a decade ago,
closing in on performances acceptable to deploy in practice. Similar to other
biometric tasks, we expect systems can benefit from additional modalities. We
hypothesize that adding modalities to FIW, which contains only still-images,
will improve performance. Thus, to narrow the gap between research and reality
and enhance the power of kinship recognition systems, we extend FIW with
multimedia (MM) data (i.e., video, audio, and text captions). Specifically, we
introduce the first publicly available multi-task MM kinship dataset. To build
FIW MM, we developed machinery to automatically collect, annotate, and prepare
the data, requiring minimal human input and no financial cost. The proposed MM
corpus allows the problem statements to be more realistic template-based
protocols. We show significant improvements in all benchmarks with the added
modalities. The results highlight edge cases to inspire future research with
different areas of improvement. FIW MM provides the data required to increase
the potential of automated systems to detect kinship in MM. It also allows
experts from diverse fields to collaborate in novel ways.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Robinson_J/0/1/0/all/0/1"&gt;Joseph P. Robinson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khan_Z/0/1/0/all/0/1"&gt;Zaid Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1"&gt;Yu Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_M/0/1/0/all/0/1"&gt;Ming Shao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1"&gt;Yun Fu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bottleneck Transformers for Visual Recognition. (arXiv:2101.11605v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.11605</id>
        <link href="http://arxiv.org/abs/2101.11605"/>
        <updated>2021-08-04T01:59:21.867Z</updated>
        <summary type="html"><![CDATA[We present BoTNet, a conceptually simple yet powerful backbone architecture
that incorporates self-attention for multiple computer vision tasks including
image classification, object detection and instance segmentation. By just
replacing the spatial convolutions with global self-attention in the final
three bottleneck blocks of a ResNet and no other changes, our approach improves
upon the baselines significantly on instance segmentation and object detection
while also reducing the parameters, with minimal overhead in latency. Through
the design of BoTNet, we also point out how ResNet bottleneck blocks with
self-attention can be viewed as Transformer blocks. Without any bells and
whistles, BoTNet achieves 44.4% Mask AP and 49.7% Box AP on the COCO Instance
Segmentation benchmark using the Mask R-CNN framework; surpassing the previous
best published single model and single scale results of ResNeSt evaluated on
the COCO validation set. Finally, we present a simple adaptation of the BoTNet
design for image classification, resulting in models that achieve a strong
performance of 84.7% top-1 accuracy on the ImageNet benchmark while being up to
1.64x faster in compute time than the popular EfficientNet models on TPU-v3
hardware. We hope our simple and effective approach will serve as a strong
baseline for future research in self-attention models for vision]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Srinivas_A/0/1/0/all/0/1"&gt;Aravind Srinivas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1"&gt;Tsung-Yi Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Parmar_N/0/1/0/all/0/1"&gt;Niki Parmar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shlens_J/0/1/0/all/0/1"&gt;Jonathon Shlens&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1"&gt;Pieter Abbeel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vaswani_A/0/1/0/all/0/1"&gt;Ashish Vaswani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Counting People by Estimating People Flows. (arXiv:2012.00452v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.00452</id>
        <link href="http://arxiv.org/abs/2012.00452"/>
        <updated>2021-08-04T01:59:21.856Z</updated>
        <summary type="html"><![CDATA[Modern methods for counting people in crowded scenes rely on deep networks to
estimate people densities in individual images. As such, only very few take
advantage of temporal consistency in video sequences, and those that do only
impose weak smoothness constraints across consecutive frames. In this paper, we
advocate estimating people flows across image locations between consecutive
images and inferring the people densities from these flows instead of directly
regressing them. This enables us to impose much stronger constraints encoding
the conservation of the number of people. As a result, it significantly boosts
performance without requiring a more complex architecture. Furthermore, it
allows us to exploit the correlation between people flow and optical flow to
further improve the results. We also show that leveraging people conservation
constraints in both a spatial and temporal manner makes it possible to train a
deep crowd counting model in an active learning setting with much fewer
annotations. This significantly reduces the annotation cost while still leading
to similar performance to the full supervision case.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Weizhe Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salzmann_M/0/1/0/all/0/1"&gt;Mathieu Salzmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fua_P/0/1/0/all/0/1"&gt;Pascal Fua&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AIBench Scenario: Scenario-distilling AI Benchmarking. (arXiv:2005.03459v3 [cs.PF] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.03459</id>
        <link href="http://arxiv.org/abs/2005.03459"/>
        <updated>2021-08-04T01:59:21.837Z</updated>
        <summary type="html"><![CDATA[Modern real-world application scenarios like Internet services consist of a
diversity of AI and non-AI modules with huge code sizes and long and
complicated execution paths, which raises serious benchmarking or evaluating
challenges. Using AI components or micro benchmarks alone can lead to
error-prone conclusions. This paper presents a methodology to attack the above
challenge. We formalize a real-world application scenario as a Directed Acyclic
Graph-based model and propose the rules to distill it into a permutation of
essential AI and non-AI tasks, which we call a scenario benchmark. Together
with seventeen industry partners, we extract nine typical scenario benchmarks.
We design and implement an extensible, configurable, and flexible benchmark
framework. We implement two Internet service AI scenario benchmarks based on
the framework as proxies to two real-world application scenarios. We consider
scenario, component, and micro benchmarks as three indispensable parts for
evaluating. Our evaluation shows the advantage of our methodology against using
component or micro AI benchmarks alone. The specifications, source code,
testbed, and results are publicly available from
\url{https://www.benchcouncil.org/aibench/scenario/}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1"&gt;Wanling Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_F/0/1/0/all/0/1"&gt;Fei Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhan_J/0/1/0/all/0/1"&gt;Jianfeng Zhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_X/0/1/0/all/0/1"&gt;Xu Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Lei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1"&gt;Zheng Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lan_C/0/1/0/all/0/1"&gt;Chuanxin Lan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1"&gt;Chunjie Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiaoli Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1"&gt;Zihan Jiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ItNet: iterative neural networks with small graphs for accurate, efficient and anytime semantic segmentation. (arXiv:2101.08685v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.08685</id>
        <link href="http://arxiv.org/abs/2101.08685"/>
        <updated>2021-08-04T01:59:21.831Z</updated>
        <summary type="html"><![CDATA[Deep neural networks have usually to be compressed and accelerated for their
usage in low-power, e.g. mobile, devices. Recently, massively-parallel hardware
accelerators were developed that offer high throughput and low latency at low
power by utilizing in-memory computation. However, to exploit these benefits
the computational graph of a neural network has to fit into the in-computation
memory of these hardware systems that is usually rather limited in size. In
this study, we introduce a class of network models that have a small memory
footprint in terms of their computational graphs. To this end, the graph is
designed to contain loops by iteratively executing a single network building
block. Furthermore, the trade-off between accuracy and latency of these
so-called iterative neural networks is improved by adding multiple intermediate
outputs during both training and inference. We show state-of-the-art results
for semantic segmentation on the CamVid and Cityscapes datasets that are
especially demanding in terms of computational resources. In ablation studies,
the improvement of network training by intermediate network outputs as well as
the trade-off between weight sharing over iterations and the network size are
investigated.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pfeil_T/0/1/0/all/0/1"&gt;Thomas Pfeil&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Image Augmentation Using a Task Guided Generative Adversarial Network for Age Estimation on Brain MRI. (arXiv:2108.01659v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2108.01659</id>
        <link href="http://arxiv.org/abs/2108.01659"/>
        <updated>2021-08-04T01:59:21.825Z</updated>
        <summary type="html"><![CDATA[Brain age estimation based on magnetic resonance imaging (MRI) is an active
research area in early diagnosis of some neurodegenerative diseases (e.g.
Alzheimer, Parkinson, Huntington, etc.) for elderly people or brain
underdevelopment for the young group. Deep learning methods have achieved the
state-of-the-art performance in many medical image analysis tasks, including
brain age estimation. However, the performance and generalisability of the deep
learning model are highly dependent on the quantity and quality of the training
data set. Both collecting and annotating brain MRI data are extremely
time-consuming. In this paper, to overcome the data scarcity problem, we
propose a generative adversarial network (GAN) based image synthesis method.
Different from the existing GAN-based methods, we integrate a task-guided
branch (a regression model for age estimation) to the end of the generator in
GAN. By adding a task-guided loss to the conventional GAN loss, the learned
low-dimensional latent space and the synthesised images are more task-specific.
It helps to boost the performance of the down-stream task by combining the
synthesised images and real images for model training. The proposed method was
evaluated on a public brain MRI data set for age estimation. Our proposed
method outperformed (statistically significant) a deep convolutional neural
network based regression model and the GAN-based image synthesis method without
the task-guided branch. More importantly, it enables the identification of
age-related brain regions in the image space. The code is available on GitHub
(https://github.com/ruizhe-l/tgb-gan).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Li_R/0/1/0/all/0/1"&gt;Ruizhe Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bastiani_M/0/1/0/all/0/1"&gt;Matteo Bastiani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Auer_D/0/1/0/all/0/1"&gt;Dorothee Auer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wagner_C/0/1/0/all/0/1"&gt;Christian Wagner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xin Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dual-Level Collaborative Transformer for Image Captioning. (arXiv:2101.06462v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.06462</id>
        <link href="http://arxiv.org/abs/2101.06462"/>
        <updated>2021-08-04T01:59:21.813Z</updated>
        <summary type="html"><![CDATA[Descriptive region features extracted by object detection networks have
played an important role in the recent advancements of image captioning.
However, they are still criticized for the lack of contextual information and
fine-grained details, which in contrast are the merits of traditional grid
features. In this paper, we introduce a novel Dual-Level Collaborative
Transformer (DLCT) network to realize the complementary advantages of the two
features. Concretely, in DLCT, these two features are first processed by a
novelDual-way Self Attenion (DWSA) to mine their intrinsic properties, where a
Comprehensive Relation Attention component is also introduced to embed the
geometric information. In addition, we propose a Locality-Constrained Cross
Attention module to address the semantic noises caused by the direct fusion of
these two features, where a geometric alignment graph is constructed to
accurately align and reinforce region and grid features. To validate our model,
we conduct extensive experiments on the highly competitive MS-COCO dataset, and
achieve new state-of-the-art performance on both local and online test sets,
i.e., 133.8% CIDEr-D on Karpathy split and 135.4% CIDEr on the official split.
Code is available at https://github.com/luo3300612/image-captioning-DLCT.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1"&gt;Yunpeng Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_J/0/1/0/all/0/1"&gt;Jiayi Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1"&gt;Xiaoshuai Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_L/0/1/0/all/0/1"&gt;Liujuan Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yongjian Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1"&gt;Feiyue Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1"&gt;Chia-Wen Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1"&gt;Rongrong Ji&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[JCS: An Explainable COVID-19 Diagnosis System by Joint Classification and Segmentation. (arXiv:2004.07054v3 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2004.07054</id>
        <link href="http://arxiv.org/abs/2004.07054"/>
        <updated>2021-08-04T01:59:21.807Z</updated>
        <summary type="html"><![CDATA[Recently, the coronavirus disease 2019 (COVID-19) has caused a pandemic
disease in over 200 countries, influencing billions of humans. To control the
infection, identifying and separating the infected people is the most crucial
step. The main diagnostic tool is the Reverse Transcription Polymerase Chain
Reaction (RT-PCR) test. Still, the sensitivity of the RT-PCR test is not high
enough to effectively prevent the pandemic. The chest CT scan test provides a
valuable complementary tool to the RT-PCR test, and it can identify the
patients in the early-stage with high sensitivity. However, the chest CT scan
test is usually time-consuming, requiring about 21.5 minutes per case. This
paper develops a novel Joint Classification and Segmentation (JCS) system to
perform real-time and explainable COVID-19 chest CT diagnosis. To train our JCS
system, we construct a large scale COVID-19 Classification and Segmentation
(COVID-CS) dataset, with 144,167 chest CT images of 400 COVID-19 patients and
350 uninfected cases. 3,855 chest CT images of 200 patients are annotated with
fine-grained pixel-level labels of opacifications, which are increased
attenuation of the lung parenchyma. We also have annotated lesion counts,
opacification areas, and locations and thus benefit various diagnosis aspects.
Extensive experiments demonstrate that the proposed JCS diagnosis system is
very efficient for COVID-19 classification and segmentation. It obtains an
average sensitivity of 95.0% and a specificity of 93.0% on the classification
test set, and 78.5% Dice score on the segmentation test set of our COVID-CS
dataset. The COVID-CS dataset and code are available at
https://github.com/yuhuan-wu/JCS.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yu-Huan Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gao_S/0/1/0/all/0/1"&gt;Shang-Hua Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mei_J/0/1/0/all/0/1"&gt;Jie Mei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Xu_J/0/1/0/all/0/1"&gt;Jun Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Fan_D/0/1/0/all/0/1"&gt;Deng-Ping Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Rong-Guo Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cheng_M/0/1/0/all/0/1"&gt;Ming-Ming Cheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Red Blood Cell Segmentation with Overlapping Cell Separation and Classification on Imbalanced Dataset. (arXiv:2012.01321v4 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.01321</id>
        <link href="http://arxiv.org/abs/2012.01321"/>
        <updated>2021-08-04T01:59:21.790Z</updated>
        <summary type="html"><![CDATA[Automated red blood cell (RBC) classification on blood smear images helps
hematologists to analyze RBC lab results in a reduced time and cost. However,
overlapping cells can cause incorrect predicted results, and so they have to be
separated into multiple single RBCs before classifying. To classify multiple
classes with deep learning, imbalance problems are common in medical imaging
because normal samples are always higher than rare disease samples. This paper
presents a new method to segment and classify RBCs from blood smear images,
specifically to tackle cell overlapping and data imbalance problems. Focusing
on overlapping cell separation, our segmentation process first estimates
ellipses to represent RBCs. The method detects the concave points and then
finds the ellipses using directed ellipse fitting. The accuracy from 20 blood
smear images was 0.889. Classification requires balanced training datasets.
However, some RBC types are rare. The imbalance ratio of this dataset was
34.538 for 12 RBC classes from 20,875 individual RBC samples. The use of
machine learning for RBC classification with an imbalanced dataset is hence
more challenging than many other applications. We analyzed techniques to deal
with this problem. The best accuracy and F1-score were 0.921 and 0.8679,
respectively, using EfficientNet-B1 with augmentation. Experimental results
showed that the weight balancing technique with augmentation had the potential
to deal with imbalance problems by improving the F1-score on minority classes,
while data augmentation significantly improved the overall classification
performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Naruenatthanaset_K/0/1/0/all/0/1"&gt;Korranat Naruenatthanaset&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chalidabhongse_T/0/1/0/all/0/1"&gt;Thanarat H. Chalidabhongse&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Palasuwan_D/0/1/0/all/0/1"&gt;Duangdao Palasuwan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Anantrasirichai_N/0/1/0/all/0/1"&gt;Nantheera Anantrasirichai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Palasuwan_A/0/1/0/all/0/1"&gt;Attakorn Palasuwan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine learning approach to force reconstruction in photoelastic materials. (arXiv:2010.01163v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.01163</id>
        <link href="http://arxiv.org/abs/2010.01163"/>
        <updated>2021-08-04T01:59:21.757Z</updated>
        <summary type="html"><![CDATA[Photoelastic techniques have a long tradition in both qualitative and
quantitative analysis of the stresses in granular materials. Over the last two
decades, computational methods for reconstructing forces between particles from
their photoelastic response have been developed by many different experimental
teams. Unfortunately, all of these methods are computationally expensive. This
limits their use for processing extensive data sets that capture the time
evolution of granular ensembles consisting of a large number of particles. In
this paper, we present a novel approach to this problem which leverages the
power of convolutional neural networks to recognize complex spatial patterns.
The main drawback of using neural networks is that training them usually
requires a large labeled data set which is hard to obtain experimentally. We
show that this problem can be successfully circumvented by pretraining the
networks on a large synthetic data set and then fine-tuning them on much
smaller experimental data sets. Due to our current lack of experimental data,
we demonstrate the potential of our method by changing the size of the
considered particles which alters the exhibited photoelastic patterns more than
typical experimental errors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sergazinov_R/0/1/0/all/0/1"&gt;Renat Sergazinov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kramar_M/0/1/0/all/0/1"&gt;Miroslav Kramar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Uniform Sampling over Episode Difficulty. (arXiv:2108.01662v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.01662</id>
        <link href="http://arxiv.org/abs/2108.01662"/>
        <updated>2021-08-04T01:59:21.741Z</updated>
        <summary type="html"><![CDATA[Episodic training is a core ingredient of few-shot learning to train models
on tasks with limited labelled data. Despite its success, episodic training
remains largely understudied, prompting us to ask the question: what is the
best way to sample episodes? In this paper, we first propose a method to
approximate episode sampling distributions based on their difficulty. Building
on this method, we perform an extensive analysis and find that sampling
uniformly over episode difficulty outperforms other sampling schemes, including
curriculum and easy-/hard-mining. As the proposed sampling method is algorithm
agnostic, we can leverage these insights to improve few-shot learning
accuracies across many episodic training algorithms. We demonstrate the
efficacy of our method across popular few-shot learning datasets, algorithms,
network architectures, and protocols.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Arnold_S/0/1/0/all/0/1"&gt;S&amp;#xe9;bastien M. R. Arnold&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dhillon_G/0/1/0/all/0/1"&gt;Guneet S. Dhillon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ravichandran_A/0/1/0/all/0/1"&gt;Avinash Ravichandran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1"&gt;Stefano Soatto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep GAN-Based Cross-Spectral Cross-Resolution Iris Recognition. (arXiv:2108.01569v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.01569</id>
        <link href="http://arxiv.org/abs/2108.01569"/>
        <updated>2021-08-04T01:59:21.705Z</updated>
        <summary type="html"><![CDATA[In recent years, cross-spectral iris recognition has emerged as a promising
biometric approach to establish the identity of individuals. However, matching
iris images acquired at different spectral bands (i.e., matching a visible
(VIS) iris probe to a gallery of near-infrared (NIR) iris images or vice versa)
shows a significant performance degradation when compared to intraband NIR
matching. Hence, in this paper, we have investigated a range of deep
convolutional generative adversarial network (DCGAN) architectures to further
improve the accuracy of cross-spectral iris recognition methods. Moreover,
unlike the existing works in the literature, we introduce a resolution
difference into the classical cross-spectral matching problem domain. We have
developed two different techniques using the conditional generative adversarial
network (cGAN) as a backbone architecture for cross-spectral iris matching. In
the first approach, we simultaneously address the cross-resolution and
cross-spectral matching problem by training a cGAN that jointly translates
cross-resolution as well as cross-spectral tasks to the same resolution and
within the same spectrum. In the second approach, we design a coupled
generative adversarial network (cpGAN) architecture consisting of a pair of
cGAN modules that project the VIS and NIR iris images into a low-dimensional
embedding domain to ensure maximum pairwise similarity between the feature
vectors from the two iris modalities of the same subject.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mostofa_M/0/1/0/all/0/1"&gt;Moktari Mostofa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mohamadi_S/0/1/0/all/0/1"&gt;Salman Mohamadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dawson_J/0/1/0/all/0/1"&gt;Jeremy Dawson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nasrabadi_N/0/1/0/all/0/1"&gt;Nasser M. Nasrabadi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Triggering Failures: Out-Of-Distribution detection by learning from local adversarial attacks in Semantic Segmentation. (arXiv:2108.01634v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.01634</id>
        <link href="http://arxiv.org/abs/2108.01634"/>
        <updated>2021-08-04T01:59:21.697Z</updated>
        <summary type="html"><![CDATA[In this paper, we tackle the detection of out-of-distribution (OOD) objects
in semantic segmentation. By analyzing the literature, we found that current
methods are either accurate or fast but not both which limits their usability
in real world applications. To get the best of both aspects, we propose to
mitigate the common shortcomings by following four design principles:
decoupling the OOD detection from the segmentation task, observing the entire
segmentation network instead of just its output, generating training data for
the OOD detector by leveraging blind spots in the segmentation network and
focusing the generated data on localized regions in the image to simulate OOD
objects. Our main contribution is a new OOD detection architecture called
ObsNet associated with a dedicated training scheme based on Local Adversarial
Attacks (LAA). We validate the soundness of our approach across numerous
ablation studies. We also show it obtains top performances both in speed and
accuracy when compared to ten recent methods of the literature on three
different datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Besnier_V/0/1/0/all/0/1"&gt;Victor Besnier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bursuc_A/0/1/0/all/0/1"&gt;Andrei Bursuc&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Picard_D/0/1/0/all/0/1"&gt;David Picard&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Briot_A/0/1/0/all/0/1"&gt;Alexandre Briot&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Comparison of modern open-source visual SLAM approaches. (arXiv:2108.01654v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2108.01654</id>
        <link href="http://arxiv.org/abs/2108.01654"/>
        <updated>2021-08-04T01:59:21.680Z</updated>
        <summary type="html"><![CDATA[SLAM is one of the most fundamental areas of research in robotics and
computer vision. State of the art solutions has advanced significantly in terms
of accuracy and stability. Unfortunately, not all the approaches are available
as open-source solutions and free to use. The results of some of them are
difficult to reproduce, and there is a lack of comparison on common datasets.
In our work, we make a comparative analysis of state of the art open-source
methods. We assess the algorithms based on accuracy, computational performance,
robustness, and fault tolerance. Moreover, we present a comparison of datasets
as well as an analysis of algorithms from a practical point of view. The
findings of the work raise several crucial questions for SLAM researchers.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sharafutdinov_D/0/1/0/all/0/1"&gt;Dinar Sharafutdinov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Griguletskii_M/0/1/0/all/0/1"&gt;Mark Griguletskii&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kopanev_P/0/1/0/all/0/1"&gt;Pavel Kopanev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kurenkov_M/0/1/0/all/0/1"&gt;Mikhail Kurenkov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ferrer_G/0/1/0/all/0/1"&gt;Gonzalo Ferrer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Burkov_A/0/1/0/all/0/1"&gt;Aleksey Burkov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gonnochenko_A/0/1/0/all/0/1"&gt;Aleksei Gonnochenko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsetserukou_D/0/1/0/all/0/1"&gt;Dzmitry Tsetserukou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Del-Net: A Single-Stage Network for Mobile Camera ISP. (arXiv:2108.01623v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.01623</id>
        <link href="http://arxiv.org/abs/2108.01623"/>
        <updated>2021-08-04T01:59:21.665Z</updated>
        <summary type="html"><![CDATA[The quality of images captured by smartphones is an important specification
since smartphones are becoming ubiquitous as primary capturing devices. The
traditional image signal processing (ISP) pipeline in a smartphone camera
consists of several image processing steps performed sequentially to
reconstruct a high quality sRGB image from the raw sensor data. These steps
consist of demosaicing, denoising, white balancing, gamma correction, colour
enhancement, etc. Since each of them are performed sequentially using
hand-crafted algorithms, the residual error from each processing module
accumulates in the final reconstructed signal. Thus, the traditional ISP
pipeline has limited reconstruction quality in terms of generalizability across
different lighting conditions and associated noise levels while capturing the
image. Deep learning methods using convolutional neural networks (CNN) have
become popular in solving many image-related tasks such as image denoising,
contrast enhancement, super resolution, deblurring, etc. Furthermore, recent
approaches for the RAW to sRGB conversion using deep learning methods have also
been published, however, their immense complexity in terms of their memory
requirement and number of Mult-Adds make them unsuitable for mobile camera ISP.
In this paper we propose DelNet - a single end-to-end deep learning model - to
learn the entire ISP pipeline within reasonable complexity for smartphone
deployment. Del-Net is a multi-scale architecture that uses spatial and channel
attention to capture global features like colour, as well as a series of
lightweight modified residual attention blocks to help with denoising. For
validation, we provide results to show the proposed Del-Net achieves compelling
reconstruction quality.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1"&gt;Saumya Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srivastava_D/0/1/0/all/0/1"&gt;Diplav Srivastava&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chaturvedi_U/0/1/0/all/0/1"&gt;Umang Chaturvedi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1"&gt;Anurag Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khandelwal_G/0/1/0/all/0/1"&gt;Gaurav Khandelwal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SPG-VTON: Semantic Prediction Guidance for Multi-pose Virtual Try-on. (arXiv:2108.01578v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.01578</id>
        <link href="http://arxiv.org/abs/2108.01578"/>
        <updated>2021-08-04T01:59:21.657Z</updated>
        <summary type="html"><![CDATA[Image-based virtual try-on is challenging in fitting a target in-shop clothes
into a reference person under diverse human poses. Previous works focus on
preserving clothing details ( e.g., texture, logos, patterns ) when
transferring desired clothes onto a target person under a fixed pose. However,
the performances of existing methods significantly dropped when extending
existing methods to multi-pose virtual try-on. In this paper, we propose an
end-to-end Semantic Prediction Guidance multi-pose Virtual Try-On Network
(SPG-VTON), which could fit the desired clothing into a reference person under
arbitrary poses. Concretely, SPG-VTON is composed of three sub-modules. First,
a Semantic Prediction Module (SPM) generates the desired semantic map. The
predicted semantic map provides more abundant guidance to locate the desired
clothes region and produce a coarse try-on image. Second, a Clothes Warping
Module (CWM) warps in-shop clothes to the desired shape according to the
predicted semantic map and the desired pose. Specifically, we introduce a
conductible cycle consistency loss to alleviate the misalignment in the clothes
warping process. Third, a Try-on Synthesis Module (TSM) combines the coarse
result and the warped clothes to generate the final virtual try-on image,
preserving details of the desired clothes and under the desired pose. Besides,
we introduce a face identity loss to refine the facial appearance and maintain
the identity of the final virtual try-on result at the same time. We evaluate
the proposed method on the most massive multi-pose dataset (MPV) and the
DeepFashion dataset. The qualitative and quantitative experiments show that
SPG-VTON is superior to the state-of-the-art methods and is robust to the data
noise, including background and accessory changes, i.e., hats and handbags,
showing good scalability to the real-world scenario.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_B/0/1/0/all/0/1"&gt;Bingwen Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1"&gt;Ping Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1"&gt;Zhedong Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_M/0/1/0/all/0/1"&gt;Mingwu Ren&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Domain Generalization via Gradient Surgery. (arXiv:2108.01621v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.01621</id>
        <link href="http://arxiv.org/abs/2108.01621"/>
        <updated>2021-08-04T01:59:21.650Z</updated>
        <summary type="html"><![CDATA[In real-life applications, machine learning models often face scenarios where
there is a change in data distribution between training and test domains. When
the aim is to make predictions on distributions different from those seen at
training, we incur in a domain generalization problem. Methods to address this
issue learn a model using data from multiple source domains, and then apply
this model to the unseen target domain. Our hypothesis is that when training
with multiple domains, conflicting gradients within each mini-batch contain
information specific to the individual domains which is irrelevant to the
others, including the test domain. If left untouched, such disagreement may
degrade generalization performance. In this work, we characterize the
conflicting gradients emerging in domain shift scenarios and devise novel
gradient agreement strategies based on gradient surgery to alleviate their
effect. We validate our approach in image classification tasks with three
multi-domain datasets, showing the value of the proposed agreement strategy in
enhancing the generalization capability of deep learning models in domain shift
scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mansilla_L/0/1/0/all/0/1"&gt;Lucas Mansilla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Echeveste_R/0/1/0/all/0/1"&gt;Rodrigo Echeveste&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Milone_D/0/1/0/all/0/1"&gt;Diego H. Milone&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ferrante_E/0/1/0/all/0/1"&gt;Enzo Ferrante&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sexing Caucasian 2D footprints using convolutional neural networks. (arXiv:2108.01554v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.01554</id>
        <link href="http://arxiv.org/abs/2108.01554"/>
        <updated>2021-08-04T01:59:21.572Z</updated>
        <summary type="html"><![CDATA[Footprints are left, or obtained, in a variety of scenarios from crime scenes
to anthropological investigations. Determining the sex of a footprint can be
useful in screening such impressions and attempts have been made to do so using
single or multi landmark distances, shape analyses and via the density of
friction ridges. Here we explore the relative importance of different
components in sexing two-dimensional foot impressions namely, size, shape and
texture. We use a machine learning approach and compare this to more
traditional methods of discrimination. Two datasets are used, a pilot data set
collected from students at Bournemouth University (N=196) and a larger data set
collected by podiatrists at Sheffield NHS Teaching Hospital (N=2677). Our
convolutional neural network can sex a footprint with accuracy of around 90% on
a test set of N=267 footprint images using all image components, which is
better than an expert can achieve. However, the quality of the impressions
impacts on this success rate, but the results are promising and in time it may
be possible to create an automated screening algorithm in which practitioners
of whatever sort (medical or forensic) can obtain a first order sexing of a
two-dimensional footprint.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Budka_M/0/1/0/all/0/1"&gt;Marcin Budka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bennet_M/0/1/0/all/0/1"&gt;Matthew R. Bennet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reynolds_S/0/1/0/all/0/1"&gt;Sally Reynolds&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barefoot_S/0/1/0/all/0/1"&gt;Shelby Barefoot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reel_S/0/1/0/all/0/1"&gt;Sarah Reel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reidy_S/0/1/0/all/0/1"&gt;Selina Reidy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Walker_J/0/1/0/all/0/1"&gt;Jeremy Walker&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SphereFace2: Binary Classification is All You Need for Deep Face Recognition. (arXiv:2108.01513v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.01513</id>
        <link href="http://arxiv.org/abs/2108.01513"/>
        <updated>2021-08-04T01:59:21.555Z</updated>
        <summary type="html"><![CDATA[State-of-the-art deep face recognition methods are mostly trained with a
softmax-based multi-class classification framework. Despite being popular and
effective, these methods still have a few shortcomings that limit empirical
performance. In this paper, we first identify the discrepancy between training
and evaluation in the existing multi-class classification framework and then
discuss the potential limitations caused by the "competitive" nature of softmax
normalization. Motivated by these limitations, we propose a novel binary
classification training framework, termed SphereFace2. In contrast to existing
methods, SphereFace2 circumvents the softmax normalization, as well as the
corresponding closed-set assumption. This effectively bridges the gap between
training and evaluation, enabling the representations to be improved
individually by each binary classification task. Besides designing a specific
well-performing loss function, we summarize a few general principles for this
"one-vs-all" binary classification framework so that it can outperform current
competitive methods. We conduct comprehensive experiments on popular benchmarks
to demonstrate that SphereFace2 can consistently outperform current
state-of-the-art deep face recognition methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1"&gt;Yandong Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Weiyang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weller_A/0/1/0/all/0/1"&gt;Adrian Weller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raj_B/0/1/0/all/0/1"&gt;Bhiksha Raj&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1"&gt;Rita Singh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Inference via Sparse Coding in a Hierarchical Vision Model. (arXiv:2108.01548v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.01548</id>
        <link href="http://arxiv.org/abs/2108.01548"/>
        <updated>2021-08-04T01:59:21.548Z</updated>
        <summary type="html"><![CDATA[Sparse coding has been incorporated in models of the visual cortex for its
computational advantages and connection to biology. But how the level of
sparsity contributes to performance on visual tasks is not well understood. In
this work, sparse coding has been integrated into an existing hierarchical V2
model (Hosoya and Hyv\"arinen, 2015), but replacing the Independent Component
Analysis (ICA) with an explicit sparse coding in which the degree of sparsity
can be controlled. After training, the sparse coding basis functions with a
higher degree of sparsity resembled qualitatively different structures, such as
curves and corners. The contributions of the models were assessed with image
classification tasks, including object classification, and tasks associated
with mid-level vision including figure-ground classification, texture
classification, and angle prediction between two line stimuli. In addition, the
models were assessed in comparison to a texture sensitivity measure that has
been reported in V2 (Freeman et al., 2013), and a deleted-region inference
task. The results from the experiments show that while sparse coding performed
worse than ICA at classifying images, only sparse coding was able to better
match the texture sensitivity level of V2 and infer deleted image regions, both
by increasing the degree of sparsity in sparse coding. Higher degrees of
sparsity allowed for inference over larger deleted image regions. The mechanism
that allows for this inference capability in sparse coding is described here.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bowren_J/0/1/0/all/0/1"&gt;Joshua Bowren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sanchez_Giraldo_L/0/1/0/all/0/1"&gt;Luis Sanchez-Giraldo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schwartz_O/0/1/0/all/0/1"&gt;Odelia Schwartz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Two New Stenoses Detection Methods of Coronary Angiograms. (arXiv:2108.01516v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2108.01516</id>
        <link href="http://arxiv.org/abs/2108.01516"/>
        <updated>2021-08-04T01:59:21.541Z</updated>
        <summary type="html"><![CDATA[Coronary angiography is the "gold standard" for the diagnosis of coronary
heart disease. At present, the methods for detecting coronary artery stenoses
and evaluating the degree of it in coronary angiograms are either subjective or
not efficient enough. Two vascular stenoses detection methods in coronary
angiograms are proposed to assist the diagnosis. The first one is an automatic
method, which can automatically segment the entire coronary vessels and mark
the stenoses. The second one is an interactive method. With this method, the
user only needs to give a start point and an end point to detect the stenoses
of a certain vascular segment. We have shown that the proposed tracking methods
are robust for angiograms with various vessel structure. The automatic
detection method can effectively measure the diameter of the vessel and mark
the stenoses in different angiograms. Further investigation proves that the
results of interactive detection method can accurately reflect the true
stenoses situation. The proposed automatic method and interactive method are
effective in various angiograms and can complement each other in clinical
practice. The first method can be used for preliminary screening and the second
method can be used for further quantitative analysis. It has the potential to
improve the level of clinical diagnosis of coronary heart disease.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yaofang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xinyue Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wan_W/0/1/0/all/0/1"&gt;Wenlong Wan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liu_S/0/1/0/all/0/1"&gt;Shaoyu Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yingdi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liu_H/0/1/0/all/0/1"&gt;Hu Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zeng_X/0/1/0/all/0/1"&gt;Xueying Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Qing Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Boosting Weakly Supervised Object Detection via Learning Bounding Box Adjusters. (arXiv:2108.01499v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.01499</id>
        <link href="http://arxiv.org/abs/2108.01499"/>
        <updated>2021-08-04T01:59:21.422Z</updated>
        <summary type="html"><![CDATA[Weakly-supervised object detection (WSOD) has emerged as an inspiring recent
topic to avoid expensive instance-level object annotations. However, the
bounding boxes of most existing WSOD methods are mainly determined by
precomputed proposals, thereby being limited in precise object localization. In
this paper, we defend the problem setting for improving localization
performance by leveraging the bounding box regression knowledge from a
well-annotated auxiliary dataset. First, we use the well-annotated auxiliary
dataset to explore a series of learnable bounding box adjusters (LBBAs) in a
multi-stage training manner, which is class-agnostic. Then, only LBBAs and a
weakly-annotated dataset with non-overlapped classes are used for training
LBBA-boosted WSOD. As such, our LBBAs are practically more convenient and
economical to implement while avoiding the leakage of the auxiliary
well-annotated dataset. In particular, we formulate learning bounding box
adjusters as a bi-level optimization problem and suggest an EM-like multi-stage
training algorithm. Then, a multi-stage scheme is further presented for
LBBA-boosted WSOD. Additionally, a masking strategy is adopted to improve
proposal classification. Experimental results verify the effectiveness of our
method. Our method performs favorably against state-of-the-art WSOD methods and
knowledge transfer model with similar problem setting. Code is publicly
available at \url{https://github.com/DongSky/lbba_boosted_wsod}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dong_B/0/1/0/all/0/1"&gt;Bowen Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zitong Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1"&gt;Yuelin Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1"&gt;Qilong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niu_Z/0/1/0/all/0/1"&gt;Zhenxing Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zuo_W/0/1/0/all/0/1"&gt;Wangmeng Zuo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Domain Adaptor Networks for Hyperspectral Image Recognition. (arXiv:2108.01555v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.01555</id>
        <link href="http://arxiv.org/abs/2108.01555"/>
        <updated>2021-08-04T01:59:21.404Z</updated>
        <summary type="html"><![CDATA[We consider the problem of adapting a network trained on three-channel color
images to a hyperspectral domain with a large number of channels. To this end,
we propose domain adaptor networks that map the input to be compatible with a
network trained on large-scale color image datasets such as ImageNet. Adaptors
enable learning on small hyperspectral datasets where training a network from
scratch may not be effective. We investigate architectures and strategies for
training adaptors and evaluate them on a benchmark consisting of multiple
hyperspectral datasets. We find that simple schemes such as linear projection
or subset selection are often the most effective, but can lead to a loss in
performance in some cases. We also propose a novel multi-view adaptor where of
the inputs are combined in an intermediate layer of the network in an order
invariant manner that provides further improvements. We present extensive
experiments by varying the number of training examples in the benchmark to
characterize the accuracy and computational trade-offs offered by these
adaptors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Perez_G/0/1/0/all/0/1"&gt;Gustavo Perez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maji_S/0/1/0/all/0/1"&gt;Subhransu Maji&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Wavelet-Based Network For High Dynamic Range Imaging. (arXiv:2108.01434v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2108.01434</id>
        <link href="http://arxiv.org/abs/2108.01434"/>
        <updated>2021-08-04T01:59:21.386Z</updated>
        <summary type="html"><![CDATA[High dynamic range (HDR) imaging from multiple low dynamic range (LDR) images
has been suffering from ghosting artifacts caused by scene and objects motion.
Existing methods, such as optical flow based and end-to-end deep learning based
solutions, are error-prone either in detail restoration or ghosting artifacts
removal. Comprehensive empirical evidence shows that ghosting artifacts caused
by large foreground motion are mainly low-frequency signals and the details are
mainly high-frequency signals. In this work, we propose a novel
frequency-guided end-to-end deep neural network (FHDRNet) to conduct HDR fusion
in the frequency domain, and Discrete Wavelet Transform (DWT) is used to
decompose inputs into different frequency bands. The low-frequency signals are
used to avoid specific ghosting artifacts, while the high-frequency signals are
used for preserving details. Using a U-Net as the backbone, we propose two
novel modules: merging module and frequency-guided upsampling module. The
merging module applies the attention mechanism to the low-frequency components
to deal with the ghost caused by large foreground motion. The frequency-guided
upsampling module reconstructs details from multiple frequency-specific
components with rich details. In addition, a new RAW dataset is created for
training and evaluating multi-frame HDR imaging algorithms in the RAW domain.
Extensive experiments are conducted on public datasets and our RAW dataset,
showing that the proposed FHDRNet achieves state-of-the-art performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Dai_T/0/1/0/all/0/1"&gt;Tianhong Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Li_W/0/1/0/all/0/1"&gt;Wei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cao_X/0/1/0/all/0/1"&gt;Xilei Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jianzhuang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jia_X/0/1/0/all/0/1"&gt;Xu Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Leonardis_A/0/1/0/all/0/1"&gt;Ales Leonardis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yan_Y/0/1/0/all/0/1"&gt;Youliang Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yuan_S/0/1/0/all/0/1"&gt;Shanxin Yuan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Non-local Graph Convolutional Network for joint Activity Recognition and Motion Prediction. (arXiv:2108.01518v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.01518</id>
        <link href="http://arxiv.org/abs/2108.01518"/>
        <updated>2021-08-04T01:59:21.379Z</updated>
        <summary type="html"><![CDATA[3D skeleton-based motion prediction and activity recognition are two
interwoven tasks in human behaviour analysis. In this work, we propose a motion
context modeling methodology that provides a new way to combine the advantages
of both graph convolutional neural networks and recurrent neural networks for
joint human motion prediction and activity recognition. Our approach is based
on using an LSTM encoder-decoder and a non-local feature extraction attention
mechanism to model the spatial correlation of human skeleton data and temporal
correlation among motion frames. The proposed network can easily include two
output branches, one for Activity Recognition and one for Future Motion
Prediction, which can be jointly trained for enhanced performance. Experimental
results on Human 3.6M, CMU Mocap and NTU RGB-D datasets show that our proposed
approach provides the best prediction capability among baseline LSTM-based
methods, while achieving comparable performance to other state-of-the-art
methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1"&gt;Dianhao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vien_N/0/1/0/all/0/1"&gt;Ngo Anh Vien&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Van_M/0/1/0/all/0/1"&gt;Mien Van&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McLoone_S/0/1/0/all/0/1"&gt;Sean McLoone&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Continuous Non-Invasive Eye Tracking In Intensive Care. (arXiv:2108.01439v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.01439</id>
        <link href="http://arxiv.org/abs/2108.01439"/>
        <updated>2021-08-04T01:59:21.371Z</updated>
        <summary type="html"><![CDATA[Delirium, an acute confusional state, is a common occurrence in Intensive
Care Units (ICUs). Patients who develop delirium have globally worse outcomes
than those who do not and thus the diagnosis of delirium is of importance.
Current diagnostic methods have several limitations leading to the suggestion
of eye-tracking for its diagnosis through in-attention. To ascertain the
requirements for an eye-tracking system in an adult ICU, measurements were
carried out at Chelsea & Westminster Hospital NHS Foundation Trust. Clinical
criteria guided empirical requirements of invasiveness and calibration methods
while accuracy and precision were measured. A non-invasive system was then
developed utilising a patient-facing RGB-camera and a scene-facing RGBD-camera.
The system's performance was measured in a replicated laboratory environment
with healthy volunteers revealing an accuracy and precision that outperforms
what is required while simultaneously being non-invasive and calibration-free
The system was then deployed as part CONfuSED, a clinical feasibility study
where we report aggregated data from 5 patients as well as the acceptability of
the system to bedside nursing staff. The system is the first eye-tracking
system to be deployed in an ICU.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Al_Hindawi_A/0/1/0/all/0/1"&gt;Ahmed Al-Hindawi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vizcaychipi_M/0/1/0/all/0/1"&gt;Marcela Paula Vizcaychipi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Demiris_Y/0/1/0/all/0/1"&gt;Yiannis Demiris&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MixMicrobleedNet: segmentation of cerebral microbleeds using nnU-Net. (arXiv:2108.01389v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2108.01389</id>
        <link href="http://arxiv.org/abs/2108.01389"/>
        <updated>2021-08-04T01:59:21.339Z</updated>
        <summary type="html"><![CDATA[Cerebral microbleeds are small hypointense lesions visible on magnetic
resonance imaging (MRI) with gradient echo, T2*, or susceptibility weighted
(SWI) imaging. Assessment of cerebral microbleeds is mostly performed by visual
inspection. The past decade has seen the rise of semi-automatic tools to assist
with rating and more recently fully automatic tools for microbleed detection.
In this work, we explore the use of nnU-Net as a fully automated tool for
microbleed segmentation. Data was provided by the ``Where is VALDO?'' challenge
of MICCAI 2021. The final method consists of nnU-Net in the ``3D full
resolution U-Net'' configuration trained on all data (fold = `all'). No
post-processing options of nnU-Net were used. Self-evaluation on the training
data showed an estimated Dice of 0.80, false discovery rate of 0.16, and false
negative rate of 0.15. Final evaluation on the test set of the VALDO challenge
is pending. Visual inspection of the results showed that most of the reported
false positives could be an actual microbleed that might have been missed
during visual rating. Source code is available at:
https://github.com/hjkuijf/MixMicrobleedNet . The docker container
hjkuijf/mixmicrobleednet can be pulled from
https://hub.docker.com/r/hjkuijf/mixmicrobleednet .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Kuijf_H/0/1/0/all/0/1"&gt;Hugo J. Kuijf&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Noise-Resistant Deep Metric Learning with Probabilistic Instance Filtering. (arXiv:2108.01431v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.01431</id>
        <link href="http://arxiv.org/abs/2108.01431"/>
        <updated>2021-08-04T01:59:21.333Z</updated>
        <summary type="html"><![CDATA[Noisy labels are commonly found in real-world data, which cause performance
degradation of deep neural networks. Cleaning data manually is labour-intensive
and time-consuming. Previous research mostly focuses on enhancing
classification models against noisy labels, while the robustness of deep metric
learning (DML) against noisy labels remains less well-explored. In this paper,
we bridge this important gap by proposing Probabilistic Ranking-based Instance
Selection with Memory (PRISM) approach for DML. PRISM calculates the
probability of a label being clean, and filters out potentially noisy samples.
Specifically, we propose three methods to calculate this probability: 1)
Average Similarity Method (AvgSim), which calculates the average similarity
between potentially noisy data and clean data; 2) Proxy Similarity Method
(ProxySim), which replaces the centers maintained by AvgSim with the proxies
trained by proxy-based method; and 3) von Mises-Fisher Distribution Similarity
(vMF-Sim), which estimates a von Mises-Fisher distribution for each data class.
With such a design, the proposed approach can deal with challenging DML
situations in which the majority of the samples are noisy. Extensive
experiments on both synthetic and real-world noisy dataset show that the
proposed approach achieves up to 8.37% higher Precision@1 compared with the
best performing state-of-the-art baseline approaches, within reasonable
training time.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1"&gt;Chang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1"&gt;Han Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Boyang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1"&gt;Zhiqi Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1"&gt;Zhanning Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_P/0/1/0/all/0/1"&gt;Peiran Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1"&gt;Xuansong Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1"&gt;Lizhen Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Miao_C/0/1/0/all/0/1"&gt;Chunyan Miao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Region-wise Loss for Biomedical Image Segmentation. (arXiv:2108.01405v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2108.01405</id>
        <link href="http://arxiv.org/abs/2108.01405"/>
        <updated>2021-08-04T01:59:21.278Z</updated>
        <summary type="html"><![CDATA[We propose Region-wise (RW) loss for biomedical image segmentation.
Region-wise loss is versatile, can simultaneously account for class imbalance
and pixel importance, and it can be easily implemented as the pixel-wise
multiplication between the softmax output and a RW map. We show that, under the
proposed Region-wise loss framework, certain loss functions, such as Active
Contour and Boundary loss, can be reformulated similarly with appropriate RW
maps, thus revealing their underlying similarities and a new perspective to
understand these loss functions. We investigate the observed optimization
instability caused by certain RW maps, such as Boundary loss distance maps, and
we introduce a mathematically-grounded principle to avoid such instability.
This principle provides excellent adaptability to any dataset and practically
ensures convergence without extra regularization terms or optimization tricks.
Following this principle, we propose a simple version of boundary distance maps
called rectified RW maps that, as we demonstrate in our experiments, achieve
state-of-the-art performance with similar or better Dice coefficients and
Hausdorff distances than Dice, Focal, and Boundary losses in three distinct
segmentation tasks. We quantify the optimization instability provided by
Boundary loss distance maps, and we empirically show that our rectified RW maps
are stable to optimize. The code to run all our experiments is publicly
available at: https://github.com/jmlipman/RegionWiseLoss.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Valverde_J/0/1/0/all/0/1"&gt;Juan Miguel Valverde&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tohka_J/0/1/0/all/0/1"&gt;Jussi Tohka&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HyperColor: A HyperNetwork Approach for Synthesizing Auto-colored 3D Models for Game Scenes Population. (arXiv:2108.01411v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.01411</id>
        <link href="http://arxiv.org/abs/2108.01411"/>
        <updated>2021-08-04T01:59:21.271Z</updated>
        <summary type="html"><![CDATA[Designing a 3D game scene is a tedious task that often requires a substantial
amount of work. Typically, this task involves synthesis, coloring, and
placement of 3D models within the game scene. To lessen this workload, we can
apply machine learning to automate some aspects of the game scene development.
Earlier research has already tackled automated generation of the game scene
background with machine learning. However, model auto-coloring remains an
underexplored problem. The automatic coloring of a 3D model is a challenging
task, especially when dealing with the digital representation of a colorful,
multipart object. In such a case, we have to ``understand'' the object's
composition and coloring scheme of each part. Existing single-stage methods
have their own caveats such as the need for segmentation of the object or
generating individual parts that have to be assembled together to yield the
final model. We address these limitations by proposing a two-stage training
approach to synthesize auto-colored 3D models. In the first stage, we obtain a
3D point cloud representing a 3D object, whilst in the second stage, we assign
colors to points within such cloud. Next, by leveraging the so-called
triangulation trick, we generate a 3D mesh in which the surfaces are colored
based on interpolation of colored points representing vertices of a given mesh
triangle. This approach allows us to generate a smooth coloring scheme.
Experimental evaluation shows that our two-stage approach gives better results
in terms of shape reconstruction and coloring when compared to traditional
single-stage techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kostiuk_I/0/1/0/all/0/1"&gt;Ivan Kostiuk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stachura_P/0/1/0/all/0/1"&gt;Przemys&amp;#x142;aw Stachura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tadeja_S/0/1/0/all/0/1"&gt;S&amp;#x142;awomir K. Tadeja&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Trzcinski_T/0/1/0/all/0/1"&gt;Tomasz Trzci&amp;#x144;ski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Spurek_P/0/1/0/all/0/1"&gt;Przemys&amp;#x142;aw Spurek&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI Based Waste classifier with Thermo-Rapid Composting. (arXiv:2108.01394v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.01394</id>
        <link href="http://arxiv.org/abs/2108.01394"/>
        <updated>2021-08-04T01:59:21.263Z</updated>
        <summary type="html"><![CDATA[Waste management is a certainly a very complex and difficult process
especially in very large cities. It needs immense man power and also uses up
other resources such as electricity and fuel. This creates a need to use a
novel method with help of latest technologies. Here in this article we present
a new waste classification technique using Computer Vision (CV) and deep
learning (DL). To further improve waste classification ability, support machine
vectors (SVM) are used. We also decompose the degradable waste with help of
rapid composting. In this article we have mainly worked on segregation of
municipal solid waste (MSW). For this model, we use YOLOv3 (You Only Look Once)
a computer vision-based algorithm popularly used to detect objects which is
developed based on Convolution Neural Networks (CNNs) which is a machine
learning (ML) based tool. They are extensively used to extract features from a
data especially image-oriented data. In this article we propose a waste
classification technique which will be faster and more efficient. And we
decompose the biodegradable waste by Berkley Method of composting (BKC)]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+behera_S/0/1/0/all/0/1"&gt;Saswati kumari behera&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Y_A/0/1/0/all/0/1"&gt;Aouthithiye Barathwaj SR Y&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+L_V/0/1/0/all/0/1"&gt;Vasundhara L&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+G_S/0/1/0/all/0/1"&gt;Saisudha G&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+C_H/0/1/0/all/0/1"&gt;Haariharan N C&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evo-ViT: Slow-Fast Token Evolution for Dynamic Vision Transformer. (arXiv:2108.01390v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.01390</id>
        <link href="http://arxiv.org/abs/2108.01390"/>
        <updated>2021-08-04T01:59:21.256Z</updated>
        <summary type="html"><![CDATA[Vision transformers have recently received explosive popularity, but huge
computational cost is still a severe issue. Recent efficient designs for vision
transformers follow two pipelines, namely, structural compression based on
local spatial prior and non-structural token pruning. However, rough token
pruning breaks the spatial structure that is indispensable for local spatial
prior. To take advantage of both two pipelines, this work seeks to dynamically
identify uninformative tokens for each instance and trim down both the training
and inference complexity while maintain complete spatial structure and
information flow. To achieve this goal, we propose Evo-ViT, a self-motivated
slow-fast token evolution method for vision transformers. Specifically, we
conduct unstructured instance-wise token selection by taking advantage of the
global class attention that is unique to vision transformers. Then, we propose
to update information tokens and placeholder tokens that contribute little to
the final prediction with different computational properties, namely, slow-fast
updating. Thanks to the slow-fast updating mechanism that guarantees
information flow and spatial structure, our Evo-ViT can accelerate vanilla
transformers of both flat and deep-narrow structures from the very beginning of
the training process. Experimental results demonstrate that the proposed method
can significantly reduce the computational costs of vision transformers while
maintaining comparable performance on image classification. For example, our
method accelerates DeiT-S by over 60% throughput while only sacrificing 0.4%
top-1 accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yifan Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhijie Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Mengdan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sheng_K/0/1/0/all/0/1"&gt;Kekai Sheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1"&gt;Ke Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_W/0/1/0/all/0/1"&gt;Weiming Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Liqing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Changsheng Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1"&gt;Xing Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Predicting Popularity of Images Over 30 Days. (arXiv:2108.01326v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.01326</id>
        <link href="http://arxiv.org/abs/2108.01326"/>
        <updated>2021-08-04T01:59:21.241Z</updated>
        <summary type="html"><![CDATA[The current work deals with the problem of attempting to predict the
popularity of images before even being uploaded. This method is specifically
focused on Flickr images. Social features of each image as well as that of the
user who had uploaded it, have been recorded. The dataset also includes the
engagement score of each image which is the ground truth value of the views
obtained by each image over a period of 30 days. The work aims to predict the
popularity of images on Flickr over a period of 30 days using the social
features of the user and the image, as well as the visual features of the
images. The method states that the engagement sequence of an image can be said
to depend on two independent quantities, namely scale and shape of an image.
Once the shape and scale of an image have been predicted, combining them the
predicted sequence of an image over 30 days is obtained. The current work
follows a previous work done in the same direction, with certain speculations
and suggestions of improvement.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dutta_A/0/1/0/all/0/1"&gt;Amartya Dutta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barbhuiya_F/0/1/0/all/0/1"&gt;Ferdous Ahmed Barbhuiya&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Classifying action correctness in physical rehabilitation exercises. (arXiv:2108.01375v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.01375</id>
        <link href="http://arxiv.org/abs/2108.01375"/>
        <updated>2021-08-04T01:59:21.235Z</updated>
        <summary type="html"><![CDATA[The work in this paper focuses on the role of machine learning in assessing
the correctness of a human motion or action. This task proves to be more
challenging than the gesture and action recognition ones. We will demonstrate,
through a set of experiments on a recent dataset, that machine learning
algorithms can produce good results for certain actions, but can also fall into
the trap of classifying an incorrect execution of an action as a correct
execution of another action.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Miron_A/0/1/0/all/0/1"&gt;Alina Miron&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grosan_C/0/1/0/all/0/1"&gt;Crina Grosan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adaptive Affinity Loss and Erroneous Pseudo-Label Refinement for Weakly Supervised Semantic Segmentation. (arXiv:2108.01344v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.01344</id>
        <link href="http://arxiv.org/abs/2108.01344"/>
        <updated>2021-08-04T01:59:21.228Z</updated>
        <summary type="html"><![CDATA[Semantic segmentation has been continuously investigated in the last ten
years, and majority of the established technologies are based on supervised
models. In recent years, image-level weakly supervised semantic segmentation
(WSSS), including single- and multi-stage process, has attracted large
attention due to data labeling efficiency. In this paper, we propose to embed
affinity learning of multi-stage approaches in a single-stage model. To be
specific, we introduce an adaptive affinity loss to thoroughly learn the local
pairwise affinity. As such, a deep neural network is used to deliver
comprehensive semantic information in the training phase, whilst improving the
performance of the final prediction module. On the other hand, considering the
existence of errors in the pseudo labels, we propose a novel label reassign
loss to mitigate over-fitting. Extensive experiments are conducted on the
PASCAL VOC 2012 dataset to evaluate the effectiveness of our proposed approach
that outperforms other standard single-stage methods and achieves comparable
performance against several multi-stage methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiangrong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_Z/0/1/0/all/0/1"&gt;Zelin Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_P/0/1/0/all/0/1"&gt;Peng Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1"&gt;Tianyang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1"&gt;Huiyu Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiao_L/0/1/0/all/0/1"&gt;Licheng Jiao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cycle-Consistent Inverse GAN for Text-to-Image Synthesis. (arXiv:2108.01361v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.01361</id>
        <link href="http://arxiv.org/abs/2108.01361"/>
        <updated>2021-08-04T01:59:21.213Z</updated>
        <summary type="html"><![CDATA[This paper investigates an open research task of text-to-image synthesis for
automatically generating or manipulating images from text descriptions.
Prevailing methods mainly use the text as conditions for GAN generation, and
train different models for the text-guided image generation and manipulation
tasks. In this paper, we propose a novel unified framework of Cycle-consistent
Inverse GAN (CI-GAN) for both text-to-image generation and text-guided image
manipulation tasks. Specifically, we first train a GAN model without text
input, aiming to generate images with high diversity and quality. Then we learn
a GAN inversion model to convert the images back to the GAN latent space and
obtain the inverted latent codes for each image, where we introduce the
cycle-consistency training to learn more robust and consistent inverted latent
codes. We further uncover the latent space semantics of the trained GAN model,
by learning a similarity model between text representations and the latent
codes. In the text-guided optimization module, we generate images with the
desired semantic attributes by optimizing the inverted latent codes. Extensive
experiments on the Recipe1M and CUB datasets validate the efficacy of our
proposed framework.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1"&gt;Guosheng Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoi_S/0/1/0/all/0/1"&gt;Steven C. H. Hoi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Miao_C/0/1/0/all/0/1"&gt;Chunyan Miao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Where do Models go Wrong? Parameter-Space Saliency Maps for Explainability. (arXiv:2108.01335v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.01335</id>
        <link href="http://arxiv.org/abs/2108.01335"/>
        <updated>2021-08-04T01:59:21.206Z</updated>
        <summary type="html"><![CDATA[Conventional saliency maps highlight input features to which neural network
predictions are highly sensitive. We take a different approach to saliency, in
which we identify and analyze the network parameters, rather than inputs, which
are responsible for erroneous decisions. We find that samples which cause
similar parameters to malfunction are semantically similar. We also show that
pruning the most salient parameters for a wrongly classified sample often
improves model behavior. Furthermore, fine-tuning a small number of the most
salient parameters on a single sample results in error correction on other
samples that are misclassified for similar reasons. Based on our parameter
saliency method, we also introduce an input-space saliency technique that
reveals how image features cause specific network components to malfunction.
Further, we rigorously validate the meaningfulness of our saliency maps on both
the dataset and case-study levels.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Levin_R/0/1/0/all/0/1"&gt;Roman Levin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shu_M/0/1/0/all/0/1"&gt;Manli Shu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Borgnia_E/0/1/0/all/0/1"&gt;Eitan Borgnia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1"&gt;Furong Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldblum_M/0/1/0/all/0/1"&gt;Micah Goldblum&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1"&gt;Tom Goldstein&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Skeleton Split Strategies for Spatial Temporal Graph Convolution Networks. (arXiv:2108.01309v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.01309</id>
        <link href="http://arxiv.org/abs/2108.01309"/>
        <updated>2021-08-04T01:59:21.185Z</updated>
        <summary type="html"><![CDATA[A skeleton representation of the human body has been proven to be effective
for this task. The skeletons are presented in graphs form-like. However, the
topology of a graph is not structured like Euclidean-based data. Therefore, a
new set of methods to perform the convolution operation upon the skeleton graph
is presented. Our proposal is based upon the ST-GCN framework proposed by Yan
et al. [1]. In this study, we present an improved set of label mapping methods
for the ST-GCN framework. We introduce three split processes (full distance
split, connection split, and index split) as an alternative approach for the
convolution operation. To evaluate the performance, the experiments presented
in this study have been trained using two benchmark datasets: NTU-RGB+D and
Kinetics. Our results indicate that all of our split processes outperform the
previous partition strategies and are more stable during training without using
the edge importance weighting additional training parameter. Therefore, our
proposal can provide a more realistic solution for real-time applications
centred on daily living recognition systems activities for indoor environments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alsawadi_M/0/1/0/all/0/1"&gt;Motasem S. Alsawadi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rio_M/0/1/0/all/0/1"&gt;Miguel Rio&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Rival Penalized Competitive Learning for Low-resolution Face Recognition. (arXiv:2108.01286v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.01286</id>
        <link href="http://arxiv.org/abs/2108.01286"/>
        <updated>2021-08-04T01:59:21.169Z</updated>
        <summary type="html"><![CDATA[Current face recognition tasks are usually carried out on high-quality face
images, but in reality, most face images are captured under unconstrained or
poor conditions, e.g., by video surveillance. Existing methods are featured by
learning data uncertainty to avoid overfitting the noise, or by adding margins
to the angle or cosine space of the normalized softmax loss to penalize the
target logit, which enforces intra-class compactness and inter-class
discrepancy. In this paper, we propose a deep Rival Penalized Competitive
Learning (RPCL) for deep face recognition in low-resolution (LR) images.
Inspired by the idea of the RPCL, our method further enforces regulation on the
rival logit, which is defined as the largest non-target logit for an input
image. Different from existing methods that only consider penalization on the
target logit, our method not only strengthens the learning towards the target
label, but also enforces a reverse direction, i.e., becoming de-learning, away
from the rival label. Comprehensive experiments demonstrate that our method
improves the existing state-of-the-art methods to be very robust for LR face
recognition.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1"&gt;Peiying Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tu_S/0/1/0/all/0/1"&gt;Shikui Tu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1"&gt;Lei Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Toward Spatially Unbiased Generative Models. (arXiv:2108.01285v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.01285</id>
        <link href="http://arxiv.org/abs/2108.01285"/>
        <updated>2021-08-04T01:59:21.163Z</updated>
        <summary type="html"><![CDATA[Recent image generation models show remarkable generation performance.
However, they mirror strong location preference in datasets, which we call
spatial bias. Therefore, generators render poor samples at unseen locations and
scales. We argue that the generators rely on their implicit positional encoding
to render spatial content. From our observations, the generator's implicit
positional encoding is translation-variant, making the generator spatially
biased. To address this issue, we propose injecting explicit positional
encoding at each scale of the generator. By learning the spatially unbiased
generator, we facilitate the robust use of generators in multiple tasks, such
as GAN inversion, multi-scale generation, generation of arbitrary sizes and
aspect ratios. Furthermore, we show that our method can also be applied to
denoising diffusion probabilistic models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1"&gt;Jooyoung Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1"&gt;Jungbeom Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jeong_Y/0/1/0/all/0/1"&gt;Yonghyun Jeong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1"&gt;Sungroh Yoon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RAIN: Reinforced Hybrid Attention Inference Network for Motion Forecasting. (arXiv:2108.01316v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.01316</id>
        <link href="http://arxiv.org/abs/2108.01316"/>
        <updated>2021-08-04T01:59:21.152Z</updated>
        <summary type="html"><![CDATA[Motion forecasting plays a significant role in various domains (e.g.,
autonomous driving, human-robot interaction), which aims to predict future
motion sequences given a set of historical observations. However, the observed
elements may be of different levels of importance. Some information may be
irrelevant or even distracting to the forecasting in certain situations. To
address this issue, we propose a generic motion forecasting framework (named
RAIN) with dynamic key information selection and ranking based on a hybrid
attention mechanism. The general framework is instantiated to handle
multi-agent trajectory prediction and human motion forecasting tasks,
respectively. In the former task, the model learns to recognize the relations
between agents with a graph representation and to determine their relative
significance. In the latter task, the model learns to capture the temporal
proximity and dependency in long-term human motions. We also propose an
effective double-stage training pipeline with an alternating training strategy
to optimize the parameters in different modules of the framework. We validate
the framework on both synthetic simulations and motion forecasting benchmarks
in different domains, demonstrating that our method not only achieves
state-of-the-art forecasting performance, but also provides interpretable and
reasonable hybrid attention weights.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jiachen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1"&gt;Fan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1"&gt;Hengbo Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Malla_S/0/1/0/all/0/1"&gt;Srikanth Malla&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tomizuka_M/0/1/0/all/0/1"&gt;Masayoshi Tomizuka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_C/0/1/0/all/0/1"&gt;Chiho Choi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dynamic Feature Regularized Loss for Weakly Supervised Semantic Segmentation. (arXiv:2108.01296v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.01296</id>
        <link href="http://arxiv.org/abs/2108.01296"/>
        <updated>2021-08-04T01:59:21.133Z</updated>
        <summary type="html"><![CDATA[We focus on tackling weakly supervised semantic segmentation with
scribble-level annotation. The regularized loss has been proven to be an
effective solution for this task. However, most existing regularized losses
only leverage static shallow features (color, spatial information) to compute
the regularized kernel, which limits its final performance since such static
shallow features fail to describe pair-wise pixel relationship in complicated
cases. In this paper, we propose a new regularized loss which utilizes both
shallow and deep features that are dynamically updated in order to aggregate
sufficient information to represent the relationship of different pixels.
Moreover, in order to provide accurate deep features, we adopt vision
transformer as the backbone and design a feature consistency head to train the
pair-wise feature relationship. Unlike most approaches that adopt multi-stage
training strategy with many bells and whistles, our approach can be directly
trained in an end-to-end manner, in which the feature consistency head and our
regularized loss can benefit from each other. Extensive experiments show that
our approach achieves new state-of-the-art performances, outperforming other
approaches by a significant margin with more than 6\% mIoU increase.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1"&gt;Bingfeng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1"&gt;Jimin Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yao Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SABER: Data-Driven Motion Planner for Autonomously Navigating Heterogeneous Robots. (arXiv:2108.01262v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2108.01262</id>
        <link href="http://arxiv.org/abs/2108.01262"/>
        <updated>2021-08-04T01:59:21.121Z</updated>
        <summary type="html"><![CDATA[We present an end-to-end online motion planning framework that uses a
data-driven approach to navigate a heterogeneous robot team towards a global
goal while avoiding obstacles in uncertain environments. First, we use
stochastic model predictive control (SMPC) to calculate control inputs that
satisfy robot dynamics, and consider uncertainty during obstacle avoidance with
chance constraints. Second, recurrent neural networks are used to provide a
quick estimate of future state uncertainty considered in the SMPC finite-time
horizon solution, which are trained on uncertainty outputs of various
simultaneous localization and mapping algorithms. When two or more robots are
in communication range, these uncertainties are then updated using a
distributed Kalman filtering approach. Lastly, a Deep Q-learning agent is
employed to serve as a high-level path planner, providing the SMPC with target
positions that move the robots towards a desired global goal. Our complete
methods are demonstrated on a ground and aerial robot simultaneously (code
available at: https://github.com/AlexS28/SABER).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schperberg_A/0/1/0/all/0/1"&gt;Alexander Schperberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsuei_S/0/1/0/all/0/1"&gt;Stephanie Tsuei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1"&gt;Stefano Soatto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hong_D/0/1/0/all/0/1"&gt;Dennis Hong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pre-trained Models for Sonar Images. (arXiv:2108.01111v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.01111</id>
        <link href="http://arxiv.org/abs/2108.01111"/>
        <updated>2021-08-04T01:59:21.106Z</updated>
        <summary type="html"><![CDATA[Machine learning and neural networks are now ubiquitous in sonar perception,
but it lags behind the computer vision field due to the lack of data and
pre-trained models specifically for sonar images. In this paper we present the
Marine Debris Turntable dataset and produce pre-trained neural networks trained
on this dataset, meant to fill the gap of missing pre-trained models for sonar
images. We train Resnet 20, MobileNets, DenseNet121, SqueezeNet, MiniXception,
and an Autoencoder, over several input image sizes, from 32 x 32 to 96 x 96, on
the Marine Debris turntable dataset. We evaluate these models using transfer
learning for low-shot classification in the Marine Debris Watertank and another
dataset captured using a Gemini 720i sonar. Our results show that in both
datasets the pre-trained models produce good features that allow good
classification accuracy with low samples (10-30 samples per class). The Gemini
dataset validates that the features transfer to other kinds of sonar sensors.
We expect that the community benefits from the public release of our
pre-trained models and the turntable dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Valdenegro_Toro_M/0/1/0/all/0/1"&gt;Matias Valdenegro-Toro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Preciado_Grijalva_A/0/1/0/all/0/1"&gt;Alan Preciado-Grijalva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wehbe_B/0/1/0/all/0/1"&gt;Bilal Wehbe&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AGAR a microbial colony dataset for deep learning detection. (arXiv:2108.01234v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.01234</id>
        <link href="http://arxiv.org/abs/2108.01234"/>
        <updated>2021-08-04T01:59:21.038Z</updated>
        <summary type="html"><![CDATA[The Annotated Germs for Automated Recognition (AGAR) dataset is an image
database of microbial colonies cultured on agar plates. It contains 18000
photos of five different microorganisms as single or mixed cultures, taken
under diverse lighting conditions with two different cameras. All the images
are classified into "countable", "uncountable", and "empty", with the
"countable" class labeled by microbiologists with colony location and species
identification (336442 colonies in total). This study describes the dataset
itself and the process of its development. In the second part, the performance
of selected deep neural network architectures for object detection, namely
Faster R-CNN and Cascade R-CNN, was evaluated on the AGAR dataset. The results
confirmed the great potential of deep learning methods to automate the process
of microbe localization and classification based on Petri dish photos.
Moreover, AGAR is the first publicly available dataset of this kind and size
and will facilitate the future development of machine learning models. The data
used in these studies can be found at https://agar.neurosys.com/.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Majchrowska_S/0/1/0/all/0/1"&gt;Sylwia Majchrowska&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pawlowski_J/0/1/0/all/0/1"&gt;Jaros&amp;#x142;aw Paw&amp;#x142;owski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gula_G/0/1/0/all/0/1"&gt;Grzegorz Gu&amp;#x142;a&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bonus_T/0/1/0/all/0/1"&gt;Tomasz Bonus&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hanas_A/0/1/0/all/0/1"&gt;Agata Hanas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Loch_A/0/1/0/all/0/1"&gt;Adam Loch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pawlak_A/0/1/0/all/0/1"&gt;Agnieszka Pawlak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roszkowiak_J/0/1/0/all/0/1"&gt;Justyna Roszkowiak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Golan_T/0/1/0/all/0/1"&gt;Tomasz Golan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Drulis_Kawa_Z/0/1/0/all/0/1"&gt;Zuzanna Drulis-Kawa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AcousticFusion: Fusing Sound Source Localization to Visual SLAM in Dynamic Environments. (arXiv:2108.01246v1 [cs.RO])]]></title>
        <id>http://arxiv.org/abs/2108.01246</id>
        <link href="http://arxiv.org/abs/2108.01246"/>
        <updated>2021-08-04T01:59:21.031Z</updated>
        <summary type="html"><![CDATA[Dynamic objects in the environment, such as people and other agents, lead to
challenges for existing simultaneous localization and mapping (SLAM)
approaches. To deal with dynamic environments, computer vision researchers
usually apply some learning-based object detectors to remove these dynamic
objects. However, these object detectors are computationally too expensive for
mobile robot on-board processing. In practical applications, these objects
output noisy sounds that can be effectively detected by on-board sound source
localization. The directional information of the sound source object can be
efficiently obtained by direction of sound arrival (DoA) estimation, but depth
estimation is difficult. Therefore, in this paper, we propose a novel
audio-visual fusion approach that fuses sound source direction into the RGB-D
image and thus removes the effect of dynamic obstacles on the multi-robot SLAM
system. Experimental results of multi-robot SLAM in different dynamic
environments show that the proposed method uses very small computational
resources to obtain very stable self-localization results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1"&gt;Tianwei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Huayan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiaofei Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Junfeng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lam_T/0/1/0/all/0/1"&gt;Tin Lun Lam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vijayakumar_S/0/1/0/all/0/1"&gt;Sethu Vijayakumar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generating Master Faces for Dictionary Attacks with a Network-Assisted Latent Space Evolution. (arXiv:2108.01077v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2108.01077</id>
        <link href="http://arxiv.org/abs/2108.01077"/>
        <updated>2021-08-04T01:59:21.024Z</updated>
        <summary type="html"><![CDATA[A master face is a face image that passes face-based identity-authentication
for a large portion of the population. These faces can be used to impersonate,
with a high probability of success, any user, without having access to any user
information. We optimize these faces, by using an evolutionary algorithm in the
latent embedding space of the StyleGAN face generator. Multiple evolutionary
strategies are compared, and we propose a novel approach that employs a neural
network in order to direct the search in the direction of promising samples,
without adding fitness evaluations. The results we present demonstrate that it
is possible to obtain a high coverage of the population (over 40%) with less
than 10 master faces, for three leading deep face recognition systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shmelkin_R/0/1/0/all/0/1"&gt;Ron Shmelkin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Friedlander_T/0/1/0/all/0/1"&gt;Tomer Friedlander&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wolf_L/0/1/0/all/0/1"&gt;Lior Wolf&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Elastic Architecture Search for Diverse Tasks with Different Resources. (arXiv:2108.01224v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.01224</id>
        <link href="http://arxiv.org/abs/2108.01224"/>
        <updated>2021-08-04T01:59:21.017Z</updated>
        <summary type="html"><![CDATA[We study a new challenging problem of efficient deployment for diverse tasks
with different resources, where the resource constraint and task of interest
corresponding to a group of classes are dynamically specified at testing time.
Previous NAS approaches seek to design architectures for all classes
simultaneously, which may not be optimal for some individual tasks. A
straightforward solution is to search an architecture from scratch for each
deployment scenario, which however is computation-intensive and impractical. To
address this, we present a novel and general framework, called Elastic
Architecture Search (EAS), permitting instant specializations at runtime for
diverse tasks with various resource constraints. To this end, we first propose
to effectively train the over-parameterized network via a task dropout strategy
to disentangle the tasks during training. In this way, the resulting model is
robust to the subsequent task dropping at inference time. Based on the
well-trained over-parameterized network, we then propose an efficient
architecture generator to obtain optimal architectures within a single forward
pass. Experiments on two image classification datasets show that EAS is able to
find more compact networks with better performance while remarkably being
orders of magnitude faster than state-of-the-art NAS methods. For example, our
proposed EAS finds compact architectures within 0.1 second for 50 deployment
scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jing Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhuang_B/0/1/0/all/0/1"&gt;Bohan Zhuang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1"&gt;Mingkui Tan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xu Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Phung_D/0/1/0/all/0/1"&gt;Dinh Phung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yuanqing Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1"&gt;Jianfei Cai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Consistent Depth of Moving Objects in Video. (arXiv:2108.01166v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.01166</id>
        <link href="http://arxiv.org/abs/2108.01166"/>
        <updated>2021-08-04T01:59:20.891Z</updated>
        <summary type="html"><![CDATA[We present a method to estimate depth of a dynamic scene, containing
arbitrary moving objects, from an ordinary video captured with a moving camera.
We seek a geometrically and temporally consistent solution to this
underconstrained problem: the depth predictions of corresponding points across
frames should induce plausible, smooth motion in 3D. We formulate this
objective in a new test-time training framework where a depth-prediction CNN is
trained in tandem with an auxiliary scene-flow prediction MLP over the entire
input video. By recursively unrolling the scene-flow prediction MLP over
varying time steps, we compute both short-range scene flow to impose local
smooth motion priors directly in 3D, and long-range scene flow to impose
multi-view consistency constraints with wide baselines. We demonstrate accurate
and temporally coherent results on a variety of challenging videos containing
diverse moving objects (pets, people, cars), as well as camera motion. Our
depth maps give rise to a number of depth-and-motion aware video editing
effects such as object and lighting insertion.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhoutong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cole_F/0/1/0/all/0/1"&gt;Forrester Cole&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tucker_R/0/1/0/all/0/1"&gt;Richard Tucker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Freeman_W/0/1/0/all/0/1"&gt;William T. Freeman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dekel_T/0/1/0/all/0/1"&gt;Tali Dekel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Image Representations for Multi-Image Fusion and Layer Separation. (arXiv:2108.01199v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.01199</id>
        <link href="http://arxiv.org/abs/2108.01199"/>
        <updated>2021-08-04T01:59:20.852Z</updated>
        <summary type="html"><![CDATA[We propose a framework for aligning and fusing multiple images into a single
coordinate-based neural representations. Our framework targets burst images
that have misalignment due to camera ego motion and small changes in the scene.
We describe different strategies for alignment depending on the assumption of
the scene motion, namely, perspective planar (i.e., homography), optical flow
with minimal scene change, and optical flow with notable occlusion and
disocclusion. Our framework effectively combines the multiple inputs into a
single neural implicit function without the need for selecting one of the
images as a reference frame. We demonstrate how to use this multi-frame fusion
framework for various layer separation tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nam_S/0/1/0/all/0/1"&gt;Seonghyeon Nam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brubaker_M/0/1/0/all/0/1"&gt;Marcus A. Brubaker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brown_M/0/1/0/all/0/1"&gt;Michael S. Brown&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A computational geometry approach for modeling neuronal fiber pathways. (arXiv:2108.01175v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.01175</id>
        <link href="http://arxiv.org/abs/2108.01175"/>
        <updated>2021-08-04T01:59:20.844Z</updated>
        <summary type="html"><![CDATA[We propose a novel and efficient algorithm to model high-level topological
structures of neuronal fibers. Tractography constructs complex neuronal fibers
in three dimensions that exhibit the geometry of white matter pathways in the
brain. However, most tractography analysis methods are time consuming and
intractable. We develop a computational geometry-based tractography
representation that aims to simplify the connectivity of white matter fibers.
Given the trajectories of neuronal fiber pathways, we model the evolution of
trajectories that encodes geometrically significant events and calculate their
point correspondence in the 3D brain space. Trajectory inter-distance is used
as a parameter to control the granularity of the model that allows local or
global representation of the tractogram. Using diffusion MRI data from
Alzheimer's patient study, we extract tractography features from our model for
distinguishing the Alzheimer's subject from the normal control. Software
implementation of our algorithm is available on GitHub.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shailja_S/0/1/0/all/0/1"&gt;S. Shailja&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1"&gt;Angela Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Manjunath_B/0/1/0/all/0/1"&gt;B.S. Manjunath&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Direction is what you need: Improving Word Embedding Compression in Large Language Models. (arXiv:2106.08181v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.08181</id>
        <link href="http://arxiv.org/abs/2106.08181"/>
        <updated>2021-08-04T01:59:20.836Z</updated>
        <summary type="html"><![CDATA[The adoption of Transformer-based models in natural language processing (NLP)
has led to great success using a massive number of parameters. However, due to
deployment constraints in edge devices, there has been a rising interest in the
compression of these models to improve their inference time and memory
footprint. This paper presents a novel loss objective to compress token
embeddings in the Transformer-based models by leveraging an AutoEncoder
architecture. More specifically, we emphasize the importance of the direction
of compressed embeddings with respect to original uncompressed embeddings. The
proposed method is task-agnostic and does not require further language modeling
pre-training. Our method significantly outperforms the commonly used SVD-based
matrix-factorization approach in terms of initial language model Perplexity.
Moreover, we evaluate our proposed approach over SQuAD v1.1 dataset and several
downstream tasks from the GLUE benchmark, where we also outperform the baseline
in most scenarios. Our code is public.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Balazy_K/0/1/0/all/0/1"&gt;Klaudia Ba&amp;#x142;azy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Banaei_M/0/1/0/all/0/1"&gt;Mohammadreza Banaei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lebret_R/0/1/0/all/0/1"&gt;R&amp;#xe9;mi Lebret&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tabor_J/0/1/0/all/0/1"&gt;Jacek Tabor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aberer_K/0/1/0/all/0/1"&gt;Karl Aberer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Boundary Knowledge Translation based Reference Semantic Segmentation. (arXiv:2108.01075v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.01075</id>
        <link href="http://arxiv.org/abs/2108.01075"/>
        <updated>2021-08-04T01:59:20.828Z</updated>
        <summary type="html"><![CDATA[Given a reference object of an unknown type in an image, human observers can
effortlessly find the objects of the same category in another image and
precisely tell their visual boundaries. Such visual cognition capability of
humans seems absent from the current research spectrum of computer vision.
Existing segmentation networks, for example, rely on a humongous amount of
labeled data, which is laborious and costly to collect and annotate; besides,
the performance of segmentation networks tend to downgrade as the number of the
category increases. In this paper, we introduce a novel Reference semantic
segmentation Network (Ref-Net) to conduct visual boundary knowledge
translation. Ref-Net contains a Reference Segmentation Module (RSM) and a
Boundary Knowledge Translation Module (BKTM). Inspired by the human recognition
mechanism, RSM is devised only to segment the same category objects based on
the features of the reference objects. BKTM, on the other hand, introduces two
boundary discriminator branches to conduct inner and outer boundary
segmentation of the target objectin an adversarial manner, and translate the
annotated boundary knowledge of open-source datasets into the segmentation
network. Exhaustive experiments demonstrate that, with tens of finely-grained
annotated samples as guidance, Ref-Net achieves results on par with fully
supervised methods on six datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1"&gt;Lechao Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1"&gt;Zunlei Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xinchao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Ya Jie Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1"&gt;Jie Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_M/0/1/0/all/0/1"&gt;Mingli Song&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cross-utterance Reranking Models with BERT and Graph Convolutional Networks for Conversational Speech Recognition. (arXiv:2106.06922v4 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.06922</id>
        <link href="http://arxiv.org/abs/2106.06922"/>
        <updated>2021-08-04T01:59:20.791Z</updated>
        <summary type="html"><![CDATA[How to effectively incorporate cross-utterance information cues into a neural
language model (LM) has emerged as one of the intriguing issues for automatic
speech recognition (ASR). Existing research efforts on improving
contextualization of an LM typically regard previous utterances as a sequence
of additional input and may fail to capture complex global structural
dependencies among these utterances. In view of this, we in this paper seek to
represent the historical context information of an utterance as
graph-structured data so as to distill cross-utterances, global word
interaction relationships. To this end, we apply a graph convolutional network
(GCN) on the resulting graph to obtain the corresponding GCN embeddings of
historical words. GCN has recently found its versatile applications on
social-network analysis, text summarization, and among others due mainly to its
ability of effectively capturing rich relational information among elements.
However, GCN remains largely underexplored in the context of ASR, especially
for dealing with conversational speech. In addition, we frame ASR N-best
reranking as a prediction problem, leveraging bidirectional encoder
representations from transformers (BERT) as the vehicle to not only seize the
local intrinsic word regularity patterns inherent in a candidate hypothesis but
also incorporate the cross-utterance, historical word interaction cues
distilled by GCN for promoting performance. Extensive experiments conducted on
the AMI benchmark dataset seem to confirm the pragmatic utility of our methods,
in relation to some current top-of-the-line methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chiu_S/0/1/0/all/0/1"&gt;Shih-Hsuan Chiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lo_T/0/1/0/all/0/1"&gt;Tien-Hong Lo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chao_F/0/1/0/all/0/1"&gt;Fu-An Chao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1"&gt;Berlin Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-type Disentanglement without Adversarial Training. (arXiv:2012.08883v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.08883</id>
        <link href="http://arxiv.org/abs/2012.08883"/>
        <updated>2021-08-04T01:59:20.743Z</updated>
        <summary type="html"><![CDATA[Controlling the style of natural language by disentangling the latent space
is an important step towards interpretable machine learning. After the latent
space is disentangled, the style of a sentence can be transformed by tuning the
style representation without affecting other features of the sentence. Previous
works usually use adversarial training to guarantee that disentangled vectors
do not affect each other. However, adversarial methods are difficult to train.
Especially when there are multiple features (e.g., sentiment, or tense, which
we call style types in this paper), each feature requires a separate
discriminator for extracting a disentangled style vector corresponding to that
feature. In this paper, we propose a unified distribution-controlling method,
which provides each specific style value (the value of style types, e.g.,
positive sentiment, or past tense) with a unique representation. This method
contributes a solid theoretical basis to avoid adversarial training in
multi-type disentanglement. We also propose multiple loss functions to achieve
a style-content disentanglement as well as a disentanglement among multiple
style types. In addition, we observe that if two different style types always
have some specific style values that occur together in the dataset, they will
affect each other when transferring the style values. We call this phenomenon
training bias, and we propose a loss function to alleviate such training bias
while disentangling multiple types. We conduct experiments on two datasets
(Yelp service reviews and Amazon product reviews) to evaluate the
style-disentangling effect and the unsupervised style transfer performance on
two style types: sentiment and tense. The experimental results show the
effectiveness of our model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sha_L/0/1/0/all/0/1"&gt;Lei Sha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lukasiewicz_T/0/1/0/all/0/1"&gt;Thomas Lukasiewicz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Greedy Gradient Ensemble for Robust Visual Question Answering. (arXiv:2107.12651v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.12651</id>
        <link href="http://arxiv.org/abs/2107.12651"/>
        <updated>2021-08-04T01:59:20.736Z</updated>
        <summary type="html"><![CDATA[Language bias is a critical issue in Visual Question Answering (VQA), where
models often exploit dataset biases for the final decision without considering
the image information. As a result, they suffer from performance drop on
out-of-distribution data and inadequate visual explanation. Based on
experimental analysis for existing robust VQA methods, we stress the language
bias in VQA that comes from two aspects, i.e., distribution bias and shortcut
bias. We further propose a new de-bias framework, Greedy Gradient Ensemble
(GGE), which combines multiple biased models for unbiased base model learning.
With the greedy strategy, GGE forces the biased models to over-fit the biased
data distribution in priority, thus makes the base model pay more attention to
examples that are hard to solve by biased models. The experiments demonstrate
that our method makes better use of visual information and achieves
state-of-the-art performance on diagnosing dataset VQA-CP without using extra
annotations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1"&gt;Xinzhe Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shuhui Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_C/0/1/0/all/0/1"&gt;Chi Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1"&gt;Qingming Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1"&gt;Qi Tian&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Back to the Future: Unsupervised Backprop-based Decoding for Counterfactual and Abductive Commonsense Reasoning. (arXiv:2010.05906v4 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.05906</id>
        <link href="http://arxiv.org/abs/2010.05906"/>
        <updated>2021-08-04T01:59:20.729Z</updated>
        <summary type="html"><![CDATA[Abductive and counterfactual reasoning, core abilities of everyday human
cognition, require reasoning about what might have happened at time t, while
conditioning on multiple contexts from the relative past and future. However,
simultaneous incorporation of past and future contexts using generative
language models (LMs) can be challenging, as they are trained either to
condition only on the past context or to perform narrowly scoped
text-infilling. In this paper, we propose DeLorean, a new unsupervised decoding
algorithm that can flexibly incorporate both the past and future contexts using
only off-the-shelf, left-to-right language models and no supervision. The key
intuition of our algorithm is incorporating the future through
back-propagation, during which, we only update the internal representation of
the output while fixing the model parameters. By alternating between forward
and backward propagation, DeLorean can decode the output representation that
reflects both the left and right contexts. We demonstrate that our approach is
general and applicable to two nonmonotonic reasoning tasks: abductive text
generation and counterfactual story revision, where DeLorean outperforms a
range of unsupervised and some supervised methods, based on automatic and human
evaluation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qin_L/0/1/0/all/0/1"&gt;Lianhui Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shwartz_V/0/1/0/all/0/1"&gt;Vered Shwartz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+West_P/0/1/0/all/0/1"&gt;Peter West&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhagavatula_C/0/1/0/all/0/1"&gt;Chandra Bhagavatula&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1"&gt;Jena Hwang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bras_R/0/1/0/all/0/1"&gt;Ronan Le Bras&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bosselut_A/0/1/0/all/0/1"&gt;Antoine Bosselut&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1"&gt;Yejin Choi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fusing Context Into Knowledge Graph for Commonsense Question Answering. (arXiv:2012.04808v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.04808</id>
        <link href="http://arxiv.org/abs/2012.04808"/>
        <updated>2021-08-04T01:59:20.708Z</updated>
        <summary type="html"><![CDATA[Commonsense question answering (QA) requires a model to grasp commonsense and
factual knowledge to answer questions about world events. Many prior methods
couple language modeling with knowledge graphs (KG). However, although a KG
contains rich structural information, it lacks the context to provide a more
precise understanding of the concepts. This creates a gap when fusing knowledge
graphs into language modeling, especially when there is insufficient labeled
data. Thus, we propose to employ external entity descriptions to provide
contextual information for knowledge understanding. We retrieve descriptions of
related concepts from Wiktionary and feed them as additional input to
pre-trained language models. The resulting model achieves state-of-the-art
result in the CommonsenseQA dataset and the best result among non-generative
models in OpenBookQA.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yichong Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1"&gt;Chenguang Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1"&gt;Ruochen Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1"&gt;Michael Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1"&gt;Xuedong Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Large-Scale Differentially Private BERT. (arXiv:2108.01624v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.01624</id>
        <link href="http://arxiv.org/abs/2108.01624"/>
        <updated>2021-08-04T01:59:20.701Z</updated>
        <summary type="html"><![CDATA[In this work, we study the large-scale pretraining of BERT-Large with
differentially private SGD (DP-SGD). We show that combined with a careful
implementation, scaling up the batch size to millions (i.e., mega-batches)
improves the utility of the DP-SGD step for BERT; we also enhance its
efficiency by using an increasing batch size schedule. Our implementation
builds on the recent work of [SVK20], who demonstrated that the overhead of a
DP-SGD step is minimized with effective use of JAX [BFH+18, FJL18] primitives
in conjunction with the XLA compiler [XLA17]. Our implementation achieves a
masked language model accuracy of 60.5% at a batch size of 2M, for $\epsilon =
5.36$. To put this number in perspective, non-private BERT models achieve an
accuracy of $\sim$70%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Anil_R/0/1/0/all/0/1"&gt;Rohan Anil&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghazi_B/0/1/0/all/0/1"&gt;Badih Ghazi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_V/0/1/0/all/0/1"&gt;Vineet Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_R/0/1/0/all/0/1"&gt;Ravi Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Manurangsi_P/0/1/0/all/0/1"&gt;Pasin Manurangsi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ExBERT: An External Knowledge Enhanced BERT for Natural Language Inference. (arXiv:2108.01589v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.01589</id>
        <link href="http://arxiv.org/abs/2108.01589"/>
        <updated>2021-08-04T01:59:20.689Z</updated>
        <summary type="html"><![CDATA[Neural language representation models such as BERT, pre-trained on
large-scale unstructured corpora lack explicit grounding to real-world
commonsense knowledge and are often unable to remember facts required for
reasoning and inference. Natural Language Inference (NLI) is a challenging
reasoning task that relies on common human understanding of language and
real-world commonsense knowledge. We introduce a new model for NLI called
External Knowledge Enhanced BERT (ExBERT), to enrich the contextual
representation with real-world commonsense knowledge from external knowledge
sources and enhance BERT's language understanding and reasoning capabilities.
ExBERT takes full advantage of contextual word representations obtained from
BERT and employs them to retrieve relevant external knowledge from knowledge
graphs and to encode the retrieved external knowledge. Our model adaptively
incorporates the external knowledge context required for reasoning over the
inputs. Extensive experiments on the challenging SciTail and SNLI benchmarks
demonstrate the effectiveness of ExBERT: in comparison to the previous
state-of-the-art, we obtain an accuracy of 95.9% on SciTail and 91.5% on SNLI.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gajbhiye_A/0/1/0/all/0/1"&gt;Amit Gajbhiye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moubayed_N/0/1/0/all/0/1"&gt;Noura Al Moubayed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bradley_S/0/1/0/all/0/1"&gt;Steven Bradley&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Are Missing Links Predictable? An Inferential Benchmark for Knowledge Graph Completion. (arXiv:2108.01387v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.01387</id>
        <link href="http://arxiv.org/abs/2108.01387"/>
        <updated>2021-08-04T01:59:20.682Z</updated>
        <summary type="html"><![CDATA[We present InferWiki, a Knowledge Graph Completion (KGC) dataset that
improves upon existing benchmarks in inferential ability, assumptions, and
patterns. First, each testing sample is predictable with supportive data in the
training set. To ensure it, we propose to utilize rule-guided train/test
generation, instead of conventional random split. Second, InferWiki initiates
the evaluation following the open-world assumption and improves the inferential
difficulty of the closed-world assumption, by providing manually annotated
negative and unknown triples. Third, we include various inference patterns
(e.g., reasoning path length and types) for comprehensive evaluation. In
experiments, we curate two settings of InferWiki varying in sizes and
structures, and apply the construction process on CoDEx as comparative
datasets. The results and empirical analyses demonstrate the necessity and
high-quality of InferWiki. Nevertheless, the performance gap among various
inferential assumptions and patterns presents the difficulty and inspires
future research direction. Our datasets can be found in
https://github.com/TaoMiner/inferwiki]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1"&gt;Yixin Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jun_K/0/1/0/all/0/1"&gt;Kuang Jun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1"&gt;Ming Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_A/0/1/0/all/0/1"&gt;Aoying Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1"&gt;Yonggang Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1"&gt;Tat-Seng Chua&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PhotoChat: A Human-Human Dialogue Dataset with Photo Sharing Behavior for Joint Image-Text Modeling. (arXiv:2108.01453v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2108.01453</id>
        <link href="http://arxiv.org/abs/2108.01453"/>
        <updated>2021-08-04T01:59:20.676Z</updated>
        <summary type="html"><![CDATA[We present a new human-human dialogue dataset - PhotoChat, the first dataset
that casts light on the photo sharing behavior in onlin emessaging. PhotoChat
contains 12k dialogues, each of which is paired with a user photo that is
shared during the conversation. Based on this dataset, we propose two tasks to
facilitate research on image-text modeling: a photo-sharing intent prediction
task that predicts whether one intends to share a photo in the next
conversation turn, and a photo retrieval task that retrieves the most relevant
photo according to the dialogue context. In addition, for both tasks, we
provide baseline models using the state-of-the-art models and report their
benchmark performances. The best image retrieval model achieves 10.4% recall@1
(out of 1000 candidates) and the best photo intent prediction model achieves
58.1% F1 score, indicating that the dataset presents interesting yet
challenging real-world problems. We are releasing PhotoChat to facilitate
future research work among the community.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zang_X/0/1/0/all/0/1"&gt;Xiaoxue Zang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Lijuan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1"&gt;Maria Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1"&gt;Yang Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Hao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jindong Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[$\textrm{WeaSuL}^{\pi}$: Weakly Supervised Dialogue Policy Learning: Reward Estimation for Multi-turn Dialogue. (arXiv:2108.01487v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.01487</id>
        <link href="http://arxiv.org/abs/2108.01487"/>
        <updated>2021-08-04T01:59:20.611Z</updated>
        <summary type="html"><![CDATA[An intelligent dialogue system in a multi-turn setting should not only
generate the responses which are of good quality, but it should also generate
the responses which can lead to long-term success of the dialogue. Although,
the current approaches improved the response quality, but they over-look the
training signals present in the dialogue data. We can leverage these signals to
generate the weakly supervised training data for learning dialog policy and
reward estimator, and make the policy take actions (generates responses) which
can foresee the future direction for a successful (rewarding) conversation. We
simulate the dialogue between an agent and a user (modelled similar to an agent
with supervised learning objective) to interact with each other. The agent uses
dynamic blocking to generate ranked diverse responses and
exploration-exploitation to select among the Top-K responses. Each simulated
state-action pair is evaluated (works as a weak annotation) with three quality
modules: Semantic Relevant, Semantic Coherence and Consistent Flow. Empirical
studies with two benchmarks indicate that our model can significantly
out-perform the response quality and lead to a successful conversation on both
automatic evaluation and human judgement.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khandelwal_A/0/1/0/all/0/1"&gt;Anant Khandelwal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[EVA: An Open-Domain Chinese Dialogue System with Large-Scale Generative Pre-Training. (arXiv:2108.01547v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.01547</id>
        <link href="http://arxiv.org/abs/2108.01547"/>
        <updated>2021-08-04T01:59:20.592Z</updated>
        <summary type="html"><![CDATA[Although pre-trained language models have remarkably enhanced the generation
ability of dialogue systems, open-domain Chinese dialogue systems are still
limited by the dialogue data and the model size compared with English ones. In
this paper, we propose EVA, a Chinese dialogue system that contains the largest
Chinese pre-trained dialogue model with 2.8B parameters. To build this model,
we collect the largest Chinese dialogue dataset named WDC-Dialogue from various
public social media. This dataset contains 1.4B context-response pairs and is
used as the pre-training corpus of EVA. Extensive experiments on automatic and
human evaluation show that EVA outperforms other Chinese pre-trained dialogue
models especially in the multi-turn interaction of human-bot conversations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1"&gt;Hao Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ke_P/0/1/0/all/0/1"&gt;Pei Ke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zheng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1"&gt;Yuxian Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1"&gt;Yinhe Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1"&gt;Chujie Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yida Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1"&gt;Chen Henry Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1"&gt;Hao Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xiaocong Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_B/0/1/0/all/0/1"&gt;Bosi Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1"&gt;Xiaoyan Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1"&gt;Minlie Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1"&gt;Jie Tang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Medical Literature Mining and Retrieval in a Conversational Setting. (arXiv:2108.01436v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2108.01436</id>
        <link href="http://arxiv.org/abs/2108.01436"/>
        <updated>2021-08-04T01:59:20.585Z</updated>
        <summary type="html"><![CDATA[The Covid-19 pandemic has caused a spur in the medical research literature.
With new research advances in understanding the virus, there is a need for
robust text mining tools which can process, extract and present answers from
the literature in a concise and consumable way. With a DialoGPT based
multi-turn conversation generation module, and BM-25 \& neural embeddings based
ensemble information retrieval module, in this paper we present a
conversational system, which can retrieve and answer coronavirus-related
queries from the rich medical literature, and present it in a conversational
setting with the user. We further perform experiments to compare neural
embedding-based document retrieval and the traditional BM25 retrieval algorithm
and report the results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1"&gt;Souvik Das&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saha_S/0/1/0/all/0/1"&gt;Sougata Saha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srihari_R/0/1/0/all/0/1"&gt;Rohini K. Srihari&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Dynamic Head Importance Computation Mechanism for Neural Machine Translation. (arXiv:2108.01377v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.01377</id>
        <link href="http://arxiv.org/abs/2108.01377"/>
        <updated>2021-08-04T01:59:20.553Z</updated>
        <summary type="html"><![CDATA[Multiple parallel attention mechanisms that use multiple attention heads
facilitate greater performance of the Transformer model for various
applications e.g., Neural Machine Translation (NMT), text classification. In
multi-head attention mechanism, different heads attend to different parts of
the input. However, the limitation is that multiple heads might attend to the
same part of the input, resulting in multiple heads being redundant. Thus, the
model resources are under-utilized. One approach to avoid this is to prune
least important heads based on certain importance score. In this work, we focus
on designing a Dynamic Head Importance Computation Mechanism (DHICM) to
dynamically calculate the importance of a head with respect to the input. Our
insight is to design an additional attention layer together with multi-head
attention, and utilize the outputs of the multi-head attention along with the
input, to compute the importance for each head. Additionally, we add an extra
loss function to prevent the model from assigning same score to all heads, to
identify more important heads and improvise performance. We analyzed
performance of DHICM for NMT with different languages. Experiments on different
datasets show that DHICM outperforms traditional Transformer-based approach by
large margin, especially, when less training data is available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Goindani_A/0/1/0/all/0/1"&gt;Akshay Goindani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shrivastava_M/0/1/0/all/0/1"&gt;Manish Shrivastava&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Study of Multilingual End-to-End Speech Recognition for Kazakh, Russian, and English. (arXiv:2108.01280v1 [eess.AS])]]></title>
        <id>http://arxiv.org/abs/2108.01280</id>
        <link href="http://arxiv.org/abs/2108.01280"/>
        <updated>2021-08-04T01:59:20.490Z</updated>
        <summary type="html"><![CDATA[We study training a single end-to-end (E2E) automatic speech recognition
(ASR) model for three languages used in Kazakhstan: Kazakh, Russian, and
English. We first describe the development of multilingual E2E ASR based on
Transformer networks and then perform an extensive assessment on the
aforementioned languages. We also compare two variants of output grapheme set
construction: combined and independent. Furthermore, we evaluate the impact of
LMs and data augmentation techniques on the recognition performance of the
multilingual E2E ASR. In addition, we present several datasets for training and
evaluation purposes. Experiment results show that the multilingual models
achieve comparable performances to the monolingual baselines with a similar
number of parameters. Our best monolingual and multilingual models achieved
20.9% and 20.5% average word error rates on the combined test set,
respectively. To ensure the reproducibility of our experiments and results, we
share our training recipes, datasets, and pre-trained models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Mussakhojayeva_S/0/1/0/all/0/1"&gt;Saida Mussakhojayeva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Khassanov_Y/0/1/0/all/0/1"&gt;Yerbolat Khassanov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Varol_H/0/1/0/all/0/1"&gt;Huseyin Atakan Varol&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[M2H2: A Multimodal Multiparty Hindi Dataset For Humor Recognition in Conversations. (arXiv:2108.01260v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.01260</id>
        <link href="http://arxiv.org/abs/2108.01260"/>
        <updated>2021-08-04T01:59:20.475Z</updated>
        <summary type="html"><![CDATA[Humor recognition in conversations is a challenging task that has recently
gained popularity due to its importance in dialogue understanding, including in
multimodal settings (i.e., text, acoustics, and visual). The few existing
datasets for humor are mostly in English. However, due to the tremendous growth
in multilingual content, there is a great demand to build models and systems
that support multilingual information access. To this end, we propose a dataset
for Multimodal Multiparty Hindi Humor (M2H2) recognition in conversations
containing 6,191 utterances from 13 episodes of a very popular TV series
"Shrimaan Shrimati Phir Se". Each utterance is annotated with humor/non-humor
labels and encompasses acoustic, visual, and textual modalities. We propose
several strong multimodal baselines and show the importance of contextual and
multimodal information for humor recognition in conversations. The empirical
results on M2H2 dataset demonstrate that multimodal information complements
unimodal information for humor recognition. The dataset and the baselines are
available at this http URL and
https://github.com/declare-lab/M2H2-dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chauhan_D/0/1/0/all/0/1"&gt;Dushyant Singh Chauhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_G/0/1/0/all/0/1"&gt;Gopendra Vikram Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Majumder_N/0/1/0/all/0/1"&gt;Navonil Majumder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zadeh_A/0/1/0/all/0/1"&gt;Amir Zadeh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ekbal_A/0/1/0/all/0/1"&gt;Asif Ekbal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhattacharyya_P/0/1/0/all/0/1"&gt;Pushpak Bhattacharyya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morency_L/0/1/0/all/0/1"&gt;Louis-philippe Morency&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Poria_S/0/1/0/all/0/1"&gt;Soujanya Poria&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[sarcasm detection and quantification in arabic tweets. (arXiv:2108.01425v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.01425</id>
        <link href="http://arxiv.org/abs/2108.01425"/>
        <updated>2021-08-04T01:59:20.468Z</updated>
        <summary type="html"><![CDATA[The role of predicting sarcasm in the text is known as automatic sarcasm
detection. Given the prevalence and challenges of sarcasm in sentiment-bearing
text, this is a critical phase in most sentiment analysis tasks. With the
increasing popularity and usage of different social media platforms among users
around the world, people are using sarcasm more and more in their day-to-day
conversations, social media posts and tweets, and it is considered as a way for
people to express their sentiment about some certain topics or issues. As a
result of the increasing popularity, researchers started to focus their
research endeavors on detecting sarcasm from a text in different languages
especially the English language. However, the task of sarcasm detection is a
challenging task due to the nature of sarcastic texts; which can be relative
and significantly differs from one person to another depending on the topic,
region, the user's mentality and other factors. In addition to these
challenges, sarcasm detection in the Arabic language has its own challenges due
to the complexity of the Arabic language, such as being morphologically rich,
with many dialects that significantly vary between each other, while also being
lowly resourced. In recent years, only few research attempts started tackling
the task of sarcasm detection in Arabic, including creating and collecting
corpora, organizing workshops and establishing baseline models. This paper
intends to create a new humanly annotated Arabic corpus for sarcasm detection
collected from tweets, and implementing a new approach for sarcasm detection
and quantification in Arabic tweets. The annotation technique followed in this
paper is unique in sarcasm detection and the proposed approach tackles the
problem as a regression problem instead of classification; i.e., the model
attempts to predict the level of sarcasm instead of binary classification.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Talafha_B/0/1/0/all/0/1"&gt;Bashar Talafha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zater_M/0/1/0/all/0/1"&gt;Muhy Eddin Za&amp;#x27;ter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suleiman_S/0/1/0/all/0/1"&gt;Samer Suleiman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Al_Ayyoub_M/0/1/0/all/0/1"&gt;Mahmoud Al-Ayyoub&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Al_Kabi_M/0/1/0/all/0/1"&gt;Mohammed N. Al-Kabi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How to Evaluate Your Dialogue Models: A Review of Approaches. (arXiv:2108.01369v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.01369</id>
        <link href="http://arxiv.org/abs/2108.01369"/>
        <updated>2021-08-04T01:59:20.461Z</updated>
        <summary type="html"><![CDATA[Evaluating the quality of a dialogue system is an understudied problem. The
recent evolution of evaluation method motivated this survey, in which an
explicit and comprehensive analysis of the existing methods is sought. We are
first to divide the evaluation methods into three classes, i.e., automatic
evaluation, human-involved evaluation and user simulator based evaluation.
Then, each class is covered with main features and the related evaluation
metrics. The existence of benchmarks, suitable for the evaluation of dialogue
techniques are also discussed in detail. Finally, some open issues are pointed
out to bring the evaluation method into a new frontier.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xinmeng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1"&gt;Wansen Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_L/0/1/0/all/0/1"&gt;Long Qin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_Q/0/1/0/all/0/1"&gt;Quanjun Yin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[More but Correct: Generating Diversified and Entity-revised Medical Response. (arXiv:2108.01266v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.01266</id>
        <link href="http://arxiv.org/abs/2108.01266"/>
        <updated>2021-08-04T01:59:20.384Z</updated>
        <summary type="html"><![CDATA[Medical Dialogue Generation (MDG) is intended to build a medical dialogue
system for intelligent consultation, which can communicate with patients in
real-time, thereby improving the efficiency of clinical diagnosis with broad
application prospects. This paper presents our proposed framework for the
Chinese MDG organized by the 2021 China conference on knowledge graph and
semantic computing (CCKS) competition, which requires generating
context-consistent and medically meaningful responses conditioned on the
dialogue history. In our framework, we propose a pipeline system composed of
entity prediction and entity-aware dialogue generation, by adding predicted
entities to the dialogue model with a fusion mechanism, thereby utilizing
information from different sources. At the decoding stage, we propose a new
decoding mechanism named Entity-revised Diverse Beam Search (EDBS) to improve
entity correctness and promote the length and quality of the final response.
The proposed method wins both the CCKS and the International Conference on
Learning Representations (ICLR) 2021 Workshop Machine Learning for Preventing
and Combating Pandemics (MLPCP) Track 1 Entity-aware MED competitions, which
demonstrate the practicality and effectiveness of our method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_E/0/1/0/all/0/1"&gt;Encheng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Hongru Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Weng_Y/0/1/0/all/0/1"&gt;Yixuan Weng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1"&gt;Bin Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1"&gt;Shutao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1"&gt;Yongping Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_M/0/1/0/all/0/1"&gt;Meiling Hu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dialogue Summarization with Supporting Utterance Flow Modeling and Fact Regularization. (arXiv:2108.01268v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.01268</id>
        <link href="http://arxiv.org/abs/2108.01268"/>
        <updated>2021-08-04T01:59:20.329Z</updated>
        <summary type="html"><![CDATA[Dialogue summarization aims to generate a summary that indicates the key
points of a given dialogue. In this work, we propose an end-to-end neural model
for dialogue summarization with two novel modules, namely, the \emph{supporting
utterance flow modeling module} and the \emph{fact regularization module}. The
supporting utterance flow modeling helps to generate a coherent summary by
smoothly shifting the focus from the former utterances to the later ones. The
fact regularization encourages the generated summary to be factually consistent
with the ground-truth summary during model training, which helps to improve the
factual correctness of the generated summary in inference time. Furthermore, we
also introduce a new benchmark dataset for dialogue summarization. Extensive
experiments on both existing and newly-introduced datasets demonstrate the
effectiveness of our model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1"&gt;Wang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1"&gt;Piji Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chan_H/0/1/0/all/0/1"&gt;Hou Pong Chan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+King_I/0/1/0/all/0/1"&gt;Irwin King&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[User-Initiated Repetition-Based Recovery in Multi-Utterance Dialogue Systems. (arXiv:2108.01208v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.01208</id>
        <link href="http://arxiv.org/abs/2108.01208"/>
        <updated>2021-08-04T01:59:20.288Z</updated>
        <summary type="html"><![CDATA[Recognition errors are common in human communication. Similar errors often
lead to unwanted behaviour in dialogue systems or virtual assistants. In human
communication, we can recover from them by repeating misrecognized words or
phrases; however in human-machine communication this recovery mechanism is not
available. In this paper, we attempt to bridge this gap and present a system
that allows a user to correct speech recognition errors in a virtual assistant
by repeating misunderstood words. When a user repeats part of the phrase the
system rewrites the original query to incorporate the correction. This rewrite
allows the virtual assistant to understand the original query successfully. We
present an end-to-end 2-step attention pointer network that can generate the
the rewritten query by merging together the incorrectly understood utterance
with the correction follow-up. We evaluate the model on data collected for this
task and compare the proposed model to a rule-based baseline and a standard
pointer network. We show that rewriting the original query is an effective way
to handle repetition-based recovery and that the proposed model outperforms the
rule based baseline, reducing Word Error Rate by 19% relative at 2% False Alarm
Rate on annotated data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1"&gt;Hoang Long Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Renkens_V/0/1/0/all/0/1"&gt;Vincent Renkens&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pelemans_J/0/1/0/all/0/1"&gt;Joris Pelemans&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Potharaju_S/0/1/0/all/0/1"&gt;Srividya Pranavi Potharaju&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nalamalapu_A/0/1/0/all/0/1"&gt;Anil Kumar Nalamalapu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Akbacak_M/0/1/0/all/0/1"&gt;Murat Akbacak&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Performance Evaluation of Attention-Based Neural ASR under Mixed Speech Input. (arXiv:2108.01245v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2108.01245</id>
        <link href="http://arxiv.org/abs/2108.01245"/>
        <updated>2021-08-04T01:59:20.235Z</updated>
        <summary type="html"><![CDATA[In order to evaluate the performance of the attention based neural ASR under
noisy conditions, the current trend is to present hours of various noisy speech
data to the model and measure the overall word/phoneme error rate (W/PER). In
general, it is unclear how these models perform when exposed to a cocktail
party setup in which two or more speakers are active. In this paper, we present
the mixtures of speech signals to a popular attention-based neural ASR, known
as Listen, Attend, and Spell (LAS), at different target-to-interference ratio
(TIR) and measure the phoneme error rate. In particular, we investigate in
details when two phonemes are mixed what will be the predicted phoneme; in this
fashion we build a model in which the most probable predictions for a phoneme
are given. We found a 65% relative increase in PER when LAS was presented with
mixed speech signals at TIR = 0 dB and the performance approaches the unmixed
scenario at TIR = 30 dB. Our results show the model, when presented with mixed
phonemes signals, tend to predict those that have higher accuracies during
evaluation of original phoneme signals.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+He_B/0/1/0/all/0/1"&gt;Bradley He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Radfar_M/0/1/0/all/0/1"&gt;Martin Radfar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The RareDis corpus: a corpus annotated with rare diseases, their signs and symptoms. (arXiv:2108.01204v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.01204</id>
        <link href="http://arxiv.org/abs/2108.01204"/>
        <updated>2021-08-04T01:59:20.176Z</updated>
        <summary type="html"><![CDATA[The RareDis corpus contains more than 5,000 rare diseases and almost 6,000
clinical manifestations are annotated. Moreover, the Inter Annotator Agreement
evaluation shows a relatively high agreement (F1-measure equal to 83.5% under
exact match criteria for the entities and equal to 81.3% for the relations).
Based on these results, this corpus is of high quality, supposing a significant
step for the field since there is a scarcity of available corpus annotated with
rare diseases. This could open the door to further NLP applications, which
would facilitate the diagnosis and treatment of these rare diseases and,
therefore, would improve dramatically the quality of life of these patients.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Martinez_deMiguel_C/0/1/0/all/0/1"&gt;Claudia Mart&amp;#xed;nez-deMiguel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Segura_Bedmar_I/0/1/0/all/0/1"&gt;Isabel Segura-Bedmar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chacon_Solano_E/0/1/0/all/0/1"&gt;Esteban Chac&amp;#xf3;n-Solano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guerrero_Aspizua_S/0/1/0/all/0/1"&gt;Sara Guerrero-Aspizua&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Your fairness may vary: Group fairness of pretrained language models in toxic text classification. (arXiv:2108.01250v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.01250</id>
        <link href="http://arxiv.org/abs/2108.01250"/>
        <updated>2021-08-04T01:59:20.162Z</updated>
        <summary type="html"><![CDATA[We study the performance-fairness trade-off in more than a dozen fine-tuned
LMs for toxic text classification. We empirically show that no blanket
statement can be made with respect to the bias of large versus regular versus
compressed models. Moreover, we find that focusing on fairness-agnostic
performance metrics can lead to models with varied fairness characteristics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Baldini_I/0/1/0/all/0/1"&gt;Ioana Baldini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_D/0/1/0/all/0/1"&gt;Dennis Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramamurthy_K/0/1/0/all/0/1"&gt;Karthikeyan Natesan Ramamurthy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yurochkin_M/0/1/0/all/0/1"&gt;Mikhail Yurochkin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1"&gt;Moninder Singh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Correcting Arabic Soft Spelling Mistakes using BiLSTM-based Machine Learning. (arXiv:2108.01141v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.01141</id>
        <link href="http://arxiv.org/abs/2108.01141"/>
        <updated>2021-08-04T01:59:20.151Z</updated>
        <summary type="html"><![CDATA[Soft spelling errors are a class of spelling mistakes that is widespread
among native Arabic speakers and foreign learners alike. Some of these errors
are typographical in nature. They occur due to orthographic variations of some
Arabic letters and the complex rules that dictate their correct usage. Many
people forgo these rules, and given the identical phonetic sounds, they often
confuse such letters. In this paper, we propose a bidirectional long short-term
memory network that corrects this class of errors. We develop, train, evaluate,
and compare a set of BiLSTM networks. We approach the spelling correction
problem at the character level. We handle Arabic texts from both classical and
modern standard Arabic. We treat the problem as a one-to-one sequence
transcription problem. Since the soft Arabic errors class encompasses omission
and addition mistakes, to preserve the one-to-one sequence transcription, we
propose a simple low-resource yet effective technique that maintains the
one-to-one sequencing and avoids using a costly encoder-decoder architecture.
We train the BiLSTM models to correct the spelling mistakes using transformed
input and stochastic error injection approaches. We recommend a configuration
that has two BiLSTM layers, uses the dropout regularization, and is trained
using the latter training approach with error injection rate of 40%. The best
model corrects 96.4% of the injected errors and achieves a low character error
rate of 1.28% on a real test set of soft spelling mistakes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Abandah_G/0/1/0/all/0/1"&gt;Gheith A. Abandah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Suyyagh_A/0/1/0/all/0/1"&gt;Ashraf Suyyagh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khedher_M/0/1/0/all/0/1"&gt;Mohammed Z. Khedher&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Underreporting of errors in NLG output, and what to do about it. (arXiv:2108.01182v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.01182</id>
        <link href="http://arxiv.org/abs/2108.01182"/>
        <updated>2021-08-04T01:59:20.125Z</updated>
        <summary type="html"><![CDATA[We observe a severe under-reporting of the different kinds of errors that
Natural Language Generation systems make. This is a problem, because mistakes
are an important indicator of where systems should still be improved. If
authors only report overall performance metrics, the research community is left
in the dark about the specific weaknesses that are exhibited by
`state-of-the-art' research. Next to quantifying the extent of error
under-reporting, this position paper provides recommendations for error
identification, analysis and reporting.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Miltenburg_E/0/1/0/all/0/1"&gt;Emiel van Miltenburg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Clinciu_M/0/1/0/all/0/1"&gt;Miruna-Adriana Clinciu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dusek_O/0/1/0/all/0/1"&gt;Ond&amp;#x159;ej Du&amp;#x161;ek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gkatzia_D/0/1/0/all/0/1"&gt;Dimitra Gkatzia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Inglis_S/0/1/0/all/0/1"&gt;Stephanie Inglis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leppanen_L/0/1/0/all/0/1"&gt;Leo Lepp&amp;#xe4;nen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mahamood_S/0/1/0/all/0/1"&gt;Saad Mahamood&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Manning_E/0/1/0/all/0/1"&gt;Emma Manning&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schoch_S/0/1/0/all/0/1"&gt;Stephanie Schoch&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thomson_C/0/1/0/all/0/1"&gt;Craig Thomson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_L/0/1/0/all/0/1"&gt;Luou Wen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PyEuroVoc: A Tool for Multilingual Legal Document Classification with EuroVoc Descriptors. (arXiv:2108.01139v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.01139</id>
        <link href="http://arxiv.org/abs/2108.01139"/>
        <updated>2021-08-04T01:59:20.114Z</updated>
        <summary type="html"><![CDATA[EuroVoc is a multilingual thesaurus that was built for organizing the
legislative documentary of the European Union institutions. It contains
thousands of categories at different levels of specificity and its descriptors
are targeted by legal texts in almost thirty languages. In this work we propose
a unified framework for EuroVoc classification on 22 languages by fine-tuning
modern Transformer-based pretrained language models. We study extensively the
performance of our trained models and show that they significantly improve the
results obtained by a similar tool - JEX - on the same dataset. The code and
the fine-tuned models were open sourced, together with a programmatic interface
that eases the process of loading the weights of a trained model and of
classifying a new document.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Avram_A/0/1/0/all/0/1"&gt;Andrei-Marius Avram&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pais_V/0/1/0/all/0/1"&gt;Vasile Pais&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tufis_D/0/1/0/all/0/1"&gt;Dan Tufis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Knowledge-intensive Language Understanding for Explainable AI. (arXiv:2108.01174v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2108.01174</id>
        <link href="http://arxiv.org/abs/2108.01174"/>
        <updated>2021-08-04T01:59:20.098Z</updated>
        <summary type="html"><![CDATA[AI systems have seen significant adoption in various domains. At the same
time, further adoption in some domains is hindered by inability to fully trust
an AI system that it will not harm a human. Besides the concerns for fairness,
privacy, transparency, and explainability are key to developing trusts in AI
systems. As stated in describing trustworthy AI "Trust comes through
understanding. How AI-led decisions are made and what determining factors were
included are crucial to understand." The subarea of explaining AI systems has
come to be known as XAI. Multiple aspects of an AI system can be explained;
these include biases that the data might have, lack of data points in a
particular region of the example space, fairness of gathering the data, feature
importances, etc. However, besides these, it is critical to have human-centered
explanations that are directly related to decision-making similar to how a
domain expert makes decisions based on "domain knowledge," that also include
well-established, peer-validated explicit guidelines. To understand and
validate an AI system's outcomes (such as classification, recommendations,
predictions), that lead to developing trust in the AI system, it is necessary
to involve explicit domain knowledge that humans understand and use.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sheth_A/0/1/0/all/0/1"&gt;Amit Sheth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gaur_M/0/1/0/all/0/1"&gt;Manas Gaur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roy_K/0/1/0/all/0/1"&gt;Kaushik Roy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Faldu_K/0/1/0/all/0/1"&gt;Keyur Faldu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[iART: A Search Engine for Art-Historical Images to Support Research in the Humanities. (arXiv:2108.01542v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2108.01542</id>
        <link href="http://arxiv.org/abs/2108.01542"/>
        <updated>2021-08-04T01:59:20.070Z</updated>
        <summary type="html"><![CDATA[In this paper, we introduce iART: an open Web platform for art-historical
research that facilitates the process of comparative vision. The system
integrates various machine learning techniques for keyword- and content-based
image retrieval as well as category formation via clustering. An intuitive GUI
supports users to define queries and explore results. By using a
state-of-the-art cross-modal deep learning approach, it is possible to search
for concepts that were not previously detected by trained classification
models. Art-historical objects from large, openly licensed collections such as
Amsterdam Rijksmuseum and Wikidata are made available to users.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Springstein_M/0/1/0/all/0/1"&gt;Matthias Springstein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schneider_S/0/1/0/all/0/1"&gt;Stefanie Schneider&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rahnama_J/0/1/0/all/0/1"&gt;Javad Rahnama&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hullermeier_E/0/1/0/all/0/1"&gt;Eyke H&amp;#xfc;llermeier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kohle_H/0/1/0/all/0/1"&gt;Hubertus Kohle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ewerth_R/0/1/0/all/0/1"&gt;Ralph Ewerth&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Role of Phonetic Units in Speech Emotion Recognition. (arXiv:2108.01132v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.01132</id>
        <link href="http://arxiv.org/abs/2108.01132"/>
        <updated>2021-08-04T01:59:20.061Z</updated>
        <summary type="html"><![CDATA[We propose a method for emotion recognition through emotiondependent speech
recognition using Wav2vec 2.0. Our method achieved a significant improvement
over most previously reported results on IEMOCAP, a benchmark emotion dataset.
Different types of phonetic units are employed and compared in terms of
accuracy and robustness of emotion recognition within and across datasets and
languages. Models of phonemes, broad phonetic classes, and syllables all
significantly outperform the utterance model, demonstrating that phonetic units
are helpful and should be incorporated in speech emotion recognition. The best
performance is from using broad phonetic classes. Further research is needed to
investigate the optimal set of broad phonetic classes for the task of emotion
recognition. Finally, we found that Wav2vec 2.0 can be fine-tuned to recognize
coarser-grained or larger phonetic units than phonemes, such as broad phonetic
classes and syllables.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1"&gt;Jiahong Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_X/0/1/0/all/0/1"&gt;Xingyu Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_R/0/1/0/all/0/1"&gt;Renjie Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1"&gt;Liang Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Church_K/0/1/0/all/0/1"&gt;Kenneth Church&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Interpretable Music Similarity Measure Based on Path Interestingness. (arXiv:2108.01632v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2108.01632</id>
        <link href="http://arxiv.org/abs/2108.01632"/>
        <updated>2021-08-04T01:59:20.053Z</updated>
        <summary type="html"><![CDATA[We introduce a novel and interpretable path-based music similarity measure.
Our similarity measure assumes that items, such as songs and artists, and
information about those items are represented in a knowledge graph. We find
paths in the graph between a seed and a target item; we score those paths based
on their interestingness; and we aggregate those scores to determine the
similarity between the seed and the target. A distinguishing feature of our
similarity measure is its interpretability. In particular, we can translate the
most interesting paths into natural language, so that the causes of the
similarity judgements can be readily understood by humans. We compare the
accuracy of our similarity measure with other competitive path-based similarity
baselines in two experimental settings and with four datasets. %\sout{The
results show that our measure has highest accuracy in general.} The results
highlight the validity of our approach to music similarity, and demonstrate
that path interestingness scores can be the basis of an accurate and
interpretable similarity measure.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gabbolini_G/0/1/0/all/0/1"&gt;Giovanni Gabbolini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bridge_D/0/1/0/all/0/1"&gt;Derek Bridge&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Hinge-Loss based Codebook Transfer for Cross-Domain Recommendation with Nonoverlapping Data. (arXiv:2108.01473v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2108.01473</id>
        <link href="http://arxiv.org/abs/2108.01473"/>
        <updated>2021-08-04T01:59:20.045Z</updated>
        <summary type="html"><![CDATA[Recommender systems(RS), especially collaborative filtering(CF) based RS, has
been playing an important role in many e-commerce applications. As the
information being searched over the internet is rapidly increasing, users often
face the difficulty of finding items of his/her own interest and RS often
provides help in such tasks. Recent studies show that, as the item space
increases, and the number of items rated by the users become very less, issues
like sparsity arise. To mitigate the sparsity problem, transfer learning
techniques are being used wherein the data from dense domain(source) is
considered in order to predict the missing entries in the sparse
domain(target). In this paper, we propose a transfer learning approach for
cross-domain recommendation when both domains have no overlap of users and
items. In our approach the transferring of knowledge from source to target
domain is done in a novel way. We make use of co-clustering technique to obtain
the codebook (cluster-level rating pattern) of source domain. By making use of
hinge loss function we transfer the learnt codebook of the source domain to
target. The use of hinge loss as a loss function is novel and has not been
tried before in transfer learning. We demonstrate that our technique improves
the approximation of the target matrix on benchmark datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Veeramachaneni_S/0/1/0/all/0/1"&gt;Sowmini Devi Veeramachaneni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pujari_A/0/1/0/all/0/1"&gt;Arun K Pujari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Padmanabhan_V/0/1/0/all/0/1"&gt;Vineet Padmanabhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1"&gt;Vikas Kumar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Music Retrieval for Fine-Grained Videos by Exploiting Cross-Modal-Encoded Voice-Overs. (arXiv:2104.10557v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.10557</id>
        <link href="http://arxiv.org/abs/2104.10557"/>
        <updated>2021-08-04T01:59:20.028Z</updated>
        <summary type="html"><![CDATA[Recently, the witness of the rapidly growing popularity of short videos on
different Internet platforms has intensified the need for a background music
(BGM) retrieval system. However, existing video-music retrieval methods only
based on the visual modality cannot show promising performance regarding videos
with fine-grained virtual contents. In this paper, we also investigate the
widely added voice-overs in short videos and propose a novel framework to
retrieve BGM for fine-grained short videos. In our framework, we use the
self-attention (SA) and the cross-modal attention (CMA) modules to explore the
intra- and the inter-relationships of different modalities respectively. For
balancing the modalities, we dynamically assign different weights to the modal
features via a fusion gate. For paring the query and the BGM embeddings, we
introduce a triplet pseudo-label loss to constrain the semantics of the modal
embeddings. As there are no existing virtual-content video-BGM retrieval
datasets, we build and release two virtual-content video datasets HoK400 and
CFM400. Experimental results show that our method achieves superior performance
and outperforms other state-of-the-art methods with large margins.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1"&gt;Tingtian Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1"&gt;Zixun Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Haoruo Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Ziming Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhan_H/0/1/0/all/0/1"&gt;Hui Zhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1"&gt;Yipeng Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1"&gt;Hengcan Shi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Decoupling recognition and transcription in Mandarin ASR. (arXiv:2108.01129v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.01129</id>
        <link href="http://arxiv.org/abs/2108.01129"/>
        <updated>2021-08-04T01:59:20.011Z</updated>
        <summary type="html"><![CDATA[Much of the recent literature on automatic speech recognition (ASR) is taking
an end-to-end approach. Unlike English where the writing system is closely
related to sound, Chinese characters (Hanzi) represent meaning, not sound. We
propose factoring audio -> Hanzi into two sub-tasks: (1) audio -> Pinyin and
(2) Pinyin -> Hanzi, where Pinyin is a system of phonetic transcription of
standard Chinese. Factoring the audio -> Hanzi task in this way achieves 3.9%
CER (character error rate) on the Aishell-1 corpus, the best result reported on
this dataset so far.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1"&gt;Jiahong Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_X/0/1/0/all/0/1"&gt;Xingyu Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_D/0/1/0/all/0/1"&gt;Dongji Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_R/0/1/0/all/0/1"&gt;Renjie Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1"&gt;Liang Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Church_K/0/1/0/all/0/1"&gt;Kenneth Church&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automatic recognition of suprasegmentals in speech. (arXiv:2108.01122v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.01122</id>
        <link href="http://arxiv.org/abs/2108.01122"/>
        <updated>2021-08-04T01:59:20.000Z</updated>
        <summary type="html"><![CDATA[This study reports our efforts to improve automatic recognition of
suprasegmentals by fine-tuning wav2vec 2.0 with CTC, a method that has been
successful in automatic speech recognition. We demonstrate that the method can
improve the state-of-the-art on automatic recognition of syllables, tones, and
pitch accents. Utilizing segmental information, by employing tonal finals or
tonal syllables as recognition units, can significantly improve Mandarin tone
recognition. Language models are helpful when tonal syllables are used as
recognition units, but not helpful when tones are recognition units. Finally,
Mandarin tone recognition can benefit from English phoneme recognition by
combing the two tasks in fine-tuning wav2vec 2.0.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1"&gt;Jiahong Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ryant_N/0/1/0/all/0/1"&gt;Neville Ryant&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_X/0/1/0/all/0/1"&gt;Xingyu Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Church_K/0/1/0/all/0/1"&gt;Kenneth Church&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liberman_M/0/1/0/all/0/1"&gt;Mark Liberman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adaptively Optimize Content Recommendation Using Multi Armed Bandit Algorithms in E-commerce. (arXiv:2108.01440v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2108.01440</id>
        <link href="http://arxiv.org/abs/2108.01440"/>
        <updated>2021-08-04T01:59:19.925Z</updated>
        <summary type="html"><![CDATA[E-commerce sites strive to provide users the most timely relevant information
in order to reduce shopping frictions and increase customer satisfaction. Multi
armed bandit models (MAB) as a type of adaptive optimization algorithms provide
possible approaches for such purposes. In this paper, we analyze using three
classic MAB algorithms, epsilon-greedy, Thompson sampling (TS), and upper
confidence bound 1 (UCB1) for dynamic content recommendations, and walk through
the process of developing these algorithms internally to solve a real world
e-commerce use case. First, we analyze the three MAB algorithms using simulated
purchasing datasets with non-stationary reward distributions to simulate the
possible time-varying customer preferences, where the traffic allocation
dynamics and the accumulative rewards of different algorithms are studied.
Second, we compare the accumulative rewards of the three MAB algorithms with
more than 1,000 trials using actual historical A/B test datasets. We find that
the larger difference between the success rates of competing recommendations
the more accumulative rewards the MAB algorithms can achieve. In addition, we
find that TS shows the highest average accumulative rewards under different
testing scenarios. Third, we develop a batch-updated MAB algorithm to overcome
the delayed reward issue in e-commerce and enable an online content
optimization on our App homepage. For a state-of-the-art comparison, a real A/B
test among our batch-updated MAB algorithm, a third-party MAB solution, and the
default business logic are conducted. The result shows that our batch-updated
MAB algorithm outperforms the counterparts and achieves 6.13% relative
click-through rate (CTR) increase and 16.1% relative conversion rate (CVR)
increase compared to the default experience, and 2.9% relative CTR increase and
1.4% relative CVR increase compared to the external MAB service.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xiang_D/0/1/0/all/0/1"&gt;Ding Xiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+West_B/0/1/0/all/0/1"&gt;Becky West&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jiaqi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_X/0/1/0/all/0/1"&gt;Xiquan Cui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1"&gt;Jinzhou Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Effective Model Integration Algorithm for Improving Link and Sign Prediction in Complex Networks. (arXiv:2108.01532v1 [physics.soc-ph])]]></title>
        <id>http://arxiv.org/abs/2108.01532</id>
        <link href="http://arxiv.org/abs/2108.01532"/>
        <updated>2021-08-04T01:59:19.917Z</updated>
        <summary type="html"><![CDATA[Link and sign prediction in complex networks bring great help to
decision-making and recommender systems, such as in predicting potential
relationships or relative status levels. Many previous studies focused on
designing the special algorithms to perform either link prediction or sign
prediction. In this work, we propose an effective model integration algorithm
consisting of network embedding, network feature engineering, and an integrated
classifier, which can perform the link and sign prediction in the same
framework. Network embedding can accurately represent the characteristics of
topological structures and cooperate with the powerful network feature
engineering and integrated classifier can achieve better prediction.
Experiments on several datasets show that the proposed model can achieve
state-of-the-art or competitive performance for both link and sign prediction
in spite of its generality. Interestingly, we find that using only very low
network embedding dimension can generate high prediction performance, which can
significantly reduce the computational overhead during training and prediction.
This study offers a powerful methodology for multi-task prediction in complex
networks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Liu_C/0/1/0/all/0/1"&gt;Chuang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Yu_S/0/1/0/all/0/1"&gt;Shimin Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Ying Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zi-Ke Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Inscriptis -- A Python-based HTML to text conversion library optimized for knowledge extraction from the Web. (arXiv:2108.01454v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2108.01454</id>
        <link href="http://arxiv.org/abs/2108.01454"/>
        <updated>2021-08-04T01:59:19.908Z</updated>
        <summary type="html"><![CDATA[Inscriptis provides a library, command line client and Web service for
converting HTML to plain text. Its development has been triggered by the need
to obtain accurate text representations for knowledge extraction tasks that
preserve the spatial alignment of text without drawing upon heavyweight,
browser-based solutions such as Selenium. In contrast to existing software
packages such as HTML2text, jusText and Lynx, Inscriptis (i) provides a
layout-aware conversion of HTML that more closely resembles the rendering
obtained from standard Web browsers and, therefore, better preserves the
spatial arrangement of text elements. Inscriptis excels in terms of conversion
quality, since it correctly converts complex HTML constructs such as nested
tables and also interprets a subset of HTML attributes that determine the text
alignment. In addition, it (ii) supports annotation rules, i.e., user-provided
mappings that allow for annotating the extracted text based on structural and
semantic information encoded in HTML tags and attributes used for controlling
structure and layout in the original HTML document. These unique features
ensure that downstream knowledge extraction components can operate on accurate
text representations, and may even use information on the semantics and
structure of the original HTML document, if annotation support has been
enabled.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Weichselbraun_A/0/1/0/all/0/1"&gt;Albert Weichselbraun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FEBR: Expert-Based Recommendation Framework for beneficial and personalized content. (arXiv:2108.01455v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2108.01455</id>
        <link href="http://arxiv.org/abs/2108.01455"/>
        <updated>2021-08-04T01:59:19.773Z</updated>
        <summary type="html"><![CDATA[So far, most research on recommender systems focused on maintaining long-term
user engagement and satisfaction, by promoting relevant and personalized
content. However, it is still very challenging to evaluate the quality and the
reliability of this content. In this paper, we propose FEBR (Expert-Based
Recommendation Framework), an apprenticeship learning framework to assess the
quality of the recommended content on online platforms. The framework exploits
the demonstrated trajectories of an expert (assumed to be reliable) in a
recommendation evaluation environment, to recover an unknown utility function.
This function is used to learn an optimal policy describing the expert's
behavior, which is then used in the framework to provide high-quality and
personalized recommendations. We evaluate the performance of our solution
through a user interest simulation environment (using RecSim). We simulate
interactions under the aforementioned expert policy for videos recommendation,
and compare its efficiency with standard recommendation methods. The results
show that our approach provides a significant gain in terms of content quality,
evaluated by experts and watched by users, while maintaining almost the same
watch time as the baseline approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lechiakh_M/0/1/0/all/0/1"&gt;Mohamed Lechiakh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maurer_A/0/1/0/all/0/1"&gt;Alexandre Maurer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sequence Adaptation via Reinforcement Learning in Recommender Systems. (arXiv:2108.01442v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2108.01442</id>
        <link href="http://arxiv.org/abs/2108.01442"/>
        <updated>2021-08-04T01:59:19.622Z</updated>
        <summary type="html"><![CDATA[Accounting for the fact that users have different sequential patterns, the
main drawback of state-of-the-art recommendation strategies is that a fixed
sequence length of user-item interactions is required as input to train the
models. This might limit the recommendation accuracy, as in practice users
follow different trends on the sequential recommendations. Hence, baseline
strategies might ignore important sequential interactions or add noise to the
models with redundant interactions, depending on the variety of users'
sequential behaviours. To overcome this problem, in this study we propose the
SAR model, which not only learns the sequential patterns but also adjusts the
sequence length of user-item interactions in a personalized manner. We first
design an actor-critic framework, where the RL agent tries to compute the
optimal sequence length as an action, given the user's state representation at
a certain time step. In addition, we optimize a joint loss function to align
the accuracy of the sequential recommendations with the expected cumulative
rewards of the critic network, while at the same time we adapt the sequence
length with the actor network in a personalized manner. Our experimental
evaluation on four real-world datasets demonstrates the superiority of our
proposed model over several baseline approaches. Finally, we make our
implementation publicly available at https://github.com/stefanosantaris/sar.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Antaris_S/0/1/0/all/0/1"&gt;Stefanos Antaris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rafailidis_D/0/1/0/all/0/1"&gt;Dimitrios Rafailidis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using Query Expansion in Manifold Ranking for Query-Oriented Multi-Document Summarization. (arXiv:2108.01441v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2108.01441</id>
        <link href="http://arxiv.org/abs/2108.01441"/>
        <updated>2021-08-04T01:59:19.612Z</updated>
        <summary type="html"><![CDATA[Manifold ranking has been successfully applied in query-oriented
multi-document summarization. It not only makes use of the relationships among
the sentences, but also the relationships between the given query and the
sentences. However, the information of original query is often insufficient. So
we present a query expansion method, which is combined in the manifold ranking
to resolve this problem. Our method not only utilizes the information of the
query term itself and the knowledge base WordNet to expand it by synonyms, but
also uses the information of the document set itself to expand the query in
various ways (mean expansion, variance expansion and TextRank expansion).
Compared with the previous query expansion methods, our method combines
multiple query expansion methods to better represent query information, and at
the same time, it makes a useful attempt on manifold ranking. In addition, we
use the degree of word overlap and the proximity between words to calculate the
similarity between sentences. We performed experiments on the datasets of DUC
2006 and DUC2007, and the evaluation results show that the proposed query
expansion method can significantly improve the system performance and make our
system comparable to the state-of-the-art systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jia_Q/0/1/0/all/0/1"&gt;Quanye Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1"&gt;Rui Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1"&gt;Jianying Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PhotoChat: A Human-Human Dialogue Dataset with Photo Sharing Behavior for Joint Image-Text Modeling. (arXiv:2108.01453v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2108.01453</id>
        <link href="http://arxiv.org/abs/2108.01453"/>
        <updated>2021-08-04T01:59:19.597Z</updated>
        <summary type="html"><![CDATA[We present a new human-human dialogue dataset - PhotoChat, the first dataset
that casts light on the photo sharing behavior in onlin emessaging. PhotoChat
contains 12k dialogues, each of which is paired with a user photo that is
shared during the conversation. Based on this dataset, we propose two tasks to
facilitate research on image-text modeling: a photo-sharing intent prediction
task that predicts whether one intends to share a photo in the next
conversation turn, and a photo retrieval task that retrieves the most relevant
photo according to the dialogue context. In addition, for both tasks, we
provide baseline models using the state-of-the-art models and report their
benchmark performances. The best image retrieval model achieves 10.4% recall@1
(out of 1000 candidates) and the best photo intent prediction model achieves
58.1% F1 score, indicating that the dataset presents interesting yet
challenging real-world problems. We are releasing PhotoChat to facilitate
future research work among the community.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zang_X/0/1/0/all/0/1"&gt;Xiaoxue Zang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Lijuan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1"&gt;Maria Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1"&gt;Yang Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Hao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jindong Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Understanding Human Reading Comprehension with brain signals. (arXiv:2108.01360v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2108.01360</id>
        <link href="http://arxiv.org/abs/2108.01360"/>
        <updated>2021-08-04T01:59:19.581Z</updated>
        <summary type="html"><![CDATA[Reading comprehension is a complex cognitive process involving many human
brain activities. Plenty of works have studied the reading patterns and
attention allocation mechanisms in the reading process. However, little is
known about what happens in human brain during reading comprehension and how we
can utilize this information as implicit feedback to facilitate information
acquisition performance. With the advances in brain imaging techniques such as
EEG, it is possible to collect high-precision brain signals in almost real
time. With neuroimaging techniques, we carefully design a lab-based user study
to investigate brain activities during reading comprehension. Our findings show
that neural responses vary with different types of contents, i.e., contents
that can satisfy users' information needs and contents that cannot. We suggest
that various cognitive activities, e.g., cognitive loading, semantic-thematic
understanding, and inferential processing, at the micro-time scale during
reading comprehension underpin these neural responses. Inspired by these
detectable differences in cognitive activities, we construct supervised
learning models based on EEG features for two reading comprehension tasks:
answer sentence classification and answer extraction. Results show that it is
feasible to improve their performance with brain signals. These findings imply
that brain signals are valuable feedback for enhancing human-computer
interactions during reading comprehension.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ye_Z/0/1/0/all/0/1"&gt;Ziyi Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1"&gt;Xiaohui Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yiqun Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhihong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xuesong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Min Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1"&gt;Shaoping Ma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Memorize, Factorize, or be Na\"ive: Learning Optimal Feature Interaction Methods for CTR Prediction. (arXiv:2108.01265v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.01265</id>
        <link href="http://arxiv.org/abs/2108.01265"/>
        <updated>2021-08-04T01:59:19.519Z</updated>
        <summary type="html"><![CDATA[Click-through rate prediction is one of the core tasks in commercial
recommender systems. It aims to predict the probability of a user clicking a
particular item given user and item features. As feature interactions bring in
non-linearity, they are widely adopted to improve the performance of CTR
prediction models. Therefore, effectively modelling feature interactions has
attracted much attention in both the research and industry field. The current
approaches can generally be categorized into three classes: (1) na\"ive
methods, which do not model feature interactions and only use original
features; (2) memorized methods, which memorize feature interactions by
explicitly viewing them as new features and assigning trainable embeddings; (3)
factorized methods, which learn latent vectors for original features and
implicitly model feature interactions through factorization functions. Studies
have shown that modelling feature interactions by one of these methods alone
are suboptimal due to the unique characteristics of different feature
interactions. To address this issue, we first propose a general framework
called OptInter which finds the most suitable modelling method for each feature
interaction. Different state-of-the-art deep CTR models can be viewed as
instances of OptInter. To realize the functionality of OptInter, we also
introduce a learning algorithm that automatically searches for the optimal
modelling method. We conduct extensive experiments on four large datasets. Our
experiments show that OptInter improves the best performed state-of-the-art
baseline deep CTR models by up to 2.21%. Compared to the memorized method,
which also outperforms baselines, we reduce up to 91% parameters. In addition,
we conduct several ablation studies to investigate the influence of different
components of OptInter. Finally, we provide interpretable discussions on the
results of OptInter.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lyu_F/0/1/0/all/0/1"&gt;Fuyuan Lyu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1"&gt;Xing Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1"&gt;Huifeng Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_R/0/1/0/all/0/1"&gt;Ruiming Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1"&gt;Xiuqiang He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Rui Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xue Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Medical Literature Mining and Retrieval in a Conversational Setting. (arXiv:2108.01436v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2108.01436</id>
        <link href="http://arxiv.org/abs/2108.01436"/>
        <updated>2021-08-04T01:59:19.503Z</updated>
        <summary type="html"><![CDATA[The Covid-19 pandemic has caused a spur in the medical research literature.
With new research advances in understanding the virus, there is a need for
robust text mining tools which can process, extract and present answers from
the literature in a concise and consumable way. With a DialoGPT based
multi-turn conversation generation module, and BM-25 \& neural embeddings based
ensemble information retrieval module, in this paper we present a
conversational system, which can retrieve and answer coronavirus-related
queries from the rich medical literature, and present it in a conversational
setting with the user. We further perform experiments to compare neural
embedding-based document retrieval and the traditional BM25 retrieval algorithm
and report the results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1"&gt;Souvik Das&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saha_S/0/1/0/all/0/1"&gt;Sougata Saha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srihari_R/0/1/0/all/0/1"&gt;Rohini K. Srihari&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[EMOPIA: A Multi-Modal Pop Piano Dataset For Emotion Recognition and Emotion-based Music Generation. (arXiv:2108.01374v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2108.01374</id>
        <link href="http://arxiv.org/abs/2108.01374"/>
        <updated>2021-08-04T01:59:19.479Z</updated>
        <summary type="html"><![CDATA[While there are many music datasets with emotion labels in the literature,
they cannot be used for research on symbolic-domain music analysis or
generation, as there are usually audio files only. In this paper, we present
the EMOPIA (pronounced `yee-m\`{o}-pi-uh') dataset, a shared multi-modal (audio
and MIDI) database focusing on perceived emotion in pop piano music, to
facilitate research on various tasks related to music emotion. The dataset
contains 1,087 music clips from 387 songs and clip-level emotion labels
annotated by four dedicated annotators. Since the clips are not restricted to
one clip per song, they can also be used for song-level analysis. We present
the methodology for building the dataset, covering the song list curation, clip
selection, and emotion annotation processes. Moreover, we prototype use cases
on clip-level music emotion classification and emotion-based symbolic music
generation by training and evaluating corresponding models using the dataset.
The result demonstrates the potential of EMOPIA for being used in future
exploration on piano emotion-related MIR tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hung_H/0/1/0/all/0/1"&gt;Hsiao-Tzu Hung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ching_J/0/1/0/all/0/1"&gt;Joann Ching&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Doh_S/0/1/0/all/0/1"&gt;Seungheon Doh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_N/0/1/0/all/0/1"&gt;Nabin Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nam_J/0/1/0/all/0/1"&gt;Juhan Nam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yi-Hsuan Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fast BCH Coding for Optimal Robust Image Watermarking in DCT Domain. (arXiv:2108.01612v1 [cs.MM])]]></title>
        <id>http://arxiv.org/abs/2108.01612</id>
        <link href="http://arxiv.org/abs/2108.01612"/>
        <updated>2021-08-04T01:59:19.455Z</updated>
        <summary type="html"><![CDATA[This paper investigates a novel approach of digital image watermarking based
on BCH error correction code in Discrete Cosine Transformation (DCT) domain. In
the proposed technique, the watermark is encoded through BCH error correction
code before embedding process, then it is embedded into the Discrete Cosine
Transformation (DCT) coefficients of cover image. The proposed algorithm also
employs lookup table method to find the best positions in the frequency domains
for watermark insertion. The significant feature of this method is the
reduction of time required in the process embedding of information, security
and ability to correct the error caused by different attacks. Experimental
results show the superiority of the proposed approach against the existing
approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nabipour_S/0/1/0/all/0/1"&gt;Saeideh Nabipour&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Many Dimensions of Truthfulness: Crowdsourcing Misinformation Assessments on a Multidimensional Scale. (arXiv:2108.01222v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2108.01222</id>
        <link href="http://arxiv.org/abs/2108.01222"/>
        <updated>2021-08-04T01:59:19.428Z</updated>
        <summary type="html"><![CDATA[Recent work has demonstrated the viability of using crowdsourcing as a tool
for evaluating the truthfulness of public statements. Under certain conditions
such as: (1) having a balanced set of workers with different backgrounds and
cognitive abilities; (2) using an adequate set of mechanisms to control the
quality of the collected data; and (3) using a coarse grained assessment scale,
the crowd can provide reliable identification of fake news. However, fake news
are a subtle matter: statements can be just biased ("cherrypicked"), imprecise,
wrong, etc. and the unidimensional truth scale used in existing work cannot
account for such differences. In this paper we propose a multidimensional
notion of truthfulness and we ask the crowd workers to assess seven different
dimensions of truthfulness selected based on existing literature: Correctness,
Neutrality, Comprehensibility, Precision, Completeness, Speaker's
Trustworthiness, and Informativeness. We deploy a set of quality control
mechanisms to ensure that the thousands of assessments collected on 180
publicly available fact-checked statements distributed over two datasets are of
adequate quality, including a custom search engine used by the crowd workers to
find web pages supporting their truthfulness assessments. A comprehensive
analysis of crowdsourced judgments shows that: (1) the crowdsourced assessments
are reliable when compared to an expert-provided gold standard; (2) the
proposed dimensions of truthfulness capture independent pieces of information;
(3) the crowdsourcing task can be easily learned by the workers; and (4) the
resulting assessments provide a useful basis for a more complete estimation of
statement truthfulness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Soprano_M/0/1/0/all/0/1"&gt;Michael Soprano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roitero_K/0/1/0/all/0/1"&gt;Kevin Roitero&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Barbera_D/0/1/0/all/0/1"&gt;David La Barbera&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ceolin_D/0/1/0/all/0/1"&gt;Davide Ceolin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Spina_D/0/1/0/all/0/1"&gt;Damiano Spina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mizzaro_S/0/1/0/all/0/1"&gt;Stefano Mizzaro&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Demartini_G/0/1/0/all/0/1"&gt;Gianluca Demartini&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Solving Fashion Recommendation -- The Farfetch Challenge. (arXiv:2108.01314v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.01314</id>
        <link href="http://arxiv.org/abs/2108.01314"/>
        <updated>2021-08-04T01:59:19.411Z</updated>
        <summary type="html"><![CDATA[Recommendation engines are integral to the modern e-commerce experience, both
for the seller and the end user. Accurate recommendations lead to higher
revenue and better user experience. In this paper, we are presenting our
solution to ECML PKDD Farfetch Fashion Recommendation Challenge.The goal of
this challenge is to maximize the chances of a click when the users are
presented with set of fashion items. We have approached this problem as a
binary classification problem. Our winning solution utilizes Catboost as the
classifier and Bayesian Optimization for hyper parameter tuning. Our baseline
model achieved MRR of 0.5153 on the validation set. Bayesian optimization of
hyper parameters improved the MRR to 0.5240 on the validation set. Our final
submission on the test set achieved a MRR of 0.5257.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pathak_M/0/1/0/all/0/1"&gt;Manish Pathak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1"&gt;Aditya Jain&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Music Retrieval for Fine-Grained Videos by Exploiting Cross-Modal-Encoded Voice-Overs. (arXiv:2104.10557v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.10557</id>
        <link href="http://arxiv.org/abs/2104.10557"/>
        <updated>2021-08-04T01:59:19.386Z</updated>
        <summary type="html"><![CDATA[Recently, the witness of the rapidly growing popularity of short videos on
different Internet platforms has intensified the need for a background music
(BGM) retrieval system. However, existing video-music retrieval methods only
based on the visual modality cannot show promising performance regarding videos
with fine-grained virtual contents. In this paper, we also investigate the
widely added voice-overs in short videos and propose a novel framework to
retrieve BGM for fine-grained short videos. In our framework, we use the
self-attention (SA) and the cross-modal attention (CMA) modules to explore the
intra- and the inter-relationships of different modalities respectively. For
balancing the modalities, we dynamically assign different weights to the modal
features via a fusion gate. For paring the query and the BGM embeddings, we
introduce a triplet pseudo-label loss to constrain the semantics of the modal
embeddings. As there are no existing virtual-content video-BGM retrieval
datasets, we build and release two virtual-content video datasets HoK400 and
CFM400. Experimental results show that our method achieves superior performance
and outperforms other state-of-the-art methods with large margins.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1"&gt;Tingtian Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1"&gt;Zixun Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Haoruo Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Ziming Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhan_H/0/1/0/all/0/1"&gt;Hui Zhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1"&gt;Yipeng Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1"&gt;Hengcan Shi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rethinking the Implementation Tricks and Monotonicity Constraint in Cooperative Multi-Agent Reinforcement Learning. (arXiv:2102.03479v13 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.03479</id>
        <link href="http://arxiv.org/abs/2102.03479"/>
        <updated>2021-08-03T02:06:35.290Z</updated>
        <summary type="html"><![CDATA[Many complex multi-robot systems such as robot swarms control and autonomous
vehicle coordination can be modeled as Multi-Agent Reinforcement Learning
(MARL) tasks. QMIX, a widely popular MARL algorithm, has been used as a
baseline for the benchmark environments, e.g., Starcraft Multi-Agent Challenge
(SMAC), Difficulty-Enhanced Predator-Prey (DEPP). Recent variants of QMIX
target relaxing the monotonicity constraint of QMIX, allowing for performance
improvement in SMAC. In this paper, we investigate the code-level optimizations
of these variants and the monotonicity constraint. (1) We find that such
improvements of the variants are significantly affected by various code-level
optimizations. (2) The experiment results show that QMIX with normalized
optimizations outperforms other works in SMAC; (3) beyond the common wisdom
from these works, the monotonicity constraint can improve sample efficiency in
SMAC and DEPP. We also discuss why monotonicity constraints work well in purely
cooperative tasks with a theoretical analysis. We open-source the code at
\url{https://github.com/hijkzzz/pymarl2}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1"&gt;Jian Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1"&gt;Siyang Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Harding_S/0/1/0/all/0/1"&gt;Seth Austin Harding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1"&gt;Haibin Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liao_S/0/1/0/all/0/1"&gt;Shih-wei Liao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Online Metro Origin-Destination Prediction via Heterogeneous Information Aggregation. (arXiv:2107.00946v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.00946</id>
        <link href="http://arxiv.org/abs/2107.00946"/>
        <updated>2021-08-03T02:06:35.235Z</updated>
        <summary type="html"><![CDATA[Metro origin-destination prediction is a crucial yet challenging time-series
analysis task in intelligent transportation systems, which aims to accurately
forecast two specific types of cross-station ridership, i.e.,
Origin-Destination (OD) one and Destination-Origin (DO) one. However, complete
OD matrices of previous time intervals can not be obtained immediately in
online metro systems, and conventional methods only used limited information to
forecast the future OD and DO ridership separately. In this work, we proposed a
novel neural network module termed Heterogeneous Information Aggregation
Machine (HIAM), which fully exploits heterogeneous information of historical
data (e.g., incomplete OD matrices, unfinished order vectors, and DO matrices)
to jointly learn the evolutionary patterns of OD and DO ridership.
Specifically, an OD modeling branch estimates the potential destinations of
unfinished orders explicitly to complement the information of incomplete OD
matrices, while a DO modeling branch takes DO matrices as input to capture the
spatial-temporal distribution of DO ridership. Moreover, a Dual Information
Transformer is introduced to propagate the mutual information among OD features
and DO features for modeling the OD-DO causality and correlation. Based on the
proposed HIAM, we develop a unified Seq2Seq network to forecast the future OD
and DO ridership simultaneously. Extensive experiments conducted on two
large-scale benchmarks demonstrate the effectiveness of our method for online
metro origin-destination prediction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Lingbo Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1"&gt;Yuying Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1"&gt;Guanbin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1"&gt;Ziyi Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1"&gt;Lei Bai Liang Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generalization Properties of Stochastic Optimizers via Trajectory Analysis. (arXiv:2108.00781v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2108.00781</id>
        <link href="http://arxiv.org/abs/2108.00781"/>
        <updated>2021-08-03T02:06:35.185Z</updated>
        <summary type="html"><![CDATA[Despite the ubiquitous use of stochastic optimization algorithms in machine
learning, the precise impact of these algorithms on generalization performance
in realistic non-convex settings is still poorly understood. In this paper, we
provide an encompassing theoretical framework for investigating the
generalization properties of stochastic optimizers, which is based on their
dynamics. We first prove a generalization bound attributable to the optimizer
dynamics in terms of the celebrated Fernique-Talagrand functional applied to
the trajectory of the optimizer. This data- and algorithm-dependent bound is
shown to be the sharpest possible in the absence of further assumptions. We
then specialize this result by exploiting the Markovian structure of stochastic
optimizers, deriving generalization bounds in terms of the (data-dependent)
transition kernels associated with the optimization algorithms. In line with
recent work that has revealed connections between generalization and
heavy-tailed behavior in stochastic optimization, we link the generalization
error to the local tail behavior of the transition kernels. We illustrate that
the local power-law exponent of the kernel acts as an effective dimension,
which decreases as the transitions become "less Gaussian". We support our
theory with empirical results from a variety of neural networks, and we show
that both the Fernique-Talagrand functional and the local power-law exponent
are predictive of generalization performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Hodgkinson_L/0/1/0/all/0/1"&gt;Liam Hodgkinson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Simsekli_U/0/1/0/all/0/1"&gt;Umut &amp;#x15e;im&amp;#x15f;ekli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Khanna_R/0/1/0/all/0/1"&gt;Rajiv Khanna&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Mahoney_M/0/1/0/all/0/1"&gt;Michael W. Mahoney&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Topological Information Retrieval with Dilation-Invariant Bottleneck Comparative Measures. (arXiv:2104.01672v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.01672</id>
        <link href="http://arxiv.org/abs/2104.01672"/>
        <updated>2021-08-03T02:06:35.179Z</updated>
        <summary type="html"><![CDATA[Appropriately representing elements in a database so that queries may be
accurately matched is a central task in information retrieval; recently, this
has been achieved by embedding the graphical structure of the database into a
manifold in a hierarchy-preserving manner using a variety of metrics.
Persistent homology is a tool commonly used in topological data analysis that
is able to rigorously characterize a database in terms of both its hierarchy
and connectivity structure. Computing persistent homology on a variety of
embedded datasets reveals that some commonly used embeddings fail to preserve
the connectivity. We show that those embeddings which successfully retain the
database topology coincide in persistent homology by introducing two
dilation-invariant comparative measures to capture this effect: in particular,
they address the issue of metric distortion on manifolds. We provide an
algorithm for their computation that exhibits greatly reduced time complexity
over existing methods. We use these measures to perform the first instance of
topology-based information retrieval and demonstrate its increased performance
over the standard bottleneck distance for persistent homology. We showcase our
approach on databases of different data varieties including text, videos, and
medical images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Vlontzos_A/0/1/0/all/0/1"&gt;Athanasios Vlontzos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Cao_Y/0/1/0/all/0/1"&gt;Yueqi Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Schmidtke_L/0/1/0/all/0/1"&gt;Luca Schmidtke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kainz_B/0/1/0/all/0/1"&gt;Bernhard Kainz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Monod_A/0/1/0/all/0/1"&gt;Anthea Monod&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tensor Clustering with Planted Structures: Statistical Optimality and Computational Limits. (arXiv:2005.10743v3 [math.ST] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2005.10743</id>
        <link href="http://arxiv.org/abs/2005.10743"/>
        <updated>2021-08-03T02:06:35.173Z</updated>
        <summary type="html"><![CDATA[This paper studies the statistical and computational limits of high-order
clustering with planted structures. We focus on two clustering models, constant
high-order clustering (CHC) and rank-one higher-order clustering (ROHC), and
study the methods and theory for testing whether a cluster exists (detection)
and identifying the support of cluster (recovery).

Specifically, we identify the sharp boundaries of signal-to-noise ratio for
which CHC and ROHC detection/recovery are statistically possible. We also
develop the tight computational thresholds: when the signal-to-noise ratio is
below these thresholds, we prove that polynomial-time algorithms cannot solve
these problems under the computational hardness conjectures of hypergraphic
planted clique (HPC) detection and hypergraphic planted dense subgraph (HPDS)
recovery. We also propose polynomial-time tensor algorithms that achieve
reliable detection and recovery when the signal-to-noise ratio is above these
thresholds. Both sparsity and tensor structures yield the computational
barriers in high-order tensor clustering. The interplay between them results in
significant differences between high-order tensor clustering and matrix
clustering in literature in aspects of statistical and computational phase
transition diagrams, algorithmic approaches, hardness conjecture, and proof
techniques. To our best knowledge, we are the first to give a thorough
characterization of the statistical and computational trade-off for such a
double computational-barrier problem. Finally, we provide evidence for the
computational hardness conjectures of HPC detection (via low-degree polynomial
and Metropolis methods) and HPDS recovery (via low-degree polynomial method).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Luo_Y/0/1/0/all/0/1"&gt;Yuetian Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Zhang_A/0/1/0/all/0/1"&gt;Anru R. Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Strategies for convex potential games and an application to decision-theoretic online learning. (arXiv:2106.10717v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.10717</id>
        <link href="http://arxiv.org/abs/2106.10717"/>
        <updated>2021-08-03T02:06:35.154Z</updated>
        <summary type="html"><![CDATA[The backwards induction method due to Bellman~\cite{bellman1952theory} is a
popular approach to solving problems in optimiztion, optimal control, and many
other areas of applied math. In this paper we analyze the backwords induction
approach, under min/max conditions. We show that if the value function is has
strictly positive derivatives of order 1-4 then the optimal strategy for the
adversary is Brownian motion. Using that fact we analyze different potential
functions and show that the Normal-Hedge potential is optimal.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Freund_Y/0/1/0/all/0/1"&gt;Yoav Freund&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dynamic Transformer for Efficient Machine Translation on Embedded Devices. (arXiv:2107.08199v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.08199</id>
        <link href="http://arxiv.org/abs/2107.08199"/>
        <updated>2021-08-03T02:06:35.148Z</updated>
        <summary type="html"><![CDATA[The Transformer architecture is widely used for machine translation tasks.
However, its resource-intensive nature makes it challenging to implement on
constrained embedded devices, particularly where available hardware resources
can vary at run-time. We propose a dynamic machine translation model that
scales the Transformer architecture based on the available resources at any
particular time. The proposed approach, 'Dynamic-HAT', uses a HAT
SuperTransformer as the backbone to search for SubTransformers with different
accuracy-latency trade-offs at design time. The optimal SubTransformers are
sampled from the SuperTransformer at run-time, depending on latency
constraints. The Dynamic-HAT is tested on the Jetson Nano and the approach uses
inherited SubTransformers sampled directly from the SuperTransformer with a
switching time of <1s. Using inherited SubTransformers results in a BLEU score
loss of <1.5% because the SubTransformer configuration is not retrained from
scratch after sampling. However, to recover this loss in performance, the
dimensions of the design space can be reduced to tailor it to a family of
target hardware. The new reduced design space results in a BLEU score increase
of approximately 1% for sub-optimal models from the original design space, with
a wide range for performance scaling between 0.356s - 1.526s for the GPU and
2.9s - 7.31s for the CPU.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Parry_H/0/1/0/all/0/1"&gt;Hishan Parry&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xun_L/0/1/0/all/0/1"&gt;Lei Xun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sabet_A/0/1/0/all/0/1"&gt;Amin Sabet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bi_J/0/1/0/all/0/1"&gt;Jia Bi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hare_J/0/1/0/all/0/1"&gt;Jonathon Hare&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Merrett_G/0/1/0/all/0/1"&gt;Geoff V. Merrett&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Certified Defense via Latent Space Randomized Smoothing with Orthogonal Encoders. (arXiv:2108.00491v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.00491</id>
        <link href="http://arxiv.org/abs/2108.00491"/>
        <updated>2021-08-03T02:06:35.142Z</updated>
        <summary type="html"><![CDATA[Randomized Smoothing (RS), being one of few provable defenses, has been
showing great effectiveness and scalability in terms of defending against
$\ell_2$-norm adversarial perturbations. However, the cost of MC sampling
needed in RS for evaluation is high and computationally expensive. To address
this issue, we investigate the possibility of performing randomized smoothing
and establishing the robust certification in the latent space of a network, so
that the overall dimensionality of tensors involved in computation could be
drastically reduced. To this end, we propose Latent Space Randomized Smoothing.
Another important aspect is that we use orthogonal modules, whose Lipschitz
property is known for free by design, to propagate the certified radius
estimated in the latent space back to the input space, providing valid
certifiable regions for the test samples in the input space. Experiments on
CIFAR10 and ImageNet show that our method achieves competitive certified
robustness but with a significant improvement of efficiency during the test
phase.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_H/0/1/0/all/0/1"&gt;Huimin Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1"&gt;Jiahao Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1"&gt;Furong Huang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lattice Paths for Persistent Diagrams. (arXiv:2105.00351v5 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.00351</id>
        <link href="http://arxiv.org/abs/2105.00351"/>
        <updated>2021-08-03T02:06:35.136Z</updated>
        <summary type="html"><![CDATA[Persistent homology has undergone significant development in recent years.
However, one outstanding challenge is to build a coherent statistical inference
procedure on persistent diagrams. In this paper, we first present a new lattice
path representation for persistent diagrams. We then develop a new exact
statistical inference procedure for lattice paths via combinatorial
enumerations. The lattice path method is applied to the topological
characterization of the protein structures of the COVID-19 virus. We
demonstrate that there are topological changes during the conformational change
of spike proteins.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Chung_M/0/1/0/all/0/1"&gt;Moo K. Chung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Ombao_H/0/1/0/all/0/1"&gt;Hernando Ombao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Zeroth-Order Alternating Randomized Gradient Projection Algorithms for General Nonconvex-Concave Minimax Problems. (arXiv:2108.00473v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2108.00473</id>
        <link href="http://arxiv.org/abs/2108.00473"/>
        <updated>2021-08-03T02:06:35.119Z</updated>
        <summary type="html"><![CDATA[In this paper, we study zeroth-order algorithms for nonconvex-concave minimax
problems, which have attracted widely attention in machine learning, signal
processing and many other fields in recent years. We propose a zeroth-order
alternating randomized gradient projection (ZO-AGP) algorithm for smooth
nonconvex-concave minimax problems, and its iteration complexity to obtain an
$\varepsilon$-stationary point is bounded by $\mathcal{O}(\varepsilon^{-4})$,
and the number of function value estimation is bounded by
$\mathcal{O}(d_{x}\varepsilon^{-4}+d_{y}\varepsilon^{-6})$ per iteration.
Moreover, we propose a zeroth-order block alternating randomized proximal
gradient algorithm (ZO-BAPG) for solving block-wise nonsmooth nonconvex-concave
minimax optimization problems, and the iteration complexity to obtain an
$\varepsilon$-stationary point is bounded by $\mathcal{O}(\varepsilon^{-4})$
and the number of function value estimation per iteration is bounded by
$\mathcal{O}(K d_{x}\varepsilon^{-4}+d_{y}\varepsilon^{-6})$. To the best of
our knowledge, this is the first time that zeroth-order algorithms with
iteration complexity gurantee are developed for solving both general smooth and
block-wise nonsmooth nonconvex-concave minimax problems. Numerical results on
data poisoning attack problem validate the efficiency of the proposed
algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zi Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Shen_J/0/1/0/all/0/1"&gt;Jingjing Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Ziqi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Dai_Y/0/1/0/all/0/1"&gt;Yuhong Dai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to synthesise the ageing brain without longitudinal data. (arXiv:1912.02620v5 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1912.02620</id>
        <link href="http://arxiv.org/abs/1912.02620"/>
        <updated>2021-08-03T02:06:35.113Z</updated>
        <summary type="html"><![CDATA[How will my face look when I get older? Or, for a more challenging question:
How will my brain look when I get older? To answer this question one must
devise (and learn from data) a multivariate auto-regressive function which
given an image and a desired target age generates an output image. While
collecting data for faces may be easier, collecting longitudinal brain data is
not trivial. We propose a deep learning-based method that learns to simulate
subject-specific brain ageing trajectories without relying on longitudinal
data. Our method synthesises images conditioned on two factors: age (a
continuous variable), and status of Alzheimer's Disease (AD, an ordinal
variable). With an adversarial formulation we learn the joint distribution of
brain appearance, age and AD status, and define reconstruction losses to
address the challenging problem of preserving subject identity. We compare with
several benchmarks using two widely used datasets. We evaluate the quality and
realism of synthesised images using ground-truth longitudinal data and a
pre-trained age predictor. We show that, despite the use of cross-sectional
data, our model learns patterns of gray matter atrophy in the middle temporal
gyrus in patients with AD. To demonstrate generalisation ability, we train on
one dataset and evaluate predictions on the other. In conclusion, our model
shows an ability to separate age, disease influence and anatomy using only 2D
cross-sectional data that should should be useful in large studies into
neurodegenerative disease, that aim to combine several data sources. To
facilitate such future studies by the community at large our code is made
available at https://github.com/xiat0616/BrainAgeing.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Xia_T/0/1/0/all/0/1"&gt;Tian Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chartsias_A/0/1/0/all/0/1"&gt;Agisilaos Chartsias&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chengjia Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tsaftaris_S/0/1/0/all/0/1"&gt;Sotirios A. Tsaftaris&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Decision Making in Monopoly using a Hybrid Deep Reinforcement Learning Approach. (arXiv:2103.00683v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.00683</id>
        <link href="http://arxiv.org/abs/2103.00683"/>
        <updated>2021-08-03T02:06:35.107Z</updated>
        <summary type="html"><![CDATA[Learning to adapt and make real-time informed decisions in a dynamic and
complex environment is a challenging problem. Monopoly is a popular strategic
board game that requires players to make multiple decisions during the game.
Decision-making in Monopoly involves many real-world elements such as
strategizing, luck, and modeling of opponent's policies. In this paper, we
present novel representations for the state and action space for the full
version of Monopoly and define an improved reward function. Using these, we
show that our deep reinforcement learning agent can learn winning strategies
for Monopoly against different fixed-policy agents. In Monopoly, players can
take multiple actions even if it is not their turn to roll the dice. Some of
these actions occur more frequently than others, resulting in a skewed
distribution that adversely affects the performance of the learning agent. To
tackle the non-uniform distribution of actions, we propose a hybrid approach
that combines deep reinforcement learning (for frequent but complex decisions)
with a fixed policy approach (for infrequent but straightforward decisions).
Experimental results show that our hybrid agent outperforms a standard deep
reinforcement learning agent by 30% in the number of games won against
fixed-policy agents.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Haliem_M/0/1/0/all/0/1"&gt;Marina Haliem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bonjour_T/0/1/0/all/0/1"&gt;Trevor Bonjour&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alsalem_A/0/1/0/all/0/1"&gt;Aala Alsalem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Thomas_S/0/1/0/all/0/1"&gt;Shilpa Thomas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Hongyu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aggarwal_V/0/1/0/all/0/1"&gt;Vaneet Aggarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhargava_B/0/1/0/all/0/1"&gt;Bharat Bhargava&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kejriwal_M/0/1/0/all/0/1"&gt;Mayank Kejriwal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Helmholtzian Eigenmap: Topological feature discovery & edge flow learning from point cloud data. (arXiv:2103.07626v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.07626</id>
        <link href="http://arxiv.org/abs/2103.07626"/>
        <updated>2021-08-03T02:06:35.101Z</updated>
        <summary type="html"><![CDATA[The manifold Helmholtzian (1-Laplacian) operator $\Delta_1$ elegantly
generalizes the Laplace-Beltrami operator to vector fields on a manifold
$\mathcal M$. In this work, we propose the estimation of the manifold
Helmholtzian from point cloud data by a weighted 1-Laplacian $\mathbf{\mathcal
L}_1$. While higher order Laplacians ave been introduced and studied, this work
is the first to present a graph Helmholtzian constructed from a simplicial
complex as an estimator for the continuous operator in a non-parametric
setting. Equipped with the geometric and topological information about
$\mathcal M$, the Helmholtzian is a useful tool for the analysis of flows and
vector fields on $\mathcal M$ via the Helmholtz-Hodge theorem. In addition, the
$\mathbf{\mathcal L}_1$ allows the smoothing, prediction, and feature
extraction of the flows. We demonstrate these possibilities on substantial sets
of synthetic and real point cloud datasets with non-trivial topological
structures; and provide theoretical results on the limit of $\mathbf{\mathcal
L}_1$ to $\Delta_1$.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yu-Chia Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Meila_M/0/1/0/all/0/1"&gt;Marina Meil&amp;#x103;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kevrekidis_I/0/1/0/all/0/1"&gt;Ioannis G. Kevrekidis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Global Convergence of Multi-Agent Policy Gradient in Markov Potential Games. (arXiv:2106.01969v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.01969</id>
        <link href="http://arxiv.org/abs/2106.01969"/>
        <updated>2021-08-03T02:06:35.094Z</updated>
        <summary type="html"><![CDATA[Potential games are arguably one of the most important and widely studied
classes of normal form games. They define the archetypal setting of multi-agent
coordination as all agent utilities are perfectly aligned with each other via a
common potential function. Can this intuitive framework be transplanted in the
setting of Markov Games? What are the similarities and differences between
multi-agent coordination with and without state dependence? We present a novel
definition of Markov Potential Games (MPG) that generalizes prior attempts at
capturing complex stateful multi-agent coordination. Counter-intuitively,
insights from normal-form potential games do not carry over as MPGs can consist
of settings where state-games can be zero-sum games. In the opposite direction,
Markov games where every state-game is a potential game are not necessarily
MPGs. Nevertheless, MPGs showcase standard desirable properties such as the
existence of deterministic Nash policies. In our main technical result, we
prove fast convergence of independent policy gradient to Nash policies by
adapting recent gradient dominance property arguments developed for single
agent MDPs to multi-agent learning settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Leonardos_S/0/1/0/all/0/1"&gt;Stefanos Leonardos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Overman_W/0/1/0/all/0/1"&gt;Will Overman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Panageas_I/0/1/0/all/0/1"&gt;Ioannis Panageas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Piliouras_G/0/1/0/all/0/1"&gt;Georgios Piliouras&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Leveraging Probabilistic Circuits for Nonparametric Multi-Output Regression. (arXiv:2106.08687v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.08687</id>
        <link href="http://arxiv.org/abs/2106.08687"/>
        <updated>2021-08-03T02:06:35.088Z</updated>
        <summary type="html"><![CDATA[Inspired by recent advances in the field of expert-based approximations of
Gaussian processes (GPs), we present an expert-based approach to large-scale
multi-output regression using single-output GP experts. Employing a deeply
structured mixture of single-output GPs encoded via a probabilistic circuit
allows us to capture correlations between multiple output dimensions
accurately. By recursively partitioning the covariate space and the output
space, posterior inference in our model reduces to inference on single-output
GP experts, which only need to be conditioned on a small subset of the
observations. We show that inference can be performed exactly and efficiently
in our model, that it can capture correlations between output dimensions and,
hence, often outperforms approaches that do not incorporate inter-output
correlations, as demonstrated on several data sets in terms of the negative log
predictive density.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1"&gt;Zhongjie Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1"&gt;Mingye Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Trapp_M/0/1/0/all/0/1"&gt;Martin Trapp&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Skryagin_A/0/1/0/all/0/1"&gt;Arseny Skryagin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1"&gt;Kristian Kersting&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[COfEE: A Comprehensive Ontology for Event Extraction from text, with an online annotation tool. (arXiv:2107.10326v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.10326</id>
        <link href="http://arxiv.org/abs/2107.10326"/>
        <updated>2021-08-03T02:06:35.069Z</updated>
        <summary type="html"><![CDATA[Data is published on the web over time in great volumes, but majority of the
data is unstructured, making it hard to understand and difficult to interpret.
Information Extraction (IE) methods extract structured information from
unstructured data. One of the challenging IE tasks is Event Extraction (EE)
which seeks to derive information about specific incidents and their actors
from the text. EE is useful in many domains such as building a knowledge base,
information retrieval, summarization and online monitoring systems. In the past
decades, some event ontologies like ACE, CAMEO and ICEWS were developed to
define event forms, actors and dimensions of events observed in the text. These
event ontologies still have some shortcomings such as covering only a few
topics like political events, having inflexible structure in defining argument
roles, lack of analytical dimensions, and complexity in choosing event
sub-types. To address these concerns, we propose an event ontology, namely
COfEE, that incorporates both expert domain knowledge, previous ontologies and
a data-driven approach for identifying events from text. COfEE consists of two
hierarchy levels (event types and event sub-types) that include new categories
relating to environmental issues, cyberspace, criminal activity and natural
disasters which need to be monitored instantly. Also, dynamic roles according
to each event sub-type are defined to capture various dimensions of events. In
a follow-up experiment, the proposed ontology is evaluated on Wikipedia events,
and it is shown to be general and comprehensive. Moreover, in order to
facilitate the preparation of gold-standard data for event extraction, a
language-independent online tool is presented based on COfEE.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Balali_A/0/1/0/all/0/1"&gt;Ali Balali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Asadpour_M/0/1/0/all/0/1"&gt;Masoud Asadpour&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jafari_S/0/1/0/all/0/1"&gt;Seyed Hossein Jafari&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Millimeter Wave Communications with an Intelligent Reflector: Performance Optimization and Distributional Reinforcement Learning. (arXiv:2002.10572v3 [cs.IT] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2002.10572</id>
        <link href="http://arxiv.org/abs/2002.10572"/>
        <updated>2021-08-03T02:06:35.062Z</updated>
        <summary type="html"><![CDATA[In this paper, a novel framework is proposed to optimize the downlink
multi-user communication of a millimeter wave base station, which is assisted
by a reconfigurable intelligent reflector (IR). In particular, a channel
estimation approach is developed to measure the channel state information (CSI)
in real-time. First, for a perfect CSI scenario, the precoding transmission of
the BS and the reflection coefficient of the IR are jointly optimized, via an
iterative approach, so as to maximize the sum of downlink rates towards
multiple users. Next, in the imperfect CSI scenario, a distributional
reinforcement learning (DRL) approach is proposed to learn the optimal IR
reflection and maximize the expectation of downlink capacity. In order to model
the transmission rate's probability distribution, a learning algorithm, based
on quantile regression (QR), is developed, and the proposed QR-DRL method is
proved to converge to a stable distribution of downlink transmission rate.
Simulation results show that, in the error-free CSI scenario, the proposed
approach yields over 30% and 2-fold increase in the downlink sum-rate, compared
with a fixed IR reflection scheme and direct transmission scheme, respectively.
Simulation results also show that by deploying more IR elements, the downlink
sum-rate can be significantly improved. However, as the number of IR components
increases, more time is required for channel estimation, and the slope of
increase in the IR-aided transmission rate will become smaller. Furthermore,
under limited knowledge of CSI, simulation results show that the proposed
QR-DRL method, which learns a full distribution of the downlink rate, yields a
better prediction accuracy and improves the downlink rate by 10% for online
deployments, compared with a Q-learning baseline.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Qianqian Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saad_W/0/1/0/all/0/1"&gt;Walid Saad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bennis_M/0/1/0/all/0/1"&gt;Mehdi Bennis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Continual Learning for Recurrent Neural Networks: an Empirical Evaluation. (arXiv:2103.07492v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.07492</id>
        <link href="http://arxiv.org/abs/2103.07492"/>
        <updated>2021-08-03T02:06:35.055Z</updated>
        <summary type="html"><![CDATA[Learning continuously during all model lifetime is fundamental to deploy
machine learning solutions robust to drifts in the data distribution. Advances
in Continual Learning (CL) with recurrent neural networks could pave the way to
a large number of applications where incoming data is non stationary, like
natural language processing and robotics. However, the existing body of work on
the topic is still fragmented, with approaches which are application-specific
and whose assessment is based on heterogeneous learning protocols and datasets.
In this paper, we organize the literature on CL for sequential data processing
by providing a categorization of the contributions and a review of the
benchmarks. We propose two new benchmarks for CL with sequential data based on
existing datasets, whose characteristics resemble real-world applications. We
also provide a broad empirical evaluation of CL and Recurrent Neural Networks
in class-incremental scenario, by testing their ability to mitigate forgetting
with a number of different strategies which are not specific to sequential data
processing. Our results highlight the key role played by the sequence length
and the importance of a clear specification of the CL scenario.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cossu_A/0/1/0/all/0/1"&gt;Andrea Cossu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Carta_A/0/1/0/all/0/1"&gt;Antonio Carta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lomonaco_V/0/1/0/all/0/1"&gt;Vincenzo Lomonaco&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bacciu_D/0/1/0/all/0/1"&gt;Davide Bacciu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MSMatch: Semi-Supervised Multispectral Scene Classification with Few Labels. (arXiv:2103.10368v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.10368</id>
        <link href="http://arxiv.org/abs/2103.10368"/>
        <updated>2021-08-03T02:06:35.048Z</updated>
        <summary type="html"><![CDATA[Supervised learning techniques are at the center of many tasks in remote
sensing. Unfortunately, these methods, especially recent deep learning methods,
often require large amounts of labeled data for training. Even though
satellites acquire large amounts of data, labeling the data is often tedious,
expensive and requires expert knowledge. Hence, improved methods that require
fewer labeled samples are needed. We present MSMatch, the first semi-supervised
learning approach competitive with supervised methods on scene classification
on the EuroSAT and UC Merced Land Use benchmark datasets. We test both RGB and
multispectral images of EuroSAT and perform various ablation studies to
identify the critical parts of the model. The trained neural network achieves
state-of-the-art results on EuroSAT with an accuracy that is up to 19.76%
better than previous methods depending on the number of labeled training
examples. With just five labeled examples per class, we reach 94.53% and 95.86%
accuracy on the EuroSAT RGB and multispectral datasets, respectively. On the UC
Merced Land Use dataset, we outperform previous works by up to 5.59% and reach
90.71% with five labeled examples. Our results show that MSMatch is capable of
greatly reducing the requirements for labeled data. It translates well to
multispectral data and should enable various applications that are currently
infeasible due to a lack of labeled data. We provide the source code of MSMatch
online to enable easy reproduction and quick adoption.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gomez_P/0/1/0/all/0/1"&gt;Pablo G&amp;#xf3;mez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meoni_G/0/1/0/all/0/1"&gt;Gabriele Meoni&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Momentum-based Gradient Methods in Multi-Objective Recommendation. (arXiv:2009.04695v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.04695</id>
        <link href="http://arxiv.org/abs/2009.04695"/>
        <updated>2021-08-03T02:06:35.040Z</updated>
        <summary type="html"><![CDATA[Multi-objective gradient methods are becoming the standard for solving
multi-objective problems. Among others, they show promising results in
developing multi-objective recommender systems with both correlated and
conflicting objectives. Classic multi-gradient descent usually relies on the
combination of the gradients, not including the computation of first and second
moments of the gradients. This leads to a brittle behavior and misses important
areas in the solution space. In this work, we create a multi-objective
model-agnostic Adamize method that leverages the benefits of the Adam optimizer
in single-objective problems. This corrects and stabilizes the gradients of
every objective before calculating a common gradient descent vector that
optimizes all the objectives simultaneously. We evaluate the benefits of
multi-objective Adamize on two multi-objective recommender systems and for
three different objective combinations, both correlated or conflicting. We
report significant improvements, measured with three different Pareto front
metrics: hypervolume, coverage, and spacing. Finally, we show that the Adamized
Pareto front strictly dominates the previous one on multiple objective pairs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mitrevski_B/0/1/0/all/0/1"&gt;Blagoj Mitrevski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Filipovic_M/0/1/0/all/0/1"&gt;Milena Filipovic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Antognini_D/0/1/0/all/0/1"&gt;Diego Antognini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Glaude_E/0/1/0/all/0/1"&gt;Emma Lejal Glaude&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Faltings_B/0/1/0/all/0/1"&gt;Boi Faltings&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Musat_C/0/1/0/all/0/1"&gt;Claudiu Musat&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[M2Lens: Visualizing and Explaining Multimodal Models for Sentiment Analysis. (arXiv:2107.08264v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.08264</id>
        <link href="http://arxiv.org/abs/2107.08264"/>
        <updated>2021-08-03T02:06:35.025Z</updated>
        <summary type="html"><![CDATA[Multimodal sentiment analysis aims to recognize people's attitudes from
multiple communication channels such as verbal content (i.e., text), voice, and
facial expressions. It has become a vibrant and important research topic in
natural language processing. Much research focuses on modeling the complex
intra- and inter-modal interactions between different communication channels.
However, current multimodal models with strong performance are often
deep-learning-based techniques and work like black boxes. It is not clear how
models utilize multimodal information for sentiment predictions. Despite recent
advances in techniques for enhancing the explainability of machine learning
models, they often target unimodal scenarios (e.g., images, sentences), and
little research has been done on explaining multimodal models. In this paper,
we present an interactive visual analytics system, M2Lens, to visualize and
explain multimodal models for sentiment analysis. M2Lens provides explanations
on intra- and inter-modal interactions at the global, subset, and local levels.
Specifically, it summarizes the influence of three typical interaction types
(i.e., dominance, complement, and conflict) on the model predictions. Moreover,
M2Lens identifies frequent and influential multimodal features and supports the
multi-faceted exploration of model behaviors from language, acoustic, and
visual modalities. Through two case studies and expert interviews, we
demonstrate our system can help users gain deep insights into the multimodal
models for sentiment analysis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xingbo Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1"&gt;Jianben He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1"&gt;Zhihua Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1"&gt;Muqiao Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qu_H/0/1/0/all/0/1"&gt;Huamin Qu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Undecidability of Learnability. (arXiv:2106.01382v2 [cs.CC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.01382</id>
        <link href="http://arxiv.org/abs/2106.01382"/>
        <updated>2021-08-03T02:06:35.018Z</updated>
        <summary type="html"><![CDATA[Machine learning researchers and practitioners steadily enlarge the multitude
of successful learning models. They achieve this through in-depth theoretical
analyses and experiential heuristics. However, there is no known
general-purpose procedure for rigorously evaluating whether newly proposed
models indeed successfully learn from data. We show that such a procedure
cannot exist. For PAC binary classification, uniform and universal online
learning, and exact learning through teacher-learner interactions, learnability
is in general undecidable, both in the sense of independence of the axioms in a
formal system and in the sense of uncomputability. Our proofs proceed via
computable constructions of function classes that encode the consistency
problem for formal systems and the halting problem for Turing machines into
complexity measures that characterize learnability. Our work shows that
undecidability appears in the theoretical foundations of machine learning:
There is no one-size-fits-all algorithm for deciding whether a machine learning
model can be successful. We cannot in general automatize the process of
assessing new learning models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Caro_M/0/1/0/all/0/1"&gt;Matthias C. Caro&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Tunnel Gaussian Process Model for Learning Interpretable Flight's Landing Parameters. (arXiv:2011.09335v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.09335</id>
        <link href="http://arxiv.org/abs/2011.09335"/>
        <updated>2021-08-03T02:06:35.012Z</updated>
        <summary type="html"><![CDATA[Approach and landing accidents have resulted in a significant number of hull
losses worldwide. Technologies (e.g., instrument landing system) and procedures
(e.g., stabilized approach criteria) have been developed to reduce the risks.
In this paper, we propose a data-driven method to learn and interpret flight's
approach and landing parameters to facilitate comprehensible and actionable
insights into flight dynamics. Specifically, we develop two variants of tunnel
Gaussian process (TGP) models to elucidate aircraft's approach and landing
dynamics using advanced surface movement guidance and control system (A-SMGCS)
data, which then indicates the stability of flight. TGP hybridizes the
strengths of sparse variational Gaussian process and polar Gaussian process to
learn from a large amount of data in cylindrical coordinates. We examine TGP
qualitatively and quantitatively by synthesizing three complex trajectory
datasets and compared TGP against existing methods on trajectory learning.
Empirically, TGP demonstrates superior modeling performance. When applied to
operational A-SMGCS data, TGP provides the generative probabilistic description
of landing dynamics and interpretable tunnel views of approach and landing
parameters. These probabilistic tunnel models can facilitate the analysis of
procedure adherence and augment existing aircrew and air traffic controllers'
displays during the approach and landing procedures, enabling necessary
corrective actions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Goh_S/0/1/0/all/0/1"&gt;Sim Kuan Goh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_N/0/1/0/all/0/1"&gt;Narendra Pratap Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lim_Z/0/1/0/all/0/1"&gt;Zhi Jun Lim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alam_S/0/1/0/all/0/1"&gt;Sameer Alam&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tensor completion using geodesics on Segre manifolds. (arXiv:2108.00735v1 [math.DG])]]></title>
        <id>http://arxiv.org/abs/2108.00735</id>
        <link href="http://arxiv.org/abs/2108.00735"/>
        <updated>2021-08-03T02:06:35.005Z</updated>
        <summary type="html"><![CDATA[We propose a Riemannian conjugate gradient (CG) optimization method for
finding low rank approximations of incomplete tensors. Our main contribution
consists of an explicit expression of the geodesics on the Segre manifold.
These are exploited in our algorithm to perform the retractions. We apply our
method to movie rating predictions in a recommender system for the MovieLens
dataset, and identification of pure fluorophores via fluorescent spectroscopy
with missing data. In this last application, we recover the tensor
decomposition from less than $10\%$ of the data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Swijsen_L/0/1/0/all/0/1"&gt;Lars Swijsen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Veken_J/0/1/0/all/0/1"&gt;Joeri Van der Veken&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Vannieuwenhoven_N/0/1/0/all/0/1"&gt;Nick Vannieuwenhoven&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Optimising cost vs accuracy of decentralised analytics in fog computing environments. (arXiv:2012.05266v3 [cs.DC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.05266</id>
        <link href="http://arxiv.org/abs/2012.05266"/>
        <updated>2021-08-03T02:06:34.987Z</updated>
        <summary type="html"><![CDATA[The exponential growth of devices and data at the edges of the Internet is
rising scalability and privacy concerns on approaches based exclusively on
remote cloud platforms. Data gravity, a fundamental concept in Fog Computing,
points towards decentralisation of computation for data analysis, as a viable
alternative to address those concerns. Decentralising AI tasks on several
cooperative devices means identifying the optimal set of locations or
Collection Points (CP for short) to use, in the continuum between full
centralisation (i.e., all data on a single device) and full decentralisation
(i.e., data on source locations). We propose an analytical framework able to
find the optimal operating point in this continuum, linking the accuracy of the
learning task with the corresponding network and computational cost for moving
data and running the distributed training at the CPs. We show through
simulations that the model accurately predicts the optimal trade-off, quite
often an intermediate point between full centralisation and full
decentralisation, showing also a significant cost saving w.r.t. both of them.
Finally, the analytical model admits closed-form or numeric solutions, making
it not only a performance evaluation instrument but also a design tool to
configure a given distributed learning task optimally before its deployment.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Valerio_L/0/1/0/all/0/1"&gt;Lorenzo Valerio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Passarella_A/0/1/0/all/0/1"&gt;Andrea Passarella&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Conti_M/0/1/0/all/0/1"&gt;Marco Conti&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Control an Unstable System with One Minute of Data: Leveraging Gaussian Process Differentiation in Predictive Control. (arXiv:2103.04548v2 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.04548</id>
        <link href="http://arxiv.org/abs/2103.04548"/>
        <updated>2021-08-03T02:06:34.981Z</updated>
        <summary type="html"><![CDATA[We present a straightforward and efficient way to control unstable robotic
systems using an estimated dynamics model. Specifically, we show how to exploit
the differentiability of Gaussian Processes to create a state-dependent
linearized approximation of the true continuous dynamics that can be integrated
with model predictive control. Our approach is compatible with most Gaussian
process approaches for system identification, and can learn an accurate model
using modest amounts of training data. We validate our approach by learning the
dynamics of an unstable system such as a segway with a 7-D state space and 2-D
input space (using only one minute of data), and we show that the resulting
controller is robust to unmodelled dynamics and disturbances, while
state-of-the-art control methods based on nominal models can fail under small
perturbations. Code is open sourced at
https://github.com/learning-and-control/core .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rodriguez_I/0/1/0/all/0/1"&gt;Ivan D. Jimenez Rodriguez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rosolia_U/0/1/0/all/0/1"&gt;Ugo Rosolia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ames_A/0/1/0/all/0/1"&gt;Aaron D. Ames&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yue_Y/0/1/0/all/0/1"&gt;Yisong Yue&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Distributed Conditional Generative Adversarial Networks (GANs) for Data-Driven Millimeter Wave Communications in UAV Networks. (arXiv:2102.01751v2 [cs.IT] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.01751</id>
        <link href="http://arxiv.org/abs/2102.01751"/>
        <updated>2021-08-03T02:06:34.976Z</updated>
        <summary type="html"><![CDATA[In this paper, a novel framework is proposed to perform data-driven
air-to-ground (A2G) channel estimation for millimeter wave (mmWave)
communications in an unmanned aerial vehicle (UAV) wireless network. First, an
effective channel estimation approach is developed to collect mmWave channel
information, allowing each UAV to train a stand-alone channel model via a
conditional generative adversarial network (CGAN) along each beamforming
direction. Next, in order to expand the application scenarios of the trained
channel model into a broader spatial-temporal domain, a cooperative framework,
based on a distributed CGAN architecture, is developed, allowing each UAV to
collaboratively learn the mmWave channel distribution in a fully-distributed
manner. To guarantee an efficient learning process, necessary and sufficient
conditions for the optimal UAV network topology that maximizes the learning
rate for cooperative channel modeling are derived, and the optimal CGAN
learning solution per UAV is subsequently characterized, based on the
distributed network structure. Simulation results show that the proposed
distributed CGAN approach is robust to the local training error at each UAV.
Meanwhile, a larger airborne network size requires more communication resources
per UAV to guarantee an efficient learning rate. The results also show that,
compared with a stand-alone CGAN without information sharing and two other
distributed schemes, namely: A multi-discriminator CGAN and a federated CGAN
method, the proposed distributed CGAN approach yields a higher modeling
accuracy while learning the environment, and it achieves a larger average data
rate in the online performance of UAV downlink mmWave communications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1"&gt;Qianqian Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ferdowsi_A/0/1/0/all/0/1"&gt;Aidin Ferdowsi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saad_W/0/1/0/all/0/1"&gt;Walid Saad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bennis_M/0/1/0/all/0/1"&gt;Mehdi Bennis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Gates are not what you need in RNNs. (arXiv:2108.00527v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.00527</id>
        <link href="http://arxiv.org/abs/2108.00527"/>
        <updated>2021-08-03T02:06:34.968Z</updated>
        <summary type="html"><![CDATA[Recurrent neural networks have flourished in many areas. Consequently, we can
see new RNN cells being developed continuously, usually by creating or using
gates in a new, original way. But what if we told you that gates in RNNs are
redundant? In this paper, we propose a new recurrent cell called Residual
Recurrent Unit (RRU) which beats traditional cells and does not employ a single
gate. It is based on the residual shortcut connection together with linear
transformations, ReLU, and normalization. To evaluate our cell's effectiveness,
we compare its performance against the widely-used GRU and LSTM cells and the
recently proposed Mogrifier LSTM on several tasks including, polyphonic music
modeling, language modeling, and sentiment analysis. Our experiments show that
RRU outperforms the traditional gated units on most of these tasks. Also, it
has better robustness to parameter selection, allowing immediate application in
new tasks without much tuning. We have implemented the RRU in TensorFlow, and
the code is made available at https://github.com/LUMII-Syslab/RRU .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zakovskis_R/0/1/0/all/0/1"&gt;Ronalds Zakovskis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Draguns_A/0/1/0/all/0/1"&gt;Andis Draguns&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gaile_E/0/1/0/all/0/1"&gt;Eliza Gaile&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ozolins_E/0/1/0/all/0/1"&gt;Emils Ozolins&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Freivalds_K/0/1/0/all/0/1"&gt;Karlis Freivalds&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Uniformly Consistent Estimator of non-Gaussian Causal Effects Under the k-Triangle-Faithfulness Assumption. (arXiv:2107.01333v2 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.01333</id>
        <link href="http://arxiv.org/abs/2107.01333"/>
        <updated>2021-08-03T02:06:34.962Z</updated>
        <summary type="html"><![CDATA[Kalisch and B\"{u}hlmann (2007) showed that for linear Gaussian models, under
the Causal Markov Assumption, the Strong Causal Faithfulness Assumption, and
the assumption of causal sufficiency, the PC algorithm is a uniformly
consistent estimator of the Markov Equivalence Class of the true causal DAG for
linear Gaussian models; it follows from this that for the identifiable causal
effects in the Markov Equivalence Class, there are uniformly consistent
estimators of causal effects as well. The $k$-Triangle-Faithfulness Assumption
is a strictly weaker assumption that avoids some implausible implications of
the Strong Causal Faithfulness Assumption and also allows for uniformly
consistent estimates of Markov Equivalence Classes (in a weakened sense), and
of identifiable causal effects. However, both of these assumptions are
restricted to linear Gaussian models. We propose the Generalized $k$-Triangle
Faithfulness, which can be applied to any smooth distribution. In addition,
under the Generalized $k$-Triangle Faithfulness Assumption, we describe the
Edge Estimation Algorithm that provides uniformly consistent estimates of
causal effects in some cases (and otherwise outputs "can't tell"), and the
\textit{Very Conservative }$SGS$ Algorithm that (in a slightly weaker sense) is
a uniformly consistent estimator of the Markov equivalence class of the true
DAG.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shuyan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Spirtes_P/0/1/0/all/0/1"&gt;Peter Spirtes&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pathwise Conditioning of Gaussian Processes. (arXiv:2011.04026v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.04026</id>
        <link href="http://arxiv.org/abs/2011.04026"/>
        <updated>2021-08-03T02:06:34.956Z</updated>
        <summary type="html"><![CDATA[As Gaussian processes are used to answer increasingly complex questions,
analytic solutions become scarcer and scarcer. Monte Carlo methods act as a
convenient bridge for connecting intractable mathematical expressions with
actionable estimates via sampling. Conventional approaches for simulating
Gaussian process posteriors view samples as draws from marginal distributions
of process values at finite sets of input locations. This distribution-centric
characterization leads to generative strategies that scale cubically in the
size of the desired random vector. These methods are prohibitively expensive in
cases where we would, ideally, like to draw high-dimensional vectors or even
continuous sample paths. In this work, we investigate a different line of
reasoning: rather than focusing on distributions, we articulate Gaussian
conditionals at the level of random variables. We show how this pathwise
interpretation of conditioning gives rise to a general family of approximations
that lend themselves to efficiently sampling Gaussian process posteriors.
Starting from first principles, we derive these methods and analyze the
approximation errors they introduce. We, then, ground these results by
exploring the practical implications of pathwise conditioning in various
applied settings, such as global optimization and reinforcement learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Wilson_J/0/1/0/all/0/1"&gt;James T. Wilson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Borovitskiy_V/0/1/0/all/0/1"&gt;Viacheslav Borovitskiy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Terenin_A/0/1/0/all/0/1"&gt;Alexander Terenin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Mostowsky_P/0/1/0/all/0/1"&gt;Peter Mostowsky&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Deisenroth_M/0/1/0/all/0/1"&gt;Marc Peter Deisenroth&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Vision Xformers: Efficient Attention for Image Classification. (arXiv:2107.02239v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.02239</id>
        <link href="http://arxiv.org/abs/2107.02239"/>
        <updated>2021-08-03T02:06:34.939Z</updated>
        <summary type="html"><![CDATA[We propose three improvements to vision transformers (ViT) to reduce the
number of trainable parameters without compromising classification accuracy. We
address two shortcomings of the early ViT architectures -- quadratic bottleneck
of the attention mechanism and the lack of an inductive bias in their
architectures that rely on unrolling the two-dimensional image structure.
Linear attention mechanisms overcome the bottleneck of quadratic complexity,
which restricts application of transformer models in vision tasks. We modify
the ViT architecture to work on longer sequence data by replacing the quadratic
attention with efficient transformers, such as Performer, Linformer and
Nystr\"omformer of linear complexity creating Vision X-formers (ViX). We show
that all three versions of ViX may be more accurate than ViT for image
classification while using far fewer parameters and computational resources. We
also compare their performance with FNet and multi-layer perceptron (MLP)
mixer. We further show that replacing the initial linear embedding layer by
convolutional layers in ViX further increases their performance. Furthermore,
our tests on recent vision transformer models, such as LeViT, Convolutional
vision Transformer (CvT), Compact Convolutional Transformer (CCT) and
Pooling-based Vision Transformer (PiT) show that replacing the attention with
Nystr\"omformer or Performer saves GPU usage and memory without deteriorating
the classification accuracy. We also show that replacing the standard learnable
1D position embeddings in ViT with Rotary Position Embedding (RoPE) give
further improvements in accuracy. Incorporating these changes can democratize
transformers by making them accessible to those with limited data and computing
resources.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jeevan_P/0/1/0/all/0/1"&gt;Pranav Jeevan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sethi_A/0/1/0/all/0/1"&gt;Amit Sethi&lt;/a&gt; (Indian Institute of Technology Bombay)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[QuantumNAS: Noise-Adaptive Search for Robust Quantum Circuits. (arXiv:2107.10845v2 [quant-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.10845</id>
        <link href="http://arxiv.org/abs/2107.10845"/>
        <updated>2021-08-03T02:06:34.932Z</updated>
        <summary type="html"><![CDATA[Quantum noise is the key challenge in Noisy Intermediate-Scale Quantum (NISQ)
computers. Previous work for mitigating noise has primarily focused on
gate-level or pulse-level noise-adaptive compilation. However, limited research
efforts have explored a higher level of optimization by making the quantum
circuits themselves resilient to noise.

We propose QuantumNAS, a comprehensive framework for noise-adaptive co-search
of the variational circuit and qubit mapping. Variational quantum circuits are
a promising approach for constructing QML and quantum simulation. However,
finding the best variational circuit and its optimal parameters is challenging
due to the large design space and parameter training cost. We propose to
decouple the circuit search and parameter training by introducing a novel
SuperCircuit. The SuperCircuit is constructed with multiple layers of
pre-defined parameterized gates and trained by iteratively sampling and
updating the parameter subsets (SubCircuits) of it. It provides an accurate
estimation of SubCircuits performance trained from scratch. Then we perform an
evolutionary co-search of SubCircuit and its qubit mapping. The SubCircuit
performance is estimated with parameters inherited from SuperCircuit and
simulated with real device noise models. Finally, we perform iterative gate
pruning and finetuning to remove redundant gates.

Extensively evaluated with 12 QML and VQE benchmarks on 10 quantum comput,
QuantumNAS significantly outperforms baselines. For QML, QuantumNAS is the
first to demonstrate over 95% 2-class, 85% 4-class, and 32% 10-class
classification accuracy on real QC. It also achieves the lowest eigenvalue for
VQE tasks on H2, H2O, LiH, CH4, BeH2 compared with UCCSD. We also open-source
QuantumEngine (https://github.com/mit-han-lab/pytorch-quantum) for fast
training of parameterized quantum circuits to facilitate future research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hanrui Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Ding_Y/0/1/0/all/0/1"&gt;Yongshan Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Gu_J/0/1/0/all/0/1"&gt;Jiaqi Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Lin_Y/0/1/0/all/0/1"&gt;Yujun Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Pan_D/0/1/0/all/0/1"&gt;David Z. Pan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Chong_F/0/1/0/all/0/1"&gt;Frederic T. Chong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Han_S/0/1/0/all/0/1"&gt;Song Han&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Eager Splitting Strategy for Online Decision Trees. (arXiv:2010.10935v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.10935</id>
        <link href="http://arxiv.org/abs/2010.10935"/>
        <updated>2021-08-03T02:06:34.925Z</updated>
        <summary type="html"><![CDATA[Decision tree ensembles are widely used in practice. In this work, we study
in ensemble settings the effectiveness of replacing the split strategy for the
state-of-the-art online tree learner, Hoeffding Tree, with a rigorous but more
eager splitting strategy that we had previously published as Hoeffding AnyTime
Tree. Hoeffding AnyTime Tree (HATT), uses the Hoeffding Test to determine
whether the current best candidate split is superior to the current split, with
the possibility of revision, while Hoeffding Tree aims to determine whether the
top candidate is better than the second best and if a test is selected, fixes
it for all posterity. HATT converges to the ideal batch tree while Hoeffding
Tree does not. We find that HATT is an efficacious base learner for online
bagging and online boosting ensembles. On UCI and synthetic streams, HATT as a
base learner outperforms HT within a 0.05 significance level for the majority
of tested ensembles on what we believe is the largest and most comprehensive
set of testbenches in the online learning literature. Our results indicate that
HATT is a superior alternative to Hoeffding Tree in a large number of ensemble
settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Manapragada_C/0/1/0/all/0/1"&gt;Chaitanya Manapragada&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gomes_H/0/1/0/all/0/1"&gt;Heitor M Gomes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salehi_M/0/1/0/all/0/1"&gt;Mahsa Salehi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bifet_A/0/1/0/all/0/1"&gt;Albert Bifet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Webb_G/0/1/0/all/0/1"&gt;Geoffrey I Webb&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[i-Pulse: A NLP based novel approach for employee engagement in logistics organization. (arXiv:2106.07341v1 [cs.SI] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2106.07341</id>
        <link href="http://arxiv.org/abs/2106.07341"/>
        <updated>2021-08-03T02:06:34.897Z</updated>
        <summary type="html"><![CDATA[Although most logistics and freight forwarding organizations, in one way or
another, claim to have core values. The engagement of employees is a vast
structure that affects almost every part of the company's core environmental
values. There is little theoretical knowledge about the relationship between
firms and the engagement of employees. Based on research literature, this paper
aims to provide a novel approach for insight around employee engagement in a
logistics organization by implementing deep natural language processing
concepts. The artificial intelligence-enabled solution named Intelligent Pulse
(I-Pulse) can evaluate hundreds and thousands of pulse survey comments and
provides the actionable insights and gist of employee feedback. I-Pulse allows
the stakeholders to think in new ways in their organization, helping them to
have a powerful influence on employee engagement, retention, and efficiency.
This study is of corresponding interest to researchers and practitioners.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Garg_R/0/1/0/all/0/1"&gt;Rachit Garg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kiwelekar_A/0/1/0/all/0/1"&gt;Arvind W Kiwelekar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Netak_L/0/1/0/all/0/1"&gt;Laxman D Netak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghodake_A/0/1/0/all/0/1"&gt;Akshay Ghodake&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Rationally Inattentive Utility Maximization for Interpretable Deep Image Classification. (arXiv:2102.04594v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.04594</id>
        <link href="http://arxiv.org/abs/2102.04594"/>
        <updated>2021-08-03T02:06:34.896Z</updated>
        <summary type="html"><![CDATA[Are deep convolutional neural networks (CNNs) for image classification
explainable by utility maximization with information acquisition costs? We
demonstrate that deep CNNs behave equivalently (in terms of necessary and
sufficient conditions) to rationally inattentive utility maximizers, a
generative model used extensively in economics for human decision making. Our
claim is based by extensive experiments on 200 deep CNNs from 5 popular
architectures. The parameters of our interpretable model are computed
efficiently via convex feasibility algorithms. As an application, we show that
our economics-based interpretable model can predict the classification
performance of deep CNNs trained with arbitrary parameters with accuracy
exceeding 94% . This eliminates the need to re-train the deep CNNs for image
classification. The theoretical foundation of our approach lies in Bayesian
revealed preference studied in micro-economics. All our results are on GitHub
and completely reproducible.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pattanayak_K/0/1/0/all/0/1"&gt;Kunal Pattanayak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Krishnamurthy_V/0/1/0/all/0/1"&gt;Vikram Krishnamurthy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[How COVID-19 Has Changed Crowdfunding: Evidence From GoFundMe. (arXiv:2106.09981v2 [cs.CY] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.09981</id>
        <link href="http://arxiv.org/abs/2106.09981"/>
        <updated>2021-08-03T02:06:34.896Z</updated>
        <summary type="html"><![CDATA[While the long-term effects of COVID-19 are yet to be determined, its
immediate impact on crowdfunding is nonetheless significant. This study takes a
computational approach to more deeply comprehend this change. Using a unique
data set of all the campaigns published over the past two years on GoFundMe, we
explore the factors that have led to the successful funding of a crowdfunding
project. In particular, we study a corpus of crowdfunded projects, analyzing
cover images and other variables commonly present on crowdfunding sites.
Furthermore, we construct a classifier and a regression model to assess the
significance of features based on XGBoost. In addition, we employ
counterfactual analysis to investigate the causality between features and the
success of crowdfunding. More importantly, sentiment analysis and the paired
sample t-test are performed to examine the differences in crowdfunding
campaigns before and after the COVID-19 outbreak that started in March 2020.
First, we note that there is significant racial disparity in crowdfunding
success. Second, we find that sad emotion expressed through the campaign's
description became significant after the COVID-19 outbreak. Considering all
these factors, our findings shed light on the impact of COVID-19 on
crowdfunding campaigns.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Junda Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xupin Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1"&gt;Jiebo Luo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Algorithms for Learning Depth-2 Neural Networks with General ReLU Activations. (arXiv:2107.10209v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.10209</id>
        <link href="http://arxiv.org/abs/2107.10209"/>
        <updated>2021-08-03T02:06:34.896Z</updated>
        <summary type="html"><![CDATA[We present polynomial time and sample efficient algorithms for learning an
unknown depth-2 feedforward neural network with general ReLU activations, under
mild non-degeneracy assumptions. In particular, we consider learning an unknown
network of the form $f(x) = {a}^{\mathsf{T}}\sigma({W}^\mathsf{T}x+b)$, where
$x$ is drawn from the Gaussian distribution, and $\sigma(t) := \max(t,0)$ is
the ReLU activation. Prior works for learning networks with ReLU activations
assume that the bias $b$ is zero. In order to deal with the presence of the
bias terms, our proposed algorithm consists of robustly decomposing multiple
higher order tensors arising from the Hermite expansion of the function $f(x)$.
Using these ideas we also establish identifiability of the network parameters
under minimal assumptions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Awasthi_P/0/1/0/all/0/1"&gt;Pranjal Awasthi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_A/0/1/0/all/0/1"&gt;Alex Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vijayaraghavan_A/0/1/0/all/0/1"&gt;Aravindan Vijayaraghavan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring the social influence of Kaggle virtual community on the M5 competition. (arXiv:2103.00501v2 [cs.SI] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.00501</id>
        <link href="http://arxiv.org/abs/2103.00501"/>
        <updated>2021-08-03T02:06:34.895Z</updated>
        <summary type="html"><![CDATA[One of the most significant differences of M5 over previous forecasting
competitions is that it was held on Kaggle, an online platform of data
scientists and machine learning practitioners. Kaggle provides a gathering
place, or virtual community, for web users who are interested in the M5
competition. Users can share code, models, features, loss functions, etc.
through online notebooks and discussion forums. This paper aims to study the
social influence of virtual community on user behaviors in the M5 competition.
We first research the content of the M5 virtual community by topic modeling and
trend analysis. Further, we perform social media analysis to identify the
potential relationship network of the virtual community. We study the roles and
characteristics of some key participants that promote the diffusion of
information within the M5 virtual community. Overall, this study provides
in-depth insights into the mechanism of the virtual community's influence on
the participants and has potential implications for future online competitions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xixi Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1"&gt;Yun Bai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1"&gt;Yanfei Kang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Gait Characterization in Duchenne Muscular Dystrophy (DMD) Using a Single-Sensor Accelerometer: Classical Machine Learning and Deep Learning Approaches. (arXiv:2105.06295v2 [eess.SP] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.06295</id>
        <link href="http://arxiv.org/abs/2105.06295"/>
        <updated>2021-08-03T02:06:34.895Z</updated>
        <summary type="html"><![CDATA[Differences in gait patterns of children with Duchenne muscular dystrophy
(DMD) and typically developing (TD) peers are visible to the eye, but
quantification of those differences outside of the gait laboratory has been
elusive. We measured vertical, mediolateral, and anteroposterior acceleration
using a waist-worn iPhone accelerometer during ambulation across a typical
range of velocities. Six TD and six DMD children from 3-15 years of age
underwent seven walking/running tasks, including five 25m walk/run tests at a
slow walk to running speeds, a 6-minute walk test (6MWT), and a
100-meter-run/walk (100MRW). We extracted temporospatial clinical gait features
(CFs) and applied multiple Artificial Intelligence (AI) tools to differentiate
between DMD and TD control children using extracted features and raw data.
Extracted CFs showed reduced step length and a greater mediolateral component
of total power (TP) consistent with shorter strides and Trendelenberg-like gait
commonly observed in DMD. AI methods using CFs and raw data varied
ineffectiveness at differentiating between DMD and TD controls at different
speeds, with an accuracy of some methods exceeding 91%. We demonstrate that by
using AI tools with accelerometer data from a consumer-level smartphone, we can
identify DMD gait disturbance in toddlers to early teens.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Ramli_A/0/1/0/all/0/1"&gt;Albara Ah Ramli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Huanle Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hou_J/0/1/0/all/0/1"&gt;Jiahui Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liu_R/0/1/0/all/0/1"&gt;Rex Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Nicorici_A/0/1/0/all/0/1"&gt;Alina Nicorici&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Aranki_D/0/1/0/all/0/1"&gt;Daniel Aranki&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Owens_C/0/1/0/all/0/1"&gt;Corey Owens&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Prasad_P/0/1/0/all/0/1"&gt;Poonam Prasad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+McDonald_C/0/1/0/all/0/1"&gt;Craig McDonald&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Henricson_E/0/1/0/all/0/1"&gt;Erik Henricson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DTGAN: Differential Private Training for Tabular GANs. (arXiv:2107.02521v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.02521</id>
        <link href="http://arxiv.org/abs/2107.02521"/>
        <updated>2021-08-03T02:06:34.895Z</updated>
        <summary type="html"><![CDATA[Tabular generative adversarial networks (TGAN) have recently emerged to cater
to the need of synthesizing tabular data -- the most widely used data format.
While synthetic tabular data offers the advantage of complying with privacy
regulations, there still exists a risk of privacy leakage via inference attacks
due to interpolating the properties of real data during training. Differential
private (DP) training algorithms provide theoretical guarantees for training
machine learning models by injecting statistical noise to prevent privacy
leaks. However, the challenges of applying DP on TGAN are to determine the most
optimal framework (i.e., PATE/DP-SGD) and neural network (i.e.,
Generator/Discriminator)to inject noise such that the data utility is well
maintained under a given privacy guarantee. In this paper, we propose DTGAN, a
novel conditional Wasserstein tabular GAN that comes in two variants DTGAN_G
and DTGAN_D, for providing a detailed comparison of tabular GANs trained using
DP-SGD for the generator vs discriminator, respectively. We elicit the privacy
analysis associated with training the generator with complex loss functions
(i.e., classification and information losses) needed for high quality tabular
data synthesis. Additionally, we rigorously evaluate the theoretical privacy
guarantees offered by DP empirically against membership and attribute inference
attacks. Our results on 3 datasets show that the DP-SGD framework is superior
to PATE and that a DP discriminator is more optimal for training convergence.
Thus, we find (i) DTGAN_D is capable of maintaining the highest data utility
across 4 ML models by up to 18% in terms of the average precision score for a
strict privacy budget, epsilon = 1, as compared to the prior studies and (ii)
DP effectively prevents privacy loss against inference attacks by restricting
the success probability of membership attacks to be close to 50%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kunar_A/0/1/0/all/0/1"&gt;Aditya Kunar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Birke_R/0/1/0/all/0/1"&gt;Robert Birke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1"&gt;Zilong Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Lydia Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MugRep: A Multi-Task Hierarchical Graph Representation Learning Framework for Real Estate Appraisal. (arXiv:2107.05180v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.05180</id>
        <link href="http://arxiv.org/abs/2107.05180"/>
        <updated>2021-08-03T02:06:34.895Z</updated>
        <summary type="html"><![CDATA[Real estate appraisal refers to the process of developing an unbiased opinion
for real property's market value, which plays a vital role in decision-making
for various players in the marketplace (e.g., real estate agents, appraisers,
lenders, and buyers). However, it is a nontrivial task for accurate real estate
appraisal because of three major challenges: (1) The complicated influencing
factors for property value; (2) The asynchronously spatiotemporal dependencies
among real estate transactions; (3) The diversified correlations between
residential communities. To this end, we propose a Multi-Task Hierarchical
Graph Representation Learning (MugRep) framework for accurate real estate
appraisal. Specifically, by acquiring and integrating multi-source urban data,
we first construct a rich feature set to comprehensively profile the real
estate from multiple perspectives (e.g., geographical distribution, human
mobility distribution, and resident demographics distribution). Then, an
evolving real estate transaction graph and a corresponding event graph
convolution module are proposed to incorporate asynchronously spatiotemporal
dependencies among real estate transactions. Moreover, to further incorporate
valuable knowledge from the view of residential communities, we devise a
hierarchical heterogeneous community graph convolution module to capture
diversified correlations between residential communities. Finally, an urban
district partitioned multi-task learning module is introduced to generate
differently distributed value opinions for real estate. Extensive experiments
on two real-world datasets demonstrate the effectiveness of MugRep and its
components and features.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Weijia Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Hao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zha_L/0/1/0/all/0/1"&gt;Lijun Zha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1"&gt;Hengshu Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Ji Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dou_D/0/1/0/all/0/1"&gt;Dejing Dou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1"&gt;Hui Xiong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Enhancing User' s Income Estimation with Super-App Alternative Data. (arXiv:2104.05831v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.05831</id>
        <link href="http://arxiv.org/abs/2104.05831"/>
        <updated>2021-08-03T02:06:34.894Z</updated>
        <summary type="html"><![CDATA[This paper presents the advantages of alternative data from Super-Apps to
enhance user' s income estimation models. It compares the performance of these
alternative data sources with the performance of industry-accepted bureau
income estimators that takes into account only financial system information;
successfully showing that the alternative data manage to capture information
that bureau income estimators do not. By implementing the TreeSHAP method for
Stochastic Gradient Boosting Interpretation, this paper highlights which of the
customer' s behavioral and transactional patterns within a Super-App have a
stronger predictive power when estimating user' s income. Ultimately, this
paper shows the incentive for financial institutions to seek to incorporate
alternative data into constructing their risk profiles.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Suarez_G/0/1/0/all/0/1"&gt;Gabriel Suarez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raful_J/0/1/0/all/0/1"&gt;Juan Raful&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luque_M/0/1/0/all/0/1"&gt;Maria A. Luque&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Valencia_C/0/1/0/all/0/1"&gt;Carlos F. Valencia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Correa_Bahnsen_A/0/1/0/all/0/1"&gt;Alejandro Correa-Bahnsen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semi-Supervised Learning of Visual Features by Non-Parametrically Predicting View Assignments with Support Samples. (arXiv:2104.13963v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.13963</id>
        <link href="http://arxiv.org/abs/2104.13963"/>
        <updated>2021-08-03T02:06:34.894Z</updated>
        <summary type="html"><![CDATA[This paper proposes a novel method of learning by predicting view assignments
with support samples (PAWS). The method trains a model to minimize a
consistency loss, which ensures that different views of the same unlabeled
instance are assigned similar pseudo-labels. The pseudo-labels are generated
non-parametrically, by comparing the representations of the image views to
those of a set of randomly sampled labeled images. The distance between the
view representations and labeled representations is used to provide a weighting
over class labels, which we interpret as a soft pseudo-label. By
non-parametrically incorporating labeled samples in this way, PAWS extends the
distance-metric loss used in self-supervised methods such as BYOL and SwAV to
the semi-supervised setting. Despite the simplicity of the approach, PAWS
outperforms other semi-supervised methods across architectures, setting a new
state-of-the-art for a ResNet-50 on ImageNet trained with either 10% or 1% of
the labels, reaching 75.5% and 66.5% top-1 respectively. PAWS requires 4x to
12x less training than the previous best methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Assran_M/0/1/0/all/0/1"&gt;Mahmoud Assran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Caron_M/0/1/0/all/0/1"&gt;Mathilde Caron&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Misra_I/0/1/0/all/0/1"&gt;Ishan Misra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bojanowski_P/0/1/0/all/0/1"&gt;Piotr Bojanowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joulin_A/0/1/0/all/0/1"&gt;Armand Joulin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ballas_N/0/1/0/all/0/1"&gt;Nicolas Ballas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rabbat_M/0/1/0/all/0/1"&gt;Michael Rabbat&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Federated Learning with Unreliable Clients: Performance Analysis and Mechanism Design. (arXiv:2105.06256v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.06256</id>
        <link href="http://arxiv.org/abs/2105.06256"/>
        <updated>2021-08-03T02:06:34.892Z</updated>
        <summary type="html"><![CDATA[Owing to the low communication costs and privacy-promoting capabilities,
Federated Learning (FL) has become a promising tool for training effective
machine learning models among distributed clients. However, with the
distributed architecture, low quality models could be uploaded to the
aggregator server by unreliable clients, leading to a degradation or even a
collapse of training. In this paper, we model these unreliable behaviors of
clients and propose a defensive mechanism to mitigate such a security risk.
Specifically, we first investigate the impact on the models caused by
unreliable clients by deriving a convergence upper bound on the loss function
based on the gradient descent updates. Our theoretical bounds reveal that with
a fixed amount of total computational resources, there exists an optimal number
of local training iterations in terms of convergence performance. We further
design a novel defensive mechanism, named deep neural network based secure
aggregation (DeepSA). Our experimental results validate our theoretical
analysis. In addition, the effectiveness of DeepSA is verified by comparing
with other state-of-the-art defensive mechanisms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1"&gt;Chuan Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jun Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1"&gt;Ming Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_K/0/1/0/all/0/1"&gt;Kang Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1"&gt;Wen Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Poor_H/0/1/0/all/0/1"&gt;H. Vincent Poor&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GPU Accelerated Exhaustive Search for Optimal Ensemble of Black-Box Optimization Algorithms. (arXiv:2012.04201v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.04201</id>
        <link href="http://arxiv.org/abs/2012.04201"/>
        <updated>2021-08-03T02:06:34.876Z</updated>
        <summary type="html"><![CDATA[Black-box optimization is essential for tuning complex machine learning
algorithms which are easier to experiment with than to understand. In this
paper, we show that a simple ensemble of black-box optimization algorithms can
outperform any single one of them. However, searching for such an optimal
ensemble requires a large number of experiments. We propose a
Multi-GPU-optimized framework to accelerate a brute force search for the
optimal ensemble of black-box optimization algorithms by running many
experiments in parallel. The lightweight optimizations are performed by CPU
while expensive model training and evaluations are assigned to GPUs. We
evaluate 15 optimizers by training 2.7 million models and running 541,440
optimizations. On a DGX-1, the search time is reduced from more than 10 days on
two 20-core CPUs to less than 24 hours on 8-GPUs. With the optimal ensemble
found by GPU-accelerated exhaustive search, we won the 2nd place of NeurIPS
2020 black-box optimization challenge.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jiwei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tunguz_B/0/1/0/all/0/1"&gt;Bojan Tunguz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Titericz_G/0/1/0/all/0/1"&gt;Gilberto Titericz&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Knowledge Graph-based Question Answering with Electronic Health Records. (arXiv:2010.09394v2 [cs.DB] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.09394</id>
        <link href="http://arxiv.org/abs/2010.09394"/>
        <updated>2021-08-03T02:06:34.868Z</updated>
        <summary type="html"><![CDATA[Question Answering (QA) is a widely-used framework for developing and
evaluating an intelligent machine. In this light, QA on Electronic Health
Records (EHR), namely EHR QA, can work as a crucial milestone towards
developing an intelligent agent in healthcare. EHR data are typically stored in
a relational database, which can also be converted to a directed acyclic graph,
allowing two approaches for EHR QA: Table-based QA and Knowledge Graph-based
QA. We hypothesize that the graph-based approach is more suitable for EHR QA as
graphs can represent relations between entities and values more naturally
compared to tables, which essentially require JOIN operations. In this paper,
we propose a graph-based EHR QA where natural language queries are converted to
SPARQL instead of SQL. To validate our hypothesis, we create four EHR QA
datasets (graph-based VS table-based, and simplified database schema VS
original database schema), based on a table-based dataset MIMICSQL. We test
both a simple Seq2Seq model and a state-of-the-art EHR QA model on all datasets
where the graph-based datasets facilitated up to 34% higher accuracy than the
table-based dataset without any modification to the model architectures.
Finally, all datasets are open-sourced to encourage further EHR QA research in
both directions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1"&gt;Junwoo Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cho_Y/0/1/0/all/0/1"&gt;Youngwoo Cho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1"&gt;Haneol Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choo_J/0/1/0/all/0/1"&gt;Jaegul Choo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_E/0/1/0/all/0/1"&gt;Edward Choi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Boosting Video Captioning with Dynamic Loss Network. (arXiv:2107.11707v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.11707</id>
        <link href="http://arxiv.org/abs/2107.11707"/>
        <updated>2021-08-03T02:06:34.862Z</updated>
        <summary type="html"><![CDATA[Video captioning is one of the challenging problems at the intersection of
vision and language, having many real-life applications in video retrieval,
video surveillance, assisting visually challenged people, Human-machine
interface, and many more. Recent deep learning-based methods have shown
promising results but are still on the lower side than other vision tasks (such
as image classification, object detection). A significant drawback with
existing video captioning methods is that they are optimized over cross-entropy
loss function, which is uncorrelated to the de facto evaluation metrics (BLEU,
METEOR, CIDER, ROUGE).In other words, cross-entropy is not a proper surrogate
of the true loss function for video captioning. This paper addresses the
drawback by introducing a dynamic loss network (DLN), which provides an
additional feedback signal that directly reflects the evaluation metrics. Our
results on Microsoft Research Video Description Corpus (MSVD) and MSR-Video to
Text (MSRVTT) datasets outperform previous methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nasibullah/0/1/0/all/0/1"&gt;Nasibullah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mohanta_P/0/1/0/all/0/1"&gt;Partha Pratim Mohanta&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Filtering in tractography using autoencoders (FINTA). (arXiv:2010.04007v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.04007</id>
        <link href="http://arxiv.org/abs/2010.04007"/>
        <updated>2021-08-03T02:06:34.855Z</updated>
        <summary type="html"><![CDATA[Current brain white matter fiber tracking techniques show a number of
problems, including: generating large proportions of streamlines that do not
accurately describe the underlying anatomy; extracting streamlines that are not
supported by the underlying diffusion signal; and under-representing some fiber
populations, among others. In this paper, we describe a novel autoencoder-based
learning method to filter streamlines from diffusion MRI tractography, and
hence, to obtain more reliable tractograms. Our method, dubbed FINTA (Filtering
in Tractography using Autoencoders) uses raw, unlabeled tractograms to train
the autoencoder, and to learn a robust representation of brain streamlines.
Such an embedding is then used to filter undesired streamline samples using a
nearest neighbor algorithm. Our experiments on both synthetic and in vivo human
brain diffusion MRI tractography data obtain accuracy scores exceeding the 90\%
threshold on the test set. Results reveal that FINTA has a superior filtering
performance compared to conventional, anatomy-based methods, and the
RecoBundles state-of-the-art method. Additionally, we demonstrate that FINTA
can be applied to partial tractograms without requiring changes to the
framework. We also show that the proposed method generalizes well across
different tracking methods and datasets, and shortens significantly the
computation time for large (>1 M streamlines) tractograms. Together, this work
brings forward a new deep learning framework in tractography based on
autoencoders, which offers a flexible and powerful method for white matter
filtering and bundling that could enhance tractometry and connectivity
analyses.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Legarreta_J/0/1/0/all/0/1"&gt;Jon Haitz Legarreta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Petit_L/0/1/0/all/0/1"&gt;Laurent Petit&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rheault_F/0/1/0/all/0/1"&gt;Fran&amp;#xe7;ois Rheault&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Theaud_G/0/1/0/all/0/1"&gt;Guillaume Theaud&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lemaire_C/0/1/0/all/0/1"&gt;Carl Lemaire&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Descoteaux_M/0/1/0/all/0/1"&gt;Maxime Descoteaux&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jodoin_P/0/1/0/all/0/1"&gt;Pierre-Marc Jodoin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Physics-Guided Deep Learning for Dynamical Systems: A survey. (arXiv:2107.01272v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.01272</id>
        <link href="http://arxiv.org/abs/2107.01272"/>
        <updated>2021-08-03T02:06:34.848Z</updated>
        <summary type="html"><![CDATA[Modeling complex physical dynamics is a fundamental task in science and
engineering. Traditional physics-based models are interpretable but rely on
rigid assumptions. And the direct numerical approximation is usually
computationally intensive, requiring significant computational resources and
expertise. While deep learning (DL) provides novel alternatives for efficiently
recognizing complex patterns and emulating nonlinear dynamics, it does not
necessarily obey the governing laws of physical systems, nor do they generalize
well across different systems. Thus, the study of physics-guided DL emerged and
has gained great progress. It aims to take the best from both physics-based
modeling and state-of-the-art DL models to better solve scientific problems. In
this paper, we provide a structured overview of existing methodologies of
integrating prior physical knowledge or physics-based modeling into DL and
discuss the emerging opportunities.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1"&gt;Rui Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adaptive KL-UCB based Bandit Algorithms for Markovian and i.i.d. Settings. (arXiv:2009.06606v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.06606</id>
        <link href="http://arxiv.org/abs/2009.06606"/>
        <updated>2021-08-03T02:06:34.842Z</updated>
        <summary type="html"><![CDATA[In the regret-based formulation of multi-armed bandit (MAB) problems, except
in rare instances, much of the literature focuses on arms with i.i.d. rewards.
In this paper, we consider the problem of obtaining regret guarantees for MAB
problems in which the rewards of each arm form a Markov chain which may not
belong to a single parameter exponential family. To achieve logarithmic regret
in such problems is not difficult: a variation of standard KL-UCB does the job.
However, the constants obtained from such an analysis are poor for the
following reason: i.i.d. rewards are a special case of Markov rewards and it is
difficult to design an algorithm that works well independent of whether the
underlying model is truly Markovian or i.i.d. To overcome this issue, we
introduce a novel algorithm that identifies whether the rewards from each arm
are truly Markovian or i.i.d. using a Hellinger distance-based test. Our
algorithm then switches from using a standard KL-UCB to a specialized version
of KL-UCB when it determines that the arm reward is Markovian, thus resulting
in low regret for both i.i.d. and Markovian settings.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Roy_A/0/1/0/all/0/1"&gt;Arghyadip Roy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shakkottai_S/0/1/0/all/0/1"&gt;Sanjay Shakkottai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srikant_R/0/1/0/all/0/1"&gt;R. Srikant&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Distributional Robust Batch Contextual Bandits. (arXiv:2006.05630v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.05630</id>
        <link href="http://arxiv.org/abs/2006.05630"/>
        <updated>2021-08-03T02:06:34.822Z</updated>
        <summary type="html"><![CDATA[Policy learning using historical observational data is an important problem
that has found widespread applications. Examples include selecting offers,
prices, advertisements to send to customers, as well as selecting which
medication to prescribe to a patient. However, existing literature rests on the
crucial assumption that the future environment where the learned policy will be
deployed is the same as the past environment that has generated the data--an
assumption that is often false or too coarse an approximation. In this paper,
we lift this assumption and aim to learn a distributional robust policy with
incomplete (bandit) observational data. We propose a novel learning algorithm
that is able to learn a robust policy to adversarial perturbations and unknown
covariate shifts. We first present a policy evaluation procedure in the
ambiguous environment and then give a performance guarantee based on the theory
of uniform convergence. Additionally, we also give a heuristic algorithm to
solve the distributional robust policy learning problems efficiently. Finally,
we demonstrate the robustness of our methods in the synthetic and real-world
datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Si_N/0/1/0/all/0/1"&gt;Nian Si&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1"&gt;Fan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1"&gt;Zhengyuan Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Blanchet_J/0/1/0/all/0/1"&gt;Jose Blanchet&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FedLab: A Flexible Federated Learning Framework. (arXiv:2107.11621v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.11621</id>
        <link href="http://arxiv.org/abs/2107.11621"/>
        <updated>2021-08-03T02:06:34.816Z</updated>
        <summary type="html"><![CDATA[Federated learning (FL) is a machine learning field in which researchers try
to facilitate model learning process among multiparty without violating privacy
protection regulations. Considerable effort has been invested in FL
optimization and communication related researches. In this work, we introduce
FedLab, a lightweight open-source framework for FL simulation. The design of
FedLab focuses on FL algorithm effectiveness and communication efficiency.
Also, FedLab is scalable in different deployment scenario. We hope FedLab could
provide flexible API as well as reliable baseline implementations, and relieve
the burden of implementing novel approaches for researchers in FL community.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_D/0/1/0/all/0/1"&gt;Dun Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1"&gt;Siqi Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1"&gt;Xiangjing Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zenglin Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[End-to-end neural network approach to 3D reservoir simulation and adaptation. (arXiv:2102.10304v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.10304</id>
        <link href="http://arxiv.org/abs/2102.10304"/>
        <updated>2021-08-03T02:06:34.809Z</updated>
        <summary type="html"><![CDATA[Reservoir simulation and adaptation (also known as history matching) are
typically considered as separate problems. While a set of models are aimed at
the solution of the forward simulation problem assuming all initial geological
parameters are known, the other set of models adjust geological parameters
under the fixed forward simulation model to fit production data. This results
in many difficulties for both reservoir engineers and developers of new
efficient computation schemes. We present a unified approach to reservoir
simulation and adaptation problems. A single neural network model allows a
forward pass from initial geological parameters of the 3D reservoir model
through dynamic state variables to well's production rates and backward
gradient propagation to any model inputs and variables. The model fitting and
geological parameters adaptation both become the optimization problem over
specific parts of the same neural network model. Standard gradient-based
optimization schemes can be used to find the optimal solution. Using real-world
oilfield model and historical production rates we demonstrate that the
suggested approach allows reservoir simulation and history matching with a
benefit of several orders of magnitude simulation speed-up. Finally, to
propagate this research we open-source a Python-based framework DeepField that
allows standard processing of reservoir models and reproducing the approach
presented in this paper.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Illarionov_E/0/1/0/all/0/1"&gt;E. Illarionov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Temirchev_P/0/1/0/all/0/1"&gt;P. Temirchev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Voloskov_D/0/1/0/all/0/1"&gt;D. Voloskov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kostoev_R/0/1/0/all/0/1"&gt;R. Kostoev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Simonov_M/0/1/0/all/0/1"&gt;M. Simonov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pissarenko_D/0/1/0/all/0/1"&gt;D. Pissarenko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Orlov_D/0/1/0/all/0/1"&gt;D. Orlov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koroteev_D/0/1/0/all/0/1"&gt;D. Koroteev&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Review of Generative Adversarial Networks in Cancer Imaging: New Applications, New Solutions. (arXiv:2107.09543v1 [eess.IV] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2107.09543</id>
        <link href="http://arxiv.org/abs/2107.09543"/>
        <updated>2021-08-03T02:06:34.809Z</updated>
        <summary type="html"><![CDATA[Despite technological and medical advances, the detection, interpretation,
and treatment of cancer based on imaging data continue to pose significant
challenges. These include high inter-observer variability, difficulty of
small-sized lesion detection, nodule interpretation and malignancy
determination, inter- and intra-tumour heterogeneity, class imbalance,
segmentation inaccuracies, and treatment effect uncertainty. The recent
advancements in Generative Adversarial Networks (GANs) in computer vision as
well as in medical imaging may provide a basis for enhanced capabilities in
cancer detection and analysis. In this review, we assess the potential of GANs
to address a number of key challenges of cancer imaging, including data
scarcity and imbalance, domain and dataset shifts, data access and privacy,
data annotation and quantification, as well as cancer detection, tumour
profiling and treatment planning. We provide a critical appraisal of the
existing literature of GANs applied to cancer imagery, together with
suggestions on future research directions to address these challenges. We
analyse and discuss 163 papers that apply adversarial training techniques in
the context of cancer imaging and elaborate their methodologies, advantages and
limitations. With this work, we strive to bridge the gap between the needs of
the clinical cancer imaging community and the current and prospective research
on GANs in the artificial intelligence community.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Osuala_R/0/1/0/all/0/1"&gt;Richard Osuala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kushibar_K/0/1/0/all/0/1"&gt;Kaisar Kushibar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Garrucho_L/0/1/0/all/0/1"&gt;Lidia Garrucho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Linardos_A/0/1/0/all/0/1"&gt;Akis Linardos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Szafranowska_Z/0/1/0/all/0/1"&gt;Zuzanna Szafranowska&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Klein_S/0/1/0/all/0/1"&gt;Stefan Klein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Glocker_B/0/1/0/all/0/1"&gt;Ben Glocker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Diaz_O/0/1/0/all/0/1"&gt;Oliver Diaz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lekadir_K/0/1/0/all/0/1"&gt;Karim Lekadir&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ATCN: Resource-Efficient Processing of Time Series on Edge. (arXiv:2011.05260v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.05260</id>
        <link href="http://arxiv.org/abs/2011.05260"/>
        <updated>2021-08-03T02:06:34.808Z</updated>
        <summary type="html"><![CDATA[This paper presents a scalable deep learning model called Agile Temporal
Convolutional Network (ATCN) for high-accurate fast classification and time
series prediction in resource-constrained embedded systems. ATCN is a family of
compact networks with formalized hyperparameters that enable
application-specific adjustments to be made to the model architecture. It is
primarily designed for embedded edge devices with very limited performance and
memory, such as wearable biomedical devices and real-time reliability
monitoring systems. ATCN makes fundamental improvements over the mainstream
temporal convolutional neural networks, including residual connections as time
attention machines to increase the network depth and accuracy and the
incorporation of separable depth-wise convolution to reduce the computational
complexity of the model. As part of the present work, three ATCN families,
namely T0, T1, and T2, are also presented and evaluated on different ranges of
embedded processors - Cortex-M7 and Cortex-A57 processor. An evaluation of the
ATCN models against the best-in-class InceptionTime shows that ATCN improves
both accuracy and execution time on a broad range of embedded and
cyber-physical applications with demand for real-time processing on the
embedded edge. At the same time, in contrast to existing solutions, ATCN is the
first deep learning-based approach that can be run on embedded microcontrollers
(Cortex-M7) with limited computational performance and memory capacity while
delivering state-of-the-art accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Baharani_M/0/1/0/all/0/1"&gt;Mohammadreza Baharani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tabkhi_H/0/1/0/all/0/1"&gt;Hamed Tabkhi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tractable structured natural gradient descent using local parameterizations. (arXiv:2102.07405v7 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.07405</id>
        <link href="http://arxiv.org/abs/2102.07405"/>
        <updated>2021-08-03T02:06:34.808Z</updated>
        <summary type="html"><![CDATA[Natural-gradient descent (NGD) on structured parameter spaces (e.g., low-rank
covariances) is computationally challenging due to difficult Fisher-matrix
computations. We address this issue by using \emph{local-parameter coordinates}
to obtain a flexible and efficient NGD method that works well for a
wide-variety of structured parameterizations. We show four applications where
our method (1) generalizes the exponential natural evolutionary strategy, (2)
recovers existing Newton-like algorithms, (3) yields new structured
second-order algorithms via matrix groups, and (4) gives new algorithms to
learn covariances of Gaussian and Wishart-based distributions. We show results
on a range of problems from deep learning, variational inference, and evolution
strategies. Our work opens a new direction for scalable structured geometric
methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Lin_W/0/1/0/all/0/1"&gt;Wu Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Nielsen_F/0/1/0/all/0/1"&gt;Frank Nielsen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Khan_M/0/1/0/all/0/1"&gt;Mohammad Emtiyaz Khan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Schmidt_M/0/1/0/all/0/1"&gt;Mark Schmidt&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Intelligent-Tire-Based Slip Ratio Estimation Using Machine Learning. (arXiv:2106.08961v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.08961</id>
        <link href="http://arxiv.org/abs/2106.08961"/>
        <updated>2021-08-03T02:06:34.808Z</updated>
        <summary type="html"><![CDATA[Autonomous vehicles are most concerned about safety control issues, and the
slip ratio is critical to the safety of the vehicle control system. In this
paper, different machine learning algorithms (Neural Networks, Gradient
Boosting Machine, Random Forest, and Support Vector Machine) are used to train
the slip ratio estimation model based on the acceleration signals ($a_x$,
$a_y$, and $a_z$) from the tri-axial Micro-Electro Mechanical System (MEMS)
accelerometer utilized in the intelligent tire system, where the acceleration
signals are divided into four sets ($a_x/a_y/a_z$, $a_x/a_z$, $a_y/a_z$, and
$a_z$) as algorithm inputs. The experimental data used in this study are
collected through the MTS Flat-Trac tire test platform. Performance of
different slip ratio estimation models is compared using the NRMS errors in
10-fold cross-validation (CV). The results indicate that NN and GBM have more
promising accuracy, and the $a_z$ input type has a better performance compared
to other input types, with the best result being the estimation model of the NN
algorithm with $a_z$ as input, which results is 4.88\%. The present study with
the fusion of intelligent tire system and machine learning paves the way for
the accurate estimation of tire slip ratio under different driving conditions,
which will open up a new way of Autonomous vehicles, intelligent tires, and
tire slip ratio estimation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_N/0/1/0/all/0/1"&gt;Nan Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1"&gt;Zepeng Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jianfeng Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Askari_H/0/1/0/all/0/1"&gt;Hassan Askari&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Explain and Improve: LRP-Inference Fine-Tuning for Image Captioning Models. (arXiv:2001.01037v5 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2001.01037</id>
        <link href="http://arxiv.org/abs/2001.01037"/>
        <updated>2021-08-03T02:06:34.807Z</updated>
        <summary type="html"><![CDATA[This paper analyzes the predictions of image captioning models with attention
mechanisms beyond visualizing the attention itself. We develop variants of
layer-wise relevance propagation (LRP) and gradient-based explanation methods,
tailored to image captioning models with attention mechanisms. We compare the
interpretability of attention heatmaps systematically against the explanations
provided by explanation methods such as LRP, Grad-CAM, and Guided Grad-CAM. We
show that explanation methods provide simultaneously pixel-wise image
explanations (supporting and opposing pixels of the input image) and linguistic
explanations (supporting and opposing words of the preceding sequence) for each
word in the predicted captions. We demonstrate with extensive experiments that
explanation methods 1) can reveal additional evidence used by the model to make
decisions compared to attention; 2) correlate to object locations with high
precision; 3) are helpful to "debug" the model, e.g. by analyzing the reasons
for hallucinated object words. With the observed properties of explanations, we
further design an LRP-inference fine-tuning strategy that reduces the issue of
object hallucination in image captioning models, and meanwhile, maintains the
sentence fluency. We conduct experiments with two widely used attention
mechanisms: the adaptive attention mechanism calculated with the additive
attention and the multi-head attention mechanism calculated with the scaled dot
product.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1"&gt;Jiamei Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lapuschkin_S/0/1/0/all/0/1"&gt;Sebastian Lapuschkin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Samek_W/0/1/0/all/0/1"&gt;Wojciech Samek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Binder_A/0/1/0/all/0/1"&gt;Alexander Binder&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An End-to-End and Accurate PPG-based Respiratory Rate Estimation Approach Using Cycle Generative Adversarial Networks. (arXiv:2105.00594v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.00594</id>
        <link href="http://arxiv.org/abs/2105.00594"/>
        <updated>2021-08-03T02:06:34.807Z</updated>
        <summary type="html"><![CDATA[Respiratory rate (RR) is a clinical sign representing ventilation. An
abnormal change in RR is often the first sign of health deterioration as the
body attempts to maintain oxygen delivery to its tissues. There has been a
growing interest in remotely monitoring of RR in everyday settings which has
made photoplethysmography (PPG) monitoring wearable devices an attractive
choice. PPG signals are useful sources for RR extraction due to the presence of
respiration-induced modulations in them. The existing PPG-based RR estimation
methods mainly rely on hand-crafted rules and manual parameters tuning. An
end-to-end deep learning approach was recently proposed, however, despite its
automatic nature, the performance of this method is not ideal using the real
world data. In this paper, we present an end-to-end and accurate pipeline for
RR estimation using Cycle Generative Adversarial Networks (CycleGAN) to
reconstruct respiratory signals from raw PPG signals. Our results demonstrate a
higher RR estimation accuracy of up to 2$\times$ (mean absolute error of
1.9$\pm$0.3 using five fold cross validation) compared to the state-of-th-art
using a identical publicly available dataset. Our results suggest that CycleGAN
can be a valuable method for RR estimation from raw PPG signals.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Aqajari_S/0/1/0/all/0/1"&gt;Seyed Amir Hossein Aqajari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_R/0/1/0/all/0/1"&gt;Rui Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zargari_A/0/1/0/all/0/1"&gt;Amir Hosein Afandizadeh Zargari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rahmani_A/0/1/0/all/0/1"&gt;Amir M. Rahmani&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Modal Detection of Alzheimer's Disease from Speech and Text. (arXiv:2012.00096v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.00096</id>
        <link href="http://arxiv.org/abs/2012.00096"/>
        <updated>2021-08-03T02:06:34.806Z</updated>
        <summary type="html"><![CDATA[Reliable detection of the prodromal stages of Alzheimer's disease (AD)
remains difficult even today because, unlike other neurocognitive impairments,
there is no definitive diagnosis of AD in vivo. In this context, existing
research has shown that patients often develop language impairment even in mild
AD conditions. We propose a multimodal deep learning method that utilizes
speech and the corresponding transcript simultaneously to detect AD. For audio
signals, the proposed audio-based network, a convolutional neural network (CNN)
based model, predicts the diagnosis for multiple speech segments, which are
combined for the final prediction. Similarly, we use contextual embedding
extracted from BERT concatenated with a CNN-generated embedding for classifying
the transcript. The individual predictions of the two models are then combined
to make the final classification. We also perform experiments to analyze the
model performance when Automated Speech Recognition (ASR) system generated
transcripts are used instead of manual transcription in the text-based model.
The proposed method achieves 85.3% 10-fold cross-validation accuracy when
trained and evaluated on the Dementiabank Pitt corpus.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mittal_A/0/1/0/all/0/1"&gt;Amish Mittal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sahoo_S/0/1/0/all/0/1"&gt;Sourav Sahoo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Datar_A/0/1/0/all/0/1"&gt;Arnhav Datar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kadiwala_J/0/1/0/all/0/1"&gt;Juned Kadiwala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shalu_H/0/1/0/all/0/1"&gt;Hrithwik Shalu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mathew_J/0/1/0/all/0/1"&gt;Jimson Mathew&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Black-box Probe for Unsupervised Domain Adaptation without Model Transferring. (arXiv:2107.10174v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.10174</id>
        <link href="http://arxiv.org/abs/2107.10174"/>
        <updated>2021-08-03T02:06:34.806Z</updated>
        <summary type="html"><![CDATA[In recent years, researchers have been paying increasing attention to the
threats brought by deep learning models to data security and privacy,
especially in the field of domain adaptation. Existing unsupervised domain
adaptation (UDA) methods can achieve promising performance without transferring
data from source domain to target domain. However, UDA with representation
alignment or self-supervised pseudo-labeling relies on the transferred source
models. In many data-critical scenarios, methods based on model transferring
may suffer from membership inference attacks and expose private data. In this
paper, we aim to overcome a challenging new setting where the source models are
only queryable but cannot be transferred to the target domain. We propose
Black-box Probe Domain Adaptation (BPDA), which adopts query mechanism to probe
and refine information from source model using third-party dataset. In order to
gain more informative query results, we further propose Distributionally
Adversarial Training (DAT) to align the distribution of third-party data with
that of target data. BPDA uses public third-party dataset and adversarial
examples based on DAT as the information carrier between source and target
domains, dispensing with transferring source data or model. Experimental
results on benchmarks of Digit-Five, Office-Caltech, Office-31, Office-Home,
and DomainNet demonstrate the feasibility of BPDA without model transferring.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1"&gt;Kunhong Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1"&gt;Yucheng Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1"&gt;Yahong Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1"&gt;Yunfeng Shao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bingshuai Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Take an Emotion Walk: Perceiving Emotions from Gaits Using Hierarchical Attention Pooling and Affective Mapping. (arXiv:1911.08708v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1911.08708</id>
        <link href="http://arxiv.org/abs/1911.08708"/>
        <updated>2021-08-03T02:06:34.805Z</updated>
        <summary type="html"><![CDATA[We present an autoencoder-based semi-supervised approach to classify
perceived human emotions from walking styles obtained from videos or
motion-captured data and represented as sequences of 3D poses. Given the motion
on each joint in the pose at each time step extracted from 3D pose sequences,
we hierarchically pool these joint motions in a bottom-up manner in the
encoder, following the kinematic chains in the human body. We also constrain
the latent embeddings of the encoder to contain the space of
psychologically-motivated affective features underlying the gaits. We train the
decoder to reconstruct the motions per joint per time step in a top-down manner
from the latent embeddings. For the annotated data, we also train a classifier
to map the latent embeddings to emotion labels. Our semi-supervised approach
achieves a mean average precision of 0.84 on the Emotion-Gait benchmark
dataset, which contains both labeled and unlabeled gaits collected from
multiple sources. We outperform current state-of-art algorithms for both
emotion recognition and action recognition from 3D gaits by 7%--23% on the
absolute. More importantly, we improve the average precision by 10%--50% on the
absolute on classes that each makes up less than 25% of the labeled part of the
Emotion-Gait benchmark dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bhattacharya_U/0/1/0/all/0/1"&gt;Uttaran Bhattacharya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roncal_C/0/1/0/all/0/1"&gt;Christian Roncal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mittal_T/0/1/0/all/0/1"&gt;Trisha Mittal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chandra_R/0/1/0/all/0/1"&gt;Rohan Chandra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kapsaskis_K/0/1/0/all/0/1"&gt;Kyra Kapsaskis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gray_K/0/1/0/all/0/1"&gt;Kurt Gray&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bera_A/0/1/0/all/0/1"&gt;Aniket Bera&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Manocha_D/0/1/0/all/0/1"&gt;Dinesh Manocha&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Surgical Data Science -- from Concepts toward Clinical Translation. (arXiv:2011.02284v2 [cs.CY] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.02284</id>
        <link href="http://arxiv.org/abs/2011.02284"/>
        <updated>2021-08-03T02:06:34.804Z</updated>
        <summary type="html"><![CDATA[Recent developments in data science in general and machine learning in
particular have transformed the way experts envision the future of surgery.
Surgical Data Science (SDS) is a new research field that aims to improve the
quality of interventional healthcare through the capture, organization,
analysis and modeling of data. While an increasing number of data-driven
approaches and clinical applications have been studied in the fields of
radiological and clinical data science, translational success stories are still
lacking in surgery. In this publication, we shed light on the underlying
reasons and provide a roadmap for future advances in the field. Based on an
international workshop involving leading researchers in the field of SDS, we
review current practice, key achievements and initiatives as well as available
standards and tools for a number of topics relevant to the field, namely (1)
infrastructure for data acquisition, storage and access in the presence of
regulatory constraints, (2) data annotation and sharing and (3) data analytics.
We further complement this technical perspective with (4) a review of currently
available SDS products and the translational progress from academia and (5) a
roadmap for faster clinical translation and exploitation of the full potential
of SDS, based on an international multi-round Delphi process.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Maier_Hein_L/0/1/0/all/0/1"&gt;Lena Maier-Hein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eisenmann_M/0/1/0/all/0/1"&gt;Matthias Eisenmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sarikaya_D/0/1/0/all/0/1"&gt;Duygu Sarikaya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marz_K/0/1/0/all/0/1"&gt;Keno M&amp;#xe4;rz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Collins_T/0/1/0/all/0/1"&gt;Toby Collins&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Malpani_A/0/1/0/all/0/1"&gt;Anand Malpani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fallert_J/0/1/0/all/0/1"&gt;Johannes Fallert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feussner_H/0/1/0/all/0/1"&gt;Hubertus Feussner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Giannarou_S/0/1/0/all/0/1"&gt;Stamatia Giannarou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mascagni_P/0/1/0/all/0/1"&gt;Pietro Mascagni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nakawala_H/0/1/0/all/0/1"&gt;Hirenkumar Nakawala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_A/0/1/0/all/0/1"&gt;Adrian Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pugh_C/0/1/0/all/0/1"&gt;Carla Pugh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stoyanov_D/0/1/0/all/0/1"&gt;Danail Stoyanov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vedula_S/0/1/0/all/0/1"&gt;Swaroop S. Vedula&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cleary_K/0/1/0/all/0/1"&gt;Kevin Cleary&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fichtinger_G/0/1/0/all/0/1"&gt;Gabor Fichtinger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Forestier_G/0/1/0/all/0/1"&gt;Germain Forestier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gibaud_B/0/1/0/all/0/1"&gt;Bernard Gibaud&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grantcharov_T/0/1/0/all/0/1"&gt;Teodor Grantcharov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hashizume_M/0/1/0/all/0/1"&gt;Makoto Hashizume&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heckmann_Notzel_D/0/1/0/all/0/1"&gt;Doreen Heckmann-N&amp;#xf6;tzel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kenngott_H/0/1/0/all/0/1"&gt;Hannes G. Kenngott&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kikinis_R/0/1/0/all/0/1"&gt;Ron Kikinis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mundermann_L/0/1/0/all/0/1"&gt;Lars M&amp;#xfc;ndermann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Navab_N/0/1/0/all/0/1"&gt;Nassir Navab&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Onogur_S/0/1/0/all/0/1"&gt;Sinan Onogur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sznitman_R/0/1/0/all/0/1"&gt;Raphael Sznitman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Taylor_R/0/1/0/all/0/1"&gt;Russell H. Taylor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tizabi_M/0/1/0/all/0/1"&gt;Minu D. Tizabi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wagner_M/0/1/0/all/0/1"&gt;Martin Wagner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hager_G/0/1/0/all/0/1"&gt;Gregory D. Hager&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neumuth_T/0/1/0/all/0/1"&gt;Thomas Neumuth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Padoy_N/0/1/0/all/0/1"&gt;Nicolas Padoy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Collins_J/0/1/0/all/0/1"&gt;Justin Collins&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gockel_I/0/1/0/all/0/1"&gt;Ines Gockel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goedeke_J/0/1/0/all/0/1"&gt;Jan Goedeke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hashimoto_D/0/1/0/all/0/1"&gt;Daniel A. Hashimoto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joyeux_L/0/1/0/all/0/1"&gt;Luc Joyeux&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lam_K/0/1/0/all/0/1"&gt;Kyle Lam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leff_D/0/1/0/all/0/1"&gt;Daniel R. Leff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Madani_A/0/1/0/all/0/1"&gt;Amin Madani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marcus_H/0/1/0/all/0/1"&gt;Hani J. Marcus&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meireles_O/0/1/0/all/0/1"&gt;Ozanan Meireles&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seitel_A/0/1/0/all/0/1"&gt;Alexander Seitel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Teber_D/0/1/0/all/0/1"&gt;Dogu Teber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Uckert_F/0/1/0/all/0/1"&gt;Frank &amp;#xdc;ckert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muller_Stich_B/0/1/0/all/0/1"&gt;Beat P. M&amp;#xfc;ller-Stich&lt;/a&gt;, et al. (2 additional authors not shown)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Selecting the independent coordinates of manifolds with large aspect ratios. (arXiv:1907.01651v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1907.01651</id>
        <link href="http://arxiv.org/abs/1907.01651"/>
        <updated>2021-08-03T02:06:34.802Z</updated>
        <summary type="html"><![CDATA[Many manifold embedding algorithms fail apparently when the data manifold has
a large aspect ratio (such as a long, thin strip). Here, we formulate success
and failure in terms of finding a smooth embedding, showing also that the
problem is pervasive and more complex than previously recognized.
Mathematically, success is possible under very broad conditions, provided that
embedding is done by carefully selected eigenfunctions of the Laplace-Beltrami
operator $\Delta$. Hence, we propose a bicriterial Independent Eigencoordinate
Selection (IES) algorithm that selects smooth embeddings with few eigenvectors.
The algorithm is grounded in theory, has low computational overhead, and is
successful on synthetic and large real data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yu-Chia Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Meila_M/0/1/0/all/0/1"&gt;Marina Meil&amp;#x103;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On the Efficiency of Sinkhorn and Greenkhorn and Their Acceleration for Optimal Transport. (arXiv:1906.01437v7 [cs.DS] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1906.01437</id>
        <link href="http://arxiv.org/abs/1906.01437"/>
        <updated>2021-08-03T02:06:34.795Z</updated>
        <summary type="html"><![CDATA[We present several new complexity results for the algorithms that
approximately solve the optimal transport (OT) problem between two discrete
probability measures with at most $n$ atoms. First, we improve the complexity
bound of a greedy variant of the Sinkhorn algorithm, known as
\textit{Greenkhorn} algorithm, from $\widetilde{O}(n^2\varepsilon^{-3})$ to
$\widetilde{O}(n^2\varepsilon^{-2})$. Notably, this matches the best known
complexity bound of the Sinkhorn algorithm and sheds the light to superior
practical performance of the Greenkhorn algorithm. Second, we generalize an
adaptive primal-dual accelerated gradient descent (APDAGD)
algorithm~\citep{Dvurechensky-2018-Computational} with mirror mapping $\phi$
and prove that the resulting APDAMD algorithm achieves the complexity bound of
$\widetilde{O}(n^2\sqrt{\delta}\varepsilon^{-1})$ where $\delta>0$ refers to
the regularity of $\phi$. We demonstrate that the complexity bound of
$\widetilde{O}(\min\{n^{9/4}\varepsilon^{-1}, n^2\varepsilon^{-2}\})$ is
invalid for the APDAGD algorithm and establish a new complexity bound of
$\widetilde{O}(n^{5/2}\varepsilon^{-1})$. Moreover, we propose a
\textit{deterministic} accelerated Sinkhorn algorithm and prove that it
achieves the complexity bound of $\widetilde{O}(n^{7/3}\varepsilon^{-4/3})$ by
incorporating an estimate sequence. Therefore, the accelerated Sinkhorn
algorithm outperforms the Sinkhorn and Greenkhorn algorithms in terms of
$1/\varepsilon$ and the APDAGD and accelerated alternating
minimization~\citep{Guminov-2021-Combination} algorithms in terms of $n$.
Finally, we conduct experiments on synthetic data and real images with the
proposed algorithms in the paper and demonstrate their efficiency via numerical
results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1"&gt;Tianyi Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ho_N/0/1/0/all/0/1"&gt;Nhat Ho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jordan_M/0/1/0/all/0/1"&gt;Michael I. Jordan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deterministic error bounds for kernel-based learning techniques under bounded noise. (arXiv:2008.04005v3 [eess.SY] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.04005</id>
        <link href="http://arxiv.org/abs/2008.04005"/>
        <updated>2021-08-03T02:06:34.780Z</updated>
        <summary type="html"><![CDATA[We consider the problem of reconstructing a function from a finite set of
noise-corrupted samples. Two kernel algorithms are analyzed, namely kernel
ridge regression and $\varepsilon$-support vector regression. By assuming the
ground-truth function belongs to the reproducing kernel Hilbert space of the
chosen kernel, and the measurement noise affecting the dataset is bounded, we
adopt an approximation theory viewpoint to establish \textit{deterministic},
finite-sample error bounds for the two models. Finally, we discuss their
connection with Gaussian processes and two numerical examples are provided. In
establishing our inequalities, we hope to help bring the fields of
non-parametric kernel learning and system identification for robust control
closer to each other.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Maddalena_E/0/1/0/all/0/1"&gt;Emilio T. Maddalena&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Scharnhorst_P/0/1/0/all/0/1"&gt;Paul Scharnhorst&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jones_C/0/1/0/all/0/1"&gt;Colin N. Jones&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Height Estimation of Children under Five Years using Depth Images. (arXiv:2105.01688v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.01688</id>
        <link href="http://arxiv.org/abs/2105.01688"/>
        <updated>2021-08-03T02:06:34.773Z</updated>
        <summary type="html"><![CDATA[Malnutrition is a global health crisis and is the leading cause of death
among children under five. Detecting malnutrition requires anthropometric
measurements of weight, height, and middle-upper arm circumference. However,
measuring them accurately is a challenge, especially in the global south, due
to limited resources. In this work, we propose a CNN-based approach to estimate
the height of standing children under five years from depth images collected
using a smart-phone. According to the SMART Methodology Manual [5], the
acceptable accuracy for height is less than 1.4 cm. On training our deep
learning model on 87131 depth images, our model achieved an average mean
absolute error of 1.64% on 57064 test images. For 70.3% test images, we
estimated height accurately within the acceptable 1.4 cm range. Thus, our
proposed solution can accurately detect stunting (low height-for-age) in
standing children below five years of age.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Trivedi_A/0/1/0/all/0/1"&gt;Anusua Trivedi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jain_M/0/1/0/all/0/1"&gt;Mohit Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_N/0/1/0/all/0/1"&gt;Nikhil Kumar Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hinsche_M/0/1/0/all/0/1"&gt;Markus Hinsche&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_P/0/1/0/all/0/1"&gt;Prashant Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Matiaschek_M/0/1/0/all/0/1"&gt;Markus Matiaschek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Behrens_T/0/1/0/all/0/1"&gt;Tristan Behrens&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Militeri_M/0/1/0/all/0/1"&gt;Mirco Militeri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Birge_C/0/1/0/all/0/1"&gt;Cameron Birge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kaushik_S/0/1/0/all/0/1"&gt;Shivangi Kaushik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mohapatra_A/0/1/0/all/0/1"&gt;Archisman Mohapatra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chatterjee_R/0/1/0/all/0/1"&gt;Rita Chatterjee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dodhia_R/0/1/0/all/0/1"&gt;Rahul Dodhia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ferres_J/0/1/0/all/0/1"&gt;Juan Lavista Ferres&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Forecasting Thermoacoustic Instabilities in Liquid Propellant Rocket Engines Using Multimodal Bayesian Deep Learning. (arXiv:2107.06396v2 [physics.flu-dyn] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.06396</id>
        <link href="http://arxiv.org/abs/2107.06396"/>
        <updated>2021-08-03T02:06:34.765Z</updated>
        <summary type="html"><![CDATA[The 100 MW cryogenic liquid oxygen/hydrogen multi-injector combustor BKD
operated by the DLR Institute of Space Propulsion is a research platform that
allows the study of thermoacoustic instabilities under realistic conditions,
representative of small upper stage rocket engines. We use data from BKD
experimental campaigns in which the static chamber pressure and fuel-oxidizer
ratio are varied such that the first tangential mode of the combustor is
excited under some conditions. We train an autoregressive Bayesian neural
network model to forecast the amplitude of the dynamic pressure time series,
inputting multiple sensor measurements (injector pressure/ temperature
measurements, static chamber pressure, high-frequency dynamic pressure
measurements, high-frequency OH* chemiluminescence measurements) and future
flow rate control signals. The Bayesian nature of our algorithms allows us to
work with a dataset whose size is restricted by the expense of each
experimental run, without making overconfident extrapolations. We find that the
networks are able to accurately forecast the evolution of the pressure
amplitude and anticipate instability events on unseen experimental runs 500
milliseconds in advance. We compare the predictive accuracy of multiple models
using different combinations of sensor inputs. We find that the high-frequency
dynamic pressure signal is particularly informative. We also use the technique
of integrated gradients to interpret the influence of different sensor inputs
on the model prediction. The negative log-likelihood of data points in the test
dataset indicates that predictive uncertainties are well-characterized by our
Bayesian model and simulating a sensor failure event results as expected in a
dramatic increase in the epistemic component of the uncertainty.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Sengupta_U/0/1/0/all/0/1"&gt;Ushnish Sengupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Waxenegger_Wilfing_G/0/1/0/all/0/1"&gt;G&amp;#xfc;nther Waxenegger-Wilfing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Martin_J/0/1/0/all/0/1"&gt;Jan Martin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Hardi_J/0/1/0/all/0/1"&gt;Justin Hardi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Juniper_M/0/1/0/all/0/1"&gt;Matthew P. Juniper&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Anonymizing Machine Learning Models. (arXiv:2007.13086v3 [cs.CR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.13086</id>
        <link href="http://arxiv.org/abs/2007.13086"/>
        <updated>2021-08-03T02:06:34.716Z</updated>
        <summary type="html"><![CDATA[There is a known tension between the need to analyze personal data to drive
business and privacy concerns. Many data protection regulations, including the
EU General Data Protection Regulation (GDPR) and the California Consumer
Protection Act (CCPA), set out strict restrictions and obligations on the
collection and processing of personal data. Moreover, machine learning models
themselves can be used to derive personal information, as demonstrated by
recent membership and attribute inference attacks. Anonymized data, however, is
exempt from the obligations set out in these regulations. It is therefore
desirable to be able to create models that are anonymized, thus also exempting
them from those obligations, in addition to providing better protection against
attacks.

Learning on anonymized data typically results in significant degradation in
accuracy. In this work, we propose a method that is able to achieve better
model accuracy by using the knowledge encoded within the trained model, and
guiding our anonymization process to minimize the impact on the model's
accuracy, a process we call accuracy-guided anonymization. We demonstrate that
by focusing on the model's accuracy rather than generic information loss
measures, our method outperforms state of the art k-anonymity methods in terms
of the achieved utility, in particular with high values of k and large numbers
of quasi-identifiers.

We also demonstrate that our approach has a similar, and sometimes even
better ability to prevent membership inference attacks as approaches based on
differential privacy, while averting some of their drawbacks such as
complexity, performance overhead and model-specific implementations. This makes
model-guided anonymization a legitimate substitute for such methods and a
practical approach to creating privacy-preserving models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Goldsteen_A/0/1/0/all/0/1"&gt;Abigail Goldsteen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ezov_G/0/1/0/all/0/1"&gt;Gilad Ezov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shmelkin_R/0/1/0/all/0/1"&gt;Ron Shmelkin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moffie_M/0/1/0/all/0/1"&gt;Micha Moffie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Farkash_A/0/1/0/all/0/1"&gt;Ariel Farkash&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Unified Batch Selection Policy for Active Metric Learning. (arXiv:2102.07365v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.07365</id>
        <link href="http://arxiv.org/abs/2102.07365"/>
        <updated>2021-08-03T02:06:34.708Z</updated>
        <summary type="html"><![CDATA[Active metric learning is the problem of incrementally selecting high-utility
batches of training data (typically, ordered triplets) to annotate, in order to
progressively improve a learned model of a metric over some input domain as
rapidly as possible. Standard approaches, which independently assess the
informativeness of each triplet in a batch, are susceptible to highly
correlated batches with many redundant triplets and hence low overall utility.
While a recent work \cite{kumari2020batch} proposes batch-decorrelation
strategies for metric learning, they rely on ad hoc heuristics to estimate the
correlation between two triplets at a time. We present a novel batch active
metric learning method that leverages the Maximum Entropy Principle to learn
the least biased estimate of triplet distribution for a given set of prior
constraints. To avoid redundancy between triplets, our method collectively
selects batches with maximum joint entropy, which simultaneously captures both
informativeness and diversity. We take advantage of the submodularity of the
joint entropy function to construct a tractable solution using an efficient
greedy algorithm based on Gram-Schmidt orthogonalization that is provably
$\left( 1 - \frac{1}{e} \right)$-optimal. Our approach is the first batch
active metric learning method to define a unified score that balances
informativeness and diversity for an entire batch of triplets. Experiments with
several real-world datasets demonstrate that our algorithm is robust,
generalizes well to different applications and input modalities, and
consistently outperforms the state-of-the-art.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+K_P/0/1/0/all/0/1"&gt;Priyadarshini K&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chaudhuri_S/0/1/0/all/0/1"&gt;Siddhartha Chaudhuri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Borkar_V/0/1/0/all/0/1"&gt;Vivek Borkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chaudhuri_S/0/1/0/all/0/1"&gt;Subhasis Chaudhuri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Random Matrix Perspective on Random Tensors. (arXiv:2108.00774v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2108.00774</id>
        <link href="http://arxiv.org/abs/2108.00774"/>
        <updated>2021-08-03T02:06:34.679Z</updated>
        <summary type="html"><![CDATA[Tensor models play an increasingly prominent role in many fields, notably in
machine learning. In several applications of such models, such as community
detection, topic modeling and Gaussian mixture learning, one must estimate a
low-rank signal from a noisy tensor. Hence, understanding the fundamental
limits and the attainable performance of estimators of that signal inevitably
calls for the study of random tensors. Substantial progress has been achieved
on this subject thanks to recent efforts, under the assumption that the tensor
dimensions grow large. Yet, some of the most significant among these
results--in particular, a precise characterization of the abrupt phase
transition (in terms of signal-to-noise ratio) that governs the performance of
the maximum likelihood (ML) estimator of a symmetric rank-one model with
Gaussian noise--were derived on the basis of statistical physics ideas, which
are not easily accessible to non-experts.

In this work, we develop a sharply distinct approach, relying instead on
standard but powerful tools brought by years of advances in random matrix
theory. The key idea is to study the spectra of random matrices arising from
contractions of a given random tensor. We show how this gives access to
spectral properties of the random tensor itself. In the specific case of a
symmetric rank-one model with Gaussian noise, our technique yields a hitherto
unknown characterization of the local maximum of the ML problem that is global
above the phase transition threshold. This characterization is in terms of a
fixed-point equation satisfied by a formula that had only been previously
obtained via statistical physics methods. Moreover, our analysis sheds light on
certain properties of the landscape of the ML problem in the large-dimensional
setting. Our approach is versatile and can be extended to other models, such as
asymmetric, non-Gaussian and higher-order ones.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Goulart_J/0/1/0/all/0/1"&gt;Jos&amp;#xe9; Henrique de Morais Goulart&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Couillet_R/0/1/0/all/0/1"&gt;Romain Couillet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Comon_P/0/1/0/all/0/1"&gt;Pierre Comon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Attend2Pack: Bin Packing through Deep Reinforcement Learning with Attention. (arXiv:2107.04333v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.04333</id>
        <link href="http://arxiv.org/abs/2107.04333"/>
        <updated>2021-08-03T02:06:34.443Z</updated>
        <summary type="html"><![CDATA[This paper seeks to tackle the bin packing problem (BPP) through a learning
perspective. Building on self-attention-based encoding and deep reinforcement
learning algorithms, we propose a new end-to-end learning model for this task
of interest. By decomposing the combinatorial action space, as well as
utilizing a new training technique denoted as prioritized oversampling, which
is a general scheme to speed up on-policy learning, we achieve state-of-the-art
performance in a range of experimental settings. Moreover, although the
proposed approach attend2pack targets offline-BPP, we strip our method down to
the strict online-BPP setting where it is also able to achieve state-of-the-art
performance. With a set of ablation studies as well as comparisons against a
range of previous works, we hope to offer as a valid baseline approach to this
field of study.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jingwei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zi_B/0/1/0/all/0/1"&gt;Bin Zi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ge_X/0/1/0/all/0/1"&gt;Xiaoyu Ge&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Machine Learning Applications on Neuroimaging for Diagnosis and Prognosis of Epilepsy: A Review. (arXiv:2102.03336v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.03336</id>
        <link href="http://arxiv.org/abs/2102.03336"/>
        <updated>2021-08-03T02:06:34.419Z</updated>
        <summary type="html"><![CDATA[Machine learning is playing an increasingly important role in medical image
analysis, spawning new advances in the clinical application of neuroimaging.
There have been some reviews of machine learning and epilepsy before, but they
mainly focused on electrophysiological signals such as
electroencephalography(EEG) or stereo electroencephalography(SEEG), while
ignoring the potential of neuroimaging in epilepsy research. Neuroimaging has
its important advantages in confirming the range of epileptic region, which
means a lot in presurgical evaluation and assessment after surgery. However,
EEG is difficult to locate the epilepsy lesion region in the brain. In this
review, we emphasize the interaction between neuroimaging and machine learning
in the context of the epilepsy diagnosis and prognosis. We start with an
overview of typical neuroimaging modalities used in epilepsy clinics, MRI, DTI,
fMRI, and PET. Then, we introduce three approaches for applying machine
learning methods to neuroimaging data: i) the two-step compositional approach
combining feature engineering and machine learning classifiers, ii) the
end-to-end approach, which is usually toward deep learning, and iii) the hybrid
approach using the advantages of the two methods. Subsequently, the application
of machine learning on epilepsy neuroimaging, such as segmentation,
localization and lateralization tasks, as well as tasks directly related to
diagnosis and prognosis are introduced in detail. Finally, we discuss the
current achievements, challenges, and potential future directions in this
field, hoping to pave the way for computer-aided diagnosis and prognosis of
epilepsy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1"&gt;Jie Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ran_X/0/1/0/all/0/1"&gt;Xuming Ran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1"&gt;Keyin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_C/0/1/0/all/0/1"&gt;Chen Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1"&gt;Yi Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1"&gt;Haiyan Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1"&gt;Quanying Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Analysis and Optimisation of Bellman Residual Errors with Neural Function Approximation. (arXiv:2106.08774v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.08774</id>
        <link href="http://arxiv.org/abs/2106.08774"/>
        <updated>2021-08-03T02:06:34.410Z</updated>
        <summary type="html"><![CDATA[Recent development of Deep Reinforcement Learning has demonstrated superior
performance of neural networks in solving challenging problems with large or
even continuous state spaces. One specific approach is to deploy neural
networks to approximate value functions by minimising the Mean Squared Bellman
Error function. Despite great successes of Deep Reinforcement Learning,
development of reliable and efficient numerical algorithms to minimise the
Bellman Error is still of great scientific interest and practical demand. Such
a challenge is partially due to the underlying optimisation problem being
highly non-convex or using incorrect gradient information as done in
Semi-Gradient algorithms. In this work, we analyse the Mean Squared Bellman
Error from a smooth optimisation perspective combined with a Residual Gradient
formulation. Our contribution is two-fold.

First, we analyse critical points of the error function and provide technical
insights on the optimisation procure and design choices for neural networks.
When the existence of global minima is assumed and the objective fulfils
certain conditions we can eliminate suboptimal local minima when using
over-parametrised neural networks. We can construct an efficient Approximate
Newton's algorithm based on our analysis and confirm theoretical properties of
this algorithm such as being locally quadratically convergent to a global
minimum numerically.

Second, we demonstrate feasibility and generalisation capabilities of the
proposed algorithm empirically using continuous control problems and provide a
numerical verification of our critical point analysis. We outline the short
coming of Semi-Gradients. To benefit from an approximate Newton's algorithm
complete derivatives of the Mean Squared Bellman error must be considered
during training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gottwald_M/0/1/0/all/0/1"&gt;Martin Gottwald&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Gronauer_S/0/1/0/all/0/1"&gt;Sven Gronauer&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1"&gt;Hao Shen&lt;/a&gt; (2), &lt;a href="http://arxiv.org/find/cs/1/au:+Diepold_K/0/1/0/all/0/1"&gt;Klaus Diepold&lt;/a&gt; (1) ((1) Technical University of Munich, (2) fortiss)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploring the representativeness of the M5 competition data. (arXiv:2103.02941v2 [stat.AP] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.02941</id>
        <link href="http://arxiv.org/abs/2103.02941"/>
        <updated>2021-08-03T02:06:34.396Z</updated>
        <summary type="html"><![CDATA[The main objective of the M5 competition, which focused on forecasting the
hierarchical unit sales of Walmart, was to evaluate the accuracy and
uncertainty of forecasting methods in the field in order to identify best
practices and highlight their practical implications. However, whether the
findings of the M5 competition can be generalized and exploited by retail firms
to better support their decisions and operation depends on the extent to which
the M5 data is sufficiently similar to unit sales data of retailers that
operate in different regions, sell different types of products, and consider
different marketing strategies. To answer this question, we analyze the
characteristics of the M5 time series and compare them with those of two
grocery retailers, namely Corporaci\'on Favorita and a major Greek supermarket
chain, using feature spaces. Our results suggest that there are only small
discrepancies between the examined data sets, supporting the representativeness
of the M5 data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Theodorou_E/0/1/0/all/0/1"&gt;Evangelos Theodorou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wang_S/0/1/0/all/0/1"&gt;Shengjie Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kang_Y/0/1/0/all/0/1"&gt;Yanfei Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Spiliotis_E/0/1/0/all/0/1"&gt;Evangelos Spiliotis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Makridakis_S/0/1/0/all/0/1"&gt;Spyros Makridakis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Assimakopoulos_V/0/1/0/all/0/1"&gt;Vassilios Assimakopoulos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Weighted A*: Learning Graph Costs and Heuristics with Differentiable Anytime A*. (arXiv:2105.01480v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.01480</id>
        <link href="http://arxiv.org/abs/2105.01480"/>
        <updated>2021-08-03T02:06:34.373Z</updated>
        <summary type="html"><![CDATA[Recently, the trend of incorporating differentiable algorithms into deep
learning architectures arose in machine learning research, as the fusion of
neural layers and algorithmic layers has been beneficial for handling
combinatorial data, such as shortest paths on graphs. Recent works related to
data-driven planning aim at learning either cost functions or heuristic
functions, but not both. We propose Neural Weighted A*, a differentiable
anytime planner able to produce improved representations of planar maps as
graph costs and heuristics. Training occurs end-to-end on raw images with
direct supervision on planning examples, thanks to a differentiable A* solver
integrated into the architecture. More importantly, the user can trade off
planning accuracy for efficiency at run-time, using a single, real-valued
parameter. The solution suboptimality is constrained within a linear bound
equal to the optimal path cost multiplied by the tradeoff parameter. We
experimentally show the validity of our claims by testing Neural Weighted A*
against several baselines, introducing a novel, tile-based navigation dataset.
We outperform similar architectures in planning accuracy and efficiency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Archetti_A/0/1/0/all/0/1"&gt;Alberto Archetti&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cannici_M/0/1/0/all/0/1"&gt;Marco Cannici&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Matteucci_M/0/1/0/all/0/1"&gt;Matteo Matteucci&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[OpenCSI: An Open-Source Dataset for Indoor Localization Using CSI-Based Fingerprinting. (arXiv:2104.07963v2 [eess.SP] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.07963</id>
        <link href="http://arxiv.org/abs/2104.07963"/>
        <updated>2021-08-03T02:06:34.359Z</updated>
        <summary type="html"><![CDATA[Many applications require accurate indoor localization. Fingerprint-based
localization methods propose a solution to this problem, but rely on a radio
map that is effort-intensive to acquire. We automate the radio map acquisition
phase using a software-defined radio (SDR) and a wheeled robot. Furthermore, we
open-source a radio map acquired with our automated tool for a 3GPP Long-Term
Evolution (LTE) wireless link. To the best of our knowledge, this is the first
publicly available radio map containing channel state information (CSI).
Finally, we describe first localization experiments on this radio map using a
convolutional neural network to regress for location coordinates.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Gassner_A/0/1/0/all/0/1"&gt;Arthur Gassner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Musat_C/0/1/0/all/0/1"&gt;Claudiu Musat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rusu_A/0/1/0/all/0/1"&gt;Alexandru Rusu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Burg_A/0/1/0/all/0/1"&gt;Andreas Burg&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Backdoor Scanning for Deep Neural Networks through K-Arm Optimization. (arXiv:2102.05123v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.05123</id>
        <link href="http://arxiv.org/abs/2102.05123"/>
        <updated>2021-08-03T02:06:34.339Z</updated>
        <summary type="html"><![CDATA[Back-door attack poses a severe threat to deep learning systems. It injects
hidden malicious behaviors to a model such that any input stamped with a
special pattern can trigger such behaviors. Detecting back-door is hence of
pressing need. Many existing defense techniques use optimization to generate
the smallest input pattern that forces the model to misclassify a set of benign
inputs injected with the pattern to a target label. However, the complexity is
quadratic to the number of class labels such that they can hardly handle models
with many classes. Inspired by Multi-Arm Bandit in Reinforcement Learning, we
propose a K-Arm optimization method for backdoor detection. By iteratively and
stochastically selecting the most promising labels for optimization with the
guidance of an objective function, we substantially reduce the complexity,
allowing to handle models with many classes. Moreover, by iteratively refining
the selection of labels to optimize, it substantially mitigates the uncertainty
in choosing the right labels, improving detection accuracy. At the time of
submission, the evaluation of our method on over 4000 models in the IARPA
TrojAI competition from round 1 to the latest round 4 achieves top performance
on the leaderboard. Our technique also supersedes three state-of-the-art
techniques in terms of accuracy and the scanning time needed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shen_G/0/1/0/all/0/1"&gt;Guangyu Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yingqi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tao_G/0/1/0/all/0/1"&gt;Guanhong Tao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+An_S/0/1/0/all/0/1"&gt;Shengwei An&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1"&gt;Qiuling Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1"&gt;Siyuan Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1"&gt;Shiqing Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiangyu Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Named Entity Recognition with Small Strongly Labeled and Large Weakly Labeled Data. (arXiv:2106.08977v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.08977</id>
        <link href="http://arxiv.org/abs/2106.08977"/>
        <updated>2021-08-03T02:06:34.336Z</updated>
        <summary type="html"><![CDATA[Weak supervision has shown promising results in many natural language
processing tasks, such as Named Entity Recognition (NER). Existing work mainly
focuses on learning deep NER models only with weak supervision, i.e., without
any human annotation, and shows that by merely using weakly labeled data, one
can achieve good performance, though still underperforms fully supervised NER
with manually/strongly labeled data. In this paper, we consider a more
practical scenario, where we have both a small amount of strongly labeled data
and a large amount of weakly labeled data. Unfortunately, we observe that
weakly labeled data does not necessarily improve, or even deteriorate the model
performance (due to the extensive noise in the weak labels) when we train deep
NER models over a simple or weighted combination of the strongly labeled and
weakly labeled data. To address this issue, we propose a new multi-stage
computational framework -- NEEDLE with three essential ingredients: (1) weak
label completion, (2) noise-aware loss function, and (3) final fine-tuning over
the strongly labeled data. Through experiments on E-commerce query NER and
Biomedical NER, we demonstrate that NEEDLE can effectively suppress the noise
of the weak labels and outperforms existing methods. In particular, we achieve
new SOTA F1-scores on 3 Biomedical NER datasets: BC5CDR-chem 93.74,
BC5CDR-disease 90.69, NCBI-disease 92.28.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1"&gt;Haoming Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1"&gt;Danqing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_T/0/1/0/all/0/1"&gt;Tianyu Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_B/0/1/0/all/0/1"&gt;Bing Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1"&gt;Tuo Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Emergent Quantumness in Neural Networks. (arXiv:2012.05082v2 [quant-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.05082</id>
        <link href="http://arxiv.org/abs/2012.05082"/>
        <updated>2021-08-03T02:06:34.320Z</updated>
        <summary type="html"><![CDATA[It was recently shown that the Madelung equations, that is, a hydrodynamic
form of the Schr\"odinger equation, can be derived from a canonical ensemble of
neural networks where the quantum phase was identified with the free energy of
hidden variables. We consider instead a grand canonical ensemble of neural
networks, by allowing an exchange of neurons with an auxiliary subsystem, to
show that the free energy must also be multivalued. By imposing the
multivaluedness condition on the free energy we derive the Schr\"odinger
equation with "Planck's constant" determined by the chemical potential of
hidden variables. This shows that quantum mechanics provides a correct
statistical description of the dynamics of the grand canonical ensemble of
neural networks at the learning equilibrium. We also discuss implications of
the results for machine learning, fundamental physics and, in a more
speculative way, evolutionary biology.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Katsnelson_M/0/1/0/all/0/1"&gt;Mikhail I. Katsnelson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Vanchurin_V/0/1/0/all/0/1"&gt;Vitaly Vanchurin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ReCU: Reviving the Dead Weights in Binary Neural Networks. (arXiv:2103.12369v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.12369</id>
        <link href="http://arxiv.org/abs/2103.12369"/>
        <updated>2021-08-03T02:06:34.310Z</updated>
        <summary type="html"><![CDATA[Binary neural networks (BNNs) have received increasing attention due to their
superior reductions of computation and memory. Most existing works focus on
either lessening the quantization error by minimizing the gap between the
full-precision weights and their binarization or designing a gradient
approximation to mitigate the gradient mismatch, while leaving the "dead
weights" untouched. This leads to slow convergence when training BNNs. In this
paper, for the first time, we explore the influence of "dead weights" which
refer to a group of weights that are barely updated during the training of
BNNs, and then introduce rectified clamp unit (ReCU) to revive the "dead
weights" for updating. We prove that reviving the "dead weights" by ReCU can
result in a smaller quantization error. Besides, we also take into account the
information entropy of the weights, and then mathematically analyze why the
weight standardization can benefit BNNs. We demonstrate the inherent
contradiction between minimizing the quantization error and maximizing the
information entropy, and then propose an adaptive exponential scheduler to
identify the range of the "dead weights". By considering the "dead weights",
our method offers not only faster BNN training, but also state-of-the-art
performance on CIFAR-10 and ImageNet, compared with recent methods. Code can be
available at https://github.com/z-hXu/ReCU.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zihan Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1"&gt;Mingbao Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jianzhuang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jie Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1"&gt;Ling Shao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1"&gt;Yue Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1"&gt;Yonghong Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1"&gt;Rongrong Ji&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Threat of Adversarial Attacks on Deep Learning in Computer Vision: Survey II. (arXiv:2108.00401v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00401</id>
        <link href="http://arxiv.org/abs/2108.00401"/>
        <updated>2021-08-03T02:06:34.290Z</updated>
        <summary type="html"><![CDATA[Deep Learning (DL) is the most widely used tool in the contemporary field of
computer vision. Its ability to accurately solve complex problems is employed
in vision research to learn deep neural models for a variety of tasks,
including security critical applications. However, it is now known that DL is
vulnerable to adversarial attacks that can manipulate its predictions by
introducing visually imperceptible perturbations in images and videos. Since
the discovery of this phenomenon in 2013~[1], it has attracted significant
attention of researchers from multiple sub-fields of machine intelligence. In
[2], we reviewed the contributions made by the computer vision community in
adversarial attacks on deep learning (and their defenses) until the advent of
year 2018. Many of those contributions have inspired new directions in this
area, which has matured significantly since witnessing the first generation
methods. Hence, as a legacy sequel of [2], this literature review focuses on
the advances in this area since 2018. To ensure authenticity, we mainly
consider peer-reviewed contributions published in the prestigious sources of
computer vision and machine learning research. Besides a comprehensive
literature review, the article also provides concise definitions of technical
terminologies for non-experts in this domain. Finally, this article discusses
challenges and future outlook of this direction based on the literature
reviewed herein and [2].]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Akhtar_N/0/1/0/all/0/1"&gt;Naveed Akhtar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mian_A/0/1/0/all/0/1"&gt;Ajmal Mian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kardan_N/0/1/0/all/0/1"&gt;Navid Kardan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1"&gt;Mubarak Shah&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SFE-Net: EEG-based Emotion Recognition with Symmetrical Spatial Feature Extraction. (arXiv:2104.06308v4 [eess.SP] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.06308</id>
        <link href="http://arxiv.org/abs/2104.06308"/>
        <updated>2021-08-03T02:06:34.284Z</updated>
        <summary type="html"><![CDATA[Emotion recognition based on EEG (electroencephalography) has been widely
used in human-computer interaction, distance education and health care.
However, the conventional methods ignore the adjacent and symmetrical
characteristics of EEG signals, which also contain salient information related
to emotion. In this paper, a spatial folding ensemble network (SFE-Net) is
presented for EEG feature extraction and emotion recognition. Firstly, for the
undetected area between EEG electrodes, an improved Bicubic-EEG interpolation
algorithm is developed for EEG channels information completion, which allows us
to extract a wider range of adjacent space features. Then, motivated by the
spatial symmetric mechanism of human brain, we fold the input EEG channels data
with five different symmetrical strategies, which enable the proposed network
to extract the information of space features of EEG signals more effectively.
Finally, a 3DCNN-based spatial, temporal extraction, and a multi-voting
strategy of ensemble learning are integrated to model a new neural network.
With this network, the spatial features of different symmetric folding signals
can be extracted simultaneously, which greatly improves the robustness and
accuracy of emotion recognition. The experimental results on DEAP and SEED
datasets show that the proposed algorithm has comparable performance in terms
of recognition accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Deng_X/0/1/0/all/0/1"&gt;Xiangwen Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Junlin Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yang_S/0/1/0/all/0/1"&gt;Shangming Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[InversionNet3D: Efficient and Scalable Learning for 3D Full Waveform Inversion. (arXiv:2103.14158v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.14158</id>
        <link href="http://arxiv.org/abs/2103.14158"/>
        <updated>2021-08-03T02:06:34.278Z</updated>
        <summary type="html"><![CDATA[Seismic full-waveform inversion (FWI) techniques aim to find a
high-resolution subsurface geophysical model provided with waveform data. Some
recent effort in data-driven FWI has shown some encouraging results in
obtaining 2D velocity maps. However, due to high computational complexity and
large memory consumption, the reconstruction of 3D high-resolution velocity
maps via deep networks is still a great challenge. In this paper, we present
InversionNet3D, an efficient and scalable encoder-decoder network for 3D FWI.
The proposed method employs group convolution in the encoder to establish an
effective hierarchy for learning information from multiple sources while
cutting down unnecessary parameters and operations at the same time. The
introduction of invertible layers further reduces the memory consumption of
intermediate features during training and thus enables the development of
deeper networks with more layers and higher capacity as required by different
application scenarios. Experiments on the 3D Kimberlina dataset demonstrate
that InversionNet3D achieves state-of-the-art reconstruction performance with
lower computational cost and lower memory footprint compared to the baseline.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_Q/0/1/0/all/0/1"&gt;Qili Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1"&gt;Shihang Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wohlberg_B/0/1/0/all/0/1"&gt;Brendt Wohlberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1"&gt;Youzuo Lin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Benchmarking Robustness of Deep Learning Classifiers Using Two-Factor Perturbation. (arXiv:2103.03102v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.03102</id>
        <link href="http://arxiv.org/abs/2103.03102"/>
        <updated>2021-08-03T02:06:34.272Z</updated>
        <summary type="html"><![CDATA[The accuracy of DL classifiers is unstable in that it often changes
significantly when retested on adversarial images, imperfect images, or
perturbed images. This paper adds to the small but fundamental body of work on
benchmarking the robustness of DL classifiers on defective images. Unlike
existed single-factor digital perturbation work, we provide state-of-the-art
two-factor perturbation that provides two natural perturbations on images
applied in different sequences. The two-factor perturbation includes (1) two
digital perturbations (Salt & pepper noise and Gaussian noise) applied in both
sequences. (2) one digital perturbation (salt & pepper noise) and a geometric
perturbation (rotation) applied in different sequences. To measure robust DL
classifiers, previous scientists provided 15 types of single-factor corruption.
We created 69 benchmarking image sets, including a clean set, sets with single
factor perturbations, and sets with two-factor perturbation conditions. To be
best of our knowledge, this is the first report that two-factor perturbed
images improves both robustness and accuracy of DL classifiers. Previous
research evaluating deep learning (DL) classifiers has often used top-1/top-5
accuracy, so researchers have usually offered tables, line diagrams, and bar
charts to display accuracy of DL classifiers. But these existed approaches
cannot quantitively evaluate robustness of DL classifiers. We innovate a new
two-dimensional, statistical visualization tool, including mean accuracy and
coefficient of variation (CV), to benchmark the robustness of DL classifiers.
All source codes and related image sets are shared on websites
(this http URL or
https://github.com/daiweiworking/RobustDeepLearningUsingPerturbations ) to
support future academic research and industry projects.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1"&gt;Wei Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berleant_D/0/1/0/all/0/1"&gt;Daniel Berleant&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fast model-based clustering of partial records. (arXiv:2103.16336v4 [stat.ME] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.16336</id>
        <link href="http://arxiv.org/abs/2103.16336"/>
        <updated>2021-08-03T02:06:34.265Z</updated>
        <summary type="html"><![CDATA[Partially recorded data are frequently encountered in many applications and
usually clustered by first removing incomplete cases or features with missing
values, or by imputing missing values, followed by application of a clustering
algorithm to the resulting altered dataset. Here, we develop clustering
methodology through a model-based approach using the marginal density for the
observed values, assuming a finite mixture model of multivariate $t$
distributions. We compare our approximate algorithm to the corresponding full
expectation-maximization (EM) approach that considers the missing values in the
incomplete data set and makes a missing at random (MAR) assumption, as well as
case deletion and imputation methods. Since only the observed values are
utilized, our approach is computationally more efficient than imputation or
full EM. Simulation studies demonstrate that our approach has favorable
recovery of the true cluster partition compared to case deletion and imputation
under various missingness mechanisms, and is at least competitive with the full
EM approach, even when MAR assumptions are violated. Our methodology is
demonstrated on a problem of clustering gamma-ray bursts and is implemented at
https://github.com/emilygoren/MixtClust.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Goren_E/0/1/0/all/0/1"&gt;Emily M. Goren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Maitra_R/0/1/0/all/0/1"&gt;Ranjan Maitra&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Human Pose Regression with Residual Log-likelihood Estimation. (arXiv:2107.11291v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.11291</id>
        <link href="http://arxiv.org/abs/2107.11291"/>
        <updated>2021-08-03T02:06:34.247Z</updated>
        <summary type="html"><![CDATA[Heatmap-based methods dominate in the field of human pose estimation by
modelling the output distribution through likelihood heatmaps. In contrast,
regression-based methods are more efficient but suffer from inferior
performance. In this work, we explore maximum likelihood estimation (MLE) to
develop an efficient and effective regression-based methods. From the
perspective of MLE, adopting different regression losses is making different
assumptions about the output density function. A density function closer to the
true distribution leads to a better regression performance. In light of this,
we propose a novel regression paradigm with Residual Log-likelihood Estimation
(RLE) to capture the underlying output distribution. Concretely, RLE learns the
change of the distribution instead of the unreferenced underlying distribution
to facilitate the training process. With the proposed reparameterization
design, our method is compatible with off-the-shelf flow models. The proposed
method is effective, efficient and flexible. We show its potential in various
human pose estimation tasks with comprehensive experiments. Compared to the
conventional regression paradigm, regression with RLE bring 12.4 mAP
improvement on MSCOCO without any test-time overhead. Moreover, for the first
time, especially on multi-person pose estimation, our regression method is
superior to the heatmap-based methods. Our code is available at
https://github.com/Jeff-sjtu/res-loglikelihood-regression]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jiefeng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bian_S/0/1/0/all/0/1"&gt;Siyuan Bian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_A/0/1/0/all/0/1"&gt;Ailing Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Can Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pang_B/0/1/0/all/0/1"&gt;Bo Pang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wentao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1"&gt;Cewu Lu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Few-shot Neural Architecture Search. (arXiv:2006.06863v9 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.06863</id>
        <link href="http://arxiv.org/abs/2006.06863"/>
        <updated>2021-08-03T02:06:34.241Z</updated>
        <summary type="html"><![CDATA[Efficient evaluation of a network architecture drawn from a large search
space remains a key challenge in Neural Architecture Search (NAS). Vanilla NAS
evaluates each architecture by training from scratch, which gives the true
performance but is extremely time-consuming. Recently, one-shot NAS
substantially reduces the computation cost by training only one supernetwork,
a.k.a. supernet, to approximate the performance of every architecture in the
search space via weight-sharing. However, the performance estimation can be
very inaccurate due to the co-adaption among operations. In this paper, we
propose few-shot NAS that uses multiple supernetworks, called sub-supernet,
each covering different regions of the search space to alleviate the undesired
co-adaption. Compared to one-shot NAS, few-shot NAS improves the accuracy of
architecture evaluation with a small increase of evaluation cost. With only up
to 7 sub-supernets, few-shot NAS establishes new SoTAs: on ImageNet, it finds
models that reach 80.5% top-1 accuracy at 600 MB FLOPS and 77.5% top-1 accuracy
at 238 MFLOPS; on CIFAR10, it reaches 98.72% top-1 accuracy without using extra
data or transfer learning. In Auto-GAN, few-shot NAS outperforms the previously
published results by up to 20%. Extensive experiments show that few-shot NAS
significantly improves various one-shot methods, including 4 gradient-based and
6 search-based methods on 3 different tasks in NasBench-201 and
NasBench1-shot-1.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yiyang Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Linnan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1"&gt;Yuandong Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fonseca_R/0/1/0/all/0/1"&gt;Rodrigo Fonseca&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_T/0/1/0/all/0/1"&gt;Tian Guo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[IPOF: An Extremely and Excitingly Simple Outlier Detection Booster via Infinite Propagation. (arXiv:2108.00360v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.00360</id>
        <link href="http://arxiv.org/abs/2108.00360"/>
        <updated>2021-08-03T02:06:34.234Z</updated>
        <summary type="html"><![CDATA[Outlier detection is one of the most popular and continuously rising topics
in the data mining field due to its crucial academic value and extensive
industrial applications. Among different settings, unsupervised outlier
detection is the most challenging and practical one, which attracts tremendous
efforts from diverse perspectives. In this paper, we consider the score-based
outlier detection category and point out that the performance of current
outlier detection algorithms might be further boosted by score propagation.
Specifically, we propose Infinite Propagation of Outlier Factor (iPOF)
algorithm, an extremely and excitingly simple outlier detection booster via
infinite propagation. By employing score-based outlier detectors for
initialization, iPOF updates each data point's outlier score by averaging the
outlier factors of its nearest common neighbors. Extensive experimental results
on numerous datasets in various domains demonstrate the effectiveness and
efficiency of iPOF significantly over several classical and recent
state-of-the-art methods. We also provide the parameter analysis on the number
of neighbors, the unique parameter in iPOF, and different initial outlier
detectors for general validation. It is worthy to note that iPOF brings in
positive improvements ranging from 2% to 46% on the average level, and in some
cases, iPOF boosts the performance over 3000% over the original outlier
detection algorithm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1"&gt;Sibo Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1"&gt;Handong Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Hongfu Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Human Haptic Gesture Interpretation for Robotic Systems. (arXiv:2012.01959v4 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.01959</id>
        <link href="http://arxiv.org/abs/2012.01959"/>
        <updated>2021-08-03T02:06:34.206Z</updated>
        <summary type="html"><![CDATA[Physical human-robot interactions (pHRI) are less efficient and communicative
than human-human interactions, and a key reason is a lack of informative sense
of touch in robotic systems. Interpreting human touch gestures is a nuanced,
challenging task with extreme gaps between human and robot capability. Among
prior works that demonstrate human touch recognition capability, differences in
sensors, gesture classes, feature sets, and classification algorithms yield a
conglomerate of non-transferable results and a glaring lack of a standard. To
address this gap, this work presents 1) four proposed touch gesture classes
that cover an important subset of the gesture characteristics identified in the
literature, 2) the collection of an extensive force dataset on a common pHRI
robotic arm with only its internal wrist force-torque sensor, and 3) an
exhaustive performance comparison of combinations of feature sets and
classification algorithms on this dataset. We demonstrate high classification
accuracies among our proposed gesture definitions on a test set, emphasizing
that neural net-work classifiers on the raw data outperform other combinations
of feature sets and algorithms. The accompanying video is here:
https://youtu.be/gJPVImNKU68]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bianchini_E/0/1/0/all/0/1"&gt;Elizabeth Bibit Bianchini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Verma_P/0/1/0/all/0/1"&gt;Prateek Verma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Salisbury_K/0/1/0/all/0/1"&gt;Kenneth Salisbury&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Escaping from Zero Gradient: Revisiting Action-Constrained Reinforcement Learning via Frank-Wolfe Policy Optimization. (arXiv:2102.11055v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.11055</id>
        <link href="http://arxiv.org/abs/2102.11055"/>
        <updated>2021-08-03T02:06:34.200Z</updated>
        <summary type="html"><![CDATA[Action-constrained reinforcement learning (RL) is a widely-used approach in
various real-world applications, such as scheduling in networked systems with
resource constraints and control of a robot with kinematic constraints. While
the existing projection-based approaches ensure zero constraint violation, they
could suffer from the zero-gradient problem due to the tight coupling of the
policy gradient and the projection, which results in sample-inefficient
training and slow convergence. To tackle this issue, we propose a learning
algorithm that decouples the action constraints from the policy parameter
update by leveraging state-wise Frank-Wolfe and a regression-based policy
update scheme. Moreover, we show that the proposed algorithm enjoys convergence
and policy improvement properties in the tabular case as well as generalizes
the popular DDPG algorithm for action-constrained RL in the general case.
Through experiments, we demonstrate that the proposed algorithm significantly
outperforms the benchmark methods on a variety of control tasks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1"&gt;Jyun-Li Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hung_W/0/1/0/all/0/1"&gt;Wei Hung&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1"&gt;Shang-Hsuan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hsieh_P/0/1/0/all/0/1"&gt;Ping-Chun Hsieh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xi Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Deep Learning for HAR with shallow LSTMs. (arXiv:2108.00702v1 [cs.HC])]]></title>
        <id>http://arxiv.org/abs/2108.00702</id>
        <link href="http://arxiv.org/abs/2108.00702"/>
        <updated>2021-08-03T02:06:34.169Z</updated>
        <summary type="html"><![CDATA[Recent studies in Human Activity Recognition (HAR) have shown that Deep
Learning methods are able to outperform classical Machine Learning algorithms.
One popular Deep Learning architecture in HAR is the DeepConvLSTM. In this
paper we propose to alter the DeepConvLSTM architecture to employ a 1-layered
instead of a 2-layered LSTM. We validate our architecture change on 5 publicly
available HAR datasets by comparing the predictive performance with and without
the change employing varying hidden units within the LSTM layer(s). Results
show that across all datasets, our architecture consistently improves on the
original one: Recognition performance increases up to 11.7% for the F1-score,
and our architecture significantly decreases the amount of learnable
parameters. This improvement over DeepConvLSTM decreases training time by as
much as 48%. Our results stand in contrast to the belief that one needs at
least a 2-layered LSTM when dealing with sequential data. Based on our results
we argue that said claim might not be applicable to sensor-based HAR.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bock_M/0/1/0/all/0/1"&gt;Marius Bock&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoelzemann_A/0/1/0/all/0/1"&gt;Alexander Hoelzemann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moeller_M/0/1/0/all/0/1"&gt;Michael Moeller&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Laerhoven_K/0/1/0/all/0/1"&gt;Kristof Van Laerhoven&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TDA-Net: Fusion of Persistent Homology and Deep Learning Features for COVID-19 Detection in Chest X-Ray Images. (arXiv:2101.08398v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.08398</id>
        <link href="http://arxiv.org/abs/2101.08398"/>
        <updated>2021-08-03T02:06:34.163Z</updated>
        <summary type="html"><![CDATA[Topological Data Analysis (TDA) has emerged recently as a robust tool to
extract and compare the structure of datasets. TDA identifies features in data
such as connected components and holes and assigns a quantitative measure to
these features. Several studies reported that topological features extracted by
TDA tools provide unique information about the data, discover new insights, and
determine which feature is more related to the outcome. On the other hand, the
overwhelming success of deep neural networks in learning patterns and
relationships has been proven on a vast array of data applications, images in
particular. To capture the characteristics of both powerful tools, we propose
\textit{TDA-Net}, a novel ensemble network that fuses topological and deep
features for the purpose of enhancing model generalizability and accuracy. We
apply the proposed \textit{TDA-Net} to a critical application, which is the
automated detection of COVID-19 from CXR images. The experimental results
showed that the proposed network achieved excellent performance and suggests
the applicability of our method in practice.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hajij_M/0/1/0/all/0/1"&gt;Mustafa Hajij&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zamzmi_G/0/1/0/all/0/1"&gt;Ghada Zamzmi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Batayneh_F/0/1/0/all/0/1"&gt;Fawwaz Batayneh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exploiting Multiple Timescales in Hierarchical Echo State Networks. (arXiv:2101.04223v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.04223</id>
        <link href="http://arxiv.org/abs/2101.04223"/>
        <updated>2021-08-03T02:06:34.152Z</updated>
        <summary type="html"><![CDATA[Echo state networks (ESNs) are a powerful form of reservoir computing that
only require training of linear output weights whilst the internal reservoir is
formed of fixed randomly connected neurons. With a correctly scaled
connectivity matrix, the neurons' activity exhibits the echo-state property and
responds to the input dynamics with certain timescales. Tuning the timescales
of the network can be necessary for treating certain tasks, and some
environments require multiple timescales for an efficient representation. Here
we explore the timescales in hierarchical ESNs, where the reservoir is
partitioned into two smaller linked reservoirs with distinct properties. Over
three different tasks (NARMA10, a reconstruction task in a volatile
environment, and psMNIST), we show that by selecting the hyper-parameters of
each partition such that they focus on different timescales, we achieve a
significant performance improvement over a single ESN. Through a linear
analysis, and under the assumption that the timescales of the first partition
are much shorter than the second's (typically corresponding to optimal
operating conditions), we interpret the feedforward coupling of the partitions
in terms of an effective representation of the input signal, provided by the
first partition to the second, whereby the instantaneous input signal is
expanded into a weighted combination of its time derivatives. Furthermore, we
propose a data-driven approach to optimise the hyper-parameters through a
gradient descent optimisation method that is an online approximation of
backpropagation through time. We demonstrate the application of the online
learning rule across all the tasks considered.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Manneschi_L/0/1/0/all/0/1"&gt;Luca Manneschi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ellis_M/0/1/0/all/0/1"&gt;Matthew O. A. Ellis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gigante_G/0/1/0/all/0/1"&gt;Guido Gigante&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_A/0/1/0/all/0/1"&gt;Andrew C. Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Giudice_P/0/1/0/all/0/1"&gt;Paolo Del Giudice&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vasilaki_E/0/1/0/all/0/1"&gt;Eleni Vasilaki&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[High-fidelity Prediction of Megapixel Longitudinal Phase-space Images of Electron Beams using Encoder-Decoder Neural Networks. (arXiv:2101.10437v4 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.10437</id>
        <link href="http://arxiv.org/abs/2101.10437"/>
        <updated>2021-08-03T02:06:34.138Z</updated>
        <summary type="html"><![CDATA[Modeling of large-scale research facilities is extremely challenging due to
complex physical processes and engineering problems. Here, we adopt a
data-driven approach to model the longitudinal phase-space diagnostic beamline
at the photoinector of the European XFEL with an encoder-decoder neural network
model. A deep convolutional neural network (decoder) is used to build images
measured on the screen from a small feature map generated by another neural
network (encoder). We demonstrate that the model trained only with experimental
data can make high-fidelity predictions of megapixel images for the
longitudinal phase-space measurement without any prior knowledge of
photoinjectors and electron beams. The prediction significantly outperforms
existing methods. We also show the scalability and interpretability of the
model by sharing the same decoder with more than one encoder used for different
setups of the photoinjector, and propose a pragmatic way to model a facility
with various diagnostics and working points. This opens the door to a new way
of accurately modeling a photoinjector using neural networks and experimental
data. The approach can possibly be extended to the whole accelerator and even
other types of scientific facilities.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1"&gt;Jun Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Ye Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brinker_F/0/1/0/all/0/1"&gt;Frank Brinker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Decking_W/0/1/0/all/0/1"&gt;Winfried Decking&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tomin_S/0/1/0/all/0/1"&gt;Sergey Tomin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schlarb_H/0/1/0/all/0/1"&gt;Holger Schlarb&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MFES-HB: Efficient Hyperband with Multi-Fidelity Quality Measurements. (arXiv:2012.03011v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.03011</id>
        <link href="http://arxiv.org/abs/2012.03011"/>
        <updated>2021-08-03T02:06:34.125Z</updated>
        <summary type="html"><![CDATA[Hyperparameter optimization (HPO) is a fundamental problem in automatic
machine learning (AutoML). However, due to the expensive evaluation cost of
models (e.g., training deep learning models or training models on large
datasets), vanilla Bayesian optimization (BO) is typically computationally
infeasible. To alleviate this issue, Hyperband (HB) utilizes the early stopping
mechanism to speed up configuration evaluations by terminating those
badly-performing configurations in advance. This leads to two kinds of quality
measurements: (1) many low-fidelity measurements for configurations that get
early-stopped, and (2) few high-fidelity measurements for configurations that
are evaluated without being early stopped. The state-of-the-art HB-style
method, BOHB, aims to combine the benefits of both BO and HB. Instead of
sampling configurations randomly in HB, BOHB samples configurations based on a
BO surrogate model, which is constructed with the high-fidelity measurements
only. However, the scarcity of high-fidelity measurements greatly hampers the
efficiency of BO to guide the configuration search. In this paper, we present
MFES-HB, an efficient Hyperband method that is capable of utilizing both the
high-fidelity and low-fidelity measurements to accelerate the convergence of
HPO tasks. Designing MFES-HB is not trivial as the low-fidelity measurements
can be biased yet informative to guide the configuration search. Thus we
propose to build a Multi- Fidelity Ensemble Surrogate (MFES) based on the
generalized Product of Experts framework, which can integrate useful
information from multi-fidelity measurements effectively. The empirical studies
on the real-world AutoML tasks demonstrate that MFES-HB can achieve 3.3-8.9x
speedups over the state-of-the-art approach - BOHB.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1"&gt;Yu Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1"&gt;Jiawei Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1"&gt;Jinyang Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Ce Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_B/0/1/0/all/0/1"&gt;Bin Cui&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[OPFython: A Python-Inspired Optimum-Path Forest Classifier. (arXiv:2001.10420v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2001.10420</id>
        <link href="http://arxiv.org/abs/2001.10420"/>
        <updated>2021-08-03T02:06:34.103Z</updated>
        <summary type="html"><![CDATA[Machine learning techniques have been paramount throughout the last years,
being applied in a wide range of tasks, such as classification, object
recognition, person identification, and image segmentation. Nevertheless,
conventional classification algorithms, e.g., Logistic Regression, Decision
Trees, and Bayesian classifiers, might lack complexity and diversity, not
suitable when dealing with real-world data. A recent graph-inspired classifier,
known as the Optimum-Path Forest, has proven to be a state-of-the-art
technique, comparable to Support Vector Machines and even surpassing it in some
tasks. This paper proposes a Python-based Optimum-Path Forest framework,
denoted as OPFython, where all of its functions and classes are based upon the
original C language implementation. Additionally, as OPFython is a Python-based
library, it provides a more friendly environment and a faster prototyping
workspace than the C language.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rosa_G/0/1/0/all/0/1"&gt;Gustavo Henrique de Rosa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Papa_J/0/1/0/all/0/1"&gt;Jo&amp;#xe3;o Paulo Papa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Falcao_A/0/1/0/all/0/1"&gt;Alexandre Xavier Falc&amp;#xe3;o&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The decomposition of the higher-order homology embedding constructed from the $k$-Laplacian. (arXiv:2107.10970v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.10970</id>
        <link href="http://arxiv.org/abs/2107.10970"/>
        <updated>2021-08-03T02:06:34.089Z</updated>
        <summary type="html"><![CDATA[The null space of the $k$-th order Laplacian $\mathbf{\mathcal L}_k$, known
as the {\em $k$-th homology vector space}, encodes the non-trivial topology of
a manifold or a network. Understanding the structure of the homology embedding
can thus disclose geometric or topological information from the data. The study
of the null space embedding of the graph Laplacian $\mathbf{\mathcal L}_0$ has
spurred new research and applications, such as spectral clustering algorithms
with theoretical guarantees and estimators of the Stochastic Block Model. In
this work, we investigate the geometry of the $k$-th homology embedding and
focus on cases reminiscent of spectral clustering. Namely, we analyze the {\em
connected sum} of manifolds as a perturbation to the direct sum of their
homology embeddings. We propose an algorithm to factorize the homology
embedding into subspaces corresponding to a manifold's simplest topological
components. The proposed framework is applied to the {\em shortest homologous
loop detection} problem, a problem known to be NP-hard in general. Our spectral
loop detection algorithm scales better than existing methods and is effective
on diverse data such as point clouds and images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yu-Chia Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Meila_M/0/1/0/all/0/1"&gt;Marina Meil&amp;#x103;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exact Pareto Optimal Search for Multi-Task Learning: Touring the Pareto Front. (arXiv:2108.00597v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.00597</id>
        <link href="http://arxiv.org/abs/2108.00597"/>
        <updated>2021-08-03T02:06:34.081Z</updated>
        <summary type="html"><![CDATA[Multi-Task Learning (MTL) is a well-established paradigm for training deep
neural network models for multiple correlated tasks. Often the task objectives
conflict, requiring trade-offs between them during model building. In such
cases, MTL models can use gradient-based multi-objective optimization (MOO) to
find one or more Pareto optimal solutions. A common requirement in MTL
applications is to find an {\it Exact} Pareto optimal (EPO) solution, which
satisfies user preferences with respect to task-specific objective functions.
Further, to improve model generalization, various constraints on the weights
may need to be enforced during training. Addressing these requirements is
challenging because it requires a search direction that allows descent not only
towards the Pareto front but also towards the input preference, within the
constraints imposed and in a manner that scales to high-dimensional gradients.
We design and theoretically analyze such search directions and develop the
first scalable algorithm, with theoretical guarantees of convergence, to find
an EPO solution, including when box and equality constraints are imposed. Our
unique method combines multiple gradient descent with carefully controlled
ascent to traverse the Pareto front in a principled manner, making it robust to
initialization. This also facilitates systematic exploration of the Pareto
front, that we utilize to approximate the Pareto front for multi-criteria
decision-making. Empirical results show that our algorithm outperforms
competing methods on benchmark MTL datasets and MOO problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mahapatra_D/0/1/0/all/0/1"&gt;Debabrata Mahapatra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rajan_V/0/1/0/all/0/1"&gt;Vaibhav Rajan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Synthesis of Compositional Animations from Textual Descriptions. (arXiv:2103.14675v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.14675</id>
        <link href="http://arxiv.org/abs/2103.14675"/>
        <updated>2021-08-03T02:06:34.075Z</updated>
        <summary type="html"><![CDATA["How can we animate 3D-characters from a movie script or move robots by
simply telling them what we would like them to do?" "How unstructured and
complex can we make a sentence and still generate plausible movements from it?"
These are questions that need to be answered in the long-run, as the field is
still in its infancy. Inspired by these problems, we present a new technique
for generating compositional actions, which handles complex input sentences.
Our output is a 3D pose sequence depicting the actions in the input sentence.
We propose a hierarchical two-stream sequential model to explore a finer
joint-level mapping between natural language sentences and 3D pose sequences
corresponding to the given motion. We learn two manifold representations of the
motion -- one each for the upper body and the lower body movements. Our model
can generate plausible pose sequences for short sentences describing single
actions as well as long compositional sentences describing multiple sequential
and superimposed actions. We evaluate our proposed model on the publicly
available KIT Motion-Language Dataset containing 3D pose data with
human-annotated sentences. Experimental results show that our model advances
the state-of-the-art on text-based motion synthesis in objective evaluations by
a margin of 50%. Qualitative evaluations based on a user study indicate that
our synthesized motions are perceived to be the closest to the ground-truth
motion captures for both short and compositional sentences.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ghosh_A/0/1/0/all/0/1"&gt;Anindita Ghosh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheema_N/0/1/0/all/0/1"&gt;Noshaba Cheema&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oguz_C/0/1/0/all/0/1"&gt;Cennet Oguz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1"&gt;Christian Theobalt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Slusallek_P/0/1/0/all/0/1"&gt;Philipp Slusallek&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning transport cost from subset correspondence. (arXiv:1909.13203v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1909.13203</id>
        <link href="http://arxiv.org/abs/1909.13203"/>
        <updated>2021-08-03T02:06:34.064Z</updated>
        <summary type="html"><![CDATA[Learning to align multiple datasets is an important problem with many
applications, and it is especially useful when we need to integrate multiple
experiments or correct for confounding. Optimal transport (OT) is a principled
approach to align datasets, but a key challenge in applying OT is that we need
to specify a transport cost function that accurately captures how the two
datasets are related. Reliable cost functions are typically not available and
practitioners often resort to using hand-crafted or Euclidean cost even if it
may not be appropriate. In this work, we investigate how to learn the cost
function using a small amount of side information which is often available. The
side information we consider captures subset correspondence -- i.e. certain
subsets of points in the two data sets are known to be related. For example, we
may have some images labeled as cars in both datasets; or we may have a common
annotated cell type in single-cell data from two batches. We develop an
end-to-end optimizer (OT-SI) that differentiates through the Sinkhorn algorithm
and effectively learns the suitable cost function from side information. On
systematic experiments in images, marriage-matching and single-cell RNA-seq,
our method substantially outperform state-of-the-art benchmarks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1"&gt;Ruishan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balsubramani_A/0/1/0/all/0/1"&gt;Akshay Balsubramani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1"&gt;James Zou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The principle of weight divergence facilitation for unsupervised pattern recognition in spiking neural networks. (arXiv:2104.09943v2 [q-bio.NC] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.09943</id>
        <link href="http://arxiv.org/abs/2104.09943"/>
        <updated>2021-08-03T02:06:34.039Z</updated>
        <summary type="html"><![CDATA[Parallels between the signal processing tasks and biological neurons lead to
an understanding of the principles of self-organized optimization of input
signal recognition. In the present paper, we discuss such similarities among
biological and technical systems. We propose adding the well-known STDP
synaptic plasticity rule to direct the weight modification towards the state
associated with the maximal difference between background noise and correlated
signals. We use the principle of physically constrained weight growth as a
basis for such weights' modification control. It is proposed that the existence
and production of bio-chemical 'substances' needed for plasticity development
restrict a biological synaptic straight modification. In this paper, the
information about the noise-to-signal ratio controls such a substances'
production and storage and drives the neuron's synaptic pressures towards the
state with the best signal-to-noise ratio. We consider several experiments with
different input signal regimes to understand the functioning of the proposed
approach.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-bio/1/au:+Nikitin_O/0/1/0/all/0/1"&gt;Oleg Nikitin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Lukyanova_O/0/1/0/all/0/1"&gt;Olga Lukyanova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-bio/1/au:+Kunin_A/0/1/0/all/0/1"&gt;Alex Kunin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Kernel Thinning. (arXiv:2105.05842v4 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.05842</id>
        <link href="http://arxiv.org/abs/2105.05842"/>
        <updated>2021-08-03T02:06:34.027Z</updated>
        <summary type="html"><![CDATA[We introduce kernel thinning, a new procedure for compressing a distribution
$\mathbb{P}$ more effectively than i.i.d. sampling or standard thinning. Given
a suitable reproducing kernel $\mathbf{k}$ and $\mathcal{O}(n^2)$ time, kernel
thinning compresses an $n$-point approximation to $\mathbb{P}$ into a
$\sqrt{n}$-point approximation with comparable worst-case integration error
across the associated reproducing kernel Hilbert space. With high probability,
the maximum discrepancy in integration error is
$\mathcal{O}_d(n^{-\frac{1}{2}}\sqrt{\log n})$ for compactly supported
$\mathbb{P}$ and $\mathcal{O}_d(n^{-\frac{1}{2}} \sqrt{(\log n)^{d+1}\log\log
n})$ for sub-exponential $\mathbb{P}$ on $\mathbb{R}^d$. In contrast, an
equal-sized i.i.d. sample from $\mathbb{P}$ suffers $\Omega(n^{-\frac14})$
integration error. Our sub-exponential guarantees resemble the classical
quasi-Monte Carlo error rates for uniform $\mathbb{P}$ on $[0,1]^d$ but apply
to general distributions on $\mathbb{R}^d$ and a wide range of common kernels.
We use our results to derive explicit non-asymptotic maximum mean discrepancy
bounds for Gaussian, Mat\'ern, and B-spline kernels and present two vignettes
illustrating the practical benefits of kernel thinning over i.i.d. sampling and
standard Markov chain Monte Carlo thinning, in dimensions $d=2$ through $100$.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Dwivedi_R/0/1/0/all/0/1"&gt;Raaz Dwivedi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Mackey_L/0/1/0/all/0/1"&gt;Lester Mackey&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Few-shot calibration of low-cost air pollution (PM2.5) sensors using meta-learning. (arXiv:2108.00640v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.00640</id>
        <link href="http://arxiv.org/abs/2108.00640"/>
        <updated>2021-08-03T02:06:34.021Z</updated>
        <summary type="html"><![CDATA[Low-cost particulate matter sensors are transforming air quality monitoring
because they have lower costs and greater mobility as compared to reference
monitors. Calibration of these low-cost sensors requires training data from
co-deployed reference monitors. Machine Learning based calibration gives better
performance than conventional techniques, but requires a large amount of
training data from the sensor, to be calibrated, co-deployed with a reference
monitor. In this work, we propose novel transfer learning methods for quick
calibration of sensors with minimal co-deployment with reference monitors.
Transfer learning utilizes a large amount of data from other sensors along with
a limited amount of data from the target sensor. Our extensive experimentation
finds the proposed Model-Agnostic- Meta-Learning (MAML) based transfer learning
method to be the most effective over other competitive baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yadav_K/0/1/0/all/0/1"&gt;Kalpit Yadav&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arora_V/0/1/0/all/0/1"&gt;Vipul Arora&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jha_S/0/1/0/all/0/1"&gt;Sonu Kumar Jha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_M/0/1/0/all/0/1"&gt;Mohit Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tripathi_S/0/1/0/all/0/1"&gt;Sachchida Nand Tripathi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Making Deep Learning-based Vulnerability Detectors Robust. (arXiv:2108.00669v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2108.00669</id>
        <link href="http://arxiv.org/abs/2108.00669"/>
        <updated>2021-08-03T02:06:33.989Z</updated>
        <summary type="html"><![CDATA[Automatically detecting software vulnerabilities in source code is an
important problem that has attracted much attention. In particular, deep
learning-based vulnerability detectors, or DL-based detectors, are attractive
because they do not need human experts to define features or patterns of
vulnerabilities. However, such detectors' robustness is unclear. In this paper,
we initiate the study in this aspect by demonstrating that DL-based detectors
are not robust against simple code transformations, dubbed attacks in this
paper, as these transformations may be leveraged for malicious purposes. As a
first step towards making DL-based detectors robust against such attacks, we
propose an innovative framework, dubbed ZigZag, which is centered at (i)
decoupling feature learning and classifier learning and (ii) using a
ZigZag-style strategy to iteratively refine them until they converge to robust
features and robust classifiers. Experimental results show that the ZigZag
framework can substantially improve the robustness of DL-based detectors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1"&gt;Jing Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zou_D/0/1/0/all/0/1"&gt;Deqing Zou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1"&gt;Qian Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1"&gt;Shouhuai Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yichen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1"&gt;Hai Jin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cohort Bias Adaptation in Aggregated Datasets for Lesion Segmentation. (arXiv:2108.00713v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2108.00713</id>
        <link href="http://arxiv.org/abs/2108.00713"/>
        <updated>2021-08-03T02:06:33.974Z</updated>
        <summary type="html"><![CDATA[Many automatic machine learning models developed for focal pathology (e.g.
lesions, tumours) detection and segmentation perform well, but do not
generalize as well to new patient cohorts, impeding their widespread adoption
into real clinical contexts. One strategy to create a more diverse,
generalizable training set is to naively pool datasets from different cohorts.
Surprisingly, training on this \it{big data} does not necessarily increase, and
may even reduce, overall performance and model generalizability, due to the
existence of cohort biases that affect label distributions. In this paper, we
propose a generalized affine conditioning framework to learn and account for
cohort biases across multi-source datasets, which we call Source-Conditioned
Instance Normalization (SCIN). Through extensive experimentation on three
different, large scale, multi-scanner, multi-centre Multiple Sclerosis (MS)
clinical trial MRI datasets, we show that our cohort bias adaptation method (1)
improves performance of the network on pooled datasets relative to naively
pooling datasets and (2) can quickly adapt to a new cohort by fine-tuning the
instance normalization parameters, thus learning the new cohort bias with only
10 labelled samples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Nichyporuk_B/0/1/0/all/0/1"&gt;Brennan Nichyporuk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cardinell_J/0/1/0/all/0/1"&gt;Jillian Cardinell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Szeto_J/0/1/0/all/0/1"&gt;Justin Szeto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mehta_R/0/1/0/all/0/1"&gt;Raghav Mehta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tsaftaris_S/0/1/0/all/0/1"&gt;Sotirios Tsaftaris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Arnold_D/0/1/0/all/0/1"&gt;Douglas L. Arnold&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Arbel_T/0/1/0/all/0/1"&gt;Tal Arbel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semi-Supervising Learning, Transfer Learning, and Knowledge Distillation with SimCLR. (arXiv:2108.00587v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00587</id>
        <link href="http://arxiv.org/abs/2108.00587"/>
        <updated>2021-08-03T02:06:33.969Z</updated>
        <summary type="html"><![CDATA[Recent breakthroughs in the field of semi-supervised learning have achieved
results that match state-of-the-art traditional supervised learning methods.
Most successful semi-supervised learning approaches in computer vision focus on
leveraging huge amount of unlabeled data, learning the general representation
via data augmentation and transformation, creating pseudo labels, implementing
different loss functions, and eventually transferring this knowledge to more
task-specific smaller models. In this paper, we aim to conduct our analyses on
three different aspects of SimCLR, the current state-of-the-art semi-supervised
learning framework for computer vision. First, we analyze properties of
contrast learning on fine-tuning, as we understand that contrast learning is
what makes this method so successful. Second, we research knowledge
distillation through teacher-forcing paradigm. We observe that when the teacher
and the student share the same base model, knowledge distillation will achieve
better result. Finally, we study how transfer learning works and its
relationship with the number of classes on different data sets. Our results
indicate that transfer learning performs better when number of classes are
smaller.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1"&gt;Khoi Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_Y/0/1/0/all/0/1"&gt;Yen Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Le_B/0/1/0/all/0/1"&gt;Bao Le&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[End-to-end Robustness for Sensing-Reasoning Machine Learning Pipelines. (arXiv:2003.00120v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.00120</id>
        <link href="http://arxiv.org/abs/2003.00120"/>
        <updated>2021-08-03T02:06:33.948Z</updated>
        <summary type="html"><![CDATA[As machine learning (ML) being applied to many mission-critical scenarios,
certifying ML model robustness becomes increasingly important. Many previous
works focuses on the robustness of independent ML and ensemble models, and can
only certify a very small magnitude of the adversarial perturbation. In this
paper, we take a different viewpoint and improve learning robustness by going
beyond independent ML and ensemble models. We aim at promoting the generic
Sensing-Reasoning machine learning pipeline which contains both the sensing
(e.g. deep neural networks) and reasoning (e.g. Markov logic networks (MLN))
components enriched with domain knowledge. Can domain knowledge help improve
learning robustness? Can we formally certify the end-to-end robustness of such
an ML pipeline? We first theoretically analyze the computational complexity of
checking the provable robustness in the reasoning component. We then derive the
provable robustness bound for several concrete reasoning components. We show
that for reasoning components such as MLN and a specific family of Bayesian
networks it is possible to certify the robustness of the whole pipeline even
with a large magnitude of perturbation which cannot be certified by existing
work. Finally, we conduct extensive real-world experiments on large scale
datasets to evaluate the certified robustness for Sensing-Reasoning ML
pipelines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Zhuolin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1"&gt;Zhikuan Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pei_H/0/1/0/all/0/1"&gt;Hengzhi Pei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1"&gt;Boxin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karlas_B/0/1/0/all/0/1"&gt;Bojan Karlas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Ji Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1"&gt;Heng Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bo Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Ce Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learn to Forget: Machine Unlearning via Neuron Masking. (arXiv:2003.10933v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2003.10933</id>
        <link href="http://arxiv.org/abs/2003.10933"/>
        <updated>2021-08-03T02:06:33.942Z</updated>
        <summary type="html"><![CDATA[Nowadays, machine learning models, especially neural networks, become
prevalent in many real-world applications.These models are trained based on a
one-way trip from user data: as long as users contribute their data, there is
no way to withdraw; and it is well-known that a neural network memorizes its
training data. This contradicts the "right to be forgotten" clause of GDPR,
potentially leading to law violations. To this end, machine unlearning becomes
a popular research topic, which allows users to eliminate memorization of their
private data from a trained machine learning model.In this paper, we propose
the first uniform metric called for-getting rate to measure the effectiveness
of a machine unlearning method. It is based on the concept of membership
inference and describes the transformation rate of the eliminated data from
"memorized" to "unknown" after conducting unlearning. We also propose a novel
unlearning method calledForsaken. It is superior to previous work in either
utility or efficiency (when achieving the same forgetting rate). We benchmark
Forsaken with eight standard datasets to evaluate its performance. The
experimental results show that it can achieve more than 90\% forgetting rate on
average and only causeless than 5\% accuracy loss.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1"&gt;Zhuo Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Ximeng Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jian Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1"&gt;Zhongyuan Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1"&gt;Jianfeng Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1"&gt;Philip Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_K/0/1/0/all/0/1"&gt;Kui Ren&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Causal Inference in Educational Systems: A Graphical Modeling Approach. (arXiv:2108.00654v1 [stat.AP])]]></title>
        <id>http://arxiv.org/abs/2108.00654</id>
        <link href="http://arxiv.org/abs/2108.00654"/>
        <updated>2021-08-03T02:06:33.934Z</updated>
        <summary type="html"><![CDATA[Educational systems have traditionally been evaluated using cross-sectional
studies, namely, examining a pretest, posttest, and single intervention.
Although this is a popular approach, it does not model valuable information
such as confounding variables, feedback to students, and other real-world
deviations of studies from ideal conditions. Moreover, learning inherently is a
sequential process and should involve a sequence of interventions. In this
paper, we propose various experimental and quasi-experimental designs for
educational systems and quantify them using the graphical model and directed
acyclic graph (DAG) language. We discuss the applications and limitations of
each method in education. Furthermore, we propose to model the education system
as time-varying treatments, confounders, and time-varying
treatments-confounders feedback. We show that if we control for a sufficient
set of confounders and use appropriate inference techniques such as the inverse
probability of treatment weighting (IPTW) or g-formula, we can close the
backdoor paths and derive the unbiased causal estimate of joint interventions
on the outcome. Finally, we compare the g-formula and IPTW performance and
discuss the pros and cons of using each method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Tadayon_M/0/1/0/all/0/1"&gt;Manie Tadayon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Pottie_G/0/1/0/all/0/1"&gt;Greg Pottie&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Group Fisher Pruning for Practical Network Compression. (arXiv:2108.00708v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00708</id>
        <link href="http://arxiv.org/abs/2108.00708"/>
        <updated>2021-08-03T02:06:33.928Z</updated>
        <summary type="html"><![CDATA[Network compression has been widely studied since it is able to reduce the
memory and computation cost during inference. However, previous methods seldom
deal with complicated structures like residual connections, group/depth-wise
convolution and feature pyramid network, where channels of multiple layers are
coupled and need to be pruned simultaneously. In this paper, we present a
general channel pruning approach that can be applied to various complicated
structures. Particularly, we propose a layer grouping algorithm to find coupled
channels automatically. Then we derive a unified metric based on Fisher
information to evaluate the importance of a single channel and coupled
channels. Moreover, we find that inference speedup on GPUs is more correlated
with the reduction of memory rather than FLOPs, and thus we employ the memory
reduction of each channel to normalize the importance. Our method can be used
to prune any structures including those with coupled channels. We conduct
extensive experiments on various backbones, including the classic ResNet and
ResNeXt, mobile-friendly MobileNetV2, and the NAS-based RegNet, both on image
classification and object detection which is under-explored. Experimental
results validate that our method can effectively prune sophisticated networks,
boosting inference speed without sacrificing accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Liyang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shilong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kuang_Z/0/1/0/all/0/1"&gt;Zhanghui Kuang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_A/0/1/0/all/0/1"&gt;Aojun Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xue_J/0/1/0/all/0/1"&gt;Jing-Hao Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xinjiang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yimin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1"&gt;Wenming Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liao_Q/0/1/0/all/0/1"&gt;Qingmin Liao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Wayne Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Sample Efficient Policy Gradient Methods with Recursive Variance Reduction. (arXiv:1909.08610v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1909.08610</id>
        <link href="http://arxiv.org/abs/1909.08610"/>
        <updated>2021-08-03T02:06:33.907Z</updated>
        <summary type="html"><![CDATA[Improving the sample efficiency in reinforcement learning has been a
long-standing research problem. In this work, we aim to reduce the sample
complexity of existing policy gradient methods. We propose a novel policy
gradient algorithm called SRVR-PG, which only requires $O(1/\epsilon^{3/2})$
episodes to find an $\epsilon$-approximate stationary point of the nonconcave
performance function $J(\boldsymbol{\theta})$ (i.e., $\boldsymbol{\theta}$ such
that $\|\nabla J(\boldsymbol{\theta})\|_2^2\leq\epsilon$). This sample
complexity improves the existing result $O(1/\epsilon^{5/3})$ for stochastic
variance reduced policy gradient algorithms by a factor of
$O(1/\epsilon^{1/6})$. In addition, we also propose a variant of SRVR-PG with
parameter exploration, which explores the initial policy parameter from a prior
probability distribution. We conduct numerical experiments on classic control
problems in reinforcement learning to validate the performance of our proposed
algorithms.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1"&gt;Pan Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_F/0/1/0/all/0/1"&gt;Felicia Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1"&gt;Quanquan Gu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Information Stealing in Federated Learning Systems Based on Generative Adversarial Networks. (arXiv:2108.00701v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.00701</id>
        <link href="http://arxiv.org/abs/2108.00701"/>
        <updated>2021-08-03T02:06:33.900Z</updated>
        <summary type="html"><![CDATA[An attack on deep learning systems where intelligent machines collaborate to
solve problems could cause a node in the network to make a mistake on a
critical judgment. At the same time, the security and privacy concerns of AI
have galvanized the attention of experts from multiple disciplines. In this
research, we successfully mounted adversarial attacks on a federated learning
(FL) environment using three different datasets. The attacks leveraged
generative adversarial networks (GANs) to affect the learning process and
strive to reconstruct the private data of users by learning hidden features
from shared local model parameters. The attack was target-oriented drawing data
with distinct class distribution from the CIFAR- 10, MNIST, and Fashion-MNIST
respectively. Moreover, by measuring the Euclidean distance between the real
data and the reconstructed adversarial samples, we evaluated the performance of
the adversary in the learning processes in various scenarios. At last, we
successfully reconstructed the real data of the victim from the shared global
model parameters with all the applied datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1"&gt;Yuwei Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chong_N/0/1/0/all/0/1"&gt;Ng Chong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ochiai_H/0/1/0/all/0/1"&gt;Hideya Ochiai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Learn to Demodulate with Uncertainty Quantification via Bayesian Meta-Learning. (arXiv:2108.00785v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.00785</id>
        <link href="http://arxiv.org/abs/2108.00785"/>
        <updated>2021-08-03T02:06:33.888Z</updated>
        <summary type="html"><![CDATA[Meta-learning, or learning to learn, offers a principled framework for
few-shot learning. It leverages data from multiple related learning tasks to
infer an inductive bias that enables fast adaptation on a new task. The
application of meta-learning was recently proposed for learning how to
demodulate from few pilots. The idea is to use pilots received and stored for
offline use from multiple devices in order to meta-learn an adaptation
procedure with the aim of speeding up online training on new devices. Standard
frequentist learning, which can yield relatively accurate "hard" classification
decisions, is known to be poorly calibrated, particularly in the small-data
regime. Poor calibration implies that the soft scores output by the demodulator
are inaccurate estimates of the true probability of correct demodulation. In
this work, we introduce the use of Bayesian meta-learning via variational
inference for the purpose of obtaining well-calibrated few-pilot demodulators.
In a Bayesian framework, each neural network weight is represented by a
distribution, capturing epistemic uncertainty. Bayesian meta-learning optimizes
over the prior distribution of the weights. The resulting Bayesian ensembles
offer better calibrated soft decisions, at the computational cost of running
multiple instances of the neural network for demodulation. Numerical results
for single-input single-output Rayleigh fading channels with transmitter's
non-linearities are provided that compare symbol error rate and expected
calibration error for both frequentist and Bayesian meta-learning, illustrating
how the latter is both more accurate and better-calibrated.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cohen_K/0/1/0/all/0/1"&gt;Kfir M. Cohen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1"&gt;Sangwoo Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Simeone_O/0/1/0/all/0/1"&gt;Osvaldo Simeone&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shamai_S/0/1/0/all/0/1"&gt;Shlomo Shamai&lt;/a&gt; (Shitz)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Ab-initio experimental violation of Bell inequalities. (arXiv:2108.00574v1 [quant-ph])]]></title>
        <id>http://arxiv.org/abs/2108.00574</id>
        <link href="http://arxiv.org/abs/2108.00574"/>
        <updated>2021-08-03T02:06:33.881Z</updated>
        <summary type="html"><![CDATA[The violation of a Bell inequality is the paradigmatic example of
device-independent quantum information: the nonclassicality of the data is
certified without the knowledge of the functioning of devices. In practice,
however, all Bell experiments rely on the precise understanding of the
underlying physical mechanisms. Given that, it is natural to ask: Can one
witness nonclassical behaviour in a truly black-box scenario? Here we propose
and implement, computationally and experimentally, a solution to this ab-initio
task. It exploits a robust automated optimization approach based on the
Stochastic Nelder-Mead algorithm. Treating preparation and measurement devices
as black-boxes, and relying on the observed statistics only, our adaptive
protocol approaches the optimal Bell inequality violation after a limited
number of iterations for a variety photonic states, measurement responses and
Bell scenarios. In particular, we exploit it for randomness certification from
unknown states and measurements. Our results demonstrate the power of automated
algorithms, opening a new venue for the experimental implementation of
device-independent quantum technologies.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Poderini_D/0/1/0/all/0/1"&gt;Davide Poderini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Polino_E/0/1/0/all/0/1"&gt;Emanuele Polino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Rodari_G/0/1/0/all/0/1"&gt;Giovanni Rodari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Suprano_A/0/1/0/all/0/1"&gt;Alessia Suprano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Chaves_R/0/1/0/all/0/1"&gt;Rafael Chaves&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Sciarrino_F/0/1/0/all/0/1"&gt;Fabio Sciarrino&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Piecewise Linear Units Improve Deep Neural Networks. (arXiv:2108.00700v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.00700</id>
        <link href="http://arxiv.org/abs/2108.00700"/>
        <updated>2021-08-03T02:06:33.875Z</updated>
        <summary type="html"><![CDATA[The activation function is at the heart of a deep neural networks
nonlinearity; the choice of the function has great impact on the success of
training. Currently, many practitioners prefer the Rectified Linear Unit (ReLU)
due to its simplicity and reliability, despite its few drawbacks. While most
previous functions proposed to supplant ReLU have been hand-designed, recent
work on learning the function during training has shown promising results. In
this paper we propose an adaptive piecewise linear activation function, the
Piecewise Linear Unit (PiLU), which can be learned independently for each
dimension of the neural network. We demonstrate how PiLU is a generalised
rectifier unit and note its similarities with the Adaptive Piecewise Linear
Units, namely adaptive and piecewise linear. Across a distribution of 30
experiments, we show that for the same model architecture, hyperparameters, and
pre-processing, PiLU significantly outperforms ReLU: reducing classification
error by 18.53% on CIFAR-10 and 13.13% on CIFAR-100, for a minor increase in
the number of neurons. Further work should be dedicated to exploring
generalised piecewise linear units, as well as verifying these results across
other challenging domains and larger problems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Inturrisi_J/0/1/0/all/0/1"&gt;Jordan Inturrisi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khoo_S/0/1/0/all/0/1"&gt;Sui Yang Khoo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kouzani_A/0/1/0/all/0/1"&gt;Abbas Kouzani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pagliarella_R/0/1/0/all/0/1"&gt;Riccardo Pagliarella&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hierarchical Indian Buffet Neural Networks for Bayesian Continual Learning. (arXiv:1912.02290v5 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1912.02290</id>
        <link href="http://arxiv.org/abs/1912.02290"/>
        <updated>2021-08-03T02:06:33.856Z</updated>
        <summary type="html"><![CDATA[We place an Indian Buffet process (IBP) prior over the structure of a
Bayesian Neural Network (BNN), thus allowing the complexity of the BNN to
increase and decrease automatically. We further extend this model such that the
prior on the structure of each hidden layer is shared globally across all
layers, using a Hierarchical-IBP (H-IBP). We apply this model to the problem of
resource allocation in Continual Learning (CL) where new tasks occur and the
network requires extra resources. Our model uses online variational inference
with reparameterisation of the Bernoulli and Beta distributions, which
constitute the IBP and H-IBP priors. As we automatically learn the number of
weights in each layer of the BNN, overfitting and underfitting problems are
largely overcome. We show empirically that our approach offers a competitive
edge over existing methods in CL.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Kessler_S/0/1/0/all/0/1"&gt;Samuel Kessler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Nguyen_V/0/1/0/all/0/1"&gt;Vu Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Zohren_S/0/1/0/all/0/1"&gt;Stefan Zohren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Roberts_S/0/1/0/all/0/1"&gt;Stephen Roberts&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Can you tell? SSNet -- a Sagittal Stratum-inspired Neural Network Framework for Sentiment Analysis. (arXiv:2006.12958v4 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.12958</id>
        <link href="http://arxiv.org/abs/2006.12958"/>
        <updated>2021-08-03T02:06:33.850Z</updated>
        <summary type="html"><![CDATA[When people try to understand nuanced language they typically process
multiple input sensor modalities to complete this cognitive task. It turns out
the human brain has even a specialized neuron formation, called sagittal
stratum, to help us understand sarcasm. We use this biological formation as the
inspiration for designing a neural network architecture that combines
predictions of different models on the same text to construct robust, accurate
and computationally efficient classifiers for sentiment analysis and study
several different realizations. Among them, we propose a systematic new
approach to combining multiple predictions based on a dedicated neural network
and develop mathematical analysis of it along with state-of-the-art
experimental results. We also propose a heuristic-hybrid technique for
combining models and back it up with experimental results on a representative
benchmark dataset and comparisons to other methods to show the advantages of
the new approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vassilev_A/0/1/0/all/0/1"&gt;Apostol Vassilev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hasan_M/0/1/0/all/0/1"&gt;Munawar Hasan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1"&gt;Honglan Jin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multiplicative updates for symmetric-cone factorizations. (arXiv:2108.00740v1 [math.OC])]]></title>
        <id>http://arxiv.org/abs/2108.00740</id>
        <link href="http://arxiv.org/abs/2108.00740"/>
        <updated>2021-08-03T02:06:33.816Z</updated>
        <summary type="html"><![CDATA[Given a matrix $X\in \mathbb{R}^{m\times n}_+$ with non-negative entries, the
cone factorization problem over a cone $\mathcal{K}\subseteq \mathbb{R}^k$
concerns computing $\{ a_1,\ldots, a_{m} \} \subseteq \mathcal{K}$ and $\{
b_1,\ldots, b_{n} \} \subseteq~\mathcal{K}^*$ belonging to its dual so that
$X_{ij} = \langle a_i, b_j \rangle$ for all $i\in [m], j\in [n]$. Cone
factorizations are fundamental to mathematical optimization as they allow us to
express convex bodies as feasible regions of linear conic programs. In this
paper, we introduce and analyze the symmetric-cone multiplicative update (SCMU)
algorithm for computing cone factorizations when $\mathcal{K}$ is symmetric;
i.e., it is self-dual and homogeneous. Symmetric cones are of central interest
in mathematical optimization as they provide a common language for studying
linear optimization over the nonnegative orthant (linear programs), over the
second-order cone (second order cone programs), and over the cone of positive
semidefinite matrices (semidefinite programs). The SCMU algorithm is
multiplicative in the sense that the iterates are updated by applying a
meticulously chosen automorphism of the cone computed using a generalization of
the geometric mean to symmetric cones. Using an extension of Lieb's concavity
theorem and von Neumann's trace inequality to symmetric cones, we show that the
squared loss objective is non-decreasing along the trajectories of the SCMU
algorithm. Specialized to the nonnegative orthant, the SCMU algorithm
corresponds to the seminal algorithm by Lee and Seung for computing Nonnegative
Matrix Factorizations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Soh_Y/0/1/0/all/0/1"&gt;Yong Sheng Soh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Varvitsiotis_A/0/1/0/all/0/1"&gt;Antonios Varvitsiotis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Statistically Robust Neural Network Classification. (arXiv:1912.04884v3 [stat.ML] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1912.04884</id>
        <link href="http://arxiv.org/abs/1912.04884"/>
        <updated>2021-08-03T02:06:33.811Z</updated>
        <summary type="html"><![CDATA[Despite their numerous successes, there are many scenarios where adversarial
risk metrics do not provide an appropriate measure of robustness. For example,
test-time perturbations may occur in a probabilistic manner rather than being
generated by an explicit adversary, while the poor train--test generalization
of adversarial metrics can limit their usage to simple problems. Motivated by
this, we develop a probabilistic robust risk framework, the statistically
robust risk (SRR), which considers pointwise corruption distributions, as
opposed to worst-case adversaries. The SRR provides a distinct and
complementary measure of robust performance, compared to natural and
adversarial risk. We show that the SRR admits estimation and training schemes
which are as simple and efficient as for the natural risk: these simply require
noising the inputs, but with a principled derivation for exactly how and why
this should be done. Furthermore, we demonstrate both theoretically and
experimentally that it can provide superior generalization performance compared
with adversarial risks, enabling application to high-dimensional datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Wang_B/0/1/0/all/0/1"&gt;Benjie Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Webb_S/0/1/0/all/0/1"&gt;Stefan Webb&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Rainforth_T/0/1/0/all/0/1"&gt;Tom Rainforth&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FLASH: Fast Neural Architecture Search with Hardware Optimization. (arXiv:2108.00568v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00568</id>
        <link href="http://arxiv.org/abs/2108.00568"/>
        <updated>2021-08-03T02:06:33.799Z</updated>
        <summary type="html"><![CDATA[Neural architecture search (NAS) is a promising technique to design efficient
and high-performance deep neural networks (DNNs). As the performance
requirements of ML applications grow continuously, the hardware accelerators
start playing a central role in DNN design. This trend makes NAS even more
complicated and time-consuming for most real applications. This paper proposes
FLASH, a very fast NAS methodology that co-optimizes the DNN accuracy and
performance on a real hardware platform. As the main theoretical contribution,
we first propose the NN-Degree, an analytical metric to quantify the
topological characteristics of DNNs with skip connections (e.g., DenseNets,
ResNets, Wide-ResNets, and MobileNets). The newly proposed NN-Degree allows us
to do training-free NAS within one second and build an accuracy predictor by
training as few as 25 samples out of a vast search space with more than 63
billion configurations. Second, by performing inference on the target hardware,
we fine-tune and validate our analytical models to estimate the latency, area,
and energy consumption of various DNN architectures while executing standard ML
datasets. Third, we construct a hierarchical algorithm based on simplicial
homology global optimization (SHGO) to optimize the model-architecture
co-design process, while considering the area, latency, and energy consumption
of the target hardware. We demonstrate that, compared to the state-of-the-art
NAS approaches, our proposed hierarchical SHGO-based algorithm enables more
than four orders of magnitude speedup (specifically, the execution time of the
proposed algorithm is about 0.1 seconds). Finally, our experimental evaluations
show that FLASH is easily transferable to different hardware architectures,
thus enabling us to do NAS on a Raspberry Pi-3B processor in less than 3
seconds.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1"&gt;Guihong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mandal_S/0/1/0/all/0/1"&gt;Sumit K. Mandal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ogras_U/0/1/0/all/0/1"&gt;Umit Y. Ogras&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marculescu_R/0/1/0/all/0/1"&gt;Radu Marculescu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Grain: Improving Data Efficiency of Graph Neural Networks via Diversified Influence Maximization. (arXiv:2108.00219v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.00219</id>
        <link href="http://arxiv.org/abs/2108.00219"/>
        <updated>2021-08-03T02:06:33.771Z</updated>
        <summary type="html"><![CDATA[Data selection methods, such as active learning and core-set selection, are
useful tools for improving the data efficiency of deep learning models on
large-scale datasets. However, recent deep learning models have moved forward
from independent and identically distributed data to graph-structured data,
such as social networks, e-commerce user-item graphs, and knowledge graphs.
This evolution has led to the emergence of Graph Neural Networks (GNNs) that go
beyond the models existing data selection methods are designed for. Therefore,
we present Grain, an efficient framework that opens up a new perspective
through connecting data selection in GNNs with social influence maximization.
By exploiting the common patterns of GNNs, Grain introduces a novel feature
propagation concept, a diversified influence maximization objective with novel
influence and diversity functions, and a greedy algorithm with an approximation
guarantee into a unified framework. Empirical studies on public datasets
demonstrate that Grain significantly improves both the performance and
efficiency of data selection (including active learning and core-set selection)
for GNNs. To the best of our knowledge, this is the first attempt to bridge two
largely parallel threads of research, data selection, and social influence
maximization, in the setting of GNNs, paving new ways for improving data
efficiency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Wentao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Zhi Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yexin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1"&gt;Yu Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Liang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cui_B/0/1/0/all/0/1"&gt;Bin Cui&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ECLARE: Extreme Classification with Label Graph Correlations. (arXiv:2108.00261v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.00261</id>
        <link href="http://arxiv.org/abs/2108.00261"/>
        <updated>2021-08-03T02:06:33.753Z</updated>
        <summary type="html"><![CDATA[Deep extreme classification (XC) seeks to train deep architectures that can
tag a data point with its most relevant subset of labels from an extremely
large label set. The core utility of XC comes from predicting labels that are
rarely seen during training. Such rare labels hold the key to personalized
recommendations that can delight and surprise a user. However, the large number
of rare labels and small amount of training data per rare label offer
significant statistical and computational challenges. State-of-the-art deep XC
methods attempt to remedy this by incorporating textual descriptions of labels
but do not adequately address the problem. This paper presents ECLARE, a
scalable deep learning architecture that incorporates not only label text, but
also label correlations, to offer accurate real-time predictions within a few
milliseconds. Core contributions of ECLARE include a frugal architecture and
scalable techniques to train deep models along with label correlation graphs at
the scale of millions of labels. In particular, ECLARE offers predictions that
are 2 to 14% more accurate on both publicly available benchmark datasets as
well as proprietary datasets for a related products recommendation task sourced
from the Bing search engine. Code for ECLARE is available at
https://github.com/Extreme-classification/ECLARE.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mittal_A/0/1/0/all/0/1"&gt;Anshul Mittal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sachdeva_N/0/1/0/all/0/1"&gt;Noveen Sachdeva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agrawal_S/0/1/0/all/0/1"&gt;Sheshansh Agrawal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1"&gt;Sumeet Agarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kar_P/0/1/0/all/0/1"&gt;Purushottam Kar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Varma_M/0/1/0/all/0/1"&gt;Manik Varma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Chest ImaGenome Dataset for Clinical Reasoning. (arXiv:2108.00316v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00316</id>
        <link href="http://arxiv.org/abs/2108.00316"/>
        <updated>2021-08-03T02:06:33.744Z</updated>
        <summary type="html"><![CDATA[Despite the progress in automatic detection of radiologic findings from chest
X-ray (CXR) images in recent years, a quantitative evaluation of the
explainability of these models is hampered by the lack of locally labeled
datasets for different findings. With the exception of a few expert-labeled
small-scale datasets for specific findings, such as pneumonia and pneumothorax,
most of the CXR deep learning models to date are trained on global "weak"
labels extracted from text reports, or trained via a joint image and
unstructured text learning strategy. Inspired by the Visual Genome effort in
the computer vision community, we constructed the first Chest ImaGenome dataset
with a scene graph data structure to describe $242,072$ images. Local
annotations are automatically produced using a joint rule-based natural
language processing (NLP) and atlas-based bounding box detection pipeline.
Through a radiologist constructed CXR ontology, the annotations for each CXR
are connected as an anatomy-centered scene graph, useful for image-level
reasoning and multimodal fusion applications. Overall, we provide: i) $1,256$
combinations of relation annotations between $29$ CXR anatomical locations
(objects with bounding box coordinates) and their attributes, structured as a
scene graph per image, ii) over $670,000$ localized comparison relations (for
improved, worsened, or no change) between the anatomical locations across
sequential exams, as well as ii) a manually annotated gold standard scene graph
dataset from $500$ unique patients.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1"&gt;Joy T. Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agu_N/0/1/0/all/0/1"&gt;Nkechinyere N. Agu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lourentzou_I/0/1/0/all/0/1"&gt;Ismini Lourentzou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1"&gt;Arjun Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paguio_J/0/1/0/all/0/1"&gt;Joseph A. Paguio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1"&gt;Jasper S. Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dee_E/0/1/0/all/0/1"&gt;Edward C. Dee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mitchell_W/0/1/0/all/0/1"&gt;William Mitchell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kashyap_S/0/1/0/all/0/1"&gt;Satyananda Kashyap&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Giovannini_A/0/1/0/all/0/1"&gt;Andrea Giovannini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Celi_L/0/1/0/all/0/1"&gt;Leo A. Celi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moradi_M/0/1/0/all/0/1"&gt;Mehdi Moradi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[You too Brutus! Trapping Hateful Users in Social Media: Challenges, Solutions & Insights. (arXiv:2108.00524v1 [cs.SI])]]></title>
        <id>http://arxiv.org/abs/2108.00524</id>
        <link href="http://arxiv.org/abs/2108.00524"/>
        <updated>2021-08-03T02:06:33.712Z</updated>
        <summary type="html"><![CDATA[Hate speech is regarded as one of the crucial issues plaguing the online
social media. The current literature on hate speech detection leverages
primarily the textual content to find hateful posts and subsequently identify
hateful users. However, this methodology disregards the social connections
between users. In this paper, we run a detailed exploration of the problem
space and investigate an array of models ranging from purely textual to graph
based to finally semi-supervised techniques using Graph Neural Networks (GNN)
that utilize both textual and graph-based features. We run exhaustive
experiments on two datasets -- Gab, which is loosely moderated and Twitter,
which is strictly moderated. Overall the AGNN model achieves 0.791 macro
F1-score on the Gab dataset and 0.780 macro F1-score on the Twitter dataset
using only 5% of the labeled instances, considerably outperforming all the
other models including the fully supervised ones. We perform detailed error
analysis on the best performing text and graph based models and observe that
hateful users have unique network neighborhood signatures and the AGNN model
benefits by paying attention to these signatures. This property, as we observe,
also allows the model to generalize well across platforms in a zero-shot
setting. Lastly, we utilize the best performing GNN model to analyze the
evolution of hateful users and their targets over time in Gab.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Das_M/0/1/0/all/0/1"&gt;Mithun Das&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saha_P/0/1/0/all/0/1"&gt;Punyajoy Saha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dutt_R/0/1/0/all/0/1"&gt;Ritam Dutt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goyal_P/0/1/0/all/0/1"&gt;Pawan Goyal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mukherjee_A/0/1/0/all/0/1"&gt;Animesh Mukherjee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mathew_B/0/1/0/all/0/1"&gt;Binny Mathew&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pruning Neural Networks with Interpolative Decompositions. (arXiv:2108.00065v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.00065</id>
        <link href="http://arxiv.org/abs/2108.00065"/>
        <updated>2021-08-03T02:06:33.692Z</updated>
        <summary type="html"><![CDATA[We introduce a principled approach to neural network pruning that casts the
problem as a structured low-rank matrix approximation. Our method uses a novel
application of a matrix factorization technique called the interpolative
decomposition to approximate the activation output of a network layer. This
technique selects neurons or channels in the layer and propagates a corrective
interpolation matrix to the next layer, resulting in a dense, pruned network
with minimal degradation before fine tuning. We demonstrate how to prune a
neural network by first building a set of primitives to prune a single fully
connected or convolution layer and then composing these primitives to prune
deep multi-layer networks. Theoretical guarantees are provided for pruning a
single hidden layer fully connected network. Pruning with interpolative
decompositions achieves strong empirical results compared to the
state-of-the-art on multiple applications from one and two hidden layer
networks on Fashion MNIST to VGG and ResNets on CIFAR-10. Notably, we achieve
an accuracy of 93.62 $\pm$ 0.36% using VGG-16 on CIFAR-10, with a 51% FLOPS
reduction. This gains 0.02% from the full-sized model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chee_J/0/1/0/all/0/1"&gt;Jerry Chee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Renz_M/0/1/0/all/0/1"&gt;Megan Renz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Damle_A/0/1/0/all/0/1"&gt;Anil Damle&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sa_C/0/1/0/all/0/1"&gt;Chris De Sa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Foundations of data imbalance and solutions for a data democracy. (arXiv:2108.00071v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.00071</id>
        <link href="http://arxiv.org/abs/2108.00071"/>
        <updated>2021-08-03T02:06:33.685Z</updated>
        <summary type="html"><![CDATA[Dealing with imbalanced data is a prevalent problem while performing
classification on the datasets. Many times, this problem contributes to bias
while making decisions or implementing policies. Thus, it is vital to
understand the factors which cause imbalance in the data (or class imbalance).
Such hidden biases and imbalances can lead to data tyranny and a major
challenge to a data democracy. In this chapter, two essential statistical
elements are resolved: the degree of class imbalance and the complexity of the
concept; solving such issues helps in building the foundations of a data
democracy. Furthermore, statistical measures which are appropriate in these
scenarios are discussed and implemented on a real-life dataset (car insurance
claims). In the end, popular data-level methods such as random oversampling,
random undersampling, synthetic minority oversampling technique, Tomek link,
and others are implemented in Python, and their performance is compared.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kulkarni_A/0/1/0/all/0/1"&gt;Ajay Kulkarni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chong_D/0/1/0/all/0/1"&gt;Deri Chong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Batarseh_F/0/1/0/all/0/1"&gt;Feras A. Batarseh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Simple, Fast, and Flexible Framework for Matrix Completion with Infinite Width Neural Networks. (arXiv:2108.00131v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.00131</id>
        <link href="http://arxiv.org/abs/2108.00131"/>
        <updated>2021-08-03T02:06:33.672Z</updated>
        <summary type="html"><![CDATA[Matrix completion problems arise in many applications including
recommendation systems, computer vision, and genomics. Increasingly larger
neural networks have been successful in many of these applications, but at
considerable computational costs. Remarkably, taking the width of a neural
network to infinity allows for improved computational performance. In this
work, we develop an infinite width neural network framework for matrix
completion that is simple, fast, and flexible. Simplicity and speed come from
the connection between the infinite width limit of neural networks and kernels
known as neural tangent kernels (NTK). In particular, we derive the NTK for
fully connected and convolutional neural networks for matrix completion. The
flexibility stems from a feature prior, which allows encoding relationships
between coordinates of the target matrix, akin to semi-supervised learning. The
effectiveness of our framework is demonstrated through competitive results for
virtual drug screening and image inpainting/reconstruction. We also provide an
implementation in Python to make our framework accessible on standard hardware
to a broad audience.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Radhakrishnan_A/0/1/0/all/0/1"&gt;Adityanarayanan Radhakrishnan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stefanakis_G/0/1/0/all/0/1"&gt;George Stefanakis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Belkin_M/0/1/0/all/0/1"&gt;Mikhail Belkin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Uhler_C/0/1/0/all/0/1"&gt;Caroline Uhler&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Diverse Linguistic Features for Assessing Reading Difficulty of Educational Filipino Texts. (arXiv:2108.00241v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.00241</id>
        <link href="http://arxiv.org/abs/2108.00241"/>
        <updated>2021-08-03T02:06:33.666Z</updated>
        <summary type="html"><![CDATA[In order to ensure quality and effective learning, fluency, and
comprehension, the proper identification of the difficulty levels of reading
materials should be observed. In this paper, we describe the development of
automatic machine learning-based readability assessment models for educational
Filipino texts using the most diverse set of linguistic features for the
language. Results show that using a Random Forest model obtained a high
performance of 62.7% in terms of accuracy, and 66.1% when using the optimal
combination of feature sets consisting of traditional and syllable
pattern-based predictors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Imperial_J/0/1/0/all/0/1"&gt;Joseph Marvin Imperial&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ong_E/0/1/0/all/0/1"&gt;Ethel Ong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Supervised Learning of Context-Aware Pitch Prosody Representations. (arXiv:2007.09060v4 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.09060</id>
        <link href="http://arxiv.org/abs/2007.09060"/>
        <updated>2021-08-03T02:06:33.659Z</updated>
        <summary type="html"><![CDATA[In music and speech, meaning is derived at multiple levels of context.
Affect, for example, can be inferred both by a short sound token and by sonic
patterns over a longer temporal window such as an entire recording. In this
letter, we focus on inferring meaning from this dichotomy of contexts. We show
how contextual representations of short sung vocal lines can be implicitly
learned from fundamental frequency ($F_0$) and thus be used as a meaningful
feature space for downstream Music Information Retrieval (MIR) tasks. We
propose three self-supervised deep learning paradigms which leverage pseudotask
learning of these two levels of context to produce latent representation
spaces. We evaluate the usefulness of these representations by embedding unseen
pitch contours into each space and conducting downstream classification tasks.
Our results show that contextual representation can enhance downstream
classification by as much as 15\% as compared to using traditional statistical
contour features.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Noufi_C/0/1/0/all/0/1"&gt;Camille Noufi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Verma_P/0/1/0/all/0/1"&gt;Prateek Verma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bilevel Optimization for Machine Learning: Algorithm Design and Convergence Analysis. (arXiv:2108.00330v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.00330</id>
        <link href="http://arxiv.org/abs/2108.00330"/>
        <updated>2021-08-03T02:06:33.630Z</updated>
        <summary type="html"><![CDATA[Bilevel optimization has become a powerful framework in various machine
learning applications including meta-learning, hyperparameter optimization, and
network architecture search. There are generally two classes of bilevel
optimization formulations for machine learning: 1) problem-based bilevel
optimization, whose inner-level problem is formulated as finding a minimizer of
a given loss function; and 2) algorithm-based bilevel optimization, whose
inner-level solution is an output of a fixed algorithm. For the first class,
two popular types of gradient-based algorithms have been proposed for
hypergradient estimation via approximate implicit differentiation (AID) and
iterative differentiation (ITD). Algorithms for the second class include the
popular model-agnostic meta-learning (MAML) and almost no inner loop (ANIL).
However, the convergence rate and fundamental limitations of bilevel
optimization algorithms have not been well explored.

This thesis provides a comprehensive convergence rate analysis for bilevel
algorithms in the aforementioned two classes. We further propose principled
algorithm designs for bilevel optimization with higher efficiency and
scalability. For the problem-based formulation, we provide a convergence rate
analysis for AID- and ITD-based bilevel algorithms. We then develop
acceleration bilevel algorithms, for which we provide shaper convergence
analysis with relaxed assumptions. We also provide the first lower bounds for
bilevel optimization, and establish the optimality by providing matching upper
bounds under certain conditions. We finally propose new stochastic bilevel
optimization algorithms with lower complexity and higher efficiency in
practice. For the algorithm-based formulation, we develop a theoretical
convergence for general multi-step MAML and ANIL, and characterize the impact
of parameter selections and loss geometries on the their complexities.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ji_K/0/1/0/all/0/1"&gt;Kaiyi Ji&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Greedy Network Enlarging. (arXiv:2108.00177v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00177</id>
        <link href="http://arxiv.org/abs/2108.00177"/>
        <updated>2021-08-03T02:06:33.600Z</updated>
        <summary type="html"><![CDATA[Recent studies on deep convolutional neural networks present a simple
paradigm of architecture design, i.e., models with more MACs typically achieve
better accuracy, such as EfficientNet and RegNet. These works try to enlarge
all the stages in the model with one unified rule by sampling and statistical
methods. However, we observe that some network architectures have similar MACs
and accuracies, but their allocations on computations for different stages are
quite different. In this paper, we propose to enlarge the capacity of CNN
models by improving their width, depth and resolution on stage level. Under the
assumption that the top-performing smaller CNNs are a proper subcomponent of
the top-performing larger CNNs, we propose an greedy network enlarging method
based on the reallocation of computations. With step-by-step modifying the
computations on different stages, the enlarged network will be equipped with
optimal allocation and utilization of MACs. On EfficientNet, our method
consistently outperforms the performance of the original scaling method. In
particular, with application of our method on GhostNet, we achieve
state-of-the-art 80.9% and 84.3% ImageNet top-1 accuracies under the setting of
600M and 4.4B MACs, respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1"&gt;Chuanjian Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1"&gt;Kai Han&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_A/0/1/0/all/0/1"&gt;An Xiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1"&gt;Yiping Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Wei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Chunjing Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yunhe Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Level Graph Matching Networks for Deep Graph Similarity Learning. (arXiv:2007.04395v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.04395</id>
        <link href="http://arxiv.org/abs/2007.04395"/>
        <updated>2021-08-03T02:06:33.589Z</updated>
        <summary type="html"><![CDATA[While the celebrated graph neural networks yield effective representations
for individual nodes of a graph, there has been relatively less success in
extending to the task of graph similarity learning. Recent work on graph
similarity learning has considered either global-level graph-graph interactions
or low-level node-node interactions, however ignoring the rich cross-level
interactions (e.g., between each node of one graph and the other whole graph).
In this paper, we propose a multi-level graph matching network (MGMN) framework
for computing the graph similarity between any pair of graph-structured objects
in an end-to-end fashion. In particular, the proposed MGMN consists of a
node-graph matching network for effectively learning cross-level interactions
between each node of one graph and the other whole graph, and a siamese graph
neural network to learn global-level interactions between two input graphs.
Furthermore, to compensate for the lack of standard benchmark datasets, we have
created and collected a set of datasets for both the graph-graph classification
and graph-graph regression tasks with different sizes in order to evaluate the
effectiveness and robustness of our models. Comprehensive experiments
demonstrate that MGMN consistently outperforms state-of-the-art baseline models
on both the graph-graph classification and graph-graph regression tasks.
Compared with previous work, MGMN also exhibits stronger robustness as the
sizes of the two input graphs increase.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ling_X/0/1/0/all/0/1"&gt;Xiang Ling&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1"&gt;Lingfei Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Saizhuo Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1"&gt;Tengfei Ma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1"&gt;Fangli Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1"&gt;Alex X. Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1"&gt;Chunming Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1"&gt;Shouling Ji&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BoA-PTA, A Bayesian Optimization Accelerated Error-Free SPICE Solver. (arXiv:2108.00257v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.00257</id>
        <link href="http://arxiv.org/abs/2108.00257"/>
        <updated>2021-08-03T02:06:33.583Z</updated>
        <summary type="html"><![CDATA[One of the greatest challenges in IC design is the repeated executions of
computationally expensive SPICE simulations, particularly when highly complex
chip testing/verification is involved. Recently, pseudo transient analysis
(PTA) has shown to be one of the most promising continuation SPICE solver.
However, the PTA efficiency is highly influenced by the inserted
pseudo-parameters. In this work, we proposed BoA-PTA, a Bayesian optimization
accelerated PTA that can substantially accelerate simulations and improve
convergence performance without introducing extra errors. Furthermore, our
method does not require any pre-computation data or offline training. The
acceleration framework can either be implemented to speed up ongoing repeated
simulations immediately or to improve new simulations of completely different
circuits. BoA-PTA is equipped with cutting-edge machine learning techniques,
e.g., deep learning, Gaussian process, Bayesian optimization, non-stationary
monotonic transformation, and variational inference via parameterization. We
assess BoA-PTA in 43 benchmark circuits against other SOTA SPICE solvers and
demonstrate an average 2.3x (maximum 3.5x) speed-up over the original CEPTA.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xing_W/0/1/0/all/0/1"&gt;Wei W. Xing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1"&gt;Xiang Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yi Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niu_D/0/1/0/all/0/1"&gt;Dan Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1"&gt;Weishen Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1"&gt;Zhou Jin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RLTutor: Reinforcement Learning Based Adaptive Tutoring System by Modeling Virtual Student with Fewer Interactions. (arXiv:2108.00268v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2108.00268</id>
        <link href="http://arxiv.org/abs/2108.00268"/>
        <updated>2021-08-03T02:06:33.576Z</updated>
        <summary type="html"><![CDATA[A major challenge in the field of education is providing review schedules
that present learned items at appropriate intervals to each student so that
memory is retained over time. In recent years, attempts have been made to
formulate item reviews as sequential decision-making problems to realize
adaptive instruction based on the knowledge state of students. It has been
reported previously that reinforcement learning can help realize mathematical
models of students learning strategies to maintain a high memory rate. However,
optimization using reinforcement learning requires a large number of
interactions, and thus it cannot be applied directly to actual students. In
this study, we propose a framework for optimizing teaching strategies by
constructing a virtual model of the student while minimizing the interaction
with the actual teaching target. In addition, we conducted an experiment
considering actual instructions using the mathematical model and confirmed that
the model performance is comparable to that of conventional teaching methods.
Our framework can directly substitute mathematical models used in experiments
with human students, and our results can serve as a buffer between theoretical
instructional optimization and practical applications in e-learning systems.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kubotani_Y/0/1/0/all/0/1"&gt;Yoshiki Kubotani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fukuhara_Y/0/1/0/all/0/1"&gt;Yoshihiro Fukuhara&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morishima_S/0/1/0/all/0/1"&gt;Shigeo Morishima&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Debiasing Samples from Online Learning Using Bootstrap. (arXiv:2108.00236v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.00236</id>
        <link href="http://arxiv.org/abs/2108.00236"/>
        <updated>2021-08-03T02:06:33.560Z</updated>
        <summary type="html"><![CDATA[It has been recently shown in the literature that the sample averages from
online learning experiments are biased when used to estimate the mean reward.
To correct the bias, off-policy evaluation methods, including importance
sampling and doubly robust estimators, typically calculate the propensity
score, which is unavailable in this setting due to unknown reward distribution
and the adaptive policy. This paper provides a procedure to debias the samples
using bootstrap, which doesn't require the knowledge of the reward distribution
at all. Numerical experiments demonstrate the effective bias reduction for
samples generated by popular multi-armed bandit algorithms such as
Explore-Then-Commit (ETC), UCB, Thompson sampling and $\epsilon$-greedy. We
also analyze and provide theoretical justifications for the procedure under the
ETC algorithm, including the asymptotic convergence of the bias decay rate in
the real and bootstrap worlds.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1"&gt;Ningyuan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1"&gt;Xuefeng Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1"&gt;Yi Xiong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Speech2AffectiveGestures: Synthesizing Co-Speech Gestures with Generative Adversarial Affective Expression Learning. (arXiv:2108.00262v1 [cs.MM])]]></title>
        <id>http://arxiv.org/abs/2108.00262</id>
        <link href="http://arxiv.org/abs/2108.00262"/>
        <updated>2021-08-03T02:06:33.553Z</updated>
        <summary type="html"><![CDATA[We present a generative adversarial network to synthesize 3D pose sequences
of co-speech upper-body gestures with appropriate affective expressions. Our
network consists of two components: a generator to synthesize gestures from a
joint embedding space of features encoded from the input speech and the seed
poses, and a discriminator to distinguish between the synthesized pose
sequences and real 3D pose sequences. We leverage the Mel-frequency cepstral
coefficients and the text transcript computed from the input speech in separate
encoders in our generator to learn the desired sentiments and the associated
affective cues. We design an affective encoder using multi-scale
spatial-temporal graph convolutions to transform 3D pose sequences into latent,
pose-based affective features. We use our affective encoder in both our
generator, where it learns affective features from the seed poses to guide the
gesture synthesis, and our discriminator, where it enforces the synthesized
gestures to contain the appropriate affective expressions. We perform extensive
evaluations on two benchmark datasets for gesture synthesis from the speech,
the TED Gesture Dataset and the GENEA Challenge 2020 Dataset. Compared to the
best baselines, we improve the mean absolute joint error by 10--33%, the mean
acceleration difference by 8--58%, and the Fr\'echet Gesture Distance by
21--34%. We also conduct a user study and observe that compared to the best
current baselines, around 15.28% of participants indicated our synthesized
gestures appear more plausible, and around 16.32% of participants felt the
gestures had more appropriate affective expressions aligned with the speech.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bhattacharya_U/0/1/0/all/0/1"&gt;Uttaran Bhattacharya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Childs_E/0/1/0/all/0/1"&gt;Elizabeth Childs&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rewkowski_N/0/1/0/all/0/1"&gt;Nicholas Rewkowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Manocha_D/0/1/0/all/0/1"&gt;Dinesh Manocha&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Flip Learning: Erase to Segment. (arXiv:2108.00752v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00752</id>
        <link href="http://arxiv.org/abs/2108.00752"/>
        <updated>2021-08-03T02:06:33.541Z</updated>
        <summary type="html"><![CDATA[Nodule segmentation from breast ultrasound images is challenging yet
essential for the diagnosis. Weakly-supervised segmentation (WSS) can help
reduce time-consuming and cumbersome manual annotation. Unlike existing
weakly-supervised approaches, in this study, we propose a novel and general WSS
framework called Flip Learning, which only needs the box annotation.
Specifically, the target in the label box will be erased gradually to flip the
classification tag, and the erased region will be considered as the
segmentation result finally. Our contribution is three-fold. First, our
proposed approach erases on superpixel level using a Multi-agent Reinforcement
Learning framework to exploit the prior boundary knowledge and accelerate the
learning process. Second, we design two rewards: classification score and
intensity distribution reward, to avoid under- and over-segmentation,
respectively. Third, we adopt a coarse-to-fine learning strategy to reduce the
residual errors and improve the segmentation performance. Extensively validated
on a large dataset, our proposed approach achieves competitive performance and
shows great potential to narrow the gap between fully-supervised and
weakly-supervised learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yuhao Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1"&gt;Yuxin Zou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Chaoyu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jian Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dou_H/0/1/0/all/0/1"&gt;Haoran Dou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ravikumar_N/0/1/0/all/0/1"&gt;Nishant Ravikumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frangi_A/0/1/0/all/0/1"&gt;Alejandro F Frangi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jianqiao Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ni_D/0/1/0/all/0/1"&gt;Dong Ni&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Plant Root System Algorithm Based on Swarm Intelligence for One-dimensional Biomedical Signal Feature Engineering. (arXiv:2108.00214v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.00214</id>
        <link href="http://arxiv.org/abs/2108.00214"/>
        <updated>2021-08-03T02:06:33.533Z</updated>
        <summary type="html"><![CDATA[To date, very few biomedical signals have transitioned from research
applications to clinical applications. This is largely due to the lack of trust
in the diagnostic ability of non-stationary signals. To reach the level of
clinical diagnostic application, classification using high-quality signal
features is necessary. While there has been considerable progress in machine
learning in recent years, especially deep learning, progress has been quite
limited in the field of feature engineering. This study proposes a feature
extraction algorithm based on group intelligence which we call a Plant Root
System (PRS) algorithm. Importantly, the correlation between features produced
by this PRS algorithm and traditional features is low, and the accuracy of
several widely-used classifiers was found to be substantially improved with the
addition of PRS features. It is expected that more biomedical signals can be
applied to clinical diagnosis using the proposed algorithm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gong_R/0/1/0/all/0/1"&gt;Rui Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hase_K/0/1/0/all/0/1"&gt;Kazunori Hase&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Empirical analysis on Transparent Algorithmic Exploration in Recommender Systems. (arXiv:2108.00151v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2108.00151</id>
        <link href="http://arxiv.org/abs/2108.00151"/>
        <updated>2021-08-03T02:06:33.527Z</updated>
        <summary type="html"><![CDATA[All learning algorithms for recommendations face inevitable and critical
trade-off between exploiting partial knowledge of a user's preferences for
short-term satisfaction and exploring additional user preferences for long-term
coverage. Although exploration is indispensable for long success of a
recommender system, the exploration has been considered as the risk to decrease
user satisfaction. The reason for the risk is that items chosen for exploration
frequently mismatch with the user's interests. To mitigate this risk,
recommender systems have mixed items chosen for exploration into a
recommendation list, disguising the items as recommendations to elicit feedback
on the items to discover the user's additional tastes. This mix-in approach has
been widely used in many recommenders, but there is rare research, evaluating
the effectiveness of the mix-in approach or proposing a new approach for
eliciting user feedback without deceiving users. In this work, we aim to
propose a new approach for feedback elicitation without any deception and
compare our approach to the conventional mix-in approach for evaluation. To
this end, we designed a recommender interface that reveals which items are for
exploration and conducted a within-subject study with 94 MTurk workers. Our
results indicated that users left significantly more feedback on items chosen
for exploration with our interface. Besides, users evaluated that our new
interface is better than the conventional mix-in interface in terms of novelty,
diversity, transparency, trust, and satisfaction. Finally, path analysis show
that, in only our new interface, exploration caused to increase user-centric
evaluation metrics. Our work paves the way for how to design an interface,
which utilizes learning algorithm based on users' feedback signals, giving
better user experience and gathering more feedback data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1"&gt;Kihwan Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bayesian analysis of the prevalence bias: learning and predicting from imbalanced data. (arXiv:2108.00250v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.00250</id>
        <link href="http://arxiv.org/abs/2108.00250"/>
        <updated>2021-08-03T02:06:33.520Z</updated>
        <summary type="html"><![CDATA[Datasets are rarely a realistic approximation of the target population. Say,
prevalence is misrepresented, image quality is above clinical standards, etc.
This mismatch is known as sampling bias. Sampling biases are a major hindrance
for machine learning models. They cause significant gaps between model
performance in the lab and in the real world. Our work is a solution to
prevalence bias. Prevalence bias is the discrepancy between the prevalence of a
pathology and its sampling rate in the training dataset, introduced upon
collecting data or due to the practioner rebalancing the training batches. This
paper lays the theoretical and computational framework for training models, and
for prediction, in the presence of prevalence bias. Concretely a bias-corrected
loss function, as well as bias-corrected predictive rules, are derived under
the principles of Bayesian risk minimization. The loss exhibits a direct
connection to the information gain. It offers a principled alternative to
heuristic training losses and complements test-time procedures based on
selecting an operating point from summary curves. It integrates seamlessly in
the current paradigm of (deep) learning using stochastic backpropagation and
naturally with Bayesian models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Folgoc_L/0/1/0/all/0/1"&gt;Loic Le Folgoc&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Baltatzis_V/0/1/0/all/0/1"&gt;Vasileios Baltatzis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alansary_A/0/1/0/all/0/1"&gt;Amir Alansary&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Desai_S/0/1/0/all/0/1"&gt;Sujal Desai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Devaraj_A/0/1/0/all/0/1"&gt;Anand Devaraj&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ellis_S/0/1/0/all/0/1"&gt;Sam Ellis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Manzanera_O/0/1/0/all/0/1"&gt;Octavio E. Martinez Manzanera&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kanavati_F/0/1/0/all/0/1"&gt;Fahdi Kanavati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nair_A/0/1/0/all/0/1"&gt;Arjun Nair&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schnabel_J/0/1/0/all/0/1"&gt;Julia Schnabel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Glocker_B/0/1/0/all/0/1"&gt;Ben Glocker&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Statistical learning method for predicting density-matrix based electron dynamics. (arXiv:2108.00318v1 [physics.chem-ph])]]></title>
        <id>http://arxiv.org/abs/2108.00318</id>
        <link href="http://arxiv.org/abs/2108.00318"/>
        <updated>2021-08-03T02:06:33.514Z</updated>
        <summary type="html"><![CDATA[We develop a statistical method to learn a molecular Hamiltonian matrix from
a time-series of electron density matrices. We extend our previous method to
larger molecular systems by incorporating physical properties to reduce
dimensionality, while also exploiting regularization techniques like ridge
regression for addressing multicollinearity. With the learned Hamiltonian we
can solve the Time-Dependent Hartree-Fock (TDHF) equation to propagate the
electron density in time, and predict its dynamics for field-free and field-on
scenarios. We observe close quantitative agreement between the predicted
dynamics and ground truth for both field-off trajectories similar to the
training data, and field-on trajectories outside of the training data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Gupta_P/0/1/0/all/0/1"&gt;Prachi Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Bhat_H/0/1/0/all/0/1"&gt;Harish S. Bhat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Ranka_K/0/1/0/all/0/1"&gt;Karnamohit Ranka&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Isborn_C/0/1/0/all/0/1"&gt;Christine M. Isborn&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Conditional Bures Metric for Domain Adaptation. (arXiv:2108.00302v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.00302</id>
        <link href="http://arxiv.org/abs/2108.00302"/>
        <updated>2021-08-03T02:06:33.503Z</updated>
        <summary type="html"><![CDATA[As a vital problem in classification-oriented transfer, unsupervised domain
adaptation (UDA) has attracted widespread attention in recent years. Previous
UDA methods assume the marginal distributions of different domains are shifted
while ignoring the discriminant information in the label distributions. This
leads to classification performance degeneration in real applications. In this
work, we focus on the conditional distribution shift problem which is of great
concern to current conditional invariant models. We aim to seek a kernel
covariance embedding for conditional distribution which remains yet unexplored.
Theoretically, we propose the Conditional Kernel Bures (CKB) metric for
characterizing conditional distribution discrepancy, and derive an empirical
estimation for the CKB metric without introducing the implicit kernel feature
map. It provides an interpretable approach to understand the knowledge transfer
mechanism. The established consistency theory of the empirical estimation
provides a theoretical guarantee for convergence. A conditional distribution
matching network is proposed to learn the conditional invariant and
discriminative features for UDA. Extensive experiments and analysis show the
superiority of our proposed model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1"&gt;You-Wei Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_C/0/1/0/all/0/1"&gt;Chuan-Xian Ren&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Inverse Reinforcement Learning for Strategy Identification. (arXiv:2108.00293v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.00293</id>
        <link href="http://arxiv.org/abs/2108.00293"/>
        <updated>2021-08-03T02:06:33.471Z</updated>
        <summary type="html"><![CDATA[In adversarial environments, one side could gain an advantage by identifying
the opponent's strategy. For example, in combat games, if an opponents strategy
is identified as overly aggressive, one could lay a trap that exploits the
opponent's aggressive nature. However, an opponent's strategy is not always
apparent and may need to be estimated from observations of their actions. This
paper proposes to use inverse reinforcement learning (IRL) to identify
strategies in adversarial environments. Specifically, the contributions of this
work are 1) the demonstration of this concept on gaming combat data generated
from three pre-defined strategies and 2) the framework for using IRL to achieve
strategy identification. The numerical experiments demonstrate that the
recovered rewards can be identified using a variety of techniques. In this
paper, the recovered reward are visually displayed, clustered using
unsupervised learning, and classified using a supervised learner.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rucker_M/0/1/0/all/0/1"&gt;Mark Rucker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Adams_S/0/1/0/all/0/1"&gt;Stephen Adams&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hayes_R/0/1/0/all/0/1"&gt;Roy Hayes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Beling_P/0/1/0/all/0/1"&gt;Peter A. Beling&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Structured DropConnect for Uncertainty Inference in Image Classification. (arXiv:2106.08624v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.08624</id>
        <link href="http://arxiv.org/abs/2106.08624"/>
        <updated>2021-08-03T02:06:33.463Z</updated>
        <summary type="html"><![CDATA[With the complexity of the network structure, uncertainty inference has
become an important task to improve the classification accuracy for artificial
intelligence systems. For image classification tasks, we propose a structured
DropConnect (SDC) framework to model the output of a deep neural network by a
Dirichlet distribution. We introduce a DropConnect strategy on weights in the
fully connected layers during training. In test, we split the network into
several sub-networks, and then model the Dirichlet distribution by match its
moments with the mean and variance of the outputs of these sub-networks. The
entropy of the estimated Dirichlet distribution is finally utilized for
uncertainty inference. In this paper, this framework is implemented on LeNet$5$
and VGG$16$ models for misclassification detection and out-of-distribution
detection on MNIST and CIFAR-$10$ datasets. Experimental results show that the
performance of the proposed SDC can be comparable to other uncertainty
inference methods. Furthermore, the SDC is adapted well to different network
structures with certain generalization capabilities and research prospects.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1"&gt;Wenqing Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1"&gt;Jiyang Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Weidong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1"&gt;Zhanyu Ma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Hybrid Ensemble Feature Selection Design for Candidate Biomarkers Discovery from Transcriptome Profiles. (arXiv:2108.00290v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.00290</id>
        <link href="http://arxiv.org/abs/2108.00290"/>
        <updated>2021-08-03T02:06:33.440Z</updated>
        <summary type="html"><![CDATA[The discovery of disease biomarkers from gene expression data has been
greatly advanced by feature selection (FS) methods, especially using ensemble
FS (EFS) strategies with perturbation at the data level (i.e., homogeneous,
Hom-EFS) or method level (i.e., heterogeneous, Het-EFS). Here we proposed a
Hybrid EFS (Hyb-EFS) design that explores both types of perturbation to improve
the stability and the predictive power of candidate biomarkers. With this,
Hyb-EFS aims to disrupt associations of good performance with a single dataset,
single algorithm, or a specific combination of both, which is particularly
interesting for better reproducibility of genomic biomarkers. We investigated
the adequacy of our approach for microarray data related to four types of
cancer, carrying out an extensive comparison with other ensemble and single FS
approaches. Five FS methods were used in our experiments: Wx, Symmetrical
Uncertainty (SU), Gain Ratio (GR), Characteristic Direction (GeoDE), and
ReliefF. We observed that the Hyb-EFS and Het-EFS approaches attenuated the
large performance variation observed for most single FS and Hom-EFS across
distinct datasets. Also, the Hyb-EFS improved upon the stability of the Het-EFS
within our domain. Comparing the Hyb-EFS and Het-EFS composed of the
top-performing selectors (Wx, GR, and SU), our hybrid approach surpassed the
equivalent heterogeneous design and the best Hom-EFS (Hom-Wx). Interestingly,
the rankings produced by our Hyb-EFS reached greater biological plausibility,
with a notably high enrichment for cancer-related genes and pathways. Thus, our
experiments suggest the potential of the proposed Hybrid EFS design in
discovering candidate biomarkers from microarray data. Finally, we provide an
open-source framework to support similar analyses in other domains, both as a
user-friendly application and a plain Python package.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Colombelli_F/0/1/0/all/0/1"&gt;Felipe Colombelli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kowalski_T/0/1/0/all/0/1"&gt;Thayne Woycinck Kowalski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Recamonde_Mendoza_M/0/1/0/all/0/1"&gt;Mariana Recamonde-Mendoza&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PAN++: Towards Efficient and Accurate End-to-End Spotting of Arbitrarily-Shaped Text. (arXiv:2105.00405v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.00405</id>
        <link href="http://arxiv.org/abs/2105.00405"/>
        <updated>2021-08-03T02:06:33.422Z</updated>
        <summary type="html"><![CDATA[Scene text detection and recognition have been well explored in the past few
years. Despite the progress, efficient and accurate end-to-end spotting of
arbitrarily-shaped text remains challenging. In this work, we propose an
end-to-end text spotting framework, termed PAN++, which can efficiently detect
and recognize text of arbitrary shapes in natural scenes. PAN++ is based on the
kernel representation that reformulates a text line as a text kernel (central
region) surrounded by peripheral pixels. By systematically comparing with
existing scene text representations, we show that our kernel representation can
not only describe arbitrarily-shaped text but also well distinguish adjacent
text. Moreover, as a pixel-based representation, the kernel representation can
be predicted by a single fully convolutional network, which is very friendly to
real-time applications. Taking the advantages of the kernel representation, we
design a series of components as follows: 1) a computationally efficient
feature enhancement network composed of stacked Feature Pyramid Enhancement
Modules (FPEMs); 2) a lightweight detection head cooperating with Pixel
Aggregation (PA); and 3) an efficient attention-based recognition head with
Masked RoI. Benefiting from the kernel representation and the tailored
components, our method achieves high inference speed while maintaining
competitive accuracy. Extensive experiments show the superiority of our method.
For example, the proposed PAN++ achieves an end-to-end text spotting F-measure
of 64.9 at 29.2 FPS on the Total-Text dataset, which significantly outperforms
the previous best method. Code will be available at: https://git.io/PAN.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wenhai Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_E/0/1/0/all/0/1"&gt;Enze Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xuebo Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_D/0/1/0/all/0/1"&gt;Ding Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Zhibo Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1"&gt;Tong Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1"&gt;Chunhua Shen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fair Representation Learning using Interpolation Enabled Disentanglement. (arXiv:2108.00295v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.00295</id>
        <link href="http://arxiv.org/abs/2108.00295"/>
        <updated>2021-08-03T02:06:33.416Z</updated>
        <summary type="html"><![CDATA[With the growing interest in the machine learning community to solve
real-world problems, it has become crucial to uncover the hidden reasoning
behind their decisions by focusing on the fairness and auditing the predictions
made by these black-box models. In this paper, we propose a novel method to
address two key issues: (a) Can we simultaneously learn fair disentangled
representations while ensuring the utility of the learned representation for
downstream tasks, and (b)Can we provide theoretical insights into when the
proposed approach will be both fair and accurate. To address the former, we
propose the method FRIED, Fair Representation learning using Interpolation
Enabled Disentanglement. In our architecture, by imposing a critic-based
adversarial framework, we enforce the interpolated points in the latent space
to be more realistic. This helps in capturing the data manifold effectively and
enhances the utility of the learned representation for downstream prediction
tasks. We address the latter question by developing a theory on
fairness-accuracy trade-offs using classifier-based conditional mutual
information estimation. We demonstrate the effectiveness of FRIED on datasets
of different modalities - tabular, text, and image datasets. We observe that
the representations learned by FRIED are overall fairer in comparison to
existing baselines and also accurate for downstream prediction tasks.
Additionally, we evaluate FRIED on a real-world healthcare claims dataset where
we conduct an expert aided model auditing study providing useful insights into
opioid ad-diction patterns.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jha_A/0/1/0/all/0/1"&gt;Akshita Jha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vinzamuri_B/0/1/0/all/0/1"&gt;Bhanukiran Vinzamuri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reddy_C/0/1/0/all/0/1"&gt;Chandan K. Reddy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Deep Learning Approach to Predict Blood Pressure from PPG Signals. (arXiv:2108.00099v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.00099</id>
        <link href="http://arxiv.org/abs/2108.00099"/>
        <updated>2021-08-03T02:06:33.398Z</updated>
        <summary type="html"><![CDATA[Blood Pressure (BP) is one of the four primary vital signs indicating the
status of the body's vital (life-sustaining) functions. BP is difficult to
continuously monitor using a sphygmomanometer (i.e. a blood pressure cuff),
especially in everyday-setting. However, other health signals which can be
easily and continuously acquired, such as photoplethysmography (PPG), show some
similarities with the Aortic Pressure waveform. Based on these similarities, in
recent years several methods were proposed to predict BP from the PPG signal.
Building on these results, we propose an advanced personalized data-driven
approach that uses a three-layer deep neural network to estimate BP based on
PPG signals. Different from previous work, the proposed model analyzes the PPG
signal in time-domain and automatically extracts the most critical features for
this specific application, then uses a variation of recurrent neural networks
called Long-Short-Term-Memory (LSTM) to map the extracted features to the BP
value associated with that time window. Experimental results on two separate
standard hospital datasets, yielded absolute errors mean and absolute error
standard deviation for systolic and diastolic BP values outperforming prior
works.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tazarv_A/0/1/0/all/0/1"&gt;Ali Tazarv&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Levorato_M/0/1/0/all/0/1"&gt;Marco Levorato&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Physics-informed Dyna-Style Model-Based Deep Reinforcement Learning for Dynamic Control. (arXiv:2108.00128v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.00128</id>
        <link href="http://arxiv.org/abs/2108.00128"/>
        <updated>2021-08-03T02:06:33.392Z</updated>
        <summary type="html"><![CDATA[Model-based reinforcement learning (MBRL) is believed to have much higher
sample efficiency compared to model-free algorithms by learning a predictive
model of the environment. However, the performance of MBRL highly relies on the
quality of the learned model, which is usually built in a black-box manner and
may have poor predictive accuracy outside of the data distribution. The
deficiencies of the learned model may prevent the policy from being fully
optimized. Although some uncertainty analysis-based remedies have been proposed
to alleviate this issue, model bias still poses a great challenge for MBRL. In
this work, we propose to leverage the prior knowledge of underlying physics of
the environment, where the governing laws are (partially) known. In particular,
we developed a physics-informed MBRL framework, where governing equations and
physical constraints are utilized to inform the model learning and policy
search. By incorporating the prior information of the environment, the quality
of the learned model can be notably improved, while the required interactions
with the environment are significantly reduced, leading to better sample
efficiency and learning performance. The effectiveness and merit have been
demonstrated over a handful of classic control problems, where the environments
are governed by canonical ordinary/partial differential equations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xin-Yang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jian-Xun Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Separation Capacity of Random Neural Networks. (arXiv:2108.00207v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.00207</id>
        <link href="http://arxiv.org/abs/2108.00207"/>
        <updated>2021-08-03T02:06:33.385Z</updated>
        <summary type="html"><![CDATA[Neural networks with random weights appear in a variety of machine learning
applications, most prominently as the initialization of many deep learning
algorithms and as a computationally cheap alternative to fully learned neural
networks. In the present article we enhance the theoretical understanding of
random neural nets by addressing the following data separation problem: under
what conditions can a random neural network make two classes $\mathcal{X}^-,
\mathcal{X}^+ \subset \mathbb{R}^d$ (with positive distance) linearly
separable? We show that a sufficiently large two-layer ReLU-network with
standard Gaussian weights and uniformly distributed biases can solve this
problem with high probability. Crucially, the number of required neurons is
explicitly linked to geometric properties of the underlying sets
$\mathcal{X}^-, \mathcal{X}^+$ and their mutual arrangement. This
instance-specific viewpoint allows us to overcome the usual curse of
dimensionality (exponential width of the layers) in non-pathological situations
where the data carries low-complexity structure. We quantify the relevant
structure of the data in terms of a novel notion of mutual complexity (based on
a localized version of Gaussian mean width), which leads to sound and
informative separation guarantees. We connect our result with related lines of
work on approximation, memorization, and generalization.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dirksen_S/0/1/0/all/0/1"&gt;Sjoerd Dirksen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Genzel_M/0/1/0/all/0/1"&gt;Martin Genzel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jacques_L/0/1/0/all/0/1"&gt;Laurent Jacques&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stollenwerk_A/0/1/0/all/0/1"&gt;Alexander Stollenwerk&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Freezing Sub-Models During Incremental Process Discovery: Extended Version. (arXiv:2108.00215v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.00215</id>
        <link href="http://arxiv.org/abs/2108.00215"/>
        <updated>2021-08-03T02:06:33.379Z</updated>
        <summary type="html"><![CDATA[Process discovery aims to learn a process model from observed process
behavior. From a user's perspective, most discovery algorithms work like a
black box. Besides parameter tuning, there is no interaction between the user
and the algorithm. Interactive process discovery allows the user to exploit
domain knowledge and to guide the discovery process. Previously, an incremental
discovery approach has been introduced where a model, considered to be under
construction, gets incrementally extended by user-selected process behavior.
This paper introduces a novel approach that additionally allows the user to
freeze model parts within the model under construction. Frozen sub-models are
not altered by the incremental approach when new behavior is added to the
model. The user can thus steer the discovery algorithm. Our experiments show
that freezing sub-models can lead to higher quality models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Schuster_D/0/1/0/all/0/1"&gt;Daniel Schuster&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zelst_S/0/1/0/all/0/1"&gt;Sebastiaan J. van Zelst&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aalst_W/0/1/0/all/0/1"&gt;Wil M. P. van der Aalst&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CrossFormer: A Versatile Vision Transformer Based on Cross-scale Attention. (arXiv:2108.00154v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00154</id>
        <link href="http://arxiv.org/abs/2108.00154"/>
        <updated>2021-08-03T02:06:33.358Z</updated>
        <summary type="html"><![CDATA[Transformers have made much progress in dealing with visual tasks. However,
existing vision transformers still do not possess an ability that is important
to visual input: building the attention among features of different scales. The
reasons for this problem are two-fold: (1) Input embeddings of each layer are
equal-scale without cross-scale features; (2) Some vision transformers
sacrifice the small-scale features of embeddings to lower the cost of the
self-attention module. To make up this defect, we propose Cross-scale Embedding
Layer (CEL) and Long Short Distance Attention (LSDA). In particular, CEL blends
each embedding with multiple patches of different scales, providing the model
with cross-scale embeddings. LSDA splits the self-attention module into a
short-distance and long-distance one, also lowering the cost but keeping both
small-scale and large-scale features in embeddings. Through these two designs,
we achieve cross-scale attention. Besides, we propose dynamic position bias for
vision transformers to make the popular relative position bias apply to
variable-sized images. Based on these proposed modules, we construct our vision
architecture called CrossFormer. Experiments show that CrossFormer outperforms
other transformers on several representative visual tasks, especially object
detection and segmentation. The code has been released:
https://github.com/cheerss/CrossFormer.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wenxiao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_L/0/1/0/all/0/1"&gt;Lu Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Long Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1"&gt;Deng Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1"&gt;Xiaofei He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wei Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Zooming Into the Darknet: Characterizing Internet Background Radiation and its Structural Changes. (arXiv:2108.00079v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2108.00079</id>
        <link href="http://arxiv.org/abs/2108.00079"/>
        <updated>2021-08-03T02:06:33.351Z</updated>
        <summary type="html"><![CDATA[Network telescopes or "Darknets" provide a unique window into Internet-wide
malicious activities associated with malware propagation, denial of service
attacks, scanning performed for network reconnaissance, and others. Analyses of
the resulting data can provide actionable insights to security analysts that
can be used to prevent or mitigate cyber-threats. Large Darknets, however,
observe millions of nefarious events on a daily basis which makes the
transformation of the captured information into meaningful insights
challenging. We present a novel framework for characterizing Darknet behavior
and its temporal evolution aiming to address this challenge. The proposed
framework: (i) Extracts a high dimensional representation of Darknet events
composed of features distilled from Darknet data and other external sources;
(ii) Learns, in an unsupervised fashion, an information-preserving
low-dimensional representation of these events (using deep representation
learning) that is amenable to clustering; (iv) Performs clustering of the
scanner data in the resulting representation space and provides interpretable
insights using optimal decision trees; and (v) Utilizes the clustering outcomes
as "signatures" that can be used to detect structural changes in the Darknet
activities. We evaluate the proposed system on a large operational Network
Telescope and demonstrate its ability to detect real-world, high-impact
cybersecurity incidents.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kallitsis_M/0/1/0/all/0/1"&gt;Michalis Kallitsis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Honavar_V/0/1/0/all/0/1"&gt;Vasant Honavar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prajapati_R/0/1/0/all/0/1"&gt;Rupesh Prajapati&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1"&gt;Dinghao Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yen_J/0/1/0/all/0/1"&gt;John Yen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pure Exploration and Regret Minimization in Matching Bandits. (arXiv:2108.00230v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2108.00230</id>
        <link href="http://arxiv.org/abs/2108.00230"/>
        <updated>2021-08-03T02:06:33.344Z</updated>
        <summary type="html"><![CDATA[Finding an optimal matching in a weighted graph is a standard combinatorial
problem. We consider its semi-bandit version where either a pair or a full
matching is sampled sequentially. We prove that it is possible to leverage a
rank-1 assumption on the adjacency matrix to reduce the sample complexity and
the regret of off-the-shelf algorithms up to reaching a linear dependency in
the number of vertices (up to poly log terms).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Sentenac_F/0/1/0/all/0/1"&gt;Flore Sentenac&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Yi_J/0/1/0/all/0/1"&gt;Jialin Yi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Calauzenes_C/0/1/0/all/0/1"&gt;Cl&amp;#xe9;ment Calauz&amp;#xe8;nes&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Perchet_V/0/1/0/all/0/1"&gt;Vianney Perchet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Vojnovic_M/0/1/0/all/0/1"&gt;Milan Vojnovic&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semi-Supervised Learning of Visual Features by Non-Parametrically Predicting View Assignments with Support Samples. (arXiv:2104.13963v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.13963</id>
        <link href="http://arxiv.org/abs/2104.13963"/>
        <updated>2021-08-03T02:06:33.337Z</updated>
        <summary type="html"><![CDATA[This paper proposes a novel method of learning by predicting view assignments
with support samples (PAWS). The method trains a model to minimize a
consistency loss, which ensures that different views of the same unlabeled
instance are assigned similar pseudo-labels. The pseudo-labels are generated
non-parametrically, by comparing the representations of the image views to
those of a set of randomly sampled labeled images. The distance between the
view representations and labeled representations is used to provide a weighting
over class labels, which we interpret as a soft pseudo-label. By
non-parametrically incorporating labeled samples in this way, PAWS extends the
distance-metric loss used in self-supervised methods such as BYOL and SwAV to
the semi-supervised setting. Despite the simplicity of the approach, PAWS
outperforms other semi-supervised methods across architectures, setting a new
state-of-the-art for a ResNet-50 on ImageNet trained with either 10% or 1% of
the labels, reaching 75.5% and 66.5% top-1 respectively. PAWS requires 4x to
12x less training than the previous best methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Assran_M/0/1/0/all/0/1"&gt;Mahmoud Assran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Caron_M/0/1/0/all/0/1"&gt;Mathilde Caron&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Misra_I/0/1/0/all/0/1"&gt;Ishan Misra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bojanowski_P/0/1/0/all/0/1"&gt;Piotr Bojanowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joulin_A/0/1/0/all/0/1"&gt;Armand Joulin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ballas_N/0/1/0/all/0/1"&gt;Nicolas Ballas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rabbat_M/0/1/0/all/0/1"&gt;Michael Rabbat&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Human Pose Regression with Residual Log-likelihood Estimation. (arXiv:2107.11291v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.11291</id>
        <link href="http://arxiv.org/abs/2107.11291"/>
        <updated>2021-08-03T02:06:33.329Z</updated>
        <summary type="html"><![CDATA[Heatmap-based methods dominate in the field of human pose estimation by
modelling the output distribution through likelihood heatmaps. In contrast,
regression-based methods are more efficient but suffer from inferior
performance. In this work, we explore maximum likelihood estimation (MLE) to
develop an efficient and effective regression-based methods. From the
perspective of MLE, adopting different regression losses is making different
assumptions about the output density function. A density function closer to the
true distribution leads to a better regression performance. In light of this,
we propose a novel regression paradigm with Residual Log-likelihood Estimation
(RLE) to capture the underlying output distribution. Concretely, RLE learns the
change of the distribution instead of the unreferenced underlying distribution
to facilitate the training process. With the proposed reparameterization
design, our method is compatible with off-the-shelf flow models. The proposed
method is effective, efficient and flexible. We show its potential in various
human pose estimation tasks with comprehensive experiments. Compared to the
conventional regression paradigm, regression with RLE bring 12.4 mAP
improvement on MSCOCO without any test-time overhead. Moreover, for the first
time, especially on multi-person pose estimation, our regression method is
superior to the heatmap-based methods. Our code is available at
https://github.com/Jeff-sjtu/res-loglikelihood-regression]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jiefeng Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bian_S/0/1/0/all/0/1"&gt;Siyuan Bian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_A/0/1/0/all/0/1"&gt;Ailing Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Can Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pang_B/0/1/0/all/0/1"&gt;Bo Pang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wentao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1"&gt;Cewu Lu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning with Noisy Labels via Sparse Regularization. (arXiv:2108.00192v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.00192</id>
        <link href="http://arxiv.org/abs/2108.00192"/>
        <updated>2021-08-03T02:06:33.321Z</updated>
        <summary type="html"><![CDATA[Learning with noisy labels is an important and challenging task for training
accurate deep neural networks. Some commonly-used loss functions, such as Cross
Entropy (CE), suffer from severe overfitting to noisy labels. Robust loss
functions that satisfy the symmetric condition were tailored to remedy this
problem, which however encounter the underfitting effect. In this paper, we
theoretically prove that \textbf{any loss can be made robust to noisy labels}
by restricting the network output to the set of permutations over a fixed
vector. When the fixed vector is one-hot, we only need to constrain the output
to be one-hot, which however produces zero gradients almost everywhere and thus
makes gradient-based optimization difficult. In this work, we introduce the
sparse regularization strategy to approximate the one-hot constraint, which is
composed of network output sharpening operation that enforces the output
distribution of a network to be sharp and the $\ell_p$-norm ($p\le 1$)
regularization that promotes the network output to be sparse. This simple
approach guarantees the robustness of arbitrary loss functions while not
hindering the fitting ability. Experimental results demonstrate that our method
can significantly improve the performance of commonly-used loss functions in
the presence of noisy labels and class imbalance, and outperform the
state-of-the-art methods. The code is available at
https://github.com/hitcszx/lnl_sr.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1"&gt;Xiong Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xianming Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chenyang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhai_D/0/1/0/all/0/1"&gt;Deming Zhai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1"&gt;Junjun Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_X/0/1/0/all/0/1"&gt;Xiangyang Ji&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Soft Calibration Objectives for Neural Networks. (arXiv:2108.00106v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.00106</id>
        <link href="http://arxiv.org/abs/2108.00106"/>
        <updated>2021-08-03T02:06:33.296Z</updated>
        <summary type="html"><![CDATA[Optimal decision making requires that classifiers produce uncertainty
estimates consistent with their empirical accuracy. However, deep neural
networks are often under- or over-confident in their predictions. Consequently,
methods have been developed to improve the calibration of their predictive
uncertainty both during training and post-hoc. In this work, we propose
differentiable losses to improve calibration based on a soft (continuous)
version of the binning operation underlying popular calibration-error
estimators. When incorporated into training, these soft calibration losses
achieve state-of-the-art single-model ECE across multiple datasets with less
than 1% decrease in accuracy. For instance, we observe an 82% reduction in ECE
(70% relative to the post-hoc rescaled ECE) in exchange for a 0.7% relative
decrease in accuracy relative to the cross entropy baseline on CIFAR-100. When
incorporated post-training, the soft-binning-based calibration error objective
improves upon temperature scaling, a popular recalibration method. Overall,
experiments across losses and datasets demonstrate that using
calibration-sensitive procedures yield better uncertainty estimates under
dataset shift than the standard practice of using a cross entropy loss and
post-hoc recalibration methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Karandikar_A/0/1/0/all/0/1"&gt;Archit Karandikar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cain_N/0/1/0/all/0/1"&gt;Nicholas Cain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tran_D/0/1/0/all/0/1"&gt;Dustin Tran&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lakshminarayanan_B/0/1/0/all/0/1"&gt;Balaji Lakshminarayanan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shlens_J/0/1/0/all/0/1"&gt;Jonathon Shlens&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mozer_M/0/1/0/all/0/1"&gt;Michael C. Mozer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roelofs_B/0/1/0/all/0/1"&gt;Becca Roelofs&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Feature Tracker: A Novel Application for Deep Convolutional Neural Networks. (arXiv:2108.00105v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00105</id>
        <link href="http://arxiv.org/abs/2108.00105"/>
        <updated>2021-08-03T02:06:33.289Z</updated>
        <summary type="html"><![CDATA[Feature tracking is the building block of many applications such as visual
odometry, augmented reality, and target tracking. Unfortunately, the
state-of-the-art vision-based tracking algorithms fail in surgical images due
to the challenges imposed by the nature of such environments. In this paper, we
proposed a novel and unified deep learning-based approach that can learn how to
track features reliably as well as learn how to detect such reliable features
for tracking purposes. The proposed network dubbed as Deep-PT, consists of a
tracker network which is a convolutional neural network simulating
cross-correlation in terms of deep learning and two fully connected networks
that operate on the output of intermediate layers of the tracker to detect
features and predict trackability of the detected points. The ability to detect
features based on the capabilities of the tracker distinguishes the proposed
method from previous algorithms used in this area and improves the robustness
of the algorithms against dynamics of the scene. The network is trained using
multiple datasets due to the lack of specialized dataset for feature tracking
datasets and extensive comparisons are conducted to compare the accuracy of
Deep-PT against recent pixel tracking algorithms. As the experiments suggest,
the proposed deep architecture deliberately learns what to track and how to
track and outperforms the state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Parchami_M/0/1/0/all/0/1"&gt;Mostafa Parchami&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sayed_S/0/1/0/all/0/1"&gt;Saif Iftekar Sayed&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A survey of Monte Carlo methods for noisy and costly densities with application to reinforcement learning. (arXiv:2108.00490v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.00490</id>
        <link href="http://arxiv.org/abs/2108.00490"/>
        <updated>2021-08-03T02:06:32.927Z</updated>
        <summary type="html"><![CDATA[This survey gives an overview of Monte Carlo methodologies using surrogate
models, for dealing with densities which are intractable, costly, and/or noisy.
This type of problem can be found in numerous real-world scenarios, including
stochastic optimization and reinforcement learning, where each evaluation of a
density function may incur some computationally-expensive or even physical
(real-world activity) cost, likely to give different results each time. The
surrogate model does not incur this cost, but there are important trade-offs
and considerations involved in the choice and design of such methodologies. We
classify the different methodologies into three main classes and describe
specific instances of algorithms under a unified notation. A modular scheme
which encompasses the considered methods is also presented. A range of
application scenarios is discussed, with special attention to the
likelihood-free setting and reinforcement learning. Several numerical
comparisons are also provided.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Llorente_F/0/1/0/all/0/1"&gt;F. Llorente&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Martino_L/0/1/0/all/0/1"&gt;L. Martino&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Read_J/0/1/0/all/0/1"&gt;J. Read&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Delgado_D/0/1/0/all/0/1"&gt;D. Delgado&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning who is in the market from time series: market participant discovery through adversarial calibration of multi-agent simulators. (arXiv:2108.00664v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.00664</id>
        <link href="http://arxiv.org/abs/2108.00664"/>
        <updated>2021-08-03T02:06:32.921Z</updated>
        <summary type="html"><![CDATA[In electronic trading markets often only the price or volume time series,
that result from interaction of multiple market participants, are directly
observable. In order to test trading strategies before deploying them to
real-time trading, multi-agent market environments calibrated so that the time
series that result from interaction of simulated agents resemble historical are
often used. To ensure adequate testing, one must test trading strategies in a
variety of market scenarios -- which includes both scenarios that represent
ordinary market days as well as stressed markets (most recently observed due to
the beginning of COVID pandemic). In this paper, we address the problem of
multi-agent simulator parameter calibration to allow simulator capture
characteristics of different market regimes. We propose a novel two-step method
to train a discriminator that is able to distinguish between "real" and "fake"
price and volume time series as a part of GAN with self-attention, and then
utilize it within an optimization framework to tune parameters of a simulator
model with known agent archetypes to represent a market scenario. We conclude
with experimental results that demonstrate effectiveness of our method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Storchan_V/0/1/0/all/0/1"&gt;Victor Storchan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vyetrenko_S/0/1/0/all/0/1"&gt;Svitlana Vyetrenko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balch_T/0/1/0/all/0/1"&gt;Tucker Balch&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data Driven Macroscopic Modeling across Knudsen Numbers for Rarefied Gas Dynamics and Application to Rayleigh Scattering. (arXiv:2108.00413v1 [physics.flu-dyn])]]></title>
        <id>http://arxiv.org/abs/2108.00413</id>
        <link href="http://arxiv.org/abs/2108.00413"/>
        <updated>2021-08-03T02:06:32.916Z</updated>
        <summary type="html"><![CDATA[Macroscopic modeling of the gas dynamics across Knudsen numbers from dense
gas region to rarefied gas region remains a great challenge. The reason is
macroscopic models lack accurate constitutive relations valid across different
Knudsen numbers. To address this problem, we proposed a Data-driven, KnUdsen
number Adaptive Linear constitutive relation model named DUAL. The DUAL model
is accurate across a range of Knudsen numbers, from dense to rarefied, through
learning to adapt Knudsen number change from observed data. It is consistent
with the Navier-Stokes equation under the hydrodynamic limit, by utilizing a
constrained neural network. In addition, it naturally satisfies the second law
of thermodynamics and is robust to noisy data. We test the DUAL model on the
calculation of Rayleigh scattering spectra. The DUAL model gives accurate
spectra for various Knudsen numbers and is superior to traditional perturbation
and moment expansion methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Zheng_C/0/1/0/all/0/1"&gt;Candi Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Chen_S/0/1/0/all/0/1"&gt;Shiyi Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Masking Neural Networks Using Reachability Graphs to Predict Process Events. (arXiv:2108.00404v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.00404</id>
        <link href="http://arxiv.org/abs/2108.00404"/>
        <updated>2021-08-03T02:06:32.910Z</updated>
        <summary type="html"><![CDATA[Decay Replay Mining is a deep learning method that utilizes process model
notations to predict the next event. However, this method does not intertwine
the neural network with the structure of the process model to its full extent.
This paper proposes an approach to further interlock the process model of Decay
Replay Mining with its neural network for next event prediction. The approach
uses a masking layer which is initialized based on the reachability graph of
the process model. Additionally, modifications to the neural network
architecture are proposed to increase the predictive performance. Experimental
results demonstrate the value of the approach and underscore the importance of
discovering precise and generalized process models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Theis_J/0/1/0/all/0/1"&gt;Julian Theis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Darabi_H/0/1/0/all/0/1"&gt;Houshang Darabi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Enhancing Environmental Enforcement with Near Real-Time Monitoring: Likelihood-Based Detection of Structural Expansion of Intensive Livestock Farms. (arXiv:2105.14159v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.14159</id>
        <link href="http://arxiv.org/abs/2105.14159"/>
        <updated>2021-08-03T02:06:32.905Z</updated>
        <summary type="html"><![CDATA[Much environmental enforcement in the United States has historically relied
on either self-reported data or physical, resource-intensive, infrequent
inspections. Advances in remote sensing and computer vision, however, have the
potential to augment compliance monitoring by detecting early warning signs of
noncompliance. We demonstrate a process for rapid identification of significant
structural expansion using Planet's 3m/pixel satellite imagery products and
focusing on Concentrated Animal Feeding Operations (CAFOs) in the US as a test
case. Unpermitted building expansion has been a particular challenge with
CAFOs, which pose significant health and environmental risks. Using new
hand-labeled dataset of 145,053 images of 1,513 CAFOs, we combine
state-of-the-art building segmentation with a likelihood-based change-point
detection model to provide a robust signal of building expansion (AUC = 0.86).
A major advantage of this approach is that it can work with higher cadence
(daily to weekly), but lower resolution (3m/pixel), satellite imagery than
previously used in similar environmental settings. It is also highly
generalizable and thus provides a near real-time monitoring tool to prioritize
enforcement resources in other settings where unpermitted construction poses
environmental risk, e.g. zoning, habitat modification, or wetland protection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chugg_B/0/1/0/all/0/1"&gt;Ben Chugg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anderson_B/0/1/0/all/0/1"&gt;Brandon Anderson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eicher_S/0/1/0/all/0/1"&gt;Seiji Eicher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1"&gt;Sandy Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ho_D/0/1/0/all/0/1"&gt;Daniel E. Ho&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CNN based Channel Estimation using NOMA for mmWave Massive MIMO System. (arXiv:2108.00367v1 [eess.SP])]]></title>
        <id>http://arxiv.org/abs/2108.00367</id>
        <link href="http://arxiv.org/abs/2108.00367"/>
        <updated>2021-08-03T02:06:32.896Z</updated>
        <summary type="html"><![CDATA[Non-Orthogonal Multiple Access (NOMA) schemes are being actively explored to
address some of the major challenges in 5th Generation (5G) Wireless
communications. Channel estimation is exceptionally challenging in scenarios
where NOMA schemes are integrated with millimeter wave (mmWave) massive
multiple-input multiple-output (MIMO) systems. An accurate estimation of the
channel is essential in exploiting the benefits of the pairing of the duo-NOMA
and mmWave. This paper proposes a convolutional neural network (CNN) based
approach to estimate the channel for NOMA based millimeter wave (mmWave)
massive multiple-input multiple-output (MIMO) systems built on a hybrid
architecture. Initially, users are grouped into different clusters based on
their channel gains and beamforming technique is performed to maximize the
signal in the direction of desired cluster. A coarse estimation of the channel
is first made from the received signal and this estimate is given as the input
to CNN to fine estimate the channel coefficients. Numerical illustrations show
that the proposed method outperforms least square (LS) estimate, minimum mean
square error (MMSE) estimate and are close to the Cramer-Rao Bound (CRB).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+S_A/0/1/0/all/0/1"&gt;Anu T S&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Raveendran_T/0/1/0/all/0/1"&gt;Tara Raveendran&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Realised Volatility Forecasting: Machine Learning via Financial Word Embedding. (arXiv:2108.00480v1 [q-fin.CP])]]></title>
        <id>http://arxiv.org/abs/2108.00480</id>
        <link href="http://arxiv.org/abs/2108.00480"/>
        <updated>2021-08-03T02:06:32.875Z</updated>
        <summary type="html"><![CDATA[We develop FinText, a novel, state-of-the-art, financial word embedding from
Dow Jones Newswires Text News Feed Database. Incorporating this word embedding
in a machine learning model produces a substantial increase in volatility
forecasting performance on days with volatility jumps for 23 NASDAQ stocks from
27 July 2007 to 18 November 2016. A simple ensemble model, combining our word
embedding and another machine learning model that uses limit order book data,
provides the best forecasting performance for both normal and jump volatility
days. Finally, we use Integrated Gradients and SHAP (SHapley Additive
exPlanations) to make the results more 'explainable' and the model comparisons
more transparent.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-fin/1/au:+Rahimikia_E/0/1/0/all/0/1"&gt;Eghbal Rahimikia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Zohren_S/0/1/0/all/0/1"&gt;Stefan Zohren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Poon_S/0/1/0/all/0/1"&gt;Ser-Huang Poon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Enhancing Underwater Image via Adaptive Color and Contrast Enhancement, and Denoising. (arXiv:2104.01073v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.01073</id>
        <link href="http://arxiv.org/abs/2104.01073"/>
        <updated>2021-08-03T02:06:32.870Z</updated>
        <summary type="html"><![CDATA[Images captured underwater are often characterized by low contrast, color
distortion, and noise. To address these visual degradations, we propose a novel
scheme by constructing an adaptive color and contrast enhancement, and
denoising (ACCE-D) framework for underwater image enhancement. In the proposed
framework, Difference of Gaussian (DoG) filter and bilateral filter are
respectively employed to decompose the high-frequency and low-frequency
components. Benefited from this separation, we utilize soft-thresholding
operation to suppress the noise in the high-frequency component. Specially, the
low-frequency component is enhanced by using an adaptive color and contrast
enhancement (ACCE) strategy. The proposed ACCE is an adaptive variational
framework implemented in the HSI color space, which integrates data term and
regularized term, as well as introduces Gaussian weight and Heaviside function
to avoid over-enhancement and oversaturation. Moreover, we derive a numerical
solution for ACCE, and adopt a pyramid-based strategy to accelerate the solving
procedure. Experimental results demonstrate that our strategy is effective in
color correction, visibility improvement, and detail revealing. Comparison with
state-of-the-art techniques also validate the superiority of proposed method.
Furthermore, we have verified the utility of our proposed ACCE-D for enhancing
other types of degraded scenes, including foggy scene, sandstorm scene and
low-light scene.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xinjie Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hou_G/0/1/0/all/0/1"&gt;Guojia Hou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1"&gt;Kunqian Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pan_Z/0/1/0/all/0/1"&gt;Zhenkuan Pan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Synthetic Active Distribution System Generation via Unbalanced Graph Generative Adversarial Network. (arXiv:2108.00599v1 [eess.SY])]]></title>
        <id>http://arxiv.org/abs/2108.00599</id>
        <link href="http://arxiv.org/abs/2108.00599"/>
        <updated>2021-08-03T02:06:32.865Z</updated>
        <summary type="html"><![CDATA[Real active distribution networks with associated smart meter (SM) data are
critical for power researchers. However, it is practically difficult for
researchers to obtain such comprehensive datasets from utilities due to privacy
concerns. To bridge this gap, an implicit generative model with Wasserstein GAN
objectives, namely unbalanced graph generative adversarial network (UG-GAN), is
designed to generate synthetic three-phase unbalanced active distribution
system connectivity. The basic idea is to learn the distribution of random
walks both over a real-world system and across each phase of line segments,
capturing the underlying local properties of an individual real-world
distribution network and generating specific synthetic networks accordingly.
Then, to create a comprehensive synthetic test case, a network correction and
extension process is proposed to obtain time-series nodal demands and standard
distribution grid components with realistic parameters, including distributed
energy resources (DERs) and capacity banks. A Midwest distribution system with
1-year SM data has been utilized to validate the performance of our method.
Case studies with several power applications demonstrate that synthetic active
networks generated by the proposed framework can mimic almost all features of
real-world networks while avoiding the disclosure of confidential information.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Yan_R/0/1/0/all/0/1"&gt;Rong Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yuan_Y/0/1/0/all/0/1"&gt;Yuxuan Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhaoyu Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Geng_G/0/1/0/all/0/1"&gt;Guangchao Geng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jiang_Q/0/1/0/all/0/1"&gt;Quanyuan Jiang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DECAF: Deep Extreme Classification with Label Features. (arXiv:2108.00368v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.00368</id>
        <link href="http://arxiv.org/abs/2108.00368"/>
        <updated>2021-08-03T02:06:32.859Z</updated>
        <summary type="html"><![CDATA[Extreme multi-label classification (XML) involves tagging a data point with
its most relevant subset of labels from an extremely large label set, with
several applications such as product-to-product recommendation with millions of
products. Although leading XML algorithms scale to millions of labels, they
largely ignore label meta-data such as textual descriptions of the labels. On
the other hand, classical techniques that can utilize label metadata via
representation learning using deep networks struggle in extreme settings. This
paper develops the DECAF algorithm that addresses these challenges by learning
models enriched by label metadata that jointly learn model parameters and
feature representations using deep networks and offer accurate classification
at the scale of millions of labels. DECAF makes specific contributions to model
architecture design, initialization, and training, enabling it to offer up to
2-6% more accurate prediction than leading extreme classifiers on publicly
available benchmark product-to-product recommendation datasets, such as
LF-AmazonTitles-1.3M. At the same time, DECAF was found to be up to 22x faster
at inference than leading deep extreme classifiers, which makes it suitable for
real-time applications that require predictions within a few milliseconds. The
code for DECAF is available at the following URL
https://github.com/Extreme-classification/DECAF.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mittal_A/0/1/0/all/0/1"&gt;Anshul Mittal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dahiya_K/0/1/0/all/0/1"&gt;Kunal Dahiya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agrawal_S/0/1/0/all/0/1"&gt;Sheshansh Agrawal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saini_D/0/1/0/all/0/1"&gt;Deepak Saini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1"&gt;Sumeet Agarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kar_P/0/1/0/all/0/1"&gt;Purushottam Kar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Varma_M/0/1/0/all/0/1"&gt;Manik Varma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Data-driven model for hydraulic fracturing design optimization. Part II: Inverse problem. (arXiv:2108.00751v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.00751</id>
        <link href="http://arxiv.org/abs/2108.00751"/>
        <updated>2021-08-03T02:06:32.852Z</updated>
        <summary type="html"><![CDATA[We describe a stacked model for predicting the cumulative fluid production
for an oil well with a multistage-fracture completion based on a combination of
Ridge Regression and CatBoost algorithms. The model is developed based on an
extended digital field data base of reservoir, well and fracturing design
parameters. The database now includes more than 5000 wells from 23 oilfields of
Western Siberia (Russia), with 6687 fracturing operations in total. Starting
with 387 parameters characterizing each well, including construction, reservoir
properties, fracturing design features and production, we end up with 38 key
parameters used as input features for each well in the model training process.
The model demonstrates physically explainable dependencies plots of the target
on the design parameters (number of stages, proppant mass, average and final
proppant concentrations and fluid rate). We developed a set of methods
including those based on the use of Euclidean distance and clustering
techniques to perform similar (offset) wells search, which is useful for a
field engineer to analyze earlier fracturing treatments on similar wells. These
approaches are also adapted for obtaining the optimization parameters
boundaries for the particular pilot well, as part of the field testing campaign
of the methodology. An inverse problem (selecting an optimum set of fracturing
design parameters to maximize production) is formulated as optimizing a high
dimensional black box approximation function constrained by boundaries and
solved with four different optimization methods: surrogate-based optimization,
sequential least squares programming, particle swarm optimization and
differential evolution. A recommendation system containing all the above
methods is designed to advise a production stimulation engineer on an optimized
fracturing design.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Duplyakov_V/0/1/0/all/0/1"&gt;Viktor Duplyakov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morozov_A/0/1/0/all/0/1"&gt;Anton Morozov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Popkov_D/0/1/0/all/0/1"&gt;Dmitriy Popkov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shel_E/0/1/0/all/0/1"&gt;Egor Shel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vainshtein_A/0/1/0/all/0/1"&gt;Albert Vainshtein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Burnaev_E/0/1/0/all/0/1"&gt;Evgeny Burnaev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Osiptsov_A/0/1/0/all/0/1"&gt;Andrei Osiptsov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paderin_G/0/1/0/all/0/1"&gt;Grigory Paderin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep graph matching meets mixed-integer linear programming: Relax at your own risk ?. (arXiv:2108.00394v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00394</id>
        <link href="http://arxiv.org/abs/2108.00394"/>
        <updated>2021-08-03T02:06:32.836Z</updated>
        <summary type="html"><![CDATA[Graph matching is an important problem that has received widespread
attention, especially in the field of computer vision. Recently,
state-of-the-art methods seek to incorporate graph matching with deep learning.
However, there is no research to explain what role the graph matching algorithm
plays in the model. Therefore, we propose an approach integrating a MILP
formulation of the graph matching problem. This formulation is solved to
optimal and it provides inherent baseline. Meanwhile, similar approaches are
derived by releasing the optimal guarantee of the graph matching solver and by
introducing a quality level. This quality level controls the quality of the
solutions provided by the graph matching solver. In addition, several
relaxations of the graph matching problem are put to the test. Our experimental
evaluation gives several theoretical insights and guides the direction of deep
graph matching methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zhoubo Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1"&gt;Puqing Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raveaux_R/0/1/0/all/0/1"&gt;Romain Raveaux&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Huadong Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Accelerating Markov Random Field Inference with Uncertainty Quantification. (arXiv:2108.00570v1 [cs.AR])]]></title>
        <id>http://arxiv.org/abs/2108.00570</id>
        <link href="http://arxiv.org/abs/2108.00570"/>
        <updated>2021-08-03T02:06:32.830Z</updated>
        <summary type="html"><![CDATA[Statistical machine learning has widespread application in various domains.
These methods include probabilistic algorithms, such as Markov Chain
Monte-Carlo (MCMC), which rely on generating random numbers from probability
distributions. These algorithms are computationally expensive on conventional
processors, yet their statistical properties, namely interpretability and
uncertainty quantification (UQ) compared to deep learning, make them an
attractive alternative approach. Therefore, hardware specialization can be
adopted to address the shortcomings of conventional processors in running these
applications.

In this paper, we propose a high-throughput accelerator for Markov Random
Field (MRF) inference, a powerful model for representing a wide range of
applications, using MCMC with Gibbs sampling. We propose a tiled architecture
which takes advantage of near-memory computing, and memory optimizations
tailored to the semantics of MRF. Additionally, we propose a novel hybrid
on-chip/off-chip memory system and logging scheme to efficiently support UQ.
This memory system design is not specific to MRF models and is applicable to
applications using probabilistic algorithms. In addition, it dramatically
reduces off-chip memory bandwidth requirements.

We implemented an FPGA prototype of our proposed architecture using
high-level synthesis tools and achieved 146MHz frequency for an accelerator
with 32 function units on an Intel Arria 10 FPGA. Compared to prior work on
FPGA, our accelerator achieves 26X speedup. Furthermore, our proposed memory
system and logging scheme to support UQ reduces off-chip bandwidth by 71% for
two applications. ASIC analysis in 15nm shows our design with 2048 function
units running at 3GHz outperforms GPU implementations of motion estimation and
stereo vision on Nvidia RTX2080Ti by 120X-210X, occupying only 7.7% of the
area.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bashizade_R/0/1/0/all/0/1"&gt;Ramin Bashizade&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiangyu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mukherjee_S/0/1/0/all/0/1"&gt;Sayan Mukherjee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lebeck_A/0/1/0/all/0/1"&gt;Alvin R. Lebeck&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Explainable Deep Few-shot Anomaly Detection with Deviation Networks. (arXiv:2108.00462v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00462</id>
        <link href="http://arxiv.org/abs/2108.00462"/>
        <updated>2021-08-03T02:06:32.820Z</updated>
        <summary type="html"><![CDATA[Existing anomaly detection paradigms overwhelmingly focus on training
detection models using exclusively normal data or unlabeled data (mostly normal
samples). One notorious issue with these approaches is that they are weak in
discriminating anomalies from normal samples due to the lack of the knowledge
about the anomalies. Here, we study the problem of few-shot anomaly detection,
in which we aim at using a few labeled anomaly examples to train
sample-efficient discriminative detection models. To address this problem, we
introduce a novel weakly-supervised anomaly detection framework to train
detection models without assuming the examples illustrating all possible
classes of anomaly.

Specifically, the proposed approach learns discriminative normality
(regularity) by leveraging the labeled anomalies and a prior probability to
enforce expressive representations of normality and unbounded deviated
representations of abnormality. This is achieved by an end-to-end optimization
of anomaly scores with a neural deviation learning, in which the anomaly scores
of normal samples are imposed to approximate scalar scores drawn from the prior
while that of anomaly examples is enforced to have statistically significant
deviations from these sampled scores in the upper tail. Furthermore, our model
is optimized to learn fine-grained normality and abnormality by top-K
multiple-instance-learning-based feature subspace deviation learning, allowing
more generalized representations. Comprehensive experiments on nine real-world
image anomaly detection benchmarks show that our model is substantially more
sample-efficient and robust, and performs significantly better than
state-of-the-art competing methods in both closed-set and open-set settings.
Our model can also offer explanation capability as a result of its prior-driven
anomaly score learning. Code and datasets are available at:
https://git.io/DevNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pang_G/0/1/0/all/0/1"&gt;Guansong Pang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1"&gt;Choubo Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1"&gt;Chunhua Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hengel_A/0/1/0/all/0/1"&gt;Anton van den Hengel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bucketed PCA Neural Networks with Neurons Mirroring Signals. (arXiv:2108.00605v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.00605</id>
        <link href="http://arxiv.org/abs/2108.00605"/>
        <updated>2021-08-03T02:06:32.814Z</updated>
        <summary type="html"><![CDATA[The bucketed PCA neural network (PCA-NN) with transforms is developed here in
an effort to benchmark deep neural networks (DNN's), for problems on supervised
classification. Most classical PCA models apply PCA to the entire training data
set to establish a reductive representation and then employ non-network tools
such as high-order polynomial classifiers. In contrast, the bucketed PCA-NN
applies PCA to individual buckets which are constructed in two consecutive
phases, as well as retains a genuine architecture of a neural network. This
facilitates a fair apple-to-apple comparison to DNN's, esp. to reveal that a
major chunk of accuracy achieved by many impressive DNN's could possibly be
explained by the bucketed PCA-NN (e.g., 96% out of 98% for the MNIST data set
as an example). Compared with most DNN's, the three building blocks of the
bucketed PCA-NN are easier to comprehend conceptually - PCA, transforms, and
bucketing for error correction. Furthermore, unlike the somewhat quasi-random
neurons ubiquitously observed in DNN's, the PCA neurons resemble or mirror the
input signals and are more straightforward to decipher as a result.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1"&gt;Jackie Shen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BadEncoder: Backdoor Attacks to Pre-trained Encoders in Self-Supervised Learning. (arXiv:2108.00352v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2108.00352</id>
        <link href="http://arxiv.org/abs/2108.00352"/>
        <updated>2021-08-03T02:06:32.809Z</updated>
        <summary type="html"><![CDATA[Self-supervised learning in computer vision aims to pre-train an image
encoder using a large amount of unlabeled images or (image, text) pairs. The
pre-trained image encoder can then be used as a feature extractor to build
downstream classifiers for many downstream tasks with a small amount of or no
labeled training data. In this work, we propose BadEncoder, the first backdoor
attack to self-supervised learning. In particular, our BadEncoder injects
backdoors into a pre-trained image encoder such that the downstream classifiers
built based on the backdoored image encoder for different downstream tasks
simultaneously inherit the backdoor behavior. We formulate our BadEncoder as an
optimization problem and we propose a gradient descent based method to solve
it, which produces a backdoored image encoder from a clean one. Our extensive
empirical evaluation results on multiple datasets show that our BadEncoder
achieves high attack success rates while preserving the accuracy of the
downstream classifiers. We also show the effectiveness of BadEncoder using two
publicly available, real-world image encoders, i.e., Google's image encoder
pre-trained on ImageNet and OpenAI's Contrastive Language-Image Pre-training
(CLIP) image encoder pre-trained on 400 million (image, text) pairs collected
from the Internet. Moreover, we consider defenses including Neural Cleanse and
MNTD (empirical defenses) as well as PatchGuard (a provable defense). Our
results show that these defenses are insufficient to defend against BadEncoder,
highlighting the needs for new defenses against our BadEncoder. Our code is
publicly available at: https://github.com/jjy1994/BadEncoder.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1"&gt;Jinyuan Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yupei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_N/0/1/0/all/0/1"&gt;Neil Zhenqiang Gong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[STEP: Spatial Temporal Graph Convolutional Networks for Emotion Perception from Gaits. (arXiv:1910.12906v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1910.12906</id>
        <link href="http://arxiv.org/abs/1910.12906"/>
        <updated>2021-08-03T02:06:32.791Z</updated>
        <summary type="html"><![CDATA[We present a novel classifier network called STEP, to classify perceived
human emotion from gaits, based on a Spatial Temporal Graph Convolutional
Network (ST-GCN) architecture. Given an RGB video of an individual walking, our
formulation implicitly exploits the gait features to classify the emotional
state of the human into one of four emotions: happy, sad, angry, or neutral. We
use hundreds of annotated real-world gait videos and augment them with
thousands of annotated synthetic gaits generated using a novel generative
network called STEP-Gen, built on an ST-GCN based Conditional Variational
Autoencoder (CVAE). We incorporate a novel push-pull regularization loss in the
CVAE formulation of STEP-Gen to generate realistic gaits and improve the
classification accuracy of STEP. We also release a novel dataset (E-Gait),
which consists of $2,177$ human gaits annotated with perceived emotions along
with thousands of synthetic gaits. In practice, STEP can learn the affective
features and exhibits classification accuracy of 89% on E-Gait, which is 14 -
30% more accurate over prior methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bhattacharya_U/0/1/0/all/0/1"&gt;Uttaran Bhattacharya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mittal_T/0/1/0/all/0/1"&gt;Trisha Mittal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chandra_R/0/1/0/all/0/1"&gt;Rohan Chandra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Randhavane_T/0/1/0/all/0/1"&gt;Tanmay Randhavane&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bera_A/0/1/0/all/0/1"&gt;Aniket Bera&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Manocha_D/0/1/0/all/0/1"&gt;Dinesh Manocha&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Robust Object Detection: Bayesian RetinaNet for Homoscedastic Aleatoric Uncertainty Modeling. (arXiv:2108.00784v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00784</id>
        <link href="http://arxiv.org/abs/2108.00784"/>
        <updated>2021-08-03T02:06:32.784Z</updated>
        <summary type="html"><![CDATA[According to recent studies, commonly used computer vision datasets contain
about 4% of label errors. For example, the COCO dataset is known for its high
level of noise in data labels, which limits its use for training robust neural
deep architectures in a real-world scenario. To model such a noise, in this
paper we have proposed the homoscedastic aleatoric uncertainty estimation, and
present a series of novel loss functions to address the problem of image object
detection at scale. Specifically, the proposed functions are based on Bayesian
inference and we have incorporated them into the common community-adopted
object detection deep learning architecture RetinaNet. We have also shown that
modeling of homoscedastic aleatoric uncertainty using our novel functions
allows to increase the model interpretability and to improve the object
detection performance being evaluated on the COCO dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khanzhina_N/0/1/0/all/0/1"&gt;Natalia Khanzhina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lapenok_A/0/1/0/all/0/1"&gt;Alexey Lapenok&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Filchenkov_A/0/1/0/all/0/1"&gt;Andrey Filchenkov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Machine-Learning-Based Direction-of-Origin Filter for the Identification of Radio Frequency Interference in the Search for Technosignatures. (arXiv:2108.00559v1 [astro-ph.IM])]]></title>
        <id>http://arxiv.org/abs/2108.00559</id>
        <link href="http://arxiv.org/abs/2108.00559"/>
        <updated>2021-08-03T02:06:32.778Z</updated>
        <summary type="html"><![CDATA[Radio frequency interference (RFI) mitigation remains a major challenge in
the search for radio technosignatures. Typical mitigation strategies include a
direction-of-origin (DoO) filter, where a signal is classified as RFI if it is
detected in multiple directions on the sky. These classifications generally
rely on estimates of signal properties, such as frequency and frequency drift
rate. Convolutional neural networks (CNNs) offer a promising complement to
existing filters because they can be trained to analyze dynamic spectra
directly, instead of relying on inferred signal properties. In this work, we
compiled several data sets consisting of labeled pairs of images of dynamic
spectra, and we designed and trained a CNN that can determine whether or not a
signal detected in one scan is also present in another scan. This CNN-based DoO
filter outperforms both a baseline 2D correlation model as well as existing DoO
filters over a range of metrics, with precision and recall values of 99.15% and
97.81%, respectively. We found that the CNN reduces the number of signals
requiring visual inspection after the application of traditional DoO filters by
a factor of 6-16 in nominal situations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/astro-ph/1/au:+Pinchuk_P/0/1/0/all/0/1"&gt;Pavlo Pinchuk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/astro-ph/1/au:+Margot_J/0/1/0/all/0/1"&gt;Jean-Luc Margot&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transformer-based Map Matching with Model Limited Ground-Truth Data using Transfer-Learning Approach. (arXiv:2108.00439v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.00439</id>
        <link href="http://arxiv.org/abs/2108.00439"/>
        <updated>2021-08-03T02:06:32.771Z</updated>
        <summary type="html"><![CDATA[In many trajectory-based applications, it is necessary to map raw GPS
trajectories onto road networks in digital maps, which is commonly referred to
as a map-matching process. While most previous map-matching methods have
focused on using rule-based algorithms to deal with the map-matching problems,
in this paper, we consider the map-matching task from the data perspective,
proposing a deep learning-based map-matching model. We build a
Transformer-based map-matching model with a transfer learning approach. We
generate synthetic trajectory data to pre-train the Transformer model and then
fine-tune the model with a limited number of ground-truth data to minimize the
model development cost and reduce the real-to-virtual gap. Three metrics
(Average Hamming Distance, F-score, and BLEU) at two levels (point and segment
level) are used to evaluate the model performance. The results indicate that
the proposed model outperforms existing models. Furthermore, we use the
attention weights of the Transformer to plot the map-matching process and find
how the model matches the road segments correctly.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1"&gt;Zhixiong Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choi_S/0/1/0/all/0/1"&gt;Seongjin Choi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yeo_H/0/1/0/all/0/1"&gt;Hwasoo Yeo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Chest ImaGenome Dataset for Clinical Reasoning. (arXiv:2108.00316v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00316</id>
        <link href="http://arxiv.org/abs/2108.00316"/>
        <updated>2021-08-03T02:06:32.765Z</updated>
        <summary type="html"><![CDATA[Despite the progress in automatic detection of radiologic findings from chest
X-ray (CXR) images in recent years, a quantitative evaluation of the
explainability of these models is hampered by the lack of locally labeled
datasets for different findings. With the exception of a few expert-labeled
small-scale datasets for specific findings, such as pneumonia and pneumothorax,
most of the CXR deep learning models to date are trained on global "weak"
labels extracted from text reports, or trained via a joint image and
unstructured text learning strategy. Inspired by the Visual Genome effort in
the computer vision community, we constructed the first Chest ImaGenome dataset
with a scene graph data structure to describe $242,072$ images. Local
annotations are automatically produced using a joint rule-based natural
language processing (NLP) and atlas-based bounding box detection pipeline.
Through a radiologist constructed CXR ontology, the annotations for each CXR
are connected as an anatomy-centered scene graph, useful for image-level
reasoning and multimodal fusion applications. Overall, we provide: i) $1,256$
combinations of relation annotations between $29$ CXR anatomical locations
(objects with bounding box coordinates) and their attributes, structured as a
scene graph per image, ii) over $670,000$ localized comparison relations (for
improved, worsened, or no change) between the anatomical locations across
sequential exams, as well as ii) a manually annotated gold standard scene graph
dataset from $500$ unique patients.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1"&gt;Joy T. Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agu_N/0/1/0/all/0/1"&gt;Nkechinyere N. Agu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lourentzou_I/0/1/0/all/0/1"&gt;Ismini Lourentzou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1"&gt;Arjun Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paguio_J/0/1/0/all/0/1"&gt;Joseph A. Paguio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1"&gt;Jasper S. Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dee_E/0/1/0/all/0/1"&gt;Edward C. Dee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mitchell_W/0/1/0/all/0/1"&gt;William Mitchell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kashyap_S/0/1/0/all/0/1"&gt;Satyananda Kashyap&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Giovannini_A/0/1/0/all/0/1"&gt;Andrea Giovannini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Celi_L/0/1/0/all/0/1"&gt;Leo A. Celi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moradi_M/0/1/0/all/0/1"&gt;Mehdi Moradi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DeepTrack: Lightweight Deep Learning for Vehicle Path Prediction in Highways. (arXiv:2108.00505v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.00505</id>
        <link href="http://arxiv.org/abs/2108.00505"/>
        <updated>2021-08-03T02:06:32.757Z</updated>
        <summary type="html"><![CDATA[Vehicle trajectory prediction is an essential task for enabling many
intelligent transportation systems. While there have been some promising
advances in the field, there is a need for new agile algorithms with smaller
model sizes and lower computational requirements. This article presents
DeepTrack, a novel deep learning algorithm customized for real-time vehicle
trajectory prediction in highways. In contrast to previous methods, the vehicle
dynamics are encoded using Agile Temporal Convolutional Networks (ATCNs) to
provide more robust time prediction with less computation. ATCN also uses
depthwise convolution, which reduces the complexity of models compared to
existing approaches in terms of model size and operations. Overall, our
experimental results demonstrate that DeepTrack achieves comparable accuracy to
state-of-the-art trajectory prediction models but with smaller model sizes and
lower computational complexity, making it more suitable for real-world
deployment.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Baharani_M/0/1/0/all/0/1"&gt;Mohammadreza Baharani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Katariya_V/0/1/0/all/0/1"&gt;Vinit Katariya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Morris_N/0/1/0/all/0/1"&gt;Nichole Morris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shoghli_O/0/1/0/all/0/1"&gt;Omidreza Shoghli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tabkhi_H/0/1/0/all/0/1"&gt;Hamed Tabkhi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Angle Based Feature Learning in GNN for 3D Object Detection using Point Cloud. (arXiv:2108.00780v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00780</id>
        <link href="http://arxiv.org/abs/2108.00780"/>
        <updated>2021-08-03T02:06:32.734Z</updated>
        <summary type="html"><![CDATA[In this paper, we present new feature encoding methods for Detection of 3D
objects in point clouds. We used a graph neural network (GNN) for Detection of
3D objects namely cars, pedestrians, and cyclists. Feature encoding is one of
the important steps in Detection of 3D objects. The dataset used is point cloud
data which is irregular and unstructured and it needs to be encoded in such a
way that ensures better feature encapsulation. Earlier works have used relative
distance as one of the methods to encode the features. These methods are not
resistant to rotation variance problems in Graph Neural Networks. We have
included angular-based measures while performing feature encoding in graph
neural networks. Along with that, we have performed a comparison between other
methods like Absolute, Relative, Euclidean distances, and a combination of the
Angle and Relative methods. The model is trained and evaluated on the subset of
the KITTI object detection benchmark dataset under resource constraints. Our
results demonstrate that a combination of angle measures and relative distance
has performed better than other methods. In comparison to the baseline
method(relative), it achieved better performance. We also performed time
analysis of various feature encoding methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ansari_M/0/1/0/all/0/1"&gt;Md Afzal Ansari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meraz_M/0/1/0/all/0/1"&gt;Md Meraz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chakraborty_P/0/1/0/all/0/1"&gt;Pavan Chakraborty&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Javed_M/0/1/0/all/0/1"&gt;Mohammed Javed&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CARLA: A Python Library to Benchmark Algorithmic Recourse and Counterfactual Explanation Algorithms. (arXiv:2108.00783v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.00783</id>
        <link href="http://arxiv.org/abs/2108.00783"/>
        <updated>2021-08-03T02:06:32.727Z</updated>
        <summary type="html"><![CDATA[Counterfactual explanations provide means for prescriptive model explanations
by suggesting actionable feature changes (e.g., increase income) that allow
individuals to achieve favorable outcomes in the future (e.g., insurance
approval). Choosing an appropriate method is a crucial aspect for meaningful
counterfactual explanations. As documented in recent reviews, there exists a
quickly growing literature with available methods. Yet, in the absence of
widely available opensource implementations, the decision in favor of certain
models is primarily based on what is readily available. Going forward - to
guarantee meaningful comparisons across explanation methods - we present CARLA
(Counterfactual And Recourse LibrAry), a python library for benchmarking
counterfactual explanation methods across both different data sets and
different machine learning models. In summary, our work provides the following
contributions: (i) an extensive benchmark of 11 popular counterfactual
explanation methods, (ii) a benchmarking framework for research on future
counterfactual explanation methods, and (iii) a standardized set of integrated
evaluation measures and data sets for transparent and extensive comparisons of
these methods. We have open-sourced CARLA and our experimental results on
Github, making them available as competitive baselines. We welcome
contributions from other research groups and practitioners.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pawelczyk_M/0/1/0/all/0/1"&gt;Martin Pawelczyk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bielawski_S/0/1/0/all/0/1"&gt;Sascha Bielawski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heuvel_J/0/1/0/all/0/1"&gt;Johannes van den Heuvel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Richter_T/0/1/0/all/0/1"&gt;Tobias Richter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kasneci_G/0/1/0/all/0/1"&gt;Gjergji Kasneci&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Control Direct Current Motor for Steering in Real Time via Reinforcement Learning. (arXiv:2108.00138v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.00138</id>
        <link href="http://arxiv.org/abs/2108.00138"/>
        <updated>2021-08-03T02:06:32.721Z</updated>
        <summary type="html"><![CDATA[Model free techniques have been successful at optimal control of complex
systems at an expense of copious amounts of data and computation. However, it
is often desired to obtain a control policy in a short period of time with
minimal data use and computational burden. To this end, we make use of the NFQ
algorithm for steering position control of a golf cart in both a real hardware
and a simulated environment that was built from real-world interaction. The
controller learns to apply a sequence of voltage signals in the presence of
environmental uncertainties and inherent non-linearities that challenge the the
control task. We were able to increase the rate of successful control under
four minutes in simulation and under 11 minutes in real hardware.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Watson_T/0/1/0/all/0/1"&gt;Thomas Watson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Poudel_B/0/1/0/all/0/1"&gt;Bibek Poudel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Distributed Learning for Time-varying Networks: A Scalable Design. (arXiv:2108.00231v1 [cs.IT])]]></title>
        <id>http://arxiv.org/abs/2108.00231</id>
        <link href="http://arxiv.org/abs/2108.00231"/>
        <updated>2021-08-03T02:06:32.711Z</updated>
        <summary type="html"><![CDATA[The wireless network is undergoing a trend from "onnection of things" to
"connection of intelligence". With data spread over the communication networks
and computing capability enhanced on the devices, distributed learning becomes
a hot topic in both industrial and academic communities. Many frameworks, such
as federated learning and federated distillation, have been proposed. However,
few of them takes good care of obstacles such as the time-varying topology
resulted by the characteristics of wireless networks. In this paper, we propose
a distributed learning framework based on a scalable deep neural network (DNN)
design. By exploiting the permutation equivalence and invariance properties of
the learning tasks, the DNNs with different scales for different clients can be
built up based on two basic parameter sub-matrices. Further, model aggregation
can also be conducted based on these two sub-matrices to improve the learning
convergence and performance. Finally, simulation results verify the benefits of
the proposed framework by compared with some baselines.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jian Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huangfu_Y/0/1/0/all/0/1"&gt;Yourui Huangfu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1"&gt;Rong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1"&gt;Yiqun Ge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jun Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Provably Efficient Lottery Ticket Discovery. (arXiv:2108.00259v1 [stat.ML])]]></title>
        <id>http://arxiv.org/abs/2108.00259</id>
        <link href="http://arxiv.org/abs/2108.00259"/>
        <updated>2021-08-03T02:06:32.704Z</updated>
        <summary type="html"><![CDATA[The lottery ticket hypothesis (LTH) claims that randomly-initialized, dense
neural networks contain (sparse) subnetworks that, when trained an equal amount
in isolation, can match the dense network's performance. Although LTH is useful
for discovering efficient network architectures, its three-step process --
pre-training, pruning, and re-training -- is computationally expensive, as the
dense model must be fully pre-trained. Luckily, "early-bird" tickets can be
discovered within neural networks that are minimally pre-trained, allowing for
the creation of efficient, LTH-inspired training procedures. Yet, no
theoretical foundation of this phenomenon exists. We derive an analytical bound
for the number of pre-training iterations that must be performed for a winning
ticket to be discovered, thus providing a theoretical understanding of when and
why such early-bird tickets exist. By adopting a greedy forward selection
pruning strategy, we directly connect the pruned network's performance to the
loss of the dense network from which it was derived, revealing a threshold in
the number of pre-training iterations beyond which high-performing subnetworks
are guaranteed to exist. We demonstrate the validity of our theoretical results
across a variety of architectures and datasets, including multi-layer
perceptrons (MLPs) trained on MNIST and several deep convolutional neural
network (CNN) architectures trained on CIFAR10 and ImageNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/stat/1/au:+Wolfe_C/0/1/0/all/0/1"&gt;Cameron R. Wolfe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Wang_Q/0/1/0/all/0/1"&gt;Qihan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kim_J/0/1/0/all/0/1"&gt;Junhyung Lyle Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/stat/1/au:+Kyrillidis_A/0/1/0/all/0/1"&gt;Anastasios Kyrillidis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Adaptive t-Momentum-based Optimization for Unknown Ratio of Outliers in Amateur Data in Imitation Learning. (arXiv:2108.00625v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.00625</id>
        <link href="http://arxiv.org/abs/2108.00625"/>
        <updated>2021-08-03T02:06:32.682Z</updated>
        <summary type="html"><![CDATA[Behavioral cloning (BC) bears a high potential for safe and direct transfer
of human skills to robots. However, demonstrations performed by human operators
often contain noise or imperfect behaviors that can affect the efficiency of
the imitator if left unchecked. In order to allow the imitators to effectively
learn from imperfect demonstrations, we propose to employ the robust t-momentum
optimization algorithm. This algorithm builds on the Student's t-distribution
in order to deal with heavy-tailed data and reduce the effect of outlying
observations. We extend the t-momentum algorithm to allow for an adaptive and
automatic robustness and show empirically how the algorithm can be used to
produce robust BC imitators against datasets with unknown heaviness. Indeed,
the imitators trained with the t-momentum-based Adam optimizers displayed
robustness to imperfect demonstrations on two different manipulation tasks with
different robots and revealed the capability to take advantage of the
additional data while reducing the adverse effect of non-optimal behaviors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ilboudo_W/0/1/0/all/0/1"&gt;Wendyam Eric Lionel Ilboudo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kobayashi_T/0/1/0/all/0/1"&gt;Taisuke Kobayashi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sugimoto_K/0/1/0/all/0/1"&gt;Kenji Sugimoto&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Faster Rates of Differentially Private Stochastic Convex Optimization. (arXiv:2108.00331v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.00331</id>
        <link href="http://arxiv.org/abs/2108.00331"/>
        <updated>2021-08-03T02:06:32.675Z</updated>
        <summary type="html"><![CDATA[In this paper, we revisit the problem of Differentially Private Stochastic
Convex Optimization (DP-SCO) and provide excess population risks for some
special classes of functions that are faster than the previous results of
general convex and strongly convex functions. In the first part of the paper,
we study the case where the population risk function satisfies the Tysbakov
Noise Condition (TNC) with some parameter $\theta>1$. Specifically, we first
show that under some mild assumptions on the loss functions, there is an
algorithm whose output could achieve an upper bound of
$\tilde{O}((\frac{1}{\sqrt{n}}+\frac{\sqrt{d\log
\frac{1}{\delta}}}{n\epsilon})^\frac{\theta}{\theta-1})$ for $(\epsilon,
\delta)$-DP when $\theta\geq 2$, here $n$ is the sample size and $d$ is the
dimension of the space. Then we address the inefficiency issue, improve the
upper bounds by $\text{Poly}(\log n)$ factors and extend to the case where
$\theta\geq \bar{\theta}>1$ for some known $\bar{\theta}$. Next we show that
the excess population risk of population functions satisfying TNC with
parameter $\theta>1$ is always lower bounded by
$\Omega((\frac{d}{n\epsilon})^\frac{\theta}{\theta-1}) $ and
$\Omega((\frac{\sqrt{d\log
\frac{1}{\delta}}}{n\epsilon})^\frac{\theta}{\theta-1})$ for $\epsilon$-DP and
$(\epsilon, \delta)$-DP, respectively. In the second part, we focus on a
special case where the population risk function is strongly convex. Unlike the
previous studies, here we assume the loss function is {\em non-negative} and
{\em the optimal value of population risk is sufficiently small}. With these
additional assumptions, we propose a new method whose output could achieve an
upper bound of
$O(\frac{d\log\frac{1}{\delta}}{n^2\epsilon^2}+\frac{1}{n^{\tau}})$ for any
$\tau\geq 1$ in $(\epsilon,\delta)$-DP model if the sample size $n$ is
sufficiently large.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1"&gt;Jinyan Su&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1"&gt;Di Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[One Million Scenes for Autonomous Driving: ONCE Dataset. (arXiv:2106.11037v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.11037</id>
        <link href="http://arxiv.org/abs/2106.11037"/>
        <updated>2021-08-03T02:06:32.661Z</updated>
        <summary type="html"><![CDATA[Current perception models in autonomous driving have become notorious for
greatly relying on a mass of annotated data to cover unseen cases and address
the long-tail problem. On the other hand, learning from unlabeled large-scale
collected data and incrementally self-training powerful recognition models have
received increasing attention and may become the solutions of next-generation
industry-level powerful and robust perception models in autonomous driving.
However, the research community generally suffered from data inadequacy of
those essential real-world scene data, which hampers the future exploration of
fully/semi/self-supervised methods for 3D perception. In this paper, we
introduce the ONCE (One millioN sCenEs) dataset for 3D object detection in the
autonomous driving scenario. The ONCE dataset consists of 1 million LiDAR
scenes and 7 million corresponding camera images. The data is selected from 144
driving hours, which is 20x longer than the largest 3D autonomous driving
dataset available (e.g. nuScenes and Waymo), and it is collected across a range
of different areas, periods and weather conditions. To facilitate future
research on exploiting unlabeled data for 3D detection, we additionally provide
a benchmark in which we reproduce and evaluate a variety of self-supervised and
semi-supervised methods on the ONCE dataset. We conduct extensive analyses on
those methods and provide valuable observations on their performance related to
the scale of used data. Data, code, and more information are available at
https://once-for-auto-driving.github.io/index.html.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mao_J/0/1/0/all/0/1"&gt;Jiageng Mao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niu_M/0/1/0/all/0/1"&gt;Minzhe Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1"&gt;Chenhan Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_H/0/1/0/all/0/1"&gt;Hanxue Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jingheng Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1"&gt;Xiaodan Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yamin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_C/0/1/0/all/0/1"&gt;Chaoqiang Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Wei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhenguo Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1"&gt;Jie Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1"&gt;Hang Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1"&gt;Chunjing Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Object-aware Contrastive Learning for Debiased Scene Representation. (arXiv:2108.00049v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00049</id>
        <link href="http://arxiv.org/abs/2108.00049"/>
        <updated>2021-08-03T02:06:32.654Z</updated>
        <summary type="html"><![CDATA[Contrastive self-supervised learning has shown impressive results in learning
visual representations from unlabeled images by enforcing invariance against
different data augmentations. However, the learned representations are often
contextually biased to the spurious scene correlations of different objects or
object and background, which may harm their generalization on the downstream
tasks. To tackle the issue, we develop a novel object-aware contrastive
learning framework that first (a) localizes objects in a self-supervised manner
and then (b) debias scene correlations via appropriate data augmentations
considering the inferred object locations. For (a), we propose the contrastive
class activation map (ContraCAM), which finds the most discriminative regions
(e.g., objects) in the image compared to the other images using the
contrastively trained models. We further improve the ContraCAM to detect
multiple objects and entire shapes via an iterative refinement procedure. For
(b), we introduce two data augmentations based on ContraCAM, object-aware
random crop and background mixup, which reduce contextual and background biases
during contrastive self-supervised learning, respectively. Our experiments
demonstrate the effectiveness of our representation learning framework,
particularly when trained under multi-object images or evaluated under the
background (and distribution) shifted images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mo_S/0/1/0/all/0/1"&gt;Sangwoo Mo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kang_H/0/1/0/all/0/1"&gt;Hyunwoo Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sohn_K/0/1/0/all/0/1"&gt;Kihyuk Sohn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chun-Liang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1"&gt;Jinwoo Shin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[UAV Trajectory Planning in Wireless Sensor Networks for Energy Consumption Minimization by Deep Reinforcement Learning. (arXiv:2108.00354v1 [eess.SY])]]></title>
        <id>http://arxiv.org/abs/2108.00354</id>
        <link href="http://arxiv.org/abs/2108.00354"/>
        <updated>2021-08-03T02:06:32.646Z</updated>
        <summary type="html"><![CDATA[Unmanned aerial vehicles (UAVs) have emerged as a promising candidate
solution for data collection of large-scale wireless sensor networks (WSNs). In
this paper, we investigate a UAV-aided WSN, where cluster heads (CHs) receive
data from their member nodes, and a UAV is dispatched to collect data from CHs
along the planned trajectory. We aim to minimize the total energy consumption
of the UAV-WSN system in a complete round of data collection. Toward this end,
we formulate the energy consumption minimization problem as a constrained
combinatorial optimization problem by jointly selecting CHs from nodes within
clusters and planning the UAV's visiting order to the selected CHs. The
formulated energy consumption minimization problem is NP-hard, and hence, hard
to solve optimally. In order to tackle this challenge, we propose a novel deep
reinforcement learning (DRL) technique, pointer network-A* (Ptr-A*), which can
efficiently learn from experiences the UAV trajectory policy for minimizing the
energy consumption. The UAV's start point and the WSN with a set of
pre-determined clusters are fed into the Ptr-A*, and the Ptr-A* outputs a group
of CHs and the visiting order to these CHs, i.e., the UAV's trajectory. The
parameters of the Ptr-A* are trained on small-scale clusters problem instances
for faster training by using the actor-critic algorithm in an unsupervised
manner. At inference, three search strategies are also proposed to improve the
quality of solutions. Simulation results show that the trained models based on
20-clusters and 40-clusters have a good generalization ability to solve the
UAV's trajectory planning problem in WSNs with different numbers of clusters,
without the need to retrain the models. Furthermore, the results show that our
proposed DRL algorithm outperforms two baseline techniques.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Zhu_B/0/1/0/all/0/1"&gt;Botao Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Bedeer_E/0/1/0/all/0/1"&gt;Ebrahim Bedeer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Nguyen_H/0/1/0/all/0/1"&gt;Ha H. Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Barton_R/0/1/0/all/0/1"&gt;Robert Barton&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Henry_J/0/1/0/all/0/1"&gt;Jerome Henry&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Take an Emotion Walk: Perceiving Emotions from Gaits Using Hierarchical Attention Pooling and Affective Mapping. (arXiv:1911.08708v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1911.08708</id>
        <link href="http://arxiv.org/abs/1911.08708"/>
        <updated>2021-08-03T02:06:32.638Z</updated>
        <summary type="html"><![CDATA[We present an autoencoder-based semi-supervised approach to classify
perceived human emotions from walking styles obtained from videos or
motion-captured data and represented as sequences of 3D poses. Given the motion
on each joint in the pose at each time step extracted from 3D pose sequences,
we hierarchically pool these joint motions in a bottom-up manner in the
encoder, following the kinematic chains in the human body. We also constrain
the latent embeddings of the encoder to contain the space of
psychologically-motivated affective features underlying the gaits. We train the
decoder to reconstruct the motions per joint per time step in a top-down manner
from the latent embeddings. For the annotated data, we also train a classifier
to map the latent embeddings to emotion labels. Our semi-supervised approach
achieves a mean average precision of 0.84 on the Emotion-Gait benchmark
dataset, which contains both labeled and unlabeled gaits collected from
multiple sources. We outperform current state-of-art algorithms for both
emotion recognition and action recognition from 3D gaits by 7%--23% on the
absolute. More importantly, we improve the average precision by 10%--50% on the
absolute on classes that each makes up less than 25% of the labeled part of the
Emotion-Gait benchmark dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bhattacharya_U/0/1/0/all/0/1"&gt;Uttaran Bhattacharya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roncal_C/0/1/0/all/0/1"&gt;Christian Roncal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mittal_T/0/1/0/all/0/1"&gt;Trisha Mittal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chandra_R/0/1/0/all/0/1"&gt;Rohan Chandra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kapsaskis_K/0/1/0/all/0/1"&gt;Kyra Kapsaskis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gray_K/0/1/0/all/0/1"&gt;Kurt Gray&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bera_A/0/1/0/all/0/1"&gt;Aniket Bera&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Manocha_D/0/1/0/all/0/1"&gt;Dinesh Manocha&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automated Pest Detection with DNN on the Edge for Precision Agriculture. (arXiv:2108.00421v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.00421</id>
        <link href="http://arxiv.org/abs/2108.00421"/>
        <updated>2021-08-03T02:06:32.612Z</updated>
        <summary type="html"><![CDATA[Artificial intelligence has smoothly penetrated several economic activities,
especially monitoring and control applications, including the agriculture
sector. However, research efforts toward low-power sensing devices with fully
functional machine learning (ML) on-board are still fragmented and limited in
smart farming. Biotic stress is one of the primary causes of crop yield
reduction. With the development of deep learning in computer vision technology,
autonomous detection of pest infestation through images has become an important
research direction for timely crop disease diagnosis. This paper presents an
embedded system enhanced with ML functionalities, ensuring continuous detection
of pest infestation inside fruit orchards. The embedded solution is based on a
low-power embedded sensing system along with a Neural Accelerator able to
capture and process images inside common pheromone-based traps. Three different
ML algorithms have been trained and deployed, highlighting the capabilities of
the platform. Moreover, the proposed approach guarantees an extended battery
life thanks to the integration of energy harvesting functionalities. Results
show how it is possible to automate the task of pest infestation for unlimited
time without the farmer's intervention.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Albanese_A/0/1/0/all/0/1"&gt;Andrea Albanese&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nardello_M/0/1/0/all/0/1"&gt;Matteo Nardello&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brunelli_D/0/1/0/all/0/1"&gt;Davide Brunelli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ReCU: Reviving the Dead Weights in Binary Neural Networks. (arXiv:2103.12369v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.12369</id>
        <link href="http://arxiv.org/abs/2103.12369"/>
        <updated>2021-08-03T02:06:32.606Z</updated>
        <summary type="html"><![CDATA[Binary neural networks (BNNs) have received increasing attention due to their
superior reductions of computation and memory. Most existing works focus on
either lessening the quantization error by minimizing the gap between the
full-precision weights and their binarization or designing a gradient
approximation to mitigate the gradient mismatch, while leaving the "dead
weights" untouched. This leads to slow convergence when training BNNs. In this
paper, for the first time, we explore the influence of "dead weights" which
refer to a group of weights that are barely updated during the training of
BNNs, and then introduce rectified clamp unit (ReCU) to revive the "dead
weights" for updating. We prove that reviving the "dead weights" by ReCU can
result in a smaller quantization error. Besides, we also take into account the
information entropy of the weights, and then mathematically analyze why the
weight standardization can benefit BNNs. We demonstrate the inherent
contradiction between minimizing the quantization error and maximizing the
information entropy, and then propose an adaptive exponential scheduler to
identify the range of the "dead weights". By considering the "dead weights",
our method offers not only faster BNN training, but also state-of-the-art
performance on CIFAR-10 and ImageNet, compared with recent methods. Code can be
available at https://github.com/z-hXu/ReCU.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zihan Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1"&gt;Mingbao Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jianzhuang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jie Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1"&gt;Ling Shao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1"&gt;Yue Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1"&gt;Yonghong Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1"&gt;Rongrong Ji&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Reinforcement Learning Approach for Scheduling in mmWave Networks. (arXiv:2108.00548v1 [cs.IT])]]></title>
        <id>http://arxiv.org/abs/2108.00548</id>
        <link href="http://arxiv.org/abs/2108.00548"/>
        <updated>2021-08-03T02:06:32.596Z</updated>
        <summary type="html"><![CDATA[We consider a source that wishes to communicate with a destination at a
desired rate, over a mmWave network where links are subject to blockage and
nodes to failure (e.g., in a hostile military environment). To achieve
resilience to link and node failures, we here explore a state-of-the-art Soft
Actor-Critic (SAC) deep reinforcement learning algorithm, that adapts the
information flow through the network, without using knowledge of the link
capacities or network topology. Numerical evaluations show that our algorithm
can achieve the desired rate even in dynamic environments and it is robust
against blockage.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dogan_M/0/1/0/all/0/1"&gt;Mine Gokce Dogan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ezzeldin_Y/0/1/0/all/0/1"&gt;Yahya H. Ezzeldin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fragouli_C/0/1/0/all/0/1"&gt;Christina Fragouli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bohannon_A/0/1/0/all/0/1"&gt;Addison W. Bohannon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using deep learning to detect patients at risk for prostate cancer despite benign biopsies. (arXiv:2106.14256v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.14256</id>
        <link href="http://arxiv.org/abs/2106.14256"/>
        <updated>2021-08-03T02:06:32.587Z</updated>
        <summary type="html"><![CDATA[Background: Transrectal ultrasound guided systematic biopsies of the prostate
is a routine procedure to establish a prostate cancer diagnosis. However, the
10-12 prostate core biopsies only sample a relatively small volume of the
prostate, and tumour lesions in regions between biopsy cores can be missed,
leading to a well-known low sensitivity to detect clinically relevant cancer.
As a proof-of-principle, we developed and validated a deep convolutional neural
network model to distinguish between morphological patterns in benign prostate
biopsy whole slide images from men with and without established cancer.
Methods: This study included 14,354 hematoxylin and eosin stained whole slide
images from benign prostate biopsies from 1,508 men in two groups: men without
an established prostate cancer (PCa) diagnosis and men with at least one core
biopsy diagnosed with PCa. 80% of the participants were assigned as training
data and used for model optimization (1,211 men), and the remaining 20% (297
men) as a held-out test set used to evaluate model performance. An ensemble of
10 deep convolutional neural network models was optimized for classification of
biopsies from men with and without established cancer. Hyperparameter
optimization and model selection was performed by cross-validation in the
training data . Results: Area under the receiver operating characteristic curve
(ROC-AUC) was estimated as 0.727 (bootstrap 95% CI: 0.708-0.745) on biopsy
level and 0.738 (bootstrap 95% CI: 0.682 - 0.796) on man level. At a
specificity of 0.9 the model had an estimated sensitivity of 0.348. Conclusion:
The developed model has the ability to detect men with risk of missed PCa due
to under-sampling of the prostate. The proposed model has the potential to
reduce the number of false negative cases in routine systematic prostate
biopsies and to indicate men who could benefit from MRI-guided re-biopsy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Liu_B/0/1/0/all/0/1"&gt;Boing Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yinxi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Weitz_P/0/1/0/all/0/1"&gt;Philippe Weitz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lindberg_J/0/1/0/all/0/1"&gt;Johan Lindberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hartman_J/0/1/0/all/0/1"&gt;Johan Hartman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Egevad_L/0/1/0/all/0/1"&gt;Lars Egevad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Gronberg_H/0/1/0/all/0/1"&gt;Henrik Gr&amp;#xf6;nberg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Eklund_M/0/1/0/all/0/1"&gt;Martin Eklund&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rantalainen_M/0/1/0/all/0/1"&gt;Mattias Rantalainen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transparent Object Tracking Benchmark. (arXiv:2011.10875v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.10875</id>
        <link href="http://arxiv.org/abs/2011.10875"/>
        <updated>2021-08-03T02:06:32.568Z</updated>
        <summary type="html"><![CDATA[Visual tracking has achieved considerable progress in recent years. However,
current research in the field mainly focuses on tracking of opaque objects,
while little attention is paid to transparent object tracking. In this paper,
we make the first attempt in exploring this problem by proposing a Transparent
Object Tracking Benchmark (TOTB). Specifically, TOTB consists of 225 videos
(86K frames) from 15 diverse transparent object categories. Each sequence is
manually labeled with axis-aligned bounding boxes. To the best of our
knowledge, TOTB is the first benchmark dedicated to transparent object
tracking. In order to understand how existing trackers perform and to provide
comparison for future research on TOTB, we extensively evaluate 25
state-of-the-art tracking algorithms. The evaluation results exhibit that more
efforts are needed to improve transparent object tracking. Besides, we observe
some nontrivial findings from the evaluation that are discrepant with some
common beliefs in opaque object tracking. For example, we find that deeper
features are not always good for improvements. Moreover, to encourage future
research, we introduce a novel tracker, named TransATOM, which leverages
transparency features for tracking and surpasses all 25 evaluated approaches by
a large margin. By releasing TOTB, we expect to facilitate future research and
application of transparent object tracking in both the academia and industry.
The TOTB and evaluation results as well as TransATOM are available at
https://hengfan2010.github.io/projects/TOTB.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1"&gt;Heng Fan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Miththanthaya_H/0/1/0/all/0/1"&gt;Halady Akhilesha Miththanthaya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Harshit/0/1/0/all/0/1"&gt;Harshit&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rajan_S/0/1/0/all/0/1"&gt;Siranjiv Ramana Rajan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiaoqiong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zou_Z/0/1/0/all/0/1"&gt;Zhilin Zou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1"&gt;Yuewei Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ling_H/0/1/0/all/0/1"&gt;Haibin Ling&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CSC-Unet: A Novel Convolutional Sparse Coding Strategy based Neural Network for Semantic Segmentation. (arXiv:2108.00408v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00408</id>
        <link href="http://arxiv.org/abs/2108.00408"/>
        <updated>2021-08-03T02:06:32.562Z</updated>
        <summary type="html"><![CDATA[It is a challenging task to accurately perform semantic segmentation due to
the complexity of real picture scenes. Many semantic segmentation methods based
on traditional deep learning insufficiently captured the semantic and
appearance information of images, which put limit on their generality and
robustness for various application scenes. In this paper, we proposed a novel
strategy that reformulated the popularly-used convolution operation to
multi-layer convolutional sparse coding block to ease the aforementioned
deficiency. This strategy can be possibly used to significantly improve the
segmentation performance of any semantic segmentation model that involves
convolutional operations. To prove the effectiveness of our idea, we chose the
widely-used U-Net model for the demonstration purpose, and we designed CSC-Unet
model series based on U-Net. Through extensive analysis and experiments, we
provided credible evidence showing that the multi-layer convolutional sparse
coding block enables semantic segmentation model to converge faster, can
extract finer semantic and appearance information of images, and improve the
ability to recover spatial detail information. The best CSC-Unet model
significantly outperforms the results of the original U-Net on three public
datasets with different scenarios, i.e., 87.14% vs. 84.71% on DeepCrack
dataset, 68.91% vs. 67.09% on Nuclei dataset, and 53.68% vs. 48.82% on CamVid
dataset, respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1"&gt;Haitong Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1"&gt;Shuang He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1"&gt;Xia Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1"&gt;Qin Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1"&gt;Kaiyue Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1"&gt;Hongjie Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1"&gt;Nizhuan Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spectroscopic Approach to Correction and Visualisation of Bright-Field Light Transmission Microscopy Biological Data. (arXiv:1903.06519v4 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1903.06519</id>
        <link href="http://arxiv.org/abs/1903.06519"/>
        <updated>2021-08-03T02:06:32.554Z</updated>
        <summary type="html"><![CDATA[The most realistic information about the transparent sample such as a live
cell can be obtained only using bright-field light microscopy. At
high-intensity pulsing LED illumination, we captured a primary
12-bit-per-channel (bpc) response from an observed sample using a bright-field
microscope equipped with a high-resolution (4872x3248) image sensor. In order
to suppress data distortions originating from the light interactions with
elements in the optical path, poor sensor reproduction (geometrical defects of
the camera sensor and some peculiarities of sensor sensitivity), we propose a
spectroscopic approach for the correction of this uncompressed 12-bpc data by
simultaneous calibration of all parts of the experimental arrangement.
Moreover, the final intensities of the corrected images are proportional to the
photon fluxes detected by a camera sensor. It can be visualized in 8-bpc
intensity depth after the Least Information Loss compression [Lect. Notes
Bioinform. 9656, 527 (2016)].]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Platonova_G/0/1/0/all/0/1"&gt;Ganna Platonova&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Stys_D/0/1/0/all/0/1"&gt;Dalibor Stys&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Soucek_P/0/1/0/all/0/1"&gt;Pavel Soucek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lonhus_K/0/1/0/all/0/1"&gt;Kirill Lonhus&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Valenta_J/0/1/0/all/0/1"&gt;Jan Valenta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rychtarikova_R/0/1/0/all/0/1"&gt;Renata Rychtarikova&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Group Fisher Pruning for Practical Network Compression. (arXiv:2108.00708v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00708</id>
        <link href="http://arxiv.org/abs/2108.00708"/>
        <updated>2021-08-03T02:06:32.548Z</updated>
        <summary type="html"><![CDATA[Network compression has been widely studied since it is able to reduce the
memory and computation cost during inference. However, previous methods seldom
deal with complicated structures like residual connections, group/depth-wise
convolution and feature pyramid network, where channels of multiple layers are
coupled and need to be pruned simultaneously. In this paper, we present a
general channel pruning approach that can be applied to various complicated
structures. Particularly, we propose a layer grouping algorithm to find coupled
channels automatically. Then we derive a unified metric based on Fisher
information to evaluate the importance of a single channel and coupled
channels. Moreover, we find that inference speedup on GPUs is more correlated
with the reduction of memory rather than FLOPs, and thus we employ the memory
reduction of each channel to normalize the importance. Our method can be used
to prune any structures including those with coupled channels. We conduct
extensive experiments on various backbones, including the classic ResNet and
ResNeXt, mobile-friendly MobileNetV2, and the NAS-based RegNet, both on image
classification and object detection which is under-explored. Experimental
results validate that our method can effectively prune sophisticated networks,
boosting inference speed without sacrificing accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Liyang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1"&gt;Shilong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kuang_Z/0/1/0/all/0/1"&gt;Zhanghui Kuang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_A/0/1/0/all/0/1"&gt;Aojun Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xue_J/0/1/0/all/0/1"&gt;Jing-Hao Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xinjiang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yimin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1"&gt;Wenming Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liao_Q/0/1/0/all/0/1"&gt;Qingmin Liao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1"&gt;Wayne Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Benchmarking Robustness of Deep Learning Classifiers Using Two-Factor Perturbation. (arXiv:2103.03102v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.03102</id>
        <link href="http://arxiv.org/abs/2103.03102"/>
        <updated>2021-08-03T02:06:32.535Z</updated>
        <summary type="html"><![CDATA[The accuracy of DL classifiers is unstable in that it often changes
significantly when retested on adversarial images, imperfect images, or
perturbed images. This paper adds to the small but fundamental body of work on
benchmarking the robustness of DL classifiers on defective images. Unlike
existed single-factor digital perturbation work, we provide state-of-the-art
two-factor perturbation that provides two natural perturbations on images
applied in different sequences. The two-factor perturbation includes (1) two
digital perturbations (Salt & pepper noise and Gaussian noise) applied in both
sequences. (2) one digital perturbation (salt & pepper noise) and a geometric
perturbation (rotation) applied in different sequences. To measure robust DL
classifiers, previous scientists provided 15 types of single-factor corruption.
We created 69 benchmarking image sets, including a clean set, sets with single
factor perturbations, and sets with two-factor perturbation conditions. To be
best of our knowledge, this is the first report that two-factor perturbed
images improves both robustness and accuracy of DL classifiers. Previous
research evaluating deep learning (DL) classifiers has often used top-1/top-5
accuracy, so researchers have usually offered tables, line diagrams, and bar
charts to display accuracy of DL classifiers. But these existed approaches
cannot quantitively evaluate robustness of DL classifiers. We innovate a new
two-dimensional, statistical visualization tool, including mean accuracy and
coefficient of variation (CV), to benchmark the robustness of DL classifiers.
All source codes and related image sets are shared on websites
(this http URL or
https://github.com/daiweiworking/RobustDeepLearningUsingPerturbations ) to
support future academic research and industry projects.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1"&gt;Wei Dai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berleant_D/0/1/0/all/0/1"&gt;Daniel Berleant&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bespoke Fractal Sampling Patterns for Discrete Fourier Space via the Kaleidoscope Transform. (arXiv:2108.00639v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2108.00639</id>
        <link href="http://arxiv.org/abs/2108.00639"/>
        <updated>2021-08-03T02:06:32.529Z</updated>
        <summary type="html"><![CDATA[Sampling strategies are important for sparse imaging methodologies,
especially those employing the discrete Fourier transform (DFT). Chaotic
sensing is one such methodology that employs deterministic, fractal sampling in
conjunction with finite, iterative reconstruction schemes to form an image from
limited samples. Using a sampling pattern constructed entirely from periodic
lines in DFT space, chaotic sensing was found to outperform traditional
compressed sensing for magnetic resonance imaging; however, only one such
sampling pattern was presented and the reason for its fractal nature was not
proven. Through the introduction of a novel image transform known as the
kaleidoscope transform, which formalises and extends upon the concept of
downsampling and concatenating an image with itself, this paper: (1)
demonstrates a fundamental relationship between multiplication in modular
arithmetic and downsampling; (2) provides a rigorous mathematical explanation
for the fractal nature of the sampling pattern in the DFT; and (3) leverages
this understanding to develop a collection of novel fractal sampling patterns
for the 2D DFT with customisable properties. The ability to design tailor-made
fractal sampling patterns expands the utility of the DFT in chaotic imaging and
may form the basis for a bespoke chaotic sensing methodology, in which the
fractal sampling matches the imaging task for improved reconstruction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+White_J/0/1/0/all/0/1"&gt;Jacob M. White&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Crozier_S/0/1/0/all/0/1"&gt;Stuart Crozier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chandra_S/0/1/0/all/0/1"&gt;Shekhar S. Chandra&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SPEAR : Semi-supervised Data Programming in Python. (arXiv:2108.00373v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.00373</id>
        <link href="http://arxiv.org/abs/2108.00373"/>
        <updated>2021-08-03T02:06:32.510Z</updated>
        <summary type="html"><![CDATA[We present SPEAR, an open-source python library for data programming with
semi supervision. The package implements several recent data programming
approaches including facility to programmatically label and build training
data. SPEAR facilitates weak supervision in the form of heuristics (or rules)
and association of noisy labels to the training dataset. These noisy labels are
aggregated to assign labels to the unlabeled data for downstream tasks. We have
implemented several label aggregation approaches that aggregate the noisy
labels and then train using the noisily labeled set in a cascaded manner. Our
implementation also includes other approaches that jointly aggregate and train
the model. Thus, in our python package, we integrate several cascade and joint
data-programming approaches while also providing the facility of data
programming by letting the user define labeling functions or rules. The code
and tutorial notebooks are available at
\url{https://github.com/decile-team/spear}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Abhishek_G/0/1/0/all/0/1"&gt;Guttu Sai Abhishek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ingole_H/0/1/0/all/0/1"&gt;Harshad Ingole&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Laturia_P/0/1/0/all/0/1"&gt;Parth Laturia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dorna_V/0/1/0/all/0/1"&gt;Vineeth Dorna&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Maheshwari_A/0/1/0/all/0/1"&gt;Ayush Maheshwari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ramakrishnan_G/0/1/0/all/0/1"&gt;Ganesh Ramakrishnan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Iyer_R/0/1/0/all/0/1"&gt;Rishabh Iyer&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Machine-learning Based Initialization for Joint Statistical Iterative Dual-energy CT with Application to Proton Therapy. (arXiv:2108.00109v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2108.00109</id>
        <link href="http://arxiv.org/abs/2108.00109"/>
        <updated>2021-08-03T02:06:32.504Z</updated>
        <summary type="html"><![CDATA[Dual-energy CT (DECT) has been widely investigated to generate more
informative and more accurate images in the past decades. For example,
Dual-Energy Alternating Minimization (DEAM) algorithm achieves sub-percentage
uncertainty in estimating proton stopping-power mappings from experimental 3-mm
collimated phantom data. However, elapsed time of iterative DECT algorithms is
not clinically acceptable, due to their low convergence rate and the tremendous
geometry of modern helical CT scanners. A CNN-based initialization method is
introduced to reduce the computational time of iterative DECT algorithms. DEAM
is used as an example of iterative DECT algorithms in this work. The simulation
results show that our method generates denoised images with greatly improved
estimation accuracy for adipose, tonsils, and muscle tissue. Also, it reduces
elapsed time by approximately 5-fold for DEAM to reach the same objective
function value for both simulated and real data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Ge_T/0/1/0/all/0/1"&gt;Tao Ge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Medrano_M/0/1/0/all/0/1"&gt;Maria Medrano&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liao_R/0/1/0/all/0/1"&gt;Rui Liao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Politte_D/0/1/0/all/0/1"&gt;David G. Politte&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Williamson_J/0/1/0/all/0/1"&gt;Jeffrey F. Williamson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+OSullivan_J/0/1/0/all/0/1"&gt;Joseph A. O&amp;#x27;Sullivan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Extracting Grammars from a Neural Network Parser for Anomaly Detection in Unknown Formats. (arXiv:2108.00103v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.00103</id>
        <link href="http://arxiv.org/abs/2108.00103"/>
        <updated>2021-08-03T02:06:32.463Z</updated>
        <summary type="html"><![CDATA[Reinforcement learning has recently shown promise as a technique for training
an artificial neural network to parse sentences in some unknown format. A key
aspect of this approach is that rather than explicitly inferring a grammar that
describes the format, the neural network learns to perform various parsing
actions (such as merging two tokens) over a corpus of sentences, with the goal
of maximizing the total reward, which is roughly based on the estimated
frequency of the resulting parse structures. This can allow the learning
process to more easily explore different action choices, since a given choice
may change the optimality of the parse (as expressed by the total reward), but
will not result in the failure to parse a sentence. However, the approach also
exhibits limitations: first, the neural network does not provide production
rules for the grammar that it uses during parsing; second, because this neural
network can successfully parse any sentence, it cannot be directly used to
identify sentences that deviate from the format of the training sentences,
i.e., that are anomalous. In this paper, we address these limitations by
presenting procedures for extracting production rules from the neural network,
and for using these rules to determine whether a given sentence is nominal or
anomalous, when compared to structures observed within training data. In the
latter case, an attempt is made to identify the location of the anomaly.
Additionally, a two pass mechanism is presented for dealing with formats
containing high-entropy information. We empirically evaluate the approach on
artificial formats, demonstrating effectiveness, but also identifying
limitations. By further improving parser learning, and leveraging rule
extraction and anomaly detection, one might begin to understand common errors,
either benign or malicious, in practical formats.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Grushin_A/0/1/0/all/0/1"&gt;Alexander Grushin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Woods_W/0/1/0/all/0/1"&gt;Walt Woods&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Semi-Supervising Learning, Transfer Learning, and Knowledge Distillation with SimCLR. (arXiv:2108.00587v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00587</id>
        <link href="http://arxiv.org/abs/2108.00587"/>
        <updated>2021-08-03T02:06:32.456Z</updated>
        <summary type="html"><![CDATA[Recent breakthroughs in the field of semi-supervised learning have achieved
results that match state-of-the-art traditional supervised learning methods.
Most successful semi-supervised learning approaches in computer vision focus on
leveraging huge amount of unlabeled data, learning the general representation
via data augmentation and transformation, creating pseudo labels, implementing
different loss functions, and eventually transferring this knowledge to more
task-specific smaller models. In this paper, we aim to conduct our analyses on
three different aspects of SimCLR, the current state-of-the-art semi-supervised
learning framework for computer vision. First, we analyze properties of
contrast learning on fine-tuning, as we understand that contrast learning is
what makes this method so successful. Second, we research knowledge
distillation through teacher-forcing paradigm. We observe that when the teacher
and the student share the same base model, knowledge distillation will achieve
better result. Finally, we study how transfer learning works and its
relationship with the number of classes on different data sets. Our results
indicate that transfer learning performs better when number of classes are
smaller.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1"&gt;Khoi Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nguyen_Y/0/1/0/all/0/1"&gt;Yen Nguyen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Le_B/0/1/0/all/0/1"&gt;Bao Le&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Cross-Modal Distillation for Thermal Infrared Tracking. (arXiv:2108.00187v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00187</id>
        <link href="http://arxiv.org/abs/2108.00187"/>
        <updated>2021-08-03T02:06:32.441Z</updated>
        <summary type="html"><![CDATA[The target representation learned by convolutional neural networks plays an
important role in Thermal Infrared (TIR) tracking. Currently, most of the
top-performing TIR trackers are still employing representations learned by the
model trained on the RGB data. However, this representation does not take into
account the information in the TIR modality itself, limiting the performance of
TIR tracking. To solve this problem, we propose to distill representations of
the TIR modality from the RGB modality with Cross-Modal Distillation (CMD) on a
large amount of unlabeled paired RGB-TIR data. We take advantage of the
two-branch architecture of the baseline tracker, i.e. DiMP, for cross-modal
distillation working on two components of the tracker. Specifically, we use one
branch as a teacher module to distill the representation learned by the model
into the other branch. Benefiting from the powerful model in the RGB modality,
the cross-modal distillation can learn the TIR-specific representation for
promoting TIR tracking. The proposed approach can be incorporated into
different baseline trackers conveniently as a generic and independent
component. Furthermore, the semantic coherence of paired RGB and TIR images is
utilized as a supervised signal in the distillation loss for cross-modal
knowledge transfer. In practice, three different approaches are explored to
generate paired RGB-TIR patches with the same semantics for training in an
unsupervised way. It is easy to extend to an even larger scale of unlabeled
training data. Extensive experiments on the LSOTB-TIR dataset and PTB-TIR
dataset demonstrate that our proposed cross-modal distillation method
effectively learns TIR-specific target representations transferred from the RGB
modality. Our tracker outperforms the baseline tracker by achieving absolute
gains of 2.3% Success, 2.7% Precision, and 2.5% Normalized Precision
respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1"&gt;Jingxian Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lichao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zha_Y/0/1/0/all/0/1"&gt;Yufei Zha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gonzalez_Garcia_A/0/1/0/all/0/1"&gt;Abel Gonzalez-Garcia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1"&gt;Peng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1"&gt;Wei Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yanning Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pro-UIGAN: Progressive Face Hallucination from Occluded Thumbnails. (arXiv:2108.00602v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00602</id>
        <link href="http://arxiv.org/abs/2108.00602"/>
        <updated>2021-08-03T02:06:32.419Z</updated>
        <summary type="html"><![CDATA[In this paper, we study the task of hallucinating an authentic
high-resolution (HR) face from an occluded thumbnail. We propose a multi-stage
Progressive Upsampling and Inpainting Generative Adversarial Network, dubbed
Pro-UIGAN, which exploits facial geometry priors to replenish and upsample (8*)
the occluded and tiny faces (16*16 pixels). Pro-UIGAN iteratively (1) estimates
facial geometry priors for low-resolution (LR) faces and (2) acquires
non-occluded HR face images under the guidance of the estimated priors. Our
multi-stage hallucination network super-resolves and inpaints occluded LR faces
in a coarse-to-fine manner, thus reducing unwanted blurriness and artifacts
significantly. Specifically, we design a novel cross-modal transformer module
for facial priors estimation, in which an input face and its landmark features
are formulated as queries and keys, respectively. Such a design encourages
joint feature learning across the input facial and landmark features, and deep
feature correspondences will be discovered by attention. Thus, facial
appearance features and facial geometry priors are learned in a mutual
promotion manner. Extensive experiments demonstrate that our Pro-UIGAN achieves
visually pleasing HR faces, reaching superior performance in downstream tasks,
i.e., face alignment, face parsing, face recognition and expression
classification, compared with other state-of-the-art (SotA) methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yang Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1"&gt;Xin Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1"&gt;Xiaobo Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1"&gt;Ping Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scale-aware Neural Network for Semantic Segmentation of Multi-resolution Remotely Sensed Images. (arXiv:2103.07935v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.07935</id>
        <link href="http://arxiv.org/abs/2103.07935"/>
        <updated>2021-08-03T02:06:32.393Z</updated>
        <summary type="html"><![CDATA[Assigning geospatial objects with specific categories at the pixel level is a
fundamental task in remote sensing image analysis. Along with rapid development
in sensor technologies, remotely sensed images can be captured at multiple
spatial resolutions (MSR) with information content manifested at different
scales. Extracting information from these MSR images represents huge
opportunities for enhanced feature representation and characterisation.
However, MSR images suffer from two critical issues: 1) increased scale
variation of geo-objects and 2) loss of detailed information at coarse spatial
resolutions. To bridge these gaps, in this paper, we propose a novel
scale-aware neural network (SaNet) for semantic segmentation of MSR remotely
sensed imagery. SaNet deploys a densely connected feature network (DCFPN)
module to capture high-quality multi-scale context, such that the scale
variation is handled properly and the quality of segmentation is increased for
both large and small objects. A spatial feature recalibration (SFR) module is
further incorporated into the network to learn intact semantic content with
enhanced spatial relationships, where the negative effects of information loss
are removed. The combination of DCFPN and SFR allows SaNet to learn scale-aware
feature representation, which outperforms the existing multi-scale feature
representation. Extensive experiments on three semantic segmentation datasets
demonstrated the effectiveness of the proposed SaNet in cross-resolution
segmentation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Libo Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fang_S/0/1/0/all/0/1"&gt;Shenghui Fang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Ce Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1"&gt;Rui Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duan_C/0/1/0/all/0/1"&gt;Chenxi Duan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meng_X/0/1/0/all/0/1"&gt;Xiaoliang Meng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Atkinson_P/0/1/0/all/0/1"&gt;Peter M. Atkinson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Developing a Compressed Object Detection Model based on YOLOv4 for Deployment on Embedded GPU Platform of Autonomous System. (arXiv:2108.00392v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00392</id>
        <link href="http://arxiv.org/abs/2108.00392"/>
        <updated>2021-08-03T02:06:32.352Z</updated>
        <summary type="html"><![CDATA[Latest CNN-based object detection models are quite accurate but require a
high-performance GPU to run in real-time. They still are heavy in terms of
memory size and speed for an embedded system with limited memory space. Since
the object detection for autonomous system is run on an embedded processor, it
is preferable to compress the detection network as light as possible while
preserving the detection accuracy. There are several popular lightweight
detection models but their accuracy is too low for safe driving applications.
Therefore, this paper proposes a new object detection model, referred as
YOffleNet, which is compressed at a high ratio while minimizing the accuracy
loss for real-time and safe driving application on an autonomous system. The
backbone network architecture is based on YOLOv4, but we could compress the
network greatly by replacing the high-calculation-load CSP DenseNet with the
lighter modules of ShuffleNet. Experiments with KITTI dataset showed that the
proposed YOffleNet is compressed by 4.7 times than the YOLOv4-s that could
achieve as fast as 46 FPS on an embedded GPU system(NVIDIA Jetson AGX Xavier).
Compared to the high compression ratio, the accuracy is reduced slightly to
85.8% mAP, that is only 2.6% lower than YOLOv4-s. Thus, the proposed network
showed a high potential to be deployed on the embedded system of the autonomous
system for the real-time and accurate object detection applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sim_I/0/1/0/all/0/1"&gt;Issac Sim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lim_J/0/1/0/all/0/1"&gt;Ju-Hyung Lim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jang_Y/0/1/0/all/0/1"&gt;Young-Wan Jang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+You_J/0/1/0/all/0/1"&gt;JiHwan You&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1"&gt;SeonTaek Oh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1"&gt;Young-Keun Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Decentralized Federated Learning Framework via Committee Mechanism with Convergence Guarantee. (arXiv:2108.00365v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.00365</id>
        <link href="http://arxiv.org/abs/2108.00365"/>
        <updated>2021-08-03T02:06:32.228Z</updated>
        <summary type="html"><![CDATA[Federated learning allows multiple participants to collaboratively train an
efficient model without exposing data privacy. However, this distributed
machine learning training method is prone to attacks from Byzantine clients,
which interfere with the training of the global model by modifying the model or
uploading the false gradient. In this paper, we propose a novel serverless
federated learning framework Committee Mechanism based Federated Learning
(CMFL), which can ensure the robustness of the algorithm with convergence
guarantee. In CMFL, a committee system is set up to screen the uploaded local
gradients. The committee system selects the local gradients rated by the
elected members for the aggregation procedure through the selection strategy,
and replaces the committee member through the election strategy. Based on the
different considerations of model performance and defense, two opposite
selection strategies are designed for the sake of both accuracy and robustness.
Extensive experiments illustrate that CMFL achieves faster convergence and
better accuracy than the typical Federated Learning, in the meanwhile obtaining
better robustness than the traditional Byzantine-tolerant algorithms, in the
manner of a decentralized approach. In addition, we theoretically analyze and
prove the convergence of CMFL under different election and selection
strategies, which coincides with the experimental results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Che_C/0/1/0/all/0/1"&gt;Chunjiang Che&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xiaoli Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Chuan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1"&gt;Xiaoyu He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1"&gt;Zibin Zheng&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SSPU-Net: Self-Supervised Point Cloud Upsampling via Differentiable Rendering. (arXiv:2108.00454v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00454</id>
        <link href="http://arxiv.org/abs/2108.00454"/>
        <updated>2021-08-03T02:06:32.193Z</updated>
        <summary type="html"><![CDATA[Point clouds obtained from 3D sensors are usually sparse. Existing methods
mainly focus on upsampling sparse point clouds in a supervised manner by using
dense ground truth point clouds. In this paper, we propose a self-supervised
point cloud upsampling network (SSPU-Net) to generate dense point clouds
without using ground truth. To achieve this, we exploit the consistency between
the input sparse point cloud and generated dense point cloud for the shapes and
rendered images. Specifically, we first propose a neighbor expansion unit (NEU)
to upsample the sparse point clouds, where the local geometric structures of
the sparse point clouds are exploited to learn weights for point interpolation.
Then, we develop a differentiable point cloud rendering unit (DRU) as an
end-to-end module in our network to render the point cloud into multi-view
images. Finally, we formulate a shape-consistent loss and an image-consistent
loss to train the network so that the shapes of the sparse and dense point
clouds are as consistent as possible. Extensive results on the CAD and scanned
datasets demonstrate that our method can achieve impressive results in a
self-supervised manner. Code is available at https://github.com/Avlon/SSPU-Net.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1"&gt;Yifan Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hui_L/0/1/0/all/0/1"&gt;Le Hui&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1"&gt;Jin Xie&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multivariate Time Series Imputation by Graph Neural Networks. (arXiv:2108.00298v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.00298</id>
        <link href="http://arxiv.org/abs/2108.00298"/>
        <updated>2021-08-03T02:06:32.175Z</updated>
        <summary type="html"><![CDATA[Dealing with missing values and incomplete time series is a labor-intensive
and time-consuming inevitable task when handling data coming from real-world
applications. Effective spatio-temporal representations would allow imputation
methods to reconstruct missing temporal data by exploiting information coming
from sensors at different locations. However, standard methods fall short in
capturing the nonlinear time and space dependencies existing within networks of
interconnected sensors and do not take full advantage of the available - and
often strong - relational information. Notably, most of state-of-the-art
imputation methods based on deep learning do not explicitly model relational
aspects and, in any case, do not exploit processing frameworks able to
adequately represent structured spatio-temporal data. Conversely, graph neural
networks have recently surged in popularity as both expressive and scalable
tools for processing sequential data with relational inductive biases. In this
work, we present the first assessment of graph neural networks in the context
of multivariate time series imputation. In particular, we introduce a novel
graph neural network architecture, named GRIL, which aims at reconstructing
missing data in the different channels of a multivariate time series by
learning spatial-temporal representations through message passing. Preliminary
empirical results show that our model outperforms state-of-the-art methods in
the imputation task on relevant benchmarks with mean absolute error
improvements often higher than 20%.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cini_A/0/1/0/all/0/1"&gt;Andrea Cini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marisca_I/0/1/0/all/0/1"&gt;Ivan Marisca&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alippi_C/0/1/0/all/0/1"&gt;Cesare Alippi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Discovering "Semantics" in Super-Resolution Networks. (arXiv:2108.00406v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00406</id>
        <link href="http://arxiv.org/abs/2108.00406"/>
        <updated>2021-08-03T02:06:32.154Z</updated>
        <summary type="html"><![CDATA[Super-resolution (SR) is a fundamental and representative task of low-level
vision area. It is generally thought that the features extracted from the SR
network have no specific semantic information, and the network simply learns
complex non-linear mappings from input to output. Can we find any "semantics"
in SR networks? In this paper, we give affirmative answers to this question. By
analyzing the feature representations with dimensionality reduction and
visualization, we successfully discover the deep semantic representations in SR
networks, \textit{i.e.}, deep degradation representations (DDR), which relate
to the image degradation types and degrees. We also reveal the differences in
representation semantics between classification and SR networks. Through
extensive experiments and analysis, we draw a series of observations and
conclusions, which are of great significance for future work, such as
interpreting the intrinsic mechanisms of low-level CNN networks and developing
new evaluation approaches for blind SR.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yihao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1"&gt;Anran Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1"&gt;Jinjin Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhipeng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1"&gt;Wenhao Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1"&gt;Yu Qiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_C/0/1/0/all/0/1"&gt;Chao Dong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Subjective Image Quality Assessment with Boosted Triplet Comparisons. (arXiv:2108.00201v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00201</id>
        <link href="http://arxiv.org/abs/2108.00201"/>
        <updated>2021-08-03T02:06:32.132Z</updated>
        <summary type="html"><![CDATA[In subjective full-reference image quality assessment, differences between
perceptual image qualities of the reference image and its distorted versions
are evaluated, often using degradation category ratings (DCR). However, the DCR
has been criticized since differences between rating categories on this ordinal
scale might not be perceptually equidistant, and observers may have different
understandings of the categories. Pair comparisons (PC) of distorted images,
followed by Thurstonian reconstruction of scale values, overcome these
problems. In addition, PC is more sensitive than DCR, and it can provide scale
values in fractional, just noticeable difference (JND) units that express a
precise perceptional interpretation. Still, the comparison of images of nearly
the same quality can be difficult. We introduce boosting techniques embedded in
more general triplet comparisons (TC) that increase the sensitivity even more.
Boosting amplifies the artefacts of distorted images, enlarges their visual
representation by zooming, increases the visibility of the distortions by a
flickering effect, or combines some of the above. Experimental results show the
effectiveness of boosted TC for seven types of distortion. We crowdsourced over
1.7 million responses to triplet questions. A detailed analysis shows that
boosting increases the discriminatory power and allows to reduce the number of
subjective ratings without sacrificing the accuracy of the resulting relative
image quality values. Our technique paves the way to fine-grained image quality
datasets, allowing for more distortion levels, yet with high-quality subjective
annotations. We also provide the details for Thurstonian scale reconstruction
from TC and our annotated dataset, KonFiG-IQA, containing 10 source images,
processed using 7 distortion types at 12 or even 30 levels, uniformly spaced
over a span of 3 JND units.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Men_H/0/1/0/all/0/1"&gt;Hui Men&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1"&gt;Hanhe Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jenadeleh_M/0/1/0/all/0/1"&gt;Mohsen Jenadeleh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saupe_D/0/1/0/all/0/1"&gt;Dietmar Saupe&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PyMAF: 3D Human Pose and Shape Regression with Pyramidal Mesh Alignment Feedback Loop. (arXiv:2103.16507v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.16507</id>
        <link href="http://arxiv.org/abs/2103.16507"/>
        <updated>2021-08-03T02:06:32.085Z</updated>
        <summary type="html"><![CDATA[Regression-based methods have recently shown promising results in
reconstructing human meshes from monocular images. By directly mapping raw
pixels to model parameters, these methods can produce parametric models in a
feed-forward manner via neural networks. However, minor deviation in parameters
may lead to noticeable misalignment between the estimated meshes and image
evidences. To address this issue, we propose a Pyramidal Mesh Alignment
Feedback (PyMAF) loop to leverage a feature pyramid and rectify the predicted
parameters explicitly based on the mesh-image alignment status in our deep
regressor. In PyMAF, given the currently predicted parameters, mesh-aligned
evidences will be extracted from finer-resolution features accordingly and fed
back for parameter rectification. To reduce noise and enhance the reliability
of these evidences, an auxiliary pixel-wise supervision is imposed on the
feature encoder, which provides mesh-image correspondence guidance for our
network to preserve the most related information in spatial features. The
efficacy of our approach is validated on several benchmarks, including
Human3.6M, 3DPW, LSP, and COCO, where experimental results show that our
approach consistently improves the mesh-image alignment of the reconstruction.
The project page with code and video results can be found at
https://hongwenzhang.github.io/pymaf.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Hongwen Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1"&gt;Yating Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1"&gt;Xinchi Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1"&gt;Wanli Ouyang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yebin Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Limin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1"&gt;Zhenan Sun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Boosting Video Captioning with Dynamic Loss Network. (arXiv:2107.11707v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.11707</id>
        <link href="http://arxiv.org/abs/2107.11707"/>
        <updated>2021-08-03T02:06:32.079Z</updated>
        <summary type="html"><![CDATA[Video captioning is one of the challenging problems at the intersection of
vision and language, having many real-life applications in video retrieval,
video surveillance, assisting visually challenged people, Human-machine
interface, and many more. Recent deep learning-based methods have shown
promising results but are still on the lower side than other vision tasks (such
as image classification, object detection). A significant drawback with
existing video captioning methods is that they are optimized over cross-entropy
loss function, which is uncorrelated to the de facto evaluation metrics (BLEU,
METEOR, CIDER, ROUGE).In other words, cross-entropy is not a proper surrogate
of the true loss function for video captioning. This paper addresses the
drawback by introducing a dynamic loss network (DLN), which provides an
additional feedback signal that directly reflects the evaluation metrics. Our
results on Microsoft Research Video Description Corpus (MSVD) and MSR-Video to
Text (MSRVTT) datasets outperform previous methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nasibullah/0/1/0/all/0/1"&gt;Nasibullah&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mohanta_P/0/1/0/all/0/1"&gt;Partha Pratim Mohanta&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Head Self-Attention via Vision Transformer for Zero-Shot Learning. (arXiv:2108.00045v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00045</id>
        <link href="http://arxiv.org/abs/2108.00045"/>
        <updated>2021-08-03T02:06:32.070Z</updated>
        <summary type="html"><![CDATA[Zero-Shot Learning (ZSL) aims to recognise unseen object classes, which are
not observed during the training phase. The existing body of works on ZSL
mostly relies on pretrained visual features and lacks the explicit attribute
localisation mechanism on images. In this work, we propose an attention-based
model in the problem settings of ZSL to learn attributes useful for unseen
class recognition. Our method uses an attention mechanism adapted from Vision
Transformer to capture and learn discriminative attributes by splitting images
into small patches. We conduct experiments on three popular ZSL benchmarks
(i.e., AWA2, CUB and SUN) and set new state-of-the-art harmonic mean results
{on all the three datasets}, which illustrate the effectiveness of our proposed
method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alamri_F/0/1/0/all/0/1"&gt;Faisal Alamri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dutta_A/0/1/0/all/0/1"&gt;Anjan Dutta&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Personalized Stress Monitoring using Wearable Sensors in Everyday Settings. (arXiv:2108.00144v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.00144</id>
        <link href="http://arxiv.org/abs/2108.00144"/>
        <updated>2021-08-03T02:06:32.061Z</updated>
        <summary type="html"><![CDATA[Since stress contributes to a broad range of mental and physical health
problems, the objective assessment of stress is essential for behavioral and
physiological studies. Although several studies have evaluated stress levels in
controlled settings, objective stress assessment in everyday settings is still
largely under-explored due to challenges arising from confounding contextual
factors and limited adherence for self-reports. In this paper, we explore the
objective prediction of stress levels in everyday settings based on heart rate
(HR) and heart rate variability (HRV) captured via low-cost and easy-to-wear
photoplethysmography (PPG) sensors that are widely available on newer smart
wearable devices. We present a layered system architecture for personalized
stress monitoring that supports a tunable collection of data samples for
labeling, and present a method for selecting informative samples from the
stream of real-time data for labeling. We captured the stress levels of
fourteen volunteers through self-reported questionnaires over periods of
between 1-3 months, and explored binary stress detection based on HR and HRV
using Machine Learning Methods. We observe promising preliminary results given
that the dataset is collected in the challenging environments of everyday
settings. The binary stress detector is fairly accurate and can detect
stressful vs non-stressful samples with a macro-F1 score of up to \%76. Our
study lays the groundwork for more sophisticated labeling strategies that
generate context-aware, personalized models that will empower health
professionals to provide personalized interventions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tazarv_A/0/1/0/all/0/1"&gt;Ali Tazarv&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Labbaf_S/0/1/0/all/0/1"&gt;Sina Labbaf&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reich_S/0/1/0/all/0/1"&gt;Stephanie M. Reich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dutt_N/0/1/0/all/0/1"&gt;Nikil Dutt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rahmani_A/0/1/0/all/0/1"&gt;Amir M. Rahmani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Levorato_M/0/1/0/all/0/1"&gt;Marco Levorato&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Surgical Data Science -- from Concepts toward Clinical Translation. (arXiv:2011.02284v2 [cs.CY] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.02284</id>
        <link href="http://arxiv.org/abs/2011.02284"/>
        <updated>2021-08-03T02:06:32.050Z</updated>
        <summary type="html"><![CDATA[Recent developments in data science in general and machine learning in
particular have transformed the way experts envision the future of surgery.
Surgical Data Science (SDS) is a new research field that aims to improve the
quality of interventional healthcare through the capture, organization,
analysis and modeling of data. While an increasing number of data-driven
approaches and clinical applications have been studied in the fields of
radiological and clinical data science, translational success stories are still
lacking in surgery. In this publication, we shed light on the underlying
reasons and provide a roadmap for future advances in the field. Based on an
international workshop involving leading researchers in the field of SDS, we
review current practice, key achievements and initiatives as well as available
standards and tools for a number of topics relevant to the field, namely (1)
infrastructure for data acquisition, storage and access in the presence of
regulatory constraints, (2) data annotation and sharing and (3) data analytics.
We further complement this technical perspective with (4) a review of currently
available SDS products and the translational progress from academia and (5) a
roadmap for faster clinical translation and exploitation of the full potential
of SDS, based on an international multi-round Delphi process.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Maier_Hein_L/0/1/0/all/0/1"&gt;Lena Maier-Hein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eisenmann_M/0/1/0/all/0/1"&gt;Matthias Eisenmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sarikaya_D/0/1/0/all/0/1"&gt;Duygu Sarikaya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marz_K/0/1/0/all/0/1"&gt;Keno M&amp;#xe4;rz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Collins_T/0/1/0/all/0/1"&gt;Toby Collins&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Malpani_A/0/1/0/all/0/1"&gt;Anand Malpani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fallert_J/0/1/0/all/0/1"&gt;Johannes Fallert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feussner_H/0/1/0/all/0/1"&gt;Hubertus Feussner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Giannarou_S/0/1/0/all/0/1"&gt;Stamatia Giannarou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mascagni_P/0/1/0/all/0/1"&gt;Pietro Mascagni&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nakawala_H/0/1/0/all/0/1"&gt;Hirenkumar Nakawala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_A/0/1/0/all/0/1"&gt;Adrian Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pugh_C/0/1/0/all/0/1"&gt;Carla Pugh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stoyanov_D/0/1/0/all/0/1"&gt;Danail Stoyanov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vedula_S/0/1/0/all/0/1"&gt;Swaroop S. Vedula&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cleary_K/0/1/0/all/0/1"&gt;Kevin Cleary&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fichtinger_G/0/1/0/all/0/1"&gt;Gabor Fichtinger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Forestier_G/0/1/0/all/0/1"&gt;Germain Forestier&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gibaud_B/0/1/0/all/0/1"&gt;Bernard Gibaud&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grantcharov_T/0/1/0/all/0/1"&gt;Teodor Grantcharov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hashizume_M/0/1/0/all/0/1"&gt;Makoto Hashizume&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Heckmann_Notzel_D/0/1/0/all/0/1"&gt;Doreen Heckmann-N&amp;#xf6;tzel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kenngott_H/0/1/0/all/0/1"&gt;Hannes G. Kenngott&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kikinis_R/0/1/0/all/0/1"&gt;Ron Kikinis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mundermann_L/0/1/0/all/0/1"&gt;Lars M&amp;#xfc;ndermann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Navab_N/0/1/0/all/0/1"&gt;Nassir Navab&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Onogur_S/0/1/0/all/0/1"&gt;Sinan Onogur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sznitman_R/0/1/0/all/0/1"&gt;Raphael Sznitman&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Taylor_R/0/1/0/all/0/1"&gt;Russell H. Taylor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tizabi_M/0/1/0/all/0/1"&gt;Minu D. Tizabi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wagner_M/0/1/0/all/0/1"&gt;Martin Wagner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hager_G/0/1/0/all/0/1"&gt;Gregory D. Hager&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Neumuth_T/0/1/0/all/0/1"&gt;Thomas Neumuth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Padoy_N/0/1/0/all/0/1"&gt;Nicolas Padoy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Collins_J/0/1/0/all/0/1"&gt;Justin Collins&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gockel_I/0/1/0/all/0/1"&gt;Ines Gockel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goedeke_J/0/1/0/all/0/1"&gt;Jan Goedeke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hashimoto_D/0/1/0/all/0/1"&gt;Daniel A. Hashimoto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Joyeux_L/0/1/0/all/0/1"&gt;Luc Joyeux&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lam_K/0/1/0/all/0/1"&gt;Kyle Lam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leff_D/0/1/0/all/0/1"&gt;Daniel R. Leff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Madani_A/0/1/0/all/0/1"&gt;Amin Madani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marcus_H/0/1/0/all/0/1"&gt;Hani J. Marcus&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meireles_O/0/1/0/all/0/1"&gt;Ozanan Meireles&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Seitel_A/0/1/0/all/0/1"&gt;Alexander Seitel&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Teber_D/0/1/0/all/0/1"&gt;Dogu Teber&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Uckert_F/0/1/0/all/0/1"&gt;Frank &amp;#xdc;ckert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Muller_Stich_B/0/1/0/all/0/1"&gt;Beat P. M&amp;#xfc;ller-Stich&lt;/a&gt;, et al. (2 additional authors not shown)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Physics-Informed Machine Learning Method for Large-Scale Data Assimilation Problems. (arXiv:2108.00037v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.00037</id>
        <link href="http://arxiv.org/abs/2108.00037"/>
        <updated>2021-08-03T02:06:32.041Z</updated>
        <summary type="html"><![CDATA[We develop a physics-informed machine learning approach for large-scale data
assimilation and parameter estimation and apply it for estimating
transmissivity and hydraulic head in the two-dimensional steady-state
subsurface flow model of the Hanford Site given synthetic measurements of said
variables. In our approach, we extend the physics-informed conditional
Karhunen-Lo\'{e}ve expansion (PICKLE) method for modeling subsurface flow with
unknown flux (Neumann) and varying head (Dirichlet) boundary conditions. We
demonstrate that the PICKLE method is comparable in accuracy with the standard
maximum a posteriori (MAP) method, but is significantly faster than MAP for
large-scale problems. Both methods use a mesh to discretize the computational
domain. In MAP, the parameters and states are discretized on the mesh;
therefore, the size of the MAP parameter estimation problem directly depends on
the mesh size. In PICKLE, the mesh is used to evaluate the residuals of the
governing equation, while the parameters and states are approximated by the
truncated conditional Karhunen-Lo\'{e}ve expansions with the number of
parameters controlled by the smoothness of the parameter and state fields, and
not by the mesh size. For a considered example, we demonstrate that the
computational cost of PICKLE increases near linearly (as $N_{FV}^{1.15}$) with
the number of grid points $N_{FV}$, while that of MAP increases much faster as
$N_{FV}^{3.28}$. We demonstrated that once trained for one set of Dirichlet
boundary conditions (i.e., one river stage), the PICKLE method provides
accurate estimates of the hydraulic head for any value of the Dirichlet
boundary conditions (i.e., for any river stage).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yeung_Y/0/1/0/all/0/1"&gt;Yu-Hong Yeung&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Barajas_Solano_D/0/1/0/all/0/1"&gt;David A. Barajas-Solano&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/cs/1/au:+Tartakovsky_A/0/1/0/all/0/1"&gt;Alexandre M. Tartakovsky&lt;/a&gt; (1 and 2) ((1) Physical and Computational Sciences Directorate, Pacific Northwest National Laboratory, (2) Department of Civil and Environmental Engineering, University of Illinois Urbana-Champaign)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Review of Generative Adversarial Networks in Cancer Imaging: New Applications, New Solutions. (arXiv:2107.09543v1 [eess.IV] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2107.09543</id>
        <link href="http://arxiv.org/abs/2107.09543"/>
        <updated>2021-08-03T02:06:32.034Z</updated>
        <summary type="html"><![CDATA[Despite technological and medical advances, the detection, interpretation,
and treatment of cancer based on imaging data continue to pose significant
challenges. These include high inter-observer variability, difficulty of
small-sized lesion detection, nodule interpretation and malignancy
determination, inter- and intra-tumour heterogeneity, class imbalance,
segmentation inaccuracies, and treatment effect uncertainty. The recent
advancements in Generative Adversarial Networks (GANs) in computer vision as
well as in medical imaging may provide a basis for enhanced capabilities in
cancer detection and analysis. In this review, we assess the potential of GANs
to address a number of key challenges of cancer imaging, including data
scarcity and imbalance, domain and dataset shifts, data access and privacy,
data annotation and quantification, as well as cancer detection, tumour
profiling and treatment planning. We provide a critical appraisal of the
existing literature of GANs applied to cancer imagery, together with
suggestions on future research directions to address these challenges. We
analyse and discuss 163 papers that apply adversarial training techniques in
the context of cancer imaging and elaborate their methodologies, advantages and
limitations. With this work, we strive to bridge the gap between the needs of
the clinical cancer imaging community and the current and prospective research
on GANs in the artificial intelligence community.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Osuala_R/0/1/0/all/0/1"&gt;Richard Osuala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kushibar_K/0/1/0/all/0/1"&gt;Kaisar Kushibar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Garrucho_L/0/1/0/all/0/1"&gt;Lidia Garrucho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Linardos_A/0/1/0/all/0/1"&gt;Akis Linardos&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Szafranowska_Z/0/1/0/all/0/1"&gt;Zuzanna Szafranowska&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Klein_S/0/1/0/all/0/1"&gt;Stefan Klein&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Glocker_B/0/1/0/all/0/1"&gt;Ben Glocker&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Diaz_O/0/1/0/all/0/1"&gt;Oliver Diaz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lekadir_K/0/1/0/all/0/1"&gt;Karim Lekadir&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Learning based Virtual Point Tracking for Real-Time Target-less Dynamic Displacement Measurement in Railway Applications. (arXiv:2101.06702v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.06702</id>
        <link href="http://arxiv.org/abs/2101.06702"/>
        <updated>2021-08-03T02:06:32.028Z</updated>
        <summary type="html"><![CDATA[In the application of computer-vision based displacement measurement, an
optical target is usually required to prove the reference. In the case that the
optical target cannot be attached to the measuring objective, edge detection,
feature matching and template matching are the most common approaches in
target-less photogrammetry. However, their performance significantly relies on
parameter settings. This becomes problematic in dynamic scenes where
complicated background texture exists and varies over time. To tackle this
issue, we propose virtual point tracking for real-time target-less dynamic
displacement measurement, incorporating deep learning techniques and domain
knowledge. Our approach consists of three steps: 1) automatic calibration for
detection of region of interest; 2) virtual point detection for each video
frame using deep convolutional neural network; 3) domain-knowledge based rule
engine for point tracking in adjacent frames. The proposed approach can be
executed on an edge computer in a real-time manner (i.e. over 30 frames per
second). We demonstrate our approach for a railway application, where the
lateral displacement of the wheel on the rail is measured during operation. We
also implement an algorithm using template matching and line detection as the
baseline for comparison. The numerical experiments have been performed to
evaluate the performance and the latency of our approach in the harsh railway
environment with noisy and varying backgrounds.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shi_D/0/1/0/all/0/1"&gt;Dachuan Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sabanovic_E/0/1/0/all/0/1"&gt;Eldar Sabanovic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rizzetto_L/0/1/0/all/0/1"&gt;Luca Rizzetto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Skrickij_V/0/1/0/all/0/1"&gt;Viktor Skrickij&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oliverio_R/0/1/0/all/0/1"&gt;Roberto Oliverio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kaviani_N/0/1/0/all/0/1"&gt;Nadia Kaviani&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1"&gt;Yunguang Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bureika_G/0/1/0/all/0/1"&gt;Gintautas Bureika&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ricci_S/0/1/0/all/0/1"&gt;Stefano Ricci&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hecht_M/0/1/0/all/0/1"&gt;Markus Hecht&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AINet: Association Implantation for Superpixel Segmentation. (arXiv:2101.10696v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.10696</id>
        <link href="http://arxiv.org/abs/2101.10696"/>
        <updated>2021-08-03T02:06:31.986Z</updated>
        <summary type="html"><![CDATA[Recently, some approaches are proposed to harness deep convolutional networks
to facilitate superpixel segmentation. The common practice is to first evenly
divide the image into a pre-defined number of grids and then learn to associate
each pixel with its surrounding grids. However, simply applying a series of
convolution operations with limited receptive fields can only implicitly
perceive the relations between the pixel and its surrounding grids.
Consequently, existing methods often fail to provide an effective context when
inferring the association map. To remedy this issue, we propose a novel
\textbf{A}ssociation \textbf{I}mplantation (AI) module to enable the network to
explicitly capture the relations between the pixel and its surrounding grids.
The proposed AI module directly implants the features of grid cells to the
surrounding of its corresponding central pixel, and conducts convolution on the
padded window to adaptively transfer knowledge between them. With such an
implantation operation, the network could explicitly harvest the pixel-grid
level context, which is more in line with the target of superpixel segmentation
comparing to the pixel-wise relation. Furthermore, to pursue better boundary
precision, we design a boundary-perceiving loss to help the network
discriminate the pixels around boundaries in hidden feature level, which could
benefit the subsequent inferring modules to accurately identify more boundary
pixels. Extensive experiments on BSDS500 and NYUv2 datasets show that our
method could not only achieve state-of-the-art performance but maintain
satisfactory inference efficiency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yaxiong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1"&gt;Yunchao Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qian_X/0/1/0/all/0/1"&gt;Xueming Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1"&gt;Li Zhu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yi Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Goal-constrained Sparse Reinforcement Learning for End-to-End Driving. (arXiv:2103.09189v2 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.09189</id>
        <link href="http://arxiv.org/abs/2103.09189"/>
        <updated>2021-08-03T02:06:31.976Z</updated>
        <summary type="html"><![CDATA[Deep reinforcement Learning for end-to-end driving is limited by the need of
complex reward engineering. Sparse rewards can circumvent this challenge but
suffers from long training time and leads to sub-optimal policy. In this work,
we explore full-control driving with only goal-constrained sparse reward and
propose a curriculum learning approach for end-to-end driving using only
navigation view maps that benefit from small virtual-to-real domain gap. To
address the complexity of multiple driving policies, we learn concurrent
individual policies selected at inference by a navigation system. We
demonstrate the ability of our proposal to generalize on unseen road layout,
and to drive significantly longer than in the training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Agarwal_P/0/1/0/all/0/1"&gt;Pranav Agarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Beaucorps_P/0/1/0/all/0/1"&gt;Pierre de Beaucorps&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Charette_R/0/1/0/all/0/1"&gt;Raoul de Charette&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PLUMENet: Efficient 3D Object Detection from Stereo Images. (arXiv:2101.06594v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.06594</id>
        <link href="http://arxiv.org/abs/2101.06594"/>
        <updated>2021-08-03T02:06:31.966Z</updated>
        <summary type="html"><![CDATA[3D object detection is a key component of many robotic applications such as
self-driving vehicles. While many approaches rely on expensive 3D sensors such
as LiDAR to produce accurate 3D estimates, methods that exploit stereo cameras
have recently shown promising results at a lower cost. Existing approaches
tackle this problem in two steps: first depth estimation from stereo images is
performed to produce a pseudo LiDAR point cloud, which is then used as input to
a 3D object detector. However, this approach is suboptimal due to the
representation mismatch, as the two tasks are optimized in two different metric
spaces. In this paper we propose a model that unifies these two tasks and
performs them in the same metric space. Specifically, we directly construct a
pseudo LiDAR feature volume (PLUME) in 3D space, which is then used to solve
both depth estimation and object detection tasks. Our approach achieves
state-of-the-art performance with much faster inference times when compared to
existing methods on the challenging KITTI benchmark.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1"&gt;Bin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_R/0/1/0/all/0/1"&gt;Rui Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_M/0/1/0/all/0/1"&gt;Ming Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Urtasun_R/0/1/0/all/0/1"&gt;Raquel Urtasun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Parameter-Efficient Person Re-identification in the 3D Space. (arXiv:2006.04569v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.04569</id>
        <link href="http://arxiv.org/abs/2006.04569"/>
        <updated>2021-08-03T02:06:31.958Z</updated>
        <summary type="html"><![CDATA[People live in a 3D world. However, existing works on person
re-identification (re-id) mostly consider the semantic representation learning
in a 2D space, intrinsically limiting the understanding of people. In this
work, we address this limitation by exploring the prior knowledge of the 3D
body structure. Specifically, we project 2D images to a 3D space and introduce
a novel parameter-efficient Omni-scale Graph Network (OG-Net) to learn the
pedestrian representation directly from 3D point clouds. OG-Net effectively
exploits the local information provided by sparse 3D points and takes advantage
of the structure and appearance information in a coherent manner. With the help
of 3D geometry information, we can learn a new type of deep re-id feature free
from noisy variants, such as scale and viewpoint. To our knowledge, we are
among the first attempts to conduct person re-identification in the 3D space.
We demonstrate through extensive experiments that the proposed method (1) eases
the matching difficulty in the traditional 2D space, (2) exploits the
complementary information of 2D appearance and 3D structure, (3) achieves
competitive results with limited parameters on four large-scale person re-id
datasets, and (4) has good scalability to unseen datasets. Our code, models and
generated 3D human data are publicly available at
https://github.com/layumi/person-reid-3d .]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1"&gt;Zhedong Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_N/0/1/0/all/0/1"&gt;Nenggan Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yi Yang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Spatial-spectral Network Learning for Hyperspectral Compressive Snapshot Reconstruction. (arXiv:2012.12086v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.12086</id>
        <link href="http://arxiv.org/abs/2012.12086"/>
        <updated>2021-08-03T02:06:31.942Z</updated>
        <summary type="html"><![CDATA[Hyperspectral compressive imaging takes advantage of compressive sensing
theory to achieve coded aperture snapshot measurement without temporal
scanning, and the entire three-dimensional spatial-spectral data is captured by
a two-dimensional projection during a single integration period. Its core issue
is how to reconstruct the underlying hyperspectral image using compressive
sensing reconstruction algorithms. Due to the diversity in the spectral
response characteristics and wavelength range of different spectral imaging
devices, previous works are often inadequate to capture complex spectral
variations or lack the adaptive capacity to new hyperspectral imagers. In order
to address these issues, we propose an unsupervised spatial-spectral network to
reconstruct hyperspectral images only from the compressive snapshot
measurement. The proposed network acts as a conditional generative model
conditioned on the snapshot measurement, and it exploits the spatial-spectral
attention module to capture the joint spatial-spectral correlation of
hyperspectral images. The network parameters are optimized to make sure that
the network output can closely match the given snapshot measurement according
to the imaging model, thus the proposed network can adapt to different imaging
settings, which can inherently enhance the applicability of the network.
Extensive experiments upon multiple datasets demonstrate that our network can
achieve better reconstruction results than the state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Sun_Y/0/1/0/all/0/1"&gt;Yubao Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Ying Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Liu_Q/0/1/0/all/0/1"&gt;Qingshan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kankanhalli_M/0/1/0/all/0/1"&gt;Mohan Kankanhalli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Edge-competing Pathological Liver Vessel Segmentation with Limited Labels. (arXiv:2108.00384v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00384</id>
        <link href="http://arxiv.org/abs/2108.00384"/>
        <updated>2021-08-03T02:06:31.925Z</updated>
        <summary type="html"><![CDATA[The microvascular invasion (MVI) is a major prognostic factor in
hepatocellular carcinoma, which is one of the malignant tumors with the highest
mortality rate. The diagnosis of MVI needs discovering the vessels that contain
hepatocellular carcinoma cells and counting their number in each vessel, which
depends heavily on experiences of the doctor, is largely subjective and
time-consuming. However, there is no algorithm as yet tailored for the MVI
detection from pathological images. This paper collects the first pathological
liver image dataset containing 522 whole slide images with labels of vessels,
MVI, and hepatocellular carcinoma grades. The first and essential step for the
automatic diagnosis of MVI is the accurate segmentation of vessels. The unique
characteristics of pathological liver images, such as super-large size,
multi-scale vessel, and blurred vessel edges, make the accurate vessel
segmentation challenging. Based on the collected dataset, we propose an
Edge-competing Vessel Segmentation Network (EVS-Net), which contains a
segmentation network and two edge segmentation discriminators. The segmentation
network, combined with an edge-aware self-supervision mechanism, is devised to
conduct vessel segmentation with limited labeled patches. Meanwhile, two
discriminators are introduced to distinguish whether the segmented vessel and
background contain residual features in an adversarial manner. In the training
stage, two discriminators are devised tocompete for the predicted position of
edges. Exhaustive experiments demonstrate that, with only limited labeled
patches, EVS-Net achieves a close performance of fully supervised methods,
which provides a convenient tool for the pathological liver vessel
segmentation. Code is publicly available at
https://github.com/zju-vipa/EVS-Net.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1"&gt;Zunlei Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhonghua Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xinchao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xiuming Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1"&gt;Lechao Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1"&gt;Jie Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yuexuan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_M/0/1/0/all/0/1"&gt;Mingli Song&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DySMHO: Data-Driven Discovery of Governing Equations for Dynamical Systems via Moving Horizon Optimization. (arXiv:2108.00069v1 [math.DS])]]></title>
        <id>http://arxiv.org/abs/2108.00069</id>
        <link href="http://arxiv.org/abs/2108.00069"/>
        <updated>2021-08-03T02:06:31.892Z</updated>
        <summary type="html"><![CDATA[Discovering the governing laws underpinning physical and chemical phenomena
is a key step towards understanding and ultimately controlling systems in
science and engineering. We introduce Discovery of Dynamical Systems via Moving
Horizon Optimization (DySMHO), a scalable machine learning framework for
identifying governing laws in the form of differential equations from
large-scale noisy experimental data sets. DySMHO consists of a novel moving
horizon dynamic optimization strategy that sequentially learns the underlying
governing equations from a large dictionary of basis functions. The sequential
nature of DySMHO allows leveraging statistical arguments for eliminating
irrelevant basis functions, avoiding overfitting to recover accurate and
parsimonious forms of the governing equations. Canonical nonlinear dynamical
system examples are used to demonstrate that DySMHO can accurately recover the
governing laws, is robust to high levels of measurement noise and that it can
handle challenges such as multiple time scale dynamics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Lejarza_F/0/1/0/all/0/1"&gt;Fernando Lejarza&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Baldea_M/0/1/0/all/0/1"&gt;Michael Baldea&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Coordinate descent on the orthogonal group for recurrent neural network training. (arXiv:2108.00051v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.00051</id>
        <link href="http://arxiv.org/abs/2108.00051"/>
        <updated>2021-08-03T02:06:31.886Z</updated>
        <summary type="html"><![CDATA[We propose to use stochastic Riemannian coordinate descent on the orthogonal
group for recurrent neural network training. The algorithm rotates successively
two columns of the recurrent matrix, an operation that can be efficiently
implemented as a multiplication by a Givens matrix. In the case when the
coordinate is selected uniformly at random at each iteration, we prove the
convergence of the proposed algorithm under standard assumptions on the loss
function, stepsize and minibatch noise. In addition, we numerically demonstrate
that the Riemannian gradient in recurrent neural network training has an
approximately sparse structure. Leveraging this observation, we propose a
faster variant of the proposed algorithm that relies on the Gauss-Southwell
rule. Experiments on a benchmark recurrent neural network training problem are
presented to demonstrate the effectiveness of the proposed algorithm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Massart_E/0/1/0/all/0/1"&gt;Estelle Massart&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abrol_V/0/1/0/all/0/1"&gt;Vinayak Abrol&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Real-time Instance Segmentation with Discriminative Orientation Maps. (arXiv:2106.12204v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.12204</id>
        <link href="http://arxiv.org/abs/2106.12204"/>
        <updated>2021-08-03T02:06:31.879Z</updated>
        <summary type="html"><![CDATA[Although instance segmentation has made considerable advancement over recent
years, it's still a challenge to design high accuracy algorithms with real-time
performance. In this paper, we propose a real-time instance segmentation
framework termed OrienMask. Upon the one-stage object detector YOLOv3, a mask
head is added to predict some discriminative orientation maps, which are
explicitly defined as spatial offset vectors for both foreground and background
pixels. Thanks to the discrimination ability of orientation maps, masks can be
recovered without the need for extra foreground segmentation. All instances
that match with the same anchor size share a common orientation map. This
special sharing strategy reduces the amortized memory utilization for mask
predictions but without loss of mask granularity. Given the surviving box
predictions after NMS, instance masks can be concurrently constructed from the
corresponding orientation maps with low complexity. Owing to the concise design
for mask representation and its effective integration with the anchor-based
object detector, our method is qualified under real-time conditions while
maintaining competitive accuracy. Experiments on COCO benchmark show that
OrienMask achieves 34.8 mask AP at the speed of 42.7 fps evaluated with a
single RTX 2080 Ti. The code is available at https://github.com/duwt/OrienMask.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Du_W/0/1/0/all/0/1"&gt;Wentao Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiang_Z/0/1/0/all/0/1"&gt;Zhiyu Xiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Shuya Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qiao_C/0/1/0/all/0/1"&gt;Chengyu Qiao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yiman Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bai_T/0/1/0/all/0/1"&gt;Tingming Bai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Unified Batch Selection Policy for Active Metric Learning. (arXiv:2102.07365v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.07365</id>
        <link href="http://arxiv.org/abs/2102.07365"/>
        <updated>2021-08-03T02:06:31.872Z</updated>
        <summary type="html"><![CDATA[Active metric learning is the problem of incrementally selecting high-utility
batches of training data (typically, ordered triplets) to annotate, in order to
progressively improve a learned model of a metric over some input domain as
rapidly as possible. Standard approaches, which independently assess the
informativeness of each triplet in a batch, are susceptible to highly
correlated batches with many redundant triplets and hence low overall utility.
While a recent work \cite{kumari2020batch} proposes batch-decorrelation
strategies for metric learning, they rely on ad hoc heuristics to estimate the
correlation between two triplets at a time. We present a novel batch active
metric learning method that leverages the Maximum Entropy Principle to learn
the least biased estimate of triplet distribution for a given set of prior
constraints. To avoid redundancy between triplets, our method collectively
selects batches with maximum joint entropy, which simultaneously captures both
informativeness and diversity. We take advantage of the submodularity of the
joint entropy function to construct a tractable solution using an efficient
greedy algorithm based on Gram-Schmidt orthogonalization that is provably
$\left( 1 - \frac{1}{e} \right)$-optimal. Our approach is the first batch
active metric learning method to define a unified score that balances
informativeness and diversity for an entire batch of triplets. Experiments with
several real-world datasets demonstrate that our algorithm is robust,
generalizes well to different applications and input modalities, and
consistently outperforms the state-of-the-art.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+K_P/0/1/0/all/0/1"&gt;Priyadarshini K&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chaudhuri_S/0/1/0/all/0/1"&gt;Siddhartha Chaudhuri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Borkar_V/0/1/0/all/0/1"&gt;Vivek Borkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chaudhuri_S/0/1/0/all/0/1"&gt;Subhasis Chaudhuri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Structured Context Enhancement Network for Mouse Pose Estimation. (arXiv:2012.00630v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.00630</id>
        <link href="http://arxiv.org/abs/2012.00630"/>
        <updated>2021-08-03T02:06:31.865Z</updated>
        <summary type="html"><![CDATA[Automated analysis of mouse behaviours is crucial for many applications in
neuroscience. However, quantifying mouse behaviours from videos or images
remains a challenging problem, where pose estimation plays an important role in
describing mouse behaviours. Although deep learning based methods have made
promising advances in human pose estimation, they cannot be directly applied to
pose estimation of mice due to different physiological natures. Particularly,
since mouse body is highly deformable, it is a challenge to accurately locate
different keypoints on the mouse body. In this paper, we propose a novel
Hourglass network based model, namely Graphical Model based Structured Context
Enhancement Network (GM-SCENet) where two effective modules, i.e., Structured
Context Mixer (SCM) and Cascaded Multi-Level Supervision (CMLS) are
subsequently implemented. SCM can adaptively learn and enhance the proposed
structured context information of each mouse part by a novel graphical model
that takes into account the motion difference between body parts. Then, the
CMLS module is designed to jointly train the proposed SCM and the Hourglass
network by generating multi-level information, increasing the robustness of the
whole network.Using the multi-level prediction information from SCM and CMLS,
we develop an inference method to ensure the accuracy of the localisation
results. Finally, we evaluate our proposed approach against several
baselines...]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_F/0/1/0/all/0/1"&gt;Feixiang Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1"&gt;Zheheng Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhihua Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1"&gt;Fang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Long Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tong_L/0/1/0/all/0/1"&gt;Lei Tong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1"&gt;Zhile Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Haikuan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fei_M/0/1/0/all/0/1"&gt;Minrui Fei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Ling Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1"&gt;Huiyu Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A New Semi-supervised Learning Benchmark for Classifying View and Diagnosing Aortic Stenosis from Echocardiograms. (arXiv:2108.00080v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00080</id>
        <link href="http://arxiv.org/abs/2108.00080"/>
        <updated>2021-08-03T02:06:31.848Z</updated>
        <summary type="html"><![CDATA[Semi-supervised image classification has shown substantial progress in
learning from limited labeled data, but recent advances remain largely untested
for clinical applications. Motivated by the urgent need to improve timely
diagnosis of life-threatening heart conditions, especially aortic stenosis, we
develop a benchmark dataset to assess semi-supervised approaches to two tasks
relevant to cardiac ultrasound (echocardiogram) interpretation: view
classification and disease severity classification. We find that a
state-of-the-art method called MixMatch achieves promising gains in heldout
accuracy on both tasks, learning from a large volume of truly unlabeled images
as well as a labeled set collected at great expense to achieve better
performance than is possible with the labeled set alone. We further pursue
patient-level diagnosis prediction, which requires aggregating across hundreds
of images of diverse view types, most of which are irrelevant, to make a
coherent prediction. The best patient-level performance is achieved by new
methods that prioritize diagnosis predictions from images that are predicted to
be clinically-relevant views and transfer knowledge from the view task to the
diagnosis task. We hope our released Tufts Medical Echocardiogram Dataset and
evaluation framework inspire further improvements in multi-task semi-supervised
learning for clinical applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zhe Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Long_G/0/1/0/all/0/1"&gt;Gary Long&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wessler_B/0/1/0/all/0/1"&gt;Benjamin Wessler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hughes_M/0/1/0/all/0/1"&gt;Michael C. Hughes&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bayesian Optimization in Materials Science: A Survey. (arXiv:2108.00002v1 [cond-mat.mtrl-sci])]]></title>
        <id>http://arxiv.org/abs/2108.00002</id>
        <link href="http://arxiv.org/abs/2108.00002"/>
        <updated>2021-08-03T02:06:31.842Z</updated>
        <summary type="html"><![CDATA[Bayesian optimization is used in many areas of AI for the optimization of
black-box processes and has achieved impressive improvements of the state of
the art for a lot of applications. It intelligently explores large and complex
design spaces while minimizing the number of evaluations of the expensive
underlying process to be optimized. Materials science considers the problem of
optimizing materials' properties given a large design space that defines how to
synthesize or process them, with evaluations requiring expensive experiments or
simulations -- a very similar setting. While Bayesian optimization is also a
popular approach to tackle such problems, there is almost no overlap between
the two communities that are investigating the same concepts. We present a
survey of Bayesian optimization approaches in materials science to increase
cross-fertilization and avoid duplication of work. We highlight common
challenges and opportunities for joint research efforts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cond-mat/1/au:+Kotthoff_L/0/1/0/all/0/1"&gt;Lars Kotthoff&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Wahab_H/0/1/0/all/0/1"&gt;Hud Wahab&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cond-mat/1/au:+Johnson_P/0/1/0/all/0/1"&gt;Patrick Johnson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tensor-Train Density Estimation. (arXiv:2108.00089v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.00089</id>
        <link href="http://arxiv.org/abs/2108.00089"/>
        <updated>2021-08-03T02:06:31.835Z</updated>
        <summary type="html"><![CDATA[Estimation of probability density function from samples is one of the central
problems in statistics and machine learning. Modern neural network-based models
can learn high dimensional distributions but have problems with hyperparameter
selection and are often prone to instabilities during training and inference.
We propose a new efficient tensor train-based model for density estimation
(TTDE). Such density parametrization allows exact sampling, calculation of
cumulative and marginal density functions, and partition function. It also has
very intuitive hyperparameters. We develop an efficient non-adversarial
training procedure for TTDE based on the Riemannian optimization. Experimental
results demonstrate the competitive performance of the proposed method in
density estimation and sampling tasks, while TTDE significantly outperforms
competitors in training speed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Novikov_G/0/1/0/all/0/1"&gt;Georgii S. Novikov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Panov_M/0/1/0/all/0/1"&gt;Maxim E. Panov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oseledets_I/0/1/0/all/0/1"&gt;Ivan V. Oseledets&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Drop Points for LiDAR Scan Synthesis. (arXiv:2102.11952v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.11952</id>
        <link href="http://arxiv.org/abs/2102.11952"/>
        <updated>2021-08-03T02:06:31.828Z</updated>
        <summary type="html"><![CDATA[3D laser scanning by LiDAR sensors plays an important role for mobile robots
to understand their surroundings. Nevertheless, not all systems have high
resolution and accuracy due to hardware limitations, weather conditions, and so
on. Generative modeling of LiDAR data as scene priors is one of the promising
solutions to compensate for unreliable or incomplete observations. In this
paper, we propose a novel generative model for learning LiDAR data based on
generative adversarial networks. As in the related studies, we process LiDAR
data as a compact yet lossless representation, a cylindrical depth map.
However, despite the smoothness of real-world objects, many points on the depth
map are dropped out through the laser measurement, which causes learning
difficulty on generative models. To circumvent this issue, we introduce
measurement uncertainty into the generation process, which allows the model to
learn a disentangled representation of the underlying shape and the dropout
noises from a collection of real LiDAR data. To simulate the lossy measurement,
we adopt a differentiable sampling framework to drop points based on the
learned uncertainty. We demonstrate the effectiveness of our method on
synthesis and reconstruction tasks using two datasets. We further showcase
potential applications by restoring LiDAR data with various types of
corruption.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nakashima_K/0/1/0/all/0/1"&gt;Kazuto Nakashima&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kurazume_R/0/1/0/all/0/1"&gt;Ryo Kurazume&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TDA-Net: Fusion of Persistent Homology and Deep Learning Features for COVID-19 Detection in Chest X-Ray Images. (arXiv:2101.08398v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.08398</id>
        <link href="http://arxiv.org/abs/2101.08398"/>
        <updated>2021-08-03T02:06:31.821Z</updated>
        <summary type="html"><![CDATA[Topological Data Analysis (TDA) has emerged recently as a robust tool to
extract and compare the structure of datasets. TDA identifies features in data
such as connected components and holes and assigns a quantitative measure to
these features. Several studies reported that topological features extracted by
TDA tools provide unique information about the data, discover new insights, and
determine which feature is more related to the outcome. On the other hand, the
overwhelming success of deep neural networks in learning patterns and
relationships has been proven on a vast array of data applications, images in
particular. To capture the characteristics of both powerful tools, we propose
\textit{TDA-Net}, a novel ensemble network that fuses topological and deep
features for the purpose of enhancing model generalizability and accuracy. We
apply the proposed \textit{TDA-Net} to a critical application, which is the
automated detection of COVID-19 from CXR images. The experimental results
showed that the proposed network achieved excellent performance and suggests
the applicability of our method in practice.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hajij_M/0/1/0/all/0/1"&gt;Mustafa Hajij&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zamzmi_G/0/1/0/all/0/1"&gt;Ghada Zamzmi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Batayneh_F/0/1/0/all/0/1"&gt;Fawwaz Batayneh&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Toward Robust Autotuning of Noisy Quantum Dot Devices. (arXiv:2108.00043v1 [quant-ph])]]></title>
        <id>http://arxiv.org/abs/2108.00043</id>
        <link href="http://arxiv.org/abs/2108.00043"/>
        <updated>2021-08-03T02:06:31.802Z</updated>
        <summary type="html"><![CDATA[The current autotuning approaches for quantum dot (QD) devices, while showing
some success, lack an assessment of data reliability. This leads to unexpected
failures when noisy data is processed by an autonomous system. In this work, we
propose a framework for robust autotuning of QD devices that combines a machine
learning (ML) state classifier with a data quality control module. The data
quality control module acts as a ``gatekeeper'' system, ensuring that only
reliable data is processed by the state classifier. Lower data quality results
in either device recalibration or termination. To train both ML systems, we
enhance the QD simulation by incorporating synthetic noise typical of QD
experiments. We confirm that the inclusion of synthetic noise in the training
of the state classifier significantly improves the performance, resulting in an
accuracy of 95.1(7) % when tested on experimental data. We then validate the
functionality of the data quality control module by showing the state
classifier performance deteriorates with decreasing data quality, as expected.
Our results establish a robust and flexible ML framework for autonomous tuning
of noisy QD devices.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/quant-ph/1/au:+Ziegler_J/0/1/0/all/0/1"&gt;Joshua Ziegler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+McJunkin_T/0/1/0/all/0/1"&gt;Thomas McJunkin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Joseph_E/0/1/0/all/0/1"&gt;E. S. Joseph&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Kalantre_S/0/1/0/all/0/1"&gt;Sandesh S. Kalantre&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Harpt_B/0/1/0/all/0/1"&gt;Benjamin Harpt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Savage_D/0/1/0/all/0/1"&gt;D. E. Savage&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Lagally_M/0/1/0/all/0/1"&gt;M. G. Lagally&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Eriksson_M/0/1/0/all/0/1"&gt;M. A. Eriksson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Taylor_J/0/1/0/all/0/1"&gt;Jacob M. Taylor&lt;/a&gt;, &lt;a href="http://arxiv.org/find/quant-ph/1/au:+Zwolak_J/0/1/0/all/0/1"&gt;Justyna P. Zwolak&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A 3D model-based approach for fitting masks to faces in the wild. (arXiv:2103.00803v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.00803</id>
        <link href="http://arxiv.org/abs/2103.00803"/>
        <updated>2021-08-03T02:06:31.794Z</updated>
        <summary type="html"><![CDATA[Face recognition now requires a large number of labelled masked face images
in the era of this unprecedented COVID-19 pandemic. Unfortunately, the rapid
spread of the virus has left us little time to prepare for such dataset in the
wild. To circumvent this issue, we present a 3D model-based approach called
WearMask3D for augmenting face images of various poses to the masked face
counterparts. Our method proceeds by first fitting a 3D morphable model on the
input image, second overlaying the mask surface onto the face model and warping
the respective mask texture, and last projecting the 3D mask back to 2D. The
mask texture is adapted based on the brightness and resolution of the input
image. By working in 3D, our method can produce more natural masked faces of
diverse poses from a single mask texture. To compare precisely between
different augmentation approaches, we have constructed a dataset comprising
masked and unmasked faces with labels called MFW-mini. Experimental results
demonstrate WearMask3D produces more realistic masked faces, and utilizing
these images for training leads to state-of-the-art recognition accuracy for
masked faces.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hong_J/0/1/0/all/0/1"&gt;Je Hyeong Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1"&gt;Hanjo Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1"&gt;Minsoo Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nam_G/0/1/0/all/0/1"&gt;Gi Pyo Nam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1"&gt;Junghyun Cho&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ko_H/0/1/0/all/0/1"&gt;Hyeong-Seok Ko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_I/0/1/0/all/0/1"&gt;Ig-Jae Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[OPFython: A Python-Inspired Optimum-Path Forest Classifier. (arXiv:2001.10420v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2001.10420</id>
        <link href="http://arxiv.org/abs/2001.10420"/>
        <updated>2021-08-03T02:06:31.787Z</updated>
        <summary type="html"><![CDATA[Machine learning techniques have been paramount throughout the last years,
being applied in a wide range of tasks, such as classification, object
recognition, person identification, and image segmentation. Nevertheless,
conventional classification algorithms, e.g., Logistic Regression, Decision
Trees, and Bayesian classifiers, might lack complexity and diversity, not
suitable when dealing with real-world data. A recent graph-inspired classifier,
known as the Optimum-Path Forest, has proven to be a state-of-the-art
technique, comparable to Support Vector Machines and even surpassing it in some
tasks. This paper proposes a Python-based Optimum-Path Forest framework,
denoted as OPFython, where all of its functions and classes are based upon the
original C language implementation. Additionally, as OPFython is a Python-based
library, it provides a more friendly environment and a faster prototyping
workspace than the C language.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rosa_G/0/1/0/all/0/1"&gt;Gustavo Henrique de Rosa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Papa_J/0/1/0/all/0/1"&gt;Jo&amp;#xe3;o Paulo Papa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Falcao_A/0/1/0/all/0/1"&gt;Alexandre Xavier Falc&amp;#xe3;o&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI Choreographer: Music Conditioned 3D Dance Generation with AIST++. (arXiv:2101.08779v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.08779</id>
        <link href="http://arxiv.org/abs/2101.08779"/>
        <updated>2021-08-03T02:06:31.779Z</updated>
        <summary type="html"><![CDATA[We present AIST++, a new multi-modal dataset of 3D dance motion and music,
along with FACT, a Full-Attention Cross-modal Transformer network for
generating 3D dance motion conditioned on music. The proposed AIST++ dataset
contains 5.2 hours of 3D dance motion in 1408 sequences, covering 10 dance
genres with multi-view videos with known camera poses -- the largest dataset of
this kind to our knowledge. We show that naively applying sequence models such
as transformers to this dataset for the task of music conditioned 3D motion
generation does not produce satisfactory 3D motion that is well correlated with
the input music. We overcome these shortcomings by introducing key changes in
its architecture design and supervision: FACT model involves a deep cross-modal
transformer block with full-attention that is trained to predict $N$ future
motions. We empirically show that these changes are key factors in generating
long sequences of realistic dance motion that are well-attuned to the input
music. We conduct extensive experiments on AIST++ with user studies, where our
method outperforms recent state-of-the-art methods both qualitatively and
quantitatively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1"&gt;Ruilong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1"&gt;Shan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ross_D/0/1/0/all/0/1"&gt;David A. Ross&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kanazawa_A/0/1/0/all/0/1"&gt;Angjoo Kanazawa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to synthesise the ageing brain without longitudinal data. (arXiv:1912.02620v5 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1912.02620</id>
        <link href="http://arxiv.org/abs/1912.02620"/>
        <updated>2021-08-03T02:06:31.769Z</updated>
        <summary type="html"><![CDATA[How will my face look when I get older? Or, for a more challenging question:
How will my brain look when I get older? To answer this question one must
devise (and learn from data) a multivariate auto-regressive function which
given an image and a desired target age generates an output image. While
collecting data for faces may be easier, collecting longitudinal brain data is
not trivial. We propose a deep learning-based method that learns to simulate
subject-specific brain ageing trajectories without relying on longitudinal
data. Our method synthesises images conditioned on two factors: age (a
continuous variable), and status of Alzheimer's Disease (AD, an ordinal
variable). With an adversarial formulation we learn the joint distribution of
brain appearance, age and AD status, and define reconstruction losses to
address the challenging problem of preserving subject identity. We compare with
several benchmarks using two widely used datasets. We evaluate the quality and
realism of synthesised images using ground-truth longitudinal data and a
pre-trained age predictor. We show that, despite the use of cross-sectional
data, our model learns patterns of gray matter atrophy in the middle temporal
gyrus in patients with AD. To demonstrate generalisation ability, we train on
one dataset and evaluate predictions on the other. In conclusion, our model
shows an ability to separate age, disease influence and anatomy using only 2D
cross-sectional data that should should be useful in large studies into
neurodegenerative disease, that aim to combine several data sources. To
facilitate such future studies by the community at large our code is made
available at https://github.com/xiat0616/BrainAgeing.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Xia_T/0/1/0/all/0/1"&gt;Tian Xia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chartsias_A/0/1/0/all/0/1"&gt;Agisilaos Chartsias&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chengjia Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tsaftaris_S/0/1/0/all/0/1"&gt;Sotirios A. Tsaftaris&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Vision Xformers: Efficient Attention for Image Classification. (arXiv:2107.02239v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.02239</id>
        <link href="http://arxiv.org/abs/2107.02239"/>
        <updated>2021-08-03T02:06:31.748Z</updated>
        <summary type="html"><![CDATA[We propose three improvements to vision transformers (ViT) to reduce the
number of trainable parameters without compromising classification accuracy. We
address two shortcomings of the early ViT architectures -- quadratic bottleneck
of the attention mechanism and the lack of an inductive bias in their
architectures that rely on unrolling the two-dimensional image structure.
Linear attention mechanisms overcome the bottleneck of quadratic complexity,
which restricts application of transformer models in vision tasks. We modify
the ViT architecture to work on longer sequence data by replacing the quadratic
attention with efficient transformers, such as Performer, Linformer and
Nystr\"omformer of linear complexity creating Vision X-formers (ViX). We show
that all three versions of ViX may be more accurate than ViT for image
classification while using far fewer parameters and computational resources. We
also compare their performance with FNet and multi-layer perceptron (MLP)
mixer. We further show that replacing the initial linear embedding layer by
convolutional layers in ViX further increases their performance. Furthermore,
our tests on recent vision transformer models, such as LeViT, Convolutional
vision Transformer (CvT), Compact Convolutional Transformer (CCT) and
Pooling-based Vision Transformer (PiT) show that replacing the attention with
Nystr\"omformer or Performer saves GPU usage and memory without deteriorating
the classification accuracy. We also show that replacing the standard learnable
1D position embeddings in ViT with Rotary Position Embedding (RoPE) give
further improvements in accuracy. Incorporating these changes can democratize
transformers by making them accessible to those with limited data and computing
resources.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jeevan_P/0/1/0/all/0/1"&gt;Pranav Jeevan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sethi_A/0/1/0/all/0/1"&gt;Amit Sethi&lt;/a&gt; (Indian Institute of Technology Bombay)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learn to Match: Automatic Matching Network Design for Visual Tracking. (arXiv:2108.00803v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00803</id>
        <link href="http://arxiv.org/abs/2108.00803"/>
        <updated>2021-08-03T02:06:31.725Z</updated>
        <summary type="html"><![CDATA[Siamese tracking has achieved groundbreaking performance in recent years,
where the essence is the efficient matching operator cross-correlation and its
variants. Besides the remarkable success, it is important to note that the
heuristic matching network design relies heavily on expert experience.
Moreover, we experimentally find that one sole matching operator is difficult
to guarantee stable tracking in all challenging environments. Thus, in this
work, we introduce six novel matching operators from the perspective of feature
fusion instead of explicit similarity learning, namely Concatenation,
Pointwise-Addition, Pairwise-Relation, FiLM, Simple-Transformer and
Transductive-Guidance, to explore more feasibility on matching operator
selection. The analyses reveal these operators' selective adaptability on
different environment degradation types, which inspires us to combine them to
explore complementary features. To this end, we propose binary channel
manipulation (BCM) to search for the optimal combination of these operators.
BCM determines to retrain or discard one operator by learning its contribution
to other tracking steps. By inserting the learned matching networks to a strong
baseline tracker Ocean, our model achieves favorable gains by $67.2 \rightarrow
71.4$, $52.6 \rightarrow 58.3$, $70.3 \rightarrow 76.0$ success on OTB100,
LaSOT, and TrackingNet, respectively. Notably, Our tracker, dubbed AutoMatch,
uses less than half of training data/time than the baseline tracker, and runs
at 50 FPS using PyTorch. Code and model will be released at
https://github.com/JudasDie/SOTS.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhipeng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yihao Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bing Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1"&gt;Weiming Hu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SHD360: A Benchmark Dataset for Salient Human Detection in 360{\deg} Videos. (arXiv:2105.11578v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.11578</id>
        <link href="http://arxiv.org/abs/2105.11578"/>
        <updated>2021-08-03T02:06:31.712Z</updated>
        <summary type="html"><![CDATA[Salient human detection (SHD) in dynamic 360{\deg} immersive videos is of
great importance for various applications such as robotics, inter-human and
human-object interaction in augmented reality. However, 360{\deg} video SHD has
been seldom discussed in the computer vision community due to a lack of
datasets with large-scale omnidirectional videos and rich annotations. To this
end, we propose SHD360, the first 360{\deg} video SHD dataset which contains
various real-life daily scenes. Our SHD360 provides six-level hierarchical
annotations for 6,268 key frames uniformly sampled from 37,403 omnidirectional
video frames at 4K resolution. Specifically, each collected frame is labeled
with a super-class, a sub-class, associated attributes (e.g., geometrical
distortion), bounding boxes and per-pixel object-/instance-level masks. As a
result, our SHD360 contains totally 16,238 salient human instances with
manually annotated pixel-wise ground truth. Since so far there is no method
proposed for 360{\deg} image/video SHD, we systematically benchmark 11
representative state-of-the-art salient object detection (SOD) approaches on
our SHD360, and explore key issues derived from extensive experimenting
results. We hope our proposed dataset and benchmark could serve as a good
starting point for advancing human-centric researches towards 360{\deg}
panoramic data. Our dataset and benchmark will be publicly available at
https://github.com/PanoAsh/SHD360.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yi Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1"&gt;Kang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hamidouche_W/0/1/0/all/0/1"&gt;Wassim Hamidouche&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deforges_O/0/1/0/all/0/1"&gt;Olivier Deforges&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Reducing Effects of Swath Gaps on Unsupervised Machine Learning Models for NASA MODIS Instruments. (arXiv:2106.07113v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.07113</id>
        <link href="http://arxiv.org/abs/2106.07113"/>
        <updated>2021-08-03T02:06:31.702Z</updated>
        <summary type="html"><![CDATA[Due to the nature of their pathways, NASA Terra and NASA Aqua satellites
capture imagery containing swath gaps, which are areas of no data. Swath gaps
can overlap the region of interest (ROI) completely, often rendering the entire
imagery unusable by Machine Learning (ML) models. This problem is further
exacerbated when the ROI rarely occurs (e.g. a hurricane) and, on occurrence,
is partially overlapped with a swath gap. With annotated data as supervision, a
model can learn to differentiate between the area of focus and the swath gap.
However, annotation is expensive and currently the vast majority of existing
data is unannotated. Hence, we propose an augmentation technique that
considerably removes the existence of swath gaps in order to allow CNNs to
focus on the ROI, and thus successfully use data with swath gaps for training.
We experiment on the UC Merced Land Use Dataset, where we add swath gaps
through empty polygons (up to 20 percent areas) and then apply augmentation
techniques to fill the swath gaps. We compare the model trained with our
augmentation techniques on the swath gap-filled data with the model trained on
the original swath gap-less data and note highly augmented performance.
Additionally, we perform a qualitative analysis using activation maps that
visualizes the effectiveness of our trained network in not paying attention to
the swath gaps. We also evaluate our results with a human baseline and show
that, in certain cases, the filled swath gaps look so realistic that even a
human evaluator did not distinguish between original satellite images and swath
gap-filled images. Since this method is aimed at unlabeled data, it is widely
generalizable and impactful for large scale unannotated datasets from various
space data domains.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1"&gt;Sarah Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_E/0/1/0/all/0/1"&gt;Esther Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Koul_A/0/1/0/all/0/1"&gt;Anirudh Koul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ganju_S/0/1/0/all/0/1"&gt;Siddha Ganju&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Praveen_S/0/1/0/all/0/1"&gt;Satyarth Praveen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kasam_M/0/1/0/all/0/1"&gt;Meher Anand Kasam&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Explain and Improve: LRP-Inference Fine-Tuning for Image Captioning Models. (arXiv:2001.01037v5 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2001.01037</id>
        <link href="http://arxiv.org/abs/2001.01037"/>
        <updated>2021-08-03T02:06:31.685Z</updated>
        <summary type="html"><![CDATA[This paper analyzes the predictions of image captioning models with attention
mechanisms beyond visualizing the attention itself. We develop variants of
layer-wise relevance propagation (LRP) and gradient-based explanation methods,
tailored to image captioning models with attention mechanisms. We compare the
interpretability of attention heatmaps systematically against the explanations
provided by explanation methods such as LRP, Grad-CAM, and Guided Grad-CAM. We
show that explanation methods provide simultaneously pixel-wise image
explanations (supporting and opposing pixels of the input image) and linguistic
explanations (supporting and opposing words of the preceding sequence) for each
word in the predicted captions. We demonstrate with extensive experiments that
explanation methods 1) can reveal additional evidence used by the model to make
decisions compared to attention; 2) correlate to object locations with high
precision; 3) are helpful to "debug" the model, e.g. by analyzing the reasons
for hallucinated object words. With the observed properties of explanations, we
further design an LRP-inference fine-tuning strategy that reduces the issue of
object hallucination in image captioning models, and meanwhile, maintains the
sentence fluency. We conduct experiments with two widely used attention
mechanisms: the adaptive attention mechanism calculated with the additive
attention and the multi-head attention mechanism calculated with the scaled dot
product.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1"&gt;Jiamei Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lapuschkin_S/0/1/0/all/0/1"&gt;Sebastian Lapuschkin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Samek_W/0/1/0/all/0/1"&gt;Wojciech Samek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Binder_A/0/1/0/all/0/1"&gt;Alexander Binder&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Channel-wise Knowledge Distillation for Dense Prediction. (arXiv:2011.13256v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.13256</id>
        <link href="http://arxiv.org/abs/2011.13256"/>
        <updated>2021-08-03T02:06:31.670Z</updated>
        <summary type="html"><![CDATA[Knowledge distillation (KD) has been proven to be a simple and effective tool
for training compact models. Almost all KD variants for dense prediction tasks
align the student and teacher networks' feature maps in the spatial domain,
typically by minimizing point-wise and/or pair-wise discrepancy. Observing that
in semantic segmentation, some layers' feature activations of each channel tend
to encode saliency of scene categories (analogue to class activation mapping),
we propose to align features channel-wise between the student and teacher
networks. To this end, we first transform the feature map of each channel into
a probabilty map using softmax normalization, and then minimize the
Kullback-Leibler (KL) divergence of the corresponding channels of the two
networks. By doing so, our method focuses on mimicking the soft distributions
of channels between networks. In particular, the KL divergence enables learning
to pay more attention to the most salient regions of the channel-wise maps,
presumably corresponding to the most useful signals for semantic segmentation.
Experiments demonstrate that our channel-wise distillation outperforms almost
all existing spatial distillation methods for semantic segmentation
considerably, and requires less computational cost during training. We
consistently achieve superior performance on three benchmarks with various
network structures. Code is available at: https://git.io/ChannelDis]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shu_C/0/1/0/all/0/1"&gt;Changyong Shu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yifan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1"&gt;Jianfei Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1"&gt;Lin Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1"&gt;Chunhua Shen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Filtering in tractography using autoencoders (FINTA). (arXiv:2010.04007v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2010.04007</id>
        <link href="http://arxiv.org/abs/2010.04007"/>
        <updated>2021-08-03T02:06:31.662Z</updated>
        <summary type="html"><![CDATA[Current brain white matter fiber tracking techniques show a number of
problems, including: generating large proportions of streamlines that do not
accurately describe the underlying anatomy; extracting streamlines that are not
supported by the underlying diffusion signal; and under-representing some fiber
populations, among others. In this paper, we describe a novel autoencoder-based
learning method to filter streamlines from diffusion MRI tractography, and
hence, to obtain more reliable tractograms. Our method, dubbed FINTA (Filtering
in Tractography using Autoencoders) uses raw, unlabeled tractograms to train
the autoencoder, and to learn a robust representation of brain streamlines.
Such an embedding is then used to filter undesired streamline samples using a
nearest neighbor algorithm. Our experiments on both synthetic and in vivo human
brain diffusion MRI tractography data obtain accuracy scores exceeding the 90\%
threshold on the test set. Results reveal that FINTA has a superior filtering
performance compared to conventional, anatomy-based methods, and the
RecoBundles state-of-the-art method. Additionally, we demonstrate that FINTA
can be applied to partial tractograms without requiring changes to the
framework. We also show that the proposed method generalizes well across
different tracking methods and datasets, and shortens significantly the
computation time for large (>1 M streamlines) tractograms. Together, this work
brings forward a new deep learning framework in tractography based on
autoencoders, which offers a flexible and powerful method for white matter
filtering and bundling that could enhance tractometry and connectivity
analyses.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Legarreta_J/0/1/0/all/0/1"&gt;Jon Haitz Legarreta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Petit_L/0/1/0/all/0/1"&gt;Laurent Petit&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rheault_F/0/1/0/all/0/1"&gt;Fran&amp;#xe7;ois Rheault&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Theaud_G/0/1/0/all/0/1"&gt;Guillaume Theaud&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lemaire_C/0/1/0/all/0/1"&gt;Carl Lemaire&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Descoteaux_M/0/1/0/all/0/1"&gt;Maxime Descoteaux&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Jodoin_P/0/1/0/all/0/1"&gt;Pierre-Marc Jodoin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dual approach for object tracking based on optical flow and swarm intelligence. (arXiv:1808.08186v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1808.08186</id>
        <link href="http://arxiv.org/abs/1808.08186"/>
        <updated>2021-08-03T02:06:31.636Z</updated>
        <summary type="html"><![CDATA[In Computer Vision,object tracking is a very old and complex problem.Though
there are several existing algorithms for object tracking, still there are
several challenges remain to be solved. For instance, variation of illumination
of light, noise, occlusion, sudden start and stop of moving object, shading
etc,make the object tracking a complex problem not only for dynamic background
but also for static background. In this paper we propose a dual approach for
object tracking based on optical flow and swarm Intelligence.The optical flow
based KLT(Kanade-Lucas-Tomasi) tracker, tracks the dominant points of the
target object from first frame to last frame of a video sequence;whereas swarm
Intelligence based PSO (Particle Swarm Optimization) tracker simultaneously
tracks the boundary information of the target object from second frame to last
frame of the same video sequence.This dual function of tracking makes the
trackers very much robust with respect to the above stated problems. The
flexibility of our approach is that it can be successfully applicable in
variable background as well as static background.We compare the performance of
the proposed dual tracking algorithm with several benchmark datasets and obtain
very competitive results in general and in most of the cases we obtained
superior results using dual tracking algorithm. We also compare the performance
of the proposed dual tracker with some existing PSO based algorithms for
tracking and achieved better results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Misra_R/0/1/0/all/0/1"&gt;Rajesh Misra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ray_K/0/1/0/all/0/1"&gt;Kumar S. Ray&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Training face verification models from generated face identity data. (arXiv:2108.00800v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00800</id>
        <link href="http://arxiv.org/abs/2108.00800"/>
        <updated>2021-08-03T02:06:31.629Z</updated>
        <summary type="html"><![CDATA[Machine learning tools are becoming increasingly powerful and widely used.
Unfortunately membership attacks, which seek to uncover information from data
sets used in machine learning, have the potential to limit data sharing. In
this paper we consider an approach to increase the privacy protection of data
sets, as applied to face recognition. Using an auxiliary face recognition
model, we build on the StyleGAN generative adversarial network and feed it with
latent codes combining two distinct sub-codes, one encoding visual identity
factors, and, the other, non-identity factors. By independently varying these
vectors during image generation, we create a synthetic data set of fictitious
face identities. We use this data set to train a face recognition model. The
model performance degrades in comparison to the state-of-the-art of face
verification. When tested with a simple membership attack our model provides
good privacy protection, however the model performance degrades in comparison
to the state-of-the-art of face verification. We find that the addition of a
small amount of private data greatly improves the performance of our model,
which highlights the limitations of using synthetic data to train machine
learning models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Conway_D/0/1/0/all/0/1"&gt;Dennis Conway&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Simon_L/0/1/0/all/0/1"&gt;Loic Simon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lechervy_A/0/1/0/all/0/1"&gt;Alexis Lechervy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jurie_F/0/1/0/all/0/1"&gt;Frederic Jurie&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient and Robust Registration on the 3D Special Euclidean Group. (arXiv:1904.05519v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1904.05519</id>
        <link href="http://arxiv.org/abs/1904.05519"/>
        <updated>2021-08-03T02:06:31.613Z</updated>
        <summary type="html"><![CDATA[We present an accurate, robust and fast method for registration of 3D scans.
Our motion estimation optimizes a robust cost function on the intrinsic
representation of rigid motions, i.e., the Special Euclidean group
$\mathbb{SE}(3)$. We exploit the geometric properties of Lie groups as well as
the robustness afforded by an iteratively reweighted least squares
optimization. We also generalize our approach to a joint multiview method that
simultaneously solves for the registration of a set of scans. We demonstrate
the efficacy of our approach by thorough experimental validation. Our approach
significantly outperforms the state-of-the-art robust 3D registration method
based on a line process in terms of both speed and accuracy. We also show that
this line process method is a special case of our principled geometric
solution. Finally, we also present scenarios where global registration based on
feature correspondences fails but multiview ICP based on our robust motion
estimation is successful.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bhattacharya_U/0/1/0/all/0/1"&gt;Uttaran Bhattacharya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Govindu_V/0/1/0/all/0/1"&gt;Venu Madhav Govindu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Representative elementary volume via averaged scalar Minkowski functionals. (arXiv:2008.03727v2 [physics.comp-ph] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2008.03727</id>
        <link href="http://arxiv.org/abs/2008.03727"/>
        <updated>2021-08-03T02:06:31.602Z</updated>
        <summary type="html"><![CDATA[Representative Elementary Volume (REV) at which the material properties do
not vary with change in volume is an important quantity for making measurements
or simulations which represent the whole. We discuss the geometrical method to
evaluation of REV based on the quantities coming in the Steiner formula from
convex geometry. For bodies in the three-space this formula gives us four
scalar functionals known as scalar Minkowski functionals. We demonstrate on
certain samples that the values of such averaged functionals almost stabilize
for cells for which the length of edges are greater than certain threshold
value R. Therefore, from this point of view, it is reasonable to consider cubes
of volume R^3 as representative elementary volumes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/physics/1/au:+Andreeva_M/0/1/0/all/0/1"&gt;M.V. Andreeva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Kalyuzhnyuk_A/0/1/0/all/0/1"&gt;A.V. Kalyuzhnyuk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Krutko_V/0/1/0/all/0/1"&gt;V.V. Krutko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Russkikh_N/0/1/0/all/0/1"&gt;N.E. Russkikh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/physics/1/au:+Taimanov_I/0/1/0/all/0/1"&gt;I.A. Taimanov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BezierSeg: Parametric Shape Representation for Fast Object Segmentation in Medical Images. (arXiv:2108.00760v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00760</id>
        <link href="http://arxiv.org/abs/2108.00760"/>
        <updated>2021-08-03T02:06:31.596Z</updated>
        <summary type="html"><![CDATA[Delineating the lesion area is an important task in image-based diagnosis.
Pixel-wise classification is a popular approach to segmenting the region of
interest. However, at fuzzy boundaries such methods usually result in glitches,
discontinuity, or disconnection, inconsistent with the fact that lesions are
solid and smooth. To overcome these undesirable artifacts, we propose the
BezierSeg model which outputs bezier curves encompassing the region of
interest. Directly modelling the contour with analytic equations ensures that
the segmentation is connected, continuous, and the boundary is smooth. In
addition, it offers sub-pixel accuracy. Without loss of accuracy, the bezier
contour can be resampled and overlaid with images of any resolution. Moreover,
a doctor can conveniently adjust the curve's control points to refine the
result. Our experiments show that the proposed method runs in real time and
achieves accuracy competitive with pixel-wise segmentation models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Haichou Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1"&gt;Yishu Deng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zeqin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1"&gt;Haohua Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jing_B/0/1/0/all/0/1"&gt;Bingzhong Jing&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chaofeng Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-scale super-resolution generation of low-resolution scanned pathological images. (arXiv:2105.07200v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.07200</id>
        <link href="http://arxiv.org/abs/2105.07200"/>
        <updated>2021-08-03T02:06:31.477Z</updated>
        <summary type="html"><![CDATA[Background. Digital pathology has aroused widespread interest in modern
pathology. The key of digitalization is to scan the whole slide image (WSI) at
high magnification. The lager the magnification is, the richer details WSI will
provide, but the scanning time is longer and the file size of obtained is
larger. Methods. We design a strategy to scan slides with low resolution (5X)
and a super-resolution method is proposed to restore the image details when in
diagnosis. The method is based on a multi-scale generative adversarial network,
which sequentially generates three high-resolution images such as 10X, 20X and
40X. Results. The peak-signal-to-noise-ratio of 10X to 40X generated images are
24.16, 22.27 and 20.44, and the structural-similarity-index are 0.845, 0.680
and 0.512, which are better than other super-resolution networks. Visual
scoring average and standard deviation from three pathologists is 3.63
plus-minus 0.52, 3.70 plus-minus 0.57 and 3.74 plus-minus 0.56 and the p value
of analysis of variance is 0.37, indicating that generated images include
sufficient information for diagnosis. The average value of Kappa test is 0.99,
meaning the diagnosis of generated images is highly consistent with that of the
real images. Conclusion. This proposed method can generate high-quality 10X,
20X, 40X images from 5X images at the same time, in which the time and storage
costs of digitalization can be effectively reduced up to 1/64 of the previous
costs. The proposed method provides a better alternative for low-cost storage,
faster image share of digital pathology. Keywords. Digital pathology;
Super-resolution; Low resolution scanning; Low cost]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Sun_K/0/1/0/all/0/1"&gt;Kai Sun&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/eess/1/au:+Gao_Y/0/1/0/all/0/1"&gt;Yanhua Gao&lt;/a&gt; (2), &lt;a href="http://arxiv.org/find/eess/1/au:+Xie_T/0/1/0/all/0/1"&gt;Ting Xie&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xun Wang&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/eess/1/au:+Yang_Q/0/1/0/all/0/1"&gt;Qingqing Yang&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/eess/1/au:+Chen_L/0/1/0/all/0/1"&gt;Le Chen&lt;/a&gt; (1), &lt;a href="http://arxiv.org/find/eess/1/au:+Wang_K/0/1/0/all/0/1"&gt;Kuansong Wang&lt;/a&gt; (3), &lt;a href="http://arxiv.org/find/eess/1/au:+Yu_G/0/1/0/all/0/1"&gt;Gang Yu&lt;/a&gt; (1) ((1) Department of Biomedical Engineering, School of Basic Medical Sciences, Central South University, 172 Tongzipo Road, Changsha, 410013, China. (2) Department of Ultrasound, Shaanxi Provincial People&amp;#x27;s Hospital,256 Youyixi Road, Xi&amp;#x27;an, 710068, China. (3) Department of Pathology, School of Basic Medical Sciences, Central South University, 172 Tongzipo Road, Changsha, 410013, China.)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Height Estimation of Children under Five Years using Depth Images. (arXiv:2105.01688v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.01688</id>
        <link href="http://arxiv.org/abs/2105.01688"/>
        <updated>2021-08-03T02:06:31.441Z</updated>
        <summary type="html"><![CDATA[Malnutrition is a global health crisis and is the leading cause of death
among children under five. Detecting malnutrition requires anthropometric
measurements of weight, height, and middle-upper arm circumference. However,
measuring them accurately is a challenge, especially in the global south, due
to limited resources. In this work, we propose a CNN-based approach to estimate
the height of standing children under five years from depth images collected
using a smart-phone. According to the SMART Methodology Manual [5], the
acceptable accuracy for height is less than 1.4 cm. On training our deep
learning model on 87131 depth images, our model achieved an average mean
absolute error of 1.64% on 57064 test images. For 70.3% test images, we
estimated height accurately within the acceptable 1.4 cm range. Thus, our
proposed solution can accurately detect stunting (low height-for-age) in
standing children below five years of age.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Trivedi_A/0/1/0/all/0/1"&gt;Anusua Trivedi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jain_M/0/1/0/all/0/1"&gt;Mohit Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_N/0/1/0/all/0/1"&gt;Nikhil Kumar Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hinsche_M/0/1/0/all/0/1"&gt;Markus Hinsche&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_P/0/1/0/all/0/1"&gt;Prashant Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Matiaschek_M/0/1/0/all/0/1"&gt;Markus Matiaschek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Behrens_T/0/1/0/all/0/1"&gt;Tristan Behrens&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Militeri_M/0/1/0/all/0/1"&gt;Mirco Militeri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Birge_C/0/1/0/all/0/1"&gt;Cameron Birge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kaushik_S/0/1/0/all/0/1"&gt;Shivangi Kaushik&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mohapatra_A/0/1/0/all/0/1"&gt;Archisman Mohapatra&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chatterjee_R/0/1/0/all/0/1"&gt;Rita Chatterjee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dodhia_R/0/1/0/all/0/1"&gt;Rahul Dodhia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ferres_J/0/1/0/all/0/1"&gt;Juan Lavista Ferres&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Mixing-AdaSIN: Constructing a De-biased Dataset using Adaptive Structural Instance Normalization and Texture Mixing. (arXiv:2103.14255v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.14255</id>
        <link href="http://arxiv.org/abs/2103.14255"/>
        <updated>2021-08-03T02:06:31.434Z</updated>
        <summary type="html"><![CDATA[Following the pandemic outbreak, several works have proposed to diagnose
COVID-19 with deep learning in computed tomography (CT); reporting performance
on-par with experts. However, models trained/tested on the same in-distribution
data may rely on the inherent data biases for successful prediction, failing to
generalize on out-of-distribution samples or CT with different scanning
protocols. Early attempts have partly addressed bias-mitigation and
generalization through augmentation or re-sampling, but are still limited by
collection costs and the difficulty of quantifying bias in medical images. In
this work, we propose Mixing-AdaSIN; a bias mitigation method that uses a
generative model to generate de-biased images by mixing texture information
between different labeled CT scans with semantically similar features. Here, we
use Adaptive Structural Instance Normalization (AdaSIN) to enhance de-biasing
generation quality and guarantee structural consistency. Following, a
classifier trained with the generated images learns to correctly predict the
label without bias and generalizes better. To demonstrate the efficacy of our
method, we construct a biased COVID-19 vs. bacterial pneumonia dataset based on
CT protocols and compare with existing state-of-the-art de-biasing methods. Our
experiments show that classifiers trained with de-biased generated images
report improved in-distribution performance and generalization on an external
COVID-19 dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Kang_M/0/1/0/all/0/1"&gt;Myeongkyun Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Chikontwe_P/0/1/0/all/0/1"&gt;Philip Chikontwe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Luna_M/0/1/0/all/0/1"&gt;Miguel Luna&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Hong_K/0/1/0/all/0/1"&gt;Kyung Soo Hong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ahn_J/0/1/0/all/0/1"&gt;June Hong Ahn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Park_S/0/1/0/all/0/1"&gt;Sang Hyun Park&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MSMatch: Semi-Supervised Multispectral Scene Classification with Few Labels. (arXiv:2103.10368v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.10368</id>
        <link href="http://arxiv.org/abs/2103.10368"/>
        <updated>2021-08-03T02:06:31.416Z</updated>
        <summary type="html"><![CDATA[Supervised learning techniques are at the center of many tasks in remote
sensing. Unfortunately, these methods, especially recent deep learning methods,
often require large amounts of labeled data for training. Even though
satellites acquire large amounts of data, labeling the data is often tedious,
expensive and requires expert knowledge. Hence, improved methods that require
fewer labeled samples are needed. We present MSMatch, the first semi-supervised
learning approach competitive with supervised methods on scene classification
on the EuroSAT and UC Merced Land Use benchmark datasets. We test both RGB and
multispectral images of EuroSAT and perform various ablation studies to
identify the critical parts of the model. The trained neural network achieves
state-of-the-art results on EuroSAT with an accuracy that is up to 19.76%
better than previous methods depending on the number of labeled training
examples. With just five labeled examples per class, we reach 94.53% and 95.86%
accuracy on the EuroSAT RGB and multispectral datasets, respectively. On the UC
Merced Land Use dataset, we outperform previous works by up to 5.59% and reach
90.71% with five labeled examples. Our results show that MSMatch is capable of
greatly reducing the requirements for labeled data. It translates well to
multispectral data and should enable various applications that are currently
infeasible due to a lack of labeled data. We provide the source code of MSMatch
online to enable easy reproduction and quick adoption.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gomez_P/0/1/0/all/0/1"&gt;Pablo G&amp;#xf3;mez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meoni_G/0/1/0/all/0/1"&gt;Gabriele Meoni&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Bootstrapped Self-Supervised Training with Monocular Video for Semantic Segmentation and Depth Estimation. (arXiv:2103.11031v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.11031</id>
        <link href="http://arxiv.org/abs/2103.11031"/>
        <updated>2021-08-03T02:06:31.403Z</updated>
        <summary type="html"><![CDATA[For a robot deployed in the world, it is desirable to have the ability of
autonomous learning to improve its initial pre-set knowledge. We formalize this
as a bootstrapped self-supervised learning problem where a system is initially
bootstrapped with supervised training on a labeled dataset and we look for a
self-supervised training method that can subsequently improve the system over
the supervised training baseline using only unlabeled data. In this work, we
leverage temporal consistency between frames in monocular video to perform this
bootstrapped self-supervised training. We show that a well-trained
state-of-the-art semantic segmentation network can be further improved through
our method. In addition, we show that the bootstrapped self-supervised training
framework can help a network learn depth estimation better than pure supervised
training or self-supervised training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yihao Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Leonard_J/0/1/0/all/0/1"&gt;John J. Leonard&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised Continual Learning Via Pseudo Labels. (arXiv:2104.07164v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.07164</id>
        <link href="http://arxiv.org/abs/2104.07164"/>
        <updated>2021-08-03T02:06:31.397Z</updated>
        <summary type="html"><![CDATA[Continual learning aims to learn new tasks incrementally using less
computation and memory resources instead of retraining the model from scratch
whenever new task arrives. However, existing approaches are designed in
supervised fashion assuming all data from new tasks have been manually
annotated, which are not practical for many real-life applications. In this
work, we propose to use pseudo label instead of the ground truth to make
continual learning feasible in unsupervised mode. The pseudo labels of new data
are obtained by applying global clustering algorithm and we propose to use the
model updated from last incremental step as the feature extractor. Due to the
scarcity of existing work, we introduce a new benchmark experimental protocol
for unsupervised continual learning of image classification task under
class-incremental setting where no class label is provided for each incremental
learning step. Our method is evaluated on the CIFAR-100 and ImageNet (ILSVRC)
datasets by incorporating the pseudo label with various existing supervised
approaches and show promising results in unsupervised scenario.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1"&gt;Jiangpeng He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1"&gt;Fengqing Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Weakly-supervised Object Localization via Causal Intervention. (arXiv:2104.10351v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.10351</id>
        <link href="http://arxiv.org/abs/2104.10351"/>
        <updated>2021-08-03T02:06:31.375Z</updated>
        <summary type="html"><![CDATA[The recent emerged weakly supervised object localization (WSOL) methods can
learn to localize an object in the image only using image-level labels.
Previous works endeavor to perceive the interval objects from the small and
sparse discriminative attention map, yet ignoring the co-occurrence confounder
(e.g., bird and sky), which makes the model inspection (e.g., CAM) hard to
distinguish between the object and context. In this paper, we make an early
attempt to tackle this challenge via causal intervention (CI). Our proposed
method, dubbed CI-CAM, explores the causalities among images, contexts, and
categories to eliminate the biased co-occurrence in the class activation maps
thus improving the accuracy of object localization. Extensive experiments on
several benchmarks demonstrate the effectiveness of CI-CAM in learning the
clear object boundaries from confounding contexts. Particularly, in
CUB-200-2011 which severely suffers from the co-occurrence confounder, CI-CAM
significantly outperforms the traditional CAM-based baseline (58.39% vs 52.4%
in top-1 localization accuracy). While in more general scenarios such as
ImageNet, CI-CAM can also perform on par with the state of the arts.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shao_F/0/1/0/all/0/1"&gt;Feifei Shao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1"&gt;Yawei Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Li Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_L/0/1/0/all/0/1"&gt;Lu Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1"&gt;Siliang Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yi Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1"&gt;Jun Xiao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Generic Event Boundary Detection: A Benchmark for Event Segmentation. (arXiv:2101.10511v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.10511</id>
        <link href="http://arxiv.org/abs/2101.10511"/>
        <updated>2021-08-03T02:06:31.367Z</updated>
        <summary type="html"><![CDATA[This paper presents a novel task together with a new benchmark for detecting
generic, taxonomy-free event boundaries that segment a whole video into chunks.
Conventional work in temporal video segmentation and action detection focuses
on localizing pre-defined action categories and thus does not scale to generic
videos. Cognitive Science has known since last century that humans consistently
segment videos into meaningful temporal chunks. This segmentation happens
naturally, without pre-defined event categories and without being explicitly
asked to do so. Here, we repeat these cognitive experiments on mainstream CV
datasets; with our novel annotation guideline which addresses the complexities
of taxonomy-free event boundary annotation, we introduce the task of Generic
Event Boundary Detection (GEBD) and the new benchmark Kinetics-GEBD. Our
Kinetics-GEBD has the largest number of boundaries (e.g. 32 of ActivityNet, 8
of EPIC-Kitchens-100) which are in-the-wild, taxonomy-free, cover generic event
change, and respect human perception diversity. We view GEBD as an important
stepping stone towards understanding the video as a whole, and believe it has
been previously neglected due to a lack of proper task definition and
annotations. Through experiment and human study we demonstrate the value of the
annotations. Further, we benchmark supervised and un-supervised GEBD approaches
on the TAPOS dataset and our Kinetics-GEBD, together with method design
explorations that suggest future directions. We release our annotations and
baseline codes at CVPR'21 LOVEU Challenge:
https://sites.google.com/view/loveucvpr21.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shou_M/0/1/0/all/0/1"&gt;Mike Zheng Shou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lei_S/0/1/0/all/0/1"&gt;Stan Weixian Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Weiyao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghadiyaram_D/0/1/0/all/0/1"&gt;Deepti Ghadiyaram&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feiszli_M/0/1/0/all/0/1"&gt;Matt Feiszli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Fast and High-Quality Blind Multi-Spectral Image Pansharpening. (arXiv:2103.09943v4 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.09943</id>
        <link href="http://arxiv.org/abs/2103.09943"/>
        <updated>2021-08-03T02:06:31.361Z</updated>
        <summary type="html"><![CDATA[Blind pansharpening addresses the problem of generating a high
spatial-resolution multi-spectral (HRMS) image given a low spatial-resolution
multi-spectral (LRMS) image with the guidance of its associated spatially
misaligned high spatial-resolution panchromatic (PAN) image without parametric
side information. In this paper, we propose a fast approach to blind
pansharpening and achieve state-of-the-art image reconstruction quality.
Typical blind pansharpening algorithms are often computationally intensive
since the blur kernel and the target HRMS image are often computed using
iterative solvers and in an alternating fashion. To achieve fast blind
pansharpening, we decouple the solution of the blur kernel and of the HRMS
image. First, we estimate the blur kernel by computing the kernel coefficients
with minimum total generalized variation that blur a downsampled version of the
PAN image to approximate a linear combination of the LRMS image channels. Then,
we estimate each channel of the HRMS image using local Laplacian prior to
regularize the relationship between each HRMS channel and the PAN image.
Solving the HRMS image is accelerated by both parallelizing across the channels
and by fast numerical algorithms for each channel. Due to the fast scheme and
the powerful priors we used on the blur kernel coefficients (total generalized
variation) and on the cross-channel relationship (local Laplacian prior),
numerical experiments demonstrate that our algorithm outperforms
state-of-the-art model-based counterparts in terms of both computational time
and reconstruction quality of the HRMS images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1"&gt;Lantao Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1"&gt;Dehong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mansour_H/0/1/0/all/0/1"&gt;Hassan Mansour&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Boufounos_P/0/1/0/all/0/1"&gt;Petros T. Boufounos&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Synthesis of Compositional Animations from Textual Descriptions. (arXiv:2103.14675v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.14675</id>
        <link href="http://arxiv.org/abs/2103.14675"/>
        <updated>2021-08-03T02:06:31.350Z</updated>
        <summary type="html"><![CDATA["How can we animate 3D-characters from a movie script or move robots by
simply telling them what we would like them to do?" "How unstructured and
complex can we make a sentence and still generate plausible movements from it?"
These are questions that need to be answered in the long-run, as the field is
still in its infancy. Inspired by these problems, we present a new technique
for generating compositional actions, which handles complex input sentences.
Our output is a 3D pose sequence depicting the actions in the input sentence.
We propose a hierarchical two-stream sequential model to explore a finer
joint-level mapping between natural language sentences and 3D pose sequences
corresponding to the given motion. We learn two manifold representations of the
motion -- one each for the upper body and the lower body movements. Our model
can generate plausible pose sequences for short sentences describing single
actions as well as long compositional sentences describing multiple sequential
and superimposed actions. We evaluate our proposed model on the publicly
available KIT Motion-Language Dataset containing 3D pose data with
human-annotated sentences. Experimental results show that our model advances
the state-of-the-art on text-based motion synthesis in objective evaluations by
a margin of 50%. Qualitative evaluations based on a user study indicate that
our synthesized motions are perceived to be the closest to the ground-truth
motion captures for both short and compositional sentences.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ghosh_A/0/1/0/all/0/1"&gt;Anindita Ghosh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheema_N/0/1/0/all/0/1"&gt;Noshaba Cheema&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Oguz_C/0/1/0/all/0/1"&gt;Cennet Oguz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1"&gt;Christian Theobalt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Slusallek_P/0/1/0/all/0/1"&gt;Philipp Slusallek&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Learning with robustness to missing data: A novel approach to the detection of COVID-19. (arXiv:2103.13833v2 [eess.IV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.13833</id>
        <link href="http://arxiv.org/abs/2103.13833"/>
        <updated>2021-08-03T02:06:31.341Z</updated>
        <summary type="html"><![CDATA[In the context of the current global pandemic and the limitations of the
RT-PCR test, we propose a novel deep learning architecture, DFCN (Denoising
Fully Connected Network). Since medical facilities around the world differ
enormously in what laboratory tests or chest imaging may be available, DFCN is
designed to be robust to missing input data. An ablation study extensively
evaluates the performance benefits of the DFCN as well as its robustness to
missing inputs. Data from 1088 patients with confirmed RT-PCR results are
obtained from two independent medical facilities. The data includes results
from 27 laboratory tests and a chest x-ray scored by a deep learning model.
Training and test datasets are taken from different medical facilities. Data is
made publicly available. The performance of DFCN in predicting the RT-PCR
result is compared with 3 related architectures as well as a Random Forest
baseline. All models are trained with varying levels of masked input data to
encourage robustness to missing inputs. Missing data is simulated at test time
by masking inputs randomly. DFCN outperforms all other models with statistical
significance using random subsets of input data with 2-27 available inputs.
When all 28 inputs are available DFCN obtains an AUC of 0.924, higher than any
other model. Furthermore, with clinically meaningful subsets of parameters
consisting of just 6 and 7 inputs respectively, DFCN achieves higher AUCs than
any other model, with values of 0.909 and 0.919.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Calli_E/0/1/0/all/0/1"&gt;Erdi &amp;#xc7;all&amp;#x131;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Murphy_K/0/1/0/all/0/1"&gt;Keelin Murphy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Kurstjens_S/0/1/0/all/0/1"&gt;Steef Kurstjens&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Samson_T/0/1/0/all/0/1"&gt;Tijs Samson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Herpers_R/0/1/0/all/0/1"&gt;Robert Herpers&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Smits_H/0/1/0/all/0/1"&gt;Henk Smits&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Rutten_M/0/1/0/all/0/1"&gt;Matthieu Rutten&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ginneken_B/0/1/0/all/0/1"&gt;Bram van Ginneken&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Simple Baseline for Semi-supervised Semantic Segmentation with Strong Data Augmentation. (arXiv:2104.07256v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.07256</id>
        <link href="http://arxiv.org/abs/2104.07256"/>
        <updated>2021-08-03T02:06:31.305Z</updated>
        <summary type="html"><![CDATA[Recently, significant progress has been made on semantic segmentation.
However, the success of supervised semantic segmentation typically relies on a
large amount of labelled data, which is time-consuming and costly to obtain.
Inspired by the success of semi-supervised learning methods in image
classification, here we propose a simple yet effective semi-supervised learning
framework for semantic segmentation. We demonstrate that the devil is in the
details: a set of simple design and training techniques can collectively
improve the performance of semi-supervised semantic segmentation significantly.
Previous works [3, 27] fail to employ strong augmentation in pseudo label
learning efficiently, as the large distribution change caused by strong
augmentation harms the batch normalisation statistics. We design a new batch
normalisation, namely distribution-specific batch normalisation (DSBN) to
address this problem and demonstrate the importance of strong augmentation for
semantic segmentation. Moreover, we design a self correction loss which is
effective in noise resistance. We conduct a series of ablation studies to show
the effectiveness of each component. Our method achieves state-of-the-art
results in the semi-supervised settings on the Cityscapes and Pascal VOC
datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1"&gt;Jianlong Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yifan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1"&gt;Chunhua Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhibin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1"&gt;Hao Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unsupervised and Unregistered Hyperspectral Image Super-Resolution with Mutual Dirichlet-Net. (arXiv:1904.12175v5 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/1904.12175</id>
        <link href="http://arxiv.org/abs/1904.12175"/>
        <updated>2021-08-03T02:06:31.292Z</updated>
        <summary type="html"><![CDATA[Hyperspectral images (HSI) provide rich spectral information that contributed
to the successful performance improvement of numerous computer vision tasks.
However, it can only be achieved at the expense of images' spatial resolution.
Hyperspectral image super-resolution (HSI-SR) addresses this problem by fusing
low resolution (LR) HSI with multispectral image (MSI) carrying much higher
spatial resolution (HR). All existing HSI-SR approaches require the LR HSI and
HR MSI to be well registered and the reconstruction accuracy of the HR HSI
relies heavily on the registration accuracy of different modalities. This paper
exploits the uncharted problem domain of HSI-SR without the requirement of
multi-modality registration. Given the unregistered LR HSI and HR MSI with
overlapped regions, we design a unique unsupervised learning structure linking
the two unregistered modalities by projecting them into the same statistical
space through the same encoder. The mutual information (MI) is further adopted
to capture the non-linear statistical dependencies between the representations
from two modalities (carrying spatial information) and their raw inputs. By
maximizing the MI, spatial correlations between different modalities can be
well characterized to further reduce the spectral distortion. A collaborative
$l_{2,1}$ norm is employed as the reconstruction error instead of the more
common $l_2$ norm, so that individual pixels can be recovered as accurately as
possible. With this design, the network allows to extract correlated spectral
and spatial information from unregistered images that better preserves the
spectral information. The proposed method is referred to as unregistered and
unsupervised mutual Dirichlet Net ($u^2$-MDN). Extensive experimental results
using benchmark HSI datasets demonstrate the superior performance of $u^2$-MDN
as compared to the state-of-the-art.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qu_Y/0/1/0/all/0/1"&gt;Ying Qu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_H/0/1/0/all/0/1"&gt;Hairong Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kwan_C/0/1/0/all/0/1"&gt;Chiman Kwan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yokoya_N/0/1/0/all/0/1"&gt;Naoto Yokoya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chanussot_J/0/1/0/all/0/1"&gt;Jocelyn Chanussot&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Angle Based Feature Learning in GNN for 3D Object Detection using Point Cloud. (arXiv:2108.00780v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00780</id>
        <link href="http://arxiv.org/abs/2108.00780"/>
        <updated>2021-08-03T02:06:31.285Z</updated>
        <summary type="html"><![CDATA[In this paper, we present new feature encoding methods for Detection of 3D
objects in point clouds. We used a graph neural network (GNN) for Detection of
3D objects namely cars, pedestrians, and cyclists. Feature encoding is one of
the important steps in Detection of 3D objects. The dataset used is point cloud
data which is irregular and unstructured and it needs to be encoded in such a
way that ensures better feature encapsulation. Earlier works have used relative
distance as one of the methods to encode the features. These methods are not
resistant to rotation variance problems in Graph Neural Networks. We have
included angular-based measures while performing feature encoding in graph
neural networks. Along with that, we have performed a comparison between other
methods like Absolute, Relative, Euclidean distances, and a combination of the
Angle and Relative methods. The model is trained and evaluated on the subset of
the KITTI object detection benchmark dataset under resource constraints. Our
results demonstrate that a combination of angle measures and relative distance
has performed better than other methods. In comparison to the baseline
method(relative), it achieved better performance. We also performed time
analysis of various feature encoding methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ansari_M/0/1/0/all/0/1"&gt;Md Afzal Ansari&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Meraz_M/0/1/0/all/0/1"&gt;Md Meraz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chakraborty_P/0/1/0/all/0/1"&gt;Pavan Chakraborty&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Javed_M/0/1/0/all/0/1"&gt;Mohammed Javed&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Flip Learning: Erase to Segment. (arXiv:2108.00752v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00752</id>
        <link href="http://arxiv.org/abs/2108.00752"/>
        <updated>2021-08-03T02:06:31.274Z</updated>
        <summary type="html"><![CDATA[Nodule segmentation from breast ultrasound images is challenging yet
essential for the diagnosis. Weakly-supervised segmentation (WSS) can help
reduce time-consuming and cumbersome manual annotation. Unlike existing
weakly-supervised approaches, in this study, we propose a novel and general WSS
framework called Flip Learning, which only needs the box annotation.
Specifically, the target in the label box will be erased gradually to flip the
classification tag, and the erased region will be considered as the
segmentation result finally. Our contribution is three-fold. First, our
proposed approach erases on superpixel level using a Multi-agent Reinforcement
Learning framework to exploit the prior boundary knowledge and accelerate the
learning process. Second, we design two rewards: classification score and
intensity distribution reward, to avoid under- and over-segmentation,
respectively. Third, we adopt a coarse-to-fine learning strategy to reduce the
residual errors and improve the segmentation performance. Extensively validated
on a large dataset, our proposed approach achieves competitive performance and
shows great potential to narrow the gap between fully-supervised and
weakly-supervised learning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yuhao Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1"&gt;Yuxin Zou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Chaoyu Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jian Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dou_H/0/1/0/all/0/1"&gt;Haoran Dou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ravikumar_N/0/1/0/all/0/1"&gt;Nishant Ravikumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frangi_A/0/1/0/all/0/1"&gt;Alejandro F Frangi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Jianqiao Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ni_D/0/1/0/all/0/1"&gt;Dong Ni&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[KiloNeRF: Speeding up Neural Radiance Fields with Thousands of Tiny MLPs. (arXiv:2103.13744v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2103.13744</id>
        <link href="http://arxiv.org/abs/2103.13744"/>
        <updated>2021-08-03T02:06:31.265Z</updated>
        <summary type="html"><![CDATA[NeRF synthesizes novel views of a scene with unprecedented quality by fitting
a neural radiance field to RGB images. However, NeRF requires querying a deep
Multi-Layer Perceptron (MLP) millions of times, leading to slow rendering
times, even on modern GPUs. In this paper, we demonstrate that real-time
rendering is possible by utilizing thousands of tiny MLPs instead of one single
large MLP. In our setting, each individual MLP only needs to represent parts of
the scene, thus smaller and faster-to-evaluate MLPs can be used. By combining
this divide-and-conquer strategy with further optimizations, rendering is
accelerated by three orders of magnitude compared to the original NeRF model
without incurring high storage costs. Further, using teacher-student
distillation for training, we show that this speed-up can be achieved without
sacrificing visual quality.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Reiser_C/0/1/0/all/0/1"&gt;Christian Reiser&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1"&gt;Songyou Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liao_Y/0/1/0/all/0/1"&gt;Yiyi Liao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Geiger_A/0/1/0/all/0/1"&gt;Andreas Geiger&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Robust Object Detection: Bayesian RetinaNet for Homoscedastic Aleatoric Uncertainty Modeling. (arXiv:2108.00784v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00784</id>
        <link href="http://arxiv.org/abs/2108.00784"/>
        <updated>2021-08-03T02:06:31.244Z</updated>
        <summary type="html"><![CDATA[According to recent studies, commonly used computer vision datasets contain
about 4% of label errors. For example, the COCO dataset is known for its high
level of noise in data labels, which limits its use for training robust neural
deep architectures in a real-world scenario. To model such a noise, in this
paper we have proposed the homoscedastic aleatoric uncertainty estimation, and
present a series of novel loss functions to address the problem of image object
detection at scale. Specifically, the proposed functions are based on Bayesian
inference and we have incorporated them into the common community-adopted
object detection deep learning architecture RetinaNet. We have also shown that
modeling of homoscedastic aleatoric uncertainty using our novel functions
allows to increase the model interpretability and to improve the object
detection performance being evaluated on the COCO dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khanzhina_N/0/1/0/all/0/1"&gt;Natalia Khanzhina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lapenok_A/0/1/0/all/0/1"&gt;Alexey Lapenok&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Filchenkov_A/0/1/0/all/0/1"&gt;Andrey Filchenkov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep Feature Tracker: A Novel Application for Deep Convolutional Neural Networks. (arXiv:2108.00105v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00105</id>
        <link href="http://arxiv.org/abs/2108.00105"/>
        <updated>2021-08-03T02:06:31.238Z</updated>
        <summary type="html"><![CDATA[Feature tracking is the building block of many applications such as visual
odometry, augmented reality, and target tracking. Unfortunately, the
state-of-the-art vision-based tracking algorithms fail in surgical images due
to the challenges imposed by the nature of such environments. In this paper, we
proposed a novel and unified deep learning-based approach that can learn how to
track features reliably as well as learn how to detect such reliable features
for tracking purposes. The proposed network dubbed as Deep-PT, consists of a
tracker network which is a convolutional neural network simulating
cross-correlation in terms of deep learning and two fully connected networks
that operate on the output of intermediate layers of the tracker to detect
features and predict trackability of the detected points. The ability to detect
features based on the capabilities of the tracker distinguishes the proposed
method from previous algorithms used in this area and improves the robustness
of the algorithms against dynamics of the scene. The network is trained using
multiple datasets due to the lack of specialized dataset for feature tracking
datasets and extensive comparisons are conducted to compare the accuracy of
Deep-PT against recent pixel tracking algorithms. As the experiments suggest,
the proposed deep architecture deliberately learns what to track and how to
track and outperforms the state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Parchami_M/0/1/0/all/0/1"&gt;Mostafa Parchami&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sayed_S/0/1/0/all/0/1"&gt;Saif Iftekar Sayed&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LDDMM-Face: Large Deformation Diffeomorphic Metric Learning for Flexible and Consistent Face Alignment. (arXiv:2108.00690v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00690</id>
        <link href="http://arxiv.org/abs/2108.00690"/>
        <updated>2021-08-03T02:06:31.233Z</updated>
        <summary type="html"><![CDATA[We innovatively propose a flexible and consistent face alignment framework,
LDDMM-Face, the key contribution of which is a deformation layer that naturally
embeds facial geometry in a diffeomorphic way. Instead of predicting facial
landmarks via heatmap or coordinate regression, we formulate this task in a
diffeomorphic registration manner and predict momenta that uniquely
parameterize the deformation between initial boundary and true boundary, and
then perform large deformation diffeomorphic metric mapping (LDDMM)
simultaneously for curve and landmark to localize the facial landmarks. Due to
the embedding of LDDMM into a deep network, LDDMM-Face can consistently
annotate facial landmarks without ambiguity and flexibly handle various
annotation schemes, and can even predict dense annotations from sparse ones.
Our method can be easily integrated into various face alignment networks. We
extensively evaluate LDDMM-Face on four benchmark datasets: 300W, WFLW, HELEN
and COFW-68. LDDMM-Face is comparable or superior to state-of-the-art methods
for traditional within-dataset and same-annotation settings, but truly
distinguishes itself with outstanding performance when dealing with
weakly-supervised learning (partial-to-full), challenging cases (e.g., occluded
faces), and different training and prediction datasets. In addition, LDDMM-Face
shows promising results on the most challenging task of predicting across
datasets with different annotation schemes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1"&gt;Huilin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lyu_J/0/1/0/all/0/1"&gt;Junyan Lyu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_P/0/1/0/all/0/1"&gt;Pujin Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1"&gt;Xiaoying Tang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PoseFusion2: Simultaneous Background Reconstruction and Human Shape Recovery in Real-time. (arXiv:2108.00695v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00695</id>
        <link href="http://arxiv.org/abs/2108.00695"/>
        <updated>2021-08-03T02:06:31.221Z</updated>
        <summary type="html"><![CDATA[Dynamic environments that include unstructured moving objects pose a hard
problem for Simultaneous Localization and Mapping (SLAM) performance. The
motion of rigid objects can be typically tracked by exploiting their texture
and geometric features. However, humans moving in the scene are often one of
the most important, interactive targets - they are very hard to track and
reconstruct robustly due to non-rigid shapes. In this work, we present a fast,
learning-based human object detector to isolate the dynamic human objects and
realise a real-time dense background reconstruction framework. We go further by
estimating and reconstructing the human pose and shape. The final output
environment maps not only provide the dense static backgrounds but also contain
the dynamic human meshes and their trajectories. Our Dynamic SLAM system runs
at around 26 frames per second (fps) on GPUs, while additionally turning on
accurate human pose estimation can be executed at up to 10 fps.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1"&gt;Huayan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1"&gt;Tianwei Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lam_T/0/1/0/all/0/1"&gt;Tin Lun Lam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vijayakumar_S/0/1/0/all/0/1"&gt;Sethu Vijayakumar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multimodal Feature Fusion for Video Advertisements Tagging Via Stacking Ensemble. (arXiv:2108.00679v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00679</id>
        <link href="http://arxiv.org/abs/2108.00679"/>
        <updated>2021-08-03T02:06:31.213Z</updated>
        <summary type="html"><![CDATA[Automated tagging of video advertisements has been a critical yet challenging
problem, and it has drawn increasing interests in last years as its
applications seem to be evident in many fields. Despite sustainable efforts
have been made, the tagging task is still suffered from several challenges,
such as, efficiently feature fusion approach is desirable, but under-explored
in previous studies. In this paper, we present our approach for Multimodal
Video Ads Tagging in the 2021 Tencent Advertising Algorithm Competition.
Specifically, we propose a novel multi-modal feature fusion framework, with the
goal to combine complementary information from multiple modalities. This
framework introduces stacking-based ensembling approach to reduce the influence
of varying levels of noise and conflicts between different modalities. Thus,
our framework can boost the performance of the tagging task, compared to
previous methods. To empirically investigate the effectiveness and robustness
of the proposed framework, we conduct extensive experiments on the challenge
datasets. The obtained results suggest that our framework can significantly
outperform related approaches and our method ranks as the 1st place on the
final leaderboard, with a Global Average Precision (GAP) of 82.63%. To better
promote the research in this field, we will release our code in the final
version.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1"&gt;Qingsong Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_H/0/1/0/all/0/1"&gt;Hai Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1"&gt;Zhimin Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1"&gt;Kele Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning TFIDF Enhanced Joint Embedding for Recipe-Image Cross-Modal Retrieval Service. (arXiv:2108.00724v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00724</id>
        <link href="http://arxiv.org/abs/2108.00724"/>
        <updated>2021-08-03T02:06:31.207Z</updated>
        <summary type="html"><![CDATA[It is widely acknowledged that learning joint embeddings of recipes with
images is challenging due to the diverse composition and deformation of
ingredients in cooking procedures. We present a Multi-modal Semantics enhanced
Joint Embedding approach (MSJE) for learning a common feature space between the
two modalities (text and image), with the ultimate goal of providing
high-performance cross-modal retrieval services. Our MSJE approach has three
unique features. First, we extract the TFIDF feature from the title,
ingredients and cooking instructions of recipes. By determining the
significance of word sequences through combining LSTM learned features with
their TFIDF features, we encode a recipe into a TFIDF weighted vector for
capturing significant key terms and how such key terms are used in the
corresponding cooking instructions. Second, we combine the recipe TFIDF feature
with the recipe sequence feature extracted through two-stage LSTM networks,
which is effective in capturing the unique relationship between a recipe and
its associated image(s). Third, we further incorporate TFIDF enhanced category
semantics to improve the mapping of image modality and to regulate the
similarity loss function during the iterative learning of cross-modal joint
embedding. Experiments on the benchmark dataset Recipe1M show the proposed
approach outperforms the state-of-the-art approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1"&gt;Zhongwei Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Ling Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yanzhao Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_L/0/1/0/all/0/1"&gt;Luo Zhong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Recursively Refined R-CNN: Instance Segmentation with Self-RoI Rebalancing. (arXiv:2104.01329v2 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.01329</id>
        <link href="http://arxiv.org/abs/2104.01329"/>
        <updated>2021-08-03T02:06:31.190Z</updated>
        <summary type="html"><![CDATA[Within the field of instance segmentation, most of the state-of-the-art deep
learning networks rely nowadays on cascade architectures, where multiple object
detectors are trained sequentially, re-sampling the ground truth at each step.
This offers a solution to the problem of exponentially vanishing positive
samples. However, it also translates into an increase in network complexity in
terms of the number of parameters. To address this issue, we propose
Recursively Refined R-CNN (R^3-CNN) which avoids duplicates by introducing a
loop mechanism instead. At the same time, it achieves a quality boost using a
recursive re-sampling technique, where a specific IoU quality is utilized in
each recursion to eventually equally cover the positive spectrum. Our
experiments highlight the specific encoding of the loop mechanism in the
weights, requiring its usage at inference time. The R^3-CNN architecture is
able to surpass the recently proposed HTC model, while reducing the number of
parameters significantly. Experiments on COCO minival 2017 dataset show
performance boost independently from the utilized baseline model. The code is
available online at https://github.com/IMPLabUniPr/mmdetection/tree/r3_cnn.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Rossi_L/0/1/0/all/0/1"&gt;Leonardo Rossi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karimi_A/0/1/0/all/0/1"&gt;Akbar Karimi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Prati_A/0/1/0/all/0/1"&gt;Andrea Prati&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Deep Feature Calibration for Cross-Modal Joint Embedding Learning. (arXiv:2108.00705v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00705</id>
        <link href="http://arxiv.org/abs/2108.00705"/>
        <updated>2021-08-03T02:06:31.184Z</updated>
        <summary type="html"><![CDATA[This paper introduces a two-phase deep feature calibration framework for
efficient learning of semantics enhanced text-image cross-modal joint
embedding, which clearly separates the deep feature calibration in data
preprocessing from training the joint embedding model. We use the Recipe1M
dataset for the technical description and empirical validation. In
preprocessing, we perform deep feature calibration by combining deep feature
engineering with semantic context features derived from raw text-image input
data. We leverage LSTM to identify key terms, NLP methods to produce ranking
scores for key terms before generating the key term feature. We leverage
wideResNet50 to extract and encode the image category semantics to help
semantic alignment of the learned recipe and image embeddings in the joint
latent space. In joint embedding learning, we perform deep feature calibration
by optimizing the batch-hard triplet loss function with soft-margin and double
negative sampling, also utilizing the category-based alignment loss and
discriminator-based alignment loss. Extensive experiments demonstrate that our
SEJE approach with the deep feature calibration significantly outperforms the
state-of-the-art approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1"&gt;Zhongwei Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Ling Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_L/0/1/0/all/0/1"&gt;Luo Zhong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Recurrent Mask Refinement for Few-Shot Medical Image Segmentation. (arXiv:2108.00622v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00622</id>
        <link href="http://arxiv.org/abs/2108.00622"/>
        <updated>2021-08-03T02:06:31.177Z</updated>
        <summary type="html"><![CDATA[Although having achieved great success in medical image segmentation, deep
convolutional neural networks usually require a large dataset with manual
annotations for training and are difficult to generalize to unseen classes.
Few-shot learning has the potential to address these challenges by learning new
classes from only a few labeled examples. In this work, we propose a new
framework for few-shot medical image segmentation based on prototypical
networks. Our innovation lies in the design of two key modules: 1) a context
relation encoder (CRE) that uses correlation to capture local relation features
between foreground and background regions; and 2) a recurrent mask refinement
module that repeatedly uses the CRE and a prototypical network to recapture the
change of context relationship and refine the segmentation mask iteratively.
Experiments on two abdomen CT datasets and an abdomen MRI dataset show the
proposed method obtains substantial improvement over the state-of-the-art
methods by an average of 16.32%, 8.45% and 6.24% in terms of DSC, respectively.
Code is publicly available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1"&gt;Hao Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xingwei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1"&gt;Shanlin Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1"&gt;Xiangyi Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1"&gt;Xiaohui Xie&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[RINDNet: Edge Detection for Discontinuity in Reflectance, Illumination, Normal and Depth. (arXiv:2108.00616v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00616</id>
        <link href="http://arxiv.org/abs/2108.00616"/>
        <updated>2021-08-03T02:06:31.171Z</updated>
        <summary type="html"><![CDATA[As a fundamental building block in computer vision, edges can be categorised
into four types according to the discontinuity in surface-Reflectance,
Illumination, surface-Normal or Depth. While great progress has been made in
detecting generic or individual types of edges, it remains under-explored to
comprehensively study all four edge types together. In this paper, we propose a
novel neural network solution, RINDNet, to jointly detect all four types of
edges. Taking into consideration the distinct attributes of each type of edges
and the relationship between them, RINDNet learns effective representations for
each of them and works in three stages. In stage I, RINDNet uses a common
backbone to extract features shared by all edges. Then in stage II it branches
to prepare discriminative features for each edge type by the corresponding
decoder. In stage III, an independent decision head for each type aggregates
the features from previous stages to predict the initial results. Additionally,
an attention module learns attention maps for all types to capture the
underlying relations between them, and these maps are combined with initial
results to generate the final edge detection results. For training and
evaluation, we construct the first public benchmark, BSDS-RIND, with all four
types of edges carefully annotated. In our experiments, RINDNet yields
promising results in comparison with state-of-the-art methods. Additional
analysis is presented in supplementary material.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pu_M/0/1/0/all/0/1"&gt;Mengyang Pu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yaping Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guan_Q/0/1/0/all/0/1"&gt;Qingji Guan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ling_H/0/1/0/all/0/1"&gt;Haibin Ling&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Investigating Attention Mechanism in 3D Point Cloud Object Detection. (arXiv:2108.00620v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00620</id>
        <link href="http://arxiv.org/abs/2108.00620"/>
        <updated>2021-08-03T02:06:31.164Z</updated>
        <summary type="html"><![CDATA[Object detection in three-dimensional (3D) space attracts much interest from
academia and industry since it is an essential task in AI-driven applications
such as robotics, autonomous driving, and augmented reality. As the basic
format of 3D data, the point cloud can provide detailed geometric information
about the objects in the original 3D space. However, due to 3D data's sparsity
and unorderedness, specially designed networks and modules are needed to
process this type of data. Attention mechanism has achieved impressive
performance in diverse computer vision tasks; however, it is unclear how
attention modules would affect the performance of 3D point cloud object
detection and what sort of attention modules could fit with the inherent
properties of 3D data. This work investigates the role of the attention
mechanism in 3D point cloud object detection and provides insights into the
potential of different attention modules. To achieve that, we comprehensively
investigate classical 2D attentions, novel 3D attentions, including the latest
point cloud transformers on SUN RGB-D and ScanNetV2 datasets. Based on the
detailed experiments and analysis, we conclude the effects of different
attention modules. This paper is expected to serve as a reference source for
benefiting attention-embedded 3D point cloud object detection. The code and
trained models are available at:
https://github.com/ShiQiu0419/attentions_in_3D_detection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qiu_S/0/1/0/all/0/1"&gt;Shi Qiu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yunfan Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Anwar_S/0/1/0/all/0/1"&gt;Saeed Anwar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chongyi Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Active Perception for Ambiguous Objects Classification. (arXiv:2108.00737v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00737</id>
        <link href="http://arxiv.org/abs/2108.00737"/>
        <updated>2021-08-03T02:06:31.148Z</updated>
        <summary type="html"><![CDATA[Recent visual pose estimation and tracking solutions provide notable results
on popular datasets such as T-LESS and YCB. However, in the real world, we can
find ambiguous objects that do not allow exact classification and detection
from a single view. In this work, we propose a framework that, given a single
view of an object, provides the coordinates of a next viewpoint to discriminate
the object against similar ones, if any, and eliminates ambiguities. We also
describe a complete pipeline from a real object's scans to the viewpoint
selection and classification. We validate our approach with a Franka Emika
Panda robot and common household objects featured with ambiguities. We released
the source code to reproduce our experiments.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Safronov_E/0/1/0/all/0/1"&gt;Evgenii Safronov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Piga_N/0/1/0/all/0/1"&gt;Nicola Piga&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Colledanchise_M/0/1/0/all/0/1"&gt;Michele Colledanchise&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Natale_L/0/1/0/all/0/1"&gt;Lorenzo Natale&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-supervised Learning with Local Attention-Aware Feature. (arXiv:2108.00475v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00475</id>
        <link href="http://arxiv.org/abs/2108.00475"/>
        <updated>2021-08-03T02:06:31.143Z</updated>
        <summary type="html"><![CDATA[In this work, we propose a novel methodology for self-supervised learning for
generating global and local attention-aware visual features. Our approach is
based on training a model to differentiate between specific image
transformations of an input sample and the patched images. Utilizing this
approach, the proposed method is able to outperform the previous best
competitor by 1.03% on the Tiny-ImageNet dataset and by 2.32% on the STL-10
dataset. Furthermore, our approach outperforms the fully-supervised learning
method on the STL-10 dataset. Experimental results and visualizations show the
capability of successfully learning global and local attention-aware visual
representations.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pham_T/0/1/0/all/0/1"&gt;Trung X. Pham&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mina_R/0/1/0/all/0/1"&gt;Rusty John Lloyd Mina&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Issa_D/0/1/0/all/0/1"&gt;Dias Issa&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yoo_C/0/1/0/all/0/1"&gt;Chang D. Yoo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-supervised Audiovisual Representation Learning for Remote Sensing Data. (arXiv:2108.00688v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00688</id>
        <link href="http://arxiv.org/abs/2108.00688"/>
        <updated>2021-08-03T02:06:31.137Z</updated>
        <summary type="html"><![CDATA[Many current deep learning approaches make extensive use of backbone networks
pre-trained on large datasets like ImageNet, which are then fine-tuned to
perform a certain task. In remote sensing, the lack of comparable large
annotated datasets and the wide diversity of sensing platforms impedes similar
developments. In order to contribute towards the availability of pre-trained
backbone networks in remote sensing, we devise a self-supervised approach for
pre-training deep neural networks. By exploiting the correspondence between
geo-tagged audio recordings and remote sensing imagery, this is done in a
completely label-free manner, eliminating the need for laborious manual
annotation. For this purpose, we introduce the SoundingEarth dataset, which
consists of co-located aerial imagery and audio samples all around the world.
Using this dataset, we then pre-train ResNet models to map samples from both
modalities into a common embedding space, which encourages the models to
understand key properties of a scene that influence both visual and auditory
appearance. To validate the usefulness of the proposed approach, we evaluate
the transfer learning performance of pre-trained weights obtained against
weights obtained through other means. By fine-tuning the models on a number of
commonly used remote sensing datasets, we show that our approach outperforms
existing pre-training strategies for remote sensing imagery. The dataset, code
and pre-trained model weights will be available at
https://github.com/khdlr/SoundingEarth.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Heidler_K/0/1/0/all/0/1"&gt;Konrad Heidler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mou_L/0/1/0/all/0/1"&gt;Lichao Mou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_D/0/1/0/all/0/1"&gt;Di Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_P/0/1/0/all/0/1"&gt;Pu Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1"&gt;Guangyao Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gan_C/0/1/0/all/0/1"&gt;Chuang Gan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1"&gt;Ji-Rong Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1"&gt;Xiao Xiang Zhu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cohort Bias Adaptation in Aggregated Datasets for Lesion Segmentation. (arXiv:2108.00713v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2108.00713</id>
        <link href="http://arxiv.org/abs/2108.00713"/>
        <updated>2021-08-03T02:06:31.120Z</updated>
        <summary type="html"><![CDATA[Many automatic machine learning models developed for focal pathology (e.g.
lesions, tumours) detection and segmentation perform well, but do not
generalize as well to new patient cohorts, impeding their widespread adoption
into real clinical contexts. One strategy to create a more diverse,
generalizable training set is to naively pool datasets from different cohorts.
Surprisingly, training on this \it{big data} does not necessarily increase, and
may even reduce, overall performance and model generalizability, due to the
existence of cohort biases that affect label distributions. In this paper, we
propose a generalized affine conditioning framework to learn and account for
cohort biases across multi-source datasets, which we call Source-Conditioned
Instance Normalization (SCIN). Through extensive experimentation on three
different, large scale, multi-scanner, multi-centre Multiple Sclerosis (MS)
clinical trial MRI datasets, we show that our cohort bias adaptation method (1)
improves performance of the network on pooled datasets relative to naively
pooling datasets and (2) can quickly adapt to a new cohort by fine-tuning the
instance normalization parameters, thus learning the new cohort bias with only
10 labelled samples.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Nichyporuk_B/0/1/0/all/0/1"&gt;Brennan Nichyporuk&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Cardinell_J/0/1/0/all/0/1"&gt;Jillian Cardinell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Szeto_J/0/1/0/all/0/1"&gt;Justin Szeto&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Mehta_R/0/1/0/all/0/1"&gt;Raghav Mehta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Tsaftaris_S/0/1/0/all/0/1"&gt;Sotirios Tsaftaris&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Arnold_D/0/1/0/all/0/1"&gt;Douglas L. Arnold&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Arbel_T/0/1/0/all/0/1"&gt;Tal Arbel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Congested Crowd Instance Localization with Dilated Convolutional Swin Transformer. (arXiv:2108.00584v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00584</id>
        <link href="http://arxiv.org/abs/2108.00584"/>
        <updated>2021-08-03T02:06:31.112Z</updated>
        <summary type="html"><![CDATA[Crowd localization is a new computer vision task, evolved from crowd
counting. Different from the latter, it provides more precise location
information for each instance, not just counting numbers for the whole crowd
scene, which brings greater challenges, especially in extremely congested crowd
scenes. In this paper, we focus on how to achieve precise instance localization
in high-density crowd scenes, and to alleviate the problem that the feature
extraction ability of the traditional model is reduced due to the target
occlusion, the image blur, etc. To this end, we propose a Dilated Convolutional
Swin Transformer (DCST) for congested crowd scenes. Specifically, a
window-based vision transformer is introduced into the crowd localization task,
which effectively improves the capacity of representation learning. Then, the
well-designed dilated convolutional module is inserted into some different
stages of the transformer to enhance the large-range contextual information.
Extensive experiments evidence the effectiveness of the proposed methods and
achieve state-of-the-art performance on five popular datasets. Especially, the
proposed model achieves F1-measure of 77.5\% and MAE of 84.2 in terms of
localization and counting performance, respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1"&gt;Junyu Gao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_M/0/1/0/all/0/1"&gt;Maoguo Gong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1"&gt;Xuelong Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[PSE-Match: A Viewpoint-free Place Recognition Method with Parallel Semantic Embedding. (arXiv:2108.00552v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00552</id>
        <link href="http://arxiv.org/abs/2108.00552"/>
        <updated>2021-08-03T02:06:31.078Z</updated>
        <summary type="html"><![CDATA[Accurate localization on autonomous driving cars is essential for autonomy
and driving safety, especially for complex urban streets and search-and-rescue
subterranean environments where high-accurate GPS is not available. However
current odometry estimation may introduce the drifting problems in long-term
navigation without robust global localization. The main challenges involve
scene divergence under the interference of dynamic environments and effective
perception of observation and object layout variance from different viewpoints.
To tackle these challenges, we present PSE-Match, a viewpoint-free place
recognition method based on parallel semantic analysis of isolated semantic
attributes from 3D point-cloud models. Compared with the original point cloud,
the observed variance of semantic attributes is smaller. PSE-Match incorporates
a divergence place learning network to capture different semantic attributes
parallelly through the spherical harmonics domain. Using both existing
benchmark datasets and two in-field collected datasets, our experiments show
that the proposed method achieves above 70% average recall with top one
retrieval and above 95% average recall with top ten retrieval cases. And
PSE-Match has also demonstrated an obvious generalization ability with a
limited training dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yin_P/0/1/0/all/0/1"&gt;Peng Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1"&gt;Lingyun Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Egorov_A/0/1/0/all/0/1"&gt;Anton Egorov&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bing Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BundleTrack: 6D Pose Tracking for Novel Objects without Instance or Category-Level 3D Models. (arXiv:2108.00516v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00516</id>
        <link href="http://arxiv.org/abs/2108.00516"/>
        <updated>2021-08-03T02:06:31.072Z</updated>
        <summary type="html"><![CDATA[Tracking the 6D pose of objects in video sequences is important for robot
manipulation. Most prior efforts, however, often assume that the target
object's CAD model, at least at a category-level, is available for offline
training or during online template matching. This work proposes BundleTrack, a
general framework for 6D pose tracking of novel objects, which does not depend
upon 3D models, either at the instance or category-level. It leverages the
complementary attributes of recent advances in deep learning for segmentation
and robust feature extraction, as well as memory-augmented pose graph
optimization for spatiotemporal consistency. This enables long-term, low-drift
tracking under various challenging scenarios, including significant occlusions
and object motions. Comprehensive experiments given two public benchmarks
demonstrate that the proposed approach significantly outperforms state-of-art,
category-level 6D tracking or dynamic SLAM methods. When compared against
state-of-art methods that rely on an object instance CAD model, comparable
performance is achieved, despite the proposed method's reduced information
requirements. An efficient implementation in CUDA provides a real-time
performance of 10Hz for the entire framework. Code is available at:
https://github.com/wenbowen123/BundleTrack]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wen_B/0/1/0/all/0/1"&gt;Bowen Wen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bekris_K/0/1/0/all/0/1"&gt;Kostas Bekris&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Object-to-Scene: Learning to Transfer Object Knowledge to Indoor Scene Recognition. (arXiv:2108.00399v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00399</id>
        <link href="http://arxiv.org/abs/2108.00399"/>
        <updated>2021-08-03T02:06:31.053Z</updated>
        <summary type="html"><![CDATA[Accurate perception of the surrounding scene is helpful for robots to make
reasonable judgments and behaviours. Therefore, developing effective scene
representation and recognition methods are of significant importance in
robotics. Currently, a large body of research focuses on developing novel
auxiliary features and networks to improve indoor scene recognition ability.
However, few of them focus on directly constructing object features and
relations for indoor scene recognition. In this paper, we analyze the
weaknesses of current methods and propose an Object-to-Scene (OTS) method,
which extracts object features and learns object relations to recognize indoor
scenes. The proposed OTS first extracts object features based on the
segmentation network and the proposed object feature aggregation module (OFAM).
Afterwards, the object relations are calculated and the scene representation is
constructed based on the proposed object attention module (OAM) and global
relation aggregation module (GRAM). The final results in this work show that
OTS successfully extracts object features and learns object relations from the
segmentation network. Moreover, OTS outperforms the state-of-the-art methods by
more than 2\% on indoor scene recognition without using any additional streams.
Code is publicly available at: https://github.com/FreeformRobotics/OTS.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Miao_B/0/1/0/all/0/1"&gt;Bo Miao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1"&gt;Liguang Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mian_A/0/1/0/all/0/1"&gt;Ajmal Mian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lam_T/0/1/0/all/0/1"&gt;Tin Lun Lam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yangsheng Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Automated Pest Detection with DNN on the Edge for Precision Agriculture. (arXiv:2108.00421v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.00421</id>
        <link href="http://arxiv.org/abs/2108.00421"/>
        <updated>2021-08-03T02:06:31.047Z</updated>
        <summary type="html"><![CDATA[Artificial intelligence has smoothly penetrated several economic activities,
especially monitoring and control applications, including the agriculture
sector. However, research efforts toward low-power sensing devices with fully
functional machine learning (ML) on-board are still fragmented and limited in
smart farming. Biotic stress is one of the primary causes of crop yield
reduction. With the development of deep learning in computer vision technology,
autonomous detection of pest infestation through images has become an important
research direction for timely crop disease diagnosis. This paper presents an
embedded system enhanced with ML functionalities, ensuring continuous detection
of pest infestation inside fruit orchards. The embedded solution is based on a
low-power embedded sensing system along with a Neural Accelerator able to
capture and process images inside common pheromone-based traps. Three different
ML algorithms have been trained and deployed, highlighting the capabilities of
the platform. Moreover, the proposed approach guarantees an extended battery
life thanks to the integration of energy harvesting functionalities. Results
show how it is possible to automate the task of pest infestation for unlimited
time without the farmer's intervention.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Albanese_A/0/1/0/all/0/1"&gt;Andrea Albanese&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nardello_M/0/1/0/all/0/1"&gt;Matteo Nardello&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brunelli_D/0/1/0/all/0/1"&gt;Davide Brunelli&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[FLASH: Fast Neural Architecture Search with Hardware Optimization. (arXiv:2108.00568v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00568</id>
        <link href="http://arxiv.org/abs/2108.00568"/>
        <updated>2021-08-03T02:06:31.033Z</updated>
        <summary type="html"><![CDATA[Neural architecture search (NAS) is a promising technique to design efficient
and high-performance deep neural networks (DNNs). As the performance
requirements of ML applications grow continuously, the hardware accelerators
start playing a central role in DNN design. This trend makes NAS even more
complicated and time-consuming for most real applications. This paper proposes
FLASH, a very fast NAS methodology that co-optimizes the DNN accuracy and
performance on a real hardware platform. As the main theoretical contribution,
we first propose the NN-Degree, an analytical metric to quantify the
topological characteristics of DNNs with skip connections (e.g., DenseNets,
ResNets, Wide-ResNets, and MobileNets). The newly proposed NN-Degree allows us
to do training-free NAS within one second and build an accuracy predictor by
training as few as 25 samples out of a vast search space with more than 63
billion configurations. Second, by performing inference on the target hardware,
we fine-tune and validate our analytical models to estimate the latency, area,
and energy consumption of various DNN architectures while executing standard ML
datasets. Third, we construct a hierarchical algorithm based on simplicial
homology global optimization (SHGO) to optimize the model-architecture
co-design process, while considering the area, latency, and energy consumption
of the target hardware. We demonstrate that, compared to the state-of-the-art
NAS approaches, our proposed hierarchical SHGO-based algorithm enables more
than four orders of magnitude speedup (specifically, the execution time of the
proposed algorithm is about 0.1 seconds). Finally, our experimental evaluations
show that FLASH is easily transferable to different hardware architectures,
thus enabling us to do NAS on a Raspberry Pi-3B processor in less than 3
seconds.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1"&gt;Guihong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mandal_S/0/1/0/all/0/1"&gt;Sumit K. Mandal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ogras_U/0/1/0/all/0/1"&gt;Umit Y. Ogras&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Marculescu_R/0/1/0/all/0/1"&gt;Radu Marculescu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Effective and Robust Detector for Logo Detection. (arXiv:2108.00422v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00422</id>
        <link href="http://arxiv.org/abs/2108.00422"/>
        <updated>2021-08-03T02:06:31.026Z</updated>
        <summary type="html"><![CDATA[In recent years, intellectual property (IP), which represents literary,
inventions, artistic works, etc, gradually attract more and more people's
attention. Particularly, with the rise of e-commerce, the IP not only
represents the product design and brands, but also represents the images/videos
displayed on e-commerce platforms. Unfortunately, some attackers adopt some
adversarial methods to fool the well-trained logo detection model for
infringement. To overcome this problem, a novel logo detector based on the
mechanism of looking and thinking twice is proposed in this paper for robust
logo detection. The proposed detector is different from other mainstream
detectors, which can effectively detect small objects, long-tail objects, and
is robust to adversarial images. In detail, we extend detectoRS algorithm to a
cascade schema with an equalization loss function, multi-scale transformations,
and adversarial data augmentation. A series of experimental results have shown
that the proposed method can effectively improve the robustness of the
detection model. Moreover, we have applied the proposed methods to competition
ACM MM2021 Robust Logo Detection that is organized by Alibaba on the Tianchi
platform and won top 2 in 36489 teams. Code is available at
https://github.com/jiaxiaojunQAQ/Robust-Logo-Detection.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1"&gt;Xiaojun Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1"&gt;Huanqian Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yonglin Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1"&gt;Xingxing Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1"&gt;Xiaochun Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yong Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GTNet:Guided Transformer Network for Detecting Human-Object Interactions. (arXiv:2108.00596v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00596</id>
        <link href="http://arxiv.org/abs/2108.00596"/>
        <updated>2021-08-03T02:06:31.008Z</updated>
        <summary type="html"><![CDATA[The human-object interaction (HOI) detection task refers to localizing
humans, localizing objects, and predicting the interactions between each
human-object pair. HOI is considered one of the fundamental steps in truly
understanding complex visual scenes. For detecting HOI, it is important to
utilize relative spatial configurations and object semantics to find salient
spatial regions of images that highlight the interactions between human object
pairs. This issue is addressed by the proposed self-attention based guided
transformer network, GTNet. GTNet encodes this spatial contextual information
in human and object visual features via self-attention while achieving a 4%-6%
improvement over previous state of the art results on both the V-COCO and
HICO-DET datasets. Code will be made available online.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Iftekhar_A/0/1/0/all/0/1"&gt;A S M Iftekhar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1"&gt;Satish Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+McEver_R/0/1/0/all/0/1"&gt;R. Austin McEver&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+You_S/0/1/0/all/0/1"&gt;Suya You&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Manjunath_B/0/1/0/all/0/1"&gt;B.S. Manjunath&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multiple Classifiers Based Maximum Classifier Discrepancy for Unsupervised Domain Adaptation. (arXiv:2108.00610v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00610</id>
        <link href="http://arxiv.org/abs/2108.00610"/>
        <updated>2021-08-03T02:06:30.996Z</updated>
        <summary type="html"><![CDATA[Adversarial training based on the maximum classifier discrepancy between the
two classifier structures has achieved great success in unsupervised domain
adaptation tasks for image classification. The approach adopts the structure of
two classifiers, though simple and intuitive, the learned classification
boundary may not well represent the data property in the new domain. In this
paper, we propose to extend the structure to multiple classifiers to further
boost its performance. To this end, we propose a very straightforward approach
to adding more classifiers. We employ the principle that the classifiers are
different from each other to construct a discrepancy loss function for multiple
classifiers. Through the loss function construction method, we make it possible
to add any number of classifiers to the original framework. The proposed
approach is validated through extensive experimental evaluations. We
demonstrate that, on average, adopting the structure of three classifiers
normally yields the best performance as a trade-off between the accuracy and
efficiency. With minimum extra computational costs, the proposed approach can
significantly improve the original algorithm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1"&gt;Yiju Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1"&gt;Taejoon Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1"&gt;Guanghui Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[GraphFPN: Graph Feature Pyramid Network for Object Detection. (arXiv:2108.00580v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00580</id>
        <link href="http://arxiv.org/abs/2108.00580"/>
        <updated>2021-08-03T02:06:30.988Z</updated>
        <summary type="html"><![CDATA[Feature pyramids have been proven powerful in image understanding tasks that
require multi-scale features. State-of-the-art methods for multi-scale feature
learning focus on performing feature interactions across space and scales using
neural networks with a fixed topology. In this paper, we propose graph feature
pyramid networks that are capable of adapting their topological structures to
varying intrinsic image structures and supporting simultaneous feature
interactions across all scales. We first define an image-specific superpixel
hierarchy for each input image to represent its intrinsic image structures. The
graph feature pyramid network inherits its structure from this superpixel
hierarchy. Contextual and hierarchical layers are designed to achieve feature
interactions within the same scale and across different scales. To make these
layers more powerful, we introduce two types of local channel attention for
graph neural networks by generalizing global channel attention for
convolutional neural networks. The proposed graph feature pyramid network can
enhance the multiscale features from a convolutional feature pyramid network.
We evaluate our graph feature pyramid network in the object detection task by
integrating it into the Faster R-CNN algorithm. The modified algorithm
outperforms not only previous state-of-the-art feature pyramid-based methods
with a clear margin but also other popular detection methods on both MS-COCO
2017 validation and test datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1"&gt;Gangming Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ge_W/0/1/0/all/0/1"&gt;Weifeng Ge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1"&gt;Yizhou Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CSC-Unet: A Novel Convolutional Sparse Coding Strategy based Neural Network for Semantic Segmentation. (arXiv:2108.00408v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00408</id>
        <link href="http://arxiv.org/abs/2108.00408"/>
        <updated>2021-08-03T02:06:30.938Z</updated>
        <summary type="html"><![CDATA[It is a challenging task to accurately perform semantic segmentation due to
the complexity of real picture scenes. Many semantic segmentation methods based
on traditional deep learning insufficiently captured the semantic and
appearance information of images, which put limit on their generality and
robustness for various application scenes. In this paper, we proposed a novel
strategy that reformulated the popularly-used convolution operation to
multi-layer convolutional sparse coding block to ease the aforementioned
deficiency. This strategy can be possibly used to significantly improve the
segmentation performance of any semantic segmentation model that involves
convolutional operations. To prove the effectiveness of our idea, we chose the
widely-used U-Net model for the demonstration purpose, and we designed CSC-Unet
model series based on U-Net. Through extensive analysis and experiments, we
provided credible evidence showing that the multi-layer convolutional sparse
coding block enables semantic segmentation model to converge faster, can
extract finer semantic and appearance information of images, and improve the
ability to recover spatial detail information. The best CSC-Unet model
significantly outperforms the results of the original U-Net on three public
datasets with different scenarios, i.e., 87.14% vs. 84.71% on DeepCrack
dataset, 68.91% vs. 67.09% on Nuclei dataset, and 53.68% vs. 48.82% on CamVid
dataset, respectively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1"&gt;Haitong Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1"&gt;Shuang He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1"&gt;Xia Lu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1"&gt;Qin Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1"&gt;Kaiyue Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1"&gt;Hongjie Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1"&gt;Nizhuan Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Threat of Adversarial Attacks on Deep Learning in Computer Vision: Survey II. (arXiv:2108.00401v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00401</id>
        <link href="http://arxiv.org/abs/2108.00401"/>
        <updated>2021-08-03T02:06:30.932Z</updated>
        <summary type="html"><![CDATA[Deep Learning (DL) is the most widely used tool in the contemporary field of
computer vision. Its ability to accurately solve complex problems is employed
in vision research to learn deep neural models for a variety of tasks,
including security critical applications. However, it is now known that DL is
vulnerable to adversarial attacks that can manipulate its predictions by
introducing visually imperceptible perturbations in images and videos. Since
the discovery of this phenomenon in 2013~[1], it has attracted significant
attention of researchers from multiple sub-fields of machine intelligence. In
[2], we reviewed the contributions made by the computer vision community in
adversarial attacks on deep learning (and their defenses) until the advent of
year 2018. Many of those contributions have inspired new directions in this
area, which has matured significantly since witnessing the first generation
methods. Hence, as a legacy sequel of [2], this literature review focuses on
the advances in this area since 2018. To ensure authenticity, we mainly
consider peer-reviewed contributions published in the prestigious sources of
computer vision and machine learning research. Besides a comprehensive
literature review, the article also provides concise definitions of technical
terminologies for non-experts in this domain. Finally, this article discusses
challenges and future outlook of this direction based on the literature
reviewed herein and [2].]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Akhtar_N/0/1/0/all/0/1"&gt;Naveed Akhtar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mian_A/0/1/0/all/0/1"&gt;Ajmal Mian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kardan_N/0/1/0/all/0/1"&gt;Navid Kardan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1"&gt;Mubarak Shah&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Explainable Deep Few-shot Anomaly Detection with Deviation Networks. (arXiv:2108.00462v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00462</id>
        <link href="http://arxiv.org/abs/2108.00462"/>
        <updated>2021-08-03T02:06:30.926Z</updated>
        <summary type="html"><![CDATA[Existing anomaly detection paradigms overwhelmingly focus on training
detection models using exclusively normal data or unlabeled data (mostly normal
samples). One notorious issue with these approaches is that they are weak in
discriminating anomalies from normal samples due to the lack of the knowledge
about the anomalies. Here, we study the problem of few-shot anomaly detection,
in which we aim at using a few labeled anomaly examples to train
sample-efficient discriminative detection models. To address this problem, we
introduce a novel weakly-supervised anomaly detection framework to train
detection models without assuming the examples illustrating all possible
classes of anomaly.

Specifically, the proposed approach learns discriminative normality
(regularity) by leveraging the labeled anomalies and a prior probability to
enforce expressive representations of normality and unbounded deviated
representations of abnormality. This is achieved by an end-to-end optimization
of anomaly scores with a neural deviation learning, in which the anomaly scores
of normal samples are imposed to approximate scalar scores drawn from the prior
while that of anomaly examples is enforced to have statistically significant
deviations from these sampled scores in the upper tail. Furthermore, our model
is optimized to learn fine-grained normality and abnormality by top-K
multiple-instance-learning-based feature subspace deviation learning, allowing
more generalized representations. Comprehensive experiments on nine real-world
image anomaly detection benchmarks show that our model is substantially more
sample-efficient and robust, and performs significantly better than
state-of-the-art competing methods in both closed-set and open-set settings.
Our model can also offer explanation capability as a result of its prior-driven
anomaly score learning. Code and datasets are available at:
https://git.io/DevNet.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pang_G/0/1/0/all/0/1"&gt;Guansong Pang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1"&gt;Choubo Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1"&gt;Chunhua Shen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hengel_A/0/1/0/all/0/1"&gt;Anton van den Hengel&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CERL: A Unified Optimization Framework for Light Enhancement with Realistic Noise. (arXiv:2108.00478v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00478</id>
        <link href="http://arxiv.org/abs/2108.00478"/>
        <updated>2021-08-03T02:06:30.919Z</updated>
        <summary type="html"><![CDATA[Low-light images captured in the real world are inevitably corrupted by
sensor noise. Such noise is spatially variant and highly dependent on the
underlying pixel intensity, deviating from the oversimplified assumptions in
conventional denoising. Existing light enhancement methods either overlook the
important impact of real-world noise during enhancement, or treat noise removal
as a separate pre- or post-processing step. We present Coordinated Enhancement
for Real-world Low-light Noisy Images (CERL), that seamlessly integrates light
enhancement and noise suppression parts into a unified and physics-grounded
optimization framework. For the real low-light noise removal part, we customize
a self-supervised denoising model that can easily be adapted without referring
to clean ground-truth images. For the light enhancement part, we also improve
the design of a state-of-the-art backbone. The two parts are then joint
formulated into one principled plug-and-play optimization. Our approach is
compared against state-of-the-art low-light enhancement methods both
qualitatively and quantitatively. Besides standard benchmarks, we further
collect and test on a new realistic low-light mobile photography dataset
(RLMP), whose mobile-captured photos display heavier realistic noise than those
taken by high-quality cameras. CERL consistently produces the most visually
pleasing and artifact-free results across all experiments. Our RLMP dataset and
codes are available at: https://github.com/VITA-Group/CERL.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zeyuan Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1"&gt;Yifan Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1"&gt;Dong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Zhangyang Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BORM: Bayesian Object Relation Model for Indoor Scene Recognition. (arXiv:2108.00397v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00397</id>
        <link href="http://arxiv.org/abs/2108.00397"/>
        <updated>2021-08-03T02:06:30.913Z</updated>
        <summary type="html"><![CDATA[Scene recognition is a fundamental task in robotic perception. For human
beings, scene recognition is reasonable because they have abundant object
knowledge of the real world. The idea of transferring prior object knowledge
from humans to scene recognition is significant but still less exploited. In
this paper, we propose to utilize meaningful object representations for indoor
scene representation. First, we utilize an improved object model (IOM) as a
baseline that enriches the object knowledge by introducing a scene parsing
algorithm pretrained on the ADE20K dataset with rich object categories related
to the indoor scene. To analyze the object co-occurrences and pairwise object
relations, we formulate the IOM from a Bayesian perspective as the Bayesian
object relation model (BORM). Meanwhile, we incorporate the proposed BORM with
the PlacesCNN model as the combined Bayesian object relation model (CBORM) for
scene recognition and significantly outperforms the state-of-the-art methods on
the reduced Places365 dataset, and SUN RGB-D dataset without retraining,
showing the excellent generalization ability of the proposed method. Code can
be found at https://github.com/hszhoushen/borm.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1"&gt;Liguang Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cen_J/0/1/0/all/0/1"&gt;Jun Cen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xingchao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1"&gt;Zhenglong Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lam_T/0/1/0/all/0/1"&gt;Tin Lun Lam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yangsheng Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Maritime Obstacle Detection from Weak Annotations by Scaffolding. (arXiv:2108.00564v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00564</id>
        <link href="http://arxiv.org/abs/2108.00564"/>
        <updated>2021-08-03T02:06:30.906Z</updated>
        <summary type="html"><![CDATA[Coastal water autonomous boats rely on robust perception methods for obstacle
detection and timely collision avoidance. The current state-of-the-art is based
on deep segmentation networks trained on large datasets. Per-pixel ground truth
labeling of such datasets, however, is labor-intensive and expensive. We
observe that far less information is required for practical obstacle avoidance
- the location of water edge on static obstacles like shore and approximate
location and bounds of dynamic obstacles in the water is sufficient to plan a
reaction. We propose a new scaffolding learning regime (SLR) that allows
training obstacle detection segmentation networks only from such weak
annotations, thus significantly reducing the cost of ground-truth labeling.
Experiments show that maritime obstacle segmentation networks trained using SLR
substantially outperform the same networks trained with dense ground truth
labels. Thus accuracy is not sacrificed for labelling simplicity but is in fact
improved, which is a remarkable result.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zust_L/0/1/0/all/0/1"&gt;Lojze &amp;#x17d;ust&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kristan_M/0/1/0/all/0/1"&gt;Matej Kristan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hyper360 -- a Next Generation Toolset for Immersive Media. (arXiv:2108.00430v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00430</id>
        <link href="http://arxiv.org/abs/2108.00430"/>
        <updated>2021-08-03T02:06:30.845Z</updated>
        <summary type="html"><![CDATA[Spherical 360{\deg} video is a novel media format, rapidly becoming adopted
in media production and consumption of immersive media. Due to its novelty,
there is a lack of tools for producing highly engaging interactive 360{\deg}
video for consumption on a multitude of platforms. In this work, we describe
the work done so far in the Hyper360 project on tools for mixed 360{\deg} video
and 3D content. Furthermore, the first pilots which have been produced with the
Hyper360 tools and results of the audience assessment of the produced pilots
are presented.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Fassold_H/0/1/0/all/0/1"&gt;Hannes Fassold&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Karakottas_A/0/1/0/all/0/1"&gt;Antonis Karakottas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tsatsou_D/0/1/0/all/0/1"&gt;Dorothea Tsatsou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zarpalas_D/0/1/0/all/0/1"&gt;Dimitrios Zarpalas&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Takacs_B/0/1/0/all/0/1"&gt;Barnabas Takacs&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fuhrhop_C/0/1/0/all/0/1"&gt;Christian Fuhrhop&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Manfredi_A/0/1/0/all/0/1"&gt;Angelo Manfredi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Patz_N/0/1/0/all/0/1"&gt;Nicolas Patz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tonoli_S/0/1/0/all/0/1"&gt;Simona Tonoli&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dulskaia_I/0/1/0/all/0/1"&gt;Iana Dulskaia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Deep graph matching meets mixed-integer linear programming: Relax at your own risk ?. (arXiv:2108.00394v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00394</id>
        <link href="http://arxiv.org/abs/2108.00394"/>
        <updated>2021-08-03T02:06:30.834Z</updated>
        <summary type="html"><![CDATA[Graph matching is an important problem that has received widespread
attention, especially in the field of computer vision. Recently,
state-of-the-art methods seek to incorporate graph matching with deep learning.
However, there is no research to explain what role the graph matching algorithm
plays in the model. Therefore, we propose an approach integrating a MILP
formulation of the graph matching problem. This formulation is solved to
optimal and it provides inherent baseline. Meanwhile, similar approaches are
derived by releasing the optimal guarantee of the graph matching solver and by
introducing a quality level. This quality level controls the quality of the
solutions provided by the graph matching solver. In addition, several
relaxations of the graph matching problem are put to the test. Our experimental
evaluation gives several theoretical insights and guides the direction of deep
graph matching methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zhoubo Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1"&gt;Puqing Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raveaux_R/0/1/0/all/0/1"&gt;Romain Raveaux&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1"&gt;Huadong Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[WAS-VTON: Warping Architecture Search for Virtual Try-on Network. (arXiv:2108.00386v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00386</id>
        <link href="http://arxiv.org/abs/2108.00386"/>
        <updated>2021-08-03T02:06:30.817Z</updated>
        <summary type="html"><![CDATA[Despite recent progress on image-based virtual try-on, current methods are
constraint by shared warping networks and thus fail to synthesize natural
try-on results when faced with clothing categories that require different
warping operations. In this paper, we address this problem by finding clothing
category-specific warping networks for the virtual try-on task via Neural
Architecture Search (NAS). We introduce a NAS-Warping Module and elaborately
design a bilevel hierarchical search space to identify the optimal
network-level and operation-level flow estimation architecture. Given the
network-level search space, containing different numbers of warping blocks, and
the operation-level search space with different convolution operations, we
jointly learn a combination of repeatable warping cells and convolution
operations specifically for the clothing-person alignment. Moreover, a
NAS-Fusion Module is proposed to synthesize more natural final try-on results,
which is realized by leveraging particular skip connections to produce
better-fused features that are required for seamlessly fusing the warped
clothing and the unchanged person part. We adopt an efficient and stable
one-shot searching strategy to search the above two modules. Extensive
experiments demonstrate that our WAS-VTON significantly outperforms the
previous fixed-architecture try-on methods with more natural warping results
and virtual try-on results.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1"&gt;Zhenyu Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1"&gt;Xujie Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_F/0/1/0/all/0/1"&gt;Fuwei Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1"&gt;Haoye Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kampffmeyer_M/0/1/0/all/0/1"&gt;Michael C. Kampffmeyer&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1"&gt;Haonan Yan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1"&gt;Xiaodan Liang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Neural Free-Viewpoint Performance Rendering under ComplexHuman-object Interactions. (arXiv:2108.00362v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00362</id>
        <link href="http://arxiv.org/abs/2108.00362"/>
        <updated>2021-08-03T02:06:30.800Z</updated>
        <summary type="html"><![CDATA[4D reconstruction of human-object interaction is critical for immersive VR/AR
experience and human activity understanding. Recent advances still fail to
recover fine geometry and texture results from sparse RGB inputs, especially
under challenging human-object interactions scenarios. In this paper, we
propose a neural human performance capture and rendering system to generate
both high-quality geometry and photo-realistic texture of both human and
objects under challenging interaction scenarios in arbitrary novel views, from
only sparse RGB streams. To deal with complex occlusions raised by human-object
interactions, we adopt a layer-wise scene decoupling strategy and perform
volumetric reconstruction and neural rendering of the human and object.
Specifically, for geometry reconstruction, we propose an interaction-aware
human-object capture scheme that jointly considers the human reconstruction and
object reconstruction with their correlations. Occlusion-aware human
reconstruction and robust human-aware object tracking are proposed for
consistent 4D human-object dynamic reconstruction. For neural texture
rendering, we propose a layer-wise human-object rendering scheme, which
combines direction-aware neural blending weight learning and spatial-temporal
texture completion to provide high-resolution and photo-realistic texture
results in the occluded scenarios. Extensive experiments demonstrate the
effectiveness of our approach to achieve high-quality geometry and texture
reconstruction in free viewpoints for challenging human-object interactions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_G/0/1/0/all/0/1"&gt;Guoxing Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1"&gt;Xin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yizhang Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pang_A/0/1/0/all/0/1"&gt;Anqi Pang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_P/0/1/0/all/0/1"&gt;Pei Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1"&gt;Yuheng Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1"&gt;Lan Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1"&gt;Jingya Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1"&gt;Jingyi Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Visual Boundary Knowledge Translation for Foreground Segmentation. (arXiv:2108.00379v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00379</id>
        <link href="http://arxiv.org/abs/2108.00379"/>
        <updated>2021-08-03T02:06:30.733Z</updated>
        <summary type="html"><![CDATA[When confronted with objects of unknown types in an image, humans can
effortlessly and precisely tell their visual boundaries. This recognition
mechanism and underlying generalization capability seem to contrast to
state-of-the-art image segmentation networks that rely on large-scale
category-aware annotated training samples. In this paper, we make an attempt
towards building models that explicitly account for visual boundary knowledge,
in hope to reduce the training effort on segmenting unseen categories.
Specifically, we investigate a new task termed as Boundary Knowledge
Translation (BKT). Given a set of fully labeled categories, BKT aims to
translate the visual boundary knowledge learned from the labeled categories, to
a set of novel categories, each of which is provided only a few labeled
samples. To this end, we propose a Translation Segmentation Network
(Trans-Net), which comprises a segmentation network and two boundary
discriminators. The segmentation network, combined with a boundary-aware
self-supervised mechanism, is devised to conduct foreground segmentation, while
the two discriminators work together in an adversarial manner to ensure an
accurate segmentation of the novel categories under light supervision.
Exhaustive experiments demonstrate that, with only tens of labeled samples as
guidance, Trans-Net achieves close results on par with fully supervised
methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1"&gt;Zunlei Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1"&gt;Lechao Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xinchao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xiang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yajie Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Du_X/0/1/0/all/0/1"&gt;Xiangtong Du&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_M/0/1/0/all/0/1"&gt;Mingli Song&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Style Curriculum Learning for Robust Medical Image Segmentation. (arXiv:2108.00402v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2108.00402</id>
        <link href="http://arxiv.org/abs/2108.00402"/>
        <updated>2021-08-03T02:06:30.727Z</updated>
        <summary type="html"><![CDATA[The performance of deep segmentation models often degrades due to
distribution shifts in image intensities between the training and test data
sets. This is particularly pronounced in multi-centre studies involving data
acquired using multi-vendor scanners, with variations in acquisition protocols.
It is challenging to address this degradation because the shift is often not
known \textit{a priori} and hence difficult to model. We propose a novel
framework to ensure robust segmentation in the presence of such distribution
shifts. Our contribution is three-fold. First, inspired by the spirit of
curriculum learning, we design a novel style curriculum to train the
segmentation models using an easy-to-hard mode. A style transfer model with
style fusion is employed to generate the curriculum samples. Gradually focusing
on complex and adversarial style samples can significantly boost the robustness
of the models. Second, instead of subjectively defining the curriculum
complexity, we adopt an automated gradient manipulation method to control the
hard and adversarial sample generation process. Third, we propose the Local
Gradient Sign strategy to aggregate the gradient locally and stabilise training
during gradient manipulation. The proposed framework can generalise to unknown
distribution without using any target data. Extensive experiments on the public
M\&Ms Challenge dataset demonstrate that our proposed framework can generalise
deep models well to unknown distributions and achieve significant improvements
in segmentation accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhendong Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Manh_V/0/1/0/all/0/1"&gt;Van Manh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Huang_X/0/1/0/all/0/1"&gt;Xiaoqiong Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Lekadir_K/0/1/0/all/0/1"&gt;Karim Lekadir&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Campello_V/0/1/0/all/0/1"&gt;V&amp;#xed;ctor Campello&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ravikumar_N/0/1/0/all/0/1"&gt;Nishant Ravikumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Frangi_A/0/1/0/all/0/1"&gt;Alejandro F Frangi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Ni_D/0/1/0/all/0/1"&gt;Dong Ni&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Instance-level Spatial-Temporal Patterns for Person Re-identification. (arXiv:2108.00171v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00171</id>
        <link href="http://arxiv.org/abs/2108.00171"/>
        <updated>2021-08-03T02:06:30.720Z</updated>
        <summary type="html"><![CDATA[Person re-identification (Re-ID) aims to match pedestrians under dis-joint
cameras. Most Re-ID methods formulate it as visual representation learning and
image search, and its accuracy is consequently affected greatly by the search
space. Spatial-temporal information has been proven to be efficient to filter
irrelevant negative samples and significantly improve Re-ID accuracy. However,
existing spatial-temporal person Re-ID methods are still rough and do not
exploit spatial-temporal information sufficiently. In this paper, we propose a
novel Instance-level and Spatial-Temporal Disentangled Re-ID method (InSTD), to
improve Re-ID accuracy. In our proposed framework, personalized information
such as moving direction is explicitly considered to further narrow down the
search space. Besides, the spatial-temporal transferring probability is
disentangled from joint distribution to marginal distribution, so that outliers
can also be well modeled. Abundant experimental analyses are presented, which
demonstrates the superiority and provides more insights into our method. The
proposed method achieves mAP of 90.8% on Market-1501 and 89.1% on
DukeMTMC-reID, improving from the baseline 82.2% and 72.7%, respectively.
Besides, in order to provide a better benchmark for person re-identification,
we release a cleaned data list of DukeMTMC-reID with this paper:
https://github.com/RenMin1991/cleaned-DukeMTMC-reID/]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ren_M/0/1/0/all/0/1"&gt;Min Ren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1"&gt;Lingxiao He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liao_X/0/1/0/all/0/1"&gt;Xingyu Liao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wu Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yunlong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tan_T/0/1/0/all/0/1"&gt;Tieniu Tan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Applications of Artificial Neural Networks in Microorganism Image Analysis: A Comprehensive Review from Conventional Multilayer Perceptron to Popular Convolutional Neural Network and Potential Visual Transformer. (arXiv:2108.00358v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00358</id>
        <link href="http://arxiv.org/abs/2108.00358"/>
        <updated>2021-08-03T02:06:30.707Z</updated>
        <summary type="html"><![CDATA[Microorganisms are widely distributed in the human daily living environment.
They play an essential role in environmental pollution control, disease
prevention and treatment, and food and drug production. The identification,
counting, and detection are the basic steps for making full use of different
microorganisms. However, the conventional analysis methods are expensive,
laborious, and time-consuming. To overcome these limitations, artificial neural
networks are applied for microorganism image analysis. We conduct this review
to understand the development process of microorganism image analysis based on
artificial neural networks. In this review, the background and motivation are
introduced first. Then, the development of artificial neural networks and
representative networks are introduced. After that, the papers related to
microorganism image analysis based on classical and deep neural networks are
reviewed from the perspectives of different tasks. In the end, the methodology
analysis and potential direction are discussed.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1"&gt;Jinghua Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Grzegorzek_M/0/1/0/all/0/1"&gt;Marcin Grzegorzek&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning Few-shot Open-set Classifiers using Exemplar Reconstruction. (arXiv:2108.00340v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00340</id>
        <link href="http://arxiv.org/abs/2108.00340"/>
        <updated>2021-08-03T02:06:30.700Z</updated>
        <summary type="html"><![CDATA[We study the problem of how to identify samples from unseen categories
(open-set classification) when there are only a few samples given from the seen
categories (few-shot setting). The challenge of learning a good abstraction for
a class with very few samples makes it extremely difficult to detect samples
from the unseen categories; consequently, open-set recognition has received
minimal attention in the few-shot setting. Most open-set few-shot
classification methods regularize the softmax score to indicate uniform
probability for open class samples but we argue that this approach is often
inaccurate, especially at a fine-grained level. Instead, we propose a novel
exemplar reconstruction-based meta-learning strategy for jointly detecting open
class samples, as well as, categorizing samples from seen classes via
metric-based classification. The exemplars, which act as representatives of a
class, can either be provided in the training dataset or estimated in the
feature domain. Our framework, named Reconstructing Exemplar based Few-shot
Open-set ClaSsifier (ReFOCS), is tested on a wide variety of datasets and the
experimental results clearly highlight our method as the new state of the art.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Nag_S/0/1/0/all/0/1"&gt;Sayak Nag&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raychaudhuri_D/0/1/0/all/0/1"&gt;Dripta S. Raychaudhuri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paul_S/0/1/0/all/0/1"&gt;Sujoy Paul&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Roy_Chowdhury_A/0/1/0/all/0/1"&gt;Amit K. Roy-Chowdhury&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SyDog: A Synthetic Dog Dataset for Improved 2D Pose Estimation. (arXiv:2108.00249v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00249</id>
        <link href="http://arxiv.org/abs/2108.00249"/>
        <updated>2021-08-03T02:06:30.683Z</updated>
        <summary type="html"><![CDATA[Estimating the pose of animals can facilitate the understanding of animal
motion which is fundamental in disciplines such as biomechanics, neuroscience,
ethology, robotics and the entertainment industry. Human pose estimation models
have achieved high performance due to the huge amount of training data
available. Achieving the same results for animal pose estimation is challenging
due to the lack of animal pose datasets. To address this problem we introduce
SyDog: a synthetic dataset of dogs containing ground truth pose and bounding
box coordinates which was generated using the game engine, Unity. We
demonstrate that pose estimation models trained on SyDog achieve better
performance than models trained purely on real data and significantly reduce
the need for the labour intensive labelling of images. We release the SyDog
dataset as a training and evaluation benchmark for research in animal motion.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shooter_M/0/1/0/all/0/1"&gt;Moira Shooter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Malleson_C/0/1/0/all/0/1"&gt;Charles Malleson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hilton_A/0/1/0/all/0/1"&gt;Adrian Hilton&lt;/a&gt; (University of Surrey)</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Word2Pix: Word to Pixel Cross Attention Transformer in Visual Grounding. (arXiv:2108.00205v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00205</id>
        <link href="http://arxiv.org/abs/2108.00205"/>
        <updated>2021-08-03T02:06:30.675Z</updated>
        <summary type="html"><![CDATA[Current one-stage methods for visual grounding encode the language query as
one holistic sentence embedding before fusion with visual feature. Such a
formulation does not treat each word of a query sentence on par when modeling
language to visual attention, therefore prone to neglect words which are less
important for sentence embedding but critical for visual grounding. In this
paper we propose Word2Pix: a one-stage visual grounding network based on
encoder-decoder transformer architecture that enables learning for textual to
visual feature correspondence via word to pixel attention. The embedding of
each word from the query sentence is treated alike by attending to visual
pixels individually instead of single holistic sentence embedding. In this way,
each word is given equivalent opportunity to adjust the language to vision
attention towards the referent target through multiple stacks of transformer
decoder layers. We conduct the experiments on RefCOCO, RefCOCO+ and RefCOCOg
datasets and the proposed Word2Pix outperforms existing one-stage methods by a
notable margin. The results obtained also show that Word2Pix surpasses
two-stage visual grounding models, while at the same time keeping the merits of
one-stage paradigm namely end-to-end training and real-time inference speed
intact.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1"&gt;Heng Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Joey Tianyi Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ong_Y/0/1/0/all/0/1"&gt;Yew-Soon Ong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Knowing When to Quit: Selective Cascaded Regression with Patch Attention for Real-Time Face Alignment. (arXiv:2108.00377v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00377</id>
        <link href="http://arxiv.org/abs/2108.00377"/>
        <updated>2021-08-03T02:06:30.642Z</updated>
        <summary type="html"><![CDATA[Facial landmarks (FLM) estimation is a critical component in many
face-related applications. In this work, we aim to optimize for both accuracy
and speed and explore the trade-off between them. Our key observation is that
not all faces are created equal. Frontal faces with neutral expressions
converge faster than faces with extreme poses or expressions. To differentiate
among samples, we train our model to predict the regression error after each
iteration. If the current iteration is accurate enough, we stop iterating,
saving redundant iterations while keeping the accuracy in check. We also
observe that as neighboring patches overlap, we can infer all facial landmarks
(FLMs) with only a small number of patches without a major accuracy sacrifice.
Architecturally, we offer a multi-scale, patch-based, lightweight feature
extractor with a fine-grained local patch attention module, which computes a
patch weighting according to the information in the patch itself and enhances
the expressive power of the patch features. We analyze the patch attention data
to infer where the model is attending when regressing facial landmarks and
compare it to face attention in humans. Our model runs in real-time on a mobile
device GPU, with 95 Mega Multiply-Add (MMA) operations, outperforming all
state-of-the-art methods under 1000 MMA, with a normalized mean error of 8.16
on the 300W challenging dataset.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shapira_G/0/1/0/all/0/1"&gt;Gil Shapira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Levy_N/0/1/0/all/0/1"&gt;Noga Levy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goldin_I/0/1/0/all/0/1"&gt;Ishay Goldin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jevnisek_R/0/1/0/all/0/1"&gt;Roy J. Jevnisek&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BadEncoder: Backdoor Attacks to Pre-trained Encoders in Self-Supervised Learning. (arXiv:2108.00352v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2108.00352</id>
        <link href="http://arxiv.org/abs/2108.00352"/>
        <updated>2021-08-03T02:06:30.633Z</updated>
        <summary type="html"><![CDATA[Self-supervised learning in computer vision aims to pre-train an image
encoder using a large amount of unlabeled images or (image, text) pairs. The
pre-trained image encoder can then be used as a feature extractor to build
downstream classifiers for many downstream tasks with a small amount of or no
labeled training data. In this work, we propose BadEncoder, the first backdoor
attack to self-supervised learning. In particular, our BadEncoder injects
backdoors into a pre-trained image encoder such that the downstream classifiers
built based on the backdoored image encoder for different downstream tasks
simultaneously inherit the backdoor behavior. We formulate our BadEncoder as an
optimization problem and we propose a gradient descent based method to solve
it, which produces a backdoored image encoder from a clean one. Our extensive
empirical evaluation results on multiple datasets show that our BadEncoder
achieves high attack success rates while preserving the accuracy of the
downstream classifiers. We also show the effectiveness of BadEncoder using two
publicly available, real-world image encoders, i.e., Google's image encoder
pre-trained on ImageNet and OpenAI's Contrastive Language-Image Pre-training
(CLIP) image encoder pre-trained on 400 million (image, text) pairs collected
from the Internet. Moreover, we consider defenses including Neural Cleanse and
MNTD (empirical defenses) as well as PatchGuard (a provable defense). Our
results show that these defenses are insufficient to defend against BadEncoder,
highlighting the needs for new defenses against our BadEncoder. Our code is
publicly available at: https://github.com/jjy1994/BadEncoder.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1"&gt;Jinyuan Jia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yupei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gong_N/0/1/0/all/0/1"&gt;Neil Zhenqiang Gong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HiFT: Hierarchical Feature Transformer for Aerial Tracking. (arXiv:2108.00202v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00202</id>
        <link href="http://arxiv.org/abs/2108.00202"/>
        <updated>2021-08-03T02:06:30.623Z</updated>
        <summary type="html"><![CDATA[Most existing Siamese-based tracking methods execute the classification and
regression of the target object based on the similarity maps. However, they
either employ a single map from the last convolutional layer which degrades the
localization accuracy in complex scenarios or separately use multiple maps for
decision making, introducing intractable computations for aerial mobile
platforms. Thus, in this work, we propose an efficient and effective
hierarchical feature transformer (HiFT) for aerial tracking. Hierarchical
similarity maps generated by multi-level convolutional layers are fed into the
feature transformer to achieve the interactive fusion of spatial (shallow
layers) and semantics cues (deep layers). Consequently, not only the global
contextual information can be raised, facilitating the target search, but also
our end-to-end architecture with the transformer can efficiently learn the
interdependencies among multi-level features, thereby discovering a
tracking-tailored feature space with strong discriminability. Comprehensive
evaluations on four aerial benchmarks have proven the effectiveness of HiFT.
Real-world tests on the aerial platform have strongly validated its
practicability with a real-time speed. Our code is available at
https://github.com/vision4robotics/HiFT.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1"&gt;Ziang Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1"&gt;Changhong Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1"&gt;Junjie Ye&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1"&gt;Bowen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yiming Li&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Unlimited Neighborhood Interaction for Heterogeneous Trajectory Prediction. (arXiv:2108.00238v1 [cs.AI])]]></title>
        <id>http://arxiv.org/abs/2108.00238</id>
        <link href="http://arxiv.org/abs/2108.00238"/>
        <updated>2021-08-03T02:06:30.605Z</updated>
        <summary type="html"><![CDATA[Understanding complex social interactions among agents is a key challenge for
trajectory prediction. Most existing methods consider the interactions between
pairwise traffic agents or in a local area, while the nature of interactions is
unlimited, involving an uncertain number of agents and non-local areas
simultaneously. Besides, they only focus on homogeneous trajectory prediction,
namely those among agents of the same category, while neglecting people's
diverse reaction patterns toward traffic agents in different categories. To
address these problems, we propose a simple yet effective Unlimited
Neighborhood Interaction Network (UNIN), which predicts trajectories of
heterogeneous agents in multiply categories. Specifically, the proposed
unlimited neighborhood interaction module generates the fused-features of all
agents involved in an interaction simultaneously, which is adaptive to any
number of agents and any range of interaction area. Meanwhile, a hierarchical
graph attention module is proposed to obtain category-tocategory interaction
and agent-to-agent interaction. Finally, parameters of a Gaussian Mixture Model
are estimated for generating the future trajectories. Extensive experimental
results on benchmark datasets demonstrate a significant performance improvement
of our method over the state-ofthe-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_F/0/1/0/all/0/1"&gt;Fang Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1"&gt;Le Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1"&gt;Sanping Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_W/0/1/0/all/0/1"&gt;Wei Tang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niu_Z/0/1/0/all/0/1"&gt;Zhenxing Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_N/0/1/0/all/0/1"&gt;Nanning Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hua_G/0/1/0/all/0/1"&gt;Gang Hua&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ELLIPSDF: Joint Object Pose and Shape Optimization with a Bi-level Ellipsoid and Signed Distance Function Description. (arXiv:2108.00355v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00355</id>
        <link href="http://arxiv.org/abs/2108.00355"/>
        <updated>2021-08-03T02:06:30.599Z</updated>
        <summary type="html"><![CDATA[Autonomous systems need to understand the semantics and geometry of their
surroundings in order to comprehend and safely execute object-level task
specifications. This paper proposes an expressive yet compact model for joint
object pose and shape optimization, and an associated optimization algorithm to
infer an object-level map from multi-view RGB-D camera observations. The model
is expressive because it captures the identities, positions, orientations, and
shapes of objects in the environment. It is compact because it relies on a
low-dimensional latent representation of implicit object shape, allowing
onboard storage of large multi-category object maps. Different from other works
that rely on a single object representation format, our approach has a bi-level
object model that captures both the coarse level scale as well as the fine
level shape details. Our approach is evaluated on the large-scale real-world
ScanNet dataset and compared against state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shan_M/0/1/0/all/0/1"&gt;Mo Shan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_Q/0/1/0/all/0/1"&gt;Qiaojun Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jau_Y/0/1/0/all/0/1"&gt;You-Yi Jau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Atanasov_N/0/1/0/all/0/1"&gt;Nikolay Atanasov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self Context and Shape Prior for Sensorless Freehand 3D Ultrasound Reconstruction. (arXiv:2108.00274v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00274</id>
        <link href="http://arxiv.org/abs/2108.00274"/>
        <updated>2021-08-03T02:06:30.590Z</updated>
        <summary type="html"><![CDATA[3D ultrasound (US) is widely used for its rich diagnostic information.
However, it is criticized for its limited field of view. 3D freehand US
reconstruction is promising in addressing the problem by providing broad range
and freeform scan. The existing deep learning based methods only focus on the
basic cases of skill sequences, and the model relies on the training data
heavily. The sequences in real clinical practice are a mix of diverse skills
and have complex scanning paths. Besides, deep models should adapt themselves
to the testing cases with prior knowledge for better robustness, rather than
only fit to the training cases. In this paper, we propose a novel approach to
sensorless freehand 3D US reconstruction considering the complex skill
sequences. Our contribution is three-fold. First, we advance a novel online
learning framework by designing a differentiable reconstruction algorithm. It
realizes an end-to-end optimization from section sequences to the reconstructed
volume. Second, a self-supervised learning method is developed to explore the
context information that reconstructed by the testing data itself, promoting
the perception of the model. Third, inspired by the effectiveness of shape
prior, we also introduce adversarial training to strengthen the learning of
anatomical shape prior in the reconstructed volume. By mining the context and
structural cues of the testing data, our online learning methods can drive the
model to handle complex skill sequences. Experimental results on developmental
dysplasia of the hip US and fetal US datasets show that, our proposed method
can outperform the start-of-the-art methods regarding the shift errors and path
similarities.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Luo_M/0/1/0/all/0/1"&gt;Mingyuan Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1"&gt;Xin Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1"&gt;Xiaoqiong Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1"&gt;Yuhao Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1"&gt;Yuxin Zou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1"&gt;Xindi Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ravikumar_N/0/1/0/all/0/1"&gt;Nishant Ravikumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frangi_A/0/1/0/all/0/1"&gt;Alejandro F Frangi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ni_D/0/1/0/all/0/1"&gt;Dong Ni&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Delving into Deep Image Prior for Adversarial Defense: A Novel Reconstruction-based Defense Framework. (arXiv:2108.00180v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00180</id>
        <link href="http://arxiv.org/abs/2108.00180"/>
        <updated>2021-08-03T02:06:30.581Z</updated>
        <summary type="html"><![CDATA[Deep learning based image classification models are shown vulnerable to
adversarial attacks by injecting deliberately crafted noises to clean images.
To defend against adversarial attacks in a training-free and attack-agnostic
manner, this work proposes a novel and effective reconstruction-based defense
framework by delving into deep image prior (DIP). Fundamentally different from
existing reconstruction-based defenses, the proposed method analyzes and
explicitly incorporates the model decision process into our defense. Given an
adversarial image, firstly we map its reconstructed images during DIP
optimization to the model decision space, where cross-boundary images can be
detected and on-boundary images can be further localized. Then, adversarial
noise is purified by perturbing on-boundary images along the reverse direction
to the adversarial image. Finally, on-manifold images are stitched to construct
an image that can be correctly predicted by the victim classifier. Extensive
experiments demonstrate that the proposed method outperforms existing
state-of-the-art reconstruction-based methods both in defending white-box
attacks and defense-aware attacks. Moreover, the proposed method can maintain a
high visual quality during adversarial image reconstruction.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1"&gt;Li Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yongwei Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1"&gt;Xin Ding&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yuan_K/0/1/0/all/0/1"&gt;Kaiwen Yuan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1"&gt;Ping Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1"&gt;Hua Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1"&gt;Z. Jane Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LASOR: Learning Accurate 3D Human Pose and Shape Via Synthetic Occlusion-Aware Data and Neural Mesh Rendering. (arXiv:2108.00351v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00351</id>
        <link href="http://arxiv.org/abs/2108.00351"/>
        <updated>2021-08-03T02:06:30.573Z</updated>
        <summary type="html"><![CDATA[A key challenge in the task of human pose and shape estimation is occlusion,
including self-occlusions, object-human occlusions, and inter-person
occlusions. The lack of diverse and accurate pose and shape training data
becomes a major bottleneck, especially for scenes with occlusions in the wild.
In this paper, we focus on the estimation of human pose and shape in the case
of inter-person occlusions, while also handling object-human occlusions and
self-occlusion. We propose a framework that synthesizes occlusion-aware
silhouette and 2D keypoints data and directly regress to the SMPL pose and
shape parameters. A neural 3D mesh renderer is exploited to enable silhouette
supervision on the fly, which contributes to great improvements in shape
estimation. In addition, keypoints-and-silhouette-driven training data in
panoramic viewpoints are synthesized to compensate for the lack of viewpoint
diversity in any existing dataset. Experimental results show that we are among
state-of-the-art on the 3DPW dataset in terms of pose accuracy and evidently
outperform the rank-1 method in terms of shape accuracy. Top performance is
also achieved on SSP-3D in terms of shape prediction accuracy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1"&gt;Kaibing Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gu_R/0/1/0/all/0/1"&gt;Renshu Gu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Toyoura_M/0/1/0/all/0/1"&gt;Masahiro Toyoura&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1"&gt;Gang Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning with Noisy Labels via Sparse Regularization. (arXiv:2108.00192v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.00192</id>
        <link href="http://arxiv.org/abs/2108.00192"/>
        <updated>2021-08-03T02:06:30.550Z</updated>
        <summary type="html"><![CDATA[Learning with noisy labels is an important and challenging task for training
accurate deep neural networks. Some commonly-used loss functions, such as Cross
Entropy (CE), suffer from severe overfitting to noisy labels. Robust loss
functions that satisfy the symmetric condition were tailored to remedy this
problem, which however encounter the underfitting effect. In this paper, we
theoretically prove that \textbf{any loss can be made robust to noisy labels}
by restricting the network output to the set of permutations over a fixed
vector. When the fixed vector is one-hot, we only need to constrain the output
to be one-hot, which however produces zero gradients almost everywhere and thus
makes gradient-based optimization difficult. In this work, we introduce the
sparse regularization strategy to approximate the one-hot constraint, which is
composed of network output sharpening operation that enforces the output
distribution of a network to be sharp and the $\ell_p$-norm ($p\le 1$)
regularization that promotes the network output to be sparse. This simple
approach guarantees the robustness of arbitrary loss functions while not
hindering the fitting ability. Experimental results demonstrate that our method
can significantly improve the performance of commonly-used loss functions in
the presence of noisy labels and class imbalance, and outperform the
state-of-the-art methods. The code is available at
https://github.com/hitcszx/lnl_sr.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1"&gt;Xiong Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xianming Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Chenyang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhai_D/0/1/0/all/0/1"&gt;Deming Zhai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1"&gt;Junjun Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_X/0/1/0/all/0/1"&gt;Xiangyang Ji&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Adversarially Robust and Domain Generalizable Stereo Matching by Rethinking DNN Feature Backbones. (arXiv:2108.00335v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00335</id>
        <link href="http://arxiv.org/abs/2108.00335"/>
        <updated>2021-08-03T02:06:30.542Z</updated>
        <summary type="html"><![CDATA[Stereo matching has recently witnessed remarkable progress using Deep Neural
Networks (DNNs). But, how robust are they? Although it has been well-known that
DNNs often suffer from adversarial vulnerability with a catastrophic drop in
performance, the situation is even worse in stereo matching. This paper first
shows that a type of weak white-box attacks can fail state-of-the-art methods.
The attack is learned by a proposed stereo-constrained projected gradient
descent (PGD) method in stereo matching. This observation raises serious
concerns for the deployment of DNN-based stereo matching. Parallel to the
adversarial vulnerability, DNN-based stereo matching is typically trained under
the so-called simulation to reality pipeline, and thus domain generalizability
is an important problem. This paper proposes to rethink the learnable DNN-based
feature backbone towards adversarially-robust and domain generalizable stereo
matching, either by completely removing it or by applying it only to the left
reference image. It computes the matching cost volume using the classic
multi-scale census transform (i.e., local binary pattern) of the raw input
stereo images, followed by a stacked Hourglass head sub-network solving the
matching problem. In experiments, the proposed method is tested in the
SceneFlow dataset and the KITTI2015 benchmark. It significantly improves the
adversarial robustness, while retaining accuracy performance comparable to
state-of-the-art methods. It also shows better generalizability from simulation
(SceneFlow) to real (KITTI) datasets when no fine-tuning is used.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Cheng_K/0/1/0/all/0/1"&gt;Kelvin Cheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Healey_C/0/1/0/all/0/1"&gt;Christopher Healey&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1"&gt;Tianfu Wu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HR-Crime: Human-Related Anomaly Detection in Surveillance Videos. (arXiv:2108.00246v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00246</id>
        <link href="http://arxiv.org/abs/2108.00246"/>
        <updated>2021-08-03T02:06:30.533Z</updated>
        <summary type="html"><![CDATA[The automatic detection of anomalies captured by surveillance settings is
essential for speeding the otherwise laborious approach. To date, UCF-Crime is
the largest available dataset for automatic visual analysis of anomalies and
consists of real-world crime scenes of various categories. In this paper, we
introduce HR-Crime, a subset of the UCF-Crime dataset suitable for
human-related anomaly detection tasks. We rely on state-of-the-art techniques
to build the feature extraction pipeline for human-related anomaly detection.
Furthermore, we present the baseline anomaly detection analysis on the
HR-Crime. HR-Crime as well as the developed feature extraction pipeline and the
extracted features will be publicly available for further research in the
field.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Boekhoudt_K/0/1/0/all/0/1"&gt;Kayleigh Boekhoudt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Matei_A/0/1/0/all/0/1"&gt;Alina Matei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Aghaei_M/0/1/0/all/0/1"&gt;Maya Aghaei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Talavera_E/0/1/0/all/0/1"&gt;Estefan&amp;#xed;a Talavera&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards explainable artificial intelligence (XAI) for early anticipation of traffic accidents. (arXiv:2108.00273v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00273</id>
        <link href="http://arxiv.org/abs/2108.00273"/>
        <updated>2021-08-03T02:06:30.522Z</updated>
        <summary type="html"><![CDATA[Traffic accident anticipation is a vital function of Automated Driving
Systems (ADSs) for providing a safety-guaranteed driving experience. An
accident anticipation model aims to predict accidents promptly and accurately
before they occur. Existing Artificial Intelligence (AI) models of accident
anticipation lack a human-interpretable explanation of their decision-making.
Although these models perform well, they remain a black-box to the ADS users,
thus difficult to get their trust. To this end, this paper presents a Gated
Recurrent Unit (GRU) network that learns spatio-temporal relational features
for the early anticipation of traffic accidents from dashcam video data. A
post-hoc attention mechanism named Grad-CAM is integrated into the network to
generate saliency maps as the visual explanation of the accident anticipation
decision. An eye tracker captures human eye fixation points for generating
human attention maps. The explainability of network-generated saliency maps is
evaluated in comparison to human attention maps. Qualitative and quantitative
results on a public crash dataset confirm that the proposed explainable network
can anticipate an accident on average 4.57 seconds before it occurs, with
94.02% average precision. In further, various post-hoc attention-based XAI
methods are evaluated and compared. It confirms that the Grad-CAM chosen by
this study can generate high-quality, human-interpretable saliency maps (with
1.42 Normalized Scanpath Saliency) for explaining the crash anticipation
decision. Importantly, results confirm that the proposed AI model, with a
human-inspired design, can outperform humans in the accident anticipation.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Karim_M/0/1/0/all/0/1"&gt;Muhammad Monjurul Karim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yu Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qin_R/0/1/0/all/0/1"&gt;Ruwen Qin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-scale Matching Networks for Semantic Correspondence. (arXiv:2108.00211v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00211</id>
        <link href="http://arxiv.org/abs/2108.00211"/>
        <updated>2021-08-03T02:06:30.514Z</updated>
        <summary type="html"><![CDATA[Deep features have been proven powerful in building accurate dense semantic
correspondences in various previous works. However, the multi-scale and
pyramidal hierarchy of convolutional neural networks has not been well studied
to learn discriminative pixel-level features for semantic correspondence. In
this paper, we propose a multi-scale matching network that is sensitive to tiny
semantic differences between neighboring pixels. We follow the coarse-to-fine
matching strategy and build a top-down feature and matching enhancement scheme
that is coupled with the multi-scale hierarchy of deep convolutional neural
networks. During feature enhancement, intra-scale enhancement fuses
same-resolution feature maps from multiple layers together via local
self-attention and cross-scale enhancement hallucinates higher-resolution
feature maps along the top-down hierarchy. Besides, we learn complementary
matching details at different scales thus the overall matching score is refined
by features of different semantic levels gradually. Our multi-scale matching
network can be trained end-to-end easily with few additional learnable
parameters. Experimental results demonstrate that the proposed method achieves
state-of-the-art performance on three popular benchmarks with high
computational efficiency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1"&gt;Dongyang Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1"&gt;Ziyang Song&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ji_Z/0/1/0/all/0/1"&gt;Zhenghao Ji&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1"&gt;Gangming Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ge_W/0/1/0/all/0/1"&gt;Weifeng Ge&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1"&gt;Yizhou Yu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[CrossFormer: A Versatile Vision Transformer Based on Cross-scale Attention. (arXiv:2108.00154v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00154</id>
        <link href="http://arxiv.org/abs/2108.00154"/>
        <updated>2021-08-03T02:06:30.495Z</updated>
        <summary type="html"><![CDATA[Transformers have made much progress in dealing with visual tasks. However,
existing vision transformers still do not possess an ability that is important
to visual input: building the attention among features of different scales. The
reasons for this problem are two-fold: (1) Input embeddings of each layer are
equal-scale without cross-scale features; (2) Some vision transformers
sacrifice the small-scale features of embeddings to lower the cost of the
self-attention module. To make up this defect, we propose Cross-scale Embedding
Layer (CEL) and Long Short Distance Attention (LSDA). In particular, CEL blends
each embedding with multiple patches of different scales, providing the model
with cross-scale embeddings. LSDA splits the self-attention module into a
short-distance and long-distance one, also lowering the cost but keeping both
small-scale and large-scale features in embeddings. Through these two designs,
we achieve cross-scale attention. Besides, we propose dynamic position bias for
vision transformers to make the popular relative position bias apply to
variable-sized images. Based on these proposed modules, we construct our vision
architecture called CrossFormer. Experiments show that CrossFormer outperforms
other transformers on several representative visual tasks, especially object
detection and segmentation. The code has been released:
https://github.com/cheerss/CrossFormer.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1"&gt;Wenxiao Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_L/0/1/0/all/0/1"&gt;Lu Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1"&gt;Long Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1"&gt;Deng Cai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1"&gt;Xiaofei He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1"&gt;Wei Liu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[On The State of Data In Computer Vision: Human Annotations Remain Indispensable for Developing Deep Learning Models. (arXiv:2108.00114v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00114</id>
        <link href="http://arxiv.org/abs/2108.00114"/>
        <updated>2021-08-03T02:06:30.486Z</updated>
        <summary type="html"><![CDATA[High-quality labeled datasets play a crucial role in fueling the development
of machine learning (ML), and in particular the development of deep learning
(DL). However, since the emergence of the ImageNet dataset and the AlexNet
model in 2012, the size of new open-source labeled vision datasets has remained
roughly constant. Consequently, only a minority of publications in the computer
vision community tackle supervised learning on datasets that are orders of
magnitude larger than Imagenet. In this paper, we survey computer vision
research domains that study the effects of such large datasets on model
performance across different vision tasks. We summarize the community's current
understanding of those effects, and highlight some open questions related to
training with massive datasets. In particular, we tackle: (a) The largest
datasets currently used in computer vision research and the interesting
takeaways from training on such datasets; (b) The effectiveness of pre-training
on large datasets; (c) Recent advancements and hurdles facing synthetic
datasets; (d) An overview of double descent and sample non-monotonicity
phenomena; and finally, (e) A brief discussion of lifelong/continual learning
and how it fares compared to learning from huge labeled datasets in an offline
setting. Overall, our findings are that research on optimization for deep
learning focuses on perfecting the training routine and thus making DL models
less data hungry, while research on synthetic datasets aims to offset the cost
of data labeling. However, for the time being, acquiring non-synthetic labeled
data remains indispensable to boost performance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Emam_Z/0/1/0/all/0/1"&gt;Zeyad Emam&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kondrich_A/0/1/0/all/0/1"&gt;Andrew Kondrich&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Harrison_S/0/1/0/all/0/1"&gt;Sasha Harrison&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lau_F/0/1/0/all/0/1"&gt;Felix Lau&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yushi Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kim_A/0/1/0/all/0/1"&gt;Aerin Kim&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Branson_E/0/1/0/all/0/1"&gt;Elliot Branson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Margin-Aware Intra-Class Novelty Identification for Medical Images. (arXiv:2108.00117v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00117</id>
        <link href="http://arxiv.org/abs/2108.00117"/>
        <updated>2021-08-03T02:06:30.477Z</updated>
        <summary type="html"><![CDATA[Traditional anomaly detection methods focus on detecting inter-class
variations while medical image novelty identification is inherently an
intra-class detection problem. For example, a machine learning model trained
with normal chest X-ray and common lung abnormalities, is expected to discover
and flag idiopathic pulmonary fibrosis which a rare lung disease and unseen by
the model during training. The nuances from intra-class variations and lack of
relevant training data in medical image analysis pose great challenges for
existing anomaly detection methods. To tackle the challenges, we propose a
hybrid model - Transformation-based Embedding learning for Novelty Detection
(TEND) which without any out-of-distribution training data, performs novelty
identification by combining both autoencoder-based and classifier-based method.
With a pre-trained autoencoder as image feature extractor, TEND learns to
discriminate the feature embeddings of in-distribution data from the
transformed counterparts as fake out-of-distribution inputs. To enhance the
separation, a distance objective is optimized to enforce a margin between the
two classes. Extensive experimental results on both natural image datasets and
medical image datasets are presented and our method out-performs
state-of-the-art approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1"&gt;Xiaoyuan Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gichoya_J/0/1/0/all/0/1"&gt;Judy Wawira Gichoya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Purkayastha_S/0/1/0/all/0/1"&gt;Saptarshi Purkayastha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Banerjee_I/0/1/0/all/0/1"&gt;Imon Banerjee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pose-Guided Feature Learning with Knowledge Distillation for Occluded Person Re-Identification. (arXiv:2108.00139v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00139</id>
        <link href="http://arxiv.org/abs/2108.00139"/>
        <updated>2021-08-03T02:06:30.442Z</updated>
        <summary type="html"><![CDATA[Occluded person re-identification (ReID) aims to match person images with
occlusion. It is fundamentally challenging because of the serious occlusion
which aggravates the misalignment problem between images. At the cost of
incorporating a pose estimator, many works introduce pose information to
alleviate the misalignment in both training and testing. To achieve high
accuracy while preserving low inference complexity, we propose a network named
Pose-Guided Feature Learning with Knowledge Distillation (PGFL-KD), where the
pose information is exploited to regularize the learning of semantics aligned
features but is discarded in testing. PGFL-KD consists of a main branch (MB),
and two pose-guided branches, \ieno, a foreground-enhanced branch (FEB), and a
body part semantics aligned branch (SAB). The FEB intends to emphasise the
features of visible body parts while excluding the interference of obstructions
and background (\ieno, foreground feature alignment). The SAB encourages
different channel groups to focus on different body parts to have body part
semantics aligned representation. To get rid of the dependency on pose
information when testing, we regularize the MB to learn the merits of the FEB
and SAB through knowledge distillation and interaction-based training.
Extensive experiments on occluded, partial, and holistic ReID tasks show the
effectiveness of our proposed network.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_K/0/1/0/all/0/1"&gt;Kecheng Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lan_C/0/1/0/all/0/1"&gt;Cuiling Lan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1"&gt;Wenjun Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jiawei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhizheng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zha_Z/0/1/0/all/0/1"&gt;Zheng-Jun Zha&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Conditional Bures Metric for Domain Adaptation. (arXiv:2108.00302v1 [cs.LG])]]></title>
        <id>http://arxiv.org/abs/2108.00302</id>
        <link href="http://arxiv.org/abs/2108.00302"/>
        <updated>2021-08-03T02:06:30.434Z</updated>
        <summary type="html"><![CDATA[As a vital problem in classification-oriented transfer, unsupervised domain
adaptation (UDA) has attracted widespread attention in recent years. Previous
UDA methods assume the marginal distributions of different domains are shifted
while ignoring the discriminant information in the label distributions. This
leads to classification performance degeneration in real applications. In this
work, we focus on the conditional distribution shift problem which is of great
concern to current conditional invariant models. We aim to seek a kernel
covariance embedding for conditional distribution which remains yet unexplored.
Theoretically, we propose the Conditional Kernel Bures (CKB) metric for
characterizing conditional distribution discrepancy, and derive an empirical
estimation for the CKB metric without introducing the implicit kernel feature
map. It provides an interpretable approach to understand the knowledge transfer
mechanism. The established consistency theory of the empirical estimation
provides a theoretical guarantee for convergence. A conditional distribution
matching network is proposed to learn the conditional invariant and
discriminative features for UDA. Extensive experiments and analysis show the
superiority of our proposed model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1"&gt;You-Wei Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ren_C/0/1/0/all/0/1"&gt;Chuan-Xian Ren&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Dynamic 3D Spontaneous Micro-expression Database: Establishment and Evaluation. (arXiv:2108.00166v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00166</id>
        <link href="http://arxiv.org/abs/2108.00166"/>
        <updated>2021-08-03T02:06:30.427Z</updated>
        <summary type="html"><![CDATA[Micro-expressions are spontaneous, unconscious facial movements that show
people's true inner emotions and have great potential in related fields of
psychological testing. Since the face is a 3D deformation object, the
occurrence of an expression can arouse spatial deformation of the face, but
limited by the available databases are 2D videos, which lack the description of
3D spatial information of micro-expressions. Therefore, we proposed a new
micro-expression database containing 2D video sequences and 3D point clouds
sequences. The database includes 259 micro-expressions sequences, and these
samples were classified using the objective method based on facial action
coding system, as well as the non-objective method that combines video contents
and participants' self-reports. We extracted facial 2D and 3D features using
local binary patterns on three orthogonal planes and curvature descriptors,
respectively, and performed baseline evaluations of the two features and their
fusion results with leave-one-subject-out(LOSO) and 10-fold cross-validation
methods. The best fusion performances were 58.84% and 73.03% for non-objective
classification and 66.36% and 77.42% for objective classification, both of
which have improved performance compared to using LBP-TOP features only.The
database offers original and cropped micro-expression samples, which will
facilitate the exploration and research on 3D Spatio-temporal features of
micro-expressions.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1"&gt;Fengping Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1"&gt;Jie Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qi_C/0/1/0/all/0/1"&gt;Chun Qi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yun Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Miao_D/0/1/0/all/0/1"&gt;Danmin Miao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[M2Lens: Visualizing and Explaining Multimodal Models for Sentiment Analysis. (arXiv:2107.08264v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.08264</id>
        <link href="http://arxiv.org/abs/2107.08264"/>
        <updated>2021-08-03T02:06:30.405Z</updated>
        <summary type="html"><![CDATA[Multimodal sentiment analysis aims to recognize people's attitudes from
multiple communication channels such as verbal content (i.e., text), voice, and
facial expressions. It has become a vibrant and important research topic in
natural language processing. Much research focuses on modeling the complex
intra- and inter-modal interactions between different communication channels.
However, current multimodal models with strong performance are often
deep-learning-based techniques and work like black boxes. It is not clear how
models utilize multimodal information for sentiment predictions. Despite recent
advances in techniques for enhancing the explainability of machine learning
models, they often target unimodal scenarios (e.g., images, sentences), and
little research has been done on explaining multimodal models. In this paper,
we present an interactive visual analytics system, M2Lens, to visualize and
explain multimodal models for sentiment analysis. M2Lens provides explanations
on intra- and inter-modal interactions at the global, subset, and local levels.
Specifically, it summarizes the influence of three typical interaction types
(i.e., dominance, complement, and conflict) on the model predictions. Moreover,
M2Lens identifies frequent and influential multimodal features and supports the
multi-faceted exploration of model behaviors from language, acoustic, and
visual modalities. Through two case studies and expert interviews, we
demonstrate our system can help users gain deep insights into the multimodal
models for sentiment analysis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xingbo Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1"&gt;Jianben He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1"&gt;Zhihua Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1"&gt;Muqiao Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qu_H/0/1/0/all/0/1"&gt;Huamin Qu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Thermal Image Super-Resolution Using Second-Order Channel Attention with Varying Receptive Fields. (arXiv:2108.00094v1 [eess.IV])]]></title>
        <id>http://arxiv.org/abs/2108.00094</id>
        <link href="http://arxiv.org/abs/2108.00094"/>
        <updated>2021-08-03T02:06:30.395Z</updated>
        <summary type="html"><![CDATA[Thermal images model the long-infrared range of the electromagnetic spectrum
and provide meaningful information even when there is no visible illumination.
Yet, unlike imagery that represents radiation from the visible continuum,
infrared images are inherently low-resolution due to hardware constraints. The
restoration of thermal images is critical for applications that involve safety,
search and rescue, and military operations. In this paper, we introduce a
system to efficiently reconstruct thermal images. Specifically, we explore how
to effectively attend to contrasting receptive fields (RFs) where increasing
the RFs of a network can be computationally expensive. For this purpose, we
introduce a deep attention to varying receptive fields network (AVRFN). We
supply a gated convolutional layer with higher-order information extracted from
disparate RFs, whereby an RF is parameterized by a dilation rate. In this way,
the dilation rate can be tuned to use fewer parameters thus increasing the
efficacy of AVRFN. Our experimental results show an improvement over the state
of the art when compared against competing thermal image super-resolution
methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/eess/1/au:+Gutierrez_N/0/1/0/all/0/1"&gt;Nolan B. Gutierrez&lt;/a&gt;, &lt;a href="http://arxiv.org/find/eess/1/au:+Beksi_W/0/1/0/all/0/1"&gt;William J. Beksi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[T$_k$ML-AP: Adversarial Attacks to Top-$k$ Multi-Label Learning. (arXiv:2108.00146v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00146</id>
        <link href="http://arxiv.org/abs/2108.00146"/>
        <updated>2021-08-03T02:06:30.388Z</updated>
        <summary type="html"><![CDATA[Top-$k$ multi-label learning, which returns the top-$k$ predicted labels from
an input, has many practical applications such as image annotation, document
analysis, and web search engine. However, the vulnerabilities of such
algorithms with regards to dedicated adversarial perturbation attacks have not
been extensively studied previously. In this work, we develop methods to create
adversarial perturbations that can be used to attack top-$k$ multi-label
learning-based image annotation systems (TkML-AP). Our methods explicitly
consider the top-$k$ ranking relation and are based on novel loss functions.
Experimental evaluations on large-scale benchmark datasets including PASCAL VOC
and MS COCO demonstrate the effectiveness of our methods in reducing the
performance of state-of-the-art top-$k$ multi-label learning methods, under
both untargeted and targeted attacks.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1"&gt;Shu Hu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ke_L/0/1/0/all/0/1"&gt;Lipeng Ke&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xin Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lyu_S/0/1/0/all/0/1"&gt;Siwei Lyu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multiplex Graph Networks for Multimodal Brain Network Analysis. (arXiv:2108.00158v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00158</id>
        <link href="http://arxiv.org/abs/2108.00158"/>
        <updated>2021-08-03T02:06:30.381Z</updated>
        <summary type="html"><![CDATA[In this paper, we propose MGNet, a simple and effective multiplex graph
convolutional network (GCN) model for multimodal brain network analysis. The
proposed method integrates tensor representation into the multiplex GCN model
to extract the latent structures of a set of multimodal brain networks, which
allows an intuitive 'grasping' of the common space for multimodal data.
Multimodal representations are then generated with multiplex GCNs to capture
specific graph structures. We conduct classification task on two challenging
real-world datasets (HIV and Bipolar disorder), and the proposed MGNet
demonstrates state-of-the-art performance compared to competitive benchmark
methods. Apart from objective evaluations, this study may bear special
significance upon network theory to the understanding of human connectome in
different modalities. The code is available at
https://github.com/ZhaomingKong/MGNets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kong_Z/0/1/0/all/0/1"&gt;Zhaoming Kong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1"&gt;Lichao Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1"&gt;Hao Peng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhan_L/0/1/0/all/0/1"&gt;Liang Zhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yong Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1"&gt;Lifang He&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scene Inference for Object Illumination Editing. (arXiv:2108.00150v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00150</id>
        <link href="http://arxiv.org/abs/2108.00150"/>
        <updated>2021-08-03T02:06:30.360Z</updated>
        <summary type="html"><![CDATA[The seamless illumination integration between a foreground object and a
background scene is an important but challenging task in computer vision and
augmented reality community. However, to our knowledge, there is no publicly
available high-quality dataset that meets the illumination seamless integration
task, which greatly hinders the development of this research direction. To this
end, we apply a physically-based rendering method to create a large-scale,
high-quality dataset, named IH dataset, which provides rich illumination
information for seamless illumination integration task. In addition, we propose
a deep learning-based SI-GAN method, a multi-task collaborative network, which
makes full use of the multi-scale attention mechanism and adversarial learning
strategy to directly infer mapping relationship between the inserted foreground
object and corresponding background environment, and edit object illumination
according to the proposed illumination exchange mechanism in parallel network.
By this means, we can achieve the seamless illumination integration without
explicit estimation of 3D geometric information. Comprehensive experiments on
both our dataset and real-world images collected from the Internet show that
our proposed SI-GAN provides a practical and effective solution for image-based
object illumination editing, and validate the superiority of our method against
state-of-the-art methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bao_Z/0/1/0/all/0/1"&gt;Zhongyun Bao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Long_C/0/1/0/all/0/1"&gt;Chengjiang Long&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fu_G/0/1/0/all/0/1"&gt;Gang Fu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1"&gt;Daquan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1"&gt;Yuanzhen Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1"&gt;Jiaming Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1"&gt;Chunxia Xiao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MTVR: Multilingual Moment Retrieval in Videos. (arXiv:2108.00061v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.00061</id>
        <link href="http://arxiv.org/abs/2108.00061"/>
        <updated>2021-08-03T02:06:30.354Z</updated>
        <summary type="html"><![CDATA[We introduce mTVR, a large-scale multilingual video moment retrieval dataset,
containing 218K English and Chinese queries from 21.8K TV show video clips. The
dataset is collected by extending the popular TVR dataset (in English) with
paired Chinese queries and subtitles. Compared to existing moment retrieval
datasets, mTVR is multilingual, larger, and comes with diverse annotations. We
further propose mXML, a multilingual moment retrieval model that learns and
operates on data from both languages, via encoder parameter sharing and
language neighborhood constraints. We demonstrate the effectiveness of mXML on
the newly collected MTVR dataset, where mXML outperforms strong monolingual
baselines while using fewer parameters. In addition, we also provide detailed
dataset analyses and model ablations. Data and code are publicly available at
https://github.com/jayleicn/mTVRetrieval]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1"&gt;Jie Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berg_T/0/1/0/all/0/1"&gt;Tamara L. Berg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1"&gt;Mohit Bansal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Object-aware Contrastive Learning for Debiased Scene Representation. (arXiv:2108.00049v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00049</id>
        <link href="http://arxiv.org/abs/2108.00049"/>
        <updated>2021-08-03T02:06:30.347Z</updated>
        <summary type="html"><![CDATA[Contrastive self-supervised learning has shown impressive results in learning
visual representations from unlabeled images by enforcing invariance against
different data augmentations. However, the learned representations are often
contextually biased to the spurious scene correlations of different objects or
object and background, which may harm their generalization on the downstream
tasks. To tackle the issue, we develop a novel object-aware contrastive
learning framework that first (a) localizes objects in a self-supervised manner
and then (b) debias scene correlations via appropriate data augmentations
considering the inferred object locations. For (a), we propose the contrastive
class activation map (ContraCAM), which finds the most discriminative regions
(e.g., objects) in the image compared to the other images using the
contrastively trained models. We further improve the ContraCAM to detect
multiple objects and entire shapes via an iterative refinement procedure. For
(b), we introduce two data augmentations based on ContraCAM, object-aware
random crop and background mixup, which reduce contextual and background biases
during contrastive self-supervised learning, respectively. Our experiments
demonstrate the effectiveness of our representation learning framework,
particularly when trained under multi-object images or evaluated under the
background (and distribution) shifted images.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mo_S/0/1/0/all/0/1"&gt;Sangwoo Mo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kang_H/0/1/0/all/0/1"&gt;Hyunwoo Kang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sohn_K/0/1/0/all/0/1"&gt;Kihyuk Sohn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1"&gt;Chun-Liang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1"&gt;Jinwoo Shin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MultiCite: Modeling realistic citations requires moving beyond the single-sentence single-label setting. (arXiv:2107.00414v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.00414</id>
        <link href="http://arxiv.org/abs/2107.00414"/>
        <updated>2021-08-03T02:06:30.299Z</updated>
        <summary type="html"><![CDATA[Citation context analysis (CCA) is an important task in natural language
processing that studies how and why scholars discuss each others' work. Despite
decades of study, traditional frameworks for CCA have largely relied on
overly-simplistic assumptions of how authors cite, which ignore several
important phenomena. For instance, scholarly papers often contain rich
discussions of cited work that span multiple sentences and express multiple
intents concurrently. Yet, CCA is typically approached as a single-sentence,
single-label classification task, and thus existing datasets fail to capture
this interesting discourse. In our work, we address this research gap by
proposing a novel framework for CCA as a document-level context extraction and
labeling task. We release MultiCite, a new dataset of 12,653 citation contexts
from over 1,200 computational linguistics papers. Not only is it the largest
collection of expert-annotated citation contexts to-date, MultiCite contains
multi-sentence, multi-label citation contexts within full paper texts. Finally,
we demonstrate how our dataset, while still usable for training classic CCA
models, also supports the development of new types of models for CCA beyond
fixed-width text classification. We release our code and dataset at
https://github.com/allenai/multicite.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lauscher_A/0/1/0/all/0/1"&gt;Anne Lauscher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ko_B/0/1/0/all/0/1"&gt;Brandon Ko&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kuehl_B/0/1/0/all/0/1"&gt;Bailey Kuehl&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Johnson_S/0/1/0/all/0/1"&gt;Sophie Johnson&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jurgens_D/0/1/0/all/0/1"&gt;David Jurgens&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cohan_A/0/1/0/all/0/1"&gt;Arman Cohan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lo_K/0/1/0/all/0/1"&gt;Kyle Lo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Exemplars-guided Empathetic Response Generation Controlled by the Elements of Human Communication. (arXiv:2106.11791v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.11791</id>
        <link href="http://arxiv.org/abs/2106.11791"/>
        <updated>2021-08-03T02:06:30.292Z</updated>
        <summary type="html"><![CDATA[The majority of existing methods for empathetic response generation rely on
the emotion of the context to generate empathetic responses. However, empathy
is much more than generating responses with an appropriate emotion. It also
often entails subtle expressions of understanding and personal resonance with
the situation of the other interlocutor. Unfortunately, such qualities are
difficult to quantify and the datasets lack the relevant annotations. To
address this issue, in this paper we propose an approach that relies on
exemplars to cue the generative model on fine stylistic properties that signal
empathy to the interlocutor. To this end, we employ dense passage retrieval to
extract relevant exemplary responses from the training set. Three elements of
human communication -- emotional presence, interpretation, and exploration, and
sentiment are additionally introduced using synthetic labels to guide the
generation towards empathy. The human evaluation is also extended by these
elements of human communication. We empirically show that these approaches
yield significant improvements in empathetic response quality in terms of both
automated and human-evaluated metrics. The implementation is available at
https://github.com/declare-lab/exemplary-empathy.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Majumder_N/0/1/0/all/0/1"&gt;Navonil Majumder&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghosal_D/0/1/0/all/0/1"&gt;Deepanway Ghosal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hazarika_D/0/1/0/all/0/1"&gt;Devamanyu Hazarika&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gelbukh_A/0/1/0/all/0/1"&gt;Alexander Gelbukh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mihalcea_R/0/1/0/all/0/1"&gt;Rada Mihalcea&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Poria_S/0/1/0/all/0/1"&gt;Soujanya Poria&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A New Semi-supervised Learning Benchmark for Classifying View and Diagnosing Aortic Stenosis from Echocardiograms. (arXiv:2108.00080v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00080</id>
        <link href="http://arxiv.org/abs/2108.00080"/>
        <updated>2021-08-03T02:06:30.267Z</updated>
        <summary type="html"><![CDATA[Semi-supervised image classification has shown substantial progress in
learning from limited labeled data, but recent advances remain largely untested
for clinical applications. Motivated by the urgent need to improve timely
diagnosis of life-threatening heart conditions, especially aortic stenosis, we
develop a benchmark dataset to assess semi-supervised approaches to two tasks
relevant to cardiac ultrasound (echocardiogram) interpretation: view
classification and disease severity classification. We find that a
state-of-the-art method called MixMatch achieves promising gains in heldout
accuracy on both tasks, learning from a large volume of truly unlabeled images
as well as a labeled set collected at great expense to achieve better
performance than is possible with the labeled set alone. We further pursue
patient-level diagnosis prediction, which requires aggregating across hundreds
of images of diverse view types, most of which are irrelevant, to make a
coherent prediction. The best patient-level performance is achieved by new
methods that prioritize diagnosis predictions from images that are predicted to
be clinically-relevant views and transfer knowledge from the view task to the
diagnosis task. We hope our released Tufts Medical Echocardiogram Dataset and
evaluation framework inspire further improvements in multi-task semi-supervised
learning for clinical applications.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1"&gt;Zhe Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Long_G/0/1/0/all/0/1"&gt;Gary Long&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wessler_B/0/1/0/all/0/1"&gt;Benjamin Wessler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hughes_M/0/1/0/all/0/1"&gt;Michael C. Hughes&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Manifold-Inspired Single Image Interpolation. (arXiv:2108.00145v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00145</id>
        <link href="http://arxiv.org/abs/2108.00145"/>
        <updated>2021-08-03T02:06:30.254Z</updated>
        <summary type="html"><![CDATA[Manifold models consider natural-image patches to be on a low-dimensional
manifold embedded in a high dimensional state space and each patch and its
similar patches to approximately lie on a linear affine subspace. Manifold
models are closely related to semi-local similarity, a well-known property of
natural images, referring to that for most natural-image patches, several
similar patches can be found in its spatial neighborhood. Many approaches to
single image interpolation use manifold models to exploit semi-local similarity
by two mutually exclusive parts: i) searching each target patch's similar
patches and ii) operating on the searched similar patches, the target patch and
the measured input pixels to estimate the target patch. Unfortunately, aliasing
in the input image makes it challenging for both parts. A very few works
explicitly deal with those challenges and only ad-hoc solutions are proposed.

To overcome the challenge in the first part, we propose a carefully-designed
adaptive technique to remove aliasing in severely aliased regions, which cannot
be removed from traditional techniques. This technique enables reliable
identification of similar patches even in the presence of strong aliasing. To
overcome the challenge in the second part, we propose to use the
aliasing-removed image to guide the initialization of the interpolated image
and develop a progressive scheme to refine the interpolated image based on
manifold models. Experimental results demonstrate that our approach
reconstructs edges with both smoothness along contours and sharpness across
profiles, and achieves an average Peak Signal-to-Noise Ratio (PSNR)
significantly higher than existing model-based approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1"&gt;Lantao Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1"&gt;Kuida Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Orchard_M/0/1/0/all/0/1"&gt;Michael T. Orchard&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Comparing object recognition in humans and deep convolutional neural networks -- An eye tracking study. (arXiv:2108.00107v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00107</id>
        <link href="http://arxiv.org/abs/2108.00107"/>
        <updated>2021-08-03T02:06:30.233Z</updated>
        <summary type="html"><![CDATA[Deep convolutional neural networks (DCNNs) and the ventral visual pathway
share vast architectural and functional similarities in visual challenges such
as object recognition. Recent insights have demonstrated that both hierarchical
cascades can be compared in terms of both exerted behavior and underlying
activation. However, these approaches ignore key differences in spatial
priorities of information processing. In this proof-of-concept study, we
demonstrate a comparison of human observers (N = 45) and three feedforward
DCNNs through eye tracking and saliency maps. The results reveal fundamentally
different resolutions in both visualization methods that need to be considered
for an insightful comparison. Moreover, we provide evidence that a DCNN with
biologically plausible receptive field sizes called vNet reveals higher
agreement with human viewing behavior as contrasted with a standard ResNet
architecture. We find that image-specific factors such as category, animacy,
arousal, and valence have a direct link to the agreement of spatial object
recognition priorities in humans and DCNNs, while other measures such as
difficulty and general image properties do not. With this approach, we try to
open up new perspectives at the intersection of biological and computer vision
research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Dyck_L/0/1/0/all/0/1"&gt;Leonard E. van Dyck&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kwitt_R/0/1/0/all/0/1"&gt;Roland Kwitt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Denzler_S/0/1/0/all/0/1"&gt;Sebastian J. Denzler&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gruber_W/0/1/0/all/0/1"&gt;Walter R. Gruber&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Head Self-Attention via Vision Transformer for Zero-Shot Learning. (arXiv:2108.00045v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00045</id>
        <link href="http://arxiv.org/abs/2108.00045"/>
        <updated>2021-08-03T02:06:30.226Z</updated>
        <summary type="html"><![CDATA[Zero-Shot Learning (ZSL) aims to recognise unseen object classes, which are
not observed during the training phase. The existing body of works on ZSL
mostly relies on pretrained visual features and lacks the explicit attribute
localisation mechanism on images. In this work, we propose an attention-based
model in the problem settings of ZSL to learn attributes useful for unseen
class recognition. Our method uses an attention mechanism adapted from Vision
Transformer to capture and learn discriminative attributes by splitting images
into small patches. We conduct experiments on three popular ZSL benchmarks
(i.e., AWA2, CUB and SUN) and set new state-of-the-art harmonic mean results
{on all the three datasets}, which illustrate the effectiveness of our proposed
method.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Alamri_F/0/1/0/all/0/1"&gt;Faisal Alamri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dutta_A/0/1/0/all/0/1"&gt;Anjan Dutta&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[i-Pulse: A NLP based novel approach for employee engagement in logistics organization. (arXiv:2106.07341v1 [cs.SI] CROSS LISTED)]]></title>
        <id>http://arxiv.org/abs/2106.07341</id>
        <link href="http://arxiv.org/abs/2106.07341"/>
        <updated>2021-08-03T02:06:30.172Z</updated>
        <summary type="html"><![CDATA[Although most logistics and freight forwarding organizations, in one way or
another, claim to have core values. The engagement of employees is a vast
structure that affects almost every part of the company's core environmental
values. There is little theoretical knowledge about the relationship between
firms and the engagement of employees. Based on research literature, this paper
aims to provide a novel approach for insight around employee engagement in a
logistics organization by implementing deep natural language processing
concepts. The artificial intelligence-enabled solution named Intelligent Pulse
(I-Pulse) can evaluate hundreds and thousands of pulse survey comments and
provides the actionable insights and gist of employee feedback. I-Pulse allows
the stakeholders to think in new ways in their organization, helping them to
have a powerful influence on employee engagement, retention, and efficiency.
This study is of corresponding interest to researchers and practitioners.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Garg_R/0/1/0/all/0/1"&gt;Rachit Garg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kiwelekar_A/0/1/0/all/0/1"&gt;Arvind W Kiwelekar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Netak_L/0/1/0/all/0/1"&gt;Laxman D Netak&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghodake_A/0/1/0/all/0/1"&gt;Akshay Ghodake&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[HLE-UPC at SemEval-2021 Task 5: Multi-Depth DistilBERT for Toxic Spans Detection. (arXiv:2104.00639v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2104.00639</id>
        <link href="http://arxiv.org/abs/2104.00639"/>
        <updated>2021-08-03T02:06:30.151Z</updated>
        <summary type="html"><![CDATA[This paper presents our submission to SemEval-2021 Task 5: Toxic Spans
Detection. The purpose of this task is to detect the spans that make a text
toxic, which is a complex labour for several reasons. Firstly, because of the
intrinsic subjectivity of toxicity, and secondly, due to toxicity not always
coming from single words like insults or offends, but sometimes from whole
expressions formed by words that may not be toxic individually. Following this
idea of focusing on both single words and multi-word expressions, we study the
impact of using a multi-depth DistilBERT model, which uses embeddings from
different layers to estimate the final per-token toxicity. Our quantitative
results show that using information from multiple depths boosts the performance
of the model. Finally, we also analyze our best model qualitatively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Palliser_Sans_R/0/1/0/all/0/1"&gt;Rafel Palliser-Sans&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rial_Farras_A/0/1/0/all/0/1"&gt;Albert Rial-Farr&amp;#xe0;s&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Controlling Weather Field Synthesis Using Variational Autoencoders. (arXiv:2108.00048v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00048</id>
        <link href="http://arxiv.org/abs/2108.00048"/>
        <updated>2021-08-03T02:06:30.145Z</updated>
        <summary type="html"><![CDATA[One of the consequences of climate change is anobserved increase in the
frequency of extreme cli-mate events. That poses a challenge for
weatherforecast and generation algorithms, which learnfrom historical data but
should embed an often un-certain bias to create correct scenarios. This
paperinvestigates how mapping climate data to a knowndistribution using
variational autoencoders mighthelp explore such biases and control the
synthesisof weather fields towards more extreme climatescenarios. We
experimented using a monsoon-affected precipitation dataset from southwest
In-dia, which should give a roughly stable pattern ofrainy days and ease our
investigation. We reportcompelling results showing that mapping complexweather
data to a known distribution implementsan efficient control for weather field
synthesis to-wards more (or less) extreme scenarios.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Oliveira_D/0/1/0/all/0/1"&gt;Dario Augusto Borges Oliveira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Diaz_J/0/1/0/all/0/1"&gt;Jorge Guevara Diaz&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zadrozny_B/0/1/0/all/0/1"&gt;Bianca Zadrozny&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Watson_C/0/1/0/all/0/1"&gt;Campbell Watson&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dynamic Transformer for Efficient Machine Translation on Embedded Devices. (arXiv:2107.08199v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.08199</id>
        <link href="http://arxiv.org/abs/2107.08199"/>
        <updated>2021-08-03T02:06:30.134Z</updated>
        <summary type="html"><![CDATA[The Transformer architecture is widely used for machine translation tasks.
However, its resource-intensive nature makes it challenging to implement on
constrained embedded devices, particularly where available hardware resources
can vary at run-time. We propose a dynamic machine translation model that
scales the Transformer architecture based on the available resources at any
particular time. The proposed approach, 'Dynamic-HAT', uses a HAT
SuperTransformer as the backbone to search for SubTransformers with different
accuracy-latency trade-offs at design time. The optimal SubTransformers are
sampled from the SuperTransformer at run-time, depending on latency
constraints. The Dynamic-HAT is tested on the Jetson Nano and the approach uses
inherited SubTransformers sampled directly from the SuperTransformer with a
switching time of <1s. Using inherited SubTransformers results in a BLEU score
loss of <1.5% because the SubTransformer configuration is not retrained from
scratch after sampling. However, to recover this loss in performance, the
dimensions of the design space can be reduced to tailor it to a family of
target hardware. The new reduced design space results in a BLEU score increase
of approximately 1% for sub-optimal models from the original design space, with
a wide range for performance scaling between 0.356s - 1.526s for the GPU and
2.9s - 7.31s for the CPU.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Parry_H/0/1/0/all/0/1"&gt;Hishan Parry&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xun_L/0/1/0/all/0/1"&gt;Lei Xun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sabet_A/0/1/0/all/0/1"&gt;Amin Sabet&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bi_J/0/1/0/all/0/1"&gt;Jia Bi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hare_J/0/1/0/all/0/1"&gt;Jonathon Hare&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Merrett_G/0/1/0/all/0/1"&gt;Geoff V. Merrett&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Spatial Language Understanding for Object Search in Partially Observed City-scale Environments. (arXiv:2012.02705v3 [cs.RO] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.02705</id>
        <link href="http://arxiv.org/abs/2012.02705"/>
        <updated>2021-08-03T02:06:30.126Z</updated>
        <summary type="html"><![CDATA[Humans use spatial language to naturally describe object locations and their
relations. Interpreting spatial language not only adds a perceptual modality
for robots, but also reduces the barrier of interfacing with humans. Previous
work primarily considers spatial language as goal specification for instruction
following tasks in fully observable domains, often paired with reference paths
for reward-based learning. However, spatial language is inherently subjective
and potentially ambiguous or misleading. Hence, in this paper, we consider
spatial language as a form of stochastic observation. We propose SLOOP (Spatial
Language Object-Oriented POMDP), a new framework for partially observable
decision making with a probabilistic observation model for spatial language. We
apply SLOOP to object search in city-scale environments. To interpret
ambiguous, context-dependent prepositions (e.g. front), we design a simple
convolutional neural network that predicts the language provider's latent frame
of reference (FoR) given the environment context. Search strategies are
computed via an online POMDP planner based on Monte Carlo Tree Search.
Evaluation based on crowdsourced language data, collected over areas of five
cities in OpenStreetMap, shows that our approach achieves faster search and
higher success rate compared to baselines, with a wider margin as the spatial
language becomes more complex. Finally, we demonstrate the proposed method in
AirSim, a realistic simulator where a drone is tasked to find cars in a
neighborhood environment.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_K/0/1/0/all/0/1"&gt;Kaiyu Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bayazit_D/0/1/0/all/0/1"&gt;Deniz Bayazit&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mathew_R/0/1/0/all/0/1"&gt;Rebecca Mathew&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pavlick_E/0/1/0/all/0/1"&gt;Ellie Pavlick&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tellex_S/0/1/0/all/0/1"&gt;Stefanie Tellex&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[COfEE: A Comprehensive Ontology for Event Extraction from text, with an online annotation tool. (arXiv:2107.10326v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.10326</id>
        <link href="http://arxiv.org/abs/2107.10326"/>
        <updated>2021-08-03T02:06:30.107Z</updated>
        <summary type="html"><![CDATA[Data is published on the web over time in great volumes, but majority of the
data is unstructured, making it hard to understand and difficult to interpret.
Information Extraction (IE) methods extract structured information from
unstructured data. One of the challenging IE tasks is Event Extraction (EE)
which seeks to derive information about specific incidents and their actors
from the text. EE is useful in many domains such as building a knowledge base,
information retrieval, summarization and online monitoring systems. In the past
decades, some event ontologies like ACE, CAMEO and ICEWS were developed to
define event forms, actors and dimensions of events observed in the text. These
event ontologies still have some shortcomings such as covering only a few
topics like political events, having inflexible structure in defining argument
roles, lack of analytical dimensions, and complexity in choosing event
sub-types. To address these concerns, we propose an event ontology, namely
COfEE, that incorporates both expert domain knowledge, previous ontologies and
a data-driven approach for identifying events from text. COfEE consists of two
hierarchy levels (event types and event sub-types) that include new categories
relating to environmental issues, cyberspace, criminal activity and natural
disasters which need to be monitored instantly. Also, dynamic roles according
to each event sub-type are defined to capture various dimensions of events. In
a follow-up experiment, the proposed ontology is evaluated on Wikipedia events,
and it is shown to be general and comprehensive. Moreover, in order to
facilitate the preparation of gold-standard data for event extraction, a
language-independent online tool is presented based on COfEE.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Balali_A/0/1/0/all/0/1"&gt;Ali Balali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Asadpour_M/0/1/0/all/0/1"&gt;Masoud Asadpour&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jafari_S/0/1/0/all/0/1"&gt;Seyed Hossein Jafari&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Named Entity Recognition with Small Strongly Labeled and Large Weakly Labeled Data. (arXiv:2106.08977v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.08977</id>
        <link href="http://arxiv.org/abs/2106.08977"/>
        <updated>2021-08-03T02:06:30.097Z</updated>
        <summary type="html"><![CDATA[Weak supervision has shown promising results in many natural language
processing tasks, such as Named Entity Recognition (NER). Existing work mainly
focuses on learning deep NER models only with weak supervision, i.e., without
any human annotation, and shows that by merely using weakly labeled data, one
can achieve good performance, though still underperforms fully supervised NER
with manually/strongly labeled data. In this paper, we consider a more
practical scenario, where we have both a small amount of strongly labeled data
and a large amount of weakly labeled data. Unfortunately, we observe that
weakly labeled data does not necessarily improve, or even deteriorate the model
performance (due to the extensive noise in the weak labels) when we train deep
NER models over a simple or weighted combination of the strongly labeled and
weakly labeled data. To address this issue, we propose a new multi-stage
computational framework -- NEEDLE with three essential ingredients: (1) weak
label completion, (2) noise-aware loss function, and (3) final fine-tuning over
the strongly labeled data. Through experiments on E-commerce query NER and
Biomedical NER, we demonstrate that NEEDLE can effectively suppress the noise
of the weak labels and outperforms existing methods. In particular, we achieve
new SOTA F1-scores on 3 Biomedical NER datasets: BC5CDR-chem 93.74,
BC5CDR-disease 90.69, NCBI-disease 92.28.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1"&gt;Haoming Jiang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1"&gt;Danqing Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Cao_T/0/1/0/all/0/1"&gt;Tianyu Cao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yin_B/0/1/0/all/0/1"&gt;Bing Yin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1"&gt;Tuo Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Scientia Potentia Est -- On the Role of Knowledge in Computational Argumentation. (arXiv:2107.00281v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.00281</id>
        <link href="http://arxiv.org/abs/2107.00281"/>
        <updated>2021-08-03T02:06:30.076Z</updated>
        <summary type="html"><![CDATA[Despite extensive research efforts in the recent years, computational
modeling of argumentation remains one of the most challenging areas of natural
language processing (NLP). This is primarily due to inherent complexity of the
cognitive processes behind human argumentation, which commonly combine and
integrate plethora of different types of knowledge, requiring from
computational models capabilities that are far beyond what is needed for most
other (i.e., simpler) natural language understanding tasks. The existing large
body of work on mining, assessing, generating, and reasoning over arguments
largely acknowledges that much more common sense and world knowledge needs to
be integrated into computational models that would accurately model
argumentation. A systematic overview and organization of the types of knowledge
introduced in existing models of computational argumentation (CA) is, however,
missing and this hinders targeted progress in the field. In this survey paper,
we fill this gap by (1) proposing a pyramid of types of knowledge required in
CA tasks, (2) analysing the state of the art with respect to the reliance and
exploitation of these types of knowledge, for each of the for main research
areas in CA, and (3) outlining and discussing directions for future research
efforts in CA.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lauscher_A/0/1/0/all/0/1"&gt;Anne Lauscher&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wachsmuth_H/0/1/0/all/0/1"&gt;Henning Wachsmuth&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1"&gt;Iryna Gurevych&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Glavas_G/0/1/0/all/0/1"&gt;Goran Glava&amp;#x161;&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[LICHEE: Improving Language Model Pre-training with Multi-grained Tokenization. (arXiv:2108.00801v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.00801</id>
        <link href="http://arxiv.org/abs/2108.00801"/>
        <updated>2021-08-03T02:06:30.069Z</updated>
        <summary type="html"><![CDATA[Language model pre-training based on large corpora has achieved tremendous
success in terms of constructing enriched contextual representations and has
led to significant performance gains on a diverse range of Natural Language
Understanding (NLU) tasks. Despite the success, most current pre-trained
language models, such as BERT, are trained based on single-grained
tokenization, usually with fine-grained characters or sub-words, making it hard
for them to learn the precise meaning of coarse-grained words and phrases. In
this paper, we propose a simple yet effective pre-training method named LICHEE
to efficiently incorporate multi-grained information of input text. Our method
can be applied to various pre-trained language models and improve their
representation capability. Extensive experiments conducted on CLUE and
SuperGLUE demonstrate that our method achieves comprehensive improvements on a
wide variety of NLU tasks in both Chinese and English with little extra
inference cost incurred, and that our best ensemble model achieves the
state-of-the-art performance on CLUE benchmark competition.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1"&gt;Weidong Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1"&gt;Mingjun Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1"&gt;Lusheng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Niu_D/0/1/0/all/0/1"&gt;Di Niu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1"&gt;Jinwen Luo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhenhua Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1"&gt;Zhenyang Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1"&gt;Jianbo Tang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Dialogue-oriented Pre-training. (arXiv:2106.00420v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2106.00420</id>
        <link href="http://arxiv.org/abs/2106.00420"/>
        <updated>2021-08-03T02:06:30.051Z</updated>
        <summary type="html"><![CDATA[Pre-trained language models (PrLM) has been shown powerful in enhancing a
broad range of downstream tasks including various dialogue related ones.
However, PrLMs are usually trained on general plain text with common language
model (LM) training objectives, which cannot sufficiently capture dialogue
exclusive features due to the limitation of such training setting, so that
there is an immediate need to fill the gap between a specific dialogue task and
the LM task. As it is unlikely to collect huge dialogue data for
dialogue-oriented pre-training, in this paper, we propose three strategies to
simulate the conversation features on general plain text. Our proposed method
differs from existing post-training methods that it may yield a general-purpose
PrLM and does not individualize to any detailed task while keeping the
capability of learning dialogue related features including speaker awareness,
continuity and consistency. The resulted Dialog-PrLM is fine-tuned on three
public multi-turn dialogue datasets and helps achieve significant and
consistent improvement over the plain PrLMs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1"&gt;Yi Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1"&gt;Hai Zhao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Distantly-Supervised Long-Tailed Relation Extraction Using Constraint Graphs. (arXiv:2105.11225v3 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2105.11225</id>
        <link href="http://arxiv.org/abs/2105.11225"/>
        <updated>2021-08-03T02:06:30.021Z</updated>
        <summary type="html"><![CDATA[Label noise and long-tailed distributions are two major challenges in
distantly supervised relation extraction. Recent studies have shown great
progress on denoising, but pay little attention to the problem of long-tailed
relations. In this paper, we introduce constraint graphs to model the
dependencies between relation labels. On top of that, we further propose a
novel constraint graph-based relation extraction framework(CGRE) to handle the
two challenges simultaneously. CGRE employs graph convolution networks (GCNs)
to propagate information from data-rich relation nodes to data-poor relation
nodes, and thus boosts the representation learning of long-tailed relations. To
further improve the noise immunity, a constraint-aware attention module is
designed in CGRE to integrate the constraint information. Experimental results
on a widely-used benchmark dataset indicate that our approach achieves
significant improvements over the previous methods for both denoising and
long-tailed relation extraction. Our dataset and codes are available at
https://github.com/tmliang/CGRE.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Liang_T/0/1/0/all/0/1"&gt;Tianming Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yang Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1"&gt;Xiaoyan Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_G/0/1/0/all/0/1"&gt;Gaurav Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1"&gt;Maozu Guo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Modal Detection of Alzheimer's Disease from Speech and Text. (arXiv:2012.00096v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2012.00096</id>
        <link href="http://arxiv.org/abs/2012.00096"/>
        <updated>2021-08-03T02:06:30.014Z</updated>
        <summary type="html"><![CDATA[Reliable detection of the prodromal stages of Alzheimer's disease (AD)
remains difficult even today because, unlike other neurocognitive impairments,
there is no definitive diagnosis of AD in vivo. In this context, existing
research has shown that patients often develop language impairment even in mild
AD conditions. We propose a multimodal deep learning method that utilizes
speech and the corresponding transcript simultaneously to detect AD. For audio
signals, the proposed audio-based network, a convolutional neural network (CNN)
based model, predicts the diagnosis for multiple speech segments, which are
combined for the final prediction. Similarly, we use contextual embedding
extracted from BERT concatenated with a CNN-generated embedding for classifying
the transcript. The individual predictions of the two models are then combined
to make the final classification. We also perform experiments to analyze the
model performance when Automated Speech Recognition (ASR) system generated
transcripts are used instead of manual transcription in the text-based model.
The proposed method achieves 85.3% 10-fold cross-validation accuracy when
trained and evaluated on the Dementiabank Pitt corpus.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mittal_A/0/1/0/all/0/1"&gt;Amish Mittal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sahoo_S/0/1/0/all/0/1"&gt;Sourav Sahoo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Datar_A/0/1/0/all/0/1"&gt;Arnhav Datar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kadiwala_J/0/1/0/all/0/1"&gt;Juned Kadiwala&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shalu_H/0/1/0/all/0/1"&gt;Hrithwik Shalu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mathew_J/0/1/0/all/0/1"&gt;Jimson Mathew&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Can you tell? SSNet -- a Sagittal Stratum-inspired Neural Network Framework for Sentiment Analysis. (arXiv:2006.12958v4 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2006.12958</id>
        <link href="http://arxiv.org/abs/2006.12958"/>
        <updated>2021-08-03T02:06:30.007Z</updated>
        <summary type="html"><![CDATA[When people try to understand nuanced language they typically process
multiple input sensor modalities to complete this cognitive task. It turns out
the human brain has even a specialized neuron formation, called sagittal
stratum, to help us understand sarcasm. We use this biological formation as the
inspiration for designing a neural network architecture that combines
predictions of different models on the same text to construct robust, accurate
and computationally efficient classifiers for sentiment analysis and study
several different realizations. Among them, we propose a systematic new
approach to combining multiple predictions based on a dedicated neural network
and develop mathematical analysis of it along with state-of-the-art
experimental results. We also propose a heuristic-hybrid technique for
combining models and back it up with experimental results on a representative
benchmark dataset and comparisons to other methods to show the advantages of
the new approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Vassilev_A/0/1/0/all/0/1"&gt;Apostol Vassilev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hasan_M/0/1/0/all/0/1"&gt;Munawar Hasan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1"&gt;Honglan Jin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Relation Aware Semi-autoregressive Semantic Parsing for NL2SQL. (arXiv:2108.00804v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.00804</id>
        <link href="http://arxiv.org/abs/2108.00804"/>
        <updated>2021-08-03T02:06:29.997Z</updated>
        <summary type="html"><![CDATA[Natural language to SQL (NL2SQL) aims to parse a natural language with a
given database into a SQL query, which widely appears in practical Internet
applications. Jointly encode database schema and question utterance is a
difficult but important task in NL2SQL. One solution is to treat the input as a
heterogeneous graph. However, it failed to learn good word representation in
question utterance. Learning better word representation is important for
constructing a well-designed NL2SQL system. To solve the challenging task, we
present a Relation aware Semi-autogressive Semantic Parsing (\MODN) ~framework,
which is more adaptable for NL2SQL. It first learns relation embedding over the
schema entities and question words with predefined schema relations with
ELECTRA and relation aware transformer layer as backbone. Then we decode the
query SQL with a semi-autoregressive parser and predefined SQL syntax. From
empirical results and case study, our model shows its effectiveness in learning
better word representation in NL2SQL.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1"&gt;Junyang Huang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yongbo Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yongliang Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1"&gt;Yang Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1"&gt;Yanghua Xiao&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multi-Domain Adaptation in Neural Machine Translation Through Multidimensional Tagging. (arXiv:2102.10160v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.10160</id>
        <link href="http://arxiv.org/abs/2102.10160"/>
        <updated>2021-08-03T02:06:29.968Z</updated>
        <summary type="html"><![CDATA[While NMT has achieved remarkable results in the last 5 years, production
systems come with strict quality requirements in arbitrarily niche domains that
are not always adequately covered by readily available parallel corpora. This
is typically addressed by training domain specific models, using fine-tuning
methods and some variation of back-translation on top of in-domain monolingual
corpora. However, industrial practitioners can rarely afford to focus on a
single domain. A far more typical scenario includes a set of closely related,
yet succinctly different sub-domains. At Booking.com, we need to translate
property descriptions, user reviews, as well as messages, (for example those
sent between a customer and an agent or property manager). An editor might need
to translate articles across a set of different topics. An e-commerce platform
would typically need to translate both the description of each item and the
user generated content related to them. To this end, we propose MDT: a novel
method to simultaneously fine-tune on several sub-domains by passing
multidimensional sentence-level information to the model during training and
inference. We show that MDT achieves results competitive to N specialist models
each fine-tuned on a single constituent domain, while effectively serving all N
sub-domains, therefore cutting development and maintenance costs by the same
factor. Besides BLEU (industry standard automatic evaluation metric known to
only weakly correlate with human judgement) we also report rigorous human
evaluation results for all models and sub-domains as well as specific examples
that better contextualise the performance of each model in terms of adequacy
and fluency. To facilitate further research, we plan to make the code available
upon acceptance.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Stergiadis_E/0/1/0/all/0/1"&gt;Emmanouil Stergiadis&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1"&gt;Satendra Kumar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kovalev_F/0/1/0/all/0/1"&gt;Fedor Kovalev&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Levin_P/0/1/0/all/0/1"&gt;Pavel Levin&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Relevance-guided Supervision for OpenQA with ColBERT. (arXiv:2007.00814v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.00814</id>
        <link href="http://arxiv.org/abs/2007.00814"/>
        <updated>2021-08-03T02:06:29.947Z</updated>
        <summary type="html"><![CDATA[Systems for Open-Domain Question Answering (OpenQA) generally depend on a
retriever for finding candidate passages in a large corpus and a reader for
extracting answers from those passages. In much recent work, the retriever is a
learned component that uses coarse-grained vector representations of questions
and passages. We argue that this modeling choice is insufficiently expressive
for dealing with the complexity of natural language questions. To address this,
we define ColBERT-QA, which adapts the scalable neural retrieval model ColBERT
to OpenQA. ColBERT creates fine-grained interactions between questions and
passages. We propose an efficient weak supervision strategy that iteratively
uses ColBERT to create its own training data. This greatly improves OpenQA
retrieval on Natural Questions, SQuAD, and TriviaQA, and the resulting system
attains state-of-the-art extractive OpenQA performance on all three datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khattab_O/0/1/0/all/0/1"&gt;Omar Khattab&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Potts_C/0/1/0/all/0/1"&gt;Christopher Potts&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zaharia_M/0/1/0/all/0/1"&gt;Matei Zaharia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-supervised Answer Retrieval on Clinical Notes. (arXiv:2108.00775v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2108.00775</id>
        <link href="http://arxiv.org/abs/2108.00775"/>
        <updated>2021-08-03T02:06:29.941Z</updated>
        <summary type="html"><![CDATA[Retrieving answer passages from long documents is a complex task requiring
semantic understanding of both discourse and document context. We approach this
challenge specifically in a clinical scenario, where doctors retrieve cohorts
of patients based on diagnoses and other latent medical aspects. We introduce
CAPR, a rule-based self-supervision objective for training Transformer language
models for domain-specific passage matching. In addition, we contribute a novel
retrieval dataset based on clinical notes to simulate this scenario on a large
corpus of clinical notes. We apply our objective in four Transformer-based
architectures: Contextual Document Vectors, Bi-, Poly- and Cross-encoders. From
our extensive evaluation on MIMIC-III and three other healthcare datasets, we
report that CAPR outperforms strong baselines in the retrieval of
domain-specific passages and effectively generalizes across rule-based and
human-labeled passages. This makes the model powerful especially in zero-shot
scenarios where only limited training data is available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Grundmann_P/0/1/0/all/0/1"&gt;Paul Grundmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arnold_S/0/1/0/all/0/1"&gt;Sebastian Arnold&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Loser_A/0/1/0/all/0/1"&gt;Alexander L&amp;#xf6;ser&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Explain and Improve: LRP-Inference Fine-Tuning for Image Captioning Models. (arXiv:2001.01037v5 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2001.01037</id>
        <link href="http://arxiv.org/abs/2001.01037"/>
        <updated>2021-08-03T02:06:29.934Z</updated>
        <summary type="html"><![CDATA[This paper analyzes the predictions of image captioning models with attention
mechanisms beyond visualizing the attention itself. We develop variants of
layer-wise relevance propagation (LRP) and gradient-based explanation methods,
tailored to image captioning models with attention mechanisms. We compare the
interpretability of attention heatmaps systematically against the explanations
provided by explanation methods such as LRP, Grad-CAM, and Guided Grad-CAM. We
show that explanation methods provide simultaneously pixel-wise image
explanations (supporting and opposing pixels of the input image) and linguistic
explanations (supporting and opposing words of the preceding sequence) for each
word in the predicted captions. We demonstrate with extensive experiments that
explanation methods 1) can reveal additional evidence used by the model to make
decisions compared to attention; 2) correlate to object locations with high
precision; 3) are helpful to "debug" the model, e.g. by analyzing the reasons
for hallucinated object words. With the observed properties of explanations, we
further design an LRP-inference fine-tuning strategy that reduces the issue of
object hallucination in image captioning models, and meanwhile, maintains the
sentence fluency. We conduct experiments with two widely used attention
mechanisms: the adaptive attention mechanism calculated with the additive
attention and the multi-head attention mechanism calculated with the scaled dot
product.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1"&gt;Jiamei Sun&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lapuschkin_S/0/1/0/all/0/1"&gt;Sebastian Lapuschkin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Samek_W/0/1/0/all/0/1"&gt;Wojciech Samek&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Binder_A/0/1/0/all/0/1"&gt;Alexander Binder&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[TabPert: An Effective Platform for Tabular Perturbation. (arXiv:2108.00603v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.00603</id>
        <link href="http://arxiv.org/abs/2108.00603"/>
        <updated>2021-08-03T02:06:29.926Z</updated>
        <summary type="html"><![CDATA[To truly grasp reasoning ability, a Natural Language Inference model should
be evaluated on counterfactual data. TabPert facilitates this by assisting in
the generation of such counterfactual data for assessing model tabular
reasoning issues. TabPert allows a user to update a table, change its
associated hypotheses, change their labels, and highlight rows that are
important for hypothesis classification. TabPert also captures information
about the techniques used to automatically produce the table, as well as the
strategies employed to generate the challenging hypotheses. These
counterfactual tables and hypotheses, as well as the metadata, can then be used
to explore an existing model's shortcomings methodically and quantitatively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Jain_N/0/1/0/all/0/1"&gt;Nupur Jain&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_V/0/1/0/all/0/1"&gt;Vivek Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rai_A/0/1/0/all/0/1"&gt;Anshul Rai&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kumar_G/0/1/0/all/0/1"&gt;Gaurav Kumar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[From LSAT: The Progress and Challenges of Complex Reasoning. (arXiv:2108.00648v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.00648</id>
        <link href="http://arxiv.org/abs/2108.00648"/>
        <updated>2021-08-03T02:06:29.896Z</updated>
        <summary type="html"><![CDATA[Complex reasoning aims to draw a correct inference based on complex rules. As
a hallmark of human intelligence, it involves a degree of explicit reading
comprehension, interpretation of logical knowledge and complex rule
application. In this paper, we take a step forward in complex reasoning by
systematically studying the three challenging and domain-general tasks of the
Law School Admission Test (LSAT), including analytical reasoning, logical
reasoning and reading comprehension. We propose a hybrid reasoning system to
integrate these three tasks and achieve impressive overall performance on the
LSAT tests. The experimental results demonstrate that our system endows itself
a certain complex reasoning ability, especially the fundamental reading
comprehension and challenging logical reasoning capacities. Further analysis
also shows the effectiveness of combining the pre-trained models with the
task-specific reasoning module, and integrating symbolic knowledge into
discrete interpretable reasoning steps in complex reasoning. We further shed a
light on the potential future directions, like unsupervised symbolic knowledge
extraction, model interpretability, few-shot learning and comprehensive
benchmark for complex reasoning.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1"&gt;Siyuan Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1"&gt;Zhongkun Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_W/0/1/0/all/0/1"&gt;Wanjun Zhong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1"&gt;Ming Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1"&gt;Zhongyu Wei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1"&gt;Zhumin Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1"&gt;Nan Duan&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Geolocation differences of language use in urban areas. (arXiv:2108.00533v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.00533</id>
        <link href="http://arxiv.org/abs/2108.00533"/>
        <updated>2021-08-03T02:06:29.874Z</updated>
        <summary type="html"><![CDATA[The explosion in the availability of natural language data in the era of
social media has given rise to a host of applications such as sentiment
analysis and opinion mining. Simultaneously, the growing availability of
precise geolocation information is enabling visualization of global phenomena
such as environmental changes and disease propagation. Opportunities for
tracking spatial variations in language use, however, have largely been
overlooked, especially on small spatial scales. Here we explore the use of
Twitter data with precise geolocation information to resolve spatial variations
in language use on an urban scale down to single city blocks. We identify
several categories of language tokens likely to show distinctive patterns of
use and develop quantitative methods to visualize the spatial distributions
associated with these patterns. Our analysis concentrates on comparison of
contrasting pairs of Tweet distributions from the same category, each defined
by a set of tokens. Our work shows that analysis of small-scale variations can
provide unique information on correlations between language use and social
context which are highly valuable to a wide range of fields from linguistic
science and commercial advertising to social services.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kellert_O/0/1/0/all/0/1"&gt;Olga Kellert&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Matlis_N/0/1/0/all/0/1"&gt;Nicholas H. Matlis&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Enhancing Content Preservation in Text Style Transfer Using Reverse Attention and Conditional Layer Normalization. (arXiv:2108.00449v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.00449</id>
        <link href="http://arxiv.org/abs/2108.00449"/>
        <updated>2021-08-03T02:06:29.866Z</updated>
        <summary type="html"><![CDATA[Text style transfer aims to alter the style (e.g., sentiment) of a sentence
while preserving its content. A common approach is to map a given sentence to
content representation that is free of style, and the content representation is
fed to a decoder with a target style. Previous methods in filtering style
completely remove tokens with style at the token level, which incurs the loss
of content information. In this paper, we propose to enhance content
preservation by implicitly removing the style information of each token with
reverse attention, and thereby retain the content. Furthermore, we fuse content
information when building the target style representation, making it dynamic
with respect to the content. Our method creates not only style-independent
content representation, but also content-dependent style representation in
transferring style. Empirical results show that our method outperforms the
state-of-the-art baselines by a large margin in terms of content preservation.
In addition, it is also competitive in terms of style transfer accuracy and
fluency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1"&gt;Dongkyu Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_Z/0/1/0/all/0/1"&gt;Zhiliang Tian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xue_L/0/1/0/all/0/1"&gt;Lanqing Xue&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1"&gt;Nevin L. Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[You too Brutus! Trapping Hateful Users in Social Media: Challenges, Solutions & Insights. (arXiv:2108.00524v1 [cs.SI])]]></title>
        <id>http://arxiv.org/abs/2108.00524</id>
        <link href="http://arxiv.org/abs/2108.00524"/>
        <updated>2021-08-03T02:06:29.855Z</updated>
        <summary type="html"><![CDATA[Hate speech is regarded as one of the crucial issues plaguing the online
social media. The current literature on hate speech detection leverages
primarily the textual content to find hateful posts and subsequently identify
hateful users. However, this methodology disregards the social connections
between users. In this paper, we run a detailed exploration of the problem
space and investigate an array of models ranging from purely textual to graph
based to finally semi-supervised techniques using Graph Neural Networks (GNN)
that utilize both textual and graph-based features. We run exhaustive
experiments on two datasets -- Gab, which is loosely moderated and Twitter,
which is strictly moderated. Overall the AGNN model achieves 0.791 macro
F1-score on the Gab dataset and 0.780 macro F1-score on the Twitter dataset
using only 5% of the labeled instances, considerably outperforming all the
other models including the fully supervised ones. We perform detailed error
analysis on the best performing text and graph based models and observe that
hateful users have unique network neighborhood signatures and the AGNN model
benefits by paying attention to these signatures. This property, as we observe,
also allows the model to generalize well across platforms in a zero-shot
setting. Lastly, we utilize the best performing GNN model to analyze the
evolution of hateful users and their targets over time in Gab.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Das_M/0/1/0/all/0/1"&gt;Mithun Das&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saha_P/0/1/0/all/0/1"&gt;Punyajoy Saha&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dutt_R/0/1/0/all/0/1"&gt;Ritam Dutt&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Goyal_P/0/1/0/all/0/1"&gt;Pawan Goyal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mukherjee_A/0/1/0/all/0/1"&gt;Animesh Mukherjee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mathew_B/0/1/0/all/0/1"&gt;Binny Mathew&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transfer Learning for Mining Feature Requests and Bug Reports from Tweets and App Store Reviews. (arXiv:2108.00663v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.00663</id>
        <link href="http://arxiv.org/abs/2108.00663"/>
        <updated>2021-08-03T02:06:29.849Z</updated>
        <summary type="html"><![CDATA[Identifying feature requests and bug reports in user comments holds great
potential for development teams. However, automated mining of RE-related
information from social media and app stores is challenging since (1) about 70%
of user comments contain noisy, irrelevant information, (2) the amount of user
comments grows daily making manual analysis unfeasible, and (3) user comments
are written in different languages. Existing approaches build on traditional
machine learning (ML) and deep learning (DL), but fail to detect feature
requests and bug reports with high Recall and acceptable Precision which is
necessary for this task. In this paper, we investigate the potential of
transfer learning (TL) for the classification of user comments. Specifically,
we train both monolingual and multilingual BERT models and compare the
performance with state-of-the-art methods. We found that monolingual BERT
models outperform existing baseline methods in the classification of English
App Reviews as well as English and Italian Tweets. However, we also observed
that the application of heavyweight TL models does not necessarily lead to
better performance. In fact, our multilingual BERT models perform worse than
traditional ML methods.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Henao_P/0/1/0/all/0/1"&gt;Pablo Restrepo Henao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Fischbach_J/0/1/0/all/0/1"&gt;Jannik Fischbach&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Spies_D/0/1/0/all/0/1"&gt;Dominik Spies&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Frattini_J/0/1/0/all/0/1"&gt;Julian Frattini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Vogelsang_A/0/1/0/all/0/1"&gt;Andreas Vogelsang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning to Look Inside: Augmenting Token-Based Encoders with Character-Level Information. (arXiv:2108.00391v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.00391</id>
        <link href="http://arxiv.org/abs/2108.00391"/>
        <updated>2021-08-03T02:06:29.842Z</updated>
        <summary type="html"><![CDATA[Commonly-used transformer language models depend on a tokenization schema
which sets an unchangeable subword vocabulary prior to pre-training, destined
to be applied to all downstream tasks regardless of domain shift, novel word
formations, or other sources of vocabulary mismatch. Recent work has shown that
"token-free" models can be trained directly on characters or bytes, but
training these models from scratch requires substantial computational
resources, and this implies discarding the many domain-specific models that
were trained on tokens. In this paper, we present XRayEmb, a method for
retrofitting existing token-based models with character-level information.
XRayEmb is composed of a character-level "encoder" that computes vector
representations of character sequences, and a generative component that decodes
from the internal representation to a character sequence. We show that
incorporating XRayEmb's learned vectors into sequences of pre-trained token
embeddings helps performance on both autoregressive and masked pre-trained
transformer architectures and on both sequence-level and sequence tagging
tasks, particularly on non-standard English text.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Pinter_Y/0/1/0/all/0/1"&gt;Yuval Pinter&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Stent_A/0/1/0/all/0/1"&gt;Amanda Stent&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dredze_M/0/1/0/all/0/1"&gt;Mark Dredze&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Eisenstein_J/0/1/0/all/0/1"&gt;Jacob Eisenstein&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MuSiQue: Multi-hop Questions via Single-hop Question Composition. (arXiv:2108.00573v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.00573</id>
        <link href="http://arxiv.org/abs/2108.00573"/>
        <updated>2021-08-03T02:06:29.835Z</updated>
        <summary type="html"><![CDATA[To build challenging multi-hop question answering datasets, we propose a
bottom-up semi-automatic process of constructing multi-hop question via
composition of single-hop questions. Constructing multi-hop questions as
composition of single-hop questions allows us to exercise greater control over
the quality of the resulting multi-hop questions. This process allows building
a dataset with (i) connected reasoning where each step needs the answer from a
previous step; (ii) minimal train-test leakage by eliminating even partial
overlap of reasoning steps; (iii) variable number of hops and composition
structures; and (iv) contrasting unanswerable questions by modifying the
context. We use this process to construct a new multihop QA dataset:
MuSiQue-Ans with ~25K 2-4 hop questions using seed questions from 5 existing
single-hop datasets. Our experiments demonstrate that MuSique is challenging
for state-of-the-art QA models (e.g., human-machine gap of $~$30 F1 pts),
significantly harder than existing datasets (2x human-machine gap), and
substantially less cheatable (e.g., a single-hop model is worse by 30 F1 pts).
We also build an even more challenging dataset, MuSiQue-Full, consisting of
answerable and unanswerable contrast question pairs, where model performance
drops further by 13+ F1 pts. For data and code, see
\url{https://github.com/stonybrooknlp/musique}.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Trivedi_H/0/1/0/all/0/1"&gt;Harsh Trivedi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Balasubramanian_N/0/1/0/all/0/1"&gt;Niranjan Balasubramanian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Khot_T/0/1/0/all/0/1"&gt;Tushar Khot&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sabharwal_A/0/1/0/all/0/1"&gt;Ashish Sabharwal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Logic-Consistency Text Generation from Semantic Parses. (arXiv:2108.00577v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.00577</id>
        <link href="http://arxiv.org/abs/2108.00577"/>
        <updated>2021-08-03T02:06:29.733Z</updated>
        <summary type="html"><![CDATA[Text generation from semantic parses is to generate textual descriptions for
formal representation inputs such as logic forms and SQL queries. This is
challenging due to two reasons: (1) the complex and intensive inner logic with
the data scarcity constraint, (2) the lack of automatic evaluation metrics for
logic consistency. To address these two challenges, this paper first proposes
SNOWBALL, a framework for logic consistent text generation from semantic parses
that employs an iterative training procedure by recursively augmenting the
training set with quality control. Second, we propose a novel automatic metric,
BLEC, for evaluating the logical consistency between the semantic parses and
generated texts. The experimental results on two benchmark datasets, Logic2Text
and Spider, demonstrate the SNOWBALL framework enhances the logic consistency
on both BLEC and human evaluation. Furthermore, our statistical analysis
reveals that BLEC is more logically consistent with human evaluation than
general-purpose automatic metrics including BLEU, ROUGE and, BLEURT. Our data
and code are available at https://github.com/Ciaranshu/relogic.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Shu_C/0/1/0/all/0/1"&gt;Chang Shu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yusen Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1"&gt;Xiangyu Dong&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_P/0/1/0/all/0/1"&gt;Peng Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1"&gt;Tao Yu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1"&gt;Rui Zhang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Is My Model Using The Right Evidence? Systematic Probes for Examining Evidence-Based Tabular Reasoning. (arXiv:2108.00578v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.00578</id>
        <link href="http://arxiv.org/abs/2108.00578"/>
        <updated>2021-08-03T02:06:29.726Z</updated>
        <summary type="html"><![CDATA[While neural models routinely report state-of-the-art performance across NLP
tasks involving reasoning, their outputs are often observed to not properly use
and reason on the evidence presented to them in the inputs. A model that
reasons properly is expected to attend to the right parts of the input, be
self-consistent in its predictions across examples, avoid spurious patterns in
inputs, and to ignore biasing from its underlying pre-trained language model in
a nuanced, context-sensitive fashion (e.g. handling counterfactuals). Do
today's models do so? In this paper, we study this question using the problem
of reasoning on tabular data. The tabular nature of the input is particularly
suited for the study as it admits systematic probes targeting the properties
listed above. Our experiments demonstrate that a BERT-based model
representative of today's state-of-the-art fails to properly reason on the
following counts: it often (a) misses the relevant evidence, (b) suffers from
hypothesis and knowledge biases, and, (c) relies on annotation artifacts and
knowledge from pre-trained language models as primary evidence rather than
relying on reasoning on the premises in the tabular input.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gupta_V/0/1/0/all/0/1"&gt;Vivek Gupta&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bhat_R/0/1/0/all/0/1"&gt;Riyaz A. Bhat&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ghosal_A/0/1/0/all/0/1"&gt;Atreya Ghosal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srivastava_M/0/1/0/all/0/1"&gt;Manish Srivastava&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1"&gt;Maneesh Singh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Srikumar_V/0/1/0/all/0/1"&gt;Vivek Srikumar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DECAF: Deep Extreme Classification with Label Features. (arXiv:2108.00368v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.00368</id>
        <link href="http://arxiv.org/abs/2108.00368"/>
        <updated>2021-08-03T02:06:29.720Z</updated>
        <summary type="html"><![CDATA[Extreme multi-label classification (XML) involves tagging a data point with
its most relevant subset of labels from an extremely large label set, with
several applications such as product-to-product recommendation with millions of
products. Although leading XML algorithms scale to millions of labels, they
largely ignore label meta-data such as textual descriptions of the labels. On
the other hand, classical techniques that can utilize label metadata via
representation learning using deep networks struggle in extreme settings. This
paper develops the DECAF algorithm that addresses these challenges by learning
models enriched by label metadata that jointly learn model parameters and
feature representations using deep networks and offer accurate classification
at the scale of millions of labels. DECAF makes specific contributions to model
architecture design, initialization, and training, enabling it to offer up to
2-6% more accurate prediction than leading extreme classifiers on publicly
available benchmark product-to-product recommendation datasets, such as
LF-AmazonTitles-1.3M. At the same time, DECAF was found to be up to 22x faster
at inference than leading deep extreme classifiers, which makes it suitable for
real-time applications that require predictions within a few milliseconds. The
code for DECAF is available at the following URL
https://github.com/Extreme-classification/DECAF.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mittal_A/0/1/0/all/0/1"&gt;Anshul Mittal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dahiya_K/0/1/0/all/0/1"&gt;Kunal Dahiya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agrawal_S/0/1/0/all/0/1"&gt;Sheshansh Agrawal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saini_D/0/1/0/all/0/1"&gt;Deepak Saini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1"&gt;Sumeet Agarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kar_P/0/1/0/all/0/1"&gt;Purushottam Kar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Varma_M/0/1/0/all/0/1"&gt;Manik Varma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Realised Volatility Forecasting: Machine Learning via Financial Word Embedding. (arXiv:2108.00480v1 [q-fin.CP])]]></title>
        <id>http://arxiv.org/abs/2108.00480</id>
        <link href="http://arxiv.org/abs/2108.00480"/>
        <updated>2021-08-03T02:06:29.702Z</updated>
        <summary type="html"><![CDATA[We develop FinText, a novel, state-of-the-art, financial word embedding from
Dow Jones Newswires Text News Feed Database. Incorporating this word embedding
in a machine learning model produces a substantial increase in volatility
forecasting performance on days with volatility jumps for 23 NASDAQ stocks from
27 July 2007 to 18 November 2016. A simple ensemble model, combining our word
embedding and another machine learning model that uses limit order book data,
provides the best forecasting performance for both normal and jump volatility
days. Finally, we use Integrated Gradients and SHAP (SHapley Additive
exPlanations) to make the results more 'explainable' and the model comparisons
more transparent.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/q-fin/1/au:+Rahimikia_E/0/1/0/all/0/1"&gt;Eghbal Rahimikia&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Zohren_S/0/1/0/all/0/1"&gt;Stefan Zohren&lt;/a&gt;, &lt;a href="http://arxiv.org/find/q-fin/1/au:+Poon_S/0/1/0/all/0/1"&gt;Ser-Huang Poon&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ConveRT, an Application to FAQ Answering. (arXiv:2108.00719v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.00719</id>
        <link href="http://arxiv.org/abs/2108.00719"/>
        <updated>2021-08-03T02:06:29.688Z</updated>
        <summary type="html"><![CDATA[Knowledgeable FAQ chatbots are a valuable resource to any organization.
Unlike traditional call centers or FAQ web pages, they provide instant
responses and are always available. Our experience running a COVID19 chatbot
revealed the lack of resources available for FAQ answering in non-English
languages. While powerful and efficient retrieval-based models exist for
English, it is rarely the case for other languages which do not have the same
amount of training data available. In this work, we propose a novel pretaining
procedure to adapt ConveRT, an English SOTA conversational agent, to other
languages with less training data available. We apply it for the first time to
the task of Dutch FAQ answering related to the COVID19 vaccine. We show it
performs better than an open-source alternative in a low-data regime and
high-data regime.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bruyn_M/0/1/0/all/0/1"&gt;Maxime De Bruyn&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lotfi_E/0/1/0/all/0/1"&gt;Ehsan Lotfi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Buhmann_J/0/1/0/all/0/1"&gt;Jeska Buhmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Daelemans_W/0/1/0/all/0/1"&gt;Walter Daelemans&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Chest ImaGenome Dataset for Clinical Reasoning. (arXiv:2108.00316v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00316</id>
        <link href="http://arxiv.org/abs/2108.00316"/>
        <updated>2021-08-03T02:06:29.681Z</updated>
        <summary type="html"><![CDATA[Despite the progress in automatic detection of radiologic findings from chest
X-ray (CXR) images in recent years, a quantitative evaluation of the
explainability of these models is hampered by the lack of locally labeled
datasets for different findings. With the exception of a few expert-labeled
small-scale datasets for specific findings, such as pneumonia and pneumothorax,
most of the CXR deep learning models to date are trained on global "weak"
labels extracted from text reports, or trained via a joint image and
unstructured text learning strategy. Inspired by the Visual Genome effort in
the computer vision community, we constructed the first Chest ImaGenome dataset
with a scene graph data structure to describe $242,072$ images. Local
annotations are automatically produced using a joint rule-based natural
language processing (NLP) and atlas-based bounding box detection pipeline.
Through a radiologist constructed CXR ontology, the annotations for each CXR
are connected as an anatomy-centered scene graph, useful for image-level
reasoning and multimodal fusion applications. Overall, we provide: i) $1,256$
combinations of relation annotations between $29$ CXR anatomical locations
(objects with bounding box coordinates) and their attributes, structured as a
scene graph per image, ii) over $670,000$ localized comparison relations (for
improved, worsened, or no change) between the anatomical locations across
sequential exams, as well as ii) a manually annotated gold standard scene graph
dataset from $500$ unique patients.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1"&gt;Joy T. Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agu_N/0/1/0/all/0/1"&gt;Nkechinyere N. Agu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lourentzou_I/0/1/0/all/0/1"&gt;Ismini Lourentzou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1"&gt;Arjun Sharma&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Paguio_J/0/1/0/all/0/1"&gt;Joseph A. Paguio&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1"&gt;Jasper S. Yao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dee_E/0/1/0/all/0/1"&gt;Edward C. Dee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mitchell_W/0/1/0/all/0/1"&gt;William Mitchell&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kashyap_S/0/1/0/all/0/1"&gt;Satyananda Kashyap&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Giovannini_A/0/1/0/all/0/1"&gt;Andrea Giovannini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Celi_L/0/1/0/all/0/1"&gt;Leo A. Celi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Moradi_M/0/1/0/all/0/1"&gt;Mehdi Moradi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Attention-based Aspect Reasoning for Knowledge Base Question Answering on Clinical Notes. (arXiv:2108.00513v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.00513</id>
        <link href="http://arxiv.org/abs/2108.00513"/>
        <updated>2021-08-03T02:06:29.652Z</updated>
        <summary type="html"><![CDATA[Question Answering (QA) in clinical notes has gained a lot of attention in
the past few years. Existing machine reading comprehension approaches in
clinical domain can only handle questions about a single block of clinical
texts and fail to retrieve information about different patients and clinical
notes. To handle more complex questions, we aim at creating knowledge base from
clinical notes to link different patients and clinical notes, and performing
knowledge base question answering (KBQA). Based on the expert annotations in
n2c2, we first created the ClinicalKBQA dataset that includes 8,952 QA pairs
and covers questions about seven medical topics through 322 question templates.
Then, we proposed an attention-based aspect reasoning (AAR) method for KBQA and
investigated the impact of different aspects of answers (e.g., entity, type,
path, and context) for prediction. The AAR method achieves better performance
due to the well-designed encoder and attention mechanism. In the experiments,
we find that both aspects, type and path, enable the model to identify answers
satisfying the general conditions and produce lower precision and higher
recall. On the other hand, the aspects, entity and context, limit the answers
by node-specific information and lead to higher precision and lower recall.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1"&gt;Ping Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_T/0/1/0/all/0/1"&gt;Tian Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agarwal_K/0/1/0/all/0/1"&gt;Khushbu Agarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Choudhury_S/0/1/0/all/0/1"&gt;Sutanay Choudhury&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Reddy_C/0/1/0/all/0/1"&gt;Chandan K. Reddy&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Transformer-Encoder-GRU (T-E-GRU) for Chinese Sentiment Analysis on Chinese Comment Text. (arXiv:2108.00400v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.00400</id>
        <link href="http://arxiv.org/abs/2108.00400"/>
        <updated>2021-08-03T02:06:29.607Z</updated>
        <summary type="html"><![CDATA[Chinese sentiment analysis (CSA) has always been one of the challenges in
natural language processing due to its complexity and uncertainty. Transformer
has succeeded in capturing semantic features, but it uses position encoding to
capture sequence features, which has great shortcomings compared with the
recurrent model. In this paper, we propose T-E-GRU for Chinese sentiment
analysis, which combine transformer encoder and GRU. We conducted experiments
on three Chinese comment datasets. In view of the confusion of punctuation
marks in Chinese comment texts, we selectively retain some punctuation marks
with sentence segmentation ability. The experimental results show that T-E-GRU
outperforms classic recurrent model and recurrent model with attention.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1"&gt;Binlong Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1"&gt;Wei Zhou&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Towards Continual Entity Learning in Language Models for Conversational Agents. (arXiv:2108.00082v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.00082</id>
        <link href="http://arxiv.org/abs/2108.00082"/>
        <updated>2021-08-03T02:06:29.557Z</updated>
        <summary type="html"><![CDATA[Neural language models (LM) trained on diverse corpora are known to work well
on previously seen entities, however, updating these models with dynamically
changing entities such as place names, song titles and shopping items requires
re-training from scratch and collecting full sentences containing these
entities. We aim to address this issue, by introducing entity-aware language
models (EALM), where we integrate entity models trained on catalogues of
entities into the pre-trained LMs. Our combined language model adaptively adds
information from the entity models into the pre-trained LM depending on the
sentence context. Our entity models can be updated independently of the
pre-trained LM, enabling us to influence the distribution of entities output by
the final LM, without any further training of the pre-trained LM. We show
significant perplexity improvements on task-oriented dialogue datasets,
especially on long-tailed utterances, with an ability to continually adapt to
new entities (to an extent).]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Gadde_R/0/1/0/all/0/1"&gt;Ravi Teja Gadde&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bulyko_I/0/1/0/all/0/1"&gt;Ivan Bulyko&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Psychologically Informed Part-of-Speech Analysis of Depression in Social Media. (arXiv:2108.00279v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.00279</id>
        <link href="http://arxiv.org/abs/2108.00279"/>
        <updated>2021-08-03T02:06:29.550Z</updated>
        <summary type="html"><![CDATA[In this work, we provide an extensive part-of-speech analysis of the
discourse of social media users with depression. Research in psychology
revealed that depressed users tend to be self-focused, more preoccupied with
themselves and ruminate more about their lives and emotions. Our work aims to
make use of large-scale datasets and computational methods for a quantitative
exploration of discourse. We use the publicly available depression dataset from
the Early Risk Prediction on the Internet Workshop (eRisk) 2018 and extract
part-of-speech features and several indices based on them. Our results reveal
statistically significant differences between the depressed and non-depressed
individuals confirming findings from the existing psychology literature. Our
work provides insights regarding the way in which depressed individuals are
expressing themselves on social media platforms, allowing for better-informed
computational models to help monitor and prevent mental illnesses.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bucur_A/0/1/0/all/0/1"&gt;Ana-Maria Bucur&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Podina_I/0/1/0/all/0/1"&gt;Ioana R. Podin&amp;#x103;&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dinu_L/0/1/0/all/0/1"&gt;Liviu P. Dinu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ECLARE: Extreme Classification with Label Graph Correlations. (arXiv:2108.00261v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.00261</id>
        <link href="http://arxiv.org/abs/2108.00261"/>
        <updated>2021-08-03T02:06:29.542Z</updated>
        <summary type="html"><![CDATA[Deep extreme classification (XC) seeks to train deep architectures that can
tag a data point with its most relevant subset of labels from an extremely
large label set. The core utility of XC comes from predicting labels that are
rarely seen during training. Such rare labels hold the key to personalized
recommendations that can delight and surprise a user. However, the large number
of rare labels and small amount of training data per rare label offer
significant statistical and computational challenges. State-of-the-art deep XC
methods attempt to remedy this by incorporating textual descriptions of labels
but do not adequately address the problem. This paper presents ECLARE, a
scalable deep learning architecture that incorporates not only label text, but
also label correlations, to offer accurate real-time predictions within a few
milliseconds. Core contributions of ECLARE include a frugal architecture and
scalable techniques to train deep models along with label correlation graphs at
the scale of millions of labels. In particular, ECLARE offers predictions that
are 2 to 14% more accurate on both publicly available benchmark datasets as
well as proprietary datasets for a related products recommendation task sourced
from the Bing search engine. Code for ECLARE is available at
https://github.com/Extreme-classification/ECLARE.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mittal_A/0/1/0/all/0/1"&gt;Anshul Mittal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sachdeva_N/0/1/0/all/0/1"&gt;Noveen Sachdeva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agrawal_S/0/1/0/all/0/1"&gt;Sheshansh Agrawal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1"&gt;Sumeet Agarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kar_P/0/1/0/all/0/1"&gt;Purushottam Kar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Varma_M/0/1/0/all/0/1"&gt;Manik Varma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Using Knowledge-Embedded Attention to Augment Pre-trained Language Models for Fine-Grained Emotion Recognition. (arXiv:2108.00194v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.00194</id>
        <link href="http://arxiv.org/abs/2108.00194"/>
        <updated>2021-08-03T02:06:29.510Z</updated>
        <summary type="html"><![CDATA[Modern emotion recognition systems are trained to recognize only a small set
of emotions, and hence fail to capture the broad spectrum of emotions people
experience and express in daily life. In order to engage in more empathetic
interactions, future AI has to perform \textit{fine-grained} emotion
recognition, distinguishing between many more varied emotions. Here, we focus
on improving fine-grained emotion recognition by introducing external knowledge
into a pre-trained self-attention model. We propose Knowledge-Embedded
Attention (KEA) to use knowledge from emotion lexicons to augment the
contextual representations from pre-trained ELECTRA and BERT models. Our
results and error analyses outperform previous models on several datasets, and
is better able to differentiate closely-confusable emotions, such as afraid and
terrified.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Suresh_V/0/1/0/all/0/1"&gt;Varsha Suresh&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ong_D/0/1/0/all/0/1"&gt;Desmond C. Ong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Tensor completion using geodesics on Segre manifolds. (arXiv:2108.00735v1 [math.DG])]]></title>
        <id>http://arxiv.org/abs/2108.00735</id>
        <link href="http://arxiv.org/abs/2108.00735"/>
        <updated>2021-08-03T02:06:29.491Z</updated>
        <summary type="html"><![CDATA[We propose a Riemannian conjugate gradient (CG) optimization method for
finding low rank approximations of incomplete tensors. Our main contribution
consists of an explicit expression of the geodesics on the Segre manifold.
These are exploited in our algorithm to perform the retractions. We apply our
method to movie rating predictions in a recommender system for the MovieLens
dataset, and identification of pure fluorophores via fluorescent spectroscopy
with missing data. In this last application, we recover the tensor
decomposition from less than $10\%$ of the data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/math/1/au:+Swijsen_L/0/1/0/all/0/1"&gt;Lars Swijsen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Veken_J/0/1/0/all/0/1"&gt;Joeri Van der Veken&lt;/a&gt;, &lt;a href="http://arxiv.org/find/math/1/au:+Vannieuwenhoven_N/0/1/0/all/0/1"&gt;Nick Vannieuwenhoven&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-Supervised Learning of Context-Aware Pitch Prosody Representations. (arXiv:2007.09060v4 [cs.SD] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.09060</id>
        <link href="http://arxiv.org/abs/2007.09060"/>
        <updated>2021-08-03T02:06:29.478Z</updated>
        <summary type="html"><![CDATA[In music and speech, meaning is derived at multiple levels of context.
Affect, for example, can be inferred both by a short sound token and by sonic
patterns over a longer temporal window such as an entire recording. In this
letter, we focus on inferring meaning from this dichotomy of contexts. We show
how contextual representations of short sung vocal lines can be implicitly
learned from fundamental frequency ($F_0$) and thus be used as a meaningful
feature space for downstream Music Information Retrieval (MIR) tasks. We
propose three self-supervised deep learning paradigms which leverage pseudotask
learning of these two levels of context to produce latent representation
spaces. We evaluate the usefulness of these representations by embedding unseen
pitch contours into each space and conducting downstream classification tasks.
Our results show that contextual representation can enhance downstream
classification by as much as 15\% as compared to using traditional statistical
contour features.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Noufi_C/0/1/0/all/0/1"&gt;Camille Noufi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Verma_P/0/1/0/all/0/1"&gt;Prateek Verma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Improving Social Meaning Detection with Pragmatic Masking and Surrogate Fine-Tuning. (arXiv:2108.00356v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.00356</id>
        <link href="http://arxiv.org/abs/2108.00356"/>
        <updated>2021-08-03T02:06:29.447Z</updated>
        <summary type="html"><![CDATA[Masked language models (MLMs) are pretrained with a denoising objective that,
while useful, is in a mismatch with the objective of downstream fine-tuning. We
propose pragmatic masking and surrogate fine-tuning as two strategies that
exploit social cues to drive pre-trained representations toward a broad set of
concepts useful for a wide class of social meaning tasks. To test our methods,
we introduce a new benchmark of 15 different Twitter datasets for social
meaning detection. Our methods achieve 2.34% F1 over a competitive baseline,
while outperforming other transfer learning methods such as multi-task learning
and domain-specific language models pretrained on large datasets. With only 5%
of training data (severely few-shot), our methods enable an impressive 68.74%
average F1, and we observe promising results in a zero-shot setting involving
six datasets from three different languages.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1"&gt;Chiyu Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Abdul_Mageed_M/0/1/0/all/0/1"&gt;Muhammad Abdul-Mageed&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Elmadany_A/0/1/0/all/0/1"&gt;AbdelRahim Elmadany&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Nagoudi_E/0/1/0/all/0/1"&gt;El Moatez Billah Nagoudi&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Structural Guidance for Transformer Language Models. (arXiv:2108.00104v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.00104</id>
        <link href="http://arxiv.org/abs/2108.00104"/>
        <updated>2021-08-03T02:06:29.439Z</updated>
        <summary type="html"><![CDATA[Transformer-based language models pre-trained on large amounts of text data
have proven remarkably successful in learning generic transferable linguistic
representations. Here we study whether structural guidance leads to more
human-like systematic linguistic generalization in Transformer language models
without resorting to pre-training on very large amounts of data. We explore two
general ideas. The "Generative Parsing" idea jointly models the incremental
parse and word sequence as part of the same sequence modeling task. The
"Structural Scaffold" idea guides the language model's representation via
additional structure loss that separately predicts the incremental constituency
parse. We train the proposed models along with a vanilla Transformer language
model baseline on a 14 million-token and a 46 million-token subset of the BLLIP
dataset, and evaluate models' syntactic generalization performances on SG Test
Suites and sized BLiMP. Experiment results across two benchmarks suggest
converging evidence that generative structural supervisions can induce more
robust and humanlike linguistic generalization in Transformer language models
without the need for data intensive pre-training.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Qian_P/0/1/0/all/0/1"&gt;Peng Qian&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Naseem_T/0/1/0/all/0/1"&gt;Tahira Naseem&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Levy_R/0/1/0/all/0/1"&gt;Roger Levy&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Astudillo_R/0/1/0/all/0/1"&gt;Ram&amp;#xf3;n Fernandez Astudillo&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cross-cultural Mood Perception in Pop Songs and its Alignment with Mood Detection Algorithms. (arXiv:2108.00768v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2108.00768</id>
        <link href="http://arxiv.org/abs/2108.00768"/>
        <updated>2021-08-03T02:06:29.432Z</updated>
        <summary type="html"><![CDATA[Do people from different cultural backgrounds perceive the mood in music the
same way? How closely do human ratings across different cultures approximate
automatic mood detection algorithms that are often trained on corpora of
predominantly Western popular music? Analyzing 166 participants responses from
Brazil, South Korea, and the US, we examined the similarity between the ratings
of nine categories of perceived moods in music and estimated their alignment
with four popular mood detection algorithms. We created a dataset of 360 recent
pop songs drawn from major music charts of the countries and constructed
semantically identical mood descriptors across English, Korean, and Portuguese
languages. Multiple participants from the three countries rated their
familiarity, preference, and perceived moods for a given song. Ratings were
highly similar within and across cultures for basic mood attributes such as
sad, cheerful, and energetic. However, we found significant cross-cultural
differences for more complex characteristics such as dreamy and love. To our
surprise, the results of mood detection algorithms were uniformly correlated
across human ratings from all three countries and did not show a detectable
bias towards any particular culture. Our study thus suggests that the mood
detection algorithms can be considered as an objective measure at least within
the popular music context.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1"&gt;Harin Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Hoeger_F/0/1/0/all/0/1"&gt;Frank Hoeger&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Schoenwiesner_M/0/1/0/all/0/1"&gt;Marc Schoenwiesner&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Park_M/0/1/0/all/0/1"&gt;Minsu Park&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jacoby_N/0/1/0/all/0/1"&gt;Nori Jacoby&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[MTVR: Multilingual Moment Retrieval in Videos. (arXiv:2108.00061v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.00061</id>
        <link href="http://arxiv.org/abs/2108.00061"/>
        <updated>2021-08-03T02:06:29.423Z</updated>
        <summary type="html"><![CDATA[We introduce mTVR, a large-scale multilingual video moment retrieval dataset,
containing 218K English and Chinese queries from 21.8K TV show video clips. The
dataset is collected by extending the popular TVR dataset (in English) with
paired Chinese queries and subtitles. Compared to existing moment retrieval
datasets, mTVR is multilingual, larger, and comes with diverse annotations. We
further propose mXML, a multilingual moment retrieval model that learns and
operates on data from both languages, via encoder parameter sharing and
language neighborhood constraints. We demonstrate the effectiveness of mXML on
the newly collected MTVR dataset, where mXML outperforms strong monolingual
baselines while using fewer parameters. In addition, we also provide detailed
dataset analyses and model ablations. Data and code are publicly available at
https://github.com/jayleicn/mTVRetrieval]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Lei_J/0/1/0/all/0/1"&gt;Jie Lei&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Berg_T/0/1/0/all/0/1"&gt;Tamara L. Berg&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1"&gt;Mohit Bansal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Opinion Prediction with User Fingerprinting. (arXiv:2108.00270v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.00270</id>
        <link href="http://arxiv.org/abs/2108.00270"/>
        <updated>2021-08-03T02:06:29.400Z</updated>
        <summary type="html"><![CDATA[Opinion prediction is an emerging research area with diverse real-world
applications, such as market research and situational awareness. We identify
two lines of approaches to the problem of opinion prediction. One uses
topic-based sentiment analysis with time-series modeling, while the other uses
static embedding of text. The latter approaches seek user-specific solutions by
generating user fingerprints. Such approaches are useful in predicting user's
reactions to unseen content. In this work, we propose a novel dynamic
fingerprinting method that leverages contextual embedding of user's comments
conditioned on relevant user's reading history. We integrate BERT variants with
a recurrent neural network to generate predictions. The results show up to 13\%
improvement in micro F1-score compared to previous approaches. Experimental
results show novel insights that were previously unknown such as better
predictions for an increase in dynamic history length, the impact of the nature
of the article on performance, thereby laying the foundation for further
research.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Tumarada_K/0/1/0/all/0/1"&gt;Kishore Tumarada&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1"&gt;Yifan Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1"&gt;Dr. Fan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dragut_D/0/1/0/all/0/1"&gt;Dr. Eduard Dragut&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gnawali_D/0/1/0/all/0/1"&gt;Dr. Omprakash Gnawali&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mukherjee_D/0/1/0/all/0/1"&gt;Dr. Arjun Mukherjee&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Relevance-guided Supervision for OpenQA with ColBERT. (arXiv:2007.00814v2 [cs.CL] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2007.00814</id>
        <link href="http://arxiv.org/abs/2007.00814"/>
        <updated>2021-08-03T02:06:29.392Z</updated>
        <summary type="html"><![CDATA[Systems for Open-Domain Question Answering (OpenQA) generally depend on a
retriever for finding candidate passages in a large corpus and a reader for
extracting answers from those passages. In much recent work, the retriever is a
learned component that uses coarse-grained vector representations of questions
and passages. We argue that this modeling choice is insufficiently expressive
for dealing with the complexity of natural language questions. To address this,
we define ColBERT-QA, which adapts the scalable neural retrieval model ColBERT
to OpenQA. ColBERT creates fine-grained interactions between questions and
passages. We propose an efficient weak supervision strategy that iteratively
uses ColBERT to create its own training data. This greatly improves OpenQA
retrieval on Natural Questions, SQuAD, and TriviaQA, and the resulting system
attains state-of-the-art extractive OpenQA performance on all three datasets.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Khattab_O/0/1/0/all/0/1"&gt;Omar Khattab&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Potts_C/0/1/0/all/0/1"&gt;Christopher Potts&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zaharia_M/0/1/0/all/0/1"&gt;Matei Zaharia&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Human Evaluation of Creative NLG Systems: An Interdisciplinary Survey on Recent Papers. (arXiv:2108.00308v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.00308</id>
        <link href="http://arxiv.org/abs/2108.00308"/>
        <updated>2021-08-03T02:06:29.385Z</updated>
        <summary type="html"><![CDATA[We survey human evaluation in papers presenting work on creative natural
language generation that have been published in INLG 2020 and ICCC 2020. The
most typical human evaluation method is a scaled survey, typically on a 5 point
scale, while many other less common methods exist. The most commonly evaluated
parameters are meaning, syntactic correctness, novelty, relevance and emotional
value, among many others. Our guidelines for future evaluation include clearly
defining the goal of the generative system, asking questions as concrete as
possible, testing the evaluation setup, using multiple different evaluation
setups, reporting the entire evaluation process and potential biases clearly,
and finally analyzing the evaluation results in a more profound way than merely
reporting the most typical statistics.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hamalainen_M/0/1/0/all/0/1"&gt;Mika H&amp;#xe4;m&amp;#xe4;l&amp;#xe4;inen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Alnajjar_K/0/1/0/all/0/1"&gt;Khalid Alnajjar&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Learning TFIDF Enhanced Joint Embedding for Recipe-Image Cross-Modal Retrieval Service. (arXiv:2108.00724v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00724</id>
        <link href="http://arxiv.org/abs/2108.00724"/>
        <updated>2021-08-03T02:06:29.377Z</updated>
        <summary type="html"><![CDATA[It is widely acknowledged that learning joint embeddings of recipes with
images is challenging due to the diverse composition and deformation of
ingredients in cooking procedures. We present a Multi-modal Semantics enhanced
Joint Embedding approach (MSJE) for learning a common feature space between the
two modalities (text and image), with the ultimate goal of providing
high-performance cross-modal retrieval services. Our MSJE approach has three
unique features. First, we extract the TFIDF feature from the title,
ingredients and cooking instructions of recipes. By determining the
significance of word sequences through combining LSTM learned features with
their TFIDF features, we encode a recipe into a TFIDF weighted vector for
capturing significant key terms and how such key terms are used in the
corresponding cooking instructions. Second, we combine the recipe TFIDF feature
with the recipe sequence feature extracted through two-stage LSTM networks,
which is effective in capturing the unique relationship between a recipe and
its associated image(s). Third, we further incorporate TFIDF enhanced category
semantics to improve the mapping of image modality and to regulate the
similarity loss function during the iterative learning of cross-modal joint
embedding. Experiments on the benchmark dataset Recipe1M show the proposed
approach outperforms the state-of-the-art approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1"&gt;Zhongwei Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Ling Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1"&gt;Yanzhao Wu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_L/0/1/0/all/0/1"&gt;Luo Zhong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Diverse Linguistic Features for Assessing Reading Difficulty of Educational Filipino Texts. (arXiv:2108.00241v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.00241</id>
        <link href="http://arxiv.org/abs/2108.00241"/>
        <updated>2021-08-03T02:06:29.368Z</updated>
        <summary type="html"><![CDATA[In order to ensure quality and effective learning, fluency, and
comprehension, the proper identification of the difficulty levels of reading
materials should be observed. In this paper, we describe the development of
automatic machine learning-based readability assessment models for educational
Filipino texts using the most diverse set of linguistic features for the
language. Results show that using a Random Forest model obtained a high
performance of 62.7% in terms of accuracy, and 66.1% when using the optimal
combination of feature sets consisting of traditional and syllable
pattern-based predictors.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Imperial_J/0/1/0/all/0/1"&gt;Joseph Marvin Imperial&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ong_E/0/1/0/all/0/1"&gt;Ethel Ong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Word2Pix: Word to Pixel Cross Attention Transformer in Visual Grounding. (arXiv:2108.00205v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00205</id>
        <link href="http://arxiv.org/abs/2108.00205"/>
        <updated>2021-08-03T02:06:29.360Z</updated>
        <summary type="html"><![CDATA[Current one-stage methods for visual grounding encode the language query as
one holistic sentence embedding before fusion with visual feature. Such a
formulation does not treat each word of a query sentence on par when modeling
language to visual attention, therefore prone to neglect words which are less
important for sentence embedding but critical for visual grounding. In this
paper we propose Word2Pix: a one-stage visual grounding network based on
encoder-decoder transformer architecture that enables learning for textual to
visual feature correspondence via word to pixel attention. The embedding of
each word from the query sentence is treated alike by attending to visual
pixels individually instead of single holistic sentence embedding. In this way,
each word is given equivalent opportunity to adjust the language to vision
attention towards the referent target through multiple stacks of transformer
decoder layers. We conduct the experiments on RefCOCO, RefCOCO+ and RefCOCOg
datasets and the proposed Word2Pix outperforms existing one-stage methods by a
notable margin. The results obtained also show that Word2Pix surpasses
two-stage visual grounding models, while at the same time keeping the merits of
one-stage paradigm namely end-to-end training and real-time inference speed
intact.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1"&gt;Heng Zhao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1"&gt;Joey Tianyi Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ong_Y/0/1/0/all/0/1"&gt;Yew-Soon Ong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Zero Attentive Relevance Matching Networkfor Review Modeling in Recommendation System. (arXiv:2101.06387v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.06387</id>
        <link href="http://arxiv.org/abs/2101.06387"/>
        <updated>2021-08-03T02:06:29.266Z</updated>
        <summary type="html"><![CDATA[User and item reviews are valuable for the construction of recommender
systems. In general, existing review-based methods for recommendation can be
broadly categorized into two groups: the siamese models that build static user
and item representations from their reviews respectively, and the
interaction-based models that encode user and item dynamically according to the
similarity or relationships of their reviews. Although the interaction-based
models have more model capacity and fit human purchasing behavior better,
several problematic model designs and assumptions of the existing
interaction-based models lead to its suboptimal performance compared to
existing siamese models. In this paper, we identify three problems of the
existing interaction-based recommendation models and propose a couple of
solutions as well as a new interaction-based model to incorporate review data
for rating prediction. Our model implements a relevance matching model with
regularized training losses to discover user relevant information from long
item reviews, and it also adapts a zero attention strategy to dynamically
balance the item-dependent and item-independent information extracted from user
reviews. Empirical experiments and case studies on Amazon Product Benchmark
datasets show that our model can extract effective and interpretable user/item
representations from their reviews and outperforms multiple types of
state-of-the-art review-based recommendation models.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_H/0/1/0/all/0/1"&gt;Hansi Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1"&gt;Zhichao Xu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ai_Q/0/1/0/all/0/1"&gt;Qingyao Ai&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Momentum-based Gradient Methods in Multi-Objective Recommendation. (arXiv:2009.04695v2 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2009.04695</id>
        <link href="http://arxiv.org/abs/2009.04695"/>
        <updated>2021-08-03T02:06:29.253Z</updated>
        <summary type="html"><![CDATA[Multi-objective gradient methods are becoming the standard for solving
multi-objective problems. Among others, they show promising results in
developing multi-objective recommender systems with both correlated and
conflicting objectives. Classic multi-gradient descent usually relies on the
combination of the gradients, not including the computation of first and second
moments of the gradients. This leads to a brittle behavior and misses important
areas in the solution space. In this work, we create a multi-objective
model-agnostic Adamize method that leverages the benefits of the Adam optimizer
in single-objective problems. This corrects and stabilizes the gradients of
every objective before calculating a common gradient descent vector that
optimizes all the objectives simultaneously. We evaluate the benefits of
multi-objective Adamize on two multi-objective recommender systems and for
three different objective combinations, both correlated or conflicting. We
report significant improvements, measured with three different Pareto front
metrics: hypervolume, coverage, and spacing. Finally, we show that the Adamized
Pareto front strictly dominates the previous one on multiple objective pairs.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mitrevski_B/0/1/0/all/0/1"&gt;Blagoj Mitrevski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Filipovic_M/0/1/0/all/0/1"&gt;Milena Filipovic&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Antognini_D/0/1/0/all/0/1"&gt;Diego Antognini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Glaude_E/0/1/0/all/0/1"&gt;Emma Lejal Glaude&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Faltings_B/0/1/0/all/0/1"&gt;Boi Faltings&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Musat_C/0/1/0/all/0/1"&gt;Claudiu Musat&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[The History of Speech Recognition to the Year 2030. (arXiv:2108.00084v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.00084</id>
        <link href="http://arxiv.org/abs/2108.00084"/>
        <updated>2021-08-03T02:06:29.238Z</updated>
        <summary type="html"><![CDATA[The decade from 2010 to 2020 saw remarkable improvements in automatic speech
recognition. Many people now use speech recognition on a daily basis, for
example to perform voice search queries, send text messages, and interact with
voice assistants like Amazon Alexa and Siri by Apple. Before 2010 most people
rarely used speech recognition. Given the remarkable changes in the state of
speech recognition over the previous decade, what can we expect over the coming
decade? I attempt to forecast the state of speech recognition research and
applications by the year 2030. While the changes to general speech recognition
accuracy will not be as dramatic as in the previous decade, I suggest we have
an exciting decade of progress in speech technology ahead of us.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Hannun_A/0/1/0/all/0/1"&gt;Awni Hannun&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Self-supervised Answer Retrieval on Clinical Notes. (arXiv:2108.00775v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2108.00775</id>
        <link href="http://arxiv.org/abs/2108.00775"/>
        <updated>2021-08-03T02:06:29.231Z</updated>
        <summary type="html"><![CDATA[Retrieving answer passages from long documents is a complex task requiring
semantic understanding of both discourse and document context. We approach this
challenge specifically in a clinical scenario, where doctors retrieve cohorts
of patients based on diagnoses and other latent medical aspects. We introduce
CAPR, a rule-based self-supervision objective for training Transformer language
models for domain-specific passage matching. In addition, we contribute a novel
retrieval dataset based on clinical notes to simulate this scenario on a large
corpus of clinical notes. We apply our objective in four Transformer-based
architectures: Contextual Document Vectors, Bi-, Poly- and Cross-encoders. From
our extensive evaluation on MIMIC-III and three other healthcare datasets, we
report that CAPR outperforms strong baselines in the retrieval of
domain-specific passages and effectively generalizes across rule-based and
human-labeled passages. This makes the model powerful especially in zero-shot
scenarios where only limited training data is available.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Grundmann_P/0/1/0/all/0/1"&gt;Paul Grundmann&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Arnold_S/0/1/0/all/0/1"&gt;Sebastian Arnold&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Loser_A/0/1/0/all/0/1"&gt;Alexander L&amp;#xf6;ser&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Jointly Optimizing Query Encoder and Product Quantization to Improve Retrieval Performance. (arXiv:2108.00644v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2108.00644</id>
        <link href="http://arxiv.org/abs/2108.00644"/>
        <updated>2021-08-03T02:06:29.167Z</updated>
        <summary type="html"><![CDATA[Recently, Information Retrieval community has witnessed fast-paced advances
in Dense Retrieval (DR), which performs first-stage retrieval by encoding
documents in a low-dimensional embedding space and querying them with
embedding-based search. Despite the impressive ranking performance, previous
studies usually adopt brute-force search to acquire candidates, which is
prohibitive in practical Web search scenarios due to its tremendous memory
usage and time cost. To overcome these problems, vector compression methods, a
branch of Approximate Nearest Neighbor Search (ANNS), have been adopted in many
practical embedding-based retrieval applications. One of the most popular
methods is Product Quantization (PQ). However, although existing vector
compression methods including PQ can help improve the efficiency of DR, they
incur severely decayed retrieval performance due to the separation between
encoding and compression. To tackle this problem, we present JPQ, which stands
for Joint optimization of query encoding and Product Quantization. It trains
the query encoder and PQ index jointly in an end-to-end manner based on three
optimization strategies, namely ranking-oriented loss, PQ centroid
optimization, and end-to-end negative sampling. We evaluate JPQ on two publicly
available retrieval benchmarks. Experimental results show that JPQ
significantly outperforms existing popular vector compression methods in terms
of different trade-off settings. Compared with previous DR models that use
brute-force search, JPQ almost matches the best retrieval performance with 30x
compression on index size. The compressed index further brings 10x speedup on
CPU and 2x speedup on GPU in query latency.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhan_J/0/1/0/all/0/1"&gt;Jingtao Zhan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mao_J/0/1/0/all/0/1"&gt;Jiaxin Mao&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1"&gt;Yiqun Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1"&gt;Jiafeng Guo&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1"&gt;Min Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1"&gt;Shaoping Ma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[WLV-RIT at GermEval 2021: Multitask Learning with Transformers to Detect Toxic, Engaging, and Fact-Claiming Comments. (arXiv:2108.00057v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.00057</id>
        <link href="http://arxiv.org/abs/2108.00057"/>
        <updated>2021-08-03T02:06:29.145Z</updated>
        <summary type="html"><![CDATA[This paper addresses the identification of toxic, engaging, and fact-claiming
comments on social media. We used the dataset made available by the organizers
of the GermEval-2021 shared task containing over 3,000 manually annotated
Facebook comments in German. Considering the relatedness of the three tasks, we
approached the problem using large pre-trained transformer models and multitask
learning. Our results indicate that multitask learning achieves performance
superior to the more common single task learning approach in all three tasks.
We submit our best systems to GermEval-2021 under the team name WLV-RIT.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Morgan_S/0/1/0/all/0/1"&gt;Skye Morgan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ranasinghe_T/0/1/0/all/0/1"&gt;Tharindu Ranasinghe&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zampieri_M/0/1/0/all/0/1"&gt;Marcos Zampieri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SamWalker++: recommendation with informative sampling strategy. (arXiv:2011.07734v2 [cs.IR] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2011.07734</id>
        <link href="http://arxiv.org/abs/2011.07734"/>
        <updated>2021-08-03T02:06:29.135Z</updated>
        <summary type="html"><![CDATA[Recommendation from implicit feedback is a highly challenging task due to the
lack of reliable negative feedback data. Existing methods address this
challenge by treating all the un-observed data as negative (dislike) but
downweight the confidence of these data. However, this treatment causes two
problems: (1) Confidence weights of the unobserved data are usually assigned
manually, which lack flexibility and may create empirical bias on evaluating
user's preference. (2) To handle massive volume of the unobserved feedback
data, most of the existing methods rely on stochastic inference and data
sampling strategies. However, since a user is only aware of a very small
fraction of items in a large dataset, it is difficult for existing samplers to
select informative training instances in which the user really dislikes the
item rather than does not know it.

To address the above two problems, we propose two novel recommendation
methods SamWalker and SamWalker++ that support both adaptive confidence
assignment and efficient model learning. SamWalker models data confidence with
a social network-aware function, which can adaptively specify different weights
to different data according to users' social contexts. However, the social
network information may not be available in many recommender systems, which
hinders application of SamWalker. Thus, we further propose SamWalker++, which
does not require any side information and models data confidence with a
constructed pseudo-social network. We also develop fast random-walk-based
sampling strategies for our SamWalker and SamWalker++ to adaptively draw
informative training instances, which can speed up gradient estimation and
reduce sampling variance. Extensive experiments on five real-world datasets
demonstrate the superiority of the proposed SamWalker and SamWalker++.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1"&gt;Can Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1"&gt;Jiawei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1"&gt;Sheng Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Shi_Q/0/1/0/all/0/1"&gt;Qihao Shi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1"&gt;Yan Feng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1"&gt;Chun Chen&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[M2Lens: Visualizing and Explaining Multimodal Models for Sentiment Analysis. (arXiv:2107.08264v3 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2107.08264</id>
        <link href="http://arxiv.org/abs/2107.08264"/>
        <updated>2021-08-03T02:06:28.916Z</updated>
        <summary type="html"><![CDATA[Multimodal sentiment analysis aims to recognize people's attitudes from
multiple communication channels such as verbal content (i.e., text), voice, and
facial expressions. It has become a vibrant and important research topic in
natural language processing. Much research focuses on modeling the complex
intra- and inter-modal interactions between different communication channels.
However, current multimodal models with strong performance are often
deep-learning-based techniques and work like black boxes. It is not clear how
models utilize multimodal information for sentiment predictions. Despite recent
advances in techniques for enhancing the explainability of machine learning
models, they often target unimodal scenarios (e.g., images, sentences), and
little research has been done on explaining multimodal models. In this paper,
we present an interactive visual analytics system, M2Lens, to visualize and
explain multimodal models for sentiment analysis. M2Lens provides explanations
on intra- and inter-modal interactions at the global, subset, and local levels.
Specifically, it summarizes the influence of three typical interaction types
(i.e., dominance, complement, and conflict) on the model predictions. Moreover,
M2Lens identifies frequent and influential multimodal features and supports the
multi-faceted exploration of model behaviors from language, acoustic, and
visual modalities. Through two case studies and expert interviews, we
demonstrate our system can help users gain deep insights into the multimodal
models for sentiment analysis.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1"&gt;Xingbo Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1"&gt;Jianben He&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1"&gt;Zhihua Jin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1"&gt;Muqiao Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1"&gt;Yong Wang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Qu_H/0/1/0/all/0/1"&gt;Huamin Qu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[An Empirical analysis on Transparent Algorithmic Exploration in Recommender Systems. (arXiv:2108.00151v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2108.00151</id>
        <link href="http://arxiv.org/abs/2108.00151"/>
        <updated>2021-08-03T02:06:28.832Z</updated>
        <summary type="html"><![CDATA[All learning algorithms for recommendations face inevitable and critical
trade-off between exploiting partial knowledge of a user's preferences for
short-term satisfaction and exploring additional user preferences for long-term
coverage. Although exploration is indispensable for long success of a
recommender system, the exploration has been considered as the risk to decrease
user satisfaction. The reason for the risk is that items chosen for exploration
frequently mismatch with the user's interests. To mitigate this risk,
recommender systems have mixed items chosen for exploration into a
recommendation list, disguising the items as recommendations to elicit feedback
on the items to discover the user's additional tastes. This mix-in approach has
been widely used in many recommenders, but there is rare research, evaluating
the effectiveness of the mix-in approach or proposing a new approach for
eliciting user feedback without deceiving users. In this work, we aim to
propose a new approach for feedback elicitation without any deception and
compare our approach to the conventional mix-in approach for evaluation. To
this end, we designed a recommender interface that reveals which items are for
exploration and conducted a within-subject study with 94 MTurk workers. Our
results indicated that users left significantly more feedback on items chosen
for exploration with our interface. Besides, users evaluated that our new
interface is better than the conventional mix-in interface in terms of novelty,
diversity, transparency, trust, and satisfaction. Finally, path analysis show
that, in only our new interface, exploration caused to increase user-centric
evaluation metrics. Our work paves the way for how to design an interface,
which utilizes learning algorithm based on users' feedback signals, giving
better user experience and gathering more feedback data.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1"&gt;Kihwan Kim&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[SurpriseNet: Melody Harmonization Conditioning on User-controlled Surprise Contours. (arXiv:2108.00378v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2108.00378</id>
        <link href="http://arxiv.org/abs/2108.00378"/>
        <updated>2021-08-03T02:06:28.809Z</updated>
        <summary type="html"><![CDATA[The surprisingness of a song is an essential and seemingly subjective factor
in determining whether the listener likes it. With the help of information
theory, it can be described as the transition probability of a music sequence
modeled as a Markov chain. In this study, we introduce the concept of deriving
entropy variations over time, so that the surprise contour of each chord
sequence can be extracted. Based on this, we propose a user-controllable
framework that uses a conditional variational autoencoder (CVAE) to harmonize
the melody based on the given chord surprise indication. Through explicit
conditions, the model can randomly generate various and harmonic chord
progressions for a melody, and the Spearman's correlation and p-value
significance show that the resulting chord progressions match the given
surprise contour quite well. The vanilla CVAE model was evaluated in a basic
melody harmonization task (no surprise control) in terms of six objective
metrics. The results of experiments on the Hooktheory Lead Sheet Dataset show
that our model achieves performance comparable to the state-of-the-art melody
harmonization model.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yi-Wei Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1"&gt;Hung-Shin Lee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1"&gt;Yen-Hsing Chen&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1"&gt;Hsin-Min Wang&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[BigGraphVis: Leveraging Streaming Algorithms and GPU Acceleration for Visualizing Big Graphs. (arXiv:2108.00529v1 [cs.DC])]]></title>
        <id>http://arxiv.org/abs/2108.00529</id>
        <link href="http://arxiv.org/abs/2108.00529"/>
        <updated>2021-08-03T02:06:28.800Z</updated>
        <summary type="html"><![CDATA[Graph layouts are key to exploring massive graphs. An enormous number of
nodes and edges do not allow network analysis software to produce meaningful
visualization of the pervasive networks. Long computation time, memory and
display limitations encircle the software's ability to explore massive graphs.
This paper introduces BigGraphVis, a new parallel graph visualization method
that uses GPU parallel processing and community detection algorithm to
visualize graph communities. We combine parallelized streaming community
detection algorithm and probabilistic data structure to leverage parallel
processing of Graphics Processing Unit (GPU). To the best of our knowledge,
this is the first attempt to combine the power of streaming algorithms coupled
with GPU computing to tackle big graph visualization challenges. Our method
extracts community information in a few passes on the edge list, and renders
the community structures using the ForceAtlas2 algorithm. Our experiment with
massive real-life graphs indicates that about 70 to 95 percent speedup can be
achieved by visualizing graph communities, and the visualization appears to be
meaningful and reliable. The biggest graph that we examined contains above 3
million nodes and 34 million edges, and the layout computation took about five
minutes. We also observed that the BigGraphVis coloring strategy can be
successfully applied to produce a more informative ForceAtlas2 layout.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Moradi_E/0/1/0/all/0/1"&gt;Ehsan Moradi&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Mondal_D/0/1/0/all/0/1"&gt;Debajyoti Mondal&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Speech2AffectiveGestures: Synthesizing Co-Speech Gestures with Generative Adversarial Affective Expression Learning. (arXiv:2108.00262v1 [cs.MM])]]></title>
        <id>http://arxiv.org/abs/2108.00262</id>
        <link href="http://arxiv.org/abs/2108.00262"/>
        <updated>2021-08-03T02:06:28.790Z</updated>
        <summary type="html"><![CDATA[We present a generative adversarial network to synthesize 3D pose sequences
of co-speech upper-body gestures with appropriate affective expressions. Our
network consists of two components: a generator to synthesize gestures from a
joint embedding space of features encoded from the input speech and the seed
poses, and a discriminator to distinguish between the synthesized pose
sequences and real 3D pose sequences. We leverage the Mel-frequency cepstral
coefficients and the text transcript computed from the input speech in separate
encoders in our generator to learn the desired sentiments and the associated
affective cues. We design an affective encoder using multi-scale
spatial-temporal graph convolutions to transform 3D pose sequences into latent,
pose-based affective features. We use our affective encoder in both our
generator, where it learns affective features from the seed poses to guide the
gesture synthesis, and our discriminator, where it enforces the synthesized
gestures to contain the appropriate affective expressions. We perform extensive
evaluations on two benchmark datasets for gesture synthesis from the speech,
the TED Gesture Dataset and the GENEA Challenge 2020 Dataset. Compared to the
best baselines, we improve the mean absolute joint error by 10--33%, the mean
acceleration difference by 8--58%, and the Fr\'echet Gesture Distance by
21--34%. We also conduct a user study and observe that compared to the best
current baselines, around 15.28% of participants indicated our synthesized
gestures appear more plausible, and around 16.32% of participants felt the
gestures had more appropriate affective expressions aligned with the speech.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bhattacharya_U/0/1/0/all/0/1"&gt;Uttaran Bhattacharya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Childs_E/0/1/0/all/0/1"&gt;Elizabeth Childs&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rewkowski_N/0/1/0/all/0/1"&gt;Nicholas Rewkowski&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Manocha_D/0/1/0/all/0/1"&gt;Dinesh Manocha&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[ECLARE: Extreme Classification with Label Graph Correlations. (arXiv:2108.00261v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.00261</id>
        <link href="http://arxiv.org/abs/2108.00261"/>
        <updated>2021-08-03T02:06:28.775Z</updated>
        <summary type="html"><![CDATA[Deep extreme classification (XC) seeks to train deep architectures that can
tag a data point with its most relevant subset of labels from an extremely
large label set. The core utility of XC comes from predicting labels that are
rarely seen during training. Such rare labels hold the key to personalized
recommendations that can delight and surprise a user. However, the large number
of rare labels and small amount of training data per rare label offer
significant statistical and computational challenges. State-of-the-art deep XC
methods attempt to remedy this by incorporating textual descriptions of labels
but do not adequately address the problem. This paper presents ECLARE, a
scalable deep learning architecture that incorporates not only label text, but
also label correlations, to offer accurate real-time predictions within a few
milliseconds. Core contributions of ECLARE include a frugal architecture and
scalable techniques to train deep models along with label correlation graphs at
the scale of millions of labels. In particular, ECLARE offers predictions that
are 2 to 14% more accurate on both publicly available benchmark datasets as
well as proprietary datasets for a related products recommendation task sourced
from the Bing search engine. Code for ECLARE is available at
https://github.com/Extreme-classification/ECLARE.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mittal_A/0/1/0/all/0/1"&gt;Anshul Mittal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Sachdeva_N/0/1/0/all/0/1"&gt;Noveen Sachdeva&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agrawal_S/0/1/0/all/0/1"&gt;Sheshansh Agrawal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1"&gt;Sumeet Agarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kar_P/0/1/0/all/0/1"&gt;Purushottam Kar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Varma_M/0/1/0/all/0/1"&gt;Manik Varma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Relevance ranking for proximity full-text search based on additional indexes with multi-component keys. (arXiv:2108.00410v1 [cs.IR])]]></title>
        <id>http://arxiv.org/abs/2108.00410</id>
        <link href="http://arxiv.org/abs/2108.00410"/>
        <updated>2021-08-03T02:06:28.765Z</updated>
        <summary type="html"><![CDATA[The problem of proximity full-text search is considered. If a search query
contains high-frequently occurring words, then multi-component key indexes
deliver an improvement in the search speed compared with ordinary inverted
indexes. It was shown that we can increase the search speed by up to 130 times
in cases when queries consist of high-frequently occurring words. In this
paper, we investigate how the multi-component key index architecture affects
the quality of the search. We consider several well-known methods of relevance
ranking, where these methods are of different authors. Using these methods, we
perform the search in the ordinary inverted index and then in an index enhanced
with multi-component key indexes. The results show that with multi-component
key indexes we obtain search results that are very close, in terms of relevance
ranking, to the search results that are obtained by means of ordinary inverted
indexes.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Veretennikov_A/0/1/0/all/0/1"&gt;Alexander B. Veretennikov&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[DECAF: Deep Extreme Classification with Label Features. (arXiv:2108.00368v1 [cs.CL])]]></title>
        <id>http://arxiv.org/abs/2108.00368</id>
        <link href="http://arxiv.org/abs/2108.00368"/>
        <updated>2021-08-03T02:06:28.738Z</updated>
        <summary type="html"><![CDATA[Extreme multi-label classification (XML) involves tagging a data point with
its most relevant subset of labels from an extremely large label set, with
several applications such as product-to-product recommendation with millions of
products. Although leading XML algorithms scale to millions of labels, they
largely ignore label meta-data such as textual descriptions of the labels. On
the other hand, classical techniques that can utilize label metadata via
representation learning using deep networks struggle in extreme settings. This
paper develops the DECAF algorithm that addresses these challenges by learning
models enriched by label metadata that jointly learn model parameters and
feature representations using deep networks and offer accurate classification
at the scale of millions of labels. DECAF makes specific contributions to model
architecture design, initialization, and training, enabling it to offer up to
2-6% more accurate prediction than leading extreme classifiers on publicly
available benchmark product-to-product recommendation datasets, such as
LF-AmazonTitles-1.3M. At the same time, DECAF was found to be up to 22x faster
at inference than leading deep extreme classifiers, which makes it suitable for
real-time applications that require predictions within a few milliseconds. The
code for DECAF is available at the following URL
https://github.com/Extreme-classification/DECAF.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Mittal_A/0/1/0/all/0/1"&gt;Anshul Mittal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Dahiya_K/0/1/0/all/0/1"&gt;Kunal Dahiya&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agrawal_S/0/1/0/all/0/1"&gt;Sheshansh Agrawal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Saini_D/0/1/0/all/0/1"&gt;Deepak Saini&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1"&gt;Sumeet Agarwal&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kar_P/0/1/0/all/0/1"&gt;Purushottam Kar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Varma_M/0/1/0/all/0/1"&gt;Manik Varma&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Pose-Guided Feature Learning with Knowledge Distillation for Occluded Person Re-Identification. (arXiv:2108.00139v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00139</id>
        <link href="http://arxiv.org/abs/2108.00139"/>
        <updated>2021-08-03T02:06:28.717Z</updated>
        <summary type="html"><![CDATA[Occluded person re-identification (ReID) aims to match person images with
occlusion. It is fundamentally challenging because of the serious occlusion
which aggravates the misalignment problem between images. At the cost of
incorporating a pose estimator, many works introduce pose information to
alleviate the misalignment in both training and testing. To achieve high
accuracy while preserving low inference complexity, we propose a network named
Pose-Guided Feature Learning with Knowledge Distillation (PGFL-KD), where the
pose information is exploited to regularize the learning of semantics aligned
features but is discarded in testing. PGFL-KD consists of a main branch (MB),
and two pose-guided branches, \ieno, a foreground-enhanced branch (FEB), and a
body part semantics aligned branch (SAB). The FEB intends to emphasise the
features of visible body parts while excluding the interference of obstructions
and background (\ieno, foreground feature alignment). The SAB encourages
different channel groups to focus on different body parts to have body part
semantics aligned representation. To get rid of the dependency on pose
information when testing, we regularize the MB to learn the merits of the FEB
and SAB through knowledge distillation and interaction-based training.
Extensive experiments on occluded, partial, and holistic ReID tasks show the
effectiveness of our proposed network.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zheng_K/0/1/0/all/0/1"&gt;Kecheng Zheng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lan_C/0/1/0/all/0/1"&gt;Cuiling Lan&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1"&gt;Wenjun Zeng&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1"&gt;Jiawei Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1"&gt;Zhizheng Zhang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zha_Z/0/1/0/all/0/1"&gt;Zheng-Jun Zha&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Efficient Deep Feature Calibration for Cross-Modal Joint Embedding Learning. (arXiv:2108.00705v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00705</id>
        <link href="http://arxiv.org/abs/2108.00705"/>
        <updated>2021-08-03T02:06:28.695Z</updated>
        <summary type="html"><![CDATA[This paper introduces a two-phase deep feature calibration framework for
efficient learning of semantics enhanced text-image cross-modal joint
embedding, which clearly separates the deep feature calibration in data
preprocessing from training the joint embedding model. We use the Recipe1M
dataset for the technical description and empirical validation. In
preprocessing, we perform deep feature calibration by combining deep feature
engineering with semantic context features derived from raw text-image input
data. We leverage LSTM to identify key terms, NLP methods to produce ranking
scores for key terms before generating the key term feature. We leverage
wideResNet50 to extract and encode the image category semantics to help
semantic alignment of the learned recipe and image embeddings in the joint
latent space. In joint embedding learning, we perform deep feature calibration
by optimizing the batch-hard triplet loss function with soft-margin and double
negative sampling, also utilizing the category-based alignment loss and
discriminator-based alignment loss. Extensive experiments demonstrate that our
SEJE approach with the deep feature calibration significantly outperforms the
state-of-the-art approaches.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1"&gt;Zhongwei Xie&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1"&gt;Ling Liu&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1"&gt;Lin Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Zhong_L/0/1/0/all/0/1"&gt;Luo Zhong&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Point-to-Distribution Joint Geometry and Color Metric for Point Cloud Quality Assessment. (arXiv:2108.00054v1 [cs.MM])]]></title>
        <id>http://arxiv.org/abs/2108.00054</id>
        <link href="http://arxiv.org/abs/2108.00054"/>
        <updated>2021-08-03T02:06:28.684Z</updated>
        <summary type="html"><![CDATA[Point clouds (PCs) are a powerful 3D visual representation paradigm for many
emerging application domains, especially virtual and augmented reality, and
autonomous vehicles. However, the large amount of PC data required for highly
immersive and realistic experiences requires the availability of efficient,
lossy PC coding solutions are critical. Recently, two MPEG PC coding standards
have been developed to address the relevant application requirements and
further developments are expected in the future. In this context, the
assessment of PC quality, notably for decoded PCs, is critical and asks for the
design of efficient objective PC quality metrics. In this paper, a novel
point-to-distribution metric is proposed for PC quality assessment considering
both the geometry and texture. This new quality metric exploits the
scale-invariance property of the Mahalanobis distance to assess first the
geometry and color point-to-distribution distortions, which are after fused to
obtain a joint geometry and color quality metric. The proposed quality metric
significantly outperforms the best PC quality assessment metrics in the
literature.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Javaheri_A/0/1/0/all/0/1"&gt;Alireza Javaheri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Brites_C/0/1/0/all/0/1"&gt;Catarina Brites&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Pereira_F/0/1/0/all/0/1"&gt;Fernando Pereira&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ascenso_J/0/1/0/all/0/1"&gt;Jo&amp;#xe3;o Ascenso&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[End to End Bangla Speech Synthesis. (arXiv:2108.00500v1 [cs.SD])]]></title>
        <id>http://arxiv.org/abs/2108.00500</id>
        <link href="http://arxiv.org/abs/2108.00500"/>
        <updated>2021-08-03T02:06:28.669Z</updated>
        <summary type="html"><![CDATA[Text-to-Speech (TTS) system is a system where speech is synthesized from a
given text following any particular approach. Concatenative synthesis, Hidden
Markov Model (HMM) based synthesis, Deep Learning (DL) based synthesis with
multiple building blocks, etc. are the main approaches for implementing a TTS
system. Here, we are presenting our deep learning-based end-to-end Bangla
speech synthesis system. It has been implemented with minimal human annotation
using only 3 major components (Encoder, Decoder, Post-processing net including
waveform synthesis). It does not require any frontend preprocessor and
Grapheme-to-Phoneme (G2P) converter. Our model has been trained with
phonetically balanced 20 hours of single speaker speech data. It has obtained a
3.79 Mean Opinion Score (MOS) on a scale of 5.0 as subjective evaluation and a
0.77 Perceptual Evaluation of Speech Quality(PESQ) score on a scale of [-0.5,
4.5] as objective evaluation. It is outperforming all existing non-commercial
state-of-the-art Bangla TTS systems based on naturalness.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Bhattacharjee_P/0/1/0/all/0/1"&gt;Prithwiraj Bhattacharjee&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Raju_R/0/1/0/all/0/1"&gt;Rajan Saha Raju&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ahmad_A/0/1/0/all/0/1"&gt;Arif Ahmad&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1"&gt;M. Shahidur Rahman&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[A Unified Batch Selection Policy for Active Metric Learning. (arXiv:2102.07365v5 [cs.LG] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2102.07365</id>
        <link href="http://arxiv.org/abs/2102.07365"/>
        <updated>2021-08-03T02:06:28.648Z</updated>
        <summary type="html"><![CDATA[Active metric learning is the problem of incrementally selecting high-utility
batches of training data (typically, ordered triplets) to annotate, in order to
progressively improve a learned model of a metric over some input domain as
rapidly as possible. Standard approaches, which independently assess the
informativeness of each triplet in a batch, are susceptible to highly
correlated batches with many redundant triplets and hence low overall utility.
While a recent work \cite{kumari2020batch} proposes batch-decorrelation
strategies for metric learning, they rely on ad hoc heuristics to estimate the
correlation between two triplets at a time. We present a novel batch active
metric learning method that leverages the Maximum Entropy Principle to learn
the least biased estimate of triplet distribution for a given set of prior
constraints. To avoid redundancy between triplets, our method collectively
selects batches with maximum joint entropy, which simultaneously captures both
informativeness and diversity. We take advantage of the submodularity of the
joint entropy function to construct a tractable solution using an efficient
greedy algorithm based on Gram-Schmidt orthogonalization that is provably
$\left( 1 - \frac{1}{e} \right)$-optimal. Our approach is the first batch
active metric learning method to define a unified score that balances
informativeness and diversity for an entire batch of triplets. Experiments with
several real-world datasets demonstrate that our algorithm is robust,
generalizes well to different applications and input modalities, and
consistently outperforms the state-of-the-art.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+K_P/0/1/0/all/0/1"&gt;Priyadarshini K&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chaudhuri_S/0/1/0/all/0/1"&gt;Siddhartha Chaudhuri&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Borkar_V/0/1/0/all/0/1"&gt;Vivek Borkar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Chaudhuri_S/0/1/0/all/0/1"&gt;Subhasis Chaudhuri&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[AI Choreographer: Music Conditioned 3D Dance Generation with AIST++. (arXiv:2101.08779v3 [cs.CV] UPDATED)]]></title>
        <id>http://arxiv.org/abs/2101.08779</id>
        <link href="http://arxiv.org/abs/2101.08779"/>
        <updated>2021-08-03T02:06:28.610Z</updated>
        <summary type="html"><![CDATA[We present AIST++, a new multi-modal dataset of 3D dance motion and music,
along with FACT, a Full-Attention Cross-modal Transformer network for
generating 3D dance motion conditioned on music. The proposed AIST++ dataset
contains 5.2 hours of 3D dance motion in 1408 sequences, covering 10 dance
genres with multi-view videos with known camera poses -- the largest dataset of
this kind to our knowledge. We show that naively applying sequence models such
as transformers to this dataset for the task of music conditioned 3D motion
generation does not produce satisfactory 3D motion that is well correlated with
the input music. We overcome these shortcomings by introducing key changes in
its architecture design and supervision: FACT model involves a deep cross-modal
transformer block with full-attention that is trained to predict $N$ future
motions. We empirically show that these changes are key factors in generating
long sequences of realistic dance motion that are well-attuned to the input
music. We conduct extensive experiments on AIST++ with user studies, where our
method outperforms recent state-of-the-art methods both qualitatively and
quantitatively.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1"&gt;Ruilong Li&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1"&gt;Shan Yang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Ross_D/0/1/0/all/0/1"&gt;David A. Ross&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Kanazawa_A/0/1/0/all/0/1"&gt;Angjoo Kanazawa&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Multimodal Feature Fusion for Video Advertisements Tagging Via Stacking Ensemble. (arXiv:2108.00679v1 [cs.CV])]]></title>
        <id>http://arxiv.org/abs/2108.00679</id>
        <link href="http://arxiv.org/abs/2108.00679"/>
        <updated>2021-08-03T02:06:28.590Z</updated>
        <summary type="html"><![CDATA[Automated tagging of video advertisements has been a critical yet challenging
problem, and it has drawn increasing interests in last years as its
applications seem to be evident in many fields. Despite sustainable efforts
have been made, the tagging task is still suffered from several challenges,
such as, efficiently feature fusion approach is desirable, but under-explored
in previous studies. In this paper, we present our approach for Multimodal
Video Ads Tagging in the 2021 Tencent Advertising Algorithm Competition.
Specifically, we propose a novel multi-modal feature fusion framework, with the
goal to combine complementary information from multiple modalities. This
framework introduces stacking-based ensembling approach to reduce the influence
of varying levels of noise and conflicts between different modalities. Thus,
our framework can boost the performance of the tagging task, compared to
previous methods. To empirically investigate the effectiveness and robustness
of the proposed framework, we conduct extensive experiments on the challenge
datasets. The obtained results suggest that our framework can significantly
outperform related approaches and our method ranks as the 1st place on the
final leaderboard, with a Global Average Precision (GAP) of 82.63%. To better
promote the research in this field, we will release our code in the final
version.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1"&gt;Qingsong Zhou&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Liang_H/0/1/0/all/0/1"&gt;Hai Liang&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1"&gt;Zhimin Lin&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1"&gt;Kele Xu&lt;/a&gt;</name>
        </author>
    </entry>
    <entry>
        <title type="html"><![CDATA[Private Retrieval, Computing and Learning: Recent Progress and Future Challenges. (arXiv:2108.00026v1 [cs.CR])]]></title>
        <id>http://arxiv.org/abs/2108.00026</id>
        <link href="http://arxiv.org/abs/2108.00026"/>
        <updated>2021-08-03T02:06:28.542Z</updated>
        <summary type="html"><![CDATA[Most of our lives are conducted in the cyberspace. The human notion of
privacy translates into a cyber notion of privacy on many functions that take
place in the cyberspace. This article focuses on three such functions: how to
privately retrieve information from cyberspace (privacy in information
retrieval), how to privately leverage large-scale distributed/parallel
processing (privacy in distributed computing), and how to learn/train machine
learning models from private data spread across multiple users (privacy in
distributed (federated) learning). The article motivates each privacy setting,
describes the problem formulation, summarizes breakthrough results in the
history of each problem, and gives recent results and discusses some of the
major ideas that emerged in each field. In addition, the cross-cutting
techniques and interconnections between the three topics are discussed along
with a set of open problems and challenges.]]></summary>
        <author>
            <name>&lt;a href="http://arxiv.org/find/cs/1/au:+Ulukus_S/0/1/0/all/0/1"&gt;Sennur Ulukus&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Avestimehr_S/0/1/0/all/0/1"&gt;Salman Avestimehr&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Gastpar_M/0/1/0/all/0/1"&gt;Michael Gastpar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Jafar_S/0/1/0/all/0/1"&gt;Syed Jafar&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tandon_R/0/1/0/all/0/1"&gt;Ravi Tandon&lt;/a&gt;, &lt;a href="http://arxiv.org/find/cs/1/au:+Tian_C/0/1/0/all/0/1"&gt;Chao Tian&lt;/a&gt;</name>
        </author>
    </entry>
</feed>