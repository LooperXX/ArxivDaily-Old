{
  "sources": [
    {
      "title": "cs.CL updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.CL",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2105.01542",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luu_S/0/1/0/all/0/1\">Son T. Luu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_M/0/1/0/all/0/1\">Mao Nguyen Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_L/0/1/0/all/0/1\">Loi Duc Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_K/0/1/0/all/0/1\">Khiem Vinh Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1\">Kiet Van Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1\">Ngan Luu-Thuy Nguyen</a>",
          "description": "Machine reading comprehension (MRC) is a sub-field in natural language\nprocessing that aims to assist computers understand unstructured texts and then\nanswer questions related to them. In practice, the conversation is an essential\nway to communicate and transfer information. To help machines understand\nconversation texts, we present UIT-ViCoQA, a new corpus for conversational\nmachine reading comprehension in the Vietnamese language. This corpus consists\nof 10,000 questions with answers over 2,000 conversations about health news\narticles. Then, we evaluate several baseline approaches for conversational\nmachine comprehension on the UIT-ViCoQA corpus. The best model obtains an F1\nscore of 45.27%, which is 30.91 points behind human performance (76.18%),\nindicating that there is ample room for improvement. Our dataset is available\nat our website: this http URL for research purposes.",
          "link": "http://arxiv.org/abs/2105.01542",
          "publishedOn": "2021-07-05T01:54:57.338Z",
          "wordCount": 637,
          "title": "Conversational Machine Reading Comprehension for Vietnamese Healthcare Texts. (arXiv:2105.01542v5 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.01616",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1\">Heng-Jui Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_L/0/1/0/all/0/1\">Lin-shan Lee</a>",
          "description": "Automatic speech recognition (ASR) technologies today are primarily optimized\nfor given datasets; thus, any changes in the application environment (e.g.,\nacoustic conditions or topic domains) may inevitably degrade the performance.\nWe can collect new data describing the new environment and fine-tune the\nsystem, but this naturally leads to higher error rates for the earlier\ndatasets, referred to as catastrophic forgetting. The concept of lifelong\nlearning (LLL) aiming to enable a machine to sequentially learn new tasks from\nnew datasets describing the changing real world without forgetting the\npreviously learned knowledge is thus brought to attention. This paper reports,\nto our knowledge, the first effort to extensively consider and analyze the use\nof various approaches of LLL in end-to-end (E2E) ASR, including proposing novel\nmethods in saving data for past domains to mitigate the catastrophic forgetting\nproblem. An overall relative reduction of 28.7% in WER was achieved compared to\nthe fine-tuning baseline when sequentially learning on three very different\nbenchmark corpora. This can be the first step toward the highly desired ASR\ntechnologies capable of synchronizing with the continuously changing real\nworld.",
          "link": "http://arxiv.org/abs/2104.01616",
          "publishedOn": "2021-07-05T01:54:57.284Z",
          "wordCount": 663,
          "title": "Towards Lifelong Learning of End-to-end ASR. (arXiv:2104.01616v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.12708",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pritzen_J/0/1/0/all/0/1\">Julia Pritzen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gref_M/0/1/0/all/0/1\">Michael Gref</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuhlke_D/0/1/0/all/0/1\">Dietlind Z&#xfc;hlke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_C/0/1/0/all/0/1\">Christoph Schmidt</a>",
          "description": "Loanwords, such as Anglicisms, are a challenge in German speech recognition.\nDue to their irregular pronunciation compared to native German words,\nautomatically generated pronunciation dictionaries often include faulty phoneme\nsequences for Anglicisms. In this work, we propose a multitask\nsequence-to-sequence approach for grapheme-to-phoneme conversion to improve the\nphonetization of Anglicisms. We extended a grapheme-to-phoneme model with a\nclassifier to distinguish Anglicisms from native German words. With this\napproach, the model learns to generate pronunciations differently depending on\nthe classification result. We used our model to create supplementary Anglicism\npronunciation dictionaries that are added to an existing German speech\nrecognition model. Tested on a dedicated Anglicism evaluation set, we improved\nthe recognition of Anglicisms compared to a baseline model, reducing the word\nerror rate by 1 % and the Anglicism error rate by 3 %. We show that multitask\nlearning can help solving the challenge of loanwords in German speech\nrecognition.",
          "link": "http://arxiv.org/abs/2105.12708",
          "publishedOn": "2021-07-05T01:54:57.265Z",
          "wordCount": 629,
          "title": "Multitask Learning for Grapheme-to-Phoneme Conversion of Anglicisms in German Speech Recognition. (arXiv:2105.12708v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12672",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1\">Yi Tay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_V/0/1/0/all/0/1\">Vinh Q. Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruder_S/0/1/0/all/0/1\">Sebastian Ruder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_J/0/1/0/all/0/1\">Jai Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_H/0/1/0/all/0/1\">Hyung Won Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bahri_D/0/1/0/all/0/1\">Dara Bahri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1\">Zhen Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baumgartner_S/0/1/0/all/0/1\">Simon Baumgartner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1\">Cong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metzler_D/0/1/0/all/0/1\">Donald Metzler</a>",
          "description": "State-of-the-art models in natural language processing rely on separate rigid\nsubword tokenization algorithms, which limit their generalization ability and\nadaptation to new settings. In this paper, we propose a new model inductive\nbias that learns a subword tokenization end-to-end as part of the model. To\nthis end, we introduce a soft gradient-based subword tokenization module (GBST)\nthat automatically learns latent subword representations from characters in a\ndata-driven fashion. Concretely, GBST enumerates candidate subword blocks and\nlearns to score them in a position-wise fashion using a block scoring network.\nWe additionally introduce Charformer, a deep Transformer model that integrates\nGBST and operates on the byte level. Via extensive experiments on English GLUE,\nmultilingual, and noisy text datasets, we show that Charformer outperforms a\nseries of competitive byte-level baselines while generally performing on par\nand sometimes outperforming subword-based models. Additionally, Charformer is\nfast, improving the speed of both vanilla byte-level and subword-level\nTransformers by 28%-100% while maintaining competitive quality. We believe this\nwork paves the way for highly performant token-free models that are trained\ncompletely end-to-end.",
          "link": "http://arxiv.org/abs/2106.12672",
          "publishedOn": "2021-07-05T01:54:57.257Z",
          "wordCount": 665,
          "title": "Charformer: Fast Character Transformers via Gradient-based Subword Tokenization. (arXiv:2106.12672v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.03979",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ding_T/0/1/0/all/0/1\">Th&#xe9;o Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vermeiren_W/0/1/0/all/0/1\">Walter Vermeiren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ranwez_S/0/1/0/all/0/1\">Sylvie Ranwez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1\">Binbin Xu</a>",
          "description": "Patent analysis and mining are time-consuming and costly processes for\ncompanies, but nevertheless essential if they are willing to remain\ncompetitive. To face the overload induced by numerous patents, the idea is to\nautomatically filter them, bringing only few to read to experts. This paper\nreports a successful application of fine-tuning and retraining on pre-trained\ndeep Natural Language Processing models on patent classification. The solution\nthat we propose combines several state-of-the-art treatments to achieve our\ngoal - decrease the workload while preserving recall and precision metrics.",
          "link": "http://arxiv.org/abs/2105.03979",
          "publishedOn": "2021-07-05T01:54:57.250Z",
          "wordCount": 557,
          "title": "Improving Patent Mining and Relevance Classification using Transformers. (arXiv:2105.03979v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.14465",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Costa_jussa_M/0/1/0/all/0/1\">Marta R. Costa-juss&#xe0;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basta_C/0/1/0/all/0/1\">Christine Basta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gallego_G/0/1/0/all/0/1\">Gerard I. G&#xe1;llego</a>",
          "description": "The scientific community is increasingly aware of the necessity to embrace\npluralism and consistently represent major and minor social groups. Currently,\nthere are no standard evaluation techniques for different types of biases.\nAccordingly, there is an urgent need to provide evaluation sets and protocols\nto measure existing biases in our automatic systems. Evaluating the biases\nshould be an essential step towards mitigating them in the systems.\n\nThis paper introduces WinoST, a new freely available challenge set for\nevaluating gender bias in speech translation. WinoST is the speech version of\nWinoMT which is a MT challenge set and both follow an evaluation protocol to\nmeasure gender accuracy. Using a state-of-the-art end-to-end speech translation\nsystem, we report the gender bias evaluation on four language pairs and we show\nthat gender accuracy in speech translation is more than 23% lower than in MT.",
          "link": "http://arxiv.org/abs/2010.14465",
          "publishedOn": "2021-07-05T01:54:57.243Z",
          "wordCount": 611,
          "title": "Evaluating Gender Bias in Speech Translation. (arXiv:2010.14465v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.01691",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Toan Q. Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murray_K/0/1/0/all/0/1\">Kenton Murray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiang_D/0/1/0/all/0/1\">David Chiang</a>",
          "description": "In this paper, we investigate the driving factors behind concatenation, a\nsimple but effective data augmentation method for low-resource neural machine\ntranslation. Our experiments suggest that discourse context is unlikely the\ncause for the improvement of about +1 BLEU across four language pairs. Instead,\nwe demonstrate that the improvement comes from three other factors unrelated to\ndiscourse: context diversity, length diversity, and (to a lesser extent)\nposition shifting.",
          "link": "http://arxiv.org/abs/2105.01691",
          "publishedOn": "2021-07-05T01:54:57.227Z",
          "wordCount": 542,
          "title": "Data Augmentation by Concatenation for Low-Resource Translation: A Mystery and a Solution. (arXiv:2105.01691v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.00171",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">Yao-Fei Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-Shin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hsin-Min Wang</a>",
          "description": "The end-to-end architecture has made promising progress in speech translation\n(ST). However, the ST task is still challenging under low-resource conditions.\nMost ST models have shown unsatisfactory results, especially in the absence of\nword information from the source speech utterance. In this study, we survey\nmethods to improve ST performance without using source transcription, and\npropose a learning framework that utilizes a language-independent universal\nphone recognizer. The framework is based on an attention-based\nsequence-to-sequence model, where the encoder generates the phonetic embeddings\nand phone-aware acoustic representations, and the decoder controls the fusion\nof the two embedding streams to produce the target token sequence. In addition\nto investigating different fusion strategies, we explore the specific usage of\nbyte pair encoding (BPE), which compresses a phone sequence into a\nsyllable-like segmented sequence. Due to the conversion of symbols, a segmented\nsequence represents not only pronunciation but also language-dependent\ninformation lacking in phones. Experiments conducted on the Fisher\nSpanish-English and Taigi-Mandarin drama corpora show that our method\noutperforms the conformer-based baseline, and the performance is close to that\nof the existing best method using source transcription.",
          "link": "http://arxiv.org/abs/2105.00171",
          "publishedOn": "2021-07-05T01:54:57.220Z",
          "wordCount": 649,
          "title": "AlloST: Low-resource Speech Translation without Source Transcription. (arXiv:2105.00171v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.06387",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Pengfei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jinlan Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yang Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_W/0/1/0/all/0/1\">Weizhe Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shuaicheng Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1\">Junqi Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yixin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Z/0/1/0/all/0/1\">Zihuiwen Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_Z/0/1/0/all/0/1\">Zi-Yi Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>",
          "description": "With the rapid development of NLP research, leaderboards have emerged as one\ntool to track the performance of various systems on various NLP tasks. They are\neffective in this goal to some extent, but generally present a rather\nsimplistic one-dimensional view of the submitted systems, communicated only\nthrough holistic accuracy numbers. In this paper, we present a new\nconceptualization and implementation of NLP evaluation: the ExplainaBoard,\nwhich in addition to inheriting the functionality of the standard leaderboard,\nalso allows researchers to (i) diagnose strengths and weaknesses of a single\nsystem (e.g.~what is the best-performing system bad at?) (ii) interpret\nrelationships between multiple systems. (e.g.~where does system A outperform\nsystem B? What if we combine systems A, B, and C?) and (iii) examine prediction\nresults closely (e.g.~what are common errors made by multiple systems, or in\nwhat contexts do particular errors occur?). So far, ExplainaBoard covers more\nthan 400 systems, 50 datasets, 40 languages, and 12 tasks. ExplainaBoard keeps\nupdated and is recently upgraded by supporting (1) multilingual multi-task\nbenchmark, (2) meta-evaluation, and (3) more complicated task: machine\ntranslation, which reviewers also suggested.} We not only released an online\nplatform on the website \\url{this http URL} but also make\nour evaluation tool an API with MIT Licence at Github\n\\url{https://github.com/neulab/explainaBoard} and PyPi\n\\url{https://pypi.org/project/interpret-eval/} that allows users to\nconveniently assess their models offline. We additionally release all output\nfiles from systems that we have run or collected to motivate \"output-driven\"\nresearch in the future.",
          "link": "http://arxiv.org/abs/2104.06387",
          "publishedOn": "2021-07-05T01:54:57.212Z",
          "wordCount": 724,
          "title": "ExplainaBoard: An Explainable Leaderboard for NLP. (arXiv:2104.06387v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.02284",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Luoqiu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_Z/0/1/0/all/0/1\">Zhen Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Hongbin Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tou_H/0/1/0/all/0/1\">Huaixiao Tou</a>",
          "description": "Recent years have witnessed the prosperity of legal artificial intelligence\nwith the development of technologies. In this paper, we propose a novel legal\napplication of legal provision prediction (LPP), which aims to predict the\nrelated legal provisions of affairs. We formulate this task as a challenging\nknowledge graph completion problem, which requires not only text understanding\nbut also graph reasoning. To this end, we propose a novel text-guided graph\nreasoning approach. We collect amounts of real-world legal provision data from\nthe Guangdong government service website and construct a legal dataset called\nLegalLPP. Extensive experimental results on the dataset show that our approach\nachieves better performance compared with baselines. The code and dataset are\navailable in \\url{https://github.com/zjunlp/LegalPP} for reproducibility.",
          "link": "http://arxiv.org/abs/2104.02284",
          "publishedOn": "2021-07-05T01:54:57.199Z",
          "wordCount": 582,
          "title": "Text-guided Legal Knowledge Graph Reasoning. (arXiv:2104.02284v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.00290",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gowda_T/0/1/0/all/0/1\">Thamme Gowda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mattmann_C/0/1/0/all/0/1\">Chris A Mattmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+May_J/0/1/0/all/0/1\">Jonathan May</a>",
          "description": "While there are more than 7000 languages in the world, most translation\nresearch efforts have targeted a few high-resource languages. Commercial\ntranslation systems support only one hundred languages or fewer, and do not\nmake these models available for transfer to low resource languages. In this\nwork, we present useful tools for machine translation research: MTData,\nNLCodec, and RTG. We demonstrate their usefulness by creating a multilingual\nneural machine translation model capable of translating from 500 source\nlanguages to English. We make this multilingual model readily downloadable and\nusable as a service, or as a parent model for transfer-learning to even\nlower-resource languages.",
          "link": "http://arxiv.org/abs/2104.00290",
          "publishedOn": "2021-07-05T01:54:57.191Z",
          "wordCount": 578,
          "title": "Many-to-English Machine Translation Tools, Data, and Pretrained Models. (arXiv:2104.00290v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01183",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mohammad_S/0/1/0/all/0/1\">Saif M. Mohammad</a>",
          "description": "Several high-profile events, such as the use of biased recidivism systems and\nmass testing of emotion recognition systems on vulnerable sub-populations, have\nhighlighted how technology will often lead to more adverse outcomes for those\nthat are already marginalized. In this paper, I will make a case for thinking\nabout ethical considerations not just at the level of individual models and\ndatasets, but also at the level of AI tasks. I will present a new form of such\nan effort, Ethics Sheets for AI Tasks, dedicated to fleshing out the\nassumptions and ethical considerations hidden in how a task is commonly framed\nand in the choices we make regarding the data, method, and evaluation. Finally,\nI will provide an example ethics sheet for automatic emotion recognition.\nTogether with Data Sheets for datasets and Model Cards for AI systems, Ethics\nSheets aid in the development and deployment of responsible AI systems.",
          "link": "http://arxiv.org/abs/2107.01183",
          "publishedOn": "2021-07-05T01:54:57.155Z",
          "wordCount": 574,
          "title": "Ethics Sheets for AI Tasks. (arXiv:2107.01183v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2007.05290",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xueqing Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lewen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1\">Yingce Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Weiqing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Lijun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1\">Shufang Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1\">Tao Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tie-Yan Liu</a>",
          "description": "Sequence learning has attracted much research attention from the machine\nlearning community in recent years. In many applications, a sequence learning\ntask is usually associated with multiple temporally correlated auxiliary tasks,\nwhich are different in terms of how much input information to use or which\nfuture step to predict. For example, (i) in simultaneous machine translation,\none can conduct translation under different latency (i.e., how many input words\nto read/wait before translation); (ii) in stock trend forecasting, one can\npredict the price of a stock in different future days (e.g., tomorrow, the day\nafter tomorrow). While it is clear that those temporally correlated tasks can\nhelp each other, there is a very limited exploration on how to better leverage\nmultiple auxiliary tasks to boost the performance of the main task. In this\nwork, we introduce a learnable scheduler to sequence learning, which can\nadaptively select auxiliary tasks for training depending on the model status\nand the current training data. The scheduler and the model for the main task\nare jointly trained through bi-level optimization. Experiments show that our\nmethod significantly improves the performance of simultaneous machine\ntranslation and stock trend forecasting.",
          "link": "http://arxiv.org/abs/2007.05290",
          "publishedOn": "2021-07-05T01:54:57.097Z",
          "wordCount": 665,
          "title": "Temporally Correlated Task Scheduling for Sequence Learning. (arXiv:2007.05290v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.10100",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luu_S/0/1/0/all/0/1\">Son T. Luu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1\">Ngan Luu-Thuy Nguyen</a>",
          "description": "We present our works on SemEval-2021 Task 5 about Toxic Spans Detection. This\ntask aims to build a model for identifying toxic words in whole posts. We use\nthe BiLSTM-CRF model combining with ToxicBERT Classification to train the\ndetection model for identifying toxic words in posts. Our model achieves 62.23%\nby F1-score on the Toxic Spans Detection task.",
          "link": "http://arxiv.org/abs/2104.10100",
          "publishedOn": "2021-07-05T01:54:57.086Z",
          "wordCount": 552,
          "title": "UIT-ISE-NLP at SemEval-2021 Task 5: Toxic Spans Detection with BiLSTM-CRF and ToxicBERT Comment Classification. (arXiv:2104.10100v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.09966",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Apel_R/0/1/0/all/0/1\">Reut Apel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erev_I/0/1/0/all/0/1\">Ido Erev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reichart_R/0/1/0/all/0/1\">Roi Reichart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tennenholtz_M/0/1/0/all/0/1\">Moshe Tennenholtz</a>",
          "description": "Sender-receiver interactions, and specifically persuasion games, are widely\nresearched in economic modeling and artificial intelligence, and serve as a\nsolid foundation for powerful applications. However, in the classic persuasion\ngames setting, the messages sent from the expert to the decision-maker are\nabstract or well-structured application-specific signals rather than natural\n(human) language messages, although natural language is a very common\ncommunication signal in real-world persuasion setups. This paper addresses the\nuse of natural language in persuasion games, exploring its impact on the\ndecisions made by the players and aiming to construct effective models for the\nprediction of these decisions. For this purpose, we conduct an online repeated\ninteraction experiment. At each trial of the interaction, an informed expert\naims to sell an uninformed decision-maker a vacation in a hotel, by sending her\na review that describes the hotel. While the expert is exposed to several\nscored reviews, the decision-maker observes only the single review sent by the\nexpert, and her payoff in case she chooses to take the hotel is a random draw\nfrom the review score distribution available to the expert only. The expert's\npayoff, in turn, depends on the number of times the decision-maker chooses the\nhotel. We consider a number of modeling approaches for this setup, differing\nfrom each other in the model type (deep neural network (DNN) vs. linear\nclassifier), the type of features used by the model (textual, behavioral or\nboth) and the source of the textual features (DNN-based vs. hand-crafted). Our\nresults demonstrate that given a prefix of the interaction sequence, our models\ncan predict the future decisions of the decision-maker, particularly when a\nsequential modeling approach and hand-crafted textual features are applied.",
          "link": "http://arxiv.org/abs/2012.09966",
          "publishedOn": "2021-07-05T01:54:57.073Z",
          "wordCount": 767,
          "title": "Predicting Decisions in Language Based Persuasion Games. (arXiv:2012.09966v3 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00955",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhukova_A/0/1/0/all/0/1\">Anastasia Zhukova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamborg_F/0/1/0/all/0/1\">Felix Hamborg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Donnay_K/0/1/0/all/0/1\">Karsten Donnay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gipp_B/0/1/0/all/0/1\">Bela Gipp</a>",
          "description": "Unsupervised concept identification through clustering, i.e., identification\nof semantically related words and phrases, is a common approach to identify\ncontextual primitives employed in various use cases, e.g., text dimension\nreduction, i.e., replace words with the concepts to reduce the vocabulary size,\nsummarization, and named entity resolution. We demonstrate the first results of\nan unsupervised approach for the identification of groups of persons as actors\nextracted from a set of related articles. Specifically, the approach clusters\nmentions of groups of persons that act as non-named entity actors in the texts,\ne.g., \"migrant families\" = \"asylum-seekers.\" Compared to our baseline, the\napproach keeps the mentions of the geopolitical entities separated, e.g., \"Iran\nleaders\" != \"European leaders,\" and clusters (in)directly related mentions with\ndiverse wording, e.g., \"American officials\" = \"Trump Administration.\"",
          "link": "http://arxiv.org/abs/2107.00955",
          "publishedOn": "2021-07-05T01:54:57.046Z",
          "wordCount": 578,
          "title": "Concept Identification of Directly and Indirectly Related Mentions Referring to Groups of Persons. (arXiv:2107.00955v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01198",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Abheesht Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chhablani_G/0/1/0/all/0/1\">Gunjan Chhablani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pandey_H/0/1/0/all/0/1\">Harshit Pandey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patil_R/0/1/0/all/0/1\">Rajaswa Patil</a>",
          "description": "In this work, we present to the NLP community, and to the wider research\ncommunity as a whole, an application for the diachronic analysis of research\ncorpora. We open source an easy-to-use tool coined: DRIFT, which allows\nresearchers to track research trends and development over the years. The\nanalysis methods are collated from well-cited research works, with a few of our\nown methods added for good measure. Succinctly put, some of the analysis\nmethods are: keyword extraction, word clouds, predicting\ndeclining/stagnant/growing trends using Productivity, tracking bi-grams using\nAcceleration plots, finding the Semantic Drift of words, tracking trends using\nsimilarity, etc. To demonstrate the utility and efficacy of our tool, we\nperform a case study on the cs.CL corpus of the arXiv repository and draw\ninferences from the analysis methods. The toolkit and the associated code are\navailable here: https://github.com/rajaswa/DRIFT.",
          "link": "http://arxiv.org/abs/2107.01198",
          "publishedOn": "2021-07-05T01:54:57.037Z",
          "wordCount": 585,
          "title": "DRIFT: A Toolkit for Diachronic Analysis of Scientific Literature. (arXiv:2107.01198v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01076",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tsakalidis_A/0/1/0/all/0/1\">Adam Tsakalidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basile_P/0/1/0/all/0/1\">Pierpaolo Basile</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bazzi_M/0/1/0/all/0/1\">Marya Bazzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cucuringu_M/0/1/0/all/0/1\">Mihai Cucuringu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McGillivray_B/0/1/0/all/0/1\">Barbara McGillivray</a>",
          "description": "Lexical semantic change (detecting shifts in the meaning and usage of words)\nis an important task for social and cultural studies as well as for Natural\nLanguage Processing applications. Diachronic word embeddings (time-sensitive\nvector representations of words that preserve their meaning) have become the\nstandard resource for this task. However, given the significant computational\nresources needed for their generation, very few resources exist that make\ndiachronic word embeddings available to the scientific community.\n\nIn this paper we present DUKweb, a set of large-scale resources designed for\nthe diachronic analysis of contemporary English. DUKweb was created from the\nJISC UK Web Domain Dataset (1996-2013), a very large archive which collects\nresources from the Internet Archive that were hosted on domains ending in\n`.uk'. DUKweb consists of a series word co-occurrence matrices and two types of\nword embeddings for each year in the JISC UK Web Domain dataset. We show the\nreuse potential of DUKweb and its quality standards via a case study on word\nmeaning change detection.",
          "link": "http://arxiv.org/abs/2107.01076",
          "publishedOn": "2021-07-05T01:54:57.024Z",
          "wordCount": 617,
          "title": "DUKweb: Diachronic word representations from the UK Web Archive corpus. (arXiv:2107.01076v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01202",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ansari_M/0/1/0/all/0/1\">Mohd Zeeshan Ansari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beg_M/0/1/0/all/0/1\">M M Sufyan Beg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_T/0/1/0/all/0/1\">Tanvir Ahmad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1\">Mohd Jazib Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wasim_G/0/1/0/all/0/1\">Ghazali Wasim</a>",
          "description": "Language identification of social media text has been an interesting problem\nof study in recent years. Social media messages are predominantly in code mixed\nin non-English speaking states. Prior knowledge by pre-training contextual\nembeddings have shown state of the art results for a range of downstream tasks.\nRecently, models such as BERT have shown that using a large amount of unlabeled\ndata, the pretrained language models are even more beneficial for learning\ncommon language representations. Extensive experiments exploiting transfer\nlearning and fine-tuning BERT models to identify language on Twitter are\npresented in this paper. The work utilizes a data collection of\nHindi-English-Urdu codemixed text for language pre-training and Hindi-English\ncodemixed for subsequent word-level language classification. The results show\nthat the representations pre-trained over codemixed data produce better results\nby their monolingual counterpart.",
          "link": "http://arxiv.org/abs/2107.01202",
          "publishedOn": "2021-07-05T01:54:57.016Z",
          "wordCount": 573,
          "title": "Language Identification of Hindi-English tweets using code-mixed BERT. (arXiv:2107.01202v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00967",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mi_H/0/1/0/all/0/1\">Haitao Mi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Z/0/1/0/all/0/1\">Zujie Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yafang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yi Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1\">Jing Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Melo_G/0/1/0/all/0/1\">Gerard de Melo</a>",
          "description": "Human language understanding operates at multiple levels of granularity\n(e.g., words, phrases, and sentences) with increasing levels of abstraction\nthat can be hierarchically combined. However, existing deep models with stacked\nlayers do not explicitly model any sort of hierarchical process. This paper\nproposes a recursive Transformer model based on differentiable CKY style binary\ntrees to emulate the composition process. We extend the bidirectional language\nmodel pre-training objective to this architecture, attempting to predict each\nword given its left and right abstraction nodes. To scale up our approach, we\nalso introduce an efficient pruned tree induction algorithm to enable encoding\nin just a linear number of composition steps. Experimental results on language\nmodeling and unsupervised parsing show the effectiveness of our approach.",
          "link": "http://arxiv.org/abs/2107.00967",
          "publishedOn": "2021-07-05T01:54:57.006Z",
          "wordCount": 582,
          "title": "R2D2: Recursive Transformer based on Differentiable Tree for Interpretable Hierarchical Language Modeling. (arXiv:2107.00967v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01068",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jalalvand_S/0/1/0/all/0/1\">Shahab Jalalvand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bangalore_S/0/1/0/all/0/1\">Srinivas Bangalore</a>",
          "description": "An intelligent virtual assistant (IVA) enables effortless conversations in\ncall routing through spoken utterance classification (SUC) which is a special\nform of spoken language understanding (SLU). Building a SUC system requires a\nlarge amount of supervised in-domain data that is not always available. In this\npaper, we introduce an unsupervised spoken utterance classification approach\n(USUC) that does not require any in-domain data except for the intent labels\nand a few para-phrases per intent. USUC is consisting of a KNN classifier (K=1)\nand a complex embedding model trained on a large amount of unsupervised\ncustomer service corpus. Among all embedding models, we demonstrate that Elmo\nworks best for USUC. However, an Elmo model is too slow to be used at run-time\nfor call routing. To resolve this issue, first, we compute the uni- and bi-gram\nembedding vectors offline and we build a lookup table of n-grams and their\ncorresponding embedding vector. Then we use this table to compute sentence\nembedding vectors at run-time, along with back-off techniques for unseen\nn-grams. Experiments show that USUC outperforms the traditional utterance\nclassification methods by reducing the classification error rate from 32.9% to\n27.0% without requiring supervised data. Moreover, our lookup and back-off\ntechnique increases the processing speed from 16 utterances per second to 118\nutterances per second.",
          "link": "http://arxiv.org/abs/2107.01068",
          "publishedOn": "2021-07-05T01:54:56.986Z",
          "wordCount": 639,
          "title": "Unsupervised Spoken Utterance Classification. (arXiv:2107.01068v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00956",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kovac_G/0/1/0/all/0/1\">Grgur Kova&#x10d;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Portelas_R/0/1/0/all/0/1\">R&#xe9;my Portelas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hofmann_K/0/1/0/all/0/1\">Katja Hofmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oudeyer_P/0/1/0/all/0/1\">Pierre-Yves Oudeyer</a>",
          "description": "Building embodied autonomous agents capable of participating in social\ninteractions with humans is one of the main challenges in AI. Within the Deep\nReinforcement Learning (DRL) field, this objective motivated multiple works on\nembodied language use. However, current approaches focus on language as a\ncommunication tool in very simplified and non-diverse social situations: the\n\"naturalness\" of language is reduced to the concept of high vocabulary size and\nvariability. In this paper, we argue that aiming towards human-level AI\nrequires a broader set of key social skills: 1) language use in complex and\nvariable social contexts; 2) beyond language, complex embodied communication in\nmultimodal settings within constantly evolving social worlds. We explain how\nconcepts from cognitive sciences could help AI to draw a roadmap towards\nhuman-like intelligence, with a focus on its social dimensions. As a first\nstep, we propose to expand current research to a broader set of core social\nskills. To do this, we present SocialAI, a benchmark to assess the acquisition\nof social skills of DRL agents using multiple grid-world environments featuring\nother (scripted) social agents. We then study the limits of a recent SOTA DRL\napproach when tested on SocialAI and discuss important next steps towards\nproficient social agents. Videos and code are available at\nhttps://sites.google.com/view/socialai.",
          "link": "http://arxiv.org/abs/2107.00956",
          "publishedOn": "2021-07-05T01:54:56.956Z",
          "wordCount": 663,
          "title": "SocialAI: Benchmarking Socio-Cognitive Abilities in Deep Reinforcement Learning Agents. (arXiv:2107.00956v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00753",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Joshi_N/0/1/0/all/0/1\">Nitish Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">He He</a>",
          "description": "While pretrained language models achieve excellent performance on natural\nlanguage understanding benchmarks, they tend to rely on spurious correlations\nand generalize poorly to out-of-distribution (OOD) data. Recent work has\nexplored using counterfactually-augmented data (CAD) -- data generated by\nminimally perturbing examples to flip the ground-truth label -- to identify\nrobust features that are invariant under distribution shift. However, empirical\nresults using CAD for OOD generalization have been mixed. To explain this\ndiscrepancy, we draw insights from a linear Gaussian model and demonstrate the\npitfalls of CAD. Specifically, we show that (a) while CAD is effective at\nidentifying robust features, it may prevent the model from learning unperturbed\nrobust features, and (b) CAD may exacerbate existing spurious correlations in\nthe data. Our results show that the lack of perturbation diversity in current\nCAD datasets limits its effectiveness on OOD generalization, calling for\ninnovative crowdsourcing procedures to elicit diverse perturbation of examples.",
          "link": "http://arxiv.org/abs/2107.00753",
          "publishedOn": "2021-07-05T01:54:56.933Z",
          "wordCount": 584,
          "title": "An Investigation of the (In)effectiveness of Counterfactually Augmented Data. (arXiv:2107.00753v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00927",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Marz_L/0/1/0/all/0/1\">Luisa M&#xe4;rz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schweter_S/0/1/0/all/0/1\">Stefan Schweter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poerner_N/0/1/0/all/0/1\">Nina Poerner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_B/0/1/0/all/0/1\">Benjamin Roth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1\">Hinrich Sch&#xfc;tze</a>",
          "description": "We propose new methods for in-domain and cross-domain Named Entity\nRecognition (NER) on historical data for Dutch and French. For the cross-domain\ncase, we address domain shift by integrating unsupervised in-domain data via\ncontextualized string embeddings; and OCR errors by injecting synthetic OCR\nerrors into the source domain and address data centric domain adaptation. We\npropose a general approach to imitate OCR errors in arbitrary input data. Our\ncross-domain as well as our in-domain results outperform several strong\nbaselines and establish state-of-the-art results. We publish preprocessed\nversions of the French and Dutch Europeana NER corpora.",
          "link": "http://arxiv.org/abs/2107.00927",
          "publishedOn": "2021-07-05T01:54:56.896Z",
          "wordCount": 543,
          "title": "Data Centric Domain Adaptation for Historical Text with OCR Errors. (arXiv:2107.00927v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00910",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sehoon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_S/0/1/0/all/0/1\">Sheng Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thorsley_D/0/1/0/all/0/1\">David Thorsley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gholami_A/0/1/0/all/0/1\">Amir Gholami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassoun_J/0/1/0/all/0/1\">Joseph Hassoun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keutzer_K/0/1/0/all/0/1\">Kurt Keutzer</a>",
          "description": "A major challenge in deploying transformer models is their prohibitive\ninference cost, which quadratically scales with the input sequence length. This\nmakes it especially difficult to use transformers for processing long\nsequences. To address this, we present a novel Learned Token Pruning (LTP)\nmethod that reduces redundant tokens as the data passes through the different\nlayers of the transformer. In particular, LTP prunes tokens with an attention\nscore below a threshold value, which is learned during training. Importantly,\nour threshold based method avoids algorithmically expensive operations such as\ntop-k token selection which are used in prior token pruning methods, and also\nleads to structured pruning. We extensively test the performance of our\napproach on multiple GLUE tasks and show that our learned threshold based\nmethod consistently outperforms the prior state-of-the-art top-k token based\nmethod by up to ~2% higher accuracy with the same amount of FLOPs. Furthermore,\nour preliminary results show up to 1.4x and 1.9x throughput improvement on\nTesla T4 GPU and Intel Haswell CPU, respectively, with less than 1% of accuracy\ndrop (and up to 2.1x FLOPs reduction). Our code has been developed in PyTorch\nand has been open-sourced.",
          "link": "http://arxiv.org/abs/2107.00910",
          "publishedOn": "2021-07-05T01:54:56.867Z",
          "wordCount": 621,
          "title": "Learned Token Pruning for Transformers. (arXiv:2107.00910v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00789",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kambara_M/0/1/0/all/0/1\">Motonari Kambara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sugiura_K/0/1/0/all/0/1\">Komei Sugiura</a>",
          "description": "There have been many studies in robotics to improve the communication skills\nof domestic service robots. Most studies, however, have not fully benefited\nfrom recent advances in deep neural networks because the training datasets are\nnot large enough. In this paper, our aim is to augment the datasets based on a\ncrossmodal language generation model. We propose the Case Relation Transformer\n(CRT), which generates a fetching instruction sentence from an image, such as\n\"Move the blue flip-flop to the lower left box.\" Unlike existing methods, the\nCRT uses the Transformer to integrate the visual features and geometry features\nof objects in the image. The CRT can handle the objects because of the Case\nRelation Block. We conducted comparison experiments and a human evaluation. The\nexperimental results show the CRT outperforms baseline methods.",
          "link": "http://arxiv.org/abs/2107.00789",
          "publishedOn": "2021-07-05T01:54:56.804Z",
          "wordCount": 580,
          "title": "Case Relation Transformer: A Crossmodal Language Generation Model for Fetching Instructions. (arXiv:2107.00789v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00811",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ishikawa_S/0/1/0/all/0/1\">Shintaro Ishikawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sugiura_K/0/1/0/all/0/1\">Komei Sugiura</a>",
          "description": "Currently, domestic service robots have an insufficient ability to interact\nnaturally through language. This is because understanding human instructions is\ncomplicated by various ambiguities and missing information. In existing\nmethods, the referring expressions that specify the relationships between\nobjects are insufficiently modeled. In this paper, we propose Target-dependent\nUNITER, which learns the relationship between the target object and other\nobjects directly by focusing on the relevant regions within an image, rather\nthan the whole image. Our method is an extension of the UNITER-based\nTransformer that can be pretrained on general-purpose datasets. We extend the\nUNITER approach by introducing a new architecture for handling the target\ncandidates. Our model is validated on two standard datasets, and the results\nshow that Target-dependent UNITER outperforms the baseline method in terms of\nclassification accuracy.",
          "link": "http://arxiv.org/abs/2107.00811",
          "publishedOn": "2021-07-05T01:54:56.794Z",
          "wordCount": 580,
          "title": "Target-dependent UNITER: A Transformer-Based Multimodal Language Comprehension Model for Domestic Service Robots. (arXiv:2107.00811v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00841",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_F/0/1/0/all/0/1\">Feng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_J/0/1/0/all/0/1\">Jian-Cheng Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1\">Peng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zi-Li Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yan-Yan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fujita_H/0/1/0/all/0/1\">Hamido Fujita</a>",
          "description": "Multi-hop machine reading comprehension is a challenging task in natural\nlanguage processing, which requires more reasoning ability and explainability.\nSpectral models based on graph convolutional networks grant the inferring\nabilities and lead to competitive results, however, part of them still face the\nchallenge of analyzing the reasoning in a human-understandable way. Inspired by\nthe concept of the Grandmother Cells in cognitive neuroscience, a spatial graph\nattention framework named crname, imitating the procedure was proposed. This\nmodel is designed to assemble the semantic features in multi-angle\nrepresentations and automatically concentrate or alleviate the information for\nreasoning. The name \"crname\" is a metaphor for the pattern of the model: regard\nthe subjects of queries as the start points of clues, take the reasoning\nentities as bridge points, and consider the latent candidate entities as the\ngrandmother cells, and the clues end up in candidate entities. The proposed\nmodel allows us to visualize the reasoning graph and analyze the importance of\nedges connecting two entities and the selectivity in the mention and candidate\nnodes, which can be easier to be comprehended empirically. The official\nevaluations in open-domain multi-hop reading dataset WikiHop and Drug-drug\nInteractions dataset MedHop prove the validity of our approach and show the\nprobability of the application of the model in the molecular biology domain.",
          "link": "http://arxiv.org/abs/2107.00841",
          "publishedOn": "2021-07-05T01:54:56.783Z",
          "wordCount": 651,
          "title": "Heterogeneous Graph Attention Network for Multi-hop Machine Reading Comprehension. (arXiv:2107.00841v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00941",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jagtap_R/0/1/0/all/0/1\">Raj Jagtap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Abhinav Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goel_R/0/1/0/all/0/1\">Rahul Goel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_S/0/1/0/all/0/1\">Shakshi Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_R/0/1/0/all/0/1\">Rajesh Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+George_C/0/1/0/all/0/1\">Clint P. George</a>",
          "description": "Millions of people use platforms such as YouTube, Facebook, Twitter, and\nother mass media. Due to the accessibility of these platforms, they are often\nused to establish a narrative, conduct propaganda, and disseminate\nmisinformation. This work proposes an approach that uses state-of-the-art NLP\ntechniques to extract features from video captions (subtitles). To evaluate our\napproach, we utilize a publicly accessible and labeled dataset for classifying\nvideos as misinformation or not. The motivation behind exploring video captions\nstems from our analysis of videos metadata. Attributes such as the number of\nviews, likes, dislikes, and comments are ineffective as videos are hard to\ndifferentiate using this information. Using caption dataset, the proposed\nmodels can classify videos among three classes (Misinformation, Debunking\nMisinformation, and Neutral) with 0.85 to 0.90 F1-score. To emphasize the\nrelevance of the misinformation class, we re-formulate our classification\nproblem as a two-class classification - Misinformation vs. others (Debunking\nMisinformation and Neutral). In our experiments, the proposed models can\nclassify videos with 0.92 to 0.95 F1-score and 0.78 to 0.90 AUC ROC.",
          "link": "http://arxiv.org/abs/2107.00941",
          "publishedOn": "2021-07-05T01:54:56.773Z",
          "wordCount": 610,
          "title": "Misinformation Detection on YouTube Using Video Captions. (arXiv:2107.00941v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00807",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_N/0/1/0/all/0/1\">Nanjiang Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marneffe_M/0/1/0/all/0/1\">Marie-Catherine de Marneffe</a>",
          "description": "We investigate how well BERT performs on predicting factuality in several\nexisting English datasets, encompassing various linguistic constructions.\nAlthough BERT obtains a strong performance on most datasets, it does so by\nexploiting common surface patterns that correlate with certain factuality\nlabels, and it fails on instances where pragmatic reasoning is necessary.\nContrary to what the high performance suggests, we are still far from having a\nrobust system for factuality prediction.",
          "link": "http://arxiv.org/abs/2107.00807",
          "publishedOn": "2021-07-05T01:54:56.744Z",
          "wordCount": 522,
          "title": "He Thinks He Knows Better than the Doctors: BERT for Event Factuality Fails on Pragmatics. (arXiv:2107.00807v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00676",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Doddapaneni_S/0/1/0/all/0/1\">Sumanth Doddapaneni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramesh_G/0/1/0/all/0/1\">Gowtham Ramesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kunchukuttan_A/0/1/0/all/0/1\">Anoop Kunchukuttan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_P/0/1/0/all/0/1\">Pratyush Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khapra_M/0/1/0/all/0/1\">Mitesh M. Khapra</a>",
          "description": "Multilingual Language Models (MLLMs) such as mBERT, XLM, XLM-R, \\textit{etc.}\nhave emerged as a viable option for bringing the power of pretraining to a\nlarge number of languages. Given their success in zero shot transfer learning,\nthere has emerged a large body of work in (i) building bigger MLLMs covering a\nlarge number of languages (ii) creating exhaustive benchmarks covering a wider\nvariety of tasks and languages for evaluating MLLMs (iii) analysing the\nperformance of MLLMs on monolingual, zero shot crosslingual and bilingual tasks\n(iv) understanding the universal language patterns (if any) learnt by MLLMs and\n(v) augmenting the (often) limited capacity of MLLMs to improve their\nperformance on seen or even unseen languages. In this survey, we review the\nexisting literature covering the above broad areas of research pertaining to\nMLLMs. Based on our survey, we recommend some promising directions of future\nresearch.",
          "link": "http://arxiv.org/abs/2107.00676",
          "publishedOn": "2021-07-05T01:54:56.663Z",
          "wordCount": 578,
          "title": "A Primer on Pretrained Multilingual Language Models. (arXiv:2107.00676v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00692",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shillingford_B/0/1/0/all/0/1\">Brendan Shillingford</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Assael_Y/0/1/0/all/0/1\">Yannis Assael</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denil_M/0/1/0/all/0/1\">Misha Denil</a>",
          "description": "This work describes an interactive decoding method to improve the performance\nof visual speech recognition systems using user input to compensate for the\ninherent ambiguity of the task. Unlike most phoneme-to-word decoding pipelines,\nwhich produce phonemes and feed these through a finite state transducer, our\nmethod instead expands words in lockstep, facilitating the insertion of\ninteraction points at each word position. Interaction points enable us to\nsolicit input during decoding, allowing users to interactively direct the\ndecoding process. We simulate the behavior of user input using an oracle to\ngive an automated evaluation, and show promise for the use of this method for\ntext input.",
          "link": "http://arxiv.org/abs/2107.00692",
          "publishedOn": "2021-07-05T01:54:56.570Z",
          "wordCount": 540,
          "title": "Interactive decoding of words from visual speech recognition models. (arXiv:2107.00692v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00653",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yu Shi</a>",
          "description": "The Transformer model is widely used in natural language processing for\nsentence representation. However, the previous Transformer-based models focus\non function words that have limited meaning in most cases and could merely\nextract high-level semantic abstraction features. In this paper, two approaches\nare introduced to improve the performance of Transformers. We calculated the\nattention score by multiplying the part-of-speech weight vector with the\ncorrelation coefficient, which helps extract the words with more practical\nmeaning. The weight vector is obtained by the input text sequence based on the\nimportance of the part-of-speech. Furthermore, we fuse the features of each\nlayer to make the sentence representation results more comprehensive and\naccurate. In experiments, we demonstrate the effectiveness of our model\nTransformer-F on three standard text classification datasets. Experimental\nresults show that our proposed model significantly boosts the performance of\ntext classification as compared to the baseline model. Specifically, we obtain\na 5.28% relative improvement over the vanilla Transformer on the simple tasks.",
          "link": "http://arxiv.org/abs/2107.00653",
          "publishedOn": "2021-07-05T01:54:56.469Z",
          "wordCount": 597,
          "title": "Transformer-F: A Transformer network with effective methods for learning universal sentence representation. (arXiv:2107.00653v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00730",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_A/0/1/0/all/0/1\">Anubhab Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Honore_A/0/1/0/all/0/1\">Antoine Honor&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Dong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henter_G/0/1/0/all/0/1\">Gustav Eje Henter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chatterjee_S/0/1/0/all/0/1\">Saikat Chatterjee</a>",
          "description": "In pursuit of explainability, we develop generative models for sequential\ndata. The proposed models provide state-of-the-art classification results and\nrobust performance for speech phone classification. We combine modern neural\nnetworks (normalizing flows) and traditional generative models (hidden Markov\nmodels - HMMs). Normalizing flow-based mixture models (NMMs) are used to model\nthe conditional probability distribution given the hidden state in the HMMs.\nModel parameters are learned through judicious combinations of time-tested\nBayesian learning methods and contemporary neural network learning methods. We\nmainly combine expectation-maximization (EM) and mini-batch gradient descent.\nThe proposed generative models can compute likelihood of a data and hence\ndirectly suitable for maximum-likelihood (ML) classification approach. Due to\nstructural flexibility of HMMs, we can use different normalizing flow models.\nThis leads to different types of HMMs providing diversity in data modeling\ncapacity. The diversity provides an opportunity for easy decision fusion from\ndifferent models. For a standard speech phone classification setup involving 39\nphones (classes) and the TIMIT dataset, we show that the use of standard\nfeatures called mel-frequency-cepstral-coeffcients (MFCCs), the proposed\ngenerative models, and the decision fusion together can achieve $86.6\\%$\naccuracy by generative training only. This result is close to state-of-the-art\nresults, for examples, $86.2\\%$ accuracy of PyTorch-Kaldi toolkit [1], and\n$85.1\\%$ accuracy using light gated recurrent units [2]. We do not use any\ndiscriminative learning approach and related sophisticated features in this\narticle.",
          "link": "http://arxiv.org/abs/2107.00730",
          "publishedOn": "2021-07-05T01:54:56.430Z",
          "wordCount": 690,
          "title": "Normalizing Flow based Hidden Markov Models for Classification of Speech Phones with Explainability. (arXiv:2107.00730v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2103.08364",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fantinuoli_C/0/1/0/all/0/1\">Claudio Fantinuoli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prandi_B/0/1/0/all/0/1\">Bianca Prandi</a>",
          "description": "In recent years, automatic speech-to-speech and speech-to-text translation\nhas gained momentum thanks to advances in artificial intelligence, especially\nin the domains of speech recognition and machine translation. The quality of\nsuch applications is commonly tested with automatic metrics, such as BLEU,\nprimarily with the goal of assessing improvements of releases or in the context\nof evaluation campaigns. However, little is known about how the output of such\nsystems is perceived by end users or how they compare to human performances in\nsimilar communicative tasks.\n\nIn this paper, we present the results of an experiment aimed at evaluating\nthe quality of a real-time speech translation engine by comparing it to the\nperformance of professional simultaneous interpreters. To do so, we adopt a\nframework developed for the assessment of human interpreters and use it to\nperform a manual evaluation on both human and machine performances. In our\nsample, we found better performance for the human interpreters in terms of\nintelligibility, while the machine performs slightly better in terms of\ninformativeness. The limitations of the study and the possible enhancements of\nthe chosen framework are discussed. Despite its intrinsic limitations, the use\nof this framework represents a first step towards a user-centric and\ncommunication-oriented methodology for evaluating real-time automatic speech\ntranslation.",
          "link": "http://arxiv.org/abs/2103.08364",
          "publishedOn": "2021-07-02T01:57:59.329Z",
          "wordCount": 672,
          "title": "Towards the evaluation of automatic simultaneous speech translation from a communicative perspective. (arXiv:2103.08364v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09997",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tran_H/0/1/0/all/0/1\">Hieu Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phan_L/0/1/0/all/0/1\">Long Phan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anibal_J/0/1/0/all/0/1\">James Anibal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_B/0/1/0/all/0/1\">Binh T. Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Truong-Son Nguyen</a>",
          "description": "In this paper, we propose SPBERT, a transformer-based language model\npre-trained on massive SPARQL query logs. By incorporating masked language\nmodeling objectives and the word structural objective, SPBERT can learn\ngeneral-purpose representations in both natural language and SPARQL query\nlanguage. We investigate how SPBERT and encoder-decoder architecture can be\nadapted for Knowledge-based QA corpora. We conduct exhaustive experiments on\ntwo additional tasks, including SPARQL Query Construction and Answer\nVerbalization Generation. The experimental results show that SPBERT can obtain\npromising results, achieving state-of-the-art BLEU scores on several of these\ntasks.",
          "link": "http://arxiv.org/abs/2106.09997",
          "publishedOn": "2021-07-02T01:57:59.304Z",
          "wordCount": 554,
          "title": "SPBERT: An Efficient Pre-training BERT on SPARQL Queries for Question Answering over Knowledge Graphs. (arXiv:2106.09997v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.08091",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Moreo_A/0/1/0/all/0/1\">Alejandro Moreo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sebastiani_F/0/1/0/all/0/1\">Fabrizio Sebastiani</a>",
          "description": "Sentiment quantification is the task of estimating the relative frequency (or\n\"prevalence\") of sentiment-related classes (such as Positive, Neutral,\nNegative) in a sample of unlabelled texts; this is especially important when\nthese texts are tweets, since most sentiment classification endeavours carried\nout on Twitter data actually have quantification (and not the classification of\nindividual tweets) as their ultimate goal. It is well-known that solving\nquantification via \"classify and count\" (i.e., by classifying all unlabelled\nitems via a standard classifier and counting the items that have been assigned\nto a given class) is suboptimal in terms of accuracy, and that more accurate\nquantification methods exist. In 2016, Gao and Sebastiani carried out a\nsystematic comparison of quantification methods on the task of tweet sentiment\nquantification. In hindsight, we observe that the experimental protocol\nfollowed in that work is flawed, and that its results are thus unreliable. We\nnow re-evaluate those quantification methods on the very same datasets, this\ntime following a now consolidated and much more robust experimental protocol,\nthat involves 5775 as many experiments as run in the original study. Our\nexperimentation yields results dramatically different from those obtained by\nGao and Sebastiani, and thus provide a different, much more solid understanding\nof the relative strengths and weaknesses of different sentiment quantification\nmethods.",
          "link": "http://arxiv.org/abs/2011.08091",
          "publishedOn": "2021-07-02T01:57:59.271Z",
          "wordCount": 676,
          "title": "Tweet Sentiment Quantification: An Experimental Re-Evaluation. (arXiv:2011.08091v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00596",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pingali_S/0/1/0/all/0/1\">Sriram Pingali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yadav_S/0/1/0/all/0/1\">Shweta Yadav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dutta_P/0/1/0/all/0/1\">Pratik Dutta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saha_S/0/1/0/all/0/1\">Sriparna Saha</a>",
          "description": "The recent advancement of pre-trained Transformer models has propelled the\ndevelopment of effective text mining models across various biomedical tasks.\nHowever, these models are primarily learned on the textual data and often lack\nthe domain knowledge of the entities to capture the context beyond the\nsentence. In this study, we introduced a novel framework that enables the model\nto learn multi-omnics biological information about entities (proteins) with the\nhelp of additional multi-modal cues like molecular structure. Towards this,\nrather developing modality-specific architectures, we devise a generalized and\noptimized graph based multi-modal learning mechanism that utilizes the\nGraphBERT model to encode the textual and molecular structure information and\nexploit the underlying features of various modalities to enable end-to-end\nlearning. We evaluated our proposed method on ProteinProtein Interaction task\nfrom the biomedical corpus, where our proposed generalized approach is observed\nto be benefited by the additional domain-specific modality.",
          "link": "http://arxiv.org/abs/2107.00596",
          "publishedOn": "2021-07-02T01:57:59.262Z",
          "wordCount": 585,
          "title": "Multimodal Graph-based Transformer Framework for Biomedical Relation Extraction. (arXiv:2107.00596v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00439",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_S/0/1/0/all/0/1\">Shammur Absar Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durrani_N/0/1/0/all/0/1\">Nadir Durrani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ali_A/0/1/0/all/0/1\">Ahmed Ali</a>",
          "description": "End-to-end DNN architectures have pushed the state-of-the-art in speech\ntechnologies, as well as in other spheres of AI, leading researchers to train\nmore complex and deeper models. These improvements came at the cost of\ntransparency. DNNs are innately opaque and difficult to interpret. We no longer\nunderstand what features are learned, where they are preserved, and how they\ninter-operate. Such an analysis is important for better model understanding,\ndebugging and to ensure fairness in ethical decision making. In this work, we\nanalyze the representations trained within deep speech models, towards the task\nof speaker recognition, dialect identification and reconstruction of masked\nsignals. We carry a layer- and neuron-level analysis on the utterance-level\nrepresentations captured within pretrained speech models for speaker, language\nand channel properties. We study: is this information captured in the learned\nrepresentations? where is it preserved? how is it distributed? and can we\nidentify a minimal subset of network that posses this information. Using\ndiagnostic classifiers, we answered these questions. Our results reveal: (i)\nchannel and gender information is omnipresent and is redundantly distributed\n(ii) complex properties such as dialectal information is encoded only in the\ntask-oriented pretrained network and is localised in the upper layers (iii) a\nminimal subset of neurons can be extracted to encode the predefined property\n(iv) salient neurons are sometimes shared between properties and can highlights\npresence of biases in the network. Our cross-architectural comparison indicates\nthat (v) the pretrained models captures speaker-invariant information and (vi)\nthe pretrained CNNs models are competitive to the Transformers for encoding\ninformation for the studied properties. To the best of our knowledge, this is\nthe first study to investigate neuron analysis on the speech models.",
          "link": "http://arxiv.org/abs/2107.00439",
          "publishedOn": "2021-07-02T01:57:59.255Z",
          "wordCount": 752,
          "title": "What do End-to-End Speech Models Learn about Speaker, Language and Channel Information? A Layer-wise and Neuron-level Analysis. (arXiv:2107.00439v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2009.12064",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kitada_S/0/1/0/all/0/1\">Shunsuke Kitada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyatomi_H/0/1/0/all/0/1\">Hitoshi Iyatomi</a>",
          "description": "Although attention mechanisms have been applied to a variety of deep learning\nmodels and have been shown to improve the prediction performance, it has been\nreported to be vulnerable to perturbations to the mechanism. To overcome the\nvulnerability to perturbations in the mechanism, we are inspired by adversarial\ntraining (AT), which is a powerful regularization technique for enhancing the\nrobustness of the models. In this paper, we propose a general training\ntechnique for natural language processing tasks, including AT for attention\n(Attention AT) and more interpretable AT for attention (Attention iAT). The\nproposed techniques improved the prediction performance and the model\ninterpretability by exploiting the mechanisms with AT. In particular, Attention\niAT boosts those advantages by introducing adversarial perturbation, which\nenhances the difference in the attention of the sentences. Evaluation\nexperiments with ten open datasets revealed that AT for attention mechanisms,\nespecially Attention iAT, demonstrated (1) the best performance in nine out of\nten tasks and (2) more interpretable attention (i.e., the resulting attention\ncorrelated more strongly with gradient-based word importance) for all tasks.\nAdditionally, the proposed techniques are (3) much less dependent on\nperturbation size in AT. Our code is available at\nhttps://github.com/shunk031/attention-meets-perturbation",
          "link": "http://arxiv.org/abs/2009.12064",
          "publishedOn": "2021-07-02T01:57:59.248Z",
          "wordCount": 683,
          "title": "Attention Meets Perturbations: Robust and Interpretable Attention with Adversarial Training. (arXiv:2009.12064v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00635",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Inaguma_H/0/1/0/all/0/1\">Hirofumi Inaguma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kawahara_T/0/1/0/all/0/1\">Tatsuya Kawahara</a>",
          "description": "While attention-based encoder-decoder (AED) models have been successfully\nextended to the online variants for streaming automatic speech recognition\n(ASR), such as monotonic chunkwise attention (MoChA), the models still have a\nlarge label emission latency because of the unconstrained end-to-end training\nobjective. Previous works tackled this problem by leveraging alignment\ninformation to control the timing to emit tokens during training. In this work,\nwe propose a simple alignment-free regularization method, StableEmit, to\nencourage MoChA to emit tokens earlier. StableEmit discounts the selection\nprobabilities in hard monotonic attention for token boundary detection by a\nconstant factor and regularizes them to recover the total attention mass during\ntraining. As a result, the scale of the selection probabilities is increased,\nand the values can reach a threshold for token emission earlier, leading to a\nreduction of emission latency and deletion errors. Moreover, StableEmit can be\ncombined with methods that constraint alignments to further improve the\naccuracy and latency. Experimental evaluations with LSTM and Conformer encoders\ndemonstrate that StableEmit significantly reduces the recognition errors and\nthe emission latency simultaneously. We also show that the use of alignment\ninformation is complementary in both metrics.",
          "link": "http://arxiv.org/abs/2107.00635",
          "publishedOn": "2021-07-02T01:57:59.240Z",
          "wordCount": 645,
          "title": "StableEmit: Selection Probability Discount for Reducing Emission Latency of Streaming Monotonic Attention ASR. (arXiv:2107.00635v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2103.15722",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liang_C/0/1/0/all/0/1\">Chengdong Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Menglong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiao-Lei Zhang</a>",
          "description": "Self-attention (SA), which encodes vector sequences according to their\npairwise similarity, is widely used in speech recognition due to its strong\ncontext modeling ability. However, when applied to long sequence data, its\naccuracy is reduced. This is caused by the fact that its weighted average\noperator may lead to the dispersion of the attention distribution, which\nresults in the relationship between adjacent signals ignored. To address this\nissue, in this paper, we introduce relative-position-awareness self-attention\n(RPSA). It not only maintains the global-range dependency modeling ability of\nself-attention, but also improves the localness modeling ability. Because the\nlocal window length of the original RPSA is fixed and sensitive to different\ntest data, here we propose Gaussian-based self-attention (GSA) whose window\nlength is learnable and adaptive to the test data automatically. We further\ngeneralize GSA to a new residual Gaussian self-attention (resGSA) for the\nperformance improvement. We apply RPSA, GSA, and resGSA to Transformer-based\nspeech recognition respectively. Experimental results on the AISHELL-1 Mandarin\nspeech recognition corpus demonstrate the effectiveness of the proposed\nmethods. For example, the resGSA-Transformer achieves a character error rate\n(CER) of 5.86% on the test set, which is relative 7.8% lower than that of the\nSA-Transformer. Although the performance of the proposed resGSA-Transformer is\nonly slightly better than that of the RPSA-Transformer, it does not have to\ntune the window length manually.",
          "link": "http://arxiv.org/abs/2103.15722",
          "publishedOn": "2021-07-02T01:57:59.220Z",
          "wordCount": 705,
          "title": "Transformer-based end-to-end speech recognition with residual Gaussian-based self-attention. (arXiv:2103.15722v3 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08126",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Arabskyy_Y/0/1/0/all/0/1\">Yuriy Arabskyy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Agarwal_A/0/1/0/all/0/1\">Aashish Agarwal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dey_S/0/1/0/all/0/1\">Subhadeep Dey</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Koller_O/0/1/0/all/0/1\">Oscar Koller</a>",
          "description": "This paper describes the winning approach in the Shared Task 3 at SwissText\n2021 on Swiss German Speech to Standard German Text, a public competition on\ndialect recognition and translation. Swiss German refers to the multitude of\nAlemannic dialects spoken in the German-speaking parts of Switzerland. Swiss\nGerman differs significantly from standard German in pronunciation, word\ninventory and grammar. It is mostly incomprehensible to native German speakers.\nMoreover, it lacks a standardized written script. To solve the challenging\ntask, we propose a hybrid automatic speech recognition system with a lexicon\nthat incorporates translations, a 1st pass language model that deals with Swiss\nGerman particularities, a transfer-learned acoustic model and a strong neural\nlanguage model for 2nd pass rescoring. Our submission reaches 46.04% BLEU on a\nblind conversational test set and outperforms the second best competitor by a\n12% relative margin.",
          "link": "http://arxiv.org/abs/2106.08126",
          "publishedOn": "2021-07-02T01:57:59.211Z",
          "wordCount": 633,
          "title": "Dialectal Speech Recognition and Translation of Swiss German Speech to Standard German Text: Microsoft's Submission to SwissText 2021. (arXiv:2106.08126v2 [eess.AS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.10182",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Geran_Y/0/1/0/all/0/1\">Yoan G&#xe9;ran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laboureix_B/0/1/0/all/0/1\">Bastien Laboureix</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mascle_C/0/1/0/all/0/1\">Corto Mascle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Richard_V/0/1/0/all/0/1\">Valentin D. Richard</a>",
          "description": "We introduce a new formalisation of languages, called keyboards. We consider\na set of elementary operations (writing/erasing a letter, going to the right or\nto the left,...) and we define a keyboard as a set of finite sequences of such\noperations, called keys. The corresponding language is the set of words\nobtained by applying some sequence of those keys. Unlike classical models of\ncomputation, every key can be applied anytime. We define various classes of\nlanguages based on different sets of elementary operations, and compare their\nexpressive powers. We also compare them to well-known classes of languages\n(Chomsky hierarchy). We obtain a strict hierarchy of languages, whose\nexpressivity is orthogonal to the one of the aforementionned classical models.\n\n--\n\nNous introduisons une nouvelle repr\\'esentation de langages, les claviers. On\nse munit d'un ensemble d'op\\'erations \\'el\\'ementaires (ajout, effacement d'une\nlettre, d\\'eplacement \\`a droite, \\`a gauche, ...), et on d\\'efinit un clavier\ncomme un ensemble de suites finies d'op\\'erations \\'el\\'ementaires, appel\\'ees\ntouches. Son langage sera l'ensemble des mots obtenus en appliquant une suite\nquelconque de touches. Contrairement \\`a des mod\\`eles de calcul classiques,\ntoutes les touches peuvent \\^etre appliqu\\'ees \\`a tout moment. En premier lieu\nnous d\\'efinissons diff\\'erentes classes de claviers en faisant varier\nl'ensemble des op\\'erations \\'el\\'ementaires autoris\\'ees, et nous comparons\nl'expressivit\\'e des classes de langages obtenues. Nous comparons \\'egalement\nces classes \\`a la hi\\'erarchie de Chomsky. Nous obtenons que toutes les\nclasses \\'etudi\\'ees sont diff\\'erentes, et nous caract\\'erisons les classes\ninclues dans les rationnels et les alg\\'ebriques. L'expressivit\\'e des claviers\nsemble orthogonale \\`a celle des mod\\`eles \\'evoqu\\'es pr\\'ec\\'edemment.",
          "link": "http://arxiv.org/abs/2102.10182",
          "publishedOn": "2021-07-02T01:57:59.203Z",
          "wordCount": 740,
          "title": "Keyboards as a new model of computation. (arXiv:2102.10182v3 [cs.FL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.15786",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Shih-Ting Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chambers_N/0/1/0/all/0/1\">Nathanael Chambers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durrett_G/0/1/0/all/0/1\">Greg Durrett</a>",
          "description": "Models of narrative schema knowledge have proven useful for a range of\nevent-related tasks, but they typically do not capture the temporal\nrelationships between events. We propose a single model that addresses both\ntemporal ordering, sorting given events into the order they occurred, and event\ninfilling, predicting new events which fit into an existing temporally-ordered\nsequence. We use a BART-based conditional generation model that can capture\nboth temporality and common event co-occurrence, meaning it can be flexibly\napplied to different tasks in this space. Our model is trained as a denoising\nautoencoder: we take temporally-ordered event sequences, shuffle them, delete\nsome events, and then attempt to recover the original event sequence. This task\nteaches the model to make inferences given incomplete knowledge about the\nevents in an underlying scenario. On the temporal ordering task, we show that\nour model is able to unscramble event sequences from existing datasets without\naccess to explicitly labeled temporal training data, outperforming both a\nBERT-based pairwise model and a BERT-based pointer network. On event infilling,\nhuman evaluation shows that our model is able to generate events that fit\nbetter temporally into the input events when compared to GPT-2 story completion\nmodels.",
          "link": "http://arxiv.org/abs/2012.15786",
          "publishedOn": "2021-07-02T01:57:59.195Z",
          "wordCount": 652,
          "title": "Conditional Generation of Temporally-ordered Event Sequences. (arXiv:2012.15786v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.06605",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Halvani_O/0/1/0/all/0/1\">Oren Halvani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Graner_L/0/1/0/all/0/1\">Lukas Graner</a>",
          "description": "Authorship verification (AV) is a fundamental research task in digital text\nforensics, which addresses the problem of whether two texts were written by the\nsame person. In recent years, a variety of AV methods have been proposed that\nfocus on this problem and can be divided into two categories: The first\ncategory refers to such methods that are based on explicitly defined features,\nwhere one has full control over which features are considered and what they\nactually represent. The second category, on the other hand, relates to such AV\nmethods that are based on implicitly defined features, where no control\nmechanism is involved, so that any character sequence in a text can serve as a\npotential feature. However, AV methods belonging to the second category bear\nthe risk that the topic of the texts may bias their classification predictions,\nwhich in turn may lead to misleading conclusions regarding their results. To\ntackle this problem, we propose a preprocessing technique called POSNoise,\nwhich effectively masks topic-related content in a given text. In this way, AV\nmethods are forced to focus on such text units that are more related to the\nwriting style. Our empirical evaluation based on six AV methods (falling into\nthe second category) and seven corpora shows that POSNoise leads to better\nresults compared to a well-known topic masking approach in 34 out of 42 cases,\nwith an increase in accuracy of up to 10%.",
          "link": "http://arxiv.org/abs/2005.06605",
          "publishedOn": "2021-07-02T01:57:59.187Z",
          "wordCount": 724,
          "title": "POSNoise: An Effective Countermeasure Against Topic Biases in Authorship Analysis. (arXiv:2005.06605v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00323",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pezeshkpour_P/0/1/0/all/0/1\">Pouya Pezeshkpour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Sarthak Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sameer Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wallace_B/0/1/0/all/0/1\">Byron C. Wallace</a>",
          "description": "Training the large deep neural networks that dominate NLP requires large\ndatasets. Many of these are collected automatically or via crowdsourcing, and\nmay exhibit systematic biases or annotation artifacts. By the latter, we mean\ncorrelations between inputs and outputs that are spurious, insofar as they do\nnot represent a generally held causal relationship between features and\nclasses; models that exploit such correlations may appear to perform a given\ntask well, but fail on out of sample data. In this paper we propose methods to\nfacilitate identification of training data artifacts, using new hybrid\napproaches that combine saliency maps (which highlight important input\nfeatures) with instance attribution methods (which retrieve training samples\ninfluential to a given prediction). We show that this proposed training-feature\nattribution approach can be used to uncover artifacts in training data, and use\nit to identify previously unreported artifacts in a few standard NLP datasets.\nWe execute a small user study to evaluate whether these methods are useful to\nNLP researchers in practice, with promising results. We make code for all\nmethods and experiments in this paper available.",
          "link": "http://arxiv.org/abs/2107.00323",
          "publishedOn": "2021-07-02T01:57:59.166Z",
          "wordCount": 617,
          "title": "Combining Feature and Instance Attribution to Detect Artifacts. (arXiv:2107.00323v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2005.00642",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hwang_W/0/1/0/all/0/1\">Wonseok Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yim_J/0/1/0/all/0/1\">Jinyeong Yim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Seunghyun Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Sohee Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_M/0/1/0/all/0/1\">Minjoon Seo</a>",
          "description": "Information Extraction (IE) for semi-structured document images is often\napproached as a sequence tagging problem by classifying each recognized input\ntoken into one of the IOB (Inside, Outside, and Beginning) categories. However,\nsuch problem setup has two inherent limitations that (1) it cannot easily\nhandle complex spatial relationships and (2) it is not suitable for highly\nstructured information, which are nevertheless frequently observed in\nreal-world document images. To tackle these issues, we first formulate the IE\ntask as spatial dependency parsing problem that focuses on the relationship\namong text tokens in the documents. Under this setup, we then propose SPADE\n(SPAtial DEpendency parser) that models highly complex spatial relationships\nand an arbitrary number of information layers in the documents in an end-to-end\nmanner. We evaluate it on various kinds of documents such as receipts, name\ncards, forms, and invoices, and show that it achieves a similar or better\nperformance compared to strong baselines including BERT-based IOB taggger.",
          "link": "http://arxiv.org/abs/2005.00642",
          "publishedOn": "2021-07-02T01:57:59.158Z",
          "wordCount": 640,
          "title": "Spatial Dependency Parsing for Semi-Structured Document Information Extraction. (arXiv:2005.00642v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.01209",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hudson_D/0/1/0/all/0/1\">Drew A. Hudson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zitnick_C/0/1/0/all/0/1\">C. Lawrence Zitnick</a>",
          "description": "We introduce the GANformer, a novel and efficient type of transformer, and\nexplore it for the task of visual generative modeling. The network employs a\nbipartite structure that enables long-range interactions across the image,\nwhile maintaining computation of linear efficiency, that can readily scale to\nhigh-resolution synthesis. It iteratively propagates information from a set of\nlatent variables to the evolving visual features and vice versa, to support the\nrefinement of each in light of the other and encourage the emergence of\ncompositional representations of objects and scenes. In contrast to the classic\ntransformer architecture, it utilizes multiplicative integration that allows\nflexible region-based modulation, and can thus be seen as a generalization of\nthe successful StyleGAN network. We demonstrate the model's strength and\nrobustness through a careful evaluation over a range of datasets, from\nsimulated multi-object environments to rich real-world indoor and outdoor\nscenes, showing it achieves state-of-the-art results in terms of image quality\nand diversity, while enjoying fast learning and better data-efficiency. Further\nqualitative and quantitative experiments offer us an insight into the model's\ninner workings, revealing improved interpretability and stronger\ndisentanglement, and illustrating the benefits and efficacy of our approach. An\nimplementation of the model is available at\nhttps://github.com/dorarad/gansformer.",
          "link": "http://arxiv.org/abs/2103.01209",
          "publishedOn": "2021-07-02T01:57:59.150Z",
          "wordCount": 686,
          "title": "Generative Adversarial Transformers. (arXiv:2103.01209v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00440",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_N/0/1/0/all/0/1\">Ning Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Piji Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Hai-Tao Zheng</a>",
          "description": "Despite pre-trained language models have proven useful for learning\nhigh-quality semantic representations, these models are still vulnerable to\nsimple perturbations. Recent works aimed to improve the robustness of\npre-trained models mainly focus on adversarial training from perturbed examples\nwith similar semantics, neglecting the utilization of different or even\nopposite semantics. Different from the image processing field, the text is\ndiscrete and few word substitutions can cause significant semantic changes. To\nstudy the impact of semantics caused by small perturbations, we conduct a\nseries of pilot experiments and surprisingly find that adversarial training is\nuseless or even harmful for the model to detect these semantic changes. To\naddress this problem, we propose Contrastive Learning with semantIc Negative\nExamples (CLINE), which constructs semantic negative examples unsupervised to\nimprove the robustness under semantically adversarial attacking. By comparing\nwith similar and opposite semantic examples, the model can effectively perceive\nthe semantic changes caused by small perturbations. Empirical results show that\nour approach yields substantial improvements on a range of sentiment analysis,\nreasoning, and reading comprehension tasks. And CLINE also ensures the\ncompactness within the same semantics and separability across different\nsemantics in sentence-level.",
          "link": "http://arxiv.org/abs/2107.00440",
          "publishedOn": "2021-07-02T01:57:59.141Z",
          "wordCount": 638,
          "title": "CLINE: Contrastive Learning with Semantic Negative Examples for Natural Language Understanding. (arXiv:2107.00440v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2101.03988",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Koloski_B/0/1/0/all/0/1\">Boshko Koloski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perdih_T/0/1/0/all/0/1\">Timen Stepi&#x161;nik Perdih</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pollak_S/0/1/0/all/0/1\">Senja Pollak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skrlj_B/0/1/0/all/0/1\">Bla&#x17e; &#x160;krlj</a>",
          "description": "Identification of Fake News plays a prominent role in the ongoing pandemic,\nimpacting multiple aspects of day-to-day life. In this work we present a\nsolution to the shared task titled COVID19 Fake News Detection in English,\nscoring the 50th place amongst 168 submissions. The solution was within 1.5% of\nthe best performing solution. The proposed solution employs a heterogeneous\nrepresentation ensemble, adapted for the classification task via an additional\nneural classification head comprised of multiple hidden layers. The paper\nconsists of detailed ablation studies further displaying the proposed method's\nbehavior and possible implications. The solution is freely available.\n\\url{https://gitlab.com/boshko.koloski/covid19-fake-news}",
          "link": "http://arxiv.org/abs/2101.03988",
          "publishedOn": "2021-07-02T01:57:59.134Z",
          "wordCount": 619,
          "title": "Identification of COVID-19 related Fake News via Neural Stacking. (arXiv:2101.03988v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.15127",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Danni Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niehues_J/0/1/0/all/0/1\">Jan Niehues</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cross_J/0/1/0/all/0/1\">James Cross</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guzman_F/0/1/0/all/0/1\">Francisco Guzm&#xe1;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xian Li</a>",
          "description": "Multilingual neural machine translation has shown the capability of directly\ntranslating between language pairs unseen in training, i.e. zero-shot\ntranslation. Despite being conceptually attractive, it often suffers from low\noutput quality. The difficulty of generalizing to new translation directions\nsuggests the model representations are highly specific to those language pairs\nseen in training. We demonstrate that a main factor causing the\nlanguage-specific representations is the positional correspondence to input\ntokens. We show that this can be easily alleviated by removing residual\nconnections in an encoder layer. With this modification, we gain up to 18.5\nBLEU points on zero-shot translation while retaining quality on supervised\ndirections. The improvements are particularly prominent between related\nlanguages, where our proposed model outperforms pivot-based translation.\nMoreover, our approach allows easy integration of new languages, which\nsubstantially expands translation coverage. By thorough inspections of the\nhidden layer outputs, we show that our approach indeed leads to more\nlanguage-independent representations.",
          "link": "http://arxiv.org/abs/2012.15127",
          "publishedOn": "2021-07-02T01:57:59.115Z",
          "wordCount": 617,
          "title": "Improving Zero-Shot Translation by Disentangling Positional Information. (arXiv:2012.15127v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00636",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Inaguma_H/0/1/0/all/0/1\">Hirofumi Inaguma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yan_B/0/1/0/all/0/1\">Brian Yan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dalmia_S/0/1/0/all/0/1\">Siddharth Dalmia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gu_P/0/1/0/all/0/1\">Pengcheng Gu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shi_J/0/1/0/all/0/1\">Jiatong Shi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Duh_K/0/1/0/all/0/1\">Kevin Duh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>",
          "description": "This paper describes the ESPnet-ST group's IWSLT 2021 submission in the\noffline speech translation track. This year we made various efforts on training\ndata, architecture, and audio segmentation. On the data side, we investigated\nsequence-level knowledge distillation (SeqKD) for end-to-end (E2E) speech\ntranslation. Specifically, we used multi-referenced SeqKD from multiple\nteachers trained on different amounts of bitext. On the architecture side, we\nadopted the Conformer encoder and the Multi-Decoder architecture, which equips\ndedicated decoders for speech recognition and translation tasks in a unified\nencoder-decoder model and enables search in both source and target language\nspaces during inference. We also significantly improved audio segmentation by\nusing the pyannote.audio toolkit and merging multiple short segments for long\ncontext modeling. Experimental evaluations showed that each of them contributed\nto large improvements in translation performance. Our best E2E system combined\nall the above techniques with model ensembling and achieved 31.4 BLEU on the\n2-ref of tst2021 and 21.2 BLEU and 19.3 BLEU on the two single references of\ntst2021.",
          "link": "http://arxiv.org/abs/2107.00636",
          "publishedOn": "2021-07-02T01:57:59.098Z",
          "wordCount": 618,
          "title": "ESPnet-ST IWSLT 2021 Offline Speech Translation System. (arXiv:2107.00636v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00368",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Baradaran_R/0/1/0/all/0/1\">Razieh Baradaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amirkhani_H/0/1/0/all/0/1\">Hossein Amirkhani</a>",
          "description": "Machine Reading Comprehension (MRC) is an active field in natural language\nprocessing with many successful developed models in recent years. Despite their\nhigh in-distribution accuracy, these models suffer from two issues: high\ntraining cost and low out-of-distribution accuracy. Even though some approaches\nhave been presented to tackle the generalization problem, they have high,\nintolerable training costs. In this paper, we investigate the effect of\nensemble learning approach to improve generalization of MRC systems without\nretraining a big model. After separately training the base models with\ndifferent structures on different datasets, they are ensembled using weighting\nand stacking approaches in probabilistic and non-probabilistic settings. Three\nconfigurations are investigated including heterogeneous, homogeneous, and\nhybrid on eight datasets and six state-of-the-art models. We identify the\nimportant factors in the effectiveness of ensemble methods. Also, we compare\nthe robustness of ensemble and fine-tuned models against data distribution\nshifts. The experimental results show the effectiveness and robustness of the\nensemble approach in improving the out-of-distribution accuracy of MRC systems,\nespecially when the base models are similar in accuracies.",
          "link": "http://arxiv.org/abs/2107.00368",
          "publishedOn": "2021-07-02T01:57:59.088Z",
          "wordCount": 608,
          "title": "Ensemble Learning-Based Approach for Improving Generalization Capability of Machine Reading Comprehension Systems. (arXiv:2107.00368v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00395",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yunxin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_B/0/1/0/all/0/1\">Baotian Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qingcai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_Y/0/1/0/all/0/1\">Yang Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaolong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yuxin Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lin Ma</a>",
          "description": "Previous works indicate that the glyph of Chinese characters contains rich\nsemantic information and has the potential to enhance the representation of\nChinese characters. The typical method to utilize the glyph features is by\nincorporating them into the character embedding space. Inspired by previous\nmethods, we innovatively propose a Chinese pre-trained representation model\nnamed as GlyphCRM, which abandons the ID-based character embedding method yet\nsolely based on sequential character images. We render each character into a\nbinary grayscale image and design two-channel position feature maps for it.\nFormally, we first design a two-layer residual convolutional neural network,\nnamely HanGlyph to generate the initial glyph representation of Chinese\ncharacters, and subsequently adopt multiple bidirectional encoder Transformer\nblocks as the superstructure to capture the context-sensitive information.\nMeanwhile, we feed the glyph features extracted from each layer of the HanGlyph\nmodule into the underlying Transformer blocks by skip-connection method to\nfully exploit the glyph features of Chinese characters. As the HanGlyph module\ncan obtain a sufficient glyph representation of any Chinese character, the\nlong-standing out-of-vocabulary problem could be effectively solved. Extensive\nexperimental results indicate that GlyphCRM substantially outperforms the\nprevious BERT-based state-of-the-art model on 9 fine-tuning tasks, and it has\nstrong transferability and generalization on specialized fields and\nlow-resource tasks. We hope this work could spark further research beyond the\nrealms of well-established representation of Chinese texts.",
          "link": "http://arxiv.org/abs/2107.00395",
          "publishedOn": "2021-07-02T01:57:59.069Z",
          "wordCount": 683,
          "title": "GlyphCRM: Bidirectional Encoder Representation for Chinese Character with its Glyph. (arXiv:2107.00395v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00334",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ri_R/0/1/0/all/0/1\">Ryokan Ri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakazawa_T/0/1/0/all/0/1\">Toshiaki Nakazawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsuruoka_Y/0/1/0/all/0/1\">Yoshimasa Tsuruoka</a>",
          "description": "Placeholder translation systems enable the users to specify how a specific\nphrase is translated in the output sentence. The system is trained to output\nspecial placeholder tokens, and the user-specified term is injected into the\noutput through the context-free replacement of the placeholder token. However,\nthis approach could result in ungrammatical sentences because it is often the\ncase that the specified term needs to be inflected according to the context of\nthe output, which is unknown before the translation. To address this problem,\nwe propose a novel method of placeholder translation that can inflect specified\nterms according to the grammatical construction of the output sentence. We\nextend the sequence-to-sequence architecture with a character-level decoder\nthat takes the lemma of a user-specified term and the words generated from the\nword-level decoder to output the correct inflected form of the lemma. We\nevaluate our approach with a Japanese-to-English translation task in the\nscientific writing domain, and show that our model can incorporate specified\nterms in the correct form more successfully than other comparable models.",
          "link": "http://arxiv.org/abs/2107.00334",
          "publishedOn": "2021-07-02T01:57:59.062Z",
          "wordCount": 601,
          "title": "Modeling Target-side Inflection in Placeholder Translation. (arXiv:2107.00334v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00414",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lauscher_A/0/1/0/all/0/1\">Anne Lauscher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ko_B/0/1/0/all/0/1\">Brandon Ko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuhl_B/0/1/0/all/0/1\">Bailey Kuhl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johnson_S/0/1/0/all/0/1\">Sophie Johnson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jurgens_D/0/1/0/all/0/1\">David Jurgens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohan_A/0/1/0/all/0/1\">Arman Cohan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lo_K/0/1/0/all/0/1\">Kyle Lo</a>",
          "description": "Citation context analysis (CCA) is an important task in natural language\nprocessing that studies how and why scholars discuss each others' work. Despite\nbeing studied for decades, traditional frameworks for CCA have largely relied\non overly-simplistic assumptions of how authors cite, which ignore several\nimportant phenomena. For instance, scholarly papers often contain rich\ndiscussions of cited work that span multiple sentences and express multiple\nintents concurrently. Yet, CCA is typically approached as a single-sentence,\nsingle-label classification task, and thus existing datasets fail to capture\nthis interesting discourse. In our work, we address this research gap by\nproposing a novel framework for CCA as a document-level context extraction and\nlabeling task. We release MultiCite, a new dataset of 12,653 citation contexts\nfrom over 1,200 computational linguistics papers. Not only is it the largest\ncollection of expert-annotated citation contexts to-date, MultiCite contains\nmulti-sentence, multi-label citation contexts within full paper texts. Finally,\nwe demonstrate how our dataset, while still usable for training classic CCA\nmodels, also supports the development of new types of models for CCA beyond\nfixed-width text classification. We release our code and dataset at\nhttps://github.com/allenai/multicite.",
          "link": "http://arxiv.org/abs/2107.00414",
          "publishedOn": "2021-07-02T01:57:59.034Z",
          "wordCount": 630,
          "title": "MultiCite: Modeling realistic citations requires moving beyond the single-sentence single-label setting. (arXiv:2107.00414v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00281",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lauscher_A/0/1/0/all/0/1\">Anne Lauscher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wachsmuth_H/0/1/0/all/0/1\">Henning Wachsmuth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glavas_G/0/1/0/all/0/1\">Goran Glava&#x161;</a>",
          "description": "Despite extensive research in the past years, the computational modeling of\nargumentation remains challenging. The primary reason lies in the inherent\ncomplexity of the human processes behind, which commonly requires the\nintegration of extensive knowledge far beyond what is needed for many other\nnatural language understanding tasks. Existing work on the mining, assessment,\nreasoning, and generation of arguments acknowledges this issue, calling for\nmore research on the integration of common sense and world knowledge into\ncomputational models. However, a systematic effort to collect and organize the\ntypes of knowledge needed is still missing, hindering targeted progress in the\nfield. In this opinionated survey paper, we address the issue by (1) proposing\na pyramid of types of knowledge required in computational argumentation, (2)\nbriefly discussing the state of the art on the role and integration of these\ntypes in the field, and (3) outlining the main challenges for future work.",
          "link": "http://arxiv.org/abs/2107.00281",
          "publishedOn": "2021-07-02T01:57:59.002Z",
          "wordCount": 588,
          "title": "Scientia Potentia Est -- On the Role of Knowledge in Computational Argumentation. (arXiv:2107.00281v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00411",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gajbhiye_A/0/1/0/all/0/1\">Amit Gajbhiye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fomicheva_M/0/1/0/all/0/1\">Marina Fomicheva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alva_Manchego_F/0/1/0/all/0/1\">Fernando Alva-Manchego</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blain_F/0/1/0/all/0/1\">Fr&#xe9;d&#xe9;ric Blain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Obamuyide_A/0/1/0/all/0/1\">Abiola Obamuyide</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aletras_N/0/1/0/all/0/1\">Nikolaos Aletras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Specia_L/0/1/0/all/0/1\">Lucia Specia</a>",
          "description": "Quality Estimation (QE) is the task of automatically predicting Machine\nTranslation quality in the absence of reference translations, making it\napplicable in real-time settings, such as translating online social media\nconversations. Recent success in QE stems from the use of multilingual\npre-trained representations, where very large models lead to impressive\nresults. However, the inference time, disk and memory requirements of such\nmodels do not allow for wide usage in the real world. Models trained on\ndistilled pre-trained representations remain prohibitively large for many usage\nscenarios. We instead propose to directly transfer knowledge from a strong QE\nteacher model to a much smaller model with a different, shallower architecture.\nWe show that this approach, in combination with data augmentation, leads to\nlight-weight QE models that perform competitively with distilled pre-trained\nrepresentations with 8x fewer parameters.",
          "link": "http://arxiv.org/abs/2107.00411",
          "publishedOn": "2021-07-02T01:57:58.995Z",
          "wordCount": 569,
          "title": "Knowledge Distillation for Quality Estimation. (arXiv:2107.00411v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00333",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guinovart_X/0/1/0/all/0/1\">Xavier G&#xf3;mez Guinovart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_Dios_I/0/1/0/all/0/1\">Itziar Gonzalez-Dios</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliver_A/0/1/0/all/0/1\">Antoni Oliver</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rigau_G/0/1/0/all/0/1\">German Rigau</a>",
          "description": "Language resources are necessary for language processing,but building them is\ncostly, involves many researches from different areas and needs constant\nupdating. In this paper, we describe the crosslingual framework used for\ndeveloping the Multilingual Central Repository (MCR), a multilingual knowledge\nbase that includes wordnets of Basque, Catalan, English, Galician, Portuguese,\nSpanish and the following ontologies: Base Concepts, Top Ontology, WordNet\nDomains and Suggested Upper Merged Ontology. We present the story of MCR, its\nstate in 2017 and the developed tools.",
          "link": "http://arxiv.org/abs/2107.00333",
          "publishedOn": "2021-07-02T01:57:58.897Z",
          "wordCount": 542,
          "title": "Multilingual Central Repository: a Cross-lingual Framework for Developing Wordnets. (arXiv:2107.00333v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00308",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Halpern_B/0/1/0/all/0/1\">Bence Mark Halpern</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fritsch_J/0/1/0/all/0/1\">Julian Fritsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hermann_E/0/1/0/all/0/1\">Enno Hermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Son_R/0/1/0/all/0/1\">Rob van Son</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scharenborg_O/0/1/0/all/0/1\">Odette Scharenborg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+_Doss_M/0/1/0/all/0/1\">Mathew Magimai.-Doss</a>",
          "description": "The development of pathological speech systems is currently hindered by the\nlack of a standardised objective evaluation framework. In this work, (1) we\nutilise existing detection and analysis techniques to propose a general\nframework for the consistent evaluation of synthetic pathological speech. This\nframework evaluates the voice quality and the intelligibility aspects of speech\nand is shown to be complementary using our experiments. (2) Using our proposed\nevaluation framework, we develop and test a dysarthric voice conversion system\n(VC) using CycleGAN-VC and a PSOLA-based speech rate modification technique. We\nshow that the developed system is able to synthesise dysarthric speech with\ndifferent levels of speech intelligibility.",
          "link": "http://arxiv.org/abs/2107.00308",
          "publishedOn": "2021-07-02T01:57:58.677Z",
          "wordCount": 570,
          "title": "An Objective Evaluation Framework for Pathological Speech Synthesis. (arXiv:2107.00308v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00189",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xi_X/0/1/0/all/0/1\">Xiangyu Xi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_W/0/1/0/all/0/1\">Wei Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shikun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Quanxiu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Huixing Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Wei Wu</a>",
          "description": "Capturing interactions among event arguments is an essential step towards\nrobust event argument extraction (EAE). However, existing efforts in this\ndirection suffer from two limitations: 1) The argument role type information of\ncontextual entities is mainly utilized as training signals, ignoring the\npotential merits of directly adopting it as semantically rich input features;\n2) The argument-level sequential semantics, which implies the overall\ndistribution pattern of argument roles over an event mention, is not well\ncharacterized. To tackle the above two bottlenecks, we formalize EAE as a\nSeq2Seq-like learning problem for the first time, where a sentence with a\nspecific event trigger is mapped to a sequence of event argument roles. A\nneural architecture with a novel Bi-directional Entity-level Recurrent Decoder\n(BERD) is proposed to generate argument roles by incorporating contextual\nentities' argument role predictions, like a word-by-word text generation\nprocess, thereby distinguishing implicit argument distribution patterns within\nan event more accurately.",
          "link": "http://arxiv.org/abs/2107.00189",
          "publishedOn": "2021-07-02T01:57:58.668Z",
          "wordCount": 594,
          "title": "Capturing Event Argument Interaction via A Bi-Directional Entity-Level Recurrent Decoder. (arXiv:2107.00189v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00318",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ri_R/0/1/0/all/0/1\">Ryokan Ri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakazawa_T/0/1/0/all/0/1\">Toshiaki Nakazawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsuruoka_Y/0/1/0/all/0/1\">Yoshimasa Tsuruoka</a>",
          "description": "For Japanese-to-English translation, zero pronouns in Japanese pose a\nchallenge, since the model needs to infer and produce the corresponding pronoun\nin the target side of the English sentence. However, although fully resolving\nzero pronouns often needs discourse context, in some cases, the local context\nwithin a sentence gives clues to the inference of the zero pronoun. In this\nstudy, we propose a data augmentation method that provides additional training\nsignals for the translation model to learn correlations between local context\nand zero pronouns. We show that the proposed method significantly improves the\naccuracy of zero pronoun translation with machine translation experiments in\nthe conversational domain.",
          "link": "http://arxiv.org/abs/2107.00318",
          "publishedOn": "2021-07-02T01:57:58.653Z",
          "wordCount": 533,
          "title": "Zero-pronoun Data Augmentation for Japanese-to-English Translation. (arXiv:2107.00318v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00186",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Zhiyuan Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuexin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1\">Guo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xingyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Akshat Gupta</a>",
          "description": "Spoken dialogue systems such as Siri and Alexa provide great convenience to\npeople's everyday life. However, current spoken language understanding (SLU)\npipelines largely depend on automatic speech recognition (ASR) modules, which\nrequire a large amount of language-specific training data. In this paper, we\npropose a Transformer-based SLU system that works directly on phones. This\nacoustic-based SLU system consists of only two blocks and does not require the\npresence of ASR module. The first block is a universal phone recognition\nsystem, and the second block is a Transformer-based language model for phones.\nWe verify the effectiveness of the system on an intent classification dataset\nin Mandarin Chinese.",
          "link": "http://arxiv.org/abs/2107.00186",
          "publishedOn": "2021-07-02T01:57:58.643Z",
          "wordCount": 546,
          "title": "Word-Free Spoken Language Understanding for Mandarin-Chinese. (arXiv:2107.00186v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00279",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Dan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_M/0/1/0/all/0/1\">Mengge Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoxi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yuchen Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_L/0/1/0/all/0/1\">Lirong Dai</a>",
          "description": "This paper describes USTC-NELSLIP's submissions to the IWSLT2021 Simultaneous\nSpeech Translation task. We proposed a novel simultaneous translation model,\nCross Attention Augmented Transducer (CAAT), which extends conventional RNN-T\nto sequence-to-sequence tasks without monotonic constraints, e.g., simultaneous\ntranslation. Experiments on speech-to-text (S2T) and text-to-text (T2T)\nsimultaneous translation tasks shows CAAT achieves better quality-latency\ntrade-offs compared to \\textit{wait-k}, one of the previous state-of-the-art\napproaches. Based on CAAT architecture and data augmentation, we build S2T and\nT2T simultaneous translation systems in this evaluation campaign. Compared to\nlast year's optimal systems, our S2T simultaneous translation system improves\nby an average of 11.3 BLEU for all latency regimes, and our T2T simultaneous\ntranslation system improves by an average of 4.6 BLEU.",
          "link": "http://arxiv.org/abs/2107.00279",
          "publishedOn": "2021-07-02T01:57:58.634Z",
          "wordCount": 565,
          "title": "The USTC-NELSLIP Systems for Simultaneous Speech Translation Task at IWSLT 2021. (arXiv:2107.00279v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00315",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Varshney_N/0/1/0/all/0/1\">Neeraj Varshney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Swaroop Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1\">Chitta Baral</a>",
          "description": "Standard NLP tasks do not incorporate several common real-world scenarios\nsuch as seeking clarifications about the question, taking advantage of clues,\nabstaining in order to avoid incorrect answers, etc. This difference in task\nformulation hinders the adoption of NLP systems in real-world settings. In this\nwork, we take a step towards bridging this gap and present a multi-stage task\nthat simulates a typical human-human questioner-responder interaction such as\nan interview. Specifically, the system is provided with question\nsimplifications, knowledge statements, examples, etc. at various stages to\nimprove its prediction when it is not sufficiently confident. We instantiate\nthe proposed task in Natural Language Inference setting where a system is\nevaluated on both in-domain and out-of-domain (OOD) inputs. We conduct\ncomprehensive experiments and find that the multi-stage formulation of our task\nleads to OOD generalization performance improvement up to 2.29% in Stage 1,\n1.91% in Stage 2, 54.88% in Stage 3, and 72.02% in Stage 4 over the standard\nunguided prediction. However, our task leaves a significant challenge for NLP\nresearchers to further improve OOD performance at each stage.",
          "link": "http://arxiv.org/abs/2107.00315",
          "publishedOn": "2021-07-02T01:57:58.602Z",
          "wordCount": 626,
          "title": "Interviewer-Candidate Role Play: Towards Developing Real-World NLP Systems. (arXiv:2107.00315v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00061",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Clark_E/0/1/0/all/0/1\">Elizabeth Clark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+August_T/0/1/0/all/0/1\">Tal August</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Serrano_S/0/1/0/all/0/1\">Sofia Serrano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haduong_N/0/1/0/all/0/1\">Nikita Haduong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gururangan_S/0/1/0/all/0/1\">Suchin Gururangan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>",
          "description": "Human evaluations are typically considered the gold standard in natural\nlanguage generation, but as models' fluency improves, how well can evaluators\ndetect and judge machine-generated text? We run a study assessing non-experts'\nability to distinguish between human- and machine-authored text (GPT2 and GPT3)\nin three domains (stories, news articles, and recipes). We find that, without\ntraining, evaluators distinguished between GPT3- and human-authored text at\nrandom chance level. We explore three approaches for quickly training\nevaluators to better identify GPT3-authored text (detailed instructions,\nannotated examples, and paired examples) and find that while evaluators'\naccuracy improved up to 55%, it did not significantly improve across the three\ndomains. Given the inconsistent results across text domains and the often\ncontradictory reasons evaluators gave for their judgments, we examine the role\nuntrained human evaluations play in NLG evaluation and provide recommendations\nto NLG researchers for improving human evaluations of text generated from\nstate-of-the-art models.",
          "link": "http://arxiv.org/abs/2107.00061",
          "publishedOn": "2021-07-02T01:57:58.594Z",
          "wordCount": 594,
          "title": "All That's 'Human' Is Not Gold: Evaluating Human Evaluation of Generated Text. (arXiv:2107.00061v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00176",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yadav_S/0/1/0/all/0/1\">Shweta Yadav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_D/0/1/0/all/0/1\">Deepak Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abacha_A/0/1/0/all/0/1\">Asma Ben Abacha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demner_Fushman_D/0/1/0/all/0/1\">Dina Demner-Fushman</a>",
          "description": "The growth of online consumer health questions has led to the necessity for\nreliable and accurate question answering systems. A recent study showed that\nmanual summarization of consumer health questions brings significant\nimprovement in retrieving relevant answers. However, the automatic\nsummarization of long questions is a challenging task due to the lack of\ntraining data and the complexity of the related subtasks, such as the question\nfocus and type recognition. In this paper, we introduce a reinforcement\nlearning-based framework for abstractive question summarization. We propose two\nnovel rewards obtained from the downstream tasks of (i) question-type\nidentification and (ii) question-focus recognition to regularize the question\ngeneration model. These rewards ensure the generation of semantically valid\nquestions and encourage the inclusion of key medical entities/foci in the\nquestion summary. We evaluated our proposed method on two benchmark datasets\nand achieved higher performance over state-of-the-art models. The manual\nevaluation of the summaries reveals that the generated questions are more\ndiverse and have fewer factual inconsistencies than the baseline summaries",
          "link": "http://arxiv.org/abs/2107.00176",
          "publishedOn": "2021-07-02T01:57:58.568Z",
          "wordCount": 609,
          "title": "Reinforcement Learning for Abstractive Question Summarization with Question-aware Semantic Rewards. (arXiv:2107.00176v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00080",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Radford_B/0/1/0/all/0/1\">Benjamin J. Radford</a>",
          "description": "Text data are an important source of detailed information about social and\npolitical events. Automated systems parse large volumes of text data to infer\nor extract structured information that describes actors, actions, dates, times,\nand locations. One of these sub-tasks is geocoding: predicting the geographic\ncoordinates associated with events or locations described by a given text. We\npresent an end-to-end probabilistic model for geocoding text data.\nAdditionally, we collect a novel data set for evaluating the performance of\ngeocoding systems. We compare the model-based solution, called ELECTRo-map, to\nthe current state-of-the-art open source system for geocoding texts for event\ndata. Finally, we discuss the benefits of end-to-end model-based geocoding,\nincluding principled uncertainty estimation and the ability of these models to\nleverage contextual information.",
          "link": "http://arxiv.org/abs/2107.00080",
          "publishedOn": "2021-07-02T01:57:58.498Z",
          "wordCount": 565,
          "title": "Regressing Location on Text for Probabilistic Geocoding. (arXiv:2107.00080v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00077",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+McCarthy_W/0/1/0/all/0/1\">William P. McCarthy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hawkins_R/0/1/0/all/0/1\">Robert D. Hawkins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haoliang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holdaway_C/0/1/0/all/0/1\">Cameron Holdaway</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1\">Judith E. Fan</a>",
          "description": "Many real-world tasks require agents to coordinate their behavior to achieve\nshared goals. Successful collaboration requires not only adopting the same\ncommunicative conventions, but also grounding these conventions in the same\ntask-appropriate conceptual abstractions. We investigate how humans use natural\nlanguage to collaboratively solve physical assembly problems more effectively\nover time. Human participants were paired up in an online environment to\nreconstruct scenes containing two block towers. One participant could see the\ntarget towers, and sent assembly instructions for the other participant to\nreconstruct. Participants provided increasingly concise instructions across\nrepeated attempts on each pair of towers, using higher-level referring\nexpressions that captured each scene's hierarchical structure. To explain these\nfindings, we extend recent probabilistic models of ad-hoc convention formation\nwith an explicit perceptual learning mechanism. These results shed light on the\ninductive biases that enable intelligent agents to coordinate upon shared\nprocedural abstractions.",
          "link": "http://arxiv.org/abs/2107.00077",
          "publishedOn": "2021-07-02T01:57:58.489Z",
          "wordCount": 579,
          "title": "Learning to communicate about shared procedural abstractions. (arXiv:2107.00077v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00042",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Catala_N/0/1/0/all/0/1\">Neus Catal&#xe0;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baixeries_J/0/1/0/all/0/1\">Jaume Baixeries</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrer_Cancho_R/0/1/0/all/0/1\">Ramon Ferrer-Cancho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Padro_L/0/1/0/all/0/1\">Llu&#xed;s Padr&#xf3;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hernandez_Fernandez_A/0/1/0/all/0/1\">Antoni Hern&#xe1;ndez-Fern&#xe1;ndez</a>",
          "description": "In his pioneering research, G. K. Zipf formulated a couple of statistical\nlaws on the relationship between the frequency of a word with its number of\nmeanings: the law of meaning distribution, relating the frequency of a word and\nits frequency rank, and the meaning-frequency law, relating the frequency of a\nword with its number of meanings. Although these laws were formulated more than\nhalf a century ago, they have been only investigated in a few languages. Here\nwe present the first study of these laws in Catalan.\n\nWe verify these laws in Catalan via the relationship among their exponents\nand that of the rank-frequency law. We present a new protocol for the analysis\nof these Zipfian laws that can be extended to other languages. We report the\nfirst evidence of two marked regimes for these laws in written language and\nspeech, paralleling the two regimes in Zipf's rank-frequency law in large\nmulti-author corpora discovered in early 2000s. Finally, the implications of\nthese two regimes will be discussed.",
          "link": "http://arxiv.org/abs/2107.00042",
          "publishedOn": "2021-07-02T01:57:58.444Z",
          "wordCount": 603,
          "title": "Zipf's laws of meaning in Catalan. (arXiv:2107.00042v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00124",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ganesan_A/0/1/0/all/0/1\">Ashwinkumar Ganesan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferraro_F/0/1/0/all/0/1\">Francis Ferraro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oates_T/0/1/0/all/0/1\">Tim Oates</a>",
          "description": "We propose a Bi-Directional Manifold Alignment (BDMA) that learns a\nnon-linear mapping between two manifolds by explicitly training it to be\nbijective. We demonstrate BDMA by training a model for a pair of languages\nrather than individual, directed source and target combinations, reducing the\nnumber of models by 50%. We show that models trained with BDMA in the \"forward\"\n(source to target) direction can successfully map words in the \"reverse\"\n(target to source) direction, yielding equivalent (or better) performance to\nstandard unidirectional translation models where the source and target language\nis flipped. We also show how BDMA reduces the overall size of the model.",
          "link": "http://arxiv.org/abs/2107.00124",
          "publishedOn": "2021-07-02T01:57:58.400Z",
          "wordCount": 555,
          "title": "Learning a Reversible Embedding Mapping using Bi-Directional Manifold Alignment. (arXiv:2107.00124v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00152",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cao_S/0/1/0/all/0/1\">Shuyang Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lu Wang</a>",
          "description": "We investigate the less-explored task of generating open-ended questions that\nare typically answered by multiple sentences. We first define a new question\ntype ontology which differentiates the nuanced nature of questions better than\nwidely used question words. A new dataset with 4,959 questions is labeled based\non the new ontology. We then propose a novel question type-aware question\ngeneration framework, augmented by a semantic graph representation, to jointly\npredict question focuses and produce the question. Based on this framework, we\nfurther use both exemplars and automatically generated templates to improve\ncontrollability and diversity. Experiments on two newly collected large-scale\ndatasets show that our model improves question quality over competitive\ncomparisons based on automatic metrics. Human judges also rate our model\noutputs highly in answerability, coverage of scope, and overall quality.\nFinally, our model variants with templates can produce questions with enhanced\ncontrollability and diversity.",
          "link": "http://arxiv.org/abs/2107.00152",
          "publishedOn": "2021-07-02T01:57:58.365Z",
          "wordCount": 578,
          "title": "Controllable Open-ended Question Generation with A New Question Type Ontology. (arXiv:2107.00152v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00175",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_K/0/1/0/all/0/1\">Keli Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Siyuan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Meiqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhongfeng Wang</a>",
          "description": "Despite the great success in Natural Language Processing (NLP) area, large\npre-trained language models like BERT are not well-suited for\nresource-constrained or real-time applications owing to the large number of\nparameters and slow inference speed. Recently, compressing and accelerating\nBERT have become important topics. By incorporating a parameter-sharing\nstrategy, ALBERT greatly reduces the number of parameters while achieving\ncompetitive performance. Nevertheless, ALBERT still suffers from a long\ninference time. In this work, we propose the ELBERT, which significantly\nimproves the average inference speed compared to ALBERT due to the proposed\nconfidence-window based early exit mechanism, without introducing additional\nparameters or extra training overhead. Experimental results show that ELBERT\nachieves an adaptive inference speedup varying from 2$\\times$ to 10$\\times$\nwith negligible accuracy degradation compared to ALBERT on various datasets.\nBesides, ELBERT achieves higher accuracy than existing early exit methods used\nfor accelerating BERT under the same computation cost. Furthermore, to\nunderstand the principle of the early exit mechanism, we also visualize the\ndecision-making process of it in ELBERT.",
          "link": "http://arxiv.org/abs/2107.00175",
          "publishedOn": "2021-07-02T01:57:58.312Z",
          "wordCount": 599,
          "title": "Elbert: Fast Albert with Confidence-Window Based Early Exit. (arXiv:2107.00175v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16038",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zijun Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoya Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xiaofei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1\">Yuxian Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ao_X/0/1/0/all/0/1\">Xiang Ao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Q/0/1/0/all/0/1\">Qing He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiwei Li</a>",
          "description": "Recent pretraining models in Chinese neglect two important aspects specific\nto the Chinese language: glyph and pinyin, which carry significant syntax and\nsemantic information for language understanding. In this work, we propose\nChineseBERT, which incorporates both the {\\it glyph} and {\\it pinyin}\ninformation of Chinese characters into language model pretraining. The glyph\nembedding is obtained based on different fonts of a Chinese character, being\nable to capture character semantics from the visual features, and the pinyin\nembedding characterizes the pronunciation of Chinese characters, which handles\nthe highly prevalent heteronym phenomenon in Chinese (the same character has\ndifferent pronunciations with different meanings). Pretrained on large-scale\nunlabeled Chinese corpus, the proposed ChineseBERT model yields significant\nperformance boost over baseline models with fewer training steps. The porpsoed\nmodel achieves new SOTA performances on a wide range of Chinese NLP tasks,\nincluding machine reading comprehension, natural language inference, text\nclassification, sentence pair matching, and competitive performances in named\nentity recognition. Code and pretrained models are publicly available at\nhttps://github.com/ShannonAI/ChineseBert.",
          "link": "http://arxiv.org/abs/2106.16038",
          "publishedOn": "2021-07-01T01:59:34.663Z",
          "wordCount": 613,
          "title": "ChineseBERT: Chinese Pretraining Enhanced by Glyph and Pinyin Information. (arXiv:2106.16038v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16034",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mahari_R/0/1/0/all/0/1\">Robert Zev Mahari</a>",
          "description": "This paper demonstrate how NLP can be used to address an unmet need of the\nlegal community and increase access to justice. The paper introduces Legal\nPrecedent Prediction (LPP), the task of predicting relevant passages from\nprecedential court decisions given the context of a legal argument. To this\nend, the paper showcases a BERT model, trained on 530,000 examples of legal\narguments made by U.S. federal judges, to predict relevant passages from\nprecedential court decisions given the context of a legal argument. In 96% of\nunseen test examples the correct target passage is among the top-10 predicted\npassages. The same model is able to predict relevant precedent given a short\nsummary of a complex and unseen legal brief, predicting the precedent that was\nactually cited by the brief's co-author, former U.S. Solicitor General and\ncurrent U.S. Supreme Court Justice Elena Kagan.",
          "link": "http://arxiv.org/abs/2106.16034",
          "publishedOn": "2021-07-01T01:59:34.535Z",
          "wordCount": 568,
          "title": "AutoLAW: Augmented Legal Reasoning through Legal Precedent Prediction. (arXiv:2106.16034v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.00510",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ghosal_D/0/1/0/all/0/1\">Deepanway Ghosal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_P/0/1/0/all/0/1\">Pengfei Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_S/0/1/0/all/0/1\">Siqi Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Majumder_N/0/1/0/all/0/1\">Navonil Majumder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihalcea_R/0/1/0/all/0/1\">Rada Mihalcea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poria_S/0/1/0/all/0/1\">Soujanya Poria</a>",
          "description": "Commonsense inference to understand and explain human language is a\nfundamental research problem in natural language processing. Explaining human\nconversations poses a great challenge as it requires contextual understanding,\nplanning, inference, and several aspects of reasoning including causal,\ntemporal, and commonsense reasoning. In this work, we introduce CIDER -- a\nmanually curated dataset that contains dyadic dialogue explanations in the form\nof implicit and explicit knowledge triplets inferred using contextual\ncommonsense inference. Extracting such rich explanations from conversations can\nbe conducive to improving several downstream applications. The annotated\ntriplets are categorized by the type of commonsense knowledge present (e.g.,\ncausal, conditional, temporal). We set up three different tasks conditioned on\nthe annotated dataset: Dialogue-level Natural Language Inference, Span\nExtraction, and Multi-choice Span Selection. Baseline results obtained with\ntransformer-based models reveal that the tasks are difficult, paving the way\nfor promising future research. The dataset and the baseline implementations are\npublicly available at https://cider-task.github.io/cider/.",
          "link": "http://arxiv.org/abs/2106.00510",
          "publishedOn": "2021-07-01T01:59:34.123Z",
          "wordCount": 618,
          "title": "CIDER: Commonsense Inference for Dialogue Explanation and Reasoning. (arXiv:2106.00510v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.16055",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Denisov_P/0/1/0/all/0/1\">Pavel Denisov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mager_M/0/1/0/all/0/1\">Manuel Mager</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vu_N/0/1/0/all/0/1\">Ngoc Thang Vu</a>",
          "description": "This paper describes the submission to the IWSLT 2021 Low-Resource Speech\nTranslation Shared Task by IMS team. We utilize state-of-the-art models\ncombined with several data augmentation, multi-task and transfer learning\napproaches for the automatic speech recognition (ASR) and machine translation\n(MT) steps of our cascaded system. Moreover, we also explore the feasibility of\na full end-to-end speech translation (ST) model in the case of very constrained\namount of ground truth labeled data. Our best system achieves the best\nperformance among all submitted systems for Congolese Swahili to English and\nFrench with BLEU scores 7.7 and 13.7 respectively, and the second best result\nfor Coastal Swahili to English with BLEU score 14.9.",
          "link": "http://arxiv.org/abs/2106.16055",
          "publishedOn": "2021-07-01T01:59:34.116Z",
          "wordCount": 559,
          "title": "IMS' Systems for the IWSLT 2021 Low-Resource Speech Translation Task. (arXiv:2106.16055v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15872",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Egetenmeyer_J/0/1/0/all/0/1\">Jakob Egetenmeyer</a>",
          "description": "German and French football language display tense-aspect-mood (TAM) forms\nwhich differ from the TAM use in other genres. In German football talk, the\npresent indicative may replace the pluperfect subjunctive. In French reports of\nfootball matches, the imperfective past may occur instead of a perfective past\ntense-aspect form. We argue that the two phenomena share a functional core and\nare licensed in the same way, which is a direct result of the genre they occur\nin. More precisely, football match reports adhere to a precise script and\nspecific events are temporally determined in terms of objective time. This\nallows speakers to exploit a secondary function of TAM forms, namely, they\nshift the temporal perspective. We argue that it is on the grounds of the genre\nthat comprehenders predict the deviating forms and are also able to decode\nthem. We present various corpus studies where we explore the functioning of\nthese phenomena in order to gain insights into their distribution,\ngrammaticalization and their functioning in discourse. Relevant factors are\nAktionsart properties, rhetorical relations and their interaction with other\nTAM forms. This allows us to discuss coping mechanisms on the part of the\ncomprehender. We broaden our understanding of the phenomena, which have only\nbeen partly covered for French and up to now seem to have been ignored in\nGerman.",
          "link": "http://arxiv.org/abs/2106.15872",
          "publishedOn": "2021-07-01T01:59:34.081Z",
          "wordCount": 652,
          "title": "Genre determining prediction: Non-standard TAM marking in football language. (arXiv:2106.15872v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15896",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Akhtar_S/0/1/0/all/0/1\">Sohail Akhtar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basile_V/0/1/0/all/0/1\">Valerio Basile</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patti_V/0/1/0/all/0/1\">Viviana Patti</a>",
          "description": "Social media platforms provide users the freedom of expression and a medium\nto exchange information and express diverse opinions. Unfortunately, this has\nalso resulted in the growth of abusive content with the purpose of\ndiscriminating people and targeting the most vulnerable communities such as\nimmigrants, LGBT, Muslims, Jews and women. Because abusive language is\nsubjective in nature, there might be highly polarizing topics or events\ninvolved in the annotation of abusive contents such as hate speech (HS).\nTherefore, we need novel approaches to model conflicting perspectives and\nopinions coming from people with different personal and demographic\nbackgrounds. In this paper, we present an in-depth study to model polarized\nopinions coming from different communities under the hypothesis that similar\ncharacteristics (ethnicity, social background, culture etc.) can influence the\nperspectives of annotators on a certain phenomenon. We believe that by relying\non this information, we can divide the annotators into groups sharing similar\nperspectives. We can create separate gold standards, one for each group, to\ntrain state-of-the-art deep learning models. We can employ an ensemble approach\nto combine the perspective-aware classifiers from different groups to an\ninclusive model. We also propose a novel resource, a multi-perspective English\nlanguage dataset annotated according to different sub-categories relevant for\ncharacterising online abuse: hate speech, aggressiveness, offensiveness and\nstereotype. By training state-of-the-art deep learning models on this novel\nresource, we show how our approach improves the prediction performance of a\nstate-of-the-art supervised classifier.",
          "link": "http://arxiv.org/abs/2106.15896",
          "publishedOn": "2021-07-01T01:59:33.016Z",
          "wordCount": 686,
          "title": "Whose Opinions Matter? Perspective-aware Models to Identify Opinions of Hate Speech Victims in Abusive Language Detection. (arXiv:2106.15896v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2007.03834",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bradley_T/0/1/0/all/0/1\">Tai-Danae Bradley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vlassopoulos_Y/0/1/0/all/0/1\">Yiannis Vlassopoulos</a>",
          "description": "This work originates from the observation that today's state of the art\nstatistical language models are impressive not only for their performance, but\nalso - and quite crucially - because they are built entirely from correlations\nin unstructured text data. The latter observation prompts a fundamental\nquestion that lies at the heart of this paper: What mathematical structure\nexists in unstructured text data? We put forth enriched category theory as a\nnatural answer. We show that sequences of symbols from a finite alphabet, such\nas those found in a corpus of text, form a category enriched over\nprobabilities. We then address a second fundamental question: How can this\ninformation be stored and modeled in a way that preserves the categorical\nstructure? We answer this by constructing a functor from our enriched category\nof text to a particular enriched category of reduced density operators. The\nlatter leverages the Loewner order on positive semidefinite operators, which\ncan further be interpreted as a toy example of entailment.",
          "link": "http://arxiv.org/abs/2007.03834",
          "publishedOn": "2021-07-01T01:59:32.858Z",
          "wordCount": 650,
          "title": "Language Modeling with Reduced Densities. (arXiv:2007.03834v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.16138",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chi_Z/0/1/0/all/0/1\">Zewen Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shaohan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1\">Li Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Shuming Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singhal_S/0/1/0/all/0/1\">Saksham Singhal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bajaj_P/0/1/0/all/0/1\">Payal Bajaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xia Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>",
          "description": "In this paper, we introduce ELECTRA-style tasks to cross-lingual language\nmodel pre-training. Specifically, we present two pre-training tasks, namely\nmultilingual replaced token detection, and translation replaced token\ndetection. Besides, we pretrain the model, named as XLM-E, on both multilingual\nand parallel corpora. Our model outperforms the baseline models on various\ncross-lingual understanding tasks with much less computation cost. Moreover,\nanalysis shows that XLM-E tends to obtain better cross-lingual transferability.",
          "link": "http://arxiv.org/abs/2106.16138",
          "publishedOn": "2021-07-01T01:59:32.852Z",
          "wordCount": 507,
          "title": "XLM-E: Cross-lingual Language Model Pre-training via ELECTRA. (arXiv:2106.16138v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15825",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Boenninghoff_B/0/1/0/all/0/1\">Benedikt Boenninghoff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nickel_R/0/1/0/all/0/1\">Robert M. Nickel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolossa_D/0/1/0/all/0/1\">Dorothea Kolossa</a>",
          "description": "The PAN 2021 authorship verification (AV) challenge is part of a three-year\nstrategy, moving from a cross-topic/closed-set to a cross-topic/open-set AV\ntask over a collection of fanfiction texts. In this work, we present our\nmodified hybrid neural-probabilistic framework. It is based on our 2020 winning\nsubmission, with updates to significantly reduce sensitivities to topical\nvariations and to further improve the system's calibration by means of an\nuncertainty-adaptation layer. Our framework additionally includes an\nOut-Of-Distribution Detector (O2D2) for defining non-responses, outperforming\nall other systems that participated in the PAN 2021 AV task.",
          "link": "http://arxiv.org/abs/2106.15825",
          "publishedOn": "2021-07-01T01:59:32.847Z",
          "wordCount": 528,
          "title": "O2D2: Out-Of-Distribution Detector to Capture Undecidable Trials in Authorship Verification. (arXiv:2106.15825v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.06456",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rogoz_A/0/1/0/all/0/1\">Ana-Cristina Rogoz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaman_M/0/1/0/all/0/1\">Mihaela Gaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ionescu_R/0/1/0/all/0/1\">Radu Tudor Ionescu</a>",
          "description": "In this work, we introduce a corpus for satire detection in Romanian news. We\ngathered 55,608 public news articles from multiple real and satirical news\nsources, composing one of the largest corpora for satire detection regardless\nof language and the only one for the Romanian language. We provide an official\nsplit of the text samples, such that training news articles belong to different\nsources than test news articles, thus ensuring that models do not achieve high\nperformance simply due to overfitting. We conduct experiments with two\nstate-of-the-art deep neural models, resulting in a set of strong baselines for\nour novel corpus. Our results show that the machine-level accuracy for satire\ndetection in Romanian is quite low (under 73% on the test set) compared to the\nhuman-level accuracy (87%), leaving enough room for improvement in future\nresearch.",
          "link": "http://arxiv.org/abs/2105.06456",
          "publishedOn": "2021-07-01T01:59:32.824Z",
          "wordCount": 621,
          "title": "SaRoCo: Detecting Satire in a Novel Romanian Corpus of News Articles. (arXiv:2105.06456v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.03706",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yeh_Y/0/1/0/all/0/1\">Yi-Ting Yeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eskenazi_M/0/1/0/all/0/1\">Maxine Eskenazi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehri_S/0/1/0/all/0/1\">Shikib Mehri</a>",
          "description": "Automatic evaluation metrics are a crucial component of dialog systems\nresearch. Standard language evaluation metrics are known to be ineffective for\nevaluating dialog. As such, recent research has proposed a number of novel,\ndialog-specific metrics that correlate better with human judgements. Due to the\nfast pace of research, many of these metrics have been assessed on different\ndatasets and there has as yet been no time for a systematic comparison between\nthem. To this end, this paper provides a comprehensive assessment of recently\nproposed dialog evaluation metrics on a number of datasets. In this paper, 17\ndifferent automatic evaluation metrics are evaluated on 10 different datasets.\nFurthermore, the metrics are assessed in different settings, to better qualify\ntheir respective strengths and weaknesses. Metrics are assessed (1) on both the\nturn level and the dialog level, (2) for different dialog lengths, (3) for\ndifferent dialog qualities (e.g., coherence, engaging), (4) for different types\nof response generation models (i.e., generative, retrieval, simple models and\nstate-of-the-art models), (5) taking into account the similarity of different\nmetrics and (6) exploring combinations of different metrics. This comprehensive\nassessment offers several takeaways pertaining to dialog evaluation metrics in\ngeneral. It also suggests how to best assess evaluation metrics and indicates\npromising directions for future work.",
          "link": "http://arxiv.org/abs/2106.03706",
          "publishedOn": "2021-07-01T01:59:32.792Z",
          "wordCount": 658,
          "title": "A Comprehensive Assessment of Dialog Evaluation Metrics. (arXiv:2106.03706v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.16053",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Voskarides_N/0/1/0/all/0/1\">Nikos Voskarides</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meij_E/0/1/0/all/0/1\">Edgar Meij</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sauer_S/0/1/0/all/0/1\">Sabrina Sauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rijke_M/0/1/0/all/0/1\">Maarten de Rijke</a>",
          "description": "Writers such as journalists often use automatic tools to find relevant\ncontent to include in their narratives. In this paper, we focus on supporting\nwriters in the news domain to develop event-centric narratives. Given an\nincomplete narrative that specifies a main event and a context, we aim to\nretrieve news articles that discuss relevant events that would enable the\ncontinuation of the narrative. We formally define this task and propose a\nretrieval dataset construction procedure that relies on existing news articles\nto simulate incomplete narratives and relevant articles. Experiments on two\ndatasets derived from this procedure show that state-of-the-art lexical and\nsemantic rankers are not sufficient for this task. We show that combining those\nwith a ranker that ranks articles by reverse chronological order outperforms\nthose rankers alone. We also perform an in-depth quantitative and qualitative\nanalysis of the results that sheds light on the characteristics of this task.",
          "link": "http://arxiv.org/abs/2106.16053",
          "publishedOn": "2021-07-01T01:59:32.765Z",
          "wordCount": 592,
          "title": "News Article Retrieval in Context for Event-centric Narrative Creation. (arXiv:2106.16053v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15760",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Thanh-Tung Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_X/0/1/0/all/0/1\">Xuan-Phi Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1\">Shafiq Joty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoli Li</a>",
          "description": "We introduce a generic seq2seq parsing framework that casts constituency\nparsing problems (syntactic and discourse parsing) into a series of conditional\nsplitting decisions. Our parsing model estimates the conditional probability\ndistribution of possible splitting points in a given text span and supports\nefficient top-down decoding, which is linear in number of nodes. The\nconditional splitting formulation together with efficient beam search inference\nfacilitate structural consistency without relying on expensive structured\ninference. Crucially, for discourse analysis we show that in our formulation,\ndiscourse segmentation can be framed as a special case of parsing which allows\nus to perform discourse parsing without requiring segmentation as a\npre-requisite. Experiments show that our model achieves good results on the\nstandard syntactic parsing tasks under settings with/without pre-trained\nrepresentations and rivals state-of-the-art (SoTA) methods that are more\ncomputationally expensive than ours. In discourse parsing, our method\noutperforms SoTA by a good margin.",
          "link": "http://arxiv.org/abs/2106.15760",
          "publishedOn": "2021-07-01T01:59:32.756Z",
          "wordCount": 588,
          "title": "A Conditional Splitting Framework for Efficient Constituency Parsing. (arXiv:2106.15760v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2006.09199",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rouditchenko_A/0/1/0/all/0/1\">Andrew Rouditchenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boggust_A/0/1/0/all/0/1\">Angie Boggust</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harwath_D/0/1/0/all/0/1\">David Harwath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Brian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_D/0/1/0/all/0/1\">Dhiraj Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomas_S/0/1/0/all/0/1\">Samuel Thomas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Audhkhasi_K/0/1/0/all/0/1\">Kartik Audhkhasi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuehne_H/0/1/0/all/0/1\">Hilde Kuehne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panda_R/0/1/0/all/0/1\">Rameswar Panda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feris_R/0/1/0/all/0/1\">Rogerio Feris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kingsbury_B/0/1/0/all/0/1\">Brian Kingsbury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Picheny_M/0/1/0/all/0/1\">Michael Picheny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torralba_A/0/1/0/all/0/1\">Antonio Torralba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glass_J/0/1/0/all/0/1\">James Glass</a>",
          "description": "Current methods for learning visually grounded language from videos often\nrely on text annotation, such as human generated captions or machine generated\nautomatic speech recognition (ASR) transcripts. In this work, we introduce the\nAudio-Video Language Network (AVLnet), a self-supervised network that learns a\nshared audio-visual embedding space directly from raw video inputs. To\ncircumvent the need for text annotation, we learn audio-visual representations\nfrom randomly segmented video clips and their raw audio waveforms. We train\nAVLnet on HowTo100M, a large corpus of publicly available instructional videos,\nand evaluate on image retrieval and video retrieval tasks, achieving\nstate-of-the-art performance. We perform analysis of AVLnet's learned\nrepresentations, showing our model utilizes speech and natural sounds to learn\naudio-visual concepts. Further, we propose a tri-modal model that jointly\nprocesses raw audio, video, and text captions from videos to learn a\nmulti-modal semantic embedding space useful for text-video retrieval. Our code,\ndata, and trained models will be released at avlnet.csail.mit.edu",
          "link": "http://arxiv.org/abs/2006.09199",
          "publishedOn": "2021-07-01T01:59:32.577Z",
          "wordCount": 675,
          "title": "AVLnet: Learning Audio-Visual Language Representations from Instructional Videos. (arXiv:2006.09199v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15674",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mahmoudi_S/0/1/0/all/0/1\">Seyyed Ehsan Mahmoudi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shamsfard_M/0/1/0/all/0/1\">Mehrnoush Shamsfard</a>",
          "description": "In recent years there has been a special interest in word embeddings as a new\napproach to convert words to vectors. It has been a focal point to understand\nhow much of the semantics of the the words has been transferred into embedding\nvectors. This is important as the embedding is going to be used as the basis\nfor downstream NLP applications and it will be costly to evaluate the\napplication end-to-end in order to identify quality of the used embedding\nmodel. Generally the word embeddings are evaluated through a number of tests,\nincluding analogy test. In this paper we propose a test framework for Persian\nembedding models. Persian is a low resource language and there is no rich\nsemantic benchmark to evaluate word embedding models for this language. In this\npaper we introduce an evaluation framework including a hand crafted Persian SAT\nbased analogy dataset, a colliquial test set (specific to Persian) and a\nbenchmark to study the impact of various parameters on the semantic evaluation\ntask.",
          "link": "http://arxiv.org/abs/2106.15674",
          "publishedOn": "2021-07-01T01:59:32.518Z",
          "wordCount": 607,
          "title": "SAT Based Analogy Evaluation Framework for Persian Word Embeddings. (arXiv:2106.15674v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15903",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhongkun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_P/0/1/0/all/0/1\">Pengjie Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhumin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Z/0/1/0/all/0/1\">Zhaochun Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rijke_M/0/1/0/all/0/1\">Maarten de Rijke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Ming Zhou</a>",
          "description": "Conversational Question Simplification (CQS) aims to simplify self-contained\nquestions into conversational ones by incorporating some conversational\ncharacteristics, e.g., anaphora and ellipsis. Existing maximum likelihood\nestimation (MLE) based methods often get trapped in easily learned tokens as\nall tokens are treated equally during training. In this work, we introduce a\nReinforcement Iterative Sequence Editing (RISE) framework that optimizes the\nminimum Levenshtein distance (MLD) through explicit editing actions. RISE is\nable to pay attention to tokens that are related to conversational\ncharacteristics. To train RISE, we devise an Iterative Reinforce Training (IRT)\nalgorithm with a Dynamic Programming based Sampling (DPS) process to improve\nexploration. Experimental results on two benchmark datasets show that RISE\nsignificantly outperforms state-of-the-art methods and generalizes well on\nunseen data.",
          "link": "http://arxiv.org/abs/2106.15903",
          "publishedOn": "2021-07-01T01:59:32.475Z",
          "wordCount": 573,
          "title": "Learning to Ask Conversational Questions by Optimizing Levenshtein Distance. (arXiv:2106.15903v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16175",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bucur_A/0/1/0/all/0/1\">Ana-Maria Bucur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cosma_A/0/1/0/all/0/1\">Adrian Cosma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dinu_L/0/1/0/all/0/1\">Liviu P. Dinu</a>",
          "description": "Early risk detection of mental illnesses has a massive positive impact upon\nthe well-being of people. The eRisk workshop has been at the forefront of\nenabling interdisciplinary research in developing computational methods to\nautomatically estimate early risk factors for mental issues such as depression,\nself-harm, anorexia and pathological gambling. In this paper, we present the\ncontributions of the BLUE team in the 2021 edition of the workshop, in which we\ntackle the problems of early detection of gambling addiction, self-harm and\nestimating depression severity from social media posts. We employ pre-trained\nBERT transformers and data crawled automatically from mental health subreddits\nand obtain reasonable results on all three tasks.",
          "link": "http://arxiv.org/abs/2106.16175",
          "publishedOn": "2021-07-01T01:59:32.469Z",
          "wordCount": 564,
          "title": "Early Risk Detection of Pathological Gambling, Self-Harm and Depression Using BERT. (arXiv:2106.16175v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15772",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Miao_S/0/1/0/all/0/1\">Shen-Yun Miao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_C/0/1/0/all/0/1\">Chao-Chun Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_K/0/1/0/all/0/1\">Keh-Yih Su</a>",
          "description": "We present ASDiv (Academia Sinica Diverse MWP Dataset), a diverse (in terms\nof both language patterns and problem types) English math word problem (MWP)\ncorpus for evaluating the capability of various MWP solvers. Existing MWP\ncorpora for studying AI progress remain limited either in language usage\npatterns or in problem types. We thus present a new English MWP corpus with\n2,305 MWPs that cover more text patterns and most problem types taught in\nelementary school. Each MWP is annotated with its problem type and grade level\n(for indicating the level of difficulty). Furthermore, we propose a metric to\nmeasure the lexicon usage diversity of a given MWP corpus, and demonstrate that\nASDiv is more diverse than existing corpora. Experiments show that our proposed\ncorpus reflects the true capability of MWP solvers more faithfully.",
          "link": "http://arxiv.org/abs/2106.15772",
          "publishedOn": "2021-07-01T01:59:32.440Z",
          "wordCount": 574,
          "title": "A Diverse Corpus for Evaluating and Developing English Math Word Problem Solvers. (arXiv:2106.15772v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15880",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haoran Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Wei Lu</a>",
          "description": "In neural machine translation, cross entropy (CE) is the standard loss\nfunction in two training methods of auto-regressive models, i.e., teacher\nforcing and scheduled sampling. In this paper, we propose mixed cross entropy\nloss (mixed CE) as a substitute for CE in both training approaches. In teacher\nforcing, the model trained with CE regards the translation problem as a\none-to-one mapping process, while in mixed CE this process can be relaxed to\none-to-many. In scheduled sampling, we show that mixed CE has the potential to\nencourage the training and testing behaviours to be similar to each other, more\neffectively mitigating the exposure bias problem. We demonstrate the\nsuperiority of mixed CE over CE on several machine translation datasets, WMT'16\nRo-En, WMT'16 Ru-En, and WMT'14 En-De in both teacher forcing and scheduled\nsampling setups. Furthermore, in WMT'14 En-De, we also find mixed CE\nconsistently outperforms CE on a multi-reference set as well as a challenging\nparaphrased reference set. We also found the model trained with mixed CE is\nable to provide a better probability distribution defined over the translation\noutput space. Our code is available at https://github.com/haorannlp/mix.",
          "link": "http://arxiv.org/abs/2106.15880",
          "publishedOn": "2021-07-01T01:59:32.425Z",
          "wordCount": 619,
          "title": "Mixed Cross Entropy Loss for Neural Machine Translation. (arXiv:2106.15880v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16196",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kennington_C/0/1/0/all/0/1\">Casey Kennington</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steenson_M/0/1/0/all/0/1\">McKenzie Steenson</a>",
          "description": "Automated speech and text interfaces are continuing to improve, resulting in\nincreased research in the area of dialogue systems. Moreover, conferences and\nworkshops from various fields are focusing more on language through speech and\ntext mediums as candidates for interaction with applications such as search\ninterfaces and robots. In this paper, we explore how visible the SigDial\nconference is to outside conferences by analysing papers from top Natural\nLangauge Processing conferences since 2015 to determine the popularity of\ncertain SigDial-related topics, as well as analysing what SigDial papers are\nbeing cited by others outside of SigDial. We find that despite a dramatic\nincrease in dialogue-related research, SigDial visibility has not increased. We\nconclude by offering some suggestions.",
          "link": "http://arxiv.org/abs/2106.16196",
          "publishedOn": "2021-07-01T01:59:32.007Z",
          "wordCount": 548,
          "title": "An Analysis of the Recent Visibility of the SigDial Conference. (arXiv:2106.16196v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16213",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Merrill_W/0/1/0/all/0/1\">William Merrill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1\">Yoav Goldberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwartz_R/0/1/0/all/0/1\">Roy Schwartz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>",
          "description": "Transformers have become a standard architecture for many NLP problems. This\nhas motivated theoretically analyzing their capabilities as models of language,\nin order to understand what makes them successful, and what their potential\nweaknesses might be. Recent work has shown that transformers with hard\nattention are quite limited in capacity, and in fact can be simulated by\nconstant-depth circuits. However, hard attention is a restrictive assumption,\nwhich may complicate the relevance of these results for practical transformers.\nIn this work, we analyze the circuit complexity of transformers with saturated\nattention: a generalization of hard attention that more closely captures the\nattention patterns learnable in practical transformers. We show that saturated\ntransformers transcend the limitations of hard-attention transformers. With\nsome minor assumptions, we prove that the number of bits needed to represent a\nsaturated transformer memory vector is $O(\\log n)$, which implies saturated\ntransformers can be simulated by log-depth circuits. Thus, the jump from hard\nto saturated attention can be understood as increasing the transformer's\neffective circuit depth by a factor of $O(\\log n)$.",
          "link": "http://arxiv.org/abs/2106.16213",
          "publishedOn": "2021-07-01T01:59:31.930Z",
          "wordCount": 622,
          "title": "On the Power of Saturated Transformers: A View from Circuit Complexity. (arXiv:2106.16213v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15971",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bilal_I/0/1/0/all/0/1\">Iman Munire Bilal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liakata_M/0/1/0/all/0/1\">Maria Liakata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Procter_R/0/1/0/all/0/1\">Rob Procter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsakalidis_A/0/1/0/all/0/1\">Adam Tsakalidis</a>",
          "description": "Collecting together microblogs representing opinions about the same topics\nwithin the same timeframe is useful to a number of different tasks and\npractitioners. A major question is how to evaluate the quality of such thematic\nclusters. Here we create a corpus of microblog clusters from three different\ndomains and time windows and define the task of evaluating thematic coherence.\nWe provide annotation guidelines and human annotations of thematic coherence by\njournalist experts. We subsequently investigate the efficacy of different\nautomated evaluation metrics for the task. We consider a range of metrics\nincluding surface level metrics, ones for topic model coherence and text\ngeneration metrics (TGMs). While surface level metrics perform well,\noutperforming topic coherence metrics, they are not as consistent as TGMs. TGMs\nare more reliable than all other metrics considered for capturing thematic\ncoherence in microblog clusters due to being less sensitive to the effect of\ntime windows.",
          "link": "http://arxiv.org/abs/2106.15971",
          "publishedOn": "2021-07-01T01:59:31.923Z",
          "wordCount": 592,
          "title": "Evaluation of Thematic Coherence in Microblogs. (arXiv:2106.15971v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16188",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yifei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_V/0/1/0/all/0/1\">Vincent Gao</a>",
          "description": "E-commerce stores collect customer feedback to let sellers learn about\ncustomer concerns and enhance customer order experience. Because customer\nfeedback often contains redundant information, a concise summary of the\nfeedback can be generated to help sellers better understand the issues causing\ncustomer dissatisfaction. Previous state-of-the-art abstractive text\nsummarization models make two major types of factual errors when producing\nsummaries from customer feedback, which are wrong entity detection (WED) and\nincorrect product-defect description (IPD). In this work, we introduce a set of\nmethods to enhance the factual consistency of abstractive summarization on\ncustomer feedback. We augment the training data with artificially corrupted\nsummaries, and use them as counterparts of the target summaries. We add a\ncontrastive loss term into the training objective so that the model learns to\navoid certain factual errors. Evaluation results show that a large portion of\nWED and IPD errors are alleviated for BART and T5. Furthermore, our approaches\ndo not depend on the structure of the summarization model and thus are\ngeneralizable to any abstractive summarization systems.",
          "link": "http://arxiv.org/abs/2106.16188",
          "publishedOn": "2021-07-01T01:59:31.768Z",
          "wordCount": 606,
          "title": "Improving Factual Consistency of Abstractive Summarization on Customer Feedback. (arXiv:2106.16188v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2103.01910",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Josiah Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madhyastha_P/0/1/0/all/0/1\">Pranava Madhyastha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Figueiredo_J/0/1/0/all/0/1\">Josiel Figueiredo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lala_C/0/1/0/all/0/1\">Chiraag Lala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Specia_L/0/1/0/all/0/1\">Lucia Specia</a>",
          "description": "This paper introduces a large-scale multimodal and multilingual dataset that\naims to facilitate research on grounding words to images in their contextual\nusage in language. The dataset consists of images selected to unambiguously\nillustrate concepts expressed in sentences from movie subtitles. The dataset is\na valuable resource as (i) the images are aligned to text fragments rather than\nwhole sentences; (ii) multiple images are possible for a text fragment and a\nsentence; (iii) the sentences are free-form and real-world like; (iv) the\nparallel texts are multilingual. We set up a fill-in-the-blank game for humans\nto evaluate the quality of the automatic image selection process of our\ndataset. We show the utility of the dataset on two automatic tasks: (i)\nfill-in-the blank; (ii) lexical translation. Results of the human evaluation\nand automatic models demonstrate that images can be a useful complement to the\ntextual context. The dataset will benefit research on visual grounding of words\nespecially in the context of free-form sentences, and can be obtained from\nhttps://doi.org/10.5281/zenodo.5034604 under a Creative Commons licence.",
          "link": "http://arxiv.org/abs/2103.01910",
          "publishedOn": "2021-07-01T01:59:31.758Z",
          "wordCount": 695,
          "title": "MultiSubs: A Large-scale Multimodal and Multilingual Dataset. (arXiv:2103.01910v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15846",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhiyuan_W/0/1/0/all/0/1\">Wen Zhiyuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiannong_C/0/1/0/all/0/1\">Cao Jiannong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruosong_Y/0/1/0/all/0/1\">Yang Ruosong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shuaiqi_L/0/1/0/all/0/1\">Liu Shuaiqi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiaxing_S/0/1/0/all/0/1\">Shen Jiaxing</a>",
          "description": "To provide consistent emotional interaction with users, dialog systems should\nbe capable to automatically select appropriate emotions for responses like\nhumans. However, most existing works focus on rendering specified emotions in\nresponses or empathetically respond to the emotion of users, yet the individual\ndifference in emotion expression is overlooked. This may lead to inconsistent\nemotional expressions and disinterest users. To tackle this issue, we propose\nto equip the dialog system with personality and enable it to automatically\nselect emotions in responses by simulating the emotion transition of humans in\nconversation. In detail, the emotion of the dialog system is transitioned from\nits preceding emotion in context. The transition is triggered by the preceding\ndialog context and affected by the specified personality trait. To achieve\nthis, we first model the emotion transition in the dialog system as the\nvariation between the preceding emotion and the response emotion in the\nValence-Arousal-Dominance (VAD) emotion space. Then, we design neural networks\nto encode the preceding dialog context and the specified personality traits to\ncompose the variation. Finally, the emotion for response is selected from the\nsum of the preceding emotion and the variation. We construct a dialog dataset\nwith emotion and personality labels and conduct emotion prediction tasks for\nevaluation. Experimental results validate the effectiveness of the\npersonality-affected emotion transition.",
          "link": "http://arxiv.org/abs/2106.15846",
          "publishedOn": "2021-07-01T01:59:31.750Z",
          "wordCount": 663,
          "title": "Automatically Select Emotion for Response via Personality-affected Emotion Transition. (arXiv:2106.15846v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.07319",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1\">Chengqi Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhicheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_J/0/1/0/all/0/1\">Jian Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mingxuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_R/0/1/0/all/0/1\">Rong Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Q/0/1/0/all/0/1\">Qianqian Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1\">Jun Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>",
          "description": "This paper describes the systems submitted to IWSLT 2021 by the Volctrans\nteam. We participate in the offline speech translation and text-to-text\nsimultaneous translation tracks. For offline speech translation, our best\nend-to-end model achieves 8.1 BLEU improvements over the benchmark on the\nMuST-C test set and is even approaching the results of a strong cascade\nsolution. For text-to-text simultaneous translation, we explore the best\npractice to optimize the wait-k model. As a result, our final submitted systems\nexceed the benchmark at around 7 BLEU on the same latency regime. We will\npublish our code and model to facilitate both future research works and\nindustrial applications.\n\nThis paper describes the systems submitted to IWSLT 2021 by the Volctrans\nteam. We participate in the offline speech translation and text-to-text\nsimultaneous translation tracks. For offline speech translation, our best\nend-to-end model achieves 7.9 BLEU improvements over the benchmark on the\nMuST-C test set and is even approaching the results of a strong cascade\nsolution. For text-to-text simultaneous translation, we explore the best\npractice to optimize the wait-k model. As a result, our final submitted systems\nexceed the benchmark at around 7 BLEU on the same latency regime. We release\nour code and model at\n\\url{https://github.com/bytedance/neurst/tree/master/examples/iwslt21} to\nfacilitate both future research works and industrial applications.",
          "link": "http://arxiv.org/abs/2105.07319",
          "publishedOn": "2021-07-01T01:59:31.741Z",
          "wordCount": 701,
          "title": "The Volctrans Neural Speech Translation System for IWSLT 2021. (arXiv:2105.07319v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15818",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Marchisio_K/0/1/0/all/0/1\">Kelly Marchisio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freitag_M/0/1/0/all/0/1\">Markus Freitag</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grangier_D/0/1/0/all/0/1\">David Grangier</a>",
          "description": "Whereas existing literature on unsupervised machine translation (MT) focuses\non exploiting unsupervised techniques for low-resource language pairs where\nbilingual training data is scare or unavailable, we investigate whether\nunsupervised MT can also improve translation quality of high-resource language\npairs where sufficient bitext does exist. We compare the style of correct\ntranslations generated by either supervised or unsupervised MT and find that\nthe unsupervised output is less monotonic and more natural than supervised\noutput. We demonstrate a way to combine the benefits of unsupervised and\nsupervised MT into a single system, resulting in better human evaluation of\nquality and fluency. Our results open the door to discussions about the\npotential contributions of unsupervised MT in high-resource settings, and how\nsupervised and unsupervised systems might be mutually-beneficial.",
          "link": "http://arxiv.org/abs/2106.15818",
          "publishedOn": "2021-07-01T01:59:31.718Z",
          "wordCount": 559,
          "title": "What Can Unsupervised Machine Translation Contribute to High-Resource Language Pairs?. (arXiv:2106.15818v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15876",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_P/0/1/0/all/0/1\">Paheli Bhattacharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poddar_S/0/1/0/all/0/1\">Soham Poddar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rudra_K/0/1/0/all/0/1\">Koustav Rudra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_K/0/1/0/all/0/1\">Kripabandhu Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Saptarshi Ghosh</a>",
          "description": "Automatic summarization of legal case documents is an important and practical\nchallenge. Apart from many domain-independent text summarization algorithms\nthat can be used for this purpose, several algorithms have been developed\nspecifically for summarizing legal case documents. However, most of the\nexisting algorithms do not systematically incorporate domain knowledge that\nspecifies what information should ideally be present in a legal case document\nsummary. To address this gap, we propose an unsupervised summarization\nalgorithm DELSumm which is designed to systematically incorporate guidelines\nfrom legal experts into an optimization setup. We conduct detailed experiments\nover case documents from the Indian Supreme Court. The experiments show that\nour proposed unsupervised method outperforms several strong baselines in terms\nof ROUGE scores, including both general summarization algorithms and\nlegal-specific ones. In fact, though our proposed algorithm is unsupervised, it\noutperforms several supervised summarization models that are trained over\nthousands of document-summary pairs.",
          "link": "http://arxiv.org/abs/2106.15876",
          "publishedOn": "2021-07-01T01:59:31.712Z",
          "wordCount": 602,
          "title": "Incorporating Domain Knowledge for Extractive Summarization of Legal Case Documents. (arXiv:2106.15876v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16013",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Baradaran_R/0/1/0/all/0/1\">Razieh Baradaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amirkhani_H/0/1/0/all/0/1\">Hossein Amirkhani</a>",
          "description": "One of the main challenges of the machine reading comprehension (MRC) models\nis their fragile out-of-domain generalization, which makes these models not\nproperly applicable to real-world general-purpose question answering problems.\nIn this paper, we leverage a zero-shot weighted ensemble method for improving\nthe robustness of out-of-domain generalization in MRC models. In the proposed\nmethod, a weight estimation module is used to estimate out-of-domain weights,\nand an ensemble module aggregate several base models' predictions based on\ntheir weights. The experiments indicate that the proposed method not only\nimproves the final accuracy, but also is robust against domain changes.",
          "link": "http://arxiv.org/abs/2106.16013",
          "publishedOn": "2021-07-01T01:59:31.705Z",
          "wordCount": 541,
          "title": "Zero-Shot Estimation of Base Models' Weights in Ensemble of Machine Reading Comprehension Systems for Robust Generalization. (arXiv:2106.16013v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15986",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ulcar_M/0/1/0/all/0/1\">Matej Ul&#x10d;ar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Robnik_Sikonja_M/0/1/0/all/0/1\">Marko Robnik-&#x160;ikonja</a>",
          "description": "Building machine learning prediction models for a specific NLP task requires\nsufficient training data, which can be difficult to obtain for low-resource\nlanguages. Cross-lingual embeddings map word embeddings from a low-resource\nlanguage to a high-resource language so that a prediction model trained on data\nfrom the high-resource language can also be used in the low-resource language.\nTo produce cross-lingual mappings of recent contextual embeddings, anchor\npoints between the embedding spaces have to be words in the same context. We\naddress this issue with a new method for creating datasets for cross-lingual\ncontextual alignments. Based on that, we propose novel cross-lingual mapping\nmethods for ELMo embeddings. Our linear mapping methods use existing vecmap and\nMUSE alignments on contextual ELMo embeddings. Our new nonlinear ELMoGAN\nmapping method is based on GANs and does not assume isomorphic embedding\nspaces. We evaluate the proposed mapping methods on nine languages, using two\ndownstream tasks, NER and dependency parsing. The ELMoGAN method performs well\non the NER task, with low cross-lingual loss compared to direct training on\nsome languages. In the dependency parsing, linear alignment variants are more\nsuccessful.",
          "link": "http://arxiv.org/abs/2106.15986",
          "publishedOn": "2021-07-01T01:59:31.698Z",
          "wordCount": 611,
          "title": "Cross-lingual alignments of ELMo contextual embeddings. (arXiv:2106.15986v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15838",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ren_L/0/1/0/all/0/1\">Liliang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Chenkai Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hockenmaier_J/0/1/0/all/0/1\">Julia Hockenmaier</a>",
          "description": "Text-to-Graph extraction aims to automatically extract information graphs\nconsisting of mentions and types from natural language texts. Existing\napproaches, such as table filling and pairwise scoring, have shown impressive\nperformance on various information extraction tasks, but they are difficult to\nscale to datasets with longer input texts because of their second-order\nspace/time complexities with respect to the input length. In this work, we\npropose a Hybrid Span Generator (HySPA) that invertibly maps the information\ngraph to an alternating sequence of nodes and edge types, and directly\ngenerates such sequences via a hybrid span decoder which can decode both the\nspans and the types recurrently in linear time and space complexities.\nExtensive experiments on the ACE05 dataset show that our approach also\nsignificantly outperforms state-of-the-art on the joint entity and relation\nextraction task.",
          "link": "http://arxiv.org/abs/2106.15838",
          "publishedOn": "2021-07-01T01:59:31.691Z",
          "wordCount": 569,
          "title": "HySPA: Hybrid Span Generation for Scalable Text-to-Graph Extraction. (arXiv:2106.15838v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2101.00421",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bogoychev_N/0/1/0/all/0/1\">Nikolay Bogoychev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Pinzhen Chen</a>",
          "description": "Machine translation systems are vulnerable to domain mismatch, especially\nwhen the task is low-resource. In this setting, out of domain translations are\noften of poor quality and prone to hallucinations, due to the translation model\npreferring to predict common words it has seen during training, as opposed to\nthe more uncommon ones from a different domain. We present two simple methods\nfor improving translation quality in this particular setting: First, we use\nlexical shortlisting in order to restrict the neural network predictions by IBM\nmodel computed alignments. Second, we perform $n$-best list reordering by\nreranking all translations based on the amount they overlap with each other.\nOur methods are computationally simpler and faster than alternative approaches,\nand show a moderate success on low-resource settings with explicit out of\ndomain test sets. However, our methods lose their effectiveness when the domain\nmismatch is too great, or in high resource setting.",
          "link": "http://arxiv.org/abs/2101.00421",
          "publishedOn": "2021-07-01T01:59:31.657Z",
          "wordCount": 606,
          "title": "Decoding Time Lexical Domain Adaptation for Neural Machine Translation. (arXiv:2101.00421v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.16163",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sellam_T/0/1/0/all/0/1\">Thibault Sellam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yadlowsky_S/0/1/0/all/0/1\">Steve Yadlowsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jason Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saphra_N/0/1/0/all/0/1\">Naomi Saphra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DAmour_A/0/1/0/all/0/1\">Alexander D&#x27;Amour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Linzen_T/0/1/0/all/0/1\">Tal Linzen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bastings_J/0/1/0/all/0/1\">Jasmijn Bastings</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turc_I/0/1/0/all/0/1\">Iulia Turc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eisenstein_J/0/1/0/all/0/1\">Jacob Eisenstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Das_D/0/1/0/all/0/1\">Dipanjan Das</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tenney_I/0/1/0/all/0/1\">Ian Tenney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavlick_E/0/1/0/all/0/1\">Ellie Pavlick</a>",
          "description": "Experiments with pretrained models such as BERT are often based on a single\ncheckpoint. While the conclusions drawn apply to the artifact (i.e., the\nparticular instance of the model), it is not always clear whether they hold for\nthe more general procedure (which includes the model architecture, training\ndata, initialization scheme, and loss function). Recent work has shown that\nre-running pretraining can lead to substantially different conclusions about\nperformance, suggesting that alternative evaluations are needed to make\nprincipled statements about procedures. To address this question, we introduce\nMultiBERTs: a set of 25 BERT-base checkpoints, trained with similar\nhyper-parameters as the original BERT model but differing in random\ninitialization and data shuffling. The aim is to enable researchers to draw\nrobust and statistically justified conclusions about pretraining procedures.\nThe full release includes 25 fully trained checkpoints, as well as statistical\nguidelines and a code library implementing our recommended hypothesis testing\nmethods. Finally, for five of these models we release a set of 28 intermediate\ncheckpoints in order to support research on learning dynamics.",
          "link": "http://arxiv.org/abs/2106.16163",
          "publishedOn": "2021-07-01T01:59:31.644Z",
          "wordCount": 624,
          "title": "The MultiBERTs: BERT Reproductions for Robustness Analysis. (arXiv:2106.16163v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15919",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Raju_A/0/1/0/all/0/1\">Anirudh Raju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tiwari_G/0/1/0/all/0/1\">Gautam Tiwari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_M/0/1/0/all/0/1\">Milind Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dheram_P/0/1/0/all/0/1\">Pranav Dheram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anderson_B/0/1/0/all/0/1\">Bryan Anderson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhe Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_B/0/1/0/all/0/1\">Bach Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rastrow_A/0/1/0/all/0/1\">Ariya Rastrow</a>",
          "description": "We propose an end-to-end trained spoken language understanding (SLU) system\nthat extracts transcripts, intents and slots from an input speech utterance. It\nconsists of a streaming recurrent neural network transducer (RNNT) based\nautomatic speech recognition (ASR) model connected to a neural natural language\nunderstanding (NLU) model through a neural interface. This interface allows for\nend-to-end training using multi-task RNNT and NLU losses. Additionally, we\nintroduce semantic sequence loss training for the joint RNNT-NLU system that\nallows direct optimization of non-differentiable SLU metrics. This end-to-end\nSLU model paradigm can leverage state-of-the-art advancements and pretrained\nmodels in both ASR and NLU research communities, outperforming recently\nproposed direct speech-to-semantics models, and conventional pipelined ASR and\nNLU systems. We show that this method improves both ASR and NLU metrics on both\npublic SLU datasets and large proprietary datasets.",
          "link": "http://arxiv.org/abs/2106.15919",
          "publishedOn": "2021-07-01T01:59:31.560Z",
          "wordCount": 582,
          "title": "End-to-End Spoken Language Understanding using RNN-Transducer ASR. (arXiv:2106.15919v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15684",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rohanian_M/0/1/0/all/0/1\">Morteza Rohanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hough_J/0/1/0/all/0/1\">Julian Hough</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Purver_M/0/1/0/all/0/1\">Matthew Purver</a>",
          "description": "We present two multimodal fusion-based deep learning models that consume ASR\ntranscribed speech and acoustic data simultaneously to classify whether a\nspeaker in a structured diagnostic task has Alzheimer's Disease and to what\ndegree, evaluating the ADReSSo challenge 2021 data. Our best model, a BiLSTM\nwith highway layers using words, word probabilities, disfluency features, pause\ninformation, and a variety of acoustic features, achieves an accuracy of 84%\nand RSME error prediction of 4.26 on MMSE cognitive scores. While predicting\ncognitive decline is more challenging, our models show improvement using the\nmultimodal approach and word probabilities, disfluency and pause information\nover word-only models. We show considerable gains for AD classification using\nmultimodal fusion and gating, which can effectively deal with noisy inputs from\nacoustic features and ASR hypotheses.",
          "link": "http://arxiv.org/abs/2106.15684",
          "publishedOn": "2021-07-01T01:59:31.528Z",
          "wordCount": 592,
          "title": "Alzheimer's Dementia Recognition Using Acoustic, Lexical, Disfluency and Speech Pause Features Robust to Noisy Inputs. (arXiv:2106.15684v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16171",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Turc_I/0/1/0/all/0/1\">Iulia Turc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kenton Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eisenstein_J/0/1/0/all/0/1\">Jacob Eisenstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_M/0/1/0/all/0/1\">Ming-Wei Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toutanova_K/0/1/0/all/0/1\">Kristina Toutanova</a>",
          "description": "Despite their success, large pre-trained multilingual models have not\ncompletely alleviated the need for labeled data, which is cumbersome to collect\nfor all target languages. Zero-shot cross-lingual transfer is emerging as a\npractical solution: pre-trained models later fine-tuned on one transfer\nlanguage exhibit surprising performance when tested on many target languages.\nEnglish is the dominant source language for transfer, as reinforced by popular\nzero-shot benchmarks. However, this default choice has not been systematically\nvetted. In our study, we compare English against other transfer languages for\nfine-tuning, on two pre-trained multilingual models (mBERT and mT5) and\nmultiple classification and question answering tasks. We find that other\nhigh-resource languages such as German and Russian often transfer more\neffectively, especially when the set of target languages is diverse or unknown\na priori. Unexpectedly, this can be true even when the training sets were\nautomatically translated from English. This finding can have immediate impact\non multilingual zero-shot systems, and should inform future benchmark designs.",
          "link": "http://arxiv.org/abs/2106.16171",
          "publishedOn": "2021-07-01T01:59:31.491Z",
          "wordCount": 596,
          "title": "Revisiting the Primacy of English in Zero-shot Cross-lingual Transfer. (arXiv:2106.16171v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2104.00952",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1\">Wei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1\">Shaoxiong Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cambria_E/0/1/0/all/0/1\">Erik Cambria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marttinen_P/0/1/0/all/0/1\">Pekka Marttinen</a>",
          "description": "Medical coding translates professionally written medical reports into\nstandardized codes, which is an essential part of medical information systems\nand health insurance reimbursement. Manual coding by trained human coders is\ntime-consuming and error-prone. Thus, automated coding algorithms have been\ndeveloped, building especially on the recent advances in machine learning and\ndeep neural networks. To solve the challenges of encoding lengthy and noisy\nclinical documents and capturing code associations, we propose a multitask\nrecalibrated aggregation network. In particular, multitask learning shares\ninformation across different coding schemes and captures the dependencies\nbetween different medical codes. Feature recalibration and aggregation in\nshared modules enhance representation learning for lengthy notes. Experiments\nwith a real-world MIMIC-III dataset show significantly improved predictive\nperformance.",
          "link": "http://arxiv.org/abs/2104.00952",
          "publishedOn": "2021-06-30T02:01:00.504Z",
          "wordCount": 589,
          "title": "Multitask Recalibrated Aggregation Network for Medical Code Prediction. (arXiv:2104.00952v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1909.07755",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Eberts_M/0/1/0/all/0/1\">Markus Eberts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ulges_A/0/1/0/all/0/1\">Adrian Ulges</a>",
          "description": "We introduce SpERT, an attention model for span-based joint entity and\nrelation extraction. Our key contribution is a light-weight reasoning on BERT\nembeddings, which features entity recognition and filtering, as well as\nrelation classification with a localized, marker-free context representation.\nThe model is trained using strong within-sentence negative samples, which are\nefficiently extracted in a single BERT pass. These aspects facilitate a search\nover all spans in the sentence.\n\nIn ablation studies, we demonstrate the benefits of pre-training, strong\nnegative sampling and localized context. Our model outperforms prior work by up\nto 2.6% F1 score on several datasets for joint entity and relation extraction.",
          "link": "http://arxiv.org/abs/1909.07755",
          "publishedOn": "2021-06-30T02:01:00.463Z",
          "wordCount": 618,
          "title": "Span-based Joint Entity and Relation Extraction with Transformer Pre-training. (arXiv:1909.07755v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.04252",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Conklin_H/0/1/0/all/0/1\">Henry Conklin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bailin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_K/0/1/0/all/0/1\">Kenny Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Titov_I/0/1/0/all/0/1\">Ivan Titov</a>",
          "description": "Natural language is compositional; the meaning of a sentence is a function of\nthe meaning of its parts. This property allows humans to create and interpret\nnovel sentences, generalizing robustly outside their prior experience. Neural\nnetworks have been shown to struggle with this kind of generalization, in\nparticular performing poorly on tasks designed to assess compositional\ngeneralization (i.e. where training and testing distributions differ in ways\nthat would be trivial for a compositional strategy to resolve). Their poor\nperformance on these tasks may in part be due to the nature of supervised\nlearning which assumes training and testing data to be drawn from the same\ndistribution. We implement a meta-learning augmented version of supervised\nlearning whose objective directly optimizes for out-of-distribution\ngeneralization. We construct pairs of tasks for meta-learning by sub-sampling\nexisting training data. Each pair of tasks is constructed to contain relevant\nexamples, as determined by a similarity metric, in an effort to inhibit models\nfrom memorizing their input. Experimental results on the COGS and SCAN datasets\nshow that our similarity-driven meta-learning can improve generalization\nperformance.",
          "link": "http://arxiv.org/abs/2106.04252",
          "publishedOn": "2021-06-30T02:01:00.446Z",
          "wordCount": 625,
          "title": "Meta-Learning to Compositionally Generalize. (arXiv:2106.04252v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.05268",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wangchunshu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jinyi Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hanlin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Chenyan Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jian Tang</a>",
          "description": "Recently generating natural language explanations has shown very promising\nresults in not only offering interpretable explanations but also providing\nadditional information and supervision for prediction. However, existing\napproaches usually require a large set of human annotated explanations for\ntraining while collecting a large set of explanations is not only time\nconsuming but also expensive. In this paper, we develop a general framework for\ninterpretable natural language understanding that requires only a small set of\nhuman annotated explanations for training. Our framework treats natural\nlanguage explanations as latent variables that model the underlying reasoning\nprocess of a neural model. We develop a variational EM framework for\noptimization where an explanation generation module and an\nexplanation-augmented prediction module are alternatively optimized and\nmutually enhance each other. Moreover, we further propose an explanation-based\nself-training method under this framework for semi-supervised learning. It\nalternates between assigning pseudo-labels to unlabeled data and generating new\nexplanations to iteratively improve each other. Experiments on two natural\nlanguage understanding tasks demonstrate that our framework can not only make\neffective predictions in both supervised and semi-supervised settings, but also\ngenerate good natural language explanation.",
          "link": "http://arxiv.org/abs/2011.05268",
          "publishedOn": "2021-06-30T02:01:00.441Z",
          "wordCount": 670,
          "title": "Towards Interpretable Natural Language Understanding with Explanations as Latent Variables. (arXiv:2011.05268v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.08454",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Hussein_A/0/1/0/all/0/1\">Amir Hussein</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ali_A/0/1/0/all/0/1\">Ahmed Ali</a>",
          "description": "Recent advances in automatic speech recognition (ASR) have achieved accuracy\nlevels comparable to human transcribers, which led researchers to debate if the\nmachine has reached human performance. Previous work focused on the English\nlanguage and modular hidden Markov model-deep neural network (HMM-DNN) systems.\nIn this paper, we perform a comprehensive benchmarking for end-to-end\ntransformer ASR, modular HMM-DNN ASR, and human speech recognition (HSR) on the\nArabic language and its dialects. For the HSR, we evaluate linguist performance\nand lay-native speaker performance on a new dataset collected as a part of this\nstudy. For ASR the end-to-end work led to 12.5%, 27.5%, 33.8% WER; a new\nperformance milestone for the MGB2, MGB3, and MGB5 challenges respectively. Our\nresults suggest that human performance in the Arabic language is still\nconsiderably better than the machine with an absolute WER gap of 3.5% on\naverage.",
          "link": "http://arxiv.org/abs/2101.08454",
          "publishedOn": "2021-06-30T02:01:00.419Z",
          "wordCount": 608,
          "title": "Arabic Speech Recognition by End-to-End, Modular Systems and Human. (arXiv:2101.08454v2 [eess.AS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15537",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rajput_G/0/1/0/all/0/1\">Gaurav Rajput</a>, <a href=\"http://arxiv.org/find/cs/1/au:+punn_N/0/1/0/all/0/1\">Narinder Singh punn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sonbhadra_S/0/1/0/all/0/1\">Sanjay Kumar Sonbhadra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1\">Sonali Agarwal</a>",
          "description": "With increasing popularity of social media platforms hate speech is emerging\nas a major concern, where it expresses abusive speech that targets specific\ngroup characteristics, such as gender, religion or ethnicity to spread\nviolence. Earlier people use to verbally deliver hate speeches but now with the\nexpansion of technology, some people are deliberately using social media\nplatforms to spread hate by posting, sharing, commenting, etc. Whether it is\nChristchurch mosque shootings or hate crimes against Asians in west, it has\nbeen observed that the convicts are very much influenced from hate text present\nonline. Even though AI systems are in place to flag such text but one of the\nkey challenges is to reduce the false positive rate (marking non hate as hate),\nso that these systems can detect hate speech without undermining the freedom of\nexpression. In this paper, we use ETHOS hate speech detection dataset and\nanalyze the performance of hate speech detection classifier by replacing or\nintegrating the word embeddings (fastText (FT), GloVe (GV) or FT + GV) with\nstatic BERT embeddings (BE). With the extensive experimental trails it is\nobserved that the neural network performed better with static BE compared to\nusing FT, GV or FT + GV as word embeddings. In comparison to fine-tuned BERT,\none metric that significantly improved is specificity.",
          "link": "http://arxiv.org/abs/2106.15537",
          "publishedOn": "2021-06-30T02:01:00.389Z",
          "wordCount": 655,
          "title": "Hate speech detection using static BERT embeddings. (arXiv:2106.15537v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15467",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shanshan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_P/0/1/0/all/0/1\">Pengjie Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhumin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Z/0/1/0/all/0/1\">Zhaochun Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_H/0/1/0/all/0/1\">Huasheng Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Q/0/1/0/all/0/1\">Qiang Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanoulas_E/0/1/0/all/0/1\">Evangelos Kanoulas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rijke_M/0/1/0/all/0/1\">Maarten de Rijke</a>",
          "description": "Electronic health record (EHR) coding is the task of assigning ICD codes to\neach EHR. Most previous studies either only focus on the frequent ICD codes or\ntreat rare and frequent ICD codes in the same way. These methods perform well\non frequent ICD codes but due to the extremely unbalanced distribution of ICD\ncodes, the performance on rare ones is far from satisfactory. We seek to\nimprove the performance for both frequent and rare ICD codes by using a\ncontrastive graph-based EHR coding framework, CoGraph, which re-casts EHR\ncoding as a few-shot learning task. First, we construct a heterogeneous EHR\nword-entity (HEWE) graph for each EHR, where the words and entities extracted\nfrom an EHR serve as nodes and the relations between them serve as edges. Then,\nCoGraph learns similarities and dissimilarities between HEWE graphs from\ndifferent ICD codes so that information can be transferred among them. In a\nfew-shot learning scenario, the model only has access to frequent ICD codes\nduring training, which might force it to encode features that are useful for\nfrequent ICD codes only. To mitigate this risk, CoGraph devises two graph\ncontrastive learning schemes, GSCL and GECL, that exploit the HEWE graph\nstructures so as to encode transferable features. GSCL utilizes the\nintra-correlation of different sub-graphs sampled from HEWE graphs while GECL\nexploits the inter-correlation among HEWE graphs at different clinical stages.\nExperiments on the MIMIC-III benchmark dataset show that CoGraph significantly\noutperforms state-of-the-art methods on EHR coding, not only on frequent ICD\ncodes, but also on rare codes, in terms of several evaluation indicators. On\nfrequent ICD codes, GSCL and GECL improve the classification accuracy and F1 by\n1.31% and 0.61%, respectively, and on rare ICD codes CoGraph has more obvious\nimprovements by 2.12% and 2.95%.",
          "link": "http://arxiv.org/abs/2106.15467",
          "publishedOn": "2021-06-30T02:01:00.382Z",
          "wordCount": 738,
          "title": "Few-Shot Electronic Health Record Coding through Graph Contrastive Learning. (arXiv:2106.15467v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15561",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Tan_X/0/1/0/all/0/1\">Xu Tan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qin_T/0/1/0/all/0/1\">Tao Qin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Soong_F/0/1/0/all/0/1\">Frank Soong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_T/0/1/0/all/0/1\">Tie-Yan Liu</a>",
          "description": "Text to speech (TTS), or speech synthesis, which aims to synthesize\nintelligible and natural speech given text, is a hot research topic in speech,\nlanguage, and machine learning communities and has broad applications in the\nindustry. As the development of deep learning and artificial intelligence,\nneural network-based TTS has significantly improved the quality of synthesized\nspeech in recent years. In this paper, we conduct a comprehensive survey on\nneural TTS, aiming to provide a good understanding of current research and\nfuture trends. We focus on the key components in neural TTS, including text\nanalysis, acoustic models and vocoders, and several advanced topics, including\nfast TTS, low-resource TTS, robust TTS, expressive TTS, and adaptive TTS, etc.\nWe further summarize resources related to TTS (e.g., datasets, opensource\nimplementations) and discuss future research directions. This survey can serve\nboth academic researchers and industry practitioners working on TTS.",
          "link": "http://arxiv.org/abs/2106.15561",
          "publishedOn": "2021-06-30T02:01:00.372Z",
          "wordCount": 607,
          "title": "A Survey on Neural Speech Synthesis. (arXiv:2106.15561v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15343",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Maniar_T/0/1/0/all/0/1\">Tabish Maniar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akkinepally_A/0/1/0/all/0/1\">Alekhya Akkinepally</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Anantha Sharma</a>",
          "description": "The use of machine learning algorithms to model user behavior and drive\nbusiness decisions has become increasingly commonplace, specifically providing\nintelligent recommendations to automated decision making. This has led to an\nincrease in the use of customers personal data to analyze customer behavior and\npredict their interests in a companys products. Increased use of this customer\npersonal data can lead to better models but also to the potential of customer\ndata being leaked, reverse engineered, and mishandled. In this paper, we assess\ndifferential privacy as a solution to address these privacy problems by\nbuilding privacy protections into the data engineering and model training\nstages of predictive model development. Our interest is a pragmatic\nimplementation in an operational environment, which necessitates a general\npurpose differentially private modeling framework, and we evaluate one such\ntool from LeapYear as applied to the Credit Risk modeling domain. Credit Risk\nModel is a major modeling methodology in banking and finance where user data is\nanalyzed to determine the total Expected Loss to the bank. We examine the\napplication of differential privacy on the credit risk model and evaluate the\nperformance of a Differentially Private Model with a Non Differentially Private\nModel. Credit Risk Model is a major modeling methodology in banking and finance\nwhere users data is analyzed to determine the total Expected Loss to the bank.\nIn this paper, we explore the application of differential privacy on the credit\nrisk model and evaluate the performance of a Non Differentially Private Model\nwith Differentially Private Model.",
          "link": "http://arxiv.org/abs/2106.15343",
          "publishedOn": "2021-06-30T02:01:00.362Z",
          "wordCount": 697,
          "title": "Differential Privacy for Credit Risk Model. (arXiv:2106.15343v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15195",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Marie_B/0/1/0/all/0/1\">Benjamin Marie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fujita_A/0/1/0/all/0/1\">Atsushi Fujita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rubino_R/0/1/0/all/0/1\">Raphael Rubino</a>",
          "description": "This paper presents the first large-scale meta-evaluation of machine\ntranslation (MT). We annotated MT evaluations conducted in 769 research papers\npublished from 2010 to 2020. Our study shows that practices for automatic MT\nevaluation have dramatically changed during the past decade and follow\nconcerning trends. An increasing number of MT evaluations exclusively rely on\ndifferences between BLEU scores to draw conclusions, without performing any\nkind of statistical significance testing nor human evaluation, while at least\n108 metrics claiming to be better than BLEU have been proposed. MT evaluations\nin recent papers tend to copy and compare automatic metric scores from previous\nwork to claim the superiority of a method or an algorithm without confirming\nneither exactly the same training, validating, and testing data have been used\nnor the metric scores are comparable. Furthermore, tools for reporting\nstandardized metric scores are still far from being widely adopted by the MT\ncommunity. After showing how the accumulation of these pitfalls leads to\ndubious evaluation, we propose a guideline to encourage better automatic MT\nevaluation along with a simple meta-evaluation scoring method to assess its\ncredibility.",
          "link": "http://arxiv.org/abs/2106.15195",
          "publishedOn": "2021-06-30T02:01:00.339Z",
          "wordCount": 622,
          "title": "Scientific Credibility of Machine Translation Research: A Meta-Evaluation of 769 Papers. (arXiv:2106.15195v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15223",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Radstok_W/0/1/0/all/0/1\">Wessel Radstok</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chekol_M/0/1/0/all/0/1\">Mel Chekol</a>",
          "description": "The inclusion of temporal scopes of facts in knowledge graph embedding (KGE)\npresents significant opportunities for improving the resulting embeddings, and\nconsequently for increased performance in downstream applications. Yet, little\nresearch effort has focussed on this area and much of the carried out research\nreports only marginally improved results compared to models trained without\ntemporal scopes (static models). Furthermore, rather than leveraging existing\nwork on static models, they introduce new models specific to temporal knowledge\ngraphs. We propose a novel perspective that takes advantage of the power of\nexisting static embedding models by focussing effort on manipulating the data\ninstead. Our method, SpliMe, draws inspiration from the field of signal\nprocessing and early work in graph embedding. We show that SpliMe competes with\nor outperforms the current state of the art in temporal KGE. Additionally, we\nuncover issues with the procedure currently used to assess the performance of\nstatic models on temporal graphs and introduce two ways to counteract them.",
          "link": "http://arxiv.org/abs/2106.15223",
          "publishedOn": "2021-06-30T02:01:00.320Z",
          "wordCount": 598,
          "title": "Leveraging Static Models for Link Prediction in Temporal Knowledge Graphs. (arXiv:2106.15223v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15498",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hagerer_G/0/1/0/all/0/1\">Gerhard Hagerer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_W/0/1/0/all/0/1\">Wenbin Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danner_H/0/1/0/all/0/1\">Hannah Danner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Groh_G/0/1/0/all/0/1\">Georg Groh</a>",
          "description": "Social media offer plenty of information to perform market research in order\nto meet the requirements of customers. One way how this research is conducted\nis that a domain expert gathers and categorizes user-generated content into a\ncomplex and fine-grained class structure. In many of such cases, little data\nmeets complex annotations. It is not yet fully understood how this can be\nleveraged successfully for classification. We examine the classification\naccuracy of expert labels when used with a) many fine-grained classes and b)\nfew abstract classes. For scenario b) we compare abstract class labels given by\nthe domain expert as baseline and by automatic hierarchical clustering. We\ncompare this to another baseline where the entire class structure is given by a\ncompletely unsupervised clustering approach. By doing so, this work can serve\nas an example of how complex expert annotations are potentially beneficial and\ncan be utilized in the most optimal way for opinion mining in highly specific\ndomains. By exploring across a range of techniques and experiments, we find\nthat automated class abstraction approaches in particular the unsupervised\napproach performs remarkably well against domain expert baseline on text\nclassification tasks. This has the potential to inspire opinion mining\napplications in order to support market researchers in practice and to inspire\nfine-grained automated content analysis on a large scale.",
          "link": "http://arxiv.org/abs/2106.15498",
          "publishedOn": "2021-06-30T02:01:00.311Z",
          "wordCount": 657,
          "title": "Classification of Consumer Belief Statements From Social Media. (arXiv:2106.15498v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15313",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Issam_K/0/1/0/all/0/1\">Kalliath Abdul Rasheed Issam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_S/0/1/0/all/0/1\">Shivam Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+N_S/0/1/0/all/0/1\">Subalalitha C. N</a>",
          "description": "Text summarization is an approach for identifying important information\npresent within text documents. This computational technique aims to generate\nshorter versions of the source text, by including only the relevant and salient\ninformation present within the source text. In this paper, we propose a novel\nmethod to summarize a text document by clustering its contents based on latent\ntopics produced using topic modeling techniques and by generating extractive\nsummaries for each of the identified text clusters. All extractive\nsub-summaries are later combined to generate a summary for any given source\ndocument. We utilize the lesser used and challenging WikiHow dataset in our\napproach to text summarization. This dataset is unlike the commonly used news\ndatasets which are available for text summarization. The well-known news\ndatasets present their most important information in the first few lines of\ntheir source texts, which make their summarization a lesser challenging task\nwhen compared to summarizing the WikiHow dataset. Contrary to these news\ndatasets, the documents in the WikiHow dataset are written using a generalized\napproach and have lesser abstractedness and higher compression ratio, thus\nproposing a greater challenge to generate summaries. A lot of the current\nstate-of-the-art text summarization techniques tend to eliminate important\ninformation present in source documents in the favor of brevity. Our proposed\ntechnique aims to capture all the varied information present in source\ndocuments. Although the dataset proved challenging, after performing extensive\ntests within our experimental setup, we have discovered that our model produces\nencouraging ROUGE results and summaries when compared to the other published\nextractive and abstractive text summarization models.",
          "link": "http://arxiv.org/abs/2106.15313",
          "publishedOn": "2021-06-30T02:01:00.304Z",
          "wordCount": 718,
          "title": "Topic Modeling Based Extractive Text Summarization. (arXiv:2106.15313v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15231",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Linyi_Y/0/1/0/all/0/1\">Yang Linyi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiazheng_L/0/1/0/all/0/1\">Li Jiazheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Padraig_C/0/1/0/all/0/1\">Cunningham P&#xe1;draig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yue_Z/0/1/0/all/0/1\">Zhang Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barry_S/0/1/0/all/0/1\">Smyth Barry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruihai_D/0/1/0/all/0/1\">Dong Ruihai</a>",
          "description": "While state-of-the-art NLP models have been achieving the excellent\nperformance of a wide range of tasks in recent years, important questions are\nbeing raised about their robustness and their underlying sensitivity to\nsystematic biases that may exist in their training and test data. Such issues\ncome to be manifest in performance problems when faced with out-of-distribution\ndata in the field. One recent solution has been to use counterfactually\naugmented datasets in order to reduce any reliance on spurious patterns that\nmay exist in the original data. Producing high-quality augmented data can be\ncostly and time-consuming as it usually needs to involve human feedback and\ncrowdsourcing efforts. In this work, we propose an alternative by describing\nand evaluating an approach to automatically generating counterfactual data for\ndata augmentation and explanation. A comprehensive evaluation on several\ndifferent datasets and using a variety of state-of-the-art benchmarks\ndemonstrate how our approach can achieve significant improvements in model\nperformance when compared to models training on the original data and even when\ncompared to models trained with the benefit of human-generated augmented data.",
          "link": "http://arxiv.org/abs/2106.15231",
          "publishedOn": "2021-06-30T02:01:00.295Z",
          "wordCount": 632,
          "title": "Exploring the Efficacy of Automatically Generated Counterfactuals for Sentiment Analysis. (arXiv:2106.15231v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15355",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_A/0/1/0/all/0/1\">Ana Valeria Gonzalez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rogers_A/0/1/0/all/0/1\">Anna Rogers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sogaard_A/0/1/0/all/0/1\">Anders S&#xf8;gaard</a>",
          "description": "A myriad of explainability methods have been proposed in recent years, but\nthere is little consensus on how to evaluate them. While automatic metrics\nallow for quick benchmarking, it isn't clear how such metrics reflect human\ninteraction with explanations. Human evaluation is of paramount importance, but\nprevious protocols fail to account for belief biases affecting human\nperformance, which may lead to misleading conclusions. We provide an overview\nof belief bias, its role in human evaluation, and ideas for NLP practitioners\non how to account for it. For two experimental paradigms, we present a case\nstudy of gradient-based explainability introducing simple ways to account for\nhumans' prior beliefs: models of varying quality and adversarial examples. We\nshow that conclusions about the highest performing methods change when\nintroducing such controls, pointing to the importance of accounting for belief\nbias in evaluation.",
          "link": "http://arxiv.org/abs/2106.15355",
          "publishedOn": "2021-06-30T02:01:00.279Z",
          "wordCount": 581,
          "title": "On the Interaction of Belief Bias and Explanations. (arXiv:2106.15355v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15153",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Yang_J/0/1/0/all/0/1\">Jinhyeok Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bae_J/0/1/0/all/0/1\">Jae-Sung Bae</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bak_T/0/1/0/all/0/1\">Taejun Bak</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_Y/0/1/0/all/0/1\">Youngik Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cho_H/0/1/0/all/0/1\">Hoon-Young Cho</a>",
          "description": "Recent advances in neural multi-speaker text-to-speech (TTS) models have\nenabled the generation of reasonably good speech quality with a single model\nand made it possible to synthesize the speech of a speaker with limited\ntraining data. Fine-tuning to the target speaker data with the multi-speaker\nmodel can achieve better quality, however, there still exists a gap compared to\nthe real speech sample and the model depends on the speaker. In this work, we\npropose GANSpeech, which is a high-fidelity multi-speaker TTS model that adopts\nthe adversarial training method to a non-autoregressive multi-speaker TTS\nmodel. In addition, we propose simple but efficient automatic scaling methods\nfor feature matching loss used in adversarial training. In the subjective\nlistening tests, GANSpeech significantly outperformed the baseline\nmulti-speaker FastSpeech and FastSpeech2 models, and showed a better MOS score\nthan the speaker-specific fine-tuned FastSpeech2.",
          "link": "http://arxiv.org/abs/2106.15153",
          "publishedOn": "2021-06-30T02:01:00.260Z",
          "wordCount": 595,
          "title": "GANSpeech: Adversarial Training for High-Fidelity Multi-Speaker Speech Synthesis. (arXiv:2106.15153v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15247",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ochieng_P/0/1/0/all/0/1\">Peter Ochieng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mugambi_D/0/1/0/all/0/1\">Dennis Mugambi</a>",
          "description": "Conversational machine reading (CMR) tools have seen a rapid progress in the\nrecent past. The current existing tools rely on the supervised learning\ntechnique which require labeled dataset for their training. The supervised\ntechnique necessitates that for every new rule text, a manually labeled dataset\nmust be created. This is tedious and error prone. This paper introduces and\ndemonstrates how unsupervised learning technique can be applied in the\ndevelopment of CMR. Specifically, we demonstrate how unsupervised learning can\nbe used in rule extraction and entailment modules of CMR. Compared to the\ncurrent best CMR tool, our developed framework reports 3.3% improvement in\nmicro averaged accuracy and 1.4 % improvement in macro averaged accuracy.",
          "link": "http://arxiv.org/abs/2106.15247",
          "publishedOn": "2021-06-30T02:01:00.252Z",
          "wordCount": 540,
          "title": "Unsupervised Technique To Conversational Machine Reading. (arXiv:2106.15247v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15236",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hammoud_J/0/1/0/all/0/1\">Jaafar Hammoud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vatian_A/0/1/0/all/0/1\">Aleksandra Vatian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dobrenko_N/0/1/0/all/0/1\">Natalia Dobrenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vedernikov_N/0/1/0/all/0/1\">Nikolai Vedernikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shalyto_A/0/1/0/all/0/1\">Anatoly Shalyto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gusarova_N/0/1/0/all/0/1\">Natalia Gusarova</a>",
          "description": "The Arabic language suffers from a great shortage of datasets suitable for\ntraining deep learning models, and the existing ones include general\nnon-specialized classifications. In this work, we introduce a new Arab medical\ndataset, which includes two thousand medical documents collected from several\nArabic medical websites, in addition to the Arab Medical Encyclopedia. The\ndataset was built for the task of classifying texts and includes 10 classes\n(Blood, Bone, Cardiovascular, Ear, Endocrine, Eye, Gastrointestinal, Immune,\nLiver and Nephrological) diseases. Experiments on the dataset were performed by\nfine-tuning three pre-trained models: BERT from Google, Arabert that based on\nBERT with large Arabic corpus, and AraBioNER that based on Arabert with Arabic\nmedical corpus.",
          "link": "http://arxiv.org/abs/2106.15236",
          "publishedOn": "2021-06-30T02:01:00.239Z",
          "wordCount": 544,
          "title": "New Arabic Medical Dataset for Diseases Classification. (arXiv:2106.15236v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15167",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tong_M/0/1/0/all/0/1\">Meihan Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1\">Bin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yixin Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Minghui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1\">Lei Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Juanzi Li</a>",
          "description": "Few-shot Named Entity Recognition (NER) exploits only a handful of\nannotations to identify and classify named entity mentions. Prototypical\nnetwork shows superior performance on few-shot NER. However, existing\nprototypical methods fail to differentiate rich semantics in other-class words,\nwhich will aggravate overfitting under few shot scenario. To address the issue,\nwe propose a novel model, Mining Undefined Classes from Other-class (MUCO),\nthat can automatically induce different undefined classes from the other class\nto improve few-shot NER. With these extra-labeled undefined classes, our method\nwill improve the discriminative ability of NER classifier and enhance the\nunderstanding of predefined classes with stand-by semantic knowledge.\nExperimental results demonstrate that our model outperforms five\nstate-of-the-art models in both 1-shot and 5-shots settings on four NER\nbenchmarks. We will release the code upon acceptance. The source code is\nreleased on https: //github.com/shuaiwa16/OtherClassNER.git.",
          "link": "http://arxiv.org/abs/2106.15167",
          "publishedOn": "2021-06-30T02:01:00.224Z",
          "wordCount": 578,
          "title": "Learning from Miscellaneous Other-Class Words for Few-shot Named Entity Recognition. (arXiv:2106.15167v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15217",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Jianhao Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chenming Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>",
          "description": "The evaluation of neural machine translation systems is usually built upon\ngenerated translation of a certain decoding method (e.g., beam search) with\nevaluation metrics over the generated translation (e.g., BLEU). However, this\nevaluation framework suffers from high search errors brought by heuristic\nsearch algorithms and is limited by its nature of evaluation over one best\ncandidate. In this paper, we propose a novel evaluation protocol, which not\nonly avoids the effect of search errors but provides a system-level evaluation\nin the perspective of model ranking. In particular, our method is based on our\nnewly proposed exact top-$k$ decoding instead of beam search. Our approach\nevaluates model errors by the distance between the candidate spaces scored by\nthe references and the model respectively. Extensive experiments on WMT'14\nEnglish-German demonstrate that bad ranking ability is connected to the\nwell-known beam search curse, and state-of-the-art Transformer models are\nfacing serious ranking errors. By evaluating various model architectures and\ntechniques, we provide several interesting findings. Finally, to effectively\napproximate the exact search algorithm with same time cost as original beam\nsearch, we present a minimum heap augmented beam search algorithm.",
          "link": "http://arxiv.org/abs/2106.15217",
          "publishedOn": "2021-06-30T02:01:00.215Z",
          "wordCount": 624,
          "title": "Rethinking the Evaluation of Neural Machine Translation. (arXiv:2106.15217v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15238",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mittal_A/0/1/0/all/0/1\">Ashish Mittal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bharadwaj_S/0/1/0/all/0/1\">Samarth Bharadwaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khare_S/0/1/0/all/0/1\">Shreya Khare</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chemmengath_S/0/1/0/all/0/1\">Saneem Chemmengath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sankaranarayanan_K/0/1/0/all/0/1\">Karthik Sankaranarayanan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kingsbury_B/0/1/0/all/0/1\">Brian Kingsbury</a>",
          "description": "Spoken intent detection has become a popular approach to interface with\nvarious smart devices with ease. However, such systems are limited to the\npreset list of intents-terms or commands, which restricts the quick\ncustomization of personal devices to new intents. This paper presents a\nfew-shot spoken intent classification approach with task-agnostic\nrepresentations via meta-learning paradigm. Specifically, we leverage the\npopular representation-based meta-learning learning to build a task-agnostic\nrepresentation of utterances, that then use a linear classifier for prediction.\nWe evaluate three such approaches on our novel experimental protocol developed\non two popular spoken intent classification datasets: Google Commands and the\nFluent Speech Commands dataset. For a 5-shot (1-shot) classification of novel\nclasses, the proposed framework provides an average classification accuracy of\n88.6% (76.3%) on the Google Commands dataset, and 78.5% (64.2%) on the Fluent\nSpeech Commands dataset. The performance is comparable to traditionally\nsupervised classification models with abundant training samples.",
          "link": "http://arxiv.org/abs/2106.15238",
          "publishedOn": "2021-06-30T02:01:00.164Z",
          "wordCount": 609,
          "title": "Representation based meta-learning for few-shot spoken intent recognition. (arXiv:2106.15238v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15102",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ansari_M/0/1/0/all/0/1\">M Zeeshan Ansari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_T/0/1/0/all/0/1\">Tanvir Ahmad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beg_M/0/1/0/all/0/1\">M M Sufyan Beg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ikram_A/0/1/0/all/0/1\">Asma Ikram</a>",
          "description": "The conventional natural language processing approaches are not accustomed to\nthe social media text due to colloquial discourse and non-homogeneous\ncharacteristics. Significantly, the language identification in a multilingual\ndocument is ascertained to be a preceding subtask in several information\nextraction applications such as information retrieval, named entity\nrecognition, relation extraction, etc. The problem is often more challenging in\ncode-mixed documents wherein foreign languages words are drawn into base\nlanguage while framing the text. The word embeddings are powerful language\nmodeling tools for representation of text documents useful in obtaining\nsimilarity between words or documents. We present a simple probabilistic\napproach for building efficient word embedding for code-mixed text and\nexemplifying it over language identification of Hindi-English short test\nmessages scrapped from Twitter. We examine its efficacy for the classification\ntask using bidirectional LSTMs and SVMs and observe its improved scores over\nvarious existing code-mixed embeddings",
          "link": "http://arxiv.org/abs/2106.15102",
          "publishedOn": "2021-06-30T02:01:00.130Z",
          "wordCount": 583,
          "title": "A Simple and Efficient Probabilistic Language model for Code-Mixed Text. (arXiv:2106.15102v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15115",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ranathunga_S/0/1/0/all/0/1\">Surangika Ranathunga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_E/0/1/0/all/0/1\">En-Shiun Annie Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skenduli_M/0/1/0/all/0/1\">Marjana Prifti Skenduli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shekhar_R/0/1/0/all/0/1\">Ravi Shekhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alam_M/0/1/0/all/0/1\">Mehreen Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaur_R/0/1/0/all/0/1\">Rishemjit Kaur</a>",
          "description": "Neural Machine Translation (NMT) has seen a tremendous spurt of growth in\nless than ten years, and has already entered a mature phase. While considered\nas the most widely used solution for Machine Translation, its performance on\nlow-resource language pairs still remains sub-optimal compared to the\nhigh-resource counterparts, due to the unavailability of large parallel\ncorpora. Therefore, the implementation of NMT techniques for low-resource\nlanguage pairs has been receiving the spotlight in the recent NMT research\narena, thus leading to a substantial amount of research reported on this topic.\nThis paper presents a detailed survey of research advancements in low-resource\nlanguage NMT (LRL-NMT), along with a quantitative analysis aimed at identifying\nthe most popular solutions. Based on our findings from reviewing previous work,\nthis survey paper provides a set of guidelines to select the possible NMT\ntechnique for a given LRL data setting. It also presents a holistic view of the\nLRL-NMT research landscape and provides a list of recommendations to further\nenhance the research efforts on LRL-NMT.",
          "link": "http://arxiv.org/abs/2106.15115",
          "publishedOn": "2021-06-30T02:01:00.108Z",
          "wordCount": 618,
          "title": "Neural Machine Translation for Low-Resource Languages: A Survey. (arXiv:2106.15115v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15078",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Guangyi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zichao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_T/0/1/0/all/0/1\">Tianhua Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Bowen Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1\">Shuguang Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhiting Hu</a>",
          "description": "Neural text generation models are typically trained by maximizing\nlog-likelihood with the sequence cross entropy loss, which encourages an exact\ntoken-by-token match between a target sequence with a generated sequence. Such\ntraining objective is sub-optimal when the target sequence not perfect, e.g.,\nwhen the target sequence is corrupted with noises, or when only weak sequence\nsupervision is available. To address this challenge, we propose a novel\nEdit-Invariant Sequence Loss (EISL), which computes the matching loss of a\ntarget n-gram with all n-grams in the generated sequence. EISL draws\ninspirations from convolutional networks (ConvNets) which are shift-invariant\nto images, hence is robust to the shift of n-grams to tolerate edits in the\ntarget sequences. Moreover, the computation of EISL is essentially a\nconvolution operation with target n-grams as kernels, which is easy to\nimplement with existing libraries. To demonstrate the effectiveness of EISL, we\nconduct experiments on three tasks: machine translation with noisy target\nsequences, unsupervised text style transfer, and non-autoregressive machine\ntranslation. Experimental results show our method significantly outperforms\ncross entropy loss on these three tasks.",
          "link": "http://arxiv.org/abs/2106.15078",
          "publishedOn": "2021-06-30T02:01:00.100Z",
          "wordCount": 631,
          "title": "Don't Take It Literally: An Edit-Invariant Sequence Loss for Text Generation. (arXiv:2106.15078v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15065",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Arora_S/0/1/0/all/0/1\">Siddhant Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ostapenko_A/0/1/0/all/0/1\">Alissa Ostapenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Viswanathan_V/0/1/0/all/0/1\">Vijay Viswanathan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dalmia_S/0/1/0/all/0/1\">Siddharth Dalmia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metze_F/0/1/0/all/0/1\">Florian Metze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Black_A/0/1/0/all/0/1\">Alan W Black</a>",
          "description": "Decomposable tasks are complex and comprise of a hierarchy of sub-tasks.\nSpoken intent prediction, for example, combines automatic speech recognition\nand natural language understanding. Existing benchmarks, however, typically\nhold out examples for only the surface-level sub-task. As a result, models with\nsimilar performance on these benchmarks may have unobserved performance\ndifferences on the other sub-tasks. To allow insightful comparisons between\ncompetitive end-to-end architectures, we propose a framework to construct\nrobust test sets using coordinate ascent over sub-task specific utility\nfunctions. Given a dataset for a decomposable task, our method optimally\ncreates a test set for each sub-task to individually assess sub-components of\nthe end-to-end model. Using spoken language understanding as a case study, we\ngenerate new splits for the Fluent Speech Commands and Snips SmartLights\ndatasets. Each split has two test sets: one with held-out utterances assessing\nnatural language understanding abilities, and one with held-out speakers to\ntest speech processing skills. Our splits identify performance gaps up to 10%\nbetween end-to-end systems that were within 1% of each other on the original\ntest sets. These performance gaps allow more realistic and actionable\ncomparisons between different architectures, driving future model development.\nWe release our splits and tools for the community.",
          "link": "http://arxiv.org/abs/2106.15065",
          "publishedOn": "2021-06-30T02:01:00.084Z",
          "wordCount": 662,
          "title": "Rethinking End-to-End Evaluation of Decomposable Tasks: A Case Study on Spoken Language Understanding. (arXiv:2106.15065v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15105",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ansari_M/0/1/0/all/0/1\">Mohd Zeeshan Ansari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_T/0/1/0/all/0/1\">Tanvir Ahmad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bari_N/0/1/0/all/0/1\">Noaima Bari</a>",
          "description": "Language Identification in textual documents is the process of automatically\ndetecting the language contained in a document based on its content. The\npresent Language Identification techniques presume that a document contains\ntext in one of the fixed set of languages, however, this presumption is\nincorrect when dealing with multilingual document which includes content in\nmore than one possible language. Due to the unavailability of large standard\ncorpora for Hindi-English mixed lingual language processing tasks we propose\nthe language lexicons, a novel kind of lexical database that supports several\nmultilingual language processing tasks. These lexicons are built by learning\nclassifiers over transliterated Hindi and English vocabulary. The designed\nlexicons possess richer quantitative characteristic than its primary source of\ncollection which is revealed using the visualization techniques.",
          "link": "http://arxiv.org/abs/2106.15105",
          "publishedOn": "2021-06-30T02:01:00.044Z",
          "wordCount": 553,
          "title": "Language Lexicons for Hindi-English Multilingual Text Processing. (arXiv:2106.15105v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14885",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nentidis_A/0/1/0/all/0/1\">Anastasios Nentidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katsimpras_G/0/1/0/all/0/1\">Georgios Katsimpras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vandorou_E/0/1/0/all/0/1\">Eirini Vandorou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krithara_A/0/1/0/all/0/1\">Anastasia Krithara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gasco_L/0/1/0/all/0/1\">Luis Gasco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krallinger_M/0/1/0/all/0/1\">Martin Krallinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paliouras_G/0/1/0/all/0/1\">Georgios Paliouras</a>",
          "description": "Advancing the state-of-the-art in large-scale biomedical semantic indexing\nand question answering is the main focus of the BioASQ challenge. BioASQ\norganizes respective tasks where different teams develop systems that are\nevaluated on the same benchmark datasets that represent the real information\nneeds of experts in the biomedical domain. This paper presents an overview of\nthe ninth edition of the BioASQ challenge in the context of the Conference and\nLabs of the Evaluation Forum (CLEF) 2021. In this year, a new question\nanswering task, named Synergy, is introduced to support researchers studying\nthe COVID-19 disease and measure the ability of the participating teams to\ndiscern information while the problem is still developing. In total, 42 teams\nwith more than 170 systems were registered to participate in the four tasks of\nthe challenge. The evaluation results, similarly to previous years, show a\nperformance gain against the baselines which indicates the continuous\nimprovement of the state-of-the-art in this field.",
          "link": "http://arxiv.org/abs/2106.14885",
          "publishedOn": "2021-06-30T02:01:00.037Z",
          "wordCount": 677,
          "title": "Overview of BioASQ 2021: The ninth BioASQ challenge on Large-Scale Biomedical Semantic Indexing and Question Answering. (arXiv:2106.14885v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15135",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1\">Fangwei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tu_S/0/1/0/all/0/1\">Shangqing Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jiaxin Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Juanzi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1\">Lei Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_T/0/1/0/all/0/1\">Tong Cui</a>",
          "description": "Wikipedia abstract generation aims to distill a Wikipedia abstract from web\nsources and has met significant success by adopting multi-document\nsummarization techniques. However, previous works generally view the abstract\nas plain text, ignoring the fact that it is a description of a certain entity\nand can be decomposed into different topics. In this paper, we propose a\ntwo-stage model TWAG that guides the abstract generation with topical\ninformation. First, we detect the topic of each input paragraph with a\nclassifier trained on existing Wikipedia articles to divide input documents\ninto different topics. Then, we predict the topic distribution of each abstract\nsentence, and decode the sentence from topic-aware representations with a\nPointer-Generator network. We evaluate our model on the WikiCatSum dataset, and\nthe results show that \\modelnames outperforms various existing baselines and is\ncapable of generating comprehensive abstracts. Our code and dataset can be\naccessed at \\url{https://github.com/THU-KEG/TWAG}",
          "link": "http://arxiv.org/abs/2106.15135",
          "publishedOn": "2021-06-30T02:00:59.970Z",
          "wordCount": 585,
          "title": "TWAG: A Topic-Guided Wikipedia Abstract Generator. (arXiv:2106.15135v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15103",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gillis_N/0/1/0/all/0/1\">Noa Baker Gillis</a>",
          "description": "We analyze 6.7 million case law documents to determine the presence of gender\nbias within our judicial system. We find that current bias detectino methods in\nNLP are insufficient to determine gender bias in our case law database and\npropose an alternative approach. We show that existing algorithms' inconsistent\nresults are consequences of prior research's definition of biases themselves.\nBias detection algorithms rely on groups of words to represent bias (e.g.,\n'salary,' 'job,' and 'boss' to represent employment as a potentially biased\ntheme against women in text). However, the methods to build these groups of\nwords have several weaknesses, primarily that the word lists are based on the\nresearchers' own intuitions. We suggest two new methods of automating the\ncreation of word lists to represent biases. We find that our methods outperform\ncurrent NLP bias detection methods. Our research improves the capabilities of\nNLP technology to detect bias and highlights gender biases present in\ninfluential case law. In order test our NLP bias detection method's\nperformance, we regress our results of bias in case law against U.S census data\nof women's participation in the workforce in the last 100 years.",
          "link": "http://arxiv.org/abs/2106.15103",
          "publishedOn": "2021-06-30T02:00:59.960Z",
          "wordCount": 617,
          "title": "Sexism in the Judiciary. (arXiv:2106.15103v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15110",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dhingra_B/0/1/0/all/0/1\">Bhuwan Dhingra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cole_J/0/1/0/all/0/1\">Jeremy R. Cole</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eisenschlos_J/0/1/0/all/0/1\">Julian Martin Eisenschlos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gillick_D/0/1/0/all/0/1\">Daniel Gillick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eisenstein_J/0/1/0/all/0/1\">Jacob Eisenstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_W/0/1/0/all/0/1\">William W. Cohen</a>",
          "description": "Many facts come with an expiration date, from the name of the President to\nthe basketball team Lebron James plays for. But language models (LMs) are\ntrained on snapshots of data collected at a specific moment in time, and this\ncan limit their utility, especially in the closed-book setting where the\npretraining corpus must contain the facts the model should memorize. We\nintroduce a diagnostic dataset aimed at probing LMs for factual knowledge that\nchanges over time and highlight problems with LMs at either end of the spectrum\n-- those trained on specific slices of temporal data, as well as those trained\non a wide range of temporal data. To mitigate these problems, we propose a\nsimple technique for jointly modeling text with its timestamp. This improves\nmemorization of seen facts from the training time period, as well as\ncalibration on predictions about unseen facts from future time periods. We also\nshow that models trained with temporal context can be efficiently ``refreshed''\nas new data arrives, without the need for retraining from scratch.",
          "link": "http://arxiv.org/abs/2106.15110",
          "publishedOn": "2021-06-30T02:00:59.929Z",
          "wordCount": 610,
          "title": "Time-Aware Language Models as Temporal Knowledge Bases. (arXiv:2106.15110v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15142",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyue Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiahai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenghong Li</a>",
          "description": "Generating high-quality and diverse essays with a set of topics is a\nchallenging task in natural language generation. Since several given topics\nonly provide limited source information, utilizing various topic-related\nknowledge is essential for improving essay generation performance. However,\nprevious works cannot sufficiently use that knowledge to facilitate the\ngeneration procedure. This paper aims to improve essay generation by extracting\ninformation from both internal and external knowledge. Thus, a topic-to-essay\ngeneration model with comprehensive knowledge enhancement, named TEGKE, is\nproposed. For internal knowledge enhancement, both topics and related essays\nare fed to a teacher network as source information. Then, informative features\nwould be obtained from the teacher network and transferred to a student network\nwhich only takes topics as input but provides comparable information compared\nwith the teacher network. For external knowledge enhancement, a topic knowledge\ngraph encoder is proposed. Unlike the previous works only using the nearest\nneighbors of topics in the commonsense base, our topic knowledge graph encoder\ncould exploit more structural and semantic information of the commonsense\nknowledge graph to facilitate essay generation. Moreover, the adversarial\ntraining based on the Wasserstein distance is proposed to improve generation\nquality. Experimental results demonstrate that TEGKE could achieve\nstate-of-the-art performance on both automatic and human evaluation.",
          "link": "http://arxiv.org/abs/2106.15142",
          "publishedOn": "2021-06-30T02:00:59.920Z",
          "wordCount": 635,
          "title": "Topic-to-Essay Generation with Comprehensive Knowledge Enhancement. (arXiv:2106.15142v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15085",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chai_J/0/1/0/all/0/1\">Junyi Chai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yujie He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hashemi_H/0/1/0/all/0/1\">Homa Hashemi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parveen_D/0/1/0/all/0/1\">Daraksha Parveen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kondapally_R/0/1/0/all/0/1\">Ranganath Kondapally</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wenjin Xu</a>",
          "description": "In this paper, we present an automatic knowledge base construction system\nfrom large scale enterprise documents with minimal efforts of human\nintervention. In the design and deployment of such a knowledge mining system\nfor enterprise, we faced several challenges including data distributional\nshift, performance evaluation, compliance requirements and other practical\nissues. We leveraged state-of-the-art deep learning models to extract\ninformation (named entities and definitions) at per document level, then\nfurther applied classical machine learning techniques to process global\nstatistical information to improve the knowledge base. Experimental results are\nreported on actual enterprise documents. This system is currently serving as\npart of a Microsoft 365 service.",
          "link": "http://arxiv.org/abs/2106.15085",
          "publishedOn": "2021-06-30T02:00:59.898Z",
          "wordCount": 539,
          "title": "Automatic Construction of Enterprise Knowledge Base. (arXiv:2106.15085v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.02317",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yunhao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yunyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quan_X/0/1/0/all/0/1\">Xiaojun Quan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jianxing Yu</a>",
          "description": "Dialogue policy learning, a subtask that determines the content of system\nresponse generation and then the degree of task completion, is essential for\ntask-oriented dialogue systems. However, the unbalanced distribution of system\nactions in dialogue datasets often causes difficulty in learning to generate\ndesired actions and responses. In this paper, we propose a\nretrieve-and-memorize framework to enhance the learning of system actions.\nSpecially, we first design a neural context-aware retrieval module to retrieve\nmultiple candidate system actions from the training set given a dialogue\ncontext. Then, we propose a memory-augmented multi-decoder network to generate\nthe system actions conditioned on the candidate actions, which allows the\nnetwork to adaptively select key information in the candidate actions and\nignore noises. We conduct experiments on the large-scale multi-domain\ntask-oriented dialogue dataset MultiWOZ 2.0 and MultiWOZ 2.1. Experimental\nresults show that our method achieves competitive performance among several\nstate-of-the-art models in the context-to-response generation task.",
          "link": "http://arxiv.org/abs/2106.02317",
          "publishedOn": "2021-06-29T01:55:14.378Z",
          "wordCount": 607,
          "title": "Retrieve & Memorize: Dialog Policy Learning with Multi-Action Memory. (arXiv:2106.02317v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.01040",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khanmohammadi_R/0/1/0/all/0/1\">Reza Khanmohammadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mirroshandel_S/0/1/0/all/0/1\">Seyed Abolghasem Mirroshandel</a>",
          "description": "Recent developments in Text Style Transfer have led this field to be more\nhighlighted than ever. The task of transferring an input's style to another is\naccompanied by plenty of challenges (e.g., fluency and content preservation)\nthat need to be taken care of. In this research, we introduce PGST, a novel\npolyglot text style transfer approach in the gender domain, composed of\ndifferent constitutive elements. In contrast to prior studies, it is feasible\nto apply a style transfer method in multiple languages by fulfilling our\nmethod's predefined elements. We have proceeded with a pre-trained word\nembedding for token replacement purposes, a character-based token classifier\nfor gender exchange purposes, and a beam search algorithm for extracting the\nmost fluent combination. Since different approaches are introduced in our\nresearch, we determine a trade-off value for evaluating different models'\nsuccess in faking our gender identification model with transferred text. To\ndemonstrate our method's multilingual applicability, we applied our method on\nboth English and Persian corpora and ended up defeating our proposed gender\nidentification model by 45.6% and 39.2%, respectively. While this research's\nfocus is not limited to a specific language, our obtained evaluation results\nare highly competitive in an analogy among English state of the art methods.",
          "link": "http://arxiv.org/abs/2009.01040",
          "publishedOn": "2021-06-29T01:55:14.265Z",
          "wordCount": 673,
          "title": "PGST: a Polyglot Gender Style Transfer method. (arXiv:2009.01040v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14373",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1\">Fei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhichao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Meishan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_D/0/1/0/all/0/1\">Donghong Ji</a>",
          "description": "Research on overlapped and discontinuous named entity recognition (NER) has\nreceived increasing attention. The majority of previous work focuses on either\noverlapped or discontinuous entities. In this paper, we propose a novel\nspan-based model that can recognize both overlapped and discontinuous entities\njointly. The model includes two major steps. First, entity fragments are\nrecognized by traversing over all possible text spans, thus, overlapped\nentities can be recognized. Second, we perform relation classification to judge\nwhether a given pair of entity fragments to be overlapping or succession. In\nthis way, we can recognize not only discontinuous entities, and meanwhile\ndoubly check the overlapped entities. As a whole, our model can be regarded as\na relation extraction paradigm essentially. Experimental results on multiple\nbenchmark datasets (i.e., CLEF, GENIA and ACE05) show that our model is highly\ncompetitive for overlapped and discontinuous NER.",
          "link": "http://arxiv.org/abs/2106.14373",
          "publishedOn": "2021-06-29T01:55:14.249Z",
          "wordCount": 586,
          "title": "A Span-Based Model for Joint Overlapped and Discontinuous Named Entity Recognition. (arXiv:2106.14373v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14444",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weijie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiaoxuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Haipang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_S/0/1/0/all/0/1\">Sanhui Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Gongfeng Li</a>",
          "description": "We present a knowledge-grounded dialog system developed for the ninth Dialog\nSystem Technology Challenge (DSTC9) Track 1 - Beyond Domain APIs: Task-oriented\nConversational Modeling with Unstructured Knowledge Access. We leverage\ntransfer learning with existing language models to accomplish the tasks in this\nchallenge track. Specifically, we divided the task into four sub-tasks and\nfine-tuned several Transformer models on each of the sub-tasks. We made\nadditional changes that yielded gains in both performance and efficiency,\nincluding the combination of the model with traditional entity-matching\ntechniques, and the addition of a pointer network to the output layer of the\nlanguage model.",
          "link": "http://arxiv.org/abs/2106.14444",
          "publishedOn": "2021-06-29T01:55:14.236Z",
          "wordCount": 539,
          "title": "A Knowledge-Grounded Dialog System Based on Pre-Trained Language Models. (arXiv:2106.14444v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2102.12254",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chhablani_G/0/1/0/all/0/1\">Gunjan Chhablani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Abheesht Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pandey_H/0/1/0/all/0/1\">Harshit Pandey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhartia_Y/0/1/0/all/0/1\">Yash Bhartia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suthaharan_S/0/1/0/all/0/1\">Shan Suthaharan</a>",
          "description": "Toxicity detection of text has been a popular NLP task in the recent years.\nIn SemEval-2021 Task-5 Toxic Spans Detection, the focus is on detecting toxic\nspans within passages. Most state-of-the-art span detection approaches employ\nvarious techniques, each of which can be broadly classified into Token\nClassification or Span Prediction approaches. In our paper, we explore simple\nversions of both of these approaches and their performance on the task.\nSpecifically, we use BERT-based models -- BERT, RoBERTa, and SpanBERT for both\napproaches. We also combine these approaches and modify them to bring\nimprovements for Toxic Spans prediction. To this end, we investigate results on\nfour hybrid approaches -- Multi-Span, Span+Token, LSTM-CRF, and a combination\nof predicted offsets using union/intersection. Additionally, we perform a\nthorough ablative analysis and analyze our observed results. Our best\nsubmission -- a combination of SpanBERT Span Predictor and RoBERTa Token\nClassifier predictions -- achieves an F1 score of 0.6753 on the test set. Our\nbest post-eval F1 score is 0.6895 on intersection of predicted offsets from\ntop-3 RoBERTa Token Classification checkpoints. These approaches improve the\nperformance by 3% on average than those of the shared baseline models -- RNNSL\nand SpaCy NER.",
          "link": "http://arxiv.org/abs/2102.12254",
          "publishedOn": "2021-06-29T01:55:14.161Z",
          "wordCount": 685,
          "title": "NLRG at SemEval-2021 Task 5: Toxic Spans Detection Leveraging BERT-based Token Classification and Span Prediction Techniques. (arXiv:2102.12254v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.06963",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fenglin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_S/0/1/0/all/0/1\">Shen Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_W/0/1/0/all/0/1\">Wei Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yuexian Zou</a>",
          "description": "Automatically generating radiology reports can improve current clinical\npractice in diagnostic radiology. On one hand, it can relieve radiologists from\nthe heavy burden of report writing; On the other hand, it can remind\nradiologists of abnormalities and avoid the misdiagnosis and missed diagnosis.\nYet, this task remains a challenging job for data-driven neural networks, due\nto the serious visual and textual data biases. To this end, we propose a\nPosterior-and-Prior Knowledge Exploring-and-Distilling approach (PPKED) to\nimitate the working patterns of radiologists, who will first examine the\nabnormal regions and assign the disease topic tags to the abnormal regions, and\nthen rely on the years of prior medical knowledge and prior working experience\naccumulations to write reports. Thus, the PPKED includes three modules:\nPosterior Knowledge Explorer (PoKE), Prior Knowledge Explorer (PrKE) and\nMulti-domain Knowledge Distiller (MKD). In detail, PoKE explores the posterior\nknowledge, which provides explicit abnormal visual regions to alleviate visual\ndata bias; PrKE explores the prior knowledge from the prior medical knowledge\ngraph (medical knowledge) and prior radiology reports (working experience) to\nalleviate textual data bias. The explored knowledge is distilled by the MKD to\ngenerate the final reports. Evaluated on MIMIC-CXR and IU-Xray datasets, our\nmethod is able to outperform previous state-of-the-art models on these two\ndatasets.",
          "link": "http://arxiv.org/abs/2106.06963",
          "publishedOn": "2021-06-29T01:55:14.155Z",
          "wordCount": 689,
          "title": "Exploring and Distilling Posterior and Prior Knowledge for Radiology Report Generation. (arXiv:2106.06963v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.11015",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Mengyu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qingtao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xinyi He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuejiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yibo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_W/0/1/0/all/0/1\">Wei Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Shi Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yining Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Daxin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dongmei Zhang</a>",
          "description": "It is common for people to create different types of charts to explore a\nmulti-dimensional dataset (table). However, to recommend commonly composed\ncharts in real world, one should take the challenges of efficiency, imbalanced\ndata and table context into consideration. In this paper, we propose\nTable2Charts framework which learns common patterns from a large corpus of\n(table, charts) pairs. Based on deep Q-learning with copying mechanism and\nheuristic searching, Table2Charts does table-to-sequence generation, where each\nsequence follows a chart template. On a large spreadsheet corpus with 165k\ntables and 266k charts, we show that Table2Charts could learn a shared\nrepresentation of table fields so that recommendation tasks on different chart\ntypes could mutually enhance each other. Table2Charts outperforms other chart\nrecommendation systems in both multi-type task (with doubled recall numbers\nR@3=0.61 and R@1=0.43) and human evaluations.",
          "link": "http://arxiv.org/abs/2008.11015",
          "publishedOn": "2021-06-29T01:55:14.104Z",
          "wordCount": 641,
          "title": "Table2Charts: Recommending Charts by Learning Shared Table Representations. (arXiv:2008.11015v4 [cs.DB] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14282",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yichu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srikumar_V/0/1/0/all/0/1\">Vivek Srikumar</a>",
          "description": "Given the prevalence of pre-trained contextualized representations in today's\nNLP, there have been several efforts to understand what information such\nrepresentations contain. A common strategy to use such representations is to\nfine-tune them for an end task. However, how fine-tuning for a task changes the\nunderlying space is less studied. In this work, we study the English BERT\nfamily and use two probing techniques to analyze how fine-tuning changes the\nspace. Our experiments reveal that fine-tuning improves performance because it\npushes points associated with a label away from other labels. By comparing the\nrepresentations before and after fine-tuning, we also discover that fine-tuning\ndoes not change the representations arbitrarily; instead, it adjusts the\nrepresentations to downstream tasks while preserving the original structure.\nFinally, using carefully constructed experiments, we show that fine-tuning can\nencode training sets in a representation, suggesting an overfitting problem of\na new kind.",
          "link": "http://arxiv.org/abs/2106.14282",
          "publishedOn": "2021-06-29T01:55:14.079Z",
          "wordCount": 574,
          "title": "A Closer Look at How Fine-tuning Changes BERT. (arXiv:2106.14282v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14365",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Arseniev_Koehler_A/0/1/0/all/0/1\">Alina Arseniev-Koehler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cochran_S/0/1/0/all/0/1\">Susan D. Cochran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mays_V/0/1/0/all/0/1\">Vickie M. Mays</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foster_J/0/1/0/all/0/1\">Jacob Gates Foster</a>",
          "description": "There is an escalating need for methods to identify latent patterns in text\ndata from many domains. We introduce a new method to identify topics in a\ncorpus and represent documents as topic sequences. Discourse Atom Topic\nModeling draws on advances in theoretical machine learning to integrate topic\nmodeling and word embedding, capitalizing on the distinct capabilities of each.\nWe first identify a set of vectors (\"discourse atoms\") that provide a sparse\nrepresentation of an embedding space. Atom vectors can be interpreted as latent\ntopics: Through a generative model, atoms map onto distributions over words;\none can also infer the topic that generated a sequence of words. We illustrate\nour method with a prominent example of underutilized text: the U.S. National\nViolent Death Reporting System (NVDRS). The NVDRS summarizes violent death\nincidents with structured variables and unstructured narratives. We identify\n225 latent topics in the narratives (e.g., preparation for death and physical\naggression); many of these topics are not captured by existing structured\nvariables. Motivated by known patterns in suicide and homicide by gender, and\nrecent research on gender biases in semantic space, we identify the gender bias\nof our topics (e.g., a topic about pain medication is feminine). We then\ncompare the gender bias of topics to their prevalence in narratives of female\nversus male victims. Results provide a detailed quantitative picture of\nreporting about lethal violence and its gendered nature. Our method offers a\nflexible and broadly applicable approach to model topics in text data.",
          "link": "http://arxiv.org/abs/2106.14365",
          "publishedOn": "2021-06-29T01:55:14.072Z",
          "wordCount": 698,
          "title": "Integrating topic modeling and word embedding to characterize violent deaths. (arXiv:2106.14365v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.00250",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_K/0/1/0/all/0/1\">Kshitij Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gautam_D/0/1/0/all/0/1\">Devansh Gautam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mamidi_R/0/1/0/all/0/1\">Radhika Mamidi</a>",
          "description": "Multimodal Machine Translation (MMT) enriches the source text with visual\ninformation for translation. It has gained popularity in recent years, and\nseveral pipelines have been proposed in the same direction. Yet, the task lacks\nquality datasets to illustrate the contribution of visual modality in the\ntranslation systems. In this paper, we propose our system under the team name\nVolta for the Multimodal Translation Task of WAT 2021 from English to Hindi. We\nalso participate in the textual-only subtask of the same language pair for\nwhich we use mBART, a pretrained multilingual sequence-to-sequence model. For\nmultimodal translation, we propose to enhance the textual input by bringing the\nvisual information to a textual domain by extracting object tags from the\nimage. We also explore the robustness of our system by systematically degrading\nthe source text. Finally, we achieve a BLEU score of 44.6 and 51.6 on the test\nset and challenge set of the multimodal task.",
          "link": "http://arxiv.org/abs/2106.00250",
          "publishedOn": "2021-06-29T01:55:13.966Z",
          "wordCount": 625,
          "title": "ViTA: Visual-Linguistic Translation by Aligning Object Tags. (arXiv:2106.00250v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11347",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gain_B/0/1/0/all/0/1\">Baban Gain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bandyopadhyay_D/0/1/0/all/0/1\">Dibyanayan Bandyopadhyay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+De_A/0/1/0/all/0/1\">Arkadipta De</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saikh_T/0/1/0/all/0/1\">Tanik Saikh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ekbal_A/0/1/0/all/0/1\">Asif Ekbal</a>",
          "description": "In this article, we present a description of our systems as a part of our\nparticipation in the shared task namely Artificial Intelligence for Legal\nAssistance (AILA 2019). This is an integral event of Forum for Information\nRetrieval Evaluation-2019. The outcomes of this track would be helpful for the\nautomation of the working process of the Indian Judiciary System. The manual\nworking procedures and documentation at any level (from lower to higher court)\nof the judiciary system are very complex in nature. The systems produced as a\npart of this track would assist the law practitioners. It would be helpful for\ncommon men too. This kind of track also opens the path of research of Natural\nLanguage Processing (NLP) in the judicial domain. This track defined two\nproblems such as Task 1: Identifying relevant prior cases for a given situation\nand Task 2: Identifying the most relevant statutes for a given situation. We\ntackled both of them. Our proposed approaches are based on BM25 and Doc2Vec. As\nper the results declared by the task organizers, we are in 3rd and a modest\nposition in Task 1 and Task 2 respectively.",
          "link": "http://arxiv.org/abs/2105.11347",
          "publishedOn": "2021-06-29T01:55:13.954Z",
          "wordCount": 669,
          "title": "IITP at AILA 2019: System Report for Artificial Intelligence for Legal Assistance Shared Task. (arXiv:2105.11347v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.12255",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Abheesht Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pandey_H/0/1/0/all/0/1\">Harshit Pandey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chhablani_G/0/1/0/all/0/1\">Gunjan Chhablani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhartia_Y/0/1/0/all/0/1\">Yash Bhartia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dash_T/0/1/0/all/0/1\">Tirtharaj Dash</a>",
          "description": "In this article, we present our methodologies for SemEval-2021 Task-4:\nReading Comprehension of Abstract Meaning. Given a fill-in-the-blank-type\nquestion and a corresponding context, the task is to predict the most suitable\nword from a list of 5 options. There are three sub-tasks within this task:\nImperceptibility (subtask-I), Non-Specificity (subtask-II), and Intersection\n(subtask-III). We use encoders of transformers-based models pre-trained on the\nmasked language modelling (MLM) task to build our Fill-in-the-blank (FitB)\nmodels. Moreover, to model imperceptibility, we define certain linguistic\nfeatures, and to model non-specificity, we leverage information from hypernyms\nand hyponyms provided by a lexical database. Specifically, for non-specificity,\nwe try out augmentation techniques, and other statistical techniques. We also\npropose variants, namely Chunk Voting and Max Context, to take care of input\nlength restrictions for BERT, etc. Additionally, we perform a thorough ablation\nstudy, and use Integrated Gradients to explain our predictions on a few\nsamples. Our best submissions achieve accuracies of 75.31% and 77.84%, on the\ntest sets for subtask-I and subtask-II, respectively. For subtask-III, we\nachieve accuracies of 65.64% and 62.27%.",
          "link": "http://arxiv.org/abs/2102.12255",
          "publishedOn": "2021-06-29T01:55:13.893Z",
          "wordCount": 666,
          "title": "LRG at SemEval-2021 Task 4: Improving Reading Comprehension with Abstract Words using Augmentation, Linguistic Features and Voting. (arXiv:2102.12255v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.01558",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Mingjiang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Chengli Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chunlin Chen</a>",
          "description": "Intelligent robots designed to interact with humans in real scenarios need to\nbe able to refer to entities actively by natural language. In spatial referring\nexpression generation, the ambiguity is unavoidable due to the diversity of\nreference frames, which will lead to an understanding gap between humans and\nrobots. To narrow this gap, in this paper, we propose a novel\nperspective-corrected spatial referring expression generation (PcSREG) approach\nfor human-robot interaction by considering the selection of reference frames.\nThe task of referring expression generation is simplified into the process of\ngenerating diverse spatial relation units. First, we pick out all landmarks in\nthese spatial relation units according to the entropy of preference and allow\nits updating through a stack model. Then all possible referring expressions are\ngenerated according to different reference frame strategies. Finally, we\nevaluate every expression using a probabilistic referring expression resolution\nmodel and find the best expression that satisfies both of the appropriateness\nand effectiveness. We implement the proposed approach on a robot system and\nempirical experiments show that our approach can generate more effective\nspatial referring expressions for practical applications.",
          "link": "http://arxiv.org/abs/2104.01558",
          "publishedOn": "2021-06-29T01:55:13.859Z",
          "wordCount": 651,
          "title": "Perspective-corrected Spatial Referring Expression Generation for Human-Robot Interaction. (arXiv:2104.01558v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.04512",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gallego_G/0/1/0/all/0/1\">Gerard I. G&#xe1;llego</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsiamas_I/0/1/0/all/0/1\">Ioannis Tsiamas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Escolano_C/0/1/0/all/0/1\">Carlos Escolano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fonollosa_J/0/1/0/all/0/1\">Jos&#xe9; A. R. Fonollosa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Costa_jussa_M/0/1/0/all/0/1\">Marta R. Costa-juss&#xe0;</a>",
          "description": "This paper describes the submission to the IWSLT 2021 offline speech\ntranslation task by the UPC Machine Translation group. The task consists of\nbuilding a system capable of translating English audio recordings extracted\nfrom TED talks into German text. Submitted systems can be either cascade or\nend-to-end and use a custom or given segmentation. Our submission is an\nend-to-end speech translation system, which combines pre-trained models\n(Wav2Vec 2.0 and mBART) with coupling modules between the encoder and decoder,\nand uses an efficient fine-tuning technique, which trains only 20% of its total\nparameters. We show that adding an Adapter to the system and pre-training it,\ncan increase the convergence speed and the final result, with which we achieve\na BLEU score of 27.3 on the MuST-C test set. Our final model is an ensemble\nthat obtains 28.22 BLEU score on the same set. Our submission also uses a\ncustom segmentation algorithm that employs pre-trained Wav2Vec 2.0 for\nidentifying periods of untranscribable text and can bring improvements of 2.5\nto 3 BLEU score on the IWSLT 2019 test set, as compared to the result with the\ngiven segmentation.",
          "link": "http://arxiv.org/abs/2105.04512",
          "publishedOn": "2021-06-29T01:55:13.784Z",
          "wordCount": 676,
          "title": "End-to-End Speech Translation with Pre-trained Models and Adapters: UPC at IWSLT 2021. (arXiv:2105.04512v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14464",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yilin Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_Y/0/1/0/all/0/1\">Yen-Chang Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ray_A/0/1/0/all/0/1\">Avik Ray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1\">Hongxia Jin</a>",
          "description": "Intent classification is a major task in spoken language understanding (SLU).\nSince most models are built with pre-collected in-domain (IND) training\nutterances, their ability to detect unsupported out-of-domain (OOD) utterances\nhas a critical effect in practical use. Recent works have shown that using\nextra data and labels can improve the OOD detection performance, yet it could\nbe costly to collect such data. This paper proposes to train a model with only\nIND data while supporting both IND intent classification and OOD detection. Our\nmethod designs a novel domain-regularized module (DRM) to reduce the\noverconfident phenomenon of a vanilla classifier, achieving a better\ngeneralization in both cases. Besides, DRM can be used as a drop-in replacement\nfor the last layer in any neural network-based intent classifier, providing a\nlow-cost strategy for a significant improvement. The evaluation on four\ndatasets shows that our method built on BERT and RoBERTa models achieves\nstate-of-the-art performance against existing approaches and the strong\nbaselines we created for the comparisons.",
          "link": "http://arxiv.org/abs/2106.14464",
          "publishedOn": "2021-06-29T01:55:13.709Z",
          "wordCount": 604,
          "title": "Enhancing the Generalization for Intent Classification and Out-of-Domain Detection in SLU. (arXiv:2106.14464v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14371",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Q/0/1/0/all/0/1\">Qingjian Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Lin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xuyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1\">Luyuan Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_C/0/1/0/all/0/1\">Chen Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Junjie Wang</a>",
          "description": "Target speech separation is the process of filtering a certain speaker's\nvoice out of speech mixtures according to the additional speaker identity\ninformation provided. Recent works have made considerable improvement by\nprocessing signals in the time domain directly. The majority of them take fully\noverlapped speech mixtures for training. However, since most real-life\nconversations occur randomly and are sparsely overlapped, we argue that\ntraining with different overlap ratio data benefits. To do so, an unavoidable\nproblem is that the popularly used SI-SNR loss has no definition for silent\nsources. This paper proposes the weighted SI-SNR loss, together with the joint\nlearning of target speech separation and personal VAD. The weighted SI-SNR loss\nimposes a weight factor that is proportional to the target speaker's duration\nand returns zero when the target speaker is absent. Meanwhile, the personal VAD\ngenerates masks and sets non-target speech to silence. Experiments show that\nour proposed method outperforms the baseline by 1.73 dB in terms of SDR on\nfully overlapped speech, as well as by 4.17 dB and 0.9 dB on sparsely\noverlapped speech of clean and noisy conditions. Besides, with slight\ndegradation in performance, our model could reduce the time costs in inference.",
          "link": "http://arxiv.org/abs/2106.14371",
          "publishedOn": "2021-06-29T01:55:13.536Z",
          "wordCount": 672,
          "title": "Sparsely Overlapped Speech Training in the Time Domain: Joint Learning of Target Speech Separation and Personal VAD Benefits. (arXiv:2106.14371v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14463",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Saahil Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_A/0/1/0/all/0/1\">Ashwin Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saporta_A/0/1/0/all/0/1\">Adriel Saporta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Truong_S/0/1/0/all/0/1\">Steven QH Truong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duong_D/0/1/0/all/0/1\">Du Nguyen Duong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1\">Tan Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chambon_P/0/1/0/all/0/1\">Pierre Chambon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lungren_M/0/1/0/all/0/1\">Matthew P. Lungren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_A/0/1/0/all/0/1\">Andrew Y. Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Langlotz_C/0/1/0/all/0/1\">Curtis P. Langlotz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajpurkar_P/0/1/0/all/0/1\">Pranav Rajpurkar</a>",
          "description": "Extracting structured clinical information from free-text radiology reports\ncan enable the use of radiology report information for a variety of critical\nhealthcare applications. In our work, we present RadGraph, a dataset of\nentities and relations in full-text chest X-ray radiology reports based on a\nnovel information extraction schema we designed to structure radiology reports.\nWe release a development dataset, which contains board-certified radiologist\nannotations for 500 radiology reports from the MIMIC-CXR dataset (14,579\nentities and 10,889 relations), and a test dataset, which contains two\nindependent sets of board-certified radiologist annotations for 100 radiology\nreports split equally across the MIMIC-CXR and CheXpert datasets. Using these\ndatasets, we train and test a deep learning model, RadGraph Benchmark, that\nachieves a micro F1 of 0.82 and 0.73 on relation extraction on the MIMIC-CXR\nand CheXpert test sets respectively. Additionally, we release an inference\ndataset, which contains annotations automatically generated by RadGraph\nBenchmark across 220,763 MIMIC-CXR reports (around 6 million entities and 4\nmillion relations) and 500 CheXpert reports (13,783 entities and 9,908\nrelations) with mappings to associated chest radiographs. Our freely available\ndataset can facilitate a wide range of research in medical natural language\nprocessing, as well as computer vision and multi-modal learning when linked to\nchest radiographs.",
          "link": "http://arxiv.org/abs/2106.14463",
          "publishedOn": "2021-06-29T01:55:13.456Z",
          "wordCount": 674,
          "title": "RadGraph: Extracting Clinical Entities and Relations from Radiology Reports. (arXiv:2106.14463v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2012.15828",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenhui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_H/0/1/0/all/0/1\">Hangbo Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shaohan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1\">Li Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>",
          "description": "We generalize deep self-attention distillation in MiniLM (Wang et al., 2020)\nby only using self-attention relation distillation for task-agnostic\ncompression of pretrained Transformers. In particular, we define multi-head\nself-attention relations as scaled dot-product between the pairs of query, key,\nand value vectors within each self-attention module. Then we employ the above\nrelational knowledge to train the student model. Besides its simplicity and\nunified principle, more favorably, there is no restriction in terms of the\nnumber of student's attention heads, while most previous work has to guarantee\nthe same head number between teacher and student. Moreover, the fine-grained\nself-attention relations tend to fully exploit the interaction knowledge\nlearned by Transformer. In addition, we thoroughly examine the layer selection\nstrategy for teacher models, rather than just relying on the last layer as in\nMiniLM. We conduct extensive experiments on compressing both monolingual and\nmultilingual pretrained models. Experimental results demonstrate that our\nmodels distilled from base-size and large-size teachers (BERT, RoBERTa and\nXLM-R) outperform the state-of-the-art.",
          "link": "http://arxiv.org/abs/2012.15828",
          "publishedOn": "2021-06-29T01:55:13.441Z",
          "wordCount": 638,
          "title": "MiniLMv2: Multi-Head Self-Attention Relation Distillation for Compressing Pretrained Transformers. (arXiv:2012.15828v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14361",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dasgupta_S/0/1/0/all/0/1\">Shib Sankar Dasgupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boratko_M/0/1/0/all/0/1\">Michael Boratko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atmakuri_S/0/1/0/all/0/1\">Shriya Atmakuri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Lorraine Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_D/0/1/0/all/0/1\">Dhruvesh Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCallum_A/0/1/0/all/0/1\">Andrew McCallum</a>",
          "description": "Learning vector representations for words is one of the most fundamental\ntopics in NLP, capable of capturing syntactic and semantic relationships useful\nin a variety of downstream NLP tasks. Vector representations can be limiting,\nhowever, in that typical scoring such as dot product similarity intertwines\nposition and magnitude of the vector in space. Exciting innovations in the\nspace of representation learning have proposed alternative fundamental\nrepresentations, such as distributions, hyperbolic vectors, or regions. Our\nmodel, Word2Box, takes a region-based approach to the problem of word\nrepresentation, representing words as $n$-dimensional rectangles. These\nrepresentations encode position and breadth independently and provide\nadditional geometric operations such as intersection and containment which\nallow them to model co-occurrence patterns vectors struggle with. We\ndemonstrate improved performance on various word similarity tasks, particularly\non less common words, and perform a qualitative analysis exploring the\nadditional unique expressivity provided by Word2Box.",
          "link": "http://arxiv.org/abs/2106.14361",
          "publishedOn": "2021-06-29T01:55:13.434Z",
          "wordCount": 589,
          "title": "Word2Box: Learning Word Representation Using Box Embeddings. (arXiv:2106.14361v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14131",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Valipour_M/0/1/0/all/0/1\">Mojtaba Valipour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_B/0/1/0/all/0/1\">Bowen You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panju_M/0/1/0/all/0/1\">Maysum Panju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghodsi_A/0/1/0/all/0/1\">Ali Ghodsi</a>",
          "description": "Symbolic regression is the task of identifying a mathematical expression that\nbest fits a provided dataset of input and output values. Due to the richness of\nthe space of mathematical expressions, symbolic regression is generally a\nchallenging problem. While conventional approaches based on genetic evolution\nalgorithms have been used for decades, deep learning-based methods are\nrelatively new and an active research area. In this work, we present\nSymbolicGPT, a novel transformer-based language model for symbolic regression.\nThis model exploits the advantages of probabilistic language models like GPT,\nincluding strength in performance and flexibility. Through comprehensive\nexperiments, we show that our model performs strongly compared to competing\nmodels with respect to the accuracy, running time, and data efficiency.",
          "link": "http://arxiv.org/abs/2106.14131",
          "publishedOn": "2021-06-29T01:55:13.427Z",
          "wordCount": 560,
          "title": "SymbolicGPT: A Generative Transformer Model for Symbolic Regression. (arXiv:2106.14131v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14332",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huber_J/0/1/0/all/0/1\">Joseph Huber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_W/0/1/0/all/0/1\">Weile Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Georgakoudis_G/0/1/0/all/0/1\">Giorgis Georgakoudis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doerfert_J/0/1/0/all/0/1\">Johannes Doerfert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hernandez_O/0/1/0/all/0/1\">Oscar Hernandez</a>",
          "description": "This paper presents a methodology for using LLVM-based tools to tune the\nDCA++ (dynamical clusterapproximation) application that targets the new ARM\nA64FX processor. The goal is to describethe changes required for the new\narchitecture and generate efficient single instruction/multiple data(SIMD)\ninstructions that target the new Scalable Vector Extension instruction set.\nDuring manualtuning, the authors used the LLVM tools to improve code\nparallelization by using OpenMP SIMD,refactored the code and applied\ntransformation that enabled SIMD optimizations, and ensured thatthe correct\nlibraries were used to achieve optimal performance. By applying these code\nchanges, codespeed was increased by 1.98X and 78 GFlops were achieved on the\nA64FX processor. The authorsaim to automatize parts of the efforts in the\nOpenMP Advisor tool, which is built on top of existingand newly introduced LLVM\ntooling.",
          "link": "http://arxiv.org/abs/2106.14332",
          "publishedOn": "2021-06-29T01:55:13.415Z",
          "wordCount": 590,
          "title": "A Case Study of LLVM-Based Analysis for Optimizing SIMD Code Generation. (arXiv:2106.14332v1 [cs.DC])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14434",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kotelnikov_E/0/1/0/all/0/1\">Evgeny Kotelnikov</a>",
          "description": "Currently, there are more than a dozen Russian-language corpora for sentiment\nanalysis, differing in the source of the texts, domain, size, number and ratio\nof sentiment classes, and annotation method. This work examines publicly\navailable Russian-language corpora, presents their qualitative and quantitative\ncharacteristics, which make it possible to get an idea of the current landscape\nof the corpora for sentiment analysis. The ranking of corpora by annotation\nquality is proposed, which can be useful when choosing corpora for training and\ntesting. The influence of the training dataset on the performance of sentiment\nanalysis is investigated based on the use of the deep neural network model\nBERT. The experiments with review corpora allow us to conclude that on average\nthe quality of models increases with an increase in the number of training\ncorpora. For the first time, quality scores were obtained for the corpus of\nreviews of ROMIP seminars based on the BERT model. Also, the study proposes the\ntask of the building a universal model for sentiment analysis.",
          "link": "http://arxiv.org/abs/2106.14434",
          "publishedOn": "2021-06-29T01:55:13.399Z",
          "wordCount": 601,
          "title": "Current Landscape of the Russian Sentiment Corpora. (arXiv:2106.14434v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13884",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tsimpoukelli_M/0/1/0/all/0/1\">Maria Tsimpoukelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menick_J/0/1/0/all/0/1\">Jacob Menick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cabi_S/0/1/0/all/0/1\">Serkan Cabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eslami_S/0/1/0/all/0/1\">S.M. Ali Eslami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vinyals_O/0/1/0/all/0/1\">Oriol Vinyals</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hill_F/0/1/0/all/0/1\">Felix Hill</a>",
          "description": "When trained at sufficient scale, auto-regressive language models exhibit the\nnotable ability to learn a new language task after being prompted with just a\nfew examples. Here, we present a simple, yet effective, approach for\ntransferring this few-shot learning ability to a multimodal setting (vision and\nlanguage). Using aligned image and caption data, we train a vision encoder to\nrepresent each image as a sequence of continuous embeddings, such that a\npre-trained, frozen language model prompted with this prefix generates the\nappropriate caption. The resulting system is a multimodal few-shot learner,\nwith the surprising ability to learn a variety of new tasks when conditioned on\nexamples, represented as a sequence of multiple interleaved image and text\nembeddings. We demonstrate that it can rapidly learn words for new objects and\nnovel visual categories, do visual question-answering with only a handful of\nexamples, and make use of outside knowledge, by measuring a single model on a\nvariety of established and new benchmarks.",
          "link": "http://arxiv.org/abs/2106.13884",
          "publishedOn": "2021-06-29T01:55:13.393Z",
          "wordCount": 608,
          "title": "Multimodal Few-Shot Learning with Frozen Language Models. (arXiv:2106.13884v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13858",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Ruiyang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Ayush Singh</a>",
          "description": "Natural interface to database (NLIDB) has been researched a lot during the\npast decades. In the core of NLIDB, is a semantic parser used to convert\nnatural language into SQL. Solutions from traditional NLP methodology focuses\non grammar rule pattern learning and pairing via intermediate logic forms.\nAlthough those methods give an acceptable performance on certain specific\ndatabase and parsing tasks, they are hard to generalize and scale. On the other\nhand, recent progress in neural deep learning seems to provide a promising\ndirection towards building a general NLIDB system. Unlike the traditional\napproach, those neural methodologies treat the parsing problem as a\nsequence-to-sequence learning problem. In this paper, we experimented on\nseveral sequence-to-sequence learning models and evaluate their performance on\ngeneral database parsing task.",
          "link": "http://arxiv.org/abs/2106.13858",
          "publishedOn": "2021-06-29T01:55:13.386Z",
          "wordCount": 560,
          "title": "Semantic Parsing Natural Language into Relational Algebra. (arXiv:2106.13858v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13822",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hasan_T/0/1/0/all/0/1\">Tahmid Hasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharjee_A/0/1/0/all/0/1\">Abhik Bhattacharjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1\">Md Saiful Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samin_K/0/1/0/all/0/1\">Kazi Samin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuan-Fang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1\">Yong-Bin Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1\">M. Sohel Rahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shahriyar_R/0/1/0/all/0/1\">Rifat Shahriyar</a>",
          "description": "Contemporary works on abstractive text summarization have focused primarily\non high-resource languages like English, mostly due to the limited availability\nof datasets for low/mid-resource ones. In this work, we present XL-Sum, a\ncomprehensive and diverse dataset comprising 1 million professionally annotated\narticle-summary pairs from BBC, extracted using a set of carefully designed\nheuristics. The dataset covers 44 languages ranging from low to high-resource,\nfor many of which no public dataset is currently available. XL-Sum is highly\nabstractive, concise, and of high quality, as indicated by human and intrinsic\nevaluation. We fine-tune mT5, a state-of-the-art pretrained multilingual model,\nwith XL-Sum and experiment on multilingual and low-resource summarization\ntasks. XL-Sum induces competitive results compared to the ones obtained using\nsimilar monolingual datasets: we show higher than 11 ROUGE-2 scores on 10\nlanguages we benchmark on, with some of them exceeding 15, as obtained by\nmultilingual training. Additionally, training on low-resource languages\nindividually also provides competitive performance. To the best of our\nknowledge, XL-Sum is the largest abstractive summarization dataset in terms of\nthe number of samples collected from a single source and the number of\nlanguages covered. We are releasing our dataset and models to encourage future\nresearch on multilingual abstractive summarization. The resources can be found\nat \\url{https://github.com/csebuetnlp/xl-sum}.",
          "link": "http://arxiv.org/abs/2106.13822",
          "publishedOn": "2021-06-29T01:55:13.378Z",
          "wordCount": 662,
          "title": "XL-Sum: Large-Scale Multilingual Abstractive Summarization for 44 Languages. (arXiv:2106.13822v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14165",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rahimi_Z/0/1/0/all/0/1\">Zeinab Rahimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+ShamsFard_M/0/1/0/all/0/1\">Mehrnoush ShamsFard</a>",
          "description": "Recognizing causal elements and causal relations in text is one of the\nchallenging issues in natural language processing; specifically, in low\nresource languages such as Persian. In this research we prepare a causality\nhuman annotated corpus for the Persian language which consists of 4446\nsentences and 5128 causal relations and three labels of cause, effect and\ncausal mark -- if possibl -- are specified for each relation. We have used this\ncorpus to train a system for detecting causal elements boundaries. Also, we\npresent a causality detection benchmark for three machine learning methods and\ntwo deep learning systems based on this corpus. Performance evaluations\nindicate that our best total result is obtained through CRF classifier which\nhas F-measure of 0.76 and the best accuracy obtained through Bi-LSTM-CRF deep\nlearning method with Accuracy equal to %91.4.",
          "link": "http://arxiv.org/abs/2106.14165",
          "publishedOn": "2021-06-29T01:55:13.370Z",
          "wordCount": 575,
          "title": "Persian Causality Corpus (PerCause) and the Causality Detection Benchmark. (arXiv:2106.14165v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14438",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fishcheva_I/0/1/0/all/0/1\">Irina Fishcheva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goloviznina_V/0/1/0/all/0/1\">Valeriya Goloviznina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kotelnikov_E/0/1/0/all/0/1\">Evgeny Kotelnikov</a>",
          "description": "Argumentation mining is a field of computational linguistics that is devoted\nto extracting from texts and classifying arguments and relations between them,\nas well as constructing an argumentative structure. A significant obstacle to\nresearch in this area for the Russian language is the lack of annotated\nRussian-language text corpora. This article explores the possibility of\nimproving the quality of argumentation mining using the extension of the\nRussian-language version of the Argumentative Microtext Corpus (ArgMicro) based\non the machine translation of the Persuasive Essays Corpus (PersEssays). To\nmake it possible to use these two corpora combined, we propose a Joint Argument\nAnnotation Scheme based on the schemes used in ArgMicro and PersEssays. We\nsolve the problem of classifying argumentative discourse units (ADUs) into two\nclasses - \"pro\" (\"for\") and \"opp\" (\"against\") using traditional machine\nlearning techniques (SVM, Bagging and XGBoost) and a deep neural network (BERT\nmodel). An ensemble of XGBoost and BERT models was proposed, which showed the\nhighest performance of ADUs classification for both corpora.",
          "link": "http://arxiv.org/abs/2106.14438",
          "publishedOn": "2021-06-29T01:55:13.355Z",
          "wordCount": 615,
          "title": "Traditional Machine Learning and Deep Learning Models for Argumentation Mining in Russian Texts. (arXiv:2106.14438v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13945",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Piji Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+King_I/0/1/0/all/0/1\">Irwin King</a>",
          "description": "In recent years, reference-based and supervised summarization evaluation\nmetrics have been widely explored. However, collecting human-annotated\nreferences and ratings are costly and time-consuming. To avoid these\nlimitations, we propose a training-free and reference-free summarization\nevaluation metric. Our metric consists of a centrality-weighted relevance score\nand a self-referenced redundancy score. The relevance score is computed between\nthe pseudo reference built from the source document and the given summary,\nwhere the pseudo reference content is weighted by the sentence centrality to\nprovide importance guidance. Besides an $F_1$-based relevance score, we also\ndesign an $F_\\beta$-based variant that pays more attention to the recall score.\nAs for the redundancy score of the summary, we compute a self-masked similarity\nscore with the summary itself to evaluate the redundant information in the\nsummary. Finally, we combine the relevance and redundancy scores to produce the\nfinal evaluation score of the given summary. Extensive experiments show that\nour methods can significantly outperform existing methods on both\nmulti-document and single-document summarization evaluation.",
          "link": "http://arxiv.org/abs/2106.13945",
          "publishedOn": "2021-06-29T01:55:13.349Z",
          "wordCount": 607,
          "title": "A Training-free and Reference-free Summarization Evaluation Metric via Centrality-weighted Relevance and Self-referenced Redundancy. (arXiv:2106.13945v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14233",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Saeedizade_M/0/1/0/all/0/1\">Mohammad Javad Saeedizade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torabian_N/0/1/0/all/0/1\">Najmeh Torabian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Minaei_Bidgoli_B/0/1/0/all/0/1\">Behrouz Minaei-Bidgoli</a>",
          "description": "Link prediction is the task of predicting missing relations between entities\nof the knowledge graph by inferring from the facts contained in it. Recent work\nin link prediction has attempted to provide a model for increasing link\nprediction accuracy by using more layers in neural network architecture or\nmethods that add to the computational complexity of models. This paper we\nproposed a method for refining the knowledge graph, which makes the knowledge\ngraph more informative, and link prediction operations can be performed more\naccurately using relatively fast translational models. Translational link\nprediction models, such as TransE, TransH, TransD, etc., have much less\ncomplexity than deep learning approaches. This method uses the hierarchy of\nrelationships and also the hierarchy of entities in the knowledge graph to add\nthe entity information as a new entity to the graph and connect it to the nodes\nwhich contain this information in their hierarchy. Our experiments show that\nour method can significantly increase the performance of translational link\nprediction methods in H@10, MR, MRR.",
          "link": "http://arxiv.org/abs/2106.14233",
          "publishedOn": "2021-06-29T01:55:13.343Z",
          "wordCount": 614,
          "title": "KGRefiner: Knowledge Graph Refinement for Improving Accuracy of Translational Link Prediction Methods. (arXiv:2106.14233v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14127",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ge_S/0/1/0/all/0/1\">Songwei Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parikh_D/0/1/0/all/0/1\">Devi Parikh</a>",
          "description": "We ask the question: to what extent can recent large-scale language and image\ngeneration models blend visual concepts? Given an arbitrary object, we identify\na relevant object and generate a single-sentence description of the blend of\nthe two using a language model. We then generate a visual depiction of the\nblend using a text-based image generation model. Quantitative and qualitative\nevaluations demonstrate the superiority of language models over classical\nmethods for conceptual blending, and of recent large-scale image generation\nmodels over prior models for the visual depiction.",
          "link": "http://arxiv.org/abs/2106.14127",
          "publishedOn": "2021-06-29T01:55:13.334Z",
          "wordCount": 527,
          "title": "Visual Conceptual Blending with Large-scale Language and Vision Models. (arXiv:2106.14127v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13973",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Basu_P/0/1/0/all/0/1\">Priyam Basu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_T/0/1/0/all/0/1\">Tiasa Singha Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naidu_R/0/1/0/all/0/1\">Rakshit Naidu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muftuoglu_Z/0/1/0/all/0/1\">Zumrut Muftuoglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sahib Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mireshghallah_F/0/1/0/all/0/1\">Fatemehsadat Mireshghallah</a>",
          "description": "Natural Language Processing (NLP) techniques can be applied to help with the\ndiagnosis of medical conditions such as depression, using a collection of a\nperson's utterances. Depression is a serious medical illness that can have\nadverse effects on how one feels, thinks, and acts, which can lead to emotional\nand physical problems. Due to the sensitive nature of such data, privacy\nmeasures need to be taken for handling and training models with such data. In\nthis work, we study the effects that the application of Differential Privacy\n(DP) has, in both a centralized and a Federated Learning (FL) setup, on\ntraining contextualized language models (BERT, ALBERT, RoBERTa and DistilBERT).\nWe offer insights on how to privately train NLP models and what architectures\nand setups provide more desirable privacy utility trade-offs. We envisage this\nwork to be used in future healthcare and mental health studies to keep medical\nhistory private. Therefore, we provide an open-source implementation of this\nwork.",
          "link": "http://arxiv.org/abs/2106.13973",
          "publishedOn": "2021-06-29T01:55:13.326Z",
          "wordCount": 612,
          "title": "Benchmarking Differential Privacy and Federated Learning for BERT Models. (arXiv:2106.13973v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14157",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kuniyoshi_F/0/1/0/all/0/1\">Fusataka Kuniyoshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ozawa_J/0/1/0/all/0/1\">Jun Ozawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miwa_M/0/1/0/all/0/1\">Makoto Miwa</a>",
          "description": "In the field of inorganic materials science, there is a growing demand to\nextract knowledge such as physical properties and synthesis processes of\nmaterials by machine-reading a large number of papers. This is because\nmaterials researchers refer to many papers in order to come up with promising\nterms of experiments for material synthesis. However, there are only a few\nsystems that can extract material names and their properties. This study\nproposes a large-scale natural language processing (NLP) pipeline for\nextracting material names and properties from materials science literature to\nenable the search and retrieval of results in materials science. Therefore, we\npropose a label definition for extracting material names and properties and\naccordingly build a corpus containing 836 annotated paragraphs extracted from\n301 papers for training a named entity recognition (NER) model. Experimental\nresults demonstrate the utility of this NER model; it achieves successful\nextraction with a micro-F1 score of 78.1%. To demonstrate the efficacy of our\napproach, we present a thorough evaluation on a real-world automatically\nannotated corpus by applying our trained NER model to 12,895 materials science\npapers. We analyze the trend in materials science by visualizing the outputs of\nthe NLP pipeline. For example, the country-by-year analysis indicates that in\nrecent years, the number of papers on \"MoS2,\" a material used in perovskite\nsolar cells, has been increasing rapidly in China but decreasing in the United\nStates. Further, according to the conditions-by-year analysis, the processing\ntemperature of the catalyst material \"PEDOT:PSS\" is shifting below 200 degree,\nand the number of reports with a processing time exceeding 5 h is increasing\nslightly.",
          "link": "http://arxiv.org/abs/2106.14157",
          "publishedOn": "2021-06-29T01:55:13.309Z",
          "wordCount": 700,
          "title": "Analyzing Research Trends in Inorganic Materials Literature Using NLP. (arXiv:2106.14157v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13833",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shahmohammadi_S/0/1/0/all/0/1\">Sara Shahmohammadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Veisi_H/0/1/0/all/0/1\">Hadi Veisi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darzi_A/0/1/0/all/0/1\">Ali Darzi</a>",
          "description": "Over the past years, interest in discourse analysis and discourse parsing has\nsteadily grown, and many discourse-annotated corpora and, as a result,\ndiscourse parsers have been built. In this paper, we present a\ndiscourse-annotated corpus for the Persian language built in the framework of\nRhetorical Structure Theory as well as a discourse parser built upon the DPLP\nparser, an open-source discourse parser. Our corpus consists of 150\njournalistic texts, each text having an average of around 400 words. Corpus\ntexts were annotated using 18 discourse relations and based on the annotation\nguideline of the English RST Discourse Treebank corpus. Our text-level\ndiscourse parser is trained using gold segmentation and is built upon the DPLP\ndiscourse parser, which uses a large-margin transition-based approach to solve\nthe problem of discourse parsing. The performance of our discourse parser in\nspan (S), nuclearity (N) and relation (R) detection is around 78%, 64%, 44%\nrespectively, in terms of F1 measure.",
          "link": "http://arxiv.org/abs/2106.13833",
          "publishedOn": "2021-06-29T01:55:13.303Z",
          "wordCount": 580,
          "title": "Persian Rhetorical Structure Theory. (arXiv:2106.13833v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14387",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sinno_B/0/1/0/all/0/1\">Barea Sinno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oviedo_B/0/1/0/all/0/1\">Bernardo Oviedo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atwell_K/0/1/0/all/0/1\">Katherine Atwell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alikhani_M/0/1/0/all/0/1\">Malihe Alikhani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junyi Jessy Li</a>",
          "description": "Analyzing political ideology and polarization is of critical importance in\nadvancing our understanding of the political context in society. Recent\nresearch has made great strides towards understanding the ideological bias\n(i.e., stance) of news media along a left-right spectrum. In this work, we take\na novel approach and study the ideology of the policy under discussion teasing\napart the nuanced co-existence of stance and ideology. Aligned with the\ntheoretical accounts in political science, we treat ideology as a\nmulti-dimensional construct, and introduce the first diachronic dataset of news\narticles whose political ideology under discussion is annotated by trained\npolitical scientists and linguists at the paragraph-level. We showcase that\nthis framework enables quantitative analysis of polarization, a temporal,\nmultifaceted measure of ideological distance. We further present baseline\nmodels for ideology prediction.",
          "link": "http://arxiv.org/abs/2106.14387",
          "publishedOn": "2021-06-29T01:55:13.297Z",
          "wordCount": 574,
          "title": "Political Ideology and Polarization of Policy Positions: A Multi-dimensional Approach. (arXiv:2106.14387v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14163",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lianbo Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1\">Huimin Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiliang Zhang</a>",
          "description": "Extracting relational triples from texts is a fundamental task in knowledge\ngraph construction. The popular way of existing methods is to jointly extract\nentities and relations using a single model, which often suffers from the\noverlapping triple problem. That is, there are multiple relational triples that\nshare the same entities within one sentence. In this work, we propose an\neffective cascade dual-decoder approach to extract overlapping relational\ntriples, which includes a text-specific relation decoder and a\nrelation-corresponded entity decoder. Our approach is straightforward: the\ntext-specific relation decoder detects relations from a sentence according to\nits text semantics and treats them as extra features to guide the entity\nextraction; for each extracted relation, which is with trainable embedding, the\nrelation-corresponded entity decoder detects the corresponding head and tail\nentities using a span-based tagging scheme. In this way, the overlapping triple\nproblem is tackled naturally. Experiments on two public datasets demonstrate\nthat our proposed approach outperforms state-of-the-art methods and achieves\nbetter F1 scores under the strict evaluation metric. Our implementation is\navailable at https://github.com/prastunlp/DualDec.",
          "link": "http://arxiv.org/abs/2106.14163",
          "publishedOn": "2021-06-29T01:55:13.290Z",
          "wordCount": 612,
          "title": "Effective Cascade Dual-Decoder Model for Joint Entity and Relation Extraction. (arXiv:2106.14163v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14103",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ren_P/0/1/0/all/0/1\">Pu Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_C/0/1/0/all/0/1\">Chengping Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianxun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Hao Sun</a>",
          "description": "Partial differential equations (PDEs) play a fundamental role in modeling and\nsimulating problems across a wide range of disciplines. Recent advances in deep\nlearning have shown the great potential of physics-informed neural networks\n(PINNs) to solve PDEs as a basis for data-driven modeling and inverse analysis.\nHowever, the majority of existing PINN methods, based on fully-connected NNs,\npose intrinsic limitations to low-dimensional spatiotemporal parameterizations.\nMoreover, since the initial/boundary conditions (I/BCs) are softly imposed via\npenalty, the solution quality heavily relies on hyperparameter tuning. To this\nend, we propose the novel physics-informed convolutional-recurrent learning\narchitectures (PhyCRNet and PhyCRNet-s) for solving PDEs without any labeled\ndata. Specifically, an encoder-decoder convolutional long short-term memory\nnetwork is proposed for low-dimensional spatial feature extraction and temporal\nevolution learning. The loss function is defined as the aggregated discretized\nPDE residuals, while the I/BCs are hard-encoded in the network to ensure\nforcible satisfaction (e.g., periodic boundary padding). The networks are\nfurther enhanced by autoregressive and residual connections that explicitly\nsimulate time marching. The performance of our proposed methods has been\nassessed by solving three nonlinear PDEs (e.g., 2D Burgers' equations, the\n$\\lambda$-$\\omega$ and FitzHugh Nagumo reaction-diffusion equations), and\ncompared against the start-of-the-art baseline algorithms. The numerical\nresults demonstrate the superiority of our proposed methodology in the context\nof solution accuracy, extrapolability and generalizability.",
          "link": "http://arxiv.org/abs/2106.14103",
          "publishedOn": "2021-06-29T01:55:13.282Z",
          "wordCount": 662,
          "title": "PhyCRNet: Physics-informed Convolutional-Recurrent Network for Solving Spatiotemporal PDEs. (arXiv:2106.14103v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14433",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mao_M/0/1/0/all/0/1\">Min Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiasheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jingyao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Haipang Wu</a>",
          "description": "This paper describes our approach to DSTC 9 Track 2: Cross-lingual\nMulti-domain Dialog State Tracking, the task goal is to build a Cross-lingual\ndialog state tracker with a training set in rich resource language and a\ntesting set in low resource language. We formulate a method for joint learning\nof slot operation classification task and state tracking task respectively.\nFurthermore, we design a novel mask mechanism for fusing contextual information\nabout dialogue, the results show the proposed model achieves excellent\nperformance on DSTC Challenge II with a joint accuracy of 62.37% and 23.96% in\nMultiWOZ(en - zh) dataset and CrossWOZ(zh - en) dataset, respectively.",
          "link": "http://arxiv.org/abs/2106.14433",
          "publishedOn": "2021-06-29T01:55:13.250Z",
          "wordCount": 544,
          "title": "Efficient Dialogue State Tracking by Masked Hierarchical Transformer. (arXiv:2106.14433v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13948",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Francis_J/0/1/0/all/0/1\">Jonathan Francis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kitamura_N/0/1/0/all/0/1\">Nariaki Kitamura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Labelle_F/0/1/0/all/0/1\">Felix Labelle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xiaopeng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navarro_I/0/1/0/all/0/1\">Ingrid Navarro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1\">Jean Oh</a>",
          "description": "Recent advances in the areas of multimodal machine learning and artificial\nintelligence (AI) have led to the development of challenging tasks at the\nintersection of Computer Vision, Natural Language Processing, and Embodied AI.\nWhereas many approaches and previous survey pursuits have characterised one or\ntwo of these dimensions, there has not been a holistic analysis at the center\nof all three. Moreover, even when combinations of these topics are considered,\nmore focus is placed on describing, e.g., current architectural methods, as\nopposed to also illustrating high-level challenges and opportunities for the\nfield. In this survey paper, we discuss Embodied Vision-Language Planning\n(EVLP) tasks, a family of prominent embodied navigation and manipulation\nproblems that jointly use computer vision and natural language. We propose a\ntaxonomy to unify these tasks and provide an in-depth analysis and comparison\nof the new and current algorithmic approaches, metrics, simulated environments,\nas well as the datasets used for EVLP tasks. Finally, we present the core\nchallenges that we believe new EVLP works should seek to address, and we\nadvocate for task construction that enables model generalizability and furthers\nreal-world deployment.",
          "link": "http://arxiv.org/abs/2106.13948",
          "publishedOn": "2021-06-29T01:55:13.244Z",
          "wordCount": 628,
          "title": "Core Challenges in Embodied Vision-Language Planning. (arXiv:2106.13948v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13876",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Majumder_B/0/1/0/all/0/1\">Bodhisattwa Prasad Majumder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camburu_O/0/1/0/all/0/1\">Oana-Maria Camburu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lukasiewicz_T/0/1/0/all/0/1\">Thomas Lukasiewicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McAuley_J/0/1/0/all/0/1\">Julian McAuley</a>",
          "description": "Explainable machine learning models primarily justify predicted labels using\neither extractive rationales (i.e., subsets of input features) or free-text\nnatural language explanations (NLEs) as abstractive justifications. While NLEs\ncan be more comprehensive than extractive rationales, machine-generated NLEs\nhave been shown to sometimes lack commonsense knowledge. Here, we show that\ncommonsense knowledge can act as a bridge between extractive rationales and\nNLEs, rendering both types of explanations better. More precisely, we introduce\na unified framework, called RExC (Rationale-Inspired Explanations with\nCommonsense), that (1) extracts rationales as a set of features responsible for\nmachine predictions, (2) expands the extractive rationales using available\ncommonsense resources, and (3) uses the expanded knowledge to generate natural\nlanguage explanations. Our framework surpasses by a large margin the previous\nstate-of-the-art in generating NLEs across five tasks in both natural language\nprocessing and vision-language understanding, with human annotators\nconsistently rating the explanations generated by RExC to be more\ncomprehensive, grounded in commonsense, and overall preferred compared to\nprevious state-of-the-art models. Moreover, our work shows that\ncommonsense-grounded explanations can enhance both task performance and\nrationales extraction capabilities.",
          "link": "http://arxiv.org/abs/2106.13876",
          "publishedOn": "2021-06-29T01:55:13.210Z",
          "wordCount": 615,
          "title": "Rationale-Inspired Natural Language Explanations with Commonsense. (arXiv:2106.13876v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14019",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hwanhee Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1\">Seunghyun Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dernoncourt_F/0/1/0/all/0/1\">Franck Dernoncourt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1\">Trung Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_K/0/1/0/all/0/1\">Kyomin Jung</a>",
          "description": "Despite the success of various text generation metrics such as BERTScore, it\nis still difficult to evaluate the image captions without enough reference\ncaptions due to the diversity of the descriptions. In this paper, we introduce\na new metric UMIC, an Unreferenced Metric for Image Captioning which does not\nrequire reference captions to evaluate image captions. Based on\nVision-and-Language BERT, we train UMIC to discriminate negative captions via\ncontrastive learning. Also, we observe critical problems of the previous\nbenchmark dataset (i.e., human annotations) on image captioning metric, and\nintroduce a new collection of human annotations on the generated captions. We\nvalidate UMIC on four datasets, including our new dataset, and show that UMIC\nhas a higher correlation than all previous metrics that require multiple\nreferences. We release the benchmark dataset and pre-trained models to compute\nthe UMIC.",
          "link": "http://arxiv.org/abs/2106.14019",
          "publishedOn": "2021-06-29T01:55:13.194Z",
          "wordCount": 585,
          "title": "UMIC: An Unreferenced Metric for Image Captioning via Contrastive Learning. (arXiv:2106.14019v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14167",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Etezadi_R/0/1/0/all/0/1\">Romina Etezadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shamsfard_M/0/1/0/all/0/1\">Mehrnoush Shamsfard</a>",
          "description": "Question answering systems may find the answers to users' questions from\neither unstructured texts or structured data such as knowledge graphs.\nAnswering questions using supervised learning approaches including deep\nlearning models need large training datasets. In recent years, some datasets\nhave been presented for the task of Question answering over knowledge graphs,\nwhich is the focus of this paper. Although many datasets in English were\nproposed, there have been a few question-answering datasets in Persian. This\npaper introduces \\textit{PeCoQ}, a dataset for Persian question answering. This\ndataset contains 10,000 complex questions and answers extracted from the\nPersian knowledge graph, FarsBase. For each question, the SPARQL query and two\nparaphrases that were written by linguists are provided as well. There are\ndifferent types of complexities in the dataset, such as multi-relation,\nmulti-entity, ordinal, and temporal constraints. In this paper, we discuss the\ndataset's characteristics and describe our methodology for building it.",
          "link": "http://arxiv.org/abs/2106.14167",
          "publishedOn": "2021-06-29T01:55:13.181Z",
          "wordCount": 593,
          "title": "PeCoQ: A Dataset for Persian Complex Question Answering over Knowledge Graph. (arXiv:2106.14167v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14321",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lachmy_R/0/1/0/all/0/1\">Royi Lachmy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pyatkin_V/0/1/0/all/0/1\">Valentina Pyatkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsarfaty_R/0/1/0/all/0/1\">Reut Tsarfaty</a>",
          "description": "Forming and interpreting abstraction is a core process in human\ncommunication. In particular, when giving and performing complex instructions\nstated in natural language (NL), people may naturally evoke abstract constructs\nsuch as objects, loops, conditions and functions to convey their intentions in\nan efficient and precise way. Yet, interpreting and grounding abstraction\nstated in NL has not been systematically studied in NLP/AI. To elicit\nnaturally-occurring abstractions in NL we develop the Hexagons referential\ngame, where players describe increasingly complex images on a two-dimensional\nHexagons board, and other players need to follow these instructions to recreate\nthe images. Using this game we collected the Hexagons dataset, which consists\nof 164 images and over 3000 naturally-occurring instructions, rich with diverse\nabstractions. Results of our baseline models on an instruction-to-execution\ntask derived from the Hexagons dataset confirm that higher-level abstractions\nin NL are indeed more challenging for current systems to process. Thus, this\ndataset exposes a new and challenging dimension for grounded semantic parsing,\nand we propose it for the community as a future benchmark to explore more\nsophisticated and high-level communication within NLP applications.",
          "link": "http://arxiv.org/abs/2106.14321",
          "publishedOn": "2021-06-29T01:55:13.172Z",
          "wordCount": 622,
          "title": "Draw Me a Flower: Grounding Formal Abstract Structures Stated in Informal Natural Language. (arXiv:2106.14321v1 [cs.CL])"
        }
      ]
    },
    {
      "title": "cs.IR updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.IR",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2107.00852",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ruihong_Q/0/1/0/all/0/1\">Qiu Ruihong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zi_H/0/1/0/all/0/1\">Huang Zi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jingjing_L/0/1/0/all/0/1\">Li Jingjing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hongzhi_Y/0/1/0/all/0/1\">Yin Hongzhi</a>",
          "description": "Different from the traditional recommender system, the session-based\nrecommender system introduces the concept of the session, i.e., a sequence of\ninteractions between a user and multiple items within a period, to preserve the\nuser's recent interest. The existing work on the session-based recommender\nsystem mainly relies on mining sequential patterns within individual sessions,\nwhich are not expressive enough to capture more complicated dependency\nrelationships among items. In addition, it does not consider the cross-session\ninformation due to the anonymity of the session data, where the linkage between\ndifferent sessions is prevented. In this paper, we solve these problems with\nthe graph neural networks technique. First, each session is represented as a\ngraph rather than a linear sequence structure, based on which a novel Full\nGraph Neural Network (FGNN) is proposed to learn complicated item dependency.\nTo exploit and incorporate cross-session information in the individual\nsession's representation learning, we further construct a Broadly Connected\nSession (BCS) graph to link different sessions and a novel Mask-Readout\nfunction to improve session embedding based on the BCS graph. Extensive\nexperiments have been conducted on two e-commerce benchmark datasets, i.e.,\nYoochoose and Diginetica, and the experimental results demonstrate the\nsuperiority of our proposal through comparisons with state-of-the-art\nsession-based recommender models.",
          "link": "http://arxiv.org/abs/2107.00852",
          "publishedOn": "2021-07-05T01:54:56.559Z",
          "wordCount": 651,
          "title": "Exploiting Cross-Session Information for Session-based Recommendation with Graph Neural Networks. (arXiv:2107.00852v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00873",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Brockmeier_M/0/1/0/all/0/1\">Malte Brockmeier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yawen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pateer_S/0/1/0/all/0/1\">Sunita Pateer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hertling_S/0/1/0/all/0/1\">Sven Hertling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paulheim_H/0/1/0/all/0/1\">Heiko Paulheim</a>",
          "description": "Modern large-scale knowledge graphs, such as DBpedia, are datasets which\nrequire large computational resources to serve and process. Moreover, they\noften have longer release cycles, which leads to outdated information in those\ngraphs. In this paper, we present DBpedia on Demand -- a system which serves\nDBpedia resources on demand without the need to materialize and store the\nentire graph, and which even provides limited querying functionality.",
          "link": "http://arxiv.org/abs/2107.00873",
          "publishedOn": "2021-07-05T01:54:56.528Z",
          "wordCount": 516,
          "title": "On-Demand and Lightweight Knowledge Graph Generation -- a Demonstration with DBpedia. (arXiv:2107.00873v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00846",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ruihong_Q/0/1/0/all/0/1\">Qiu Ruihong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zi_H/0/1/0/all/0/1\">Huang Zi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_C/0/1/0/all/0/1\">Chen Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hongzhi_Y/0/1/0/all/0/1\">Yin Hongzhi</a>",
          "description": "For present e-commerce platforms, session-based recommender systems are\ndeveloped to predict users' preference for next-item recommendation. Although a\nsession can usually reflect a user's current preference, a local shift of the\nuser's intention within the session may still exist. Specifically, the\ninteractions that take place in the early positions within a session generally\nindicate the user's initial intention, while later interactions are more likely\nto represent the latest intention. Such positional information has been rarely\nconsidered in existing methods, which restricts their ability to capture the\nsignificance of interactions at different positions. To thoroughly exploit the\npositional information within a session, a theoretical framework is developed\nin this paper to provide an in-depth analysis of the positional information. We\nformally define the properties of forward-awareness and backward-awareness to\nevaluate the ability of positional encoding schemes in capturing the initial\nand the latest intention. According to our analysis, existing positional\nencoding schemes are generally forward-aware only, which can hardly represent\nthe dynamics of the intention in a session. To enhance the positional encoding\nscheme for the session-based recommendation, a dual positional encoding (DPE)\nis proposed to account for both forward-awareness and backward-awareness. Based\non DPE, we propose a novel Positional Recommender (PosRec) model with a\nwell-designed Position-aware Gated Graph Neural Network module to fully exploit\nthe positional information for session-based recommendation tasks. Extensive\nexperiments are conducted on two e-commerce benchmark datasets, Yoochoose and\nDiginetica and the experimental results show the superiority of the PosRec by\ncomparing it with the state-of-the-art session-based recommender models.",
          "link": "http://arxiv.org/abs/2107.00846",
          "publishedOn": "2021-07-05T01:54:56.486Z",
          "wordCount": 680,
          "title": "Exploiting Positional Information for Session-based Recommendation. (arXiv:2107.00846v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00833",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Curmei_M/0/1/0/all/0/1\">Mihaela Curmei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dean_S/0/1/0/all/0/1\">Sarah Dean</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Recht_B/0/1/0/all/0/1\">Benjamin Recht</a>",
          "description": "In this work, we consider how preference models in interactive recommendation\nsystems determine the availability of content and users' opportunities for\ndiscovery. We propose an evaluation procedure based on stochastic reachability\nto quantify the maximum probability of recommending a target piece of content\nto an user for a set of allowable strategic modifications. This framework\nallows us to compute an upper bound on the likelihood of recommendation with\nminimal assumptions about user behavior. Stochastic reachability can be used to\ndetect biases in the availability of content and diagnose limitations in the\nopportunities for discovery granted to users. We show that this metric can be\ncomputed efficiently as a convex program for a variety of practical settings,\nand further argue that reachability is not inherently at odds with accuracy. We\ndemonstrate evaluations of recommendation algorithms trained on large datasets\nof explicit and implicit ratings. Our results illustrate how preference models,\nselection rules, and user interventions impact reachability and how these\neffects can be distributed unevenly.",
          "link": "http://arxiv.org/abs/2107.00833",
          "publishedOn": "2021-07-05T01:54:56.398Z",
          "wordCount": 610,
          "title": "Quantifying Availability and Discovery in Recommender Systems via Stochastic Reachability. (arXiv:2107.00833v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00221",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jing Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingya Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sigdel_M/0/1/0/all/0/1\">Madhav Sigdel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bopeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoang_P/0/1/0/all/0/1\">Phuong Hoang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Mengshu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Korayem_M/0/1/0/all/0/1\">Mohammed Korayem</a>",
          "description": "The online recruitment matching system has been the core technology and\nservice platform in CareerBuilder. One of the major challenges in an online\nrecruitment scenario is to provide good matches between job posts and\ncandidates using a recommender system on the scale. In this paper, we discussed\nthe techniques for applying an embedding-based recommender system for the large\nscale of job to candidates matching. To learn the comprehensive and effective\nembedding for job posts and candidates, we have constructed a fused-embedding\nvia different levels of representation learning from raw text, semantic\nentities and location information. The clusters of fused-embedding of job and\ncandidates are then used to build and train the Faiss index that supports\nruntime approximate nearest neighbor search for candidate retrieval. After the\nfirst stage of candidate retrieval, a second stage reranking model that\nutilizes other contextual information was used to generate the final matching\nresult. Both offline and online evaluation results indicate a significant\nimprovement of our proposed two-staged embedding-based system in terms of\nclick-through rate (CTR), quality and normalized discounted accumulated gain\n(nDCG), compared to those obtained from our baseline system. We further\ndescribed the deployment of the system that supports the million-scale job and\ncandidate matching process at CareerBuilder. The overall improvement of our job\nto candidate matching system has demonstrated its feasibility and scalability\nat a major online recruitment site.",
          "link": "http://arxiv.org/abs/2107.00221",
          "publishedOn": "2021-07-02T01:57:58.582Z",
          "wordCount": 668,
          "title": "Embedding-based Recommender System for Job to Candidate Matching on Scale. (arXiv:2107.00221v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00214",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khurana_P/0/1/0/all/0/1\">Parul Khurana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganesan_G/0/1/0/all/0/1\">Geetha Ganesan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_G/0/1/0/all/0/1\">Gulshan Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_K/0/1/0/all/0/1\">Kiran Sharma</a>",
          "description": "Bibliometrics is useful to analyze the research impact for measuring the\nresearch quality. Different bibliographic databases like Scopus, Web of\nScience, Google Scholar etc. are accessed for evaluating the trend of\npublications and citations from time to time. Some of these databases are free\nand some are subscription based. Its always debatable that which bibliographic\ndatabase is better and in what terms. To provide an optimal solution to\navailability of multiple bibliographic databases, we have implemented a single\nauthentic database named as ``conflate'' which can be used for fetching\npublication and citation trend of an author. To further strengthen the\ngenerated database and to provide the transparent system to the stakeholders, a\nconsensus mechanism ``proof of reference (PoR)'' is proposed. Due to three\nconsent based checks implemented in PoR, we feel that it could be considered as\na authentic and honest citation data source for the calculation of unified\ninformetrics for an author.",
          "link": "http://arxiv.org/abs/2107.00214",
          "publishedOn": "2021-07-02T01:57:58.507Z",
          "wordCount": 594,
          "title": "Proof of Reference(PoR): A unified informetrics based consensus mechanism. (arXiv:2107.00214v1 [cs.DL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00525",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xia_X/0/1/0/all/0/1\">Xinlin Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Han Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Songlin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Sulong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yun Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_B/0/1/0/all/0/1\">Bo Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wen-Yun Yang</a>",
          "description": "Graph convolution networks (GCN), which recently becomes new state-of-the-art\nmethod for graph node classification, recommendation and other applications,\nhas not been successfully applied to industrial-scale search engine yet. In\nthis proposal, we introduce our approach, namely SearchGCN, for embedding-based\ncandidate retrieval in one of the largest e-commerce search engine in the\nworld. Empirical studies demonstrate that SearchGCN learns better embedding\nrepresentations than existing methods, especially for long tail queries and\nitems. Thus, SearchGCN has been deployed into JD.com's search production since\nJuly 2020.",
          "link": "http://arxiv.org/abs/2107.00525",
          "publishedOn": "2021-07-02T01:57:58.469Z",
          "wordCount": 543,
          "title": "SearchGCN: Powering Embedding Retrieval by Graph Convolution Networks for E-Commerce Search. (arXiv:2107.00525v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2011.12683",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jin_J/0/1/0/all/0/1\">Jiarui Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_K/0/1/0/all/0/1\">Kounianhua Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weinan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1\">Jiarui Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yuchen Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smola_A/0/1/0/all/0/1\">Alexander J. Smola</a>",
          "description": "Heterogeneous information network (HIN) has been widely used to characterize\nentities of various types and their complex relations. Recent attempts either\nrely on explicit path reachability to leverage path-based semantic relatedness\nor graph neighborhood to learn heterogeneous network representations before\npredictions. These weakly coupled manners overlook the rich interactions among\nneighbor nodes, which introduces an early summarization issue. In this paper,\nwe propose GraphHINGE (Heterogeneous INteract and aggreGatE), which captures\nand aggregates the interactive patterns between each pair of nodes through\ntheir structured neighborhoods. Specifically, we first introduce\nNeighborhood-based Interaction (NI) module to model the interactive patterns\nunder the same metapaths, and then extend it to Cross Neighborhood-based\nInteraction (CNI) module to deal with different metapaths. Next, in order to\naddress the complexity issue on large-scale networks, we formulate the\ninteraction modules via a convolutional framework and learn the parameters\nefficiently with fast Fourier transform. Furthermore, we design a novel\nneighborhood-based selection (NS) mechanism, a sampling strategy, to filter\nhigh-order neighborhood information based on their low-order performance. The\nextensive experiments on six different types of heterogeneous graphs\ndemonstrate the performance gains by comparing with state-of-the-arts in both\nclick-through rate prediction and top-N recommendation tasks.",
          "link": "http://arxiv.org/abs/2011.12683",
          "publishedOn": "2021-07-02T01:57:58.431Z",
          "wordCount": 688,
          "title": "GraphHINGE: Learning Interaction Models of Structured Neighborhood on Heterogeneous Information Network. (arXiv:2011.12683v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11251",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Macdonald_C/0/1/0/all/0/1\">Craig Macdonald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tonellotto_N/0/1/0/all/0/1\">Nicola Tonellotto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ounis_I/0/1/0/all/0/1\">Iadh Ounis</a>",
          "description": "Pseudo-relevance feedback mechanisms, from Rocchio to the relevance models,\nhave shown the usefulness of expanding and reweighting the users' initial\nqueries using information occurring in an initial set of retrieved documents,\nknown as the pseudo-relevant set. Recently, dense retrieval -- through the use\nof neural contextual language models such as BERT for analysing the documents'\nand queries' contents and computing their relevance scores -- has shown a\npromising performance on several information retrieval tasks still relying on\nthe traditional inverted index for identifying documents relevant to a query.\nTwo different dense retrieval families have emerged: the use of single embedded\nrepresentations for each passage and query (e.g. using BERT's [CLS] token), or\nvia multiple representations (e.g. using an embedding for each token of the\nquery and document). In this work, we conduct the first study into the\npotential for multiple representation dense retrieval to be enhanced using\npseudo-relevance feedback. In particular, based on the pseudo-relevant set of\ndocuments identified using a first-pass dense retrieval, we extract\nrepresentative feedback embeddings (using KMeans clustering) -- while ensuring\nthat these embeddings discriminate among passages (based on IDF) -- which are\nthen added to the query representation. These additional feedback embeddings\nare shown to both enhance the effectiveness of a reranking as well as an\nadditional dense retrieval operation. Indeed, experiments on the MSMARCO\npassage ranking dataset show that MAP can be improved by upto 26% on the TREC\n2019 query set and 10% on the TREC 2020 query set by the application of our\nproposed ColBERT-PRF method on a ColBERT dense retrieval approach.",
          "link": "http://arxiv.org/abs/2106.11251",
          "publishedOn": "2021-07-02T01:57:58.414Z",
          "wordCount": 716,
          "title": "Pseudo-Relevance Feedback for Multiple Representation Dense Retrieval. (arXiv:2106.11251v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.08091",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Moreo_A/0/1/0/all/0/1\">Alejandro Moreo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sebastiani_F/0/1/0/all/0/1\">Fabrizio Sebastiani</a>",
          "description": "Sentiment quantification is the task of estimating the relative frequency (or\n\"prevalence\") of sentiment-related classes (such as Positive, Neutral,\nNegative) in a sample of unlabelled texts; this is especially important when\nthese texts are tweets, since most sentiment classification endeavours carried\nout on Twitter data actually have quantification (and not the classification of\nindividual tweets) as their ultimate goal. It is well-known that solving\nquantification via \"classify and count\" (i.e., by classifying all unlabelled\nitems via a standard classifier and counting the items that have been assigned\nto a given class) is suboptimal in terms of accuracy, and that more accurate\nquantification methods exist. In 2016, Gao and Sebastiani carried out a\nsystematic comparison of quantification methods on the task of tweet sentiment\nquantification. In hindsight, we observe that the experimental protocol\nfollowed in that work is flawed, and that its results are thus unreliable. We\nnow re-evaluate those quantification methods on the very same datasets, this\ntime following a now consolidated and much more robust experimental protocol,\nthat involves 5775 as many experiments as run in the original study. Our\nexperimentation yields results dramatically different from those obtained by\nGao and Sebastiani, and thus provide a different, much more solid understanding\nof the relative strengths and weaknesses of different sentiment quantification\nmethods.",
          "link": "http://arxiv.org/abs/2011.08091",
          "publishedOn": "2021-07-02T01:57:58.387Z",
          "wordCount": 676,
          "title": "Tweet Sentiment Quantification: An Experimental Re-Evaluation. (arXiv:2011.08091v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00082",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Oliveira_N/0/1/0/all/0/1\">Nuno Oliveira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sousa_N/0/1/0/all/0/1\">Norberto Sousa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Praca_I/0/1/0/all/0/1\">Isabel Pra&#xe7;a</a>",
          "description": "Cybersecurity is a very challenging topic of research nowadays, as\ndigitalization increases the interaction of people, software and services on\nthe Internet by means of technology devices and networks connected to it. The\nfield is broad and has a lot of unexplored ground under numerous disciplines\nsuch as management, psychology, and data science. Its large disciplinary\nspectrum and many significant research topics generate a considerable amount of\ninformation, making it hard for us to find what we are looking for when\nresearching a particular subject. This work proposes a new search engine for\nscientific publications which combines both information retrieval and reading\ncomprehension algorithms to extract answers from a collection of\ndomain-specific documents. The proposed solution although being applied to the\ncontext of cybersecurity exhibited great generalization capabilities and can be\neasily adapted to perform under other distinct knowledge domains.",
          "link": "http://arxiv.org/abs/2107.00082",
          "publishedOn": "2021-07-02T01:57:57.911Z",
          "wordCount": 575,
          "title": "A Search Engine for Scientific Publications: a Cybersecurity Case Study. (arXiv:2107.00082v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00161",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qing Wang</a>",
          "description": "In today's business marketplace, many high-tech Internet enterprises\nconstantly explore innovative ways to provide optimal online user experiences\nfor gaining competitive advantages. The great needs of developing intelligent\ninteractive recommendation systems are indicated, which could sequentially\nsuggest users the most proper items by accurately predicting their preferences,\nwhile receiving the up-to-date feedback to refine the recommendation results,\ncontinuously. Multi-armed bandit algorithms, which have been widely applied\ninto various online systems, are quite capable of delivering such efficient\nrecommendation services. However, few existing bandit models are able to adapt\nto new changes introduced by the modern recommender systems.",
          "link": "http://arxiv.org/abs/2107.00161",
          "publishedOn": "2021-07-02T01:57:57.853Z",
          "wordCount": 532,
          "title": "The Use of Bandit Algorithms in Intelligent Interactive Recommender Systems. (arXiv:2107.00161v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16102",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_V/0/1/0/all/0/1\">Victor Zitian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Montano_Campos_F/0/1/0/all/0/1\">Felipe Montano-Campos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zadrozny_W/0/1/0/all/0/1\">Wlodek Zadrozny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Canfield_E/0/1/0/all/0/1\">Evan Canfield</a>",
          "description": "The volume of scientific publications in organizational research becomes\nexceedingly overwhelming for human researchers who seek to timely extract and\nreview knowledge. This paper introduces natural language processing (NLP)\nmodels to accelerate the discovery, extraction, and organization of theoretical\ndevelopments (i.e., hypotheses) from social science publications. We illustrate\nand evaluate NLP models in the context of a systematic review of stakeholder\nvalue constructs and hypotheses. Specifically, we develop NLP models to\nautomatically 1) detect sentences in scholarly documents as hypotheses or not\n(Hypothesis Detection), 2) deconstruct the hypotheses into nodes (constructs)\nand links (causal/associative relationships) (Relationship Deconstruction ),\nand 3) classify the features of links in terms causality (versus association)\nand direction (positive, negative, versus nonlinear) (Feature Classification).\nOur models have reported high performance metrics for all three tasks. While\nour models are built in Python, we have made the pre-trained models fully\naccessible for non-programmers. We have provided instructions on installing and\nusing our pre-trained models via an R Shiny app graphic user interface (GUI).\nFinally, we suggest the next paths to extend our methodology for\ncomputer-assisted knowledge synthesis.",
          "link": "http://arxiv.org/abs/2106.16102",
          "publishedOn": "2021-07-01T01:59:31.579Z",
          "wordCount": 632,
          "title": "Machine Reading of Hypotheses for Organizational Research Reviews and Pre-trained Models via R Shiny App for Non-Programmers. (arXiv:2106.16102v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2008.08903",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_N/0/1/0/all/0/1\">Nan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1\">Hao Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_W/0/1/0/all/0/1\">Wei Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Sichen Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_K/0/1/0/all/0/1\">Kyle Kai Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prabowo_A/0/1/0/all/0/1\">Arian Prabowo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahaman_M/0/1/0/all/0/1\">Mohammad Saiedur Rahaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salim_F/0/1/0/all/0/1\">Flora D. Salim</a>",
          "description": "Generative Adversarial Networks (GANs) have shown remarkable success in\nproducing realistic-looking images in the computer vision area. Recently,\nGAN-based techniques are shown to be promising for spatio-temporal-based\napplications such as trajectory prediction, events generation and time-series\ndata imputation. While several reviews for GANs in computer vision have been\npresented, no one has considered addressing the practical applications and\nchallenges relevant to spatio-temporal data. In this paper, we have conducted a\ncomprehensive review of the recent developments of GANs for spatio-temporal\ndata. We summarise the application of popular GAN architectures for\nspatio-temporal data and the common practices for evaluating the performance of\nspatio-temporal applications with GANs. Finally, we point out future research\ndirections to benefit researchers in this area.",
          "link": "http://arxiv.org/abs/2008.08903",
          "publishedOn": "2021-07-01T01:59:31.548Z",
          "wordCount": 625,
          "title": "Generative Adversarial Networks for Spatio-temporal Data: A Survey. (arXiv:2008.08903v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.16153",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiaan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhixu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_B/0/1/0/all/0/1\">Binbin Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tingyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qingsheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhigang Chen</a>",
          "description": "We discuss a novel task, Chorus Recognition, which could potentially benefit\ndownstream tasks such as song search and music summarization. Different from\nthe existing tasks such as music summarization or lyrics summarization relying\non single-modal information, this paper models chorus recognition as a\nmulti-modal one by utilizing both the lyrics and the tune information of songs.\nWe propose a multi-modal Chorus Recognition model that considers diverse\nfeatures. Besides, we also create and publish the first Chorus Recognition\ndataset containing 627 songs for public use. Our empirical study performed on\nthe dataset demonstrates that our approach outperforms several baselines in\nchorus recognition. In addition, our approach also helps to improve the\naccuracy of its downstream task - song search by more than 10.6%.",
          "link": "http://arxiv.org/abs/2106.16153",
          "publishedOn": "2021-07-01T01:59:31.536Z",
          "wordCount": 577,
          "title": "Multi-Modal Chorus Recognition for Improving Song Search. (arXiv:2106.16153v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2105.14069",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dehpanah_A/0/1/0/all/0/1\">Arman Dehpanah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghori_M/0/1/0/all/0/1\">Muheeb Faizan Ghori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gemmell_J/0/1/0/all/0/1\">Jonathan Gemmell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mobasher_B/0/1/0/all/0/1\">Bamshad Mobasher</a>",
          "description": "Online competitive games have become a mainstream entertainment platform. To\ncreate a fair and exciting experience, these games use rating systems to match\nplayers with similar skills. While there has been an increasing amount of\nresearch on improving the performance of these systems, less attention has been\npaid to how their performance is evaluated. In this paper, we explore the\nutility of several metrics for evaluating three popular rating systems on a\nreal-world dataset of over 25,000 team battle royale matches. Our results\nsuggest considerable differences in their evaluation patterns. Some metrics\nwere highly impacted by the inclusion of new players. Many could not capture\nthe real differences between certain groups of players. Among all metrics\nstudied, normalized discounted cumulative gain (NDCG) demonstrated more\nreliable performance and more flexibility. It alleviated most of the challenges\nfaced by the other metrics while adding the freedom to adjust the focus of the\nevaluations on different groups of players.",
          "link": "http://arxiv.org/abs/2105.14069",
          "publishedOn": "2021-07-01T01:59:31.521Z",
          "wordCount": 633,
          "title": "The Evaluation of Rating Systems in Team-based Battle Royale Games. (arXiv:2105.14069v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15903",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhongkun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_P/0/1/0/all/0/1\">Pengjie Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhumin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Z/0/1/0/all/0/1\">Zhaochun Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rijke_M/0/1/0/all/0/1\">Maarten de Rijke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Ming Zhou</a>",
          "description": "Conversational Question Simplification (CQS) aims to simplify self-contained\nquestions into conversational ones by incorporating some conversational\ncharacteristics, e.g., anaphora and ellipsis. Existing maximum likelihood\nestimation (MLE) based methods often get trapped in easily learned tokens as\nall tokens are treated equally during training. In this work, we introduce a\nReinforcement Iterative Sequence Editing (RISE) framework that optimizes the\nminimum Levenshtein distance (MLD) through explicit editing actions. RISE is\nable to pay attention to tokens that are related to conversational\ncharacteristics. To train RISE, we devise an Iterative Reinforce Training (IRT)\nalgorithm with a Dynamic Programming based Sampling (DPS) process to improve\nexploration. Experimental results on two benchmark datasets show that RISE\nsignificantly outperforms state-of-the-art methods and generalizes well on\nunseen data.",
          "link": "http://arxiv.org/abs/2106.15903",
          "publishedOn": "2021-07-01T01:59:31.503Z",
          "wordCount": 573,
          "title": "Learning to Ask Conversational Questions by Optimizing Levenshtein Distance. (arXiv:2106.15903v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16053",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Voskarides_N/0/1/0/all/0/1\">Nikos Voskarides</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meij_E/0/1/0/all/0/1\">Edgar Meij</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sauer_S/0/1/0/all/0/1\">Sabrina Sauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rijke_M/0/1/0/all/0/1\">Maarten de Rijke</a>",
          "description": "Writers such as journalists often use automatic tools to find relevant\ncontent to include in their narratives. In this paper, we focus on supporting\nwriters in the news domain to develop event-centric narratives. Given an\nincomplete narrative that specifies a main event and a context, we aim to\nretrieve news articles that discuss relevant events that would enable the\ncontinuation of the narrative. We formally define this task and propose a\nretrieval dataset construction procedure that relies on existing news articles\nto simulate incomplete narratives and relevant articles. Experiments on two\ndatasets derived from this procedure show that state-of-the-art lexical and\nsemantic rankers are not sufficient for this task. We show that combining those\nwith a ranker that ranks articles by reverse chronological order outperforms\nthose rankers alone. We also perform an in-depth quantitative and qualitative\nanalysis of the results that sheds light on the characteristics of this task.",
          "link": "http://arxiv.org/abs/2106.16053",
          "publishedOn": "2021-07-01T01:59:31.458Z",
          "wordCount": 592,
          "title": "News Article Retrieval in Context for Event-centric Narrative Creation. (arXiv:2106.16053v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11866",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zekun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Z/0/1/0/all/0/1\">Zeyu Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaoyu Zhang</a>",
          "description": "Factorization machine (FM) is a prevalent approach to modeling pairwise\n(second-order) feature interactions when dealing with high-dimensional sparse\ndata. However, on the one hand, FM fails to capture higher-order feature\ninteractions suffering from combinatorial expansion, on the other hand, taking\ninto account interaction between every pair of features may introduce noise and\ndegrade prediction accuracy. To solve the problems, we propose a novel approach\nGraph Factorization Machine (GraphFM) by naturally representing features in the\ngraph structure. In particular, a novel mechanism is designed to select the\nbeneficial feature interactions and formulate them as edges between features.\nThen our proposed model which integrates the interaction function of FM into\nthe feature aggregation strategy of Graph Neural Network (GNN), can model\narbitrary-order feature interactions on the graph-structured features by\nstacking layers. Experimental results on several real-world datasets has\ndemonstrated the rationality and effectiveness of our proposed approach.",
          "link": "http://arxiv.org/abs/2105.11866",
          "publishedOn": "2021-07-01T01:59:31.292Z",
          "wordCount": 615,
          "title": "GraphFM: Graph Factorization Machines for Feature Interaction Modeling. (arXiv:2105.11866v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15984",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yadan Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sadiq_S/0/1/0/all/0/1\">Shazia W. Sadiq</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_P/0/1/0/all/0/1\">Peng Cui</a>",
          "description": "With the rapid growth of location-based social networks (LBSNs),\nPoint-Of-Interest (POI) recommendation has been broadly studied in this decade.\nRecently, the next POI recommendation, a natural extension of POI\nrecommendation, has attracted much attention. It aims at suggesting the next\nPOI to a user in spatial and temporal context, which is a practical yet\nchallenging task in various applications. Existing approaches mainly model the\nspatial and temporal information, and memorize historical patterns through\nuser's trajectories for recommendation. However, they suffer from the negative\nimpact of missing and irregular check-in data, which significantly influences\nthe model performance. In this paper, we propose an attention-based\nsequence-to-sequence generative model, namely POI-Augmentation Seq2Seq\n(PA-Seq2Seq), to address the sparsity of training set by making check-in\nrecords to be evenly-spaced. Specifically, the encoder summarises each check-in\nsequence and the decoder predicts the possible missing check-ins based on the\nencoded information. In order to learn time-aware correlation among user\nhistory, we employ local attention mechanism to help the decoder focus on a\nspecific range of context information when predicting a certain missing\ncheck-in point. Extensive experiments have been conducted on two real-world\ncheck-in datasets, Gowalla and Brightkite, for performance and effectiveness\nevaluation.",
          "link": "http://arxiv.org/abs/2106.15984",
          "publishedOn": "2021-07-01T01:59:31.223Z",
          "wordCount": 646,
          "title": "Context-Aware Attention-Based Data Augmentation for POI Recommendation. (arXiv:2106.15984v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15876",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_P/0/1/0/all/0/1\">Paheli Bhattacharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poddar_S/0/1/0/all/0/1\">Soham Poddar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rudra_K/0/1/0/all/0/1\">Koustav Rudra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_K/0/1/0/all/0/1\">Kripabandhu Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Saptarshi Ghosh</a>",
          "description": "Automatic summarization of legal case documents is an important and practical\nchallenge. Apart from many domain-independent text summarization algorithms\nthat can be used for this purpose, several algorithms have been developed\nspecifically for summarizing legal case documents. However, most of the\nexisting algorithms do not systematically incorporate domain knowledge that\nspecifies what information should ideally be present in a legal case document\nsummary. To address this gap, we propose an unsupervised summarization\nalgorithm DELSumm which is designed to systematically incorporate guidelines\nfrom legal experts into an optimization setup. We conduct detailed experiments\nover case documents from the Indian Supreme Court. The experiments show that\nour proposed unsupervised method outperforms several strong baselines in terms\nof ROUGE scores, including both general summarization algorithms and\nlegal-specific ones. In fact, though our proposed algorithm is unsupervised, it\noutperforms several supervised summarization models that are trained over\nthousands of document-summary pairs.",
          "link": "http://arxiv.org/abs/2106.15876",
          "publishedOn": "2021-07-01T01:59:31.127Z",
          "wordCount": 602,
          "title": "Incorporating Domain Knowledge for Extractive Summarization of Legal Case Documents. (arXiv:2106.15876v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15779",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yi_Q/0/1/0/all/0/1\">Qiaomin Yi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_N/0/1/0/all/0/1\">Ning Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip S. Yu</a>",
          "description": "Robust recommendation aims at capturing true preference of users from noisy\ndata, for which there are two lines of methods have been proposed. One is based\non noise injection, and the other is to adopt the generative model Variational\nAuto-encoder (VAE). However, the existing works still face two challenges.\nFirst, the noise injection based methods often draw the noise from a fixed\nnoise distribution given in advance, while in real world, the noise\ndistributions of different users and items may differ from each other due to\npersonal behaviors and item usage patterns. Second, the VAE based models are\nnot expressive enough to capture the true preference since VAE often yields an\nembedding space of a single modal, while in real world, user-item interactions\nusually exhibit multi-modality on user preference distribution. In this paper,\nwe propose a novel model called Dual Adversarial Variational Embedding (DAVE)\nfor robust recommendation, which can provide personalized noise reduction for\ndifferent users and items, and capture the multi-modality of the embedding\nspace, by combining the advantages of VAE and adversarial training between the\nintroduced auxiliary discriminators and the variational inference networks. The\nextensive experiments conducted on real datasets verify the effectiveness of\nDAVE on robust recommendation.",
          "link": "http://arxiv.org/abs/2106.15779",
          "publishedOn": "2021-07-01T01:59:31.084Z",
          "wordCount": 626,
          "title": "Dual Adversarial Variational Embedding for Robust Recommendation. (arXiv:2106.15779v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15814",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1\">Hongzhi Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zi Huang</a>",
          "description": "Being an indispensable component in location-based social networks, next\npoint-of-interest (POI) recommendation recommends users unexplored POIs based\non their recent visiting histories. However, existing work mainly models\ncheck-in data as isolated POI sequences, neglecting the crucial collaborative\nsignals from cross-sequence check-in information. Furthermore, the sparse\nPOI-POI transitions restrict the ability of a model to learn effective\nsequential patterns for recommendation. In this paper, we propose\nSequence-to-Graph (Seq2Graph) augmentation for each POI sequence, allowing\ncollaborative signals to be propagated from correlated POIs belonging to other\nsequences. We then devise a novel Sequence-to-Graph POI Recommender (SGRec),\nwhich jointly learns POI embeddings and infers a user's temporal preferences\nfrom the graph-augmented POI sequence. To overcome the sparsity of POI-level\ninteractions, we further infuse category-awareness into SGRec with a multi-task\nlearning scheme that captures the denser category-wise transitions. As such,\nSGRec makes full use of the collaborative signals for learning expressive POI\nrepresentations, and also comprehensively uncovers multi-level sequential\npatterns for user preference modelling. Extensive experiments on two real-world\ndatasets demonstrate the superiority of SGRec against state-of-the-art methods\nin next POI recommendation.",
          "link": "http://arxiv.org/abs/2106.15814",
          "publishedOn": "2021-07-01T01:59:31.058Z",
          "wordCount": 614,
          "title": "Discovering Collaborative Signals for Next POI Recommendation with Iterative Seq2Graph Augmentation. (arXiv:2106.15814v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15313",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Issam_K/0/1/0/all/0/1\">Kalliath Abdul Rasheed Issam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_S/0/1/0/all/0/1\">Shivam Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+N_S/0/1/0/all/0/1\">Subalalitha C. N</a>",
          "description": "Text summarization is an approach for identifying important information\npresent within text documents. This computational technique aims to generate\nshorter versions of the source text, by including only the relevant and salient\ninformation present within the source text. In this paper, we propose a novel\nmethod to summarize a text document by clustering its contents based on latent\ntopics produced using topic modeling techniques and by generating extractive\nsummaries for each of the identified text clusters. All extractive\nsub-summaries are later combined to generate a summary for any given source\ndocument. We utilize the lesser used and challenging WikiHow dataset in our\napproach to text summarization. This dataset is unlike the commonly used news\ndatasets which are available for text summarization. The well-known news\ndatasets present their most important information in the first few lines of\ntheir source texts, which make their summarization a lesser challenging task\nwhen compared to summarizing the WikiHow dataset. Contrary to these news\ndatasets, the documents in the WikiHow dataset are written using a generalized\napproach and have lesser abstractedness and higher compression ratio, thus\nproposing a greater challenge to generate summaries. A lot of the current\nstate-of-the-art text summarization techniques tend to eliminate important\ninformation present in source documents in the favor of brevity. Our proposed\ntechnique aims to capture all the varied information present in source\ndocuments. Although the dataset proved challenging, after performing extensive\ntests within our experimental setup, we have discovered that our model produces\nencouraging ROUGE results and summaries when compared to the other published\nextractive and abstractive text summarization models.",
          "link": "http://arxiv.org/abs/2106.15313",
          "publishedOn": "2021-06-30T02:01:00.059Z",
          "wordCount": 718,
          "title": "Topic Modeling Based Extractive Text Summarization. (arXiv:2106.15313v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14652",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_P/0/1/0/all/0/1\">Peiyuan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaofeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sang_Z/0/1/0/all/0/1\">Zisen Sang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_A/0/1/0/all/0/1\">Aiquan Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_G/0/1/0/all/0/1\">Guodong Cao</a>",
          "description": "As a new type of e-commerce platform developed in recent years, local\nconsumer service platform provides users with software to consume service to\nthe nearby store or to the home, such as Groupon and Koubei. Different from\nother common e-commerce platforms, the behavior of users on the local consumer\nservice platform is closely related to their real-time local context\ninformation. Therefore, building a context-aware user behavior prediction\nsystem is able to provide both merchants and users better service in local\nconsumer service platforms. However, most of the previous work just treats the\ncontextual information as an ordinary feature into the prediction model to\nobtain the prediction list under a specific context, which ignores the fact\nthat the interest of a user in different contexts is often significantly\ndifferent. Hence, in this paper, we propose a context-aware heterogeneous graph\nattention network (CHGAT) to dynamically generate the representation of the\nuser and to estimate the probability for future behavior. Specifically, we\nfirst construct the meta-path based heterogeneous graphs with the historical\nbehaviors from multiple sources and comprehend heterogeneous vertices in the\ngraph with a novel unified knowledge representing approach. Next, a multi-level\nattention mechanism is introduced for context-aware aggregation with graph\nvertices, which contains the vertex-level attention network and the path-level\nattention network. Both of them aim to capture the semantic correlation between\ninformation contained in the graph and the outside real-time contextual\ninformation in the search system. Then the model proposed in this paper\naggregates specific graphs with their corresponding context features and\nobtains the representation of user interest under a specific context and input\nit into the prediction network to finally obtain the predicted probability of\nuser behavior.",
          "link": "http://arxiv.org/abs/2106.14652",
          "publishedOn": "2021-06-30T02:01:00.050Z",
          "wordCount": 742,
          "title": "Context-aware Heterogeneous Graph Attention Network for User Behavior Prediction in Local Consumer Service Platform. (arXiv:2106.14652v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15498",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hagerer_G/0/1/0/all/0/1\">Gerhard Hagerer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_W/0/1/0/all/0/1\">Wenbin Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danner_H/0/1/0/all/0/1\">Hannah Danner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Groh_G/0/1/0/all/0/1\">Georg Groh</a>",
          "description": "Social media offer plenty of information to perform market research in order\nto meet the requirements of customers. One way how this research is conducted\nis that a domain expert gathers and categorizes user-generated content into a\ncomplex and fine-grained class structure. In many of such cases, little data\nmeets complex annotations. It is not yet fully understood how this can be\nleveraged successfully for classification. We examine the classification\naccuracy of expert labels when used with a) many fine-grained classes and b)\nfew abstract classes. For scenario b) we compare abstract class labels given by\nthe domain expert as baseline and by automatic hierarchical clustering. We\ncompare this to another baseline where the entire class structure is given by a\ncompletely unsupervised clustering approach. By doing so, this work can serve\nas an example of how complex expert annotations are potentially beneficial and\ncan be utilized in the most optimal way for opinion mining in highly specific\ndomains. By exploring across a range of techniques and experiments, we find\nthat automated class abstraction approaches in particular the unsupervised\napproach performs remarkably well against domain expert baseline on text\nclassification tasks. This has the potential to inspire opinion mining\napplications in order to support market researchers in practice and to inspire\nfine-grained automated content analysis on a large scale.",
          "link": "http://arxiv.org/abs/2106.15498",
          "publishedOn": "2021-06-30T02:01:00.011Z",
          "wordCount": 657,
          "title": "Classification of Consumer Belief Statements From Social Media. (arXiv:2106.15498v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15497",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_C/0/1/0/all/0/1\">Chaochen Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_Y/0/1/0/all/0/1\">Yong Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doss_R/0/1/0/all/0/1\">Robin Ram Mohan Doss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jiangshan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sood_K/0/1/0/all/0/1\">Keshav Sood</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Longxiang Gao</a>",
          "description": "With the development of blockchain technologies, the number of smart\ncontracts deployed on blockchain platforms is growing exponentially, which\nmakes it difficult for users to find desired services by manual screening. The\nautomatic classification of smart contracts can provide blockchain users with\nkeyword-based contract searching and helps to manage smart contracts\neffectively. Current research on smart contract classification focuses on\nNatural Language Processing (NLP) solutions which are based on contract source\ncode. However, more than 94% of smart contracts are not open-source, so the\napplication scenarios of NLP methods are very limited. Meanwhile, NLP models\nare vulnerable to adversarial attacks. This paper proposes a classification\nmodel based on features from contract bytecode instead of source code to solve\nthese problems. We also use feature selection and ensemble learning to optimize\nthe model. Our experimental studies on over 3,300 real-world Ethereum smart\ncontracts show that our model can classify smart contracts without source code\nand has better performance than baseline models. Our model also has good\nresistance to adversarial attacks compared with NLP-based models. In addition,\nour analysis reveals that account features used in many smart contract\nclassification models have little effect on classification and can be excluded.",
          "link": "http://arxiv.org/abs/2106.15497",
          "publishedOn": "2021-06-30T02:00:59.992Z",
          "wordCount": 645,
          "title": "A Bytecode-based Approach for Smart Contract Classification. (arXiv:2106.15497v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/1705.10351",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tellez_E/0/1/0/all/0/1\">Eric S. Tellez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruiz_G/0/1/0/all/0/1\">Guillermo Ruiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chavez_E/0/1/0/all/0/1\">Edgar Chavez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Graff_M/0/1/0/all/0/1\">Mario Graff</a>",
          "description": "Near neighbor search (NNS) is a powerful abstraction for data access;\nhowever, data indexing is troublesome even for approximate indexes. For\nintrinsically high-dimensional data, high-quality fast searches demand either\nindexes with impractically large memory usage or preprocessing time.\n\nIn this paper, we introduce an algorithm to solve a nearest-neighbor query\n$q$ by minimizing a kernel function defined by the distance from $q$ to each\nobject in the database. The minimization is performed using metaheuristics to\nsolve the problem rapidly; even when some methods in the literature use this\nstrategy behind the scenes, our approach is the first one using it explicitly.\nWe also provide two approaches to select edges in the graph's construction\nstage that limit memory footprint and reduce the number of free parameters\nsimultaneously.\n\nWe carry out a thorough experimental comparison with state-of-the-art indexes\nthrough synthetic and real-world datasets; we found out that our contributions\nachieve competitive performances regarding speed, accuracy, and memory in\nalmost any of our benchmarks.",
          "link": "http://arxiv.org/abs/1705.10351",
          "publishedOn": "2021-06-30T02:00:59.984Z",
          "wordCount": 674,
          "title": "A scalable solution to the nearest neighbor search problem through local-search methods on neighbor graphs. (arXiv:1705.10351v4 [cs.DS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15541",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vaccario_G/0/1/0/all/0/1\">Giacomo Vaccario</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verginer_L/0/1/0/all/0/1\">Luca Verginer</a>",
          "description": "Journal rankings are widely used and are often based on citation data in\ncombination with a network perspective. We argue that some of these\nnetwork-based rankings can produce misleading results. From a theoretical point\nof view, we show that the standard network modelling approach of citation data\nat the journal level (i.e., the projection of paper citations onto journals)\nintroduces fictitious relations among journals. To overcome this problem, we\npropose a citation path perspective, and empirically show that rankings based\non the network and the citation path perspective are very different. Based on\nour theoretical and empirical analysis, we highlight the limitations of\nstandard network metrics, and propose a method to overcome these limitations\nand compute journal rankings.",
          "link": "http://arxiv.org/abs/2106.15541",
          "publishedOn": "2021-06-30T02:00:59.978Z",
          "wordCount": 578,
          "title": "When standard network measures fail to rank journals: A theoretical and empirical analysis. (arXiv:2106.15541v1 [cs.DL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14719",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Allaix_M/0/1/0/all/0/1\">Matteo Allaix</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Seunghoan Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holzbaur_L/0/1/0/all/0/1\">Lukas Holzbaur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pllaha_T/0/1/0/all/0/1\">Tefjol Pllaha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayashi_M/0/1/0/all/0/1\">Masahito Hayashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hollanti_C/0/1/0/all/0/1\">Camilla Hollanti</a>",
          "description": "In quantum private information retrieval (QPIR), a user retrieves a classical\nfile from multiple servers by downloading quantum systems without revealing the\nidentity of the file. The QPIR capacity is the maximal achievable ratio of the\nretrieved file size to the total download size. In this paper, the capacity of\nQPIR from MDS-coded and colluding servers is studied. Two classes of QPIR,\ncalled stabilizer QPIR and dimension squared QPIR induced from classical\nstrongly linear PIR are defined, and the related QPIR capacities are derived.\nFor the non-colluding case, the general QPIR capacity is derived when the\nnumber of files goes to infinity. The capacities of symmetric and non-symmetric\nQPIR with coded and colluding servers are proved to coincide, being double to\ntheir classical counterparts. A general statement on the converse bound for\nQPIR with coded and colluding servers is derived showing that the capacities of\nstabilizer QPIR and dimension squared QPIR induced from any class of PIR are\nupper bounded by twice the classical capacity of the respective PIR class. The\nproposed capacity-achieving scheme combines the star-product scheme by\nFreij-Hollanti et al. and the stabilizer QPIR scheme by Song et al. by\nemploying (weakly) self-dual Reed--Solomon codes.",
          "link": "http://arxiv.org/abs/2106.14719",
          "publishedOn": "2021-06-30T02:00:59.939Z",
          "wordCount": 676,
          "title": "On the Capacity of Quantum Private Information Retrieval from MDS-Coded and Colluding Servers. (arXiv:2106.14719v2 [cs.IT] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14885",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nentidis_A/0/1/0/all/0/1\">Anastasios Nentidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katsimpras_G/0/1/0/all/0/1\">Georgios Katsimpras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vandorou_E/0/1/0/all/0/1\">Eirini Vandorou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krithara_A/0/1/0/all/0/1\">Anastasia Krithara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gasco_L/0/1/0/all/0/1\">Luis Gasco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krallinger_M/0/1/0/all/0/1\">Martin Krallinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paliouras_G/0/1/0/all/0/1\">Georgios Paliouras</a>",
          "description": "Advancing the state-of-the-art in large-scale biomedical semantic indexing\nand question answering is the main focus of the BioASQ challenge. BioASQ\norganizes respective tasks where different teams develop systems that are\nevaluated on the same benchmark datasets that represent the real information\nneeds of experts in the biomedical domain. This paper presents an overview of\nthe ninth edition of the BioASQ challenge in the context of the Conference and\nLabs of the Evaluation Forum (CLEF) 2021. In this year, a new question\nanswering task, named Synergy, is introduced to support researchers studying\nthe COVID-19 disease and measure the ability of the participating teams to\ndiscern information while the problem is still developing. In total, 42 teams\nwith more than 170 systems were registered to participate in the four tasks of\nthe challenge. The evaluation results, similarly to previous years, show a\nperformance gain against the baselines which indicates the continuous\nimprovement of the state-of-the-art in this field.",
          "link": "http://arxiv.org/abs/2106.14885",
          "publishedOn": "2021-06-30T02:00:59.350Z",
          "wordCount": 677,
          "title": "Overview of BioASQ 2021: The ninth BioASQ challenge on Large-Scale Biomedical Semantic Indexing and Question Answering. (arXiv:2106.14885v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14979",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hron_J/0/1/0/all/0/1\">Jiri Hron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krauth_K/0/1/0/all/0/1\">Karl Krauth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jordan_M/0/1/0/all/0/1\">Michael I. Jordan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kilbertus_N/0/1/0/all/0/1\">Niki Kilbertus</a>",
          "description": "Thanks to their scalability, two-stage recommenders are used by many of\ntoday's largest online platforms, including YouTube, LinkedIn, and Pinterest.\nThese systems produce recommendations in two steps: (i) multiple nominators --\ntuned for low prediction latency -- preselect a small subset of candidates from\nthe whole item pool; (ii)~a slower but more accurate ranker further narrows\ndown the nominated items, and serves to the user. Despite their popularity, the\nliterature on two-stage recommenders is relatively scarce, and the algorithms\nare often treated as the sum of their parts. Such treatment presupposes that\nthe two-stage performance is explained by the behavior of individual components\nif they were deployed independently. This is not the case: using synthetic and\nreal-world data, we demonstrate that interactions between the ranker and the\nnominators substantially affect the overall performance. Motivated by these\nfindings, we derive a generalization lower bound which shows that careful\nchoice of each nominator's training set is sometimes the only difference\nbetween a poor and an optimal two-stage recommender. Since searching for a good\nchoice manually is difficult, we learn one instead. In particular, using a\nMixture-of-Experts approach, we train the nominators (experts) to specialize on\ndifferent subsets of the item pool. This significantly improves performance.",
          "link": "http://arxiv.org/abs/2106.14979",
          "publishedOn": "2021-06-30T02:00:59.314Z",
          "wordCount": 641,
          "title": "On component interactions in two-stage recommender systems. (arXiv:2106.14979v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15230",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Demir_C/0/1/0/all/0/1\">Caglar Demir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moussallem_D/0/1/0/all/0/1\">Diego Moussallem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heindorf_S/0/1/0/all/0/1\">Stefan Heindorf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ngomo_A/0/1/0/all/0/1\">Axel-Cyrille Ngonga Ngomo</a>",
          "description": "Knowledge graph embedding research has mainly focused on the two smallest\nnormed division algebras, $\\mathbb{R}$ and $\\mathbb{C}$. Recent results suggest\nthat trilinear products of quaternion-valued embeddings can be a more effective\nmeans to tackle link prediction. In addition, models based on convolutions on\nreal-valued embeddings often yield state-of-the-art results for link\nprediction. In this paper, we investigate a composition of convolution\noperations with hypercomplex multiplications. We propose the four approaches\nQMult, OMult, ConvQ and ConvO to tackle the link prediction problem. QMult and\nOMult can be considered as quaternion and octonion extensions of previous\nstate-of-the-art approaches, including DistMult and ComplEx. ConvQ and ConvO\nbuild upon QMult and OMult by including convolution operations in a way\ninspired by the residual learning framework. We evaluated our approaches on\nseven link prediction datasets including WN18RR, FB15K-237 and YAGO3-10.\nExperimental results suggest that the benefits of learning hypercomplex-valued\nvector representations become more apparent as the size and complexity of the\nknowledge graph grows. ConvO outperforms state-of-the-art approaches on\nFB15K-237 in MRR, Hit@1 and Hit@3, while QMult, OMult, ConvQ and ConvO\noutperform state-of-the-approaches on YAGO3-10 in all metrics. Results also\nsuggest that link prediction performances can be further improved via\nprediction averaging. To foster reproducible research, we provide an\nopen-source implementation of approaches, including training and evaluation\nscripts as well as pretrained models.",
          "link": "http://arxiv.org/abs/2106.15230",
          "publishedOn": "2021-06-30T02:00:59.290Z",
          "wordCount": 648,
          "title": "Convolutional Hypercomplex Embeddings for Link Prediction. (arXiv:2106.15230v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09665",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhichao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_H/0/1/0/all/0/1\">Hansi Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ai_Q/0/1/0/all/0/1\">Qingyao Ai</a>",
          "description": "Modern E-commerce websites contain heterogeneous sources of information, such\nas numerical ratings, textual reviews and images. These information can be\nutilized to assist recommendation. Through textual reviews, a user explicitly\nexpress her affinity towards the item. Previous researchers found that by using\nthe information extracted from these reviews, we can better profile the users'\nexplicit preferences as well as the item features, leading to the improvement\nof recommendation performance. However, most of the previous algorithms were\nonly utilizing the review information for explicit-feedback problem i.e. rating\nprediction, and when it comes to implicit-feedback ranking problem such as\ntop-N recommendation, the usage of review information has not been fully\nexplored. Seeing this gap, in this work, we investigate the effectiveness of\ntextual review information for top-N recommendation under E-commerce settings.\nWe adapt several SOTA review-based rating prediction models for top-N\nrecommendation tasks and compare them to existing top-N recommendation models\nfrom both performance and efficiency. We find that models utilizing only review\ninformation can not achieve better performances than vanilla implicit-feedback\nmatrix factorization method. When utilizing review information as a regularizer\nor auxiliary information, the performance of implicit-feedback matrix\nfactorization method can be further improved. However, the optimal model\nstructure to utilize textual reviews for E-commerce top-N recommendation is yet\nto be determined.",
          "link": "http://arxiv.org/abs/2106.09665",
          "publishedOn": "2021-06-29T01:55:13.421Z",
          "wordCount": 667,
          "title": "Understanding the Effectiveness of Reviews in E-commerce Top-N Recommendation. (arXiv:2106.09665v2 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14226",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chang_J/0/1/0/all/0/1\">Jianxin Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1\">Chen Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yu Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hui_Y/0/1/0/all/0/1\">Yiqun Hui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_Y/0/1/0/all/0/1\">Yanan Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yang Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_D/0/1/0/all/0/1\">Depeng Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yong Li</a>",
          "description": "Sequential recommendation aims to leverage users' historical behaviors to\npredict their next interaction. Existing works have not yet addressed two main\nchallenges in sequential recommendation. First, user behaviors in their rich\nhistorical sequences are often implicit and noisy preference signals, they\ncannot sufficiently reflect users' actual preferences. In addition, users'\ndynamic preferences often change rapidly over time, and hence it is difficult\nto capture user patterns in their historical sequences. In this work, we\npropose a graph neural network model called SURGE (short for SeqUential\nRecommendation with Graph neural nEtworks) to address these two issues.\nSpecifically, SURGE integrates different types of preferences in long-term user\nbehaviors into clusters in the graph by re-constructing loose item sequences\ninto tight item-item interest graphs based on metric learning. This helps\nexplicitly distinguish users' core interests, by forming dense clusters in the\ninterest graph. Then, we perform cluster-aware and query-aware graph\nconvolutional propagation and graph pooling on the constructed graph. It\ndynamically fuses and extracts users' current activated core interests from\nnoisy user behavior sequences. We conduct extensive experiments on both public\nand proprietary industrial datasets. Experimental results demonstrate\nsignificant performance gains of our proposed method compared to\nstate-of-the-art methods. Further studies on sequence length confirm that our\nmethod can model long behavioral sequences effectively and efficiently.",
          "link": "http://arxiv.org/abs/2106.14226",
          "publishedOn": "2021-06-29T01:55:13.218Z",
          "wordCount": 652,
          "title": "Sequential Recommendation with Graph Neural Networks. (arXiv:2106.14226v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2102.13392",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gominski_D/0/1/0/all/0/1\">Dimitri Gominski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gouet_Brunet_V/0/1/0/all/0/1\">Val&#xe9;rie Gouet-Brunet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liming Chen</a>",
          "description": "Advances in high resolution remote sensing image analysis are currently\nhampered by the difficulty of gathering enough annotated data for training deep\nlearning methods, giving rise to a variety of small datasets and associated\ndataset-specific methods. Moreover, typical tasks such as classification and\nretrieval lack a systematic evaluation on standard benchmarks and training\ndatasets, which make it hard to identify durable and generalizable scientific\ncontributions. We aim at unifying remote sensing image retrieval and\nclassification with a new large-scale training and testing dataset, SF300,\nincluding both vertical and oblique aerial images and made available to the\nresearch community, and an associated fine-tuning method. We additionally\npropose a new adversarial fine-tuning method for global descriptors. We show\nthat our framework systematically achieves a boost of retrieval and\nclassification performance on nine different datasets compared to an ImageNet\npretrained baseline, with currently no other method to compare to.",
          "link": "http://arxiv.org/abs/2102.13392",
          "publishedOn": "2021-06-29T01:55:13.161Z",
          "wordCount": 646,
          "title": "Unifying Remote Sensing Image Retrieval and Classification with Robust Fine-tuning. (arXiv:2102.13392v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14031",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1\">Xu Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hongshen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yonghao Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xiaofang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1\">Zhuoye Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zhen He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_B/0/1/0/all/0/1\">Bo Long</a>",
          "description": "Most sequential recommendation models capture the features of consecutive\nitems in a user-item interaction history. Though effective, their\nrepresentation expressiveness is still hindered by the sparse learning signals.\nAs a result, the sequential recommender is prone to make inconsistent\npredictions. In this paper, we propose a model, \\textbf{SSI}, to improve\nsequential recommendation consistency with Self-Supervised Imitation.\nPrecisely, we extract the consistency knowledge by utilizing three\nself-supervised pre-training tasks, where temporal consistency and persona\nconsistency capture user-interaction dynamics in terms of the chronological\norder and persona sensitivities, respectively. Furthermore, to provide the\nmodel with a global perspective, global session consistency is introduced by\nmaximizing the mutual information among global and local interaction sequences.\nFinally, to comprehensively take advantage of all three independent aspects of\nconsistency-enhanced knowledge, we establish an integrated imitation learning\nframework. The consistency knowledge is effectively internalized and\ntransferred to the student model by imitating the conventional prediction logit\nas well as the consistency-enhanced item representations. In addition, the\nflexible self-supervised imitation framework can also benefit other student\nrecommenders. Experiments on four real-world datasets show that SSI effectively\noutperforms the state-of-the-art sequential recommendation methods.",
          "link": "http://arxiv.org/abs/2106.14031",
          "publishedOn": "2021-06-29T01:55:13.150Z",
          "wordCount": 623,
          "title": "Improving Sequential Recommendation Consistency with Self-Supervised Imitation. (arXiv:2106.14031v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14213",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mansoor_M/0/1/0/all/0/1\">Muvazima Mansoor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandar_S/0/1/0/all/0/1\">Srikanth Chandar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinath_R/0/1/0/all/0/1\">Ramamoorthy Srinath</a>",
          "description": "In this paper, we propose an architecture to solve a novel problem statement\nthat has stemmed more so in recent times with an increase in demand for virtual\ncontent delivery due to the COVID-19 pandemic. All educational institutions,\nworkplaces, research centers, etc. are trying to bridge the gap of\ncommunication during these socially distanced times with the use of online\ncontent delivery. The trend now is to create presentations, and then\nsubsequently deliver the same using various virtual meeting platforms. The time\nbeing spent in such creation of presentations and delivering is what we try to\nreduce and eliminate through this paper which aims to use Machine Learning (ML)\nalgorithms and Natural Language Processing (NLP) modules to automate the\nprocess of creating a slides-based presentation from a document, and then use\nstate-of-the-art voice cloning models to deliver the content in the desired\nauthor's voice. We consider a structured document such as a research paper to\nbe the content that has to be presented. The research paper is first summarized\nusing BERT summarization techniques and condensed into bullet points that go\ninto the slides. Tacotron inspired architecture with Encoder, Synthesizer, and\na Generative Adversarial Network (GAN) based vocoder, is used to convey the\ncontents of the slides in the author's voice (or any customized voice). Almost\nall learning has now been shifted to online mode, and professionals are now\nworking from the comfort of their homes. Due to the current situation, teachers\nand professionals have shifted to presentations to help them in imparting\ninformation. In this paper, we aim to reduce the considerable amount of time\nthat is taken in creating a presentation by automating this process and\nsubsequently delivering this presentation in a customized voice, using a\ncontent delivery mechanism that can clone any voice using a short audio clip.",
          "link": "http://arxiv.org/abs/2106.14213",
          "publishedOn": "2021-06-29T01:55:13.143Z",
          "wordCount": 783,
          "title": "AI based Presentation Creator With Customized Audio Content Delivery. (arXiv:2106.14213v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.11347",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gain_B/0/1/0/all/0/1\">Baban Gain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bandyopadhyay_D/0/1/0/all/0/1\">Dibyanayan Bandyopadhyay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+De_A/0/1/0/all/0/1\">Arkadipta De</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saikh_T/0/1/0/all/0/1\">Tanik Saikh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ekbal_A/0/1/0/all/0/1\">Asif Ekbal</a>",
          "description": "In this article, we present a description of our systems as a part of our\nparticipation in the shared task namely Artificial Intelligence for Legal\nAssistance (AILA 2019). This is an integral event of Forum for Information\nRetrieval Evaluation-2019. The outcomes of this track would be helpful for the\nautomation of the working process of the Indian Judiciary System. The manual\nworking procedures and documentation at any level (from lower to higher court)\nof the judiciary system are very complex in nature. The systems produced as a\npart of this track would assist the law practitioners. It would be helpful for\ncommon men too. This kind of track also opens the path of research of Natural\nLanguage Processing (NLP) in the judicial domain. This track defined two\nproblems such as Task 1: Identifying relevant prior cases for a given situation\nand Task 2: Identifying the most relevant statutes for a given situation. We\ntackled both of them. Our proposed approaches are based on BM25 and Doc2Vec. As\nper the results declared by the task organizers, we are in 3rd and a modest\nposition in Task 1 and Task 2 respectively.",
          "link": "http://arxiv.org/abs/2105.11347",
          "publishedOn": "2021-06-29T01:55:13.121Z",
          "wordCount": 669,
          "title": "IITP at AILA 2019: System Report for Artificial Intelligence for Legal Assistance Shared Task. (arXiv:2105.11347v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14174",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rahmani_S/0/1/0/all/0/1\">Sana Rahmani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hosseini_S/0/1/0/all/0/1\">Saeid Hosseini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zall_R/0/1/0/all/0/1\">Raziyeh Zall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kangavari_M/0/1/0/all/0/1\">Mohammad Reza Kangavari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamran_S/0/1/0/all/0/1\">Sara Kamran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_W/0/1/0/all/0/1\">Wen Hua</a>",
          "description": "Multimodal sentiment analysis benefits various applications such as\nhuman-computer interaction and recommendation systems. It aims to infer the\nusers' bipolar ideas using visual, textual, and acoustic signals. Although\nresearchers affirm the association between cognitive cues and emotional\nmanifestations, most of the current multimodal approaches in sentiment analysis\ndisregard user-specific aspects. To tackle this issue, we devise a novel method\nto perform multimodal sentiment prediction using cognitive cues, such as\npersonality. Our framework constructs an adaptive tree by hierarchically\ndividing users and trains the LSTM-based submodels, utilizing an\nattention-based fusion to transfer cognitive-oriented knowledge within the\ntree. Subsequently, the framework consumes the conclusive agglomerative\nknowledge from the adaptive tree to predict final sentiments. We also devise a\ndynamic dropout method to facilitate data sharing between neighboring nodes,\nreducing data sparsity. The empirical results on real-world datasets determine\nthat our proposed model for sentiment prediction can surpass trending rivals.\nMoreover, compared to other ensemble approaches, the proposed transfer-based\nalgorithm can better utilize the latent cognitive cues and foster the\nprediction outcomes. Based on the given extrinsic and intrinsic analysis\nresults, we note that compared to other theoretical-based techniques, the\nproposed hierarchical clustering approach can better group the users within the\nadaptive tree.",
          "link": "http://arxiv.org/abs/2106.14174",
          "publishedOn": "2021-06-29T01:55:13.112Z",
          "wordCount": 664,
          "title": "Transfer-based adaptive tree for multimodal sentiment analysis based on user latent aspects. (arXiv:2106.14174v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14269",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Shuo Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jianxi Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jie Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Magee_C/0/1/0/all/0/1\">Christopher L. Magee</a>",
          "description": "In large technology companies, the requirements for managing and organizing\ntechnical documents created by engineers and managers in supporting relevant\ndecision making have increased dramatically in recent years, which has led to a\nhigher demand for more scalable, accurate, and automated document\nclassification. Prior studies have primarily focused on processing text for\nclassification and small-scale databases. This paper describes a novel\nmultimodal deep learning architecture, called TechDoc, for technical document\nclassification, which utilizes both natural language and descriptive images to\ntrain hierarchical classifiers. The architecture synthesizes convolutional\nneural networks and recurrent neural networks through an integrated training\nprocess. We applied the architecture to a large multimodal technical document\ndatabase and trained the model for classifying documents based on the\nhierarchical International Patent Classification system. Our results show that\nthe trained neural network presents a greater classification accuracy than\nthose using a single modality and several earlier text classification methods.\nThe trained model can potentially be scaled to millions of real-world technical\ndocuments with both text and figures, which is useful for data and knowledge\nmanagement in large technology companies and organizations.",
          "link": "http://arxiv.org/abs/2106.14269",
          "publishedOn": "2021-06-29T01:55:13.104Z",
          "wordCount": 625,
          "title": "Deep Learning for Technical Document Classification. (arXiv:2106.14269v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14388",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qian_T/0/1/0/all/0/1\">Tieyun Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yile Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xuan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_K/0/1/0/all/0/1\">Ke Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Z/0/1/0/all/0/1\">Zhiyong Peng</a>",
          "description": "One key property in recommender systems is the long-tail distribution in\nuser-item interactions where most items only have few user feedback. Improving\nthe recommendation of tail items can promote novelty and bring positive effects\nto both users and providers, and thus is a desirable property of recommender\nsystems. Current novel recommendation studies over-emphasize the importance of\ntail items without differentiating the degree of users' intent on popularity\nand often incur a sharp decline of accuracy. Moreover, none of existing methods\nhas ever taken the extreme case of tail items, i.e., cold-start items without\nany interaction, into consideration.\n\nIn this work, we first disclose the mechanism that drives a user's\ninteraction towards popular or niche items by disentangling her intent into\nconformity influence (popularity) and personal interests (preference). We then\npresent a unified end-to-end framework to simultaneously optimize accuracy and\nnovelty targets based on the disentangled intent of popularity and that of\npreference. We further develop a new paradigm for novel recommendation of\ncold-start items which exploits the self-supervised learning technique to model\nthe correlation between collaborative features and content features. We conduct\nextensive experimental results on three real-world datasets. The results\ndemonstrate that our proposed model yields significant improvements over the\nstate-of-the-art baselines in terms of accuracy, novelty, coverage, and\ntrade-off.",
          "link": "http://arxiv.org/abs/2106.14388",
          "publishedOn": "2021-06-29T01:55:13.094Z",
          "wordCount": 644,
          "title": "Intent Disentanglement and Feature Self-supervision for Novel Recommendation. (arXiv:2106.14388v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2006.04279",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Boratto_L/0/1/0/all/0/1\">Ludovico Boratto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fenu_G/0/1/0/all/0/1\">Gianni Fenu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marras_M/0/1/0/all/0/1\">Mirko Marras</a>",
          "description": "Considering the impact of recommendations on item providers is one of the\nduties of multi-sided recommender systems. Item providers are key stakeholders\nin online platforms, and their earnings and plans are influenced by the\nexposure their items receive in recommended lists. Prior work showed that\ncertain minority groups of providers, characterized by a common sensitive\nattribute (e.g., gender or race), are being disproportionately affected by\nindirect and unintentional discrimination. Our study in this paper handles a\nsituation where ($i$) the same provider is associated with multiple items of a\nlist suggested to a user, ($ii$) an item is created by more than one provider\njointly, and ($iii$) predicted user-item relevance scores are biasedly\nestimated for items of provider groups. Under this scenario, we assess\ndisparities in relevance, visibility, and exposure, by simulating diverse\nrepresentations of the minority group in the catalog and the interactions.\nBased on emerged unfair outcomes, we devise a treatment that combines\nobservation upsampling and loss regularization, while learning user-item\nrelevance scores. Experiments on real-world data demonstrate that our treatment\nleads to lower disparate relevance. The resulting recommended lists show fairer\nvisibility and exposure, higher minority item coverage, and negligible loss in\nrecommendation utility.",
          "link": "http://arxiv.org/abs/2006.04279",
          "publishedOn": "2021-06-29T01:55:13.076Z",
          "wordCount": 683,
          "title": "Interplay between Upsampling and Regularization for Provider Fairness in Recommender Systems. (arXiv:2006.04279v3 [cs.IR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14072",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Makhortykh_M/0/1/0/all/0/1\">Mykola Makhortykh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Urman_A/0/1/0/all/0/1\">Aleksandra Urman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ulloa_R/0/1/0/all/0/1\">Roberto Ulloa</a>",
          "description": "Web search engines influence perception of social reality by filtering and\nranking information. However, their outputs are often subjected to bias that\ncan lead to skewed representation of subjects such as professional occupations\nor gender. In our paper, we use a mixed-method approach to investigate presence\nof race and gender bias in representation of artificial intelligence (AI) in\nimage search results coming from six different search engines. Our findings\nshow that search engines prioritize anthropomorphic images of AI that portray\nit as white, whereas non-white images of AI are present only in non-Western\nsearch engines. By contrast, gender representation of AI is more diverse and\nless skewed towards a specific gender that can be attributed to higher\nawareness about gender bias in search outputs. Our observations indicate both\nthe the need and the possibility for addressing bias in representation of\nsocietally relevant subjects, such as technological innovation, and emphasize\nthe importance of designing new approaches for detecting bias in information\nretrieval systems.",
          "link": "http://arxiv.org/abs/2106.14072",
          "publishedOn": "2021-06-29T01:55:13.044Z",
          "wordCount": 627,
          "title": "Detecting race and gender bias in visual representation of AI on web search engines. (arXiv:2106.14072v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14463",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Saahil Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_A/0/1/0/all/0/1\">Ashwin Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saporta_A/0/1/0/all/0/1\">Adriel Saporta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Truong_S/0/1/0/all/0/1\">Steven QH Truong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duong_D/0/1/0/all/0/1\">Du Nguyen Duong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1\">Tan Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chambon_P/0/1/0/all/0/1\">Pierre Chambon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lungren_M/0/1/0/all/0/1\">Matthew P. Lungren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_A/0/1/0/all/0/1\">Andrew Y. Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Langlotz_C/0/1/0/all/0/1\">Curtis P. Langlotz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajpurkar_P/0/1/0/all/0/1\">Pranav Rajpurkar</a>",
          "description": "Extracting structured clinical information from free-text radiology reports\ncan enable the use of radiology report information for a variety of critical\nhealthcare applications. In our work, we present RadGraph, a dataset of\nentities and relations in full-text chest X-ray radiology reports based on a\nnovel information extraction schema we designed to structure radiology reports.\nWe release a development dataset, which contains board-certified radiologist\nannotations for 500 radiology reports from the MIMIC-CXR dataset (14,579\nentities and 10,889 relations), and a test dataset, which contains two\nindependent sets of board-certified radiologist annotations for 100 radiology\nreports split equally across the MIMIC-CXR and CheXpert datasets. Using these\ndatasets, we train and test a deep learning model, RadGraph Benchmark, that\nachieves a micro F1 of 0.82 and 0.73 on relation extraction on the MIMIC-CXR\nand CheXpert test sets respectively. Additionally, we release an inference\ndataset, which contains annotations automatically generated by RadGraph\nBenchmark across 220,763 MIMIC-CXR reports (around 6 million entities and 4\nmillion relations) and 500 CheXpert reports (13,783 entities and 9,908\nrelations) with mappings to associated chest radiographs. Our freely available\ndataset can facilitate a wide range of research in medical natural language\nprocessing, as well as computer vision and multi-modal learning when linked to\nchest radiographs.",
          "link": "http://arxiv.org/abs/2106.14463",
          "publishedOn": "2021-06-29T01:55:13.015Z",
          "wordCount": 674,
          "title": "RadGraph: Extracting Clinical Entities and Relations from Radiology Reports. (arXiv:2106.14463v1 [cs.CL])"
        }
      ]
    },
    {
      "title": "cs.MM updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.MM",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2107.01175",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Su Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yi Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Ziquan Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_C/0/1/0/all/0/1\">Cuntai Guan</a>",
          "description": "We propose an audio-visual spatial-temporal deep neural network with: (1) a\nvisual block containing a pretrained 2D-CNN followed by a temporal\nconvolutional network (TCN); (2) an aural block containing several parallel\nTCNs; and (3) a leader-follower attentive fusion block combining the\naudio-visual information. The TCN with large history coverage enables our model\nto exploit spatial-temporal information within a much larger window length\n(i.e., 300) than that from the baseline and state-of-the-art methods (i.e., 36\nor 48). The fusion block emphasizes the visual modality while exploits the\nnoisy aural modality using the inter-modality attention mechanism. To make full\nuse of the data and alleviate over-fitting, cross-validation is carried out on\nthe training and validation set. The concordance correlation coefficient (CCC)\ncentering is used to merge the results from each fold. On the development set,\nthe achieved CCC is 0.410 for valence and 0.661 for arousal, which\nsignificantly outperforms the baseline method with the corresponding CCC of\n0.210 and 0.230 for valence and arousal, respectively. The code is available at\nhttps://github.com/sucv/ABAW2.",
          "link": "http://arxiv.org/abs/2107.01175",
          "publishedOn": "2021-07-05T01:54:56.329Z",
          "wordCount": 611,
          "title": "Audio-visual Attentive Fusion for Continuous Emotion Recognition. (arXiv:2107.01175v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00938",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+de_Lima_Santos_M/0/1/0/all/0/1\">Mathias-Felipe de-Lima-Santos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kooli_A/0/1/0/all/0/1\">Arwa Kooli</a>",
          "description": "News outlets are developing formats dedicated to social platforms that\ncapture audience attention, such as Instagram stories, Facebook Instant\narticles, and YouTube videos. In some cases, these formats are created in\ncollaboration with the tech companies themselves. At the same time, the use of\ndata-driven storytelling is becoming increasingly integrated into the\never-complex business models of news outlets, generating more impact and\nvisibility. Previous studies have focused on studying these two effects\nseparately. To address this gap in the literature, this paper identifies and\nanalyzes the use of data journalism on the Instagram content of AJ Labs, the\nteam dedicated to producing data-driven and interactive stories for the Al\nJazeera news network. Drawing upon a mixed-method approach, this study examines\nthe use and characteristics of data stories on social media platforms. Results\nsuggest that there is reliance on producing visual content that covers topics\nsuch as politics and violence. In general, AJ Labs relies on the use of\ninfographics and produces its own unique data. To conclude, this paper suggests\npotential ways to improve the use of Instagram to tell data stories.",
          "link": "http://arxiv.org/abs/2107.00938",
          "publishedOn": "2021-07-05T01:54:56.294Z",
          "wordCount": 635,
          "title": "Instagrammable Data: Using Visuals to Showcase More Than Numbers on AJ Labs Instagram Page. (arXiv:2107.00938v1 [cs.CY])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00650",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Narasimhan_M/0/1/0/all/0/1\">Medhini Narasimhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rohrbach_A/0/1/0/all/0/1\">Anna Rohrbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1\">Trevor Darrell</a>",
          "description": "A generic video summary is an abridged version of a video that conveys the\nwhole story and features the most important scenes. Yet the importance of\nscenes in a video is often subjective, and users should have the option of\ncustomizing the summary by using natural language to specify what is important\nto them. Further, existing models for fully automatic generic summarization\nhave not exploited available language models, which can serve as an effective\nprior for saliency. This work introduces CLIP-It, a single framework for\naddressing both generic and query-focused video summarization, typically\napproached separately in the literature. We propose a language-guided\nmultimodal transformer that learns to score frames in a video based on their\nimportance relative to one another and their correlation with a user-defined\nquery (for query-focused summarization) or an automatically generated dense\nvideo caption (for generic video summarization). Our model can be extended to\nthe unsupervised setting by training without ground-truth supervision. We\noutperform baselines and prior work by a significant margin on both standard\nvideo summarization datasets (TVSum and SumMe) and a query-focused video\nsummarization dataset (QFVS). Particularly, we achieve large improvements in\nthe transfer setting, attesting to our method's strong generalization\ncapabilities.",
          "link": "http://arxiv.org/abs/2107.00650",
          "publishedOn": "2021-07-02T01:57:58.543Z",
          "wordCount": 635,
          "title": "CLIP-It! Language-Guided Video Summarization. (arXiv:2107.00650v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00328",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shurun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shiqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1\">Yan Ye</a>",
          "description": "The research of visual signal compression has a long history. Fueled by deep\nlearning, exciting progress has been made recently. Despite achieving better\ncompression performance, existing end-to-end compression algorithms are still\ndesigned towards better signal quality in terms of rate-distortion\noptimization. In this paper, we show that the design and optimization of\nnetwork architecture could be further improved for compression towards machine\nvision. We propose an inverted bottleneck structure for end-to-end compression\ntowards machine vision, which specifically accounts for efficient\nrepresentation of the semantic information. Moreover, we quest the capability\nof optimization by incorporating the analytics accuracy into the optimization\nprocess, and the optimality is further explored with generalized rate-accuracy\noptimization in an iterative manner. We use object detection as a showcase for\nend-to-end compression towards machine vision, and extensive experiments show\nthat the proposed scheme achieves significant BD-rate savings in terms of\nanalysis performance. Moreover, the promise of the scheme is also demonstrated\nwith strong generalization capability towards other machine vision tasks, due\nto the enabling of signal-level reconstruction.",
          "link": "http://arxiv.org/abs/2107.00328",
          "publishedOn": "2021-07-02T01:57:58.517Z",
          "wordCount": 619,
          "title": "End-to-end Compression Towards Machine Vision: Network Architecture Design and Optimization. (arXiv:2107.00328v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00648",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Braman_N/0/1/0/all/0/1\">Nathaniel Braman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gordon_J/0/1/0/all/0/1\">Jacob W. H. Gordon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goossens_E/0/1/0/all/0/1\">Emery T. Goossens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Willis_C/0/1/0/all/0/1\">Caleb Willis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stumpe_M/0/1/0/all/0/1\">Martin C. Stumpe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venkataraman_J/0/1/0/all/0/1\">Jagadish Venkataraman</a>",
          "description": "Clinical decision-making in oncology involves multimodal data such as\nradiology scans, molecular profiling, histopathology slides, and clinical\nfactors. Despite the importance of these modalities individually, no deep\nlearning framework to date has combined them all to predict patient prognosis.\nHere, we predict the overall survival (OS) of glioma patients from diverse\nmultimodal data with a Deep Orthogonal Fusion (DOF) model. The model learns to\ncombine information from multiparametric MRI exams, biopsy-based modalities\n(such as H&E slide images and/or DNA sequencing), and clinical variables into a\ncomprehensive multimodal risk score. Prognostic embeddings from each modality\nare learned and combined via attention-gated tensor fusion. To maximize the\ninformation gleaned from each modality, we introduce a multimodal\northogonalization (MMO) loss term that increases model performance by\nincentivizing constituent embeddings to be more complementary. DOF predicts OS\nin glioma patients with a median C-index of 0.788 +/- 0.067, significantly\noutperforming (p=0.023) the best performing unimodal model with a median\nC-index of 0.718 +/- 0.064. The prognostic model significantly stratifies\nglioma patients by OS within clinical subsets, adding further granularity to\nprognostic clinical grading and molecular subtyping.",
          "link": "http://arxiv.org/abs/2107.00648",
          "publishedOn": "2021-07-02T01:57:58.479Z",
          "wordCount": 659,
          "title": "Deep Orthogonal Fusion: Multimodal Prognostic Biomarker Discovery Integrating Radiology, Pathology, Genomic, and Clinical Data. (arXiv:2107.00648v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2008.09883",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tahir_G/0/1/0/all/0/1\">Ghalib Ahmed Tahir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loo_C/0/1/0/all/0/1\">Chu Kiong Loo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moy_F/0/1/0/all/0/1\">Foong Ming Moy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_N/0/1/0/all/0/1\">Nadine Kong</a>",
          "description": "Obesity is known to lower the quality of life substantially. It is often\nassociated with increased chances of non-communicable diseases such as\ndiabetes, cardiovascular problems, different types of cancers, etc. Evidence\nsuggests that diet-related mobile applications play a vital role in assisting\nan individual in making healthier choices and keeping track of food intake.\nHowever, due to an abundance of similar applications, it becomes pertinent to\nevaluate each of them in terms of functionality, usability, and possible design\nissues to truly determine state-of-the-art solutions for the future. Since\nthese applications involve implementing multiple user requirements and\nrecommendations from different dietitians, the evaluation becomes quite\ncomplex. Therefore, this study aims to review existing dietary applications at\nlength to highlight key features and problems that enhance or undermine an\napplication's usability. For this purpose, we have examined the published\nliterature from various scientific databases of the CINAHL, Science Direct, and\nPUBMED. Out of our findings, fifty-six primary studies met our inclusion\ncriteria after filtering out titles, abstracts, and full text. A total of 35\napps are analyzed from the selected studies. Our detailed analysis concluded\nthe comprehensiveness of freely available mHealth applications from users and\ndietitians' frames of reference. Furthermore, we have also specified potential\nfuture challenges and stated recommendations to help develop clinically\naccurate diet-related applications.",
          "link": "http://arxiv.org/abs/2008.09883",
          "publishedOn": "2021-07-01T01:59:31.601Z",
          "wordCount": 712,
          "title": "A Systematic Literature Review of Critical Features and General Issues of Freely Available mHealth Apps For Dietary Assessment. (arXiv:2008.09883v3 [cs.CY] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.12148",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jun Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Donglin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shu_Z/0/1/0/all/0/1\">Zhenqiu Shu</a>",
          "description": "Hashing plays an important role in information retrieval, due to its low\nstorage and high speed of processing. Among the techniques available in the\nliterature, multi-modal hashing, which can encode heterogeneous multi-modal\nfeatures into compact hash codes, has received particular attention. Most of\nthe existing multi-modal hashing methods adopt the fixed weighting factors to\nfuse multiple modalities for any query data, which cannot capture the variation\nof different queries. Besides, many methods introduce hyper-parameters to\nbalance many regularization terms that make the optimization harder. Meanwhile,\nit is time-consuming and labor-intensive to set proper parameter values. The\nlimitations may significantly hinder their promotion in real applications. In\nthis paper, we propose a simple, yet effective method that is inspired by the\nHadamard matrix. The proposed method captures the multi-modal feature\ninformation in an adaptive manner and preserves the discriminative semantic\ninformation in the hash codes. Our framework is flexible and involves a very\nfew hyper-parameters. Extensive experimental results show the method is\neffective and achieves superior performance compared to state-of-the-art\nalgorithms.",
          "link": "http://arxiv.org/abs/2009.12148",
          "publishedOn": "2021-07-01T01:59:31.469Z",
          "wordCount": 641,
          "title": "Adaptive Multi-modal Fusion Hashing via Hadamard Matrix. (arXiv:2009.12148v3 [cs.MM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.09199",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rouditchenko_A/0/1/0/all/0/1\">Andrew Rouditchenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boggust_A/0/1/0/all/0/1\">Angie Boggust</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harwath_D/0/1/0/all/0/1\">David Harwath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Brian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_D/0/1/0/all/0/1\">Dhiraj Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomas_S/0/1/0/all/0/1\">Samuel Thomas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Audhkhasi_K/0/1/0/all/0/1\">Kartik Audhkhasi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuehne_H/0/1/0/all/0/1\">Hilde Kuehne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panda_R/0/1/0/all/0/1\">Rameswar Panda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feris_R/0/1/0/all/0/1\">Rogerio Feris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kingsbury_B/0/1/0/all/0/1\">Brian Kingsbury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Picheny_M/0/1/0/all/0/1\">Michael Picheny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torralba_A/0/1/0/all/0/1\">Antonio Torralba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glass_J/0/1/0/all/0/1\">James Glass</a>",
          "description": "Current methods for learning visually grounded language from videos often\nrely on text annotation, such as human generated captions or machine generated\nautomatic speech recognition (ASR) transcripts. In this work, we introduce the\nAudio-Video Language Network (AVLnet), a self-supervised network that learns a\nshared audio-visual embedding space directly from raw video inputs. To\ncircumvent the need for text annotation, we learn audio-visual representations\nfrom randomly segmented video clips and their raw audio waveforms. We train\nAVLnet on HowTo100M, a large corpus of publicly available instructional videos,\nand evaluate on image retrieval and video retrieval tasks, achieving\nstate-of-the-art performance. We perform analysis of AVLnet's learned\nrepresentations, showing our model utilizes speech and natural sounds to learn\naudio-visual concepts. Further, we propose a tri-modal model that jointly\nprocesses raw audio, video, and text captions from videos to learn a\nmulti-modal semantic embedding space useful for text-video retrieval. Our code,\ndata, and trained models will be released at avlnet.csail.mit.edu",
          "link": "http://arxiv.org/abs/2006.09199",
          "publishedOn": "2021-07-01T01:59:31.361Z",
          "wordCount": 675,
          "title": "AVLnet: Learning Audio-Visual Language Representations from Instructional Videos. (arXiv:2006.09199v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2004.13274",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_P/0/1/0/all/0/1\">Prasanta Bhattacharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_R/0/1/0/all/0/1\">Raj Kumar Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yinping Yang</a>",
          "description": "Emotional expressions form a key part of user behavior on today's digital\nplatforms. While multimodal emotion recognition techniques are gaining research\nattention, there is a lack of deeper understanding on how visual and non-visual\nfeatures can be used to better recognize emotions in certain contexts, but not\nothers. This study analyzes the interplay between the effects of multimodal\nemotion features derived from facial expressions, tone and text in conjunction\nwith two key contextual factors: i) gender of the speaker, and ii) duration of\nthe emotional episode. Using a large public dataset of 2,176 manually annotated\nYouTube videos, we found that while multimodal features consistently\noutperformed bimodal and unimodal features, their performance varied\nsignificantly across different emotions, gender and duration contexts.\nMultimodal features performed particularly better for male speakers in\nrecognizing most emotions. Furthermore, multimodal features performed\nparticularly better for shorter than for longer videos in recognizing neutral\nand happiness, but not sadness and anger. These findings offer new insights\ntowards the development of more context-aware emotion recognition and\nempathetic systems.",
          "link": "http://arxiv.org/abs/2004.13274",
          "publishedOn": "2021-07-01T01:59:31.319Z",
          "wordCount": 668,
          "title": "Exploring the contextual factors affecting multimodal emotion recognition in videos. (arXiv:2004.13274v5 [cs.MM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.16036",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Verma_P/0/1/0/all/0/1\">Prateek Verma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chafe_C/0/1/0/all/0/1\">Chris Chafe</a>",
          "description": "This paper proposes a novel way of doing audio synthesis at the waveform\nlevel using Transformer architectures. We propose a deep neural network for\ngenerating waveforms, similar to wavenet \\cite{oord2016wavenet}. This is fully\nprobabilistic, auto-regressive, and causal, i.e. each sample generated depends\nonly on the previously observed samples. Our approach outperforms a widely used\nwavenet architecture by up to 9\\% on a similar dataset for predicting the next\nstep. Using the attention mechanism, we enable the architecture to learn which\naudio samples are important for the prediction of the future sample. We show\nhow causal transformer generative models can be used for raw waveform\nsynthesis. We also show that this performance can be improved by another 2\\% by\nconditioning samples over a wider context. The flexibility of the current model\nto synthesize audio from latent representations suggests a large number of\npotential applications. The novel approach of using generative transformer\narchitectures for raw audio synthesis is, however, still far away from\ngenerating any meaningful music, without using latent codes/meta-data to aid\nthe generation process.",
          "link": "http://arxiv.org/abs/2106.16036",
          "publishedOn": "2021-07-01T01:59:31.307Z",
          "wordCount": 620,
          "title": "A Generative Model for Raw Audio Using Transformer Architectures. (arXiv:2106.16036v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15989",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Maruyama_M/0/1/0/all/0/1\">Mizuki Maruyama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghose_S/0/1/0/all/0/1\">Shuvozit Ghose</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Inoue_K/0/1/0/all/0/1\">Katsufumi Inoue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_P/0/1/0/all/0/1\">Partha Pratim Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iwamura_M/0/1/0/all/0/1\">Masakazu Iwamura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoshioka_M/0/1/0/all/0/1\">Michifumi Yoshioka</a>",
          "description": "In recent years, Word-level Sign Language Recognition (WSLR) research has\ngained popularity in the computer vision community, and thus various approaches\nhave been proposed. Among these approaches, the method using I3D network\nachieves the highest recognition accuracy on large public datasets for WSLR.\nHowever, the method with I3D only utilizes appearance information of the upper\nbody of the signers to recognize sign language words. On the other hand, in\nWSLR, the information of local regions, such as the hand shape and facial\nexpression, and the positional relationship among the body and both hands are\nimportant. Thus in this work, we utilized local region images of both hands and\nface, along with skeletal information to capture local information and the\npositions of both hands relative to the body, respectively. In other words, we\npropose a novel multi-stream WSLR framework, in which a stream with local\nregion images and a stream with skeletal information are introduced by\nextending I3D network to improve the recognition accuracy of WSLR. From the\nexperimental results on WLASL dataset, it is evident that the proposed method\nhas achieved about 15% improvement in the Top-1 accuracy than the existing\nconventional methods.",
          "link": "http://arxiv.org/abs/2106.15989",
          "publishedOn": "2021-07-01T01:59:31.271Z",
          "wordCount": 645,
          "title": "Word-level Sign Language Recognition with Multi-stream Neural Networks Focusing on Local Regions. (arXiv:2106.15989v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16125",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Sicheng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_X/0/1/0/all/0/1\">Xingxu Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jufeng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_G/0/1/0/all/0/1\">Guoli Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_G/0/1/0/all/0/1\">Guiguang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1\">Tat-Seng Chua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuller_B/0/1/0/all/0/1\">Bj&#xf6;rn W. Schuller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keutzer_K/0/1/0/all/0/1\">Kurt Keutzer</a>",
          "description": "Images can convey rich semantics and induce various emotions in viewers.\nRecently, with the rapid advancement of emotional intelligence and the\nexplosive growth of visual data, extensive research efforts have been dedicated\nto affective image content analysis (AICA). In this survey, we will\ncomprehensively review the development of AICA in the recent two decades,\nespecially focusing on the state-of-the-art methods with respect to three main\nchallenges -- the affective gap, perception subjectivity, and label noise and\nabsence. We begin with an introduction to the key emotion representation models\nthat have been widely employed in AICA and description of available datasets\nfor performing evaluation with quantitative comparison of label noise and\ndataset bias. We then summarize and compare the representative approaches on\n(1) emotion feature extraction, including both handcrafted and deep features,\n(2) learning methods on dominant emotion recognition, personalized emotion\nprediction, emotion distribution learning, and learning from noisy data or few\nlabels, and (3) AICA based applications. Finally, we discuss some challenges\nand promising research directions in the future, such as image content and\ncontext understanding, group emotion clustering, and viewer-image interaction.",
          "link": "http://arxiv.org/abs/2106.16125",
          "publishedOn": "2021-07-01T01:59:31.259Z",
          "wordCount": 643,
          "title": "Affective Image Content Analysis: Two Decades Review and New Perspectives. (arXiv:2106.16125v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15561",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Tan_X/0/1/0/all/0/1\">Xu Tan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qin_T/0/1/0/all/0/1\">Tao Qin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Soong_F/0/1/0/all/0/1\">Frank Soong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_T/0/1/0/all/0/1\">Tie-Yan Liu</a>",
          "description": "Text to speech (TTS), or speech synthesis, which aims to synthesize\nintelligible and natural speech given text, is a hot research topic in speech,\nlanguage, and machine learning communities and has broad applications in the\nindustry. As the development of deep learning and artificial intelligence,\nneural network-based TTS has significantly improved the quality of synthesized\nspeech in recent years. In this paper, we conduct a comprehensive survey on\nneural TTS, aiming to provide a good understanding of current research and\nfuture trends. We focus on the key components in neural TTS, including text\nanalysis, acoustic models and vocoders, and several advanced topics, including\nfast TTS, low-resource TTS, robust TTS, expressive TTS, and adaptive TTS, etc.\nWe further summarize resources related to TTS (e.g., datasets, opensource\nimplementations) and discuss future research directions. This survey can serve\nboth academic researchers and industry practitioners working on TTS.",
          "link": "http://arxiv.org/abs/2106.15561",
          "publishedOn": "2021-06-30T02:00:59.384Z",
          "wordCount": 607,
          "title": "A Survey on Neural Speech Synthesis. (arXiv:2106.15561v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2101.11530",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_P/0/1/0/all/0/1\">Pranay Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_D/0/1/0/all/0/1\">Divyanshu Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarvadevabhatla_R/0/1/0/all/0/1\">Ravi Kiran Sarvadevabhatla</a>",
          "description": "We introduce SynSE, a novel syntactically guided generative approach for\nZero-Shot Learning (ZSL). Our end-to-end approach learns progressively refined\ngenerative embedding spaces constrained within and across the involved\nmodalities (visual, language). The inter-modal constraints are defined between\naction sequence embedding and embeddings of Parts of Speech (PoS) tagged words\nin the corresponding action description. We deploy SynSE for the task of\nskeleton-based action sequence recognition. Our design choices enable SynSE to\ngeneralize compositionally, i.e., recognize sequences whose action descriptions\ncontain words not encountered during training. We also extend our approach to\nthe more challenging Generalized Zero-Shot Learning (GZSL) problem via a\nconfidence-based gating mechanism. We are the first to present zero-shot\nskeleton action recognition results on the large-scale NTU-60 and NTU-120\nskeleton action datasets with multiple splits. Our results demonstrate SynSE's\nstate of the art performance in both ZSL and GZSL settings compared to strong\nbaselines on the NTU-60 and NTU-120 datasets. The code and pretrained models\nare available at https://github.com/skelemoa/synse-zsl",
          "link": "http://arxiv.org/abs/2101.11530",
          "publishedOn": "2021-06-30T02:00:59.373Z",
          "wordCount": 646,
          "title": "Syntactically Guided Generative Embeddings for Zero-Shot Skeleton Action Recognition. (arXiv:2101.11530v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.05535",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Perov_I/0/1/0/all/0/1\">Ivan Perov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_D/0/1/0/all/0/1\">Daiheng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chervoniy_N/0/1/0/all/0/1\">Nikolay Chervoniy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1\">Kunlin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marangonda_S/0/1/0/all/0/1\">Sugasa Marangonda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ume_C/0/1/0/all/0/1\">Chris Um&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dpfks_M/0/1/0/all/0/1\">Mr. Dpfks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Facenheim_C/0/1/0/all/0/1\">Carl Shift Facenheim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+RP_L/0/1/0/all/0/1\">Luis RP</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jian Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Sheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_P/0/1/0/all/0/1\">Pingyu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Bo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weiming Zhang</a>",
          "description": "Deepfake defense not only requires the research of detection but also\nrequires the efforts of generation methods. However, current deepfake methods\nsuffer the effects of obscure workflow and poor performance. To solve this\nproblem, we present DeepFaceLab, the current dominant deepfake framework for\nface-swapping. It provides the necessary tools as well as an easy-to-use way to\nconduct high-quality face-swapping. It also offers a flexible and loose\ncoupling structure for people who need to strengthen their pipeline with other\nfeatures without writing complicated boilerplate code. We detail the principles\nthat drive the implementation of DeepFaceLab and introduce its pipeline,\nthrough which every aspect of the pipeline can be modified painlessly by users\nto achieve their customization purpose. It is noteworthy that DeepFaceLab could\nachieve cinema-quality results with high fidelity. We demonstrate the advantage\nof our system by comparing our approach with other face-swapping methods.For\nmore information, please visit:https://github.com/iperov/DeepFaceLab/.",
          "link": "http://arxiv.org/abs/2005.05535",
          "publishedOn": "2021-06-30T02:00:59.142Z",
          "wordCount": 674,
          "title": "DeepFaceLab: Integrated, flexible and extensible face-swapping framework. (arXiv:2005.05535v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15262",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pasandi_H/0/1/0/all/0/1\">Hannaneh Barahouei Pasandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nadeem_T/0/1/0/all/0/1\">Tamer Nadeem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amirpour_H/0/1/0/all/0/1\">Hadi Amirpour</a>",
          "description": "Over the last decade, the bandwidth expansion and MU-MIMO spectral efficiency\nhave promised to increase data throughput by allowing concurrent communication\nbetween one Access Point and multiple users. However, we are still a long way\nfrom enjoying such MU-MIMO MAC protocol improvements for bandwidth hungry\napplications such as video streaming in practical WiFi network settings due to\nheterogeneous channel conditions and devices, unreliable transmissions, and\nlack of useful feedback exchange among the lower and upper layers'\nrequirements. This paper introduces MuViS, a novel dual-phase optimization\nframework that proposes a Quality of Experience (QoE) aware MU-MIMO\noptimization for multi-user video streaming over IEEE 802.11ac. MuViS first\nemploys reinforcement learning to optimize the MU-MIMO user group and mode\nselection for users based on their PHY/MAC layer characteristics. The video\nbitrate is then optimized based on the user's mode (Multi-User (MU) or\nSingle-User (SU)). We present our design and its evaluation on smartphones and\nlaptops using 802.11ac WiFi. Our experimental results in various indoor\nenvironments and configurations show a scalable framework that can support a\nlarge number of users with streaming at high video rates and satisfying QoE\nrequirements.",
          "link": "http://arxiv.org/abs/2106.15262",
          "publishedOn": "2021-06-30T02:00:59.126Z",
          "wordCount": 632,
          "title": "MuViS: Online MU-MIMO Grouping for Multi-User Applications Over Commodity WiFi. (arXiv:2106.15262v1 [cs.NI])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15130",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Conti_M/0/1/0/all/0/1\">Mauro Conti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milani_S/0/1/0/all/0/1\">Simone Milani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nowroozi_E/0/1/0/all/0/1\">Ehsan Nowroozi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orazi_G/0/1/0/all/0/1\">Gabriele Orazi</a>",
          "description": "The last-generation video conferencing software allows users to utilize a\nvirtual background to conceal their personal environment due to privacy\nconcerns, especially in official meetings with other employers. On the other\nhand, users maybe want to fool people in the meeting by considering the virtual\nbackground to conceal where they are. In this case, developing tools to\nunderstand the virtual background utilize for fooling people in meeting plays\nan important role. Besides, such detectors must prove robust against different\nkinds of attacks since a malicious user can fool the detector by applying a set\nof adversarial editing steps on the video to conceal any revealing footprint.\nIn this paper, we study the feasibility of an efficient tool to detect whether\na videoconferencing user background is real. In particular, we provide the\nfirst tool which computes pixel co-occurrences matrices and uses them to search\nfor inconsistencies among spectral and spatial bands. Our experiments confirm\nthat cross co-occurrences matrices improve the robustness of the detector\nagainst different kinds of attacks. This work's performance is especially\nnoteworthy with regard to color SPAM features. Moreover, the performance\nespecially is significant with regard to robustness versus post-processing,\nlike geometric transformations, filtering, contrast enhancement, and JPEG\ncompression with different quality factors.",
          "link": "http://arxiv.org/abs/2106.15130",
          "publishedOn": "2021-06-30T02:00:59.106Z",
          "wordCount": 669,
          "title": "Do Not Deceive Your Employer with a Virtual Background: A Video Conferencing Manipulation-Detection System. (arXiv:2106.15130v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14150",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hosseini_S/0/1/0/all/0/1\">Samira Hosseini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahdavi_M/0/1/0/all/0/1\">Mojtaba Mahdavi</a>",
          "description": "Content-independent watermarks and block-wise independency can be considered\nas vulnerabilities in semi-fragile watermarking methods. In this paper to\nachieve the objectives of semi-fragile watermarking techniques, a method is\nproposed to not have the mentioned shortcomings. In the proposed method, the\nwatermark is generated by relying on image content and a key. Furthermore, the\nembedding scheme causes the watermarked blocks to become dependent on each\nother, using a key. In the embedding phase, the image is partitioned into\nnon-overlapping blocks. In order to detect and separate the different types of\nattacks more precisely, the proposed method embeds three copies of each\nwatermark bit into LWT coefficients of each 4x4 block. In the authentication\nphase, by voting between the extracted bits the error maps are created; these\nmaps indicate image authenticity and reveal the modified regions. Also, in\norder to automate the authentication, the images are classified into four\ncategories using seven features. Classification accuracy in the experiments is\n97.97 percent. It is noted that our experiments demonstrate that the proposed\nmethod is robust against JPEG compression and is competitive with a\nstate-of-the-art semi-fragile watermarking method, in terms of robustness and\nsemi-fragility.",
          "link": "http://arxiv.org/abs/2106.14150",
          "publishedOn": "2021-06-29T01:55:13.227Z",
          "wordCount": 633,
          "title": "Image content dependent semi-fragile watermarking with localized tamper detection. (arXiv:2106.14150v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14174",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rahmani_S/0/1/0/all/0/1\">Sana Rahmani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hosseini_S/0/1/0/all/0/1\">Saeid Hosseini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zall_R/0/1/0/all/0/1\">Raziyeh Zall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kangavari_M/0/1/0/all/0/1\">Mohammad Reza Kangavari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamran_S/0/1/0/all/0/1\">Sara Kamran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_W/0/1/0/all/0/1\">Wen Hua</a>",
          "description": "Multimodal sentiment analysis benefits various applications such as\nhuman-computer interaction and recommendation systems. It aims to infer the\nusers' bipolar ideas using visual, textual, and acoustic signals. Although\nresearchers affirm the association between cognitive cues and emotional\nmanifestations, most of the current multimodal approaches in sentiment analysis\ndisregard user-specific aspects. To tackle this issue, we devise a novel method\nto perform multimodal sentiment prediction using cognitive cues, such as\npersonality. Our framework constructs an adaptive tree by hierarchically\ndividing users and trains the LSTM-based submodels, utilizing an\nattention-based fusion to transfer cognitive-oriented knowledge within the\ntree. Subsequently, the framework consumes the conclusive agglomerative\nknowledge from the adaptive tree to predict final sentiments. We also devise a\ndynamic dropout method to facilitate data sharing between neighboring nodes,\nreducing data sparsity. The empirical results on real-world datasets determine\nthat our proposed model for sentiment prediction can surpass trending rivals.\nMoreover, compared to other ensemble approaches, the proposed transfer-based\nalgorithm can better utilize the latent cognitive cues and foster the\nprediction outcomes. Based on the given extrinsic and intrinsic analysis\nresults, we note that compared to other theoretical-based techniques, the\nproposed hierarchical clustering approach can better group the users within the\nadaptive tree.",
          "link": "http://arxiv.org/abs/2106.14174",
          "publishedOn": "2021-06-29T01:55:13.055Z",
          "wordCount": 664,
          "title": "Transfer-based adaptive tree for multimodal sentiment analysis based on user latent aspects. (arXiv:2106.14174v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14118",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bagchi_A/0/1/0/all/0/1\">Anurag Bagchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahmood_J/0/1/0/all/0/1\">Jazib Mahmood</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandes_D/0/1/0/all/0/1\">Dolton Fernandes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarvadevabhatla_R/0/1/0/all/0/1\">Ravi Kiran Sarvadevabhatla</a>",
          "description": "State of the art architectures for untrimmed video Temporal Action\nLocalization (TAL) have only considered RGB and Flow modalities, leaving the\ninformation-rich audio modality totally unexploited. Audio fusion has been\nexplored for the related but arguably easier problem of trimmed (clip-level)\naction recognition. However, TAL poses a unique set of challenges. In this\npaper, we propose simple but effective fusion-based approaches for TAL. To the\nbest of our knowledge, our work is the first to jointly consider audio and\nvideo modalities for supervised TAL. We experimentally show that our schemes\nconsistently improve performance for state of the art video-only TAL\napproaches. Specifically, they help achieve new state of the art performance on\nlarge-scale benchmark datasets - ActivityNet-1.3 (52.73 mAP@0.5) and THUMOS14\n(57.18 mAP@0.5). Our experiments include ablations involving multiple fusion\nschemes, modality combinations and TAL architectures. Our code, models and\nassociated data will be made available.",
          "link": "http://arxiv.org/abs/2106.14118",
          "publishedOn": "2021-06-29T01:55:13.025Z",
          "wordCount": 593,
          "title": "Hear Me Out: Fusional Approaches for Audio Augmented Temporal Action Localization. (arXiv:2106.14118v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14136",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Haoyu Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jihua Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Q/0/1/0/all/0/1\">Qinghai Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1\">Zhiyong Cheng</a>",
          "description": "In this paper, we address the text-to-audio grounding issue, namely,\ngrounding the segments of the sound event described by a natural language query\nin the untrimmed audio. This is a newly proposed but challenging audio-language\ntask, since it requires to not only precisely localize all the on- and off-sets\nof the desired segments in the audio, but to perform comprehensive acoustic and\nlinguistic understandings and reason the multimodal interactions between the\naudio and query. To tackle those problems, the existing method treats the query\nholistically as a single unit by a global query representation, which fails to\nhighlight the keywords that contain rich semantics. Besides, this method has\nnot fully exploited interactions between the query and audio. Moreover, since\nthe audio and queries are arbitrary and variable in length, many meaningless\nparts of them are not filtered out in this method, which hinders the grounding\nof the desired segments.\n\nTo this end, we propose a novel Query Graph with Cross-gating Attention\n(QGCA) model, which models the comprehensive relations between the words in\nquery through a novel query graph. Besides, to capture the fine-grained\ninteractions between audio and query, a cross-modal attention module that\nassigns higher weights to the keywords is introduced to generate the\nsnippet-specific query representations. Finally, we also design a cross-gating\nmodule to emphasize the crucial parts as well as weaken the irrelevant ones in\nthe audio and query. We extensively evaluate the proposed QGCA model on the\npublic Audiogrounding dataset with significant improvements over several\nstate-of-the-art methods. Moreover, further ablation study shows the consistent\neffectiveness of different modules in the proposed QGCA model.",
          "link": "http://arxiv.org/abs/2106.14136",
          "publishedOn": "2021-06-29T01:55:12.995Z",
          "wordCount": 707,
          "title": "Query-graph with Cross-gating Attention Model for Text-to-Audio Grounding. (arXiv:2106.14136v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14076",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhihua Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1\">Zhiri Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zekuang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiangguo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yuming Fang</a>",
          "description": "Nowadays, most existing blind image quality assessment (BIQA) models 1) are\ndeveloped for synthetically-distorted images and often generalize poorly to\nauthentic ones; 2) heavily rely on human ratings, which are prohibitively\nlabor-expensive to collect. Here, we propose an $opinion$-$free$ BIQA method\nthat learns from synthetically-distorted images and multiple agents to assess\nthe perceptual quality of authentically-distorted ones captured in the wild\nwithout relying on human labels. Specifically, we first assemble a large number\nof image pairs from synthetically-distorted images and use a set of\nfull-reference image quality assessment (FR-IQA) models to assign pseudo-binary\nlabels of each pair indicating which image has higher quality as the\nsupervisory signal. We then train a convolutional neural network (CNN)-based\nBIQA model to rank the perceptual quality, optimized for consistency with the\nbinary labels. Since there exists domain shift between the synthetically- and\nauthentically-distorted images, an unsupervised domain adaptation (UDA) module\nis introduced to alleviate this issue. Extensive experiments demonstrate the\neffectiveness of our proposed $opinion$-$free$ BIQA model, yielding\nstate-of-the-art performance in terms of correlation with human opinion scores,\nas well as gMAD competition. Codes will be made publicly available upon\nacceptance.",
          "link": "http://arxiv.org/abs/2106.14076",
          "publishedOn": "2021-06-29T01:55:12.531Z",
          "wordCount": 633,
          "title": "Learning from Synthetic Data for Opinion-free Blind Image Quality Assessment in the Wild. (arXiv:2106.14076v1 [cs.MM])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13921",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zytko_D/0/1/0/all/0/1\">Douglas Zytko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zexin Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gleason_J/0/1/0/all/0/1\">Jacob Gleason</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lundquist_N/0/1/0/all/0/1\">Nathaniel Lundquist</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taylor_M/0/1/0/all/0/1\">Medina Taylor</a>",
          "description": "Immersive stories for health are 360-degree videos that intend to alter\nviewer perceptions about behaviors detrimental to health. They have potential\nto inform public health at scale, however, immersive story design is still in\nearly stages and largely devoid of best practices. This paper presents a focus\ngroup study with 147 viewers of an immersive story about binge drinking\nexperienced through VR headsets and mobile phones. The objective of the study\nis to identify aspects of immersive story design that influence attitudes\ntowards the health issue exhibited, and to understand how health information is\nconsumed in immersive stories. Findings emphasize the need for an immersive\nstory to provide reasoning behind character engagement in the focal health\nbehavior, to show the main character clearly engaging in the behavior, and to\nenable viewers to experience escalating symptoms of the behavior before the\npenultimate health consequence. Findings also show how the design of supporting\ncharacters can inadvertently distract viewers and lead them to justify the\ndetrimental behavior being exhibited. The paper concludes with design\nconsiderations for enabling immersive stories to better inform public\nperception of health issues.",
          "link": "http://arxiv.org/abs/2106.13921",
          "publishedOn": "2021-06-29T01:55:12.509Z",
          "wordCount": 652,
          "title": "Immersive Stories for Health Information: Design Considerations from Binge Drinking in VR. (arXiv:2106.13921v1 [cs.HC])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14014",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Tandon_P/0/1/0/all/0/1\">Pulkit Tandon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chandak_S/0/1/0/all/0/1\">Shubham Chandak</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pataranutaporn_P/0/1/0/all/0/1\">Pat Pataranutaporn</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1\">Yimeng Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mapuranga_A/0/1/0/all/0/1\">Anesu M. Mapuranga</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Maes_P/0/1/0/all/0/1\">Pattie Maes</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Weissman_T/0/1/0/all/0/1\">Tsachy Weissman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sra_M/0/1/0/all/0/1\">Misha Sra</a>",
          "description": "Video represents the majority of internet traffic today leading to a\ncontinuous technological arms race between generating higher quality content,\ntransmitting larger file sizes and supporting network infrastructure. Adding to\nthis is the recent COVID-19 pandemic fueled surge in the use of video\nconferencing tools. Since videos take up substantial bandwidth (~100 Kbps to\nfew Mbps), improved video compression can have a substantial impact on network\nperformance for live and pre-recorded content, providing broader access to\nmultimedia content worldwide. In this work, we present a novel video\ncompression pipeline, called Txt2Vid, which substantially reduces data\ntransmission rates by compressing webcam videos (\"talking-head videos\") to a\ntext transcript. The text is transmitted and decoded into a realistic\nreconstruction of the original video using recent advances in deep learning\nbased voice cloning and lip syncing models. Our generative pipeline achieves\ntwo to three orders of magnitude reduction in the bitrate as compared to the\nstandard audio-video codecs (encoders-decoders), while maintaining equivalent\nQuality-of-Experience based on a subjective evaluation by users (n=242) in an\nonline study. The code for this work is available at\nhttps://github.com/tpulkit/txt2vid.git.",
          "link": "http://arxiv.org/abs/2106.14014",
          "publishedOn": "2021-06-29T01:55:12.484Z",
          "wordCount": 686,
          "title": "Txt2Vid: Ultra-Low Bitrate Compression of Talking-Head Videos via Text. (arXiv:2106.14014v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14016",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianrong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_N/0/1/0/all/0/1\">Nan Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_M/0/1/0/all/0/1\">Mei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xuewei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Q/0/1/0/all/0/1\">Qiang Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Li Liu</a>",
          "description": "Cued Speech (CS) is a communication system for deaf people or hearing\nimpaired people, in which a speaker uses it to aid a lipreader in phonetic\nlevel by clarifying potentially ambiguous mouth movements with hand shape and\npositions. Feature extraction of multi-modal CS is a key step in CS\nrecognition. Recent supervised deep learning based methods suffer from noisy CS\ndata annotations especially for hand shape modality. In this work, we first\npropose a self-supervised contrastive learning method to learn the feature\nrepresentation of image without using labels. Secondly, a small amount of\nmanually annotated CS data are used to fine-tune the first module. Thirdly, we\npresent a module, which combines Bi-LSTM and self-attention networks to further\nlearn sequential features with temporal and contextual information. Besides, to\nenlarge the volume and the diversity of the current limited CS datasets, we\nbuild a new British English dataset containing 5 native CS speakers. Evaluation\nresults on both French and British English datasets show that our model\nachieves over 90% accuracy in hand shape recognition. Significant improvements\nof 8.75% (for French) and 10.09% (for British English) are achieved in CS\nphoneme recognition correctness compared with the state-of-the-art.",
          "link": "http://arxiv.org/abs/2106.14016",
          "publishedOn": "2021-06-29T01:55:12.438Z",
          "wordCount": 641,
          "title": "An Attention Self-supervised Contrastive Learning based Three-stage Model for Hand Shape Feature Representation in Cued Speech. (arXiv:2106.14016v1 [cs.MM])"
        }
      ]
    },
    {
      "title": "cs.CV updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.CV",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2101.00122",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiulong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Hui Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1\">Yang Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1\">Shihao Ji</a>",
          "description": "Joint Energy-based Model (JEM) of Grathwohl et al. shows that a standard\nsoftmax classifier can be reinterpreted as an energy-based model (EBM) for the\njoint distribution p(x,y); the resulting model can be optimized to improve\ncalibration, robustness, and out-of-distribution detection, while generating\nsamples rivaling the quality of recent GAN-based approaches. However, the\nsoftmax classifier that JEM exploits is inherently discriminative and its\nlatent feature space is not well formulated as probabilistic distributions,\nwhich may hinder its potential for image generation and incur training\ninstability. We hypothesize that generative classifiers, such as Linear\nDiscriminant Analysis (LDA), might be more suitable for image generation since\ngenerative classifiers model the data generation process explicitly. This paper\ntherefore investigates an LDA classifier for image classification and\ngeneration. In particular, the Max-Mahalanobis Classifier (MMC), a special case\nof LDA, fits our goal very well. We show that our Generative MMC (GMMC) can be\ntrained discriminatively, generatively, or jointly for image classification and\ngeneration. Extensive experiments on multiple datasets show that GMMC achieves\nstate-of-the-art discriminative and generative performances, while\noutperforming JEM in calibration, adversarial robustness, and\nout-of-distribution detection by a significant margin. Our source code is\navailable at https://github.com/sndnyang/GMMC.",
          "link": "http://arxiv.org/abs/2101.00122",
          "publishedOn": "2021-07-05T01:54:58.207Z",
          "wordCount": 692,
          "title": "Generative Max-Mahalanobis Classifiers for Image Classification, Generation and More. (arXiv:2101.00122v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01189",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jerrick Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Inkawhich_N/0/1/0/all/0/1\">Nathan Inkawhich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nina_O/0/1/0/all/0/1\">Oliver Nina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Timofte_R/0/1/0/all/0/1\">Radu Timofte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Sahil Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_B/0/1/0/all/0/1\">Bob Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_Y/0/1/0/all/0/1\">Yuru Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_W/0/1/0/all/0/1\">Wei Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Songzheng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yuxuan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jiaqi Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1\">Xueli Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1\">Mengru Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Gongzhe Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1\">Xueli Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1\">Huanqia Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_C/0/1/0/all/0/1\">Chengxue Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cummings_S/0/1/0/all/0/1\">Sol Cummings</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miron_C/0/1/0/all/0/1\">Casian Miron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pasarica_A/0/1/0/all/0/1\">Alexandru Pasarica</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Cheng-Yen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_H/0/1/0/all/0/1\">Hung-Min Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1\">Jiarui Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_J/0/1/0/all/0/1\">Jie Mei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeh_C/0/1/0/all/0/1\">Chia-Ying Yeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1\">Jenq-Neng Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xin_M/0/1/0/all/0/1\">Michael Xin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shangguan_Z/0/1/0/all/0/1\">Zhongkai Shangguan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zihe Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yifei_X/0/1/0/all/0/1\">Xu Yifei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Lehan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kele Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_M/0/1/0/all/0/1\">Min Feng</a>",
          "description": "In this paper, we introduce the first Challenge on Multi-modal Aerial View\nObject Classification (MAVOC) in conjunction with the NTIRE 2021 workshop at\nCVPR. This challenge is composed of two different tracks using EO andSAR\nimagery. Both EO and SAR sensors possess different advantages and drawbacks.\nThe purpose of this competition is to analyze how to use both sets of sensory\ninformation in complementary ways. We discuss the top methods submitted for\nthis competition and evaluate their results on our blind test set. Our\nchallenge results show significant improvement of more than 15% accuracy from\nour current baselines for each track of the competition",
          "link": "http://arxiv.org/abs/2107.01189",
          "publishedOn": "2021-07-05T01:54:58.200Z",
          "wordCount": 632,
          "title": "NTIRE 2021 Multi-modal Aerial View Object Classification Challenge. (arXiv:2107.01189v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01153",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenguan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1\">Tianfei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Porikli_F/0/1/0/all/0/1\">Fatih Porikli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Crandall_D/0/1/0/all/0/1\">David Crandall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>",
          "description": "Video segmentation, i.e., partitioning video frames into multiple segments or\nobjects, plays a critical role in a broad range of practical applications,\ne.g., visual effect assistance in movie, scene understanding in autonomous\ndriving, and virtual background creation in video conferencing, to name a few.\nRecently, due to the renaissance of connectionism in computer vision, there has\nbeen an influx of numerous deep learning based approaches that have been\ndedicated to video segmentation and delivered compelling performance. In this\nsurvey, we comprehensively review two basic lines of research in this area,\ni.e., generic object segmentation (of unknown categories) in videos and video\nsemantic segmentation, by introducing their respective task settings,\nbackground concepts, perceived need, development history, and main challenges.\nWe also provide a detailed overview of representative literature on both\nmethods and datasets. Additionally, we present quantitative performance\ncomparisons of the reviewed methods on benchmark datasets. At last, we point\nout a set of unsolved open issues in this field, and suggest possible\nopportunities for further research.",
          "link": "http://arxiv.org/abs/2107.01153",
          "publishedOn": "2021-07-05T01:54:58.182Z",
          "wordCount": 607,
          "title": "A Survey on Deep Learning Technique for Video Segmentation. (arXiv:2107.01153v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2101.03409",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pereira_G/0/1/0/all/0/1\">Gabriel Henrique de Almeida Pereira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fusioka_A/0/1/0/all/0/1\">Andr&#xe9; Minoro Fusioka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nassu_B/0/1/0/all/0/1\">Bogdan Tomoyuki Nassu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Minetto_R/0/1/0/all/0/1\">Rodrigo Minetto</a>",
          "description": "Active fire detection in satellite imagery is of critical importance to the\nmanagement of environmental conservation policies, supporting decision-making\nand law enforcement. This is a well established field, with many techniques\nbeing proposed over the years, usually based on pixel or region-level\ncomparisons involving sensor-specific thresholds and neighborhood statistics.\nIn this paper, we address the problem of active fire detection using deep\nlearning techniques. In recent years, deep learning techniques have been\nenjoying an enormous success in many fields, but their use for active fire\ndetection is relatively new, with open questions and demand for datasets and\narchitectures for evaluation. This paper addresses these issues by introducing\na new large-scale dataset for active fire detection, with over 150,000 image\npatches (more than 200 GB of data) extracted from Landsat-8 images captured\naround the world in August and September 2020, containing wildfires in several\nlocations. The dataset was split in two parts, and contains 10-band spectral\nimages with associated outputs, produced by three well known handcrafted\nalgorithms for active fire detection in the first part, and manually annotated\nmasks in the second part. We also present a study on how different\nconvolutional neural network architectures can be used to approximate these\nhandcrafted algorithms, and how models trained on automatically segmented\npatches can be combined to achieve better performance than the original\nalgorithms - with the best combination having 87.2% precision and 92.4% recall\non our manually annotated dataset. The proposed dataset, source codes and\ntrained models are available on Github\n(https://github.com/pereira-gha/activefire), creating opportunities for further\nadvances in the field",
          "link": "http://arxiv.org/abs/2101.03409",
          "publishedOn": "2021-07-05T01:54:58.151Z",
          "wordCount": 747,
          "title": "Active Fire Detection in Landsat-8 Imagery: a Large-Scale Dataset and a Deep-Learning Study. (arXiv:2101.03409v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12182",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jalal_A/0/1/0/all/0/1\">Ajil Jalal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karmalkar_S/0/1/0/all/0/1\">Sushrut Karmalkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoffmann_J/0/1/0/all/0/1\">Jessica Hoffmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dimakis_A/0/1/0/all/0/1\">Alexandros G. Dimakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Price_E/0/1/0/all/0/1\">Eric Price</a>",
          "description": "This work tackles the issue of fairness in the context of generative\nprocedures, such as image super-resolution, which entail different definitions\nfrom the standard classification setting. Moreover, while traditional group\nfairness definitions are typically defined with respect to specified protected\ngroups -- camouflaging the fact that these groupings are artificial and carry\nhistorical and political motivations -- we emphasize that there are no ground\ntruth identities. For instance, should South and East Asians be viewed as a\nsingle group or separate groups? Should we consider one race as a whole or\nfurther split by gender? Choosing which groups are valid and who belongs in\nthem is an impossible dilemma and being \"fair\" with respect to Asians may\nrequire being \"unfair\" with respect to South Asians. This motivates the\nintroduction of definitions that allow algorithms to be \\emph{oblivious} to the\nrelevant groupings.\n\nWe define several intuitive notions of group fairness and study their\nincompatibilities and trade-offs. We show that the natural extension of\ndemographic parity is strongly dependent on the grouping, and \\emph{impossible}\nto achieve obliviously. On the other hand, the conceptually new definition we\nintroduce, Conditional Proportional Representation, can be achieved obliviously\nthrough Posterior Sampling. Our experiments validate our theoretical results\nand achieve fair image reconstruction using state-of-the-art generative models.",
          "link": "http://arxiv.org/abs/2106.12182",
          "publishedOn": "2021-07-05T01:54:58.144Z",
          "wordCount": 685,
          "title": "Fairness for Image Generation with Uncertain Sensitive Attributes. (arXiv:2106.12182v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12917",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Petersen_J/0/1/0/all/0/1\">Jens Petersen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Isensee_F/0/1/0/all/0/1\">Fabian Isensee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kohler_G/0/1/0/all/0/1\">Gregor K&#xf6;hler</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jager_P/0/1/0/all/0/1\">Paul F. J&#xe4;ger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zimmerer_D/0/1/0/all/0/1\">David Zimmerer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Neuberger_U/0/1/0/all/0/1\">Ulf Neuberger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wick_W/0/1/0/all/0/1\">Wolfgang Wick</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Debus_J/0/1/0/all/0/1\">J&#xfc;rgen Debus</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Heiland_S/0/1/0/all/0/1\">Sabine Heiland</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bendszus_M/0/1/0/all/0/1\">Martin Bendszus</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vollmuth_P/0/1/0/all/0/1\">Philipp Vollmuth</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Maier_Hein_K/0/1/0/all/0/1\">Klaus H. Maier-Hein</a>",
          "description": "The ability to estimate how a tumor might evolve in the future could have\ntremendous clinical benefits, from improved treatment decisions to better dose\ndistribution in radiation therapy. Recent work has approached the glioma growth\nmodeling problem via deep learning and variational inference, thus learning\ngrowth dynamics entirely from a real patient data distribution. So far, this\napproach was constrained to predefined image acquisition intervals and\nsequences of fixed length, which limits its applicability in more realistic\nscenarios. We overcome these limitations by extending Neural Processes, a class\nof conditional generative models for stochastic time series, with a\nhierarchical multi-scale representation encoding including a spatio-temporal\nattention mechanism. The result is a learned growth model that can be\nconditioned on an arbitrary number of observations, and that can produce a\ndistribution of temporally consistent growth trajectories on a continuous time\naxis. On a dataset of 379 patients, the approach successfully captures both\nglobal and finer-grained variations in the images, exhibiting superior\nperformance compared to other learned growth models.",
          "link": "http://arxiv.org/abs/2106.12917",
          "publishedOn": "2021-07-05T01:54:58.137Z",
          "wordCount": 643,
          "title": "Continuous-Time Deep Glioma Growth Models. (arXiv:2106.12917v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.17119",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhenhua Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yuxiang Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Ming Liu</a>",
          "description": "Road-boundary detection is important for autonomous driving. It can be used\nto constrain autonomous vehicles running on road areas to ensure driving\nsafety. Compared with online road-boundary detection using on-vehicle\ncameras/Lidars, offline detection using aerial images could alleviate the\nsevere occlusion issue. Moreover, the offline detection results can be directly\nemployed to annotate high-definition (HD) maps. In recent years, deep-learning\ntechnologies have been used in offline detection. But there still lacks a\npublicly available dataset for this task, which hinders the research progress\nin this area. So in this paper, we propose a new benchmark dataset, named\n\\textit{Topo-boundary}, for offline topological road-boundary detection. The\ndataset contains 25,295 $1000\\times1000$-sized 4-channel aerial images. Each\nimage is provided with 8 training labels for different sub-tasks. We also\ndesign a new entropy-based metric for connectivity evaluation, which could\nbetter handle noises or outliers. We implement and evaluate 3\nsegmentation-based baselines and 5 graph-based baselines using the dataset. We\nalso propose a new imitation-learning-based baseline which is enhanced from our\nprevious work. The superiority of our enhancement is demonstrated from the\ncomparison. The dataset and our-implemented code for the baselines are\navailable at \\texttt{\\url{https://tonyxuqaq.github.io/Topo-boundary/}}.",
          "link": "http://arxiv.org/abs/2103.17119",
          "publishedOn": "2021-07-05T01:54:58.117Z",
          "wordCount": 687,
          "title": "Topo-boundary: A Benchmark Dataset on Topological Road-boundary Detection Using Aerial Images for Autonomous Driving. (arXiv:2103.17119v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01205",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Malik_J/0/1/0/all/0/1\">Jameel Malik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shimada_S/0/1/0/all/0/1\">Soshi Shimada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elhayek_A/0/1/0/all/0/1\">Ahmed Elhayek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ali_S/0/1/0/all/0/1\">Sk Aziz Ali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1\">Christian Theobalt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Golyanik_V/0/1/0/all/0/1\">Vladislav Golyanik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stricker_D/0/1/0/all/0/1\">Didier Stricker</a>",
          "description": "3D hand shape and pose estimation from a single depth map is a new and\nchallenging computer vision problem with many applications. Existing methods\naddressing it directly regress hand meshes via 2D convolutional neural\nnetworks, which leads to artifacts due to perspective distortions in the\nimages. To address the limitations of the existing methods, we develop\nHandVoxNet++, i.e., a voxel-based deep network with 3D and graph convolutions\ntrained in a fully supervised manner. The input to our network is a 3D\nvoxelized-depth-map-based on the truncated signed distance function (TSDF).\nHandVoxNet++ relies on two hand shape representations. The first one is the 3D\nvoxelized grid of hand shape, which does not preserve the mesh topology and\nwhich is the most accurate representation. The second representation is the\nhand surface that preserves the mesh topology. We combine the advantages of\nboth representations by aligning the hand surface to the voxelized hand shape\neither with a new neural Graph-Convolutions-based Mesh Registration\n(GCN-MeshReg) or classical segment-wise Non-Rigid Gravitational Approach\n(NRGA++) which does not rely on training data. In extensive evaluations on\nthree public benchmarks, i.e., SynHand5M, depth-based HANDS19 challenge and\nHO-3D, the proposed HandVoxNet++ achieves the state-of-the-art performance. In\nthis journal extension of our previous approach presented at CVPR 2020, we gain\n41.09% and 13.7% higher shape alignment accuracy on SynHand5M and HANDS19\ndatasets, respectively. Our method is ranked first on the HANDS19 challenge\ndataset (Task 1: Depth-Based 3D Hand Pose Estimation) at the moment of the\nsubmission of our results to the portal in August 2020.",
          "link": "http://arxiv.org/abs/2107.01205",
          "publishedOn": "2021-07-05T01:54:58.108Z",
          "wordCount": 721,
          "title": "HandVoxNet++: 3D Hand Shape and Pose Estimation using Voxel-Based Neural Networks. (arXiv:2107.01205v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11272",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhiqin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>",
          "description": "We introduce Neural Marching Cubes (NMC), a data-driven approach for\nextracting a triangle mesh from a discretized implicit field. Classical MC is\ndefined by coarse tessellation templates isolated to individual cubes. While\nmore refined tessellations have been proposed, they all make heuristic\nassumptions, such as trilinearity, when determining the vertex positions and\nlocal mesh topologies in each cube. In principle, none of these approaches can\nreconstruct geometric features that reveal coherence or dependencies between\nnearby cubes (e.g., a sharp edge), as such information is unaccounted for,\nresulting in poor estimates of the true underlying implicit field. To tackle\nthese challenges, we re-cast MC from a deep learning perspective, by designing\ntessellation templates more apt at preserving geometric features, and learning\nthe vertex positions and mesh topologies from training meshes, to account for\ncontextual information from nearby cubes. We develop a compact per-cube\nparameterization to represent the output triangle mesh, while being compatible\nwith neural processing, so that a simple 3D convolutional network can be\nemployed for the training. We show that all topological cases in each cube that\nare applicable to our design can be easily derived using our representation,\nand the resulting tessellations can also be obtained naturally and efficiently\nby following a few design guidelines. In addition, our network learns local\nfeatures with limited receptive fields, hence it generalizes well to new shapes\nand new datasets. We evaluate our neural MC approach by quantitative and\nqualitative comparisons to all well-known MC variants. In particular, we\ndemonstrate the ability of our network to recover sharp features such as edges\nand corners, a long-standing issue of MC and its variants. Our network also\nreconstructs local mesh topologies more accurately than previous approaches.",
          "link": "http://arxiv.org/abs/2106.11272",
          "publishedOn": "2021-07-05T01:54:58.101Z",
          "wordCount": 738,
          "title": "Neural Marching Cubes. (arXiv:2106.11272v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11154",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Korschens_M/0/1/0/all/0/1\">Matthias K&#xf6;rschens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bodesheim_P/0/1/0/all/0/1\">Paul Bodesheim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romermann_C/0/1/0/all/0/1\">Christine R&#xf6;mermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bucher_S/0/1/0/all/0/1\">Solveig Franziska Bucher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Migliavacca_M/0/1/0/all/0/1\">Mirco Migliavacca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ulrich_J/0/1/0/all/0/1\">Josephine Ulrich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denzler_J/0/1/0/all/0/1\">Joachim Denzler</a>",
          "description": "Monitoring the responses of plants to environmental changes is essential for\nplant biodiversity research. This, however, is currently still being done\nmanually by botanists in the field. This work is very laborious, and the data\nobtained is, though following a standardized method to estimate plant coverage,\nusually subjective and has a coarse temporal resolution. To remedy these\ncaveats, we investigate approaches using convolutional neural networks (CNNs)\nto automatically extract the relevant data from images, focusing on plant\ncommunity composition and species coverages of 9 herbaceous plant species. To\nthis end, we investigate several standard CNN architectures and different\npretraining methods. We find that we outperform our previous approach at higher\nimage resolutions using a custom CNN with a mean absolute error of 5.16%. In\naddition to these investigations, we also conduct an error analysis based on\nthe temporal aspect of the plant cover images. This analysis gives insight into\nwhere problems for automatic approaches lie, like occlusion and likely\nmisclassifications caused by temporal changes.",
          "link": "http://arxiv.org/abs/2106.11154",
          "publishedOn": "2021-07-05T01:54:58.093Z",
          "wordCount": 635,
          "title": "Automatic Plant Cover Estimation with Convolutional Neural Networks. (arXiv:2106.11154v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10601",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaxiong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1\">Yunchao Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_X/0/1/0/all/0/1\">Xueming Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1\">Li Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>",
          "description": "We aim to tackle the challenging yet practical scenery image outpainting task\nin this work. Recently, generative adversarial learning has significantly\nadvanced the image outpainting by producing semantic consistent content for the\ngiven image. However, the existing methods always suffer from the blurry\ntexture and the artifacts of the generative part, making the overall\noutpainting results lack authenticity. To overcome the weakness, this work\ninvestigates a principle way to synthesize texture-rich results by borrowing\npixels from its neighbors (\\ie, reference images), named\n\\textbf{Re}ference-\\textbf{G}uided \\textbf{O}utpainting (ReGO). Particularly,\nthe ReGO designs an Adaptive Content Selection (ACS) module to transfer the\npixel of reference images for texture compensating of the target one. To\nprevent the style of the generated part from being affected by the reference\nimages, a style ranking loss is further proposed to augment the ReGO to\nsynthesize style-consistent results. Extensive experiments on two popular\nbenchmarks, NS6K~\\cite{yangzx} and NS8K~\\cite{wang}, well demonstrate the\neffectiveness of our ReGO.",
          "link": "http://arxiv.org/abs/2106.10601",
          "publishedOn": "2021-07-05T01:54:58.084Z",
          "wordCount": 623,
          "title": "ReGO: Reference-Guided Outpainting for Scenery Image. (arXiv:2106.10601v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11478",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chen Liu</a>",
          "description": "Image reconstruction is likely the most predominant auxiliary task for image\nclassification, but we would like to think twice about this convention. In this\npaper, we investigated \"approximating the Fourier Transform of the input image\"\nas a potential alternative, in the hope that it may further boost the\nperformances on the primary task or introduce novel constraints not well\ncovered by image reconstruction. We experimented with five popular\nclassification architectures on the CIFAR-10 dataset, and the empirical results\nindicated that our proposed auxiliary task generally improves the\nclassification accuracy. More notably, the results showed that in certain cases\nour proposed auxiliary task may enhance the classifiers' resistance to\nadversarial attacks generated using the fast gradient sign method.",
          "link": "http://arxiv.org/abs/2106.11478",
          "publishedOn": "2021-07-05T01:54:58.075Z",
          "wordCount": 610,
          "title": "Fourier Transform Approximation as an Auxiliary Task for Image Classification. (arXiv:2106.11478v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.14010",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gallardo_J/0/1/0/all/0/1\">Jhair Gallardo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hayes_T/0/1/0/all/0/1\">Tyler L. Hayes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanan_C/0/1/0/all/0/1\">Christopher Kanan</a>",
          "description": "In continual learning, a system must incrementally learn from a\nnon-stationary data stream without catastrophic forgetting. Recently, multiple\nmethods have been devised for incrementally learning classes on large-scale\nimage classification tasks, such as ImageNet. State-of-the-art continual\nlearning methods use an initial supervised pre-training phase, in which the\nfirst 10% - 50% of the classes in a dataset are used to learn representations\nin an offline manner before continual learning of new classes begins. We\nhypothesize that self-supervised pre-training could yield features that\ngeneralize better than supervised learning, especially when the number of\nsamples used for pre-training is small. We test this hypothesis using the\nself-supervised MoCo-V2, Barlow Twins, and SwAV algorithms. On ImageNet, we\nfind that these methods outperform supervised pre-training considerably for\nonline continual learning, and the gains are larger when fewer samples are\navailable. Our findings are consistent across three online continual learning\nalgorithms. Our best system achieves a 14.95% relative increase in top-1\naccuracy on class incremental ImageNet over the prior state of the art for\nonline continual learning.",
          "link": "http://arxiv.org/abs/2103.14010",
          "publishedOn": "2021-07-05T01:54:58.055Z",
          "wordCount": 633,
          "title": "Self-Supervised Training Enhances Online Continual Learning. (arXiv:2103.14010v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.08028",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Bae_J/0/1/0/all/0/1\">Joseph Bae</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Kapse_S/0/1/0/all/0/1\">Saarthak Kapse</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Singh_G/0/1/0/all/0/1\">Gagandeep Singh</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Gattu_R/0/1/0/all/0/1\">Rishabh Gattu</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Ali_S/0/1/0/all/0/1\">Syed Ali</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Shah_N/0/1/0/all/0/1\">Neal Shah</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Marshall_C/0/1/0/all/0/1\">Colin Marshall</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Pierce_J/0/1/0/all/0/1\">Jonathan Pierce</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Phatak_T/0/1/0/all/0/1\">Tej Phatak</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Gupta_A/0/1/0/all/0/1\">Amit Gupta</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Green_J/0/1/0/all/0/1\">Jeremy Green</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Madan_N/0/1/0/all/0/1\">Nikhil Madan</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Prasanna_P/0/1/0/all/0/1\">Prateek Prasanna</a>",
          "description": "We predict mechanical ventilation requirement and mortality using\ncomputational modeling of chest radiographs (CXRs) for coronavirus disease 2019\n(COVID-19) patients. This two-center, retrospective study analyzed 530\ndeidentified CXRs from 515 COVID-19 patients treated at Stony Brook University\nHospital and Newark Beth Israel Medical Center between March and August 2020.\nDL and machine learning classifiers to predict mechanical ventilation\nrequirement and mortality were trained and evaluated using patient CXRs. A\nnovel radiomic embedding framework was also explored for outcome prediction.\nAll results are compared against radiologist grading of CXRs (zone-wise expert\nseverity scores). Radiomic and DL classification models had mAUCs of\n0.78+/-0.02 and 0.81+/-0.04, compared with expert scores mAUCs of 0.75+/-0.02\nand 0.79+/-0.05 for mechanical ventilation requirement and mortality\nprediction, respectively. Combined classifiers using both radiomics and expert\nseverity scores resulted in mAUCs of 0.79+/-0.04 and 0.83+/-0.04 for each\nprediction task, demonstrating improvement over either artificial intelligence\nor radiologist interpretation alone. Our results also suggest instances where\ninclusion of radiomic features in DL improves model predictions, something that\nmight be explored in other pathologies. The models proposed in this study and\nthe prognostic information they provide might aid physician decision making and\nresource allocation during the COVID-19 pandemic.",
          "link": "http://arxiv.org/abs/2007.08028",
          "publishedOn": "2021-07-05T01:54:58.049Z",
          "wordCount": 757,
          "title": "Predicting Clinical Outcomes in COVID-19 using Radiomics and Deep Learning on Chest Radiographs: A Multi-Institutional Study. (arXiv:2007.08028v2 [q-bio.QM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.15109",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Antonante_P/0/1/0/all/0/1\">Pasquale Antonante</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tzoumas_V/0/1/0/all/0/1\">Vasileios Tzoumas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Heng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carlone_L/0/1/0/all/0/1\">Luca Carlone</a>",
          "description": "Nonlinear estimation in robotics and vision is typically plagued with\noutliers due to wrong data association, or to incorrect detections from signal\nprocessing and machine learning methods. This paper introduces two unifying\nformulations for outlier-robust estimation, Generalized Maximum Consensus\n(G-MC) and Generalized Truncated Least Squares (G-TLS), and investigates\nfundamental limits, practical algorithms, and applications. Our first\ncontribution is a proof that outlier-robust estimation is inapproximable: in\nthe worst case, it is impossible to (even approximately) find the set of\noutliers, even with slower-than-polynomial-time algorithms (particularly,\nalgorithms running in quasi-polynomial time). As a second contribution, we\nreview and extend two general-purpose algorithms. The first, Adaptive Trimming\n(ADAPT), is combinatorial, and is suitable for G-MC; the second, Graduated\nNon-Convexity (GNC), is based on homotopy methods, and is suitable for G-TLS.\nWe extend ADAPT and GNC to the case where the user does not have prior\nknowledge of the inlier-noise statistics (or the statistics may vary over time)\nand is unable to guess a reasonable threshold to separate inliers from outliers\n(as the one commonly used in RANSAC). We propose the first minimally tuned\nalgorithms for outlier rejection, that dynamically decide how to separate\ninliers from outliers. Our third contribution is an evaluation of the proposed\nalgorithms on robot perception problems: mesh registration, image-based object\ndetection (shape alignment), and pose graph optimization. ADAPT and GNC execute\nin real-time, are deterministic, outperform RANSAC, and are robust up to 80-90%\noutliers. Their minimally tuned versions also compare favorably with the state\nof the art, even though they do not rely on a noise bound for the inliers.",
          "link": "http://arxiv.org/abs/2007.15109",
          "publishedOn": "2021-07-05T01:54:58.041Z",
          "wordCount": 739,
          "title": "Outlier-Robust Estimation: Hardness, Minimally Tuned Algorithms, and Applications. (arXiv:2007.15109v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.07770",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Topiwala_P/0/1/0/all/0/1\">Pankaj Topiwala</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dai_W/0/1/0/all/0/1\">Wei Dai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pian_J/0/1/0/all/0/1\">Jiangfeng Pian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Biondi_K/0/1/0/all/0/1\">Katalina Biondi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Krovvidi_A/0/1/0/all/0/1\">Arvind Krovvidi</a>",
          "description": "Video quality assessment (VQA) is now a fastgrowing subject, beginning to\nmature in the full reference (FR) case, while the burgeoning no reference (NR)\ncase remains challenging. We investigate variants of the popular VMAF video\nquality assessment algorithm for the FR case, using support vector regression\nand feedforward neural networks, and extend it to the NR case, using the same\nlearning architectures, to develop a partially unified framework for VQA. When\nheavily trained, algorithms such as VMAF perform well on test datasets, with\n90%+ match; but predicting performance in the wild is better done by\ntraining/testing from scratch, as we do. Even from scratch, we achieve 90%+\nperformance in FR, with gains over VMAF. And we greatly reduce complexity vs.\nleading recent NR algorithms, VIDEVAL, RAPIQUE, yet exceed 80% in SRCC. In our\npreliminary testing, we find the improvements in trainability, while also\nconstraining computational complexity, as quite encouraging, suggesting further\nstudy and analysis.",
          "link": "http://arxiv.org/abs/2103.07770",
          "publishedOn": "2021-07-05T01:54:58.034Z",
          "wordCount": 625,
          "title": "VMAF And Variants: Towards A Unified VQA. (arXiv:2103.07770v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.03046",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hongbin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1\">Jinyuan Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_N/0/1/0/all/0/1\">Neil Zhenqiang Gong</a>",
          "description": "3D point cloud classification has many safety-critical applications such as\nautonomous driving and robotic grasping. However, several studies showed that\nit is vulnerable to adversarial attacks. In particular, an attacker can make a\nclassifier predict an incorrect label for a 3D point cloud via carefully\nmodifying, adding, and/or deleting a small number of its points. Randomized\nsmoothing is state-of-the-art technique to build certifiably robust 2D image\nclassifiers. However, when applied to 3D point cloud classification, randomized\nsmoothing can only certify robustness against adversarially modified points.\n\nIn this work, we propose PointGuard, the first defense that has provable\nrobustness guarantees against adversarially modified, added, and/or deleted\npoints. Specifically, given a 3D point cloud and an arbitrary point cloud\nclassifier, our PointGuard first creates multiple subsampled point clouds, each\nof which contains a random subset of the points in the original point cloud;\nthen our PointGuard predicts the label of the original point cloud as the\nmajority vote among the labels of the subsampled point clouds predicted by the\npoint cloud classifier. Our first major theoretical contribution is that we\nshow PointGuard provably predicts the same label for a 3D point cloud when the\nnumber of adversarially modified, added, and/or deleted points is bounded. Our\nsecond major theoretical contribution is that we prove the tightness of our\nderived bound when no assumptions on the point cloud classifier are made.\nMoreover, we design an efficient algorithm to compute our certified robustness\nguarantees. We also empirically evaluate PointGuard on ModelNet40 and ScanNet\nbenchmark datasets.",
          "link": "http://arxiv.org/abs/2103.03046",
          "publishedOn": "2021-07-05T01:54:58.017Z",
          "wordCount": 732,
          "title": "PointGuard: Provably Robust 3D Point Cloud Classification. (arXiv:2103.03046v2 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09672",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Douze_M/0/1/0/all/0/1\">Matthijs Douze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tolias_G/0/1/0/all/0/1\">Giorgos Tolias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pizzi_E/0/1/0/all/0/1\">Ed Pizzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papakipos_Z/0/1/0/all/0/1\">Zo&#xeb; Papakipos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chanussot_L/0/1/0/all/0/1\">Lowik Chanussot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radenovic_F/0/1/0/all/0/1\">Filip Radenovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jenicek_T/0/1/0/all/0/1\">Tomas Jenicek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maximov_M/0/1/0/all/0/1\">Maxim Maximov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leal_Taixe_L/0/1/0/all/0/1\">Laura Leal-Taix&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elezi_I/0/1/0/all/0/1\">Ismail Elezi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chum_O/0/1/0/all/0/1\">Ond&#x159;ej Chum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrer_C/0/1/0/all/0/1\">Cristian Canton Ferrer</a>",
          "description": "This paper introduces a new benchmark for large-scale image similarity\ndetection. This benchmark is used for the Image Similarity Challenge at\nNeurIPS'21 (ISC2021). The goal is to determine whether a query image is a\nmodified copy of any image in a reference corpus of size 1~million. The\nbenchmark features a variety of image transformations such as automated\ntransformations, hand-crafted image edits and machine-learning based\nmanipulations. This mimics real-life cases appearing in social media, for\nexample for integrity-related problems dealing with misinformation and\nobjectionable content. The strength of the image manipulations, and therefore\nthe difficulty of the benchmark, is calibrated according to the performance of\na set of baseline approaches. Both the query and reference set contain a\nmajority of \"distractor\" images that do not match, which corresponds to a\nreal-life needle-in-haystack setting, and the evaluation metric reflects that.\nWe expect the DISC21 benchmark to promote image copy detection as an important\nand challenging computer vision task and refresh the state of the art.",
          "link": "http://arxiv.org/abs/2106.09672",
          "publishedOn": "2021-07-05T01:54:58.004Z",
          "wordCount": 645,
          "title": "The 2021 Image Similarity Dataset and Challenge. (arXiv:2106.09672v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01175",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Su Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yi Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Ziquan Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_C/0/1/0/all/0/1\">Cuntai Guan</a>",
          "description": "We propose an audio-visual spatial-temporal deep neural network with: (1) a\nvisual block containing a pretrained 2D-CNN followed by a temporal\nconvolutional network (TCN); (2) an aural block containing several parallel\nTCNs; and (3) a leader-follower attentive fusion block combining the\naudio-visual information. The TCN with large history coverage enables our model\nto exploit spatial-temporal information within a much larger window length\n(i.e., 300) than that from the baseline and state-of-the-art methods (i.e., 36\nor 48). The fusion block emphasizes the visual modality while exploits the\nnoisy aural modality using the inter-modality attention mechanism. To make full\nuse of the data and alleviate over-fitting, cross-validation is carried out on\nthe training and validation set. The concordance correlation coefficient (CCC)\ncentering is used to merge the results from each fold. On the development set,\nthe achieved CCC is 0.410 for valence and 0.661 for arousal, which\nsignificantly outperforms the baseline method with the corresponding CCC of\n0.210 and 0.230 for valence and arousal, respectively. The code is available at\nhttps://github.com/sucv/ABAW2.",
          "link": "http://arxiv.org/abs/2107.01175",
          "publishedOn": "2021-07-05T01:54:57.997Z",
          "wordCount": 611,
          "title": "Audio-visual Attentive Fusion for Continuous Emotion Recognition. (arXiv:2107.01175v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2007.12391",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Anantrasirichai_N/0/1/0/all/0/1\">Nantheera Anantrasirichai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bull_D/0/1/0/all/0/1\">David Bull</a>",
          "description": "This paper reviews the current state of the art in Artificial Intelligence\n(AI) technologies and applications in the context of the creative industries. A\nbrief background of AI, and specifically Machine Learning (ML) algorithms, is\nprovided including Convolutional Neural Network (CNNs), Generative Adversarial\nNetworks (GANs), Recurrent Neural Networks (RNNs) and Deep Reinforcement\nLearning (DRL). We categorise creative applications into five groups related to\nhow AI technologies are used: i) content creation, ii) information analysis,\niii) content enhancement and post production workflows, iv) information\nextraction and enhancement, and v) data compression. We critically examine the\nsuccesses and limitations of this rapidly advancing technology in each of these\nareas. We further differentiate between the use of AI as a creative tool and\nits potential as a creator in its own right. We foresee that, in the near\nfuture, machine learning-based AI will be adopted widely as a tool or\ncollaborative assistant for creativity. In contrast, we observe that the\nsuccesses of machine learning in domains with fewer constraints, where AI is\nthe `creator', remain modest. The potential of AI (or its developers) to win\nawards for its original creations in competition with human creatives is also\nlimited, based on contemporary technologies. We therefore conclude that, in the\ncontext of creative industries, maximum benefit from AI will be derived where\nits focus is human centric -- where it is designed to augment, rather than\nreplace, human creativity.",
          "link": "http://arxiv.org/abs/2007.12391",
          "publishedOn": "2021-07-05T01:54:57.982Z",
          "wordCount": 746,
          "title": "Artificial Intelligence in the Creative Industries: A Review. (arXiv:2007.12391v6 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01181",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mi_L/0/1/0/all/0/1\">Li Mi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ou_Y/0/1/0/all/0/1\">Yangjun Ou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhenzhong Chen</a>",
          "description": "Real-world scenarios often require the anticipation of object interactions in\nunknown future, which would assist the decision-making process of both humans\nand agents. To meet this challenge, we present a new task named Visual\nRelationship Forecasting (VRF) in videos to explore the prediction of visual\nrelationships in a reasoning manner. Specifically, given a subject-object pair\nwith H existing frames, VRF aims to predict their future interactions for the\nnext T frames without visual evidence. To evaluate the VRF task, we introduce\ntwo video datasets named VRF-AG and VRF-VidOR, with a series of\nspatio-temporally localized visual relation annotations in a video. These two\ndatasets densely annotate 13 and 35 visual relationships in 1923 and 13447\nvideo clips, respectively. In addition, we present a novel Graph Convolutional\nTransformer (GCT) framework, which captures both object-level and frame-level\ndependencies by spatio-temporal Graph Convolution Network and Transformer.\nExperimental results on both VRF-AG and VRF-VidOR datasets demonstrate that GCT\noutperforms the state-of-the-art sequence modelling methods on visual\nrelationship forecasting.",
          "link": "http://arxiv.org/abs/2107.01181",
          "publishedOn": "2021-07-05T01:54:57.974Z",
          "wordCount": 595,
          "title": "Visual Relationship Forecasting in Videos. (arXiv:2107.01181v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.04324",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zimmermann_C/0/1/0/all/0/1\">Christian Zimmermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Argus_M/0/1/0/all/0/1\">Max Argus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brox_T/0/1/0/all/0/1\">Thomas Brox</a>",
          "description": "This work presents improvements in monocular hand shape estimation by\nbuilding on top of recent advances in unsupervised learning. We extend momentum\ncontrastive learning and contribute a structured collection of hand images,\nwell suited for visual representation learning, which we call HanCo. We find\nthat the representation learned by established contrastive learning methods can\nbe improved significantly by exploiting advanced background removal techniques\nand multi-view information. These allow us to generate more diverse instance\npairs than those obtained by augmentations commonly used in exemplar based\napproaches. Our method leads to a more suitable representation for the hand\nshape estimation task and shows a 4.7% reduction in mesh error and a 3.6%\nimprovement in F-score compared to an ImageNet pretrained baseline. We make our\nbenchmark dataset publicly available, to encourage further research into this\ndirection.",
          "link": "http://arxiv.org/abs/2106.04324",
          "publishedOn": "2021-07-05T01:54:57.968Z",
          "wordCount": 593,
          "title": "Contrastive Representation Learning for Hand Shape Estimation. (arXiv:2106.04324v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01130",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zabihzadeh_D/0/1/0/all/0/1\">Davood Zabihzadeh</a>",
          "description": "Deep Metric Learning (DML) learns a non-linear semantic embedding from input\ndata that brings similar pairs together while keeps dissimilar data away from\neach other. To this end, many different methods are proposed in the last decade\nwith promising results in various applications. The success of a DML algorithm\ngreatly depends on its loss function. However, no loss function is perfect, and\nit deals only with some aspects of an optimal similarity embedding. Besides,\nthe generalizability of the DML on unseen categories during the test stage is\nan important matter that is not considered by existing loss functions. To\naddress these challenges, we propose novel approaches to combine different\nlosses built on top of a shared deep feature extractor. The proposed ensemble\nof losses enforces the deep model to extract features that are consistent with\nall losses. Since the selected losses are diverse and each emphasizes different\naspects of an optimal semantic embedding, our effective combining methods yield\na considerable improvement over any individual loss and generalize well on\nunseen categories. Here, there is no limitation in choosing loss functions, and\nour methods can work with any set of existing ones. Besides, they can optimize\neach loss function as well as its weight in an end-to-end paradigm with no need\nto adjust any hyper-parameter. We evaluate our methods on some popular datasets\nfrom the machine vision domain in conventional Zero-Shot-Learning (ZSL)\nsettings. The results are very encouraging and show that our methods outperform\nall baseline losses by a large margin in all datasets.",
          "link": "http://arxiv.org/abs/2107.01130",
          "publishedOn": "2021-07-05T01:54:57.951Z",
          "wordCount": 713,
          "title": "Ensemble of Loss Functions to Improve Generalizability of Deep Metric Learning methods. (arXiv:2107.01130v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01194",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+She_Q/0/1/0/all/0/1\">Qi She</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1\">Zhengyang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Changhu Wang</a>",
          "description": "Contrastive learning applied to self-supervised representation learning has\nseen a resurgence in deep models. In this paper, we find that existing\ncontrastive learning based solutions for self-supervised video recognition\nfocus on inter-variance encoding but ignore the intra-variance existing in\nclips within the same video. We thus propose to learn dual representations for\neach clip which (\\romannumeral 1) encode intra-variance through a shuffle-rank\npretext task; (\\romannumeral 2) encode inter-variance through a temporal\ncoherent contrastive loss. Experiment results show that our method plays an\nessential role in balancing inter and intra variances and brings consistent\nperformance gains on multiple backbones and contrastive learning frameworks.\nIntegrated with SimCLR and pretrained on Kinetics-400, our method achieves\n$\\textbf{82.0\\%}$ and $\\textbf{51.2\\%}$ downstream classification accuracy on\nUCF101 and HMDB51 test sets respectively and $\\textbf{46.1\\%}$ video retrieval\naccuracy on UCF101, outperforming both pretext-task based and contrastive\nlearning based counterparts.",
          "link": "http://arxiv.org/abs/2107.01194",
          "publishedOn": "2021-07-05T01:54:57.944Z",
          "wordCount": 595,
          "title": "How Incomplete is Contrastive Learning? AnInter-intra Variant Dual Representation Method forSelf-supervised Video Recognition. (arXiv:2107.01194v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00932",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wong_C/0/1/0/all/0/1\">Conghao Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_B/0/1/0/all/0/1\">Beihao Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Q/0/1/0/all/0/1\">Qinmu Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_X/0/1/0/all/0/1\">Xinge You</a>",
          "description": "It is essential but challenging to predict future trajectories of various\nagents in complex scenes. Whether it is internal personality factors of agents,\ninteractive behavior of the neighborhood, or the influence of surroundings, it\nwill have an impact on their future behavior styles. It means that even for the\nsame physical type of agents, there are huge differences in their behavior\npreferences. Although recent works have made significant progress in studying\nagents' multi-modal plannings, most of them still apply the same prediction\nstrategy to all agents, which makes them difficult to fully show the multiple\nstyles of vast agents. In this paper, we propose the Multi-Style Network (MSN)\nto focus on this problem by divide agents' preference styles into several\nhidden behavior categories adaptively and train each category's prediction\nnetwork separately, therefore giving agents all styles of predictions\nsimultaneously. Experiments demonstrate that our deterministic MSN-D and\ngenerative MSN-G outperform many recent state-of-the-art methods and show\nbetter multi-style characteristics in the visualized results.",
          "link": "http://arxiv.org/abs/2107.00932",
          "publishedOn": "2021-07-05T01:54:57.937Z",
          "wordCount": 594,
          "title": "MSN: Multi-Style Network for Trajectory Prediction. (arXiv:2107.00932v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01125",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Shi_Z/0/1/0/all/0/1\">Zenglin Shi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mettes_P/0/1/0/all/0/1\">Pascal Mettes</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Maji_S/0/1/0/all/0/1\">Subhransu Maji</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Snoek_C/0/1/0/all/0/1\">Cees G. M. Snoek</a>",
          "description": "The deep image prior has demonstrated the remarkable ability that untrained\nnetworks can address inverse imaging problems, such as denoising, inpainting\nand super-resolution, by optimizing on just a single degraded image. Despite\nits promise, it suffers from two limitations. First, it remains unclear how one\ncan control the prior beyond the choice of the network architecture. Second, it\nrequires an oracle to determine when to stop the optimization as the\nperformance degrades after reaching a peak. In this paper, we study the deep\nimage prior from a spectral bias perspective to address these problems. By\nintroducing a frequency-band correspondence measure, we observe that deep image\npriors for inverse imaging exhibit a spectral bias during optimization, where\nlow-frequency image signals are learned faster and better than high-frequency\nnoise signals. This pinpoints why degraded images can be denoised or inpainted\nwhen the optimization is stopped at the right time. Based on our observations,\nwe propose to control the spectral bias in the deep image prior to prevent\nperformance degradation and to speed up optimization convergence. We do so in\nthe two core layer types of inverse imaging networks: the convolution layer and\nthe upsampling layer. We present a Lipschitz-controlled approach for the\nconvolution and a Gaussian-controlled approach for the upsampling layer. We\nfurther introduce a stopping criterion to avoid superfluous computation. The\nexperiments on denoising, inpainting and super-resolution show that our method\nno longer suffers from performance degradation during optimization, relieving\nus from the need for an oracle criterion to stop early. We further outline a\nstopping criterion to avoid superfluous computation. Finally, we show that our\napproach obtains favorable restoration results compared to current approaches,\nacross all tasks.",
          "link": "http://arxiv.org/abs/2107.01125",
          "publishedOn": "2021-07-05T01:54:57.860Z",
          "wordCount": 739,
          "title": "On Measuring and Controlling the Spectral Bias of the Deep Image Prior. (arXiv:2107.01125v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01152",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Chen_J/0/1/0/all/0/1\">Junya Chen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Gan_Z/0/1/0/all/0/1\">Zhe Gan</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Li_X/0/1/0/all/0/1\">Xuan Li</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Guo_Q/0/1/0/all/0/1\">Qing Guo</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Chen_L/0/1/0/all/0/1\">Liqun Chen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Gao_S/0/1/0/all/0/1\">Shuyang Gao</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Chung_T/0/1/0/all/0/1\">Tagyoung Chung</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Xu_Y/0/1/0/all/0/1\">Yi Xu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zeng_B/0/1/0/all/0/1\">Belinda Zeng</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lu_W/0/1/0/all/0/1\">Wenlian Lu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Li_F/0/1/0/all/0/1\">Fan Li</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Carin_L/0/1/0/all/0/1\">Lawrence Carin</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Tao_C/0/1/0/all/0/1\">Chenyang Tao</a>",
          "description": "InfoNCE-based contrastive representation learners, such as SimCLR, have been\ntremendously successful in recent years. However, these contrastive schemes are\nnotoriously resource demanding, as their effectiveness breaks down with\nsmall-batch training (i.e., the log-K curse, whereas K is the batch-size). In\nthis work, we reveal mathematically why contrastive learners fail in the\nsmall-batch-size regime, and present a novel simple, non-trivial contrastive\nobjective named FlatNCE, which fixes this issue. Unlike InfoNCE, our FlatNCE no\nlonger explicitly appeals to a discriminative classification goal for\ncontrastive learning. Theoretically, we show FlatNCE is the mathematical dual\nformulation of InfoNCE, thus bridging the classical literature on energy\nmodeling; and empirically, we demonstrate that, with minimal modification of\ncode, FlatNCE enables immediate performance boost independent of the\nsubject-matter engineering efforts. The significance of this work is furthered\nby the powerful generalization of contrastive learning techniques, and the\nintroduction of new tools to monitor and diagnose contrastive training. We\nsubstantiate our claims with empirical evidence on CIFAR10, ImageNet, and other\ndatasets, where FlatNCE consistently outperforms InfoNCE.",
          "link": "http://arxiv.org/abs/2107.01152",
          "publishedOn": "2021-07-05T01:54:57.853Z",
          "wordCount": 644,
          "title": "Simpler, Faster, Stronger: Breaking The log-K Curse On Contrastive Learners With FlatNCE. (arXiv:2107.01152v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00987",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Akhmetyanov_A/0/1/0/all/0/1\">Azat Akhmetyanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kornilova_A/0/1/0/all/0/1\">Anastasiia Kornilova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faizullin_M/0/1/0/all/0/1\">Marsel Faizullin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pozo_D/0/1/0/all/0/1\">David Pozo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrer_G/0/1/0/all/0/1\">Gonzalo Ferrer</a>",
          "description": "This paper addresses the problem of building an affordable easy-to-setup\nsynchronized multi-view camera system, which is in demand for many Computer\nVision and Robotics applications in high-dynamic environments. In our work, we\npropose a solution for this problem - a publicly-available Android application\nfor synchronized video recording on multiple smartphones with sub-millisecond\naccuracy. We present a generalized mathematical model of timestamping for\nAndroid smartphones and prove its applicability on 47 different physical\ndevices. Also, we estimate the time drift parameter for those smartphones,\nwhich is less than 1.2 millisecond per minute for most of the considered\ndevices, that makes smartphones' camera system a worthy analog for professional\nmulti-view systems. Finally, we demonstrate Android-app performance on the\ncamera system built from Android smartphones quantitatively, showing less than\n300 microseconds synchronization error, and qualitatively - on panorama\nstitching task.",
          "link": "http://arxiv.org/abs/2107.00987",
          "publishedOn": "2021-07-05T01:54:57.845Z",
          "wordCount": 572,
          "title": "Sub-millisecond Video Synchronization of Multiple Android Smartphones. (arXiv:2107.00987v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01086",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Airaksinen_M/0/1/0/all/0/1\">Manu Airaksinen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vanhatalo_S/0/1/0/all/0/1\">Sampsa Vanhatalo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rasanen_O/0/1/0/all/0/1\">Okko R&#xe4;s&#xe4;nen</a>",
          "description": "Infant motility assessment using intelligent wearables is a promising new\napproach for assessment of infant neurophysiological development, and where\nefficient signal analysis plays a central role. This study investigates the use\nof different end-to-end neural network architectures for processing infant\nmotility data from wearable sensors. We focus on the performance and\ncomputational burden of alternative sensor encoder and time-series modelling\nmodules and their combinations. In addition, we explore the benefits of data\naugmentation methods in ideal and non-ideal recording conditions. The\nexperiments are conducted using a data-set of multi-sensor movement recordings\nfrom 7-month-old infants, as captured by a recently proposed smart jumpsuit for\ninfant motility assessment. Our results indicate that the choice of the encoder\nmodule has a major impact on classifier performance. For sensor encoders, the\nbest performance was obtained with parallel 2-dimensional convolutions for\nintra-sensor channel fusion with shared weights for all sensors. The results\nalso indicate that a relatively compact feature representation is obtainable\nfor within-sensor feature extraction without a drastic loss to classifier\nperformance. Comparison of time-series models revealed that feed-forward\ndilated convolutions with residual and skip connections outperformed all\nRNN-based models in performance, training time, and training stability. The\nexperiments also indicate that data augmentation improves model robustness in\nsimulated packet loss or sensor dropout scenarios. In particular, signal- and\nsensor-dropout-based augmentation strategies provided considerable boosts to\nperformance without negatively affecting the baseline performance. Overall the\nresults provide tangible suggestions on how to optimize end-to-end neural\nnetwork training for multi-channel movement sensor data.",
          "link": "http://arxiv.org/abs/2107.01086",
          "publishedOn": "2021-07-05T01:54:57.833Z",
          "wordCount": 705,
          "title": "Comparison of end-to-end neural network architectures and data augmentation methods for automatic infant motility assessment using wearable sensors. (arXiv:2107.01086v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00842",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hongji Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xiufan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yingying Zhu</a>",
          "description": "In this work, we address the problem of cross-view geo-localization, which\nestimates the geospatial location of a street view image by matching it with a\ndatabase of geo-tagged aerial images. The cross-view matching task is extremely\nchallenging due to drastic appearance and geometry differences across views.\nUnlike existing methods that predominantly fall back on CNN, here we devise a\nnovel evolving geo-localization Transformer (EgoTR) that utilizes the\nproperties of self-attention in Transformer to model global dependencies, thus\nsignificantly decreasing visual ambiguities in cross-view geo-localization. We\nalso exploit the positional encoding of Transformer to help the EgoTR\nunderstand and correspond geometric configurations between ground and aerial\nimages. Compared to state-of-the-art methods that impose strong assumption on\ngeometry knowledge, the EgoTR flexibly learns the positional embeddings through\nthe training objective and hence becomes more practical in many real-world\nscenarios. Although Transformer is well suited to our task, its vanilla\nself-attention mechanism independently interacts within image patches in each\nlayer, which overlooks correlations between layers. Instead, this paper propose\na simple yet effective self-cross attention mechanism to improve the quality of\nlearned representations. The self-cross attention models global dependencies\nbetween adjacent layers, which relates between image patches while modeling how\nfeatures evolve in the previous layer. As a result, the proposed self-cross\nattention leads to more stable training, improves the generalization ability\nand encourages representations to keep evolving as the network goes deeper.\nExtensive experiments demonstrate that our EgoTR performs favorably against\nstate-of-the-art methods on standard, fine-grained and cross-dataset cross-view\ngeo-localization tasks.",
          "link": "http://arxiv.org/abs/2107.00842",
          "publishedOn": "2021-07-05T01:54:57.826Z",
          "wordCount": 679,
          "title": "Cross-view Geo-localization with Evolving Transformer. (arXiv:2107.00842v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00968",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Suwannaphong_T/0/1/0/all/0/1\">Thanaphon Suwannaphong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chavana_S/0/1/0/all/0/1\">Sawaphob Chavana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tongsom_S/0/1/0/all/0/1\">Sahapol Tongsom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palasuwan_D/0/1/0/all/0/1\">Duangdao Palasuwan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chalidabhongse_T/0/1/0/all/0/1\">Thanarat H. Chalidabhongse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anantrasirichai_N/0/1/0/all/0/1\">Nantheera Anantrasirichai</a>",
          "description": "Intestinal parasitic infection leads to several morbidities to humans\nworldwide, especially in tropical countries. The traditional diagnosis usually\nrelies on manual analysis from microscopic images which is prone to human error\ndue to morphological similarity of different parasitic eggs and abundance of\nimpurities in a sample. Many studies have developed automatic systems for\nparasite egg detection to reduce human workload. However, they work with high\nquality microscopes, which unfortunately remain unaffordable in some rural\nareas. Our work thus exploits a benefit of a low-cost USB microscope. This\ninstrument however provides poor quality of images due to limitation of\nmagnification (10x), causing difficulty in parasite detection and species\nclassification. In this paper, we propose a CNN-based technique using transfer\nlearning strategy to enhance the efficiency of automatic parasite\nclassification in poor-quality microscopic images. The patch-based technique\nwith sliding window is employed to search for location of the eggs. Two\nnetworks, AlexNet and ResNet50, are examined with a trade-off between\narchitecture size and classification performance. The results show that our\nproposed framework outperforms the state-of-the-art object recognition methods.\nOur system combined with final decision from an expert may improve the real\nfaecal examination with low-cost microscopes.",
          "link": "http://arxiv.org/abs/2107.00968",
          "publishedOn": "2021-07-05T01:54:57.820Z",
          "wordCount": 654,
          "title": "Parasitic Egg Detection and Classification in Low-cost Microscopic Images using Transfer Learning. (arXiv:2107.00968v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01151",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haiyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenguan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xizhou Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1\">Jifeng Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liwei Wang</a>",
          "description": "As a fundamental problem for Artificial Intelligence, multi-agent system\n(MAS) is making rapid progress, mainly driven by multi-agent reinforcement\nlearning (MARL) techniques. However, previous MARL methods largely focused on\ngrid-world like or game environments; MAS in visually rich environments has\nremained less explored. To narrow this gap and emphasize the crucial role of\nperception in MAS, we propose a large-scale 3D dataset, CollaVN, for\nmulti-agent visual navigation (MAVN). In CollaVN, multiple agents are entailed\nto cooperatively navigate across photo-realistic environments to reach target\nlocations. Diverse MAVN variants are explored to make our problem more general.\nMoreover, a memory-augmented communication framework is proposed. Each agent is\nequipped with a private, external memory to persistently store communication\ninformation. This allows agents to make better use of their past communication\ninformation, enabling more efficient collaboration and robust long-term\nplanning. In our experiments, several baselines and evaluation metrics are\ndesigned. We also empirically verify the efficacy of our proposed MARL approach\nacross different MAVN task settings.",
          "link": "http://arxiv.org/abs/2107.01151",
          "publishedOn": "2021-07-05T01:54:57.814Z",
          "wordCount": 596,
          "title": "Collaborative Visual Navigation. (arXiv:2107.01151v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00993",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khanam_Z/0/1/0/all/0/1\">Zeba Khanam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Usmani_A/0/1/0/all/0/1\">Atiya Usmani</a>",
          "description": "Braille has empowered visually challenged community to read and write. But at\nthe same time, it has created a gap due to widespread inability of non-Braille\nusers to understand Braille scripts. This gap has fuelled researchers to\npropose Optical Braille Recognition techniques to convert Braille documents to\nnatural language. The main motivation of this work is to cement the\ncommunication gap at academic institutions by translating personal documents of\nblind students. This has been accomplished by proposing an economical and\neffective technique which digitizes Braille documents using a smartphone\ncamera. For any given Braille image, a dot detection mechanism based on Hough\ntransform is proposed which is invariant to skewness, noise and other\ndeterrents. The detected dots are then clustered into Braille cells using\ndistance-based clustering algorithm. In succession, the standard physical\nparameters of each Braille cells are estimated for feature extraction and\nclassification as natural language characters. The comprehensive evaluation of\nthis technique on the proposed dataset of 54 Braille scripts has yielded into\naccuracy of 98.71%.",
          "link": "http://arxiv.org/abs/2107.00993",
          "publishedOn": "2021-07-05T01:54:57.793Z",
          "wordCount": 598,
          "title": "Optical Braille Recognition using Circular Hough Transform. (arXiv:2107.00993v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01079",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hammernik_K/0/1/0/all/0/1\">Kerstin Hammernik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_C/0/1/0/all/0/1\">Cheng Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_C/0/1/0/all/0/1\">Chen Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_W/0/1/0/all/0/1\">Wenjia Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rueckert_D/0/1/0/all/0/1\">Daniel Rueckert</a>",
          "description": "Deep learning-based segmentation methods are vulnerable to unforeseen data\ndistribution shifts during deployment, e.g. change of image appearances or\ncontrasts caused by different scanners, unexpected imaging artifacts etc. In\nthis paper, we present a cooperative framework for training image segmentation\nmodels and a latent space augmentation method for generating hard examples.\nBoth contributions improve model generalization and robustness with limited\ndata. The cooperative training framework consists of a fast-thinking network\n(FTN) and a slow-thinking network (STN). The FTN learns decoupled image\nfeatures and shape features for image reconstruction and segmentation tasks.\nThe STN learns shape priors for segmentation correction and refinement. The two\nnetworks are trained in a cooperative manner. The latent space augmentation\ngenerates challenging examples for training by masking the decoupled latent\nspace in both channel-wise and spatial-wise manners. We performed extensive\nexperiments on public cardiac imaging datasets. Using only 10 subjects from a\nsingle site for training, we demonstrated improved cross-site segmentation\nperformance and increased robustness against various unforeseen imaging\nartifacts compared to strong baseline methods. Particularly, cooperative\ntraining with latent space data augmentation yields 15% improvement in terms of\naverage Dice score when compared to a standard training method.",
          "link": "http://arxiv.org/abs/2107.01079",
          "publishedOn": "2021-07-05T01:54:57.734Z",
          "wordCount": 657,
          "title": "Cooperative Training and Latent Space Data Augmentation for Robust Medical Image Segmentation. (arXiv:2107.01079v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01002",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Karmanov_I/0/1/0/all/0/1\">Ilia Karmanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zanjani_F/0/1/0/all/0/1\">Farhad G. Zanjani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Merlin_S/0/1/0/all/0/1\">Simone Merlin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kadampot_I/0/1/0/all/0/1\">Ishaque Kadampot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dijkman_D/0/1/0/all/0/1\">Daniel Dijkman</a>",
          "description": "We introduce WiCluster, a new machine learning (ML) approach for passive\nindoor positioning using radio frequency (RF) channel state information (CSI).\nWiCluster can predict both a zone-level position and a precise 2D or 3D\nposition, without using any precise position labels during training. Prior\nCSI-based indoor positioning work has relied on non-parametric approaches using\ndigital signal-processing (DSP) and, more recently, parametric approaches\n(e.g., fully supervised ML methods). However these do not handle the complexity\nof real-world environments well and do not meet requirements for large-scale\ncommercial deployments: the accuracy of DSP-based method deteriorates\nsignificantly in non-line-of-sight conditions, while supervised ML methods need\nlarge amounts of hard-to-acquire centimeter accuracy position labels. In\ncontrast, WiCluster is both precise and requires weaker label-information that\ncan be easily collected. Our first contribution is a novel dimensionality\nreduction method for charting. It combines a triplet-loss with a multi-scale\nclustering-loss to map the high-dimensional CSI representation to a 2D/3D\nlatent space. Our second contribution is two weakly supervised losses that map\nthis latent space into a Cartesian map, resulting in meter-accuracy position\nresults. These losses only require simple to acquire priors: a sketch of the\nfloorplan, approximate location of access-point locations and a few CSI packets\nthat are labeled with the corresponding zone in the floorplan. Thirdly, we\nreport results and a robustness study for 2D positioning in a single-floor\noffice building and 3D positioning in a two-floor home to show the robustness\nof our method.",
          "link": "http://arxiv.org/abs/2107.01002",
          "publishedOn": "2021-07-05T01:54:57.709Z",
          "wordCount": 696,
          "title": "WiCluster: Passive Indoor 2D/3D Positioning using WiFi without Precise Labels. (arXiv:2107.01002v1 [cs.NI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00964",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zafeiropoulos_C/0/1/0/all/0/1\">Charalampos Zafeiropoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tzortzis_I/0/1/0/all/0/1\">Ioannis N. Tzortzis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rallis_I/0/1/0/all/0/1\">Ioannis Rallis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Protopapadakis_E/0/1/0/all/0/1\">Eftychios Protopapadakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doulamis_N/0/1/0/all/0/1\">Nikolaos Doulamis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doulamis_A/0/1/0/all/0/1\">Anastasios Doulamis</a>",
          "description": "In this paper, we scrutinize the effectiveness of various clustering\ntechniques, investigating their applicability in Cultural Heritage monitoring\napplications. In the context of this paper, we detect the level of\ndecomposition and corrosion on the walls of Saint Nicholas fort in Rhodes\nutilizing hyperspectral images. A total of 6 different clustering approaches\nhave been evaluated over a set of 14 different orthorectified hyperspectral\nimages. Experimental setup in this study involves K-means, Spectral, Meanshift,\nDBSCAN, Birch and Optics algorithms. For each of these techniques we evaluate\nits performance by the use of performance metrics such as Calinski-Harabasz,\nDavies-Bouldin indexes and Silhouette value. In this approach, we evaluate the\noutcomes of the clustering methods by comparing them with a set of annotated\nimages which denotes the ground truth regarding the decomposition and/or\ncorrosion area of the original images. The results depict that a few clustering\ntechniques applied on the given dataset succeeded decent accuracy, precision,\nrecall and f1 scores. Eventually, it was observed that the deterioration was\ndetected quite accurately.",
          "link": "http://arxiv.org/abs/2107.00964",
          "publishedOn": "2021-07-05T01:54:57.695Z",
          "wordCount": 616,
          "title": "Evaluating the Usefulness of Unsupervised monitoring in Cultural Heritage Monuments. (arXiv:2107.00964v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00875",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ghamsarian_N/0/1/0/all/0/1\">Negin Ghamsarian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Taschwer_M/0/1/0/all/0/1\">Mario Taschwer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Putzgruber_Adamitsch_D/0/1/0/all/0/1\">Doris Putzgruber-Adamitsch</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sarny_S/0/1/0/all/0/1\">Stephanie Sarny</a>, <a href=\"http://arxiv.org/find/eess/1/au:+El_Shabrawi_Y/0/1/0/all/0/1\">Yosuf El-Shabrawi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Schoeffmann_K/0/1/0/all/0/1\">Klaus Schoeffmann</a>",
          "description": "A critical complication after cataract surgery is the dislocation of the lens\nimplant leading to vision deterioration and eye trauma. In order to reduce the\nrisk of this complication, it is vital to discover the risk factors during the\nsurgery. However, studying the relationship between lens dislocation and its\nsuspicious risk factors using numerous videos is a time-extensive procedure.\nHence, the surgeons demand an automatic approach to enable a larger-scale and,\naccordingly, more reliable study. In this paper, we propose a novel framework\nas the major step towards lens irregularity detection. In particular, we\npropose (I) an end-to-end recurrent neural network to recognize the\nlens-implantation phase and (II) a novel semantic segmentation network to\nsegment the lens and pupil after the implantation phase. The phase recognition\nresults reveal the effectiveness of the proposed surgical phase recognition\napproach. Moreover, the segmentation results confirm the proposed segmentation\nnetwork's effectiveness compared to state-of-the-art rival approaches.",
          "link": "http://arxiv.org/abs/2107.00875",
          "publishedOn": "2021-07-05T01:54:57.683Z",
          "wordCount": 628,
          "title": "LensID: A CNN-RNN-Based Framework Towards Lens Irregularity Detection in Cataract Surgery Videos. (arXiv:2107.00875v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00961",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dun_C/0/1/0/all/0/1\">Chen Dun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolfe_C/0/1/0/all/0/1\">Cameron R. Wolfe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jermaine_C/0/1/0/all/0/1\">Christopher M. Jermaine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kyrillidis_A/0/1/0/all/0/1\">Anastasios Kyrillidis</a>",
          "description": "We propose {\\rm \\texttt{ResIST}}, a novel distributed training protocol for\nResidual Networks (ResNets). {\\rm \\texttt{ResIST}} randomly decomposes a global\nResNet into several shallow sub-ResNets that are trained independently in a\ndistributed manner for several local iterations, before having their updates\nsynchronized and aggregated into the global model. In the next round, new\nsub-ResNets are randomly generated and the process repeats. By construction,\nper iteration, {\\rm \\texttt{ResIST}} communicates only a small portion of\nnetwork parameters to each machine and never uses the full model during\ntraining. Thus, {\\rm \\texttt{ResIST}} reduces the communication, memory, and\ntime requirements of ResNet training to only a fraction of the requirements of\nprevious methods. In comparison to common protocols like data-parallel training\nand data-parallel training with local SGD, {\\rm \\texttt{ResIST}} yields a\ndecrease in wall-clock training time, while being competitive with respect to\nmodel performance.",
          "link": "http://arxiv.org/abs/2107.00961",
          "publishedOn": "2021-07-05T01:54:57.676Z",
          "wordCount": 593,
          "title": "ResIST: Layer-Wise Decomposition of ResNets for Distributed Training. (arXiv:2107.00961v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00986",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yue_Z/0/1/0/all/0/1\">Zongsheng Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1\">Qian Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jianwen Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_D/0/1/0/all/0/1\">Deyu Meng</a>",
          "description": "While the researches on single image super-resolution (SISR), especially\nequipped with deep neural networks (DNNs), have achieved tremendous successes\nrecently, they still suffer from two major limitations. Firstly, the real image\ndegradation is usually unknown and highly variant from one to another, making\nit extremely hard to train a single model to handle the general SISR task.\nSecondly, most of current methods mainly focus on the downsampling process of\nthe degradation, but ignore or underestimate the inevitable noise\ncontamination. For example, the commonly-used independent and identically\ndistributed (i.i.d.) Gaussian noise distribution always largely deviates from\nthe real image noise (e.g., camera sensor noise), which limits their\nperformance in real scenarios. To address these issues, this paper proposes a\nmodel-based unsupervised SISR method to deal with the general SISR task with\nunknown degradations. Instead of the traditional i.i.d. Gaussian noise\nassumption, a novel patch-based non-i.i.d. noise modeling method is proposed to\nfit the complex real noise. Besides, a deep generator parameterized by a DNN is\nused to map the latent variable to the high-resolution image, and the\nconventional hyper-Laplacian prior is also elaborately embedded into such\ngenerator to further constrain the image gradients. Finally, a Monte Carlo EM\nalgorithm is designed to solve our model, which provides a general inference\nframework to update the image generator both w.r.t. the latent variable and the\nnetwork parameters. Comprehensive experiments demonstrate that the proposed\nmethod can evidently surpass the current state of the art (SotA) method (about\n1dB PSNR) not only with a slighter model (0.34M vs. 2.40M) but also faster\nspeed.",
          "link": "http://arxiv.org/abs/2107.00986",
          "publishedOn": "2021-07-05T01:54:57.667Z",
          "wordCount": 696,
          "title": "Unsupervised Single Image Super-resolution Under Complex Noise. (arXiv:2107.00986v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00977",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Reynaud_H/0/1/0/all/0/1\">Hadrien Reynaud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vlontzos_A/0/1/0/all/0/1\">Athanasios Vlontzos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_B/0/1/0/all/0/1\">Benjamin Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beqiri_A/0/1/0/all/0/1\">Arian Beqiri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leeson_P/0/1/0/all/0/1\">Paul Leeson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kainz_B/0/1/0/all/0/1\">Bernhard Kainz</a>",
          "description": "Cardiac ultrasound imaging is used to diagnose various heart diseases. Common\nanalysis pipelines involve manual processing of the video frames by expert\nclinicians. This suffers from intra- and inter-observer variability. We propose\na novel approach to ultrasound video analysis using a transformer architecture\nbased on a Residual Auto-Encoder Network and a BERT model adapted for token\nclassification. This enables videos of any length to be processed. We apply our\nmodel to the task of End-Systolic (ES) and End-Diastolic (ED) frame detection\nand the automated computation of the left ventricular ejection fraction. We\nachieve an average frame distance of 3.36 frames for the ES and 7.17 frames for\nthe ED on videos of arbitrary length. Our end-to-end learnable approach can\nestimate the ejection fraction with a MAE of 5.95 and $R^2$ of 0.52 in 0.15s\nper video, showing that segmentation is not the only way to predict ejection\nfraction. Code and models are available at https://github.com/HReynaud/UVT.",
          "link": "http://arxiv.org/abs/2107.00977",
          "publishedOn": "2021-07-05T01:54:57.648Z",
          "wordCount": 602,
          "title": "Ultrasound Video Transformers for Cardiac Ejection Fraction Estimation. (arXiv:2107.00977v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00860",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hayeon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hyung_E/0/1/0/all/0/1\">Eunyoung Hyung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1\">Sung Ju Hwang</a>",
          "description": "Despite the success of recent Neural Architecture Search (NAS) methods on\nvarious tasks which have shown to output networks that largely outperform\nhuman-designed networks, conventional NAS methods have mostly tackled the\noptimization of searching for the network architecture for a single task\n(dataset), which does not generalize well across multiple tasks (datasets).\nMoreover, since such task-specific methods search for a neural architecture\nfrom scratch for every given task, they incur a large computational cost, which\nis problematic when the time and monetary budget are limited. In this paper, we\npropose an efficient NAS framework that is trained once on a database\nconsisting of datasets and pretrained networks and can rapidly search for a\nneural architecture for a novel dataset. The proposed MetaD2A (Meta\nDataset-to-Architecture) model can stochastically generate graphs\n(architectures) from a given set (dataset) via a cross-modal latent space\nlearned with amortized meta-learning. Moreover, we also propose a\nmeta-performance predictor to estimate and select the best architecture without\ndirect training on target datasets. The experimental results demonstrate that\nour model meta-learned on subsets of ImageNet-1K and architectures from\nNAS-Bench 201 search space successfully generalizes to multiple unseen datasets\nincluding CIFAR-10 and CIFAR-100, with an average search time of 33 GPU\nseconds. Even under MobileNetV3 search space, MetaD2A is 5.5K times faster than\nNSGANetV2, a transferable NAS method, with comparable performance. We believe\nthat the MetaD2A proposes a new research direction for rapid NAS as well as\nways to utilize the knowledge from rich databases of datasets and architectures\naccumulated over the past years. Code is available at\nhttps://github.com/HayeonLee/MetaD2A.",
          "link": "http://arxiv.org/abs/2107.00860",
          "publishedOn": "2021-07-05T01:54:57.638Z",
          "wordCount": 705,
          "title": "Rapid Neural Architecture Search by Learning to Generate Graphs from Datasets. (arXiv:2107.00860v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00934",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiahui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xiaodi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhiqiang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_Q/0/1/0/all/0/1\">Qi Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongsheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metaxas_D/0/1/0/all/0/1\">Dimitris N. Metaxas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shaoting Zhang</a>",
          "description": "Weak supervision learning on classification labels has demonstrated high\nperformance in various tasks. When a few pixel-level fine annotations are also\naffordable, it is natural to leverage both of the pixel-level (e.g.,\nsegmentation) and image level (e.g., classification) annotation to further\nimprove the performance. In computational pathology, however, such weak or\nmixed supervision learning is still a challenging task, since the high\nresolution of whole slide images makes it unattainable to perform end-to-end\ntraining of classification models. An alternative approach is to analyze such\ndata by patch-base model training, i.e., using self-supervised learning to\ngenerate pixel-level pseudo labels for patches. However, such methods usually\nhave model drifting issues, i.e., hard to converge, because the noise\naccumulates during the self-training process. To handle those problems, we\npropose a mixed supervision learning framework for super high-resolution images\nto effectively utilize their various labels (e.g., sufficient image-level\ncoarse annotations and a few pixel-level fine labels). During the patch\ntraining stage, this framework can make use of coarse image-level labels to\nrefine self-supervised learning and generate high-quality pixel-level pseudo\nlabels. A comprehensive strategy is proposed to suppress pixel-level false\npositives and false negatives. Three real-world datasets with very large number\nof images (i.e., more than 10,000 whole slide images) and various types of\nlabels are used to evaluate the effectiveness of mixed supervision learning. We\nreduced the false positive rate by around one third compared to state of the\nart while retaining 100\\% sensitivity, in the task of image-level\nclassification.",
          "link": "http://arxiv.org/abs/2107.00934",
          "publishedOn": "2021-07-05T01:54:57.630Z",
          "wordCount": 690,
          "title": "Mixed Supervision Learning for Whole Slide Image Classification. (arXiv:2107.00934v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01063",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yibao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xingru Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Huiyu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qianni Zhang</a>",
          "description": "The classification of histopathological images is of great value in both\ncancer diagnosis and pathological studies. However, multiple reasons, such as\nvariations caused by magnification factors and class imbalance, make it a\nchallenging task where conventional methods that learn from image-label\ndatasets perform unsatisfactorily in many cases. We observe that tumours of the\nsame class often share common morphological patterns. To exploit this fact, we\npropose an approach that learns similarity-based multi-scale embeddings (SMSE)\nfor magnification-independent histopathological image classification. In\nparticular, a pair loss and a triplet loss are leveraged to learn\nsimilarity-based embeddings from image pairs or image triplets. The learned\nembeddings provide accurate measurements of similarities between images, which\nare regarded as a more effective form of representation for histopathological\nmorphology than normal image features. Furthermore, in order to ensure the\ngenerated models are magnification-independent, images acquired at different\nmagnification factors are simultaneously fed to networks during training for\nlearning multi-scale embeddings. In addition to the SMSE, to eliminate the\nimpact of class imbalance, instead of using the hard sample mining strategy\nthat intuitively discards some easy samples, we introduce a new reinforced\nfocal loss to simultaneously punish hard misclassified samples while\nsuppressing easy well-classified samples. Experimental results show that the\nSMSE improves the performance for histopathological image classification tasks\nfor both breast and liver cancers by a large margin compared to previous\nmethods. In particular, the SMSE achieves the best performance on the BreakHis\nbenchmark with an improvement ranging from 5% to 18% compared to previous\nmethods using traditional features.",
          "link": "http://arxiv.org/abs/2107.01063",
          "publishedOn": "2021-07-05T01:54:57.623Z",
          "wordCount": 690,
          "title": "Magnification-independent Histopathological Image Classification with Similarity-based Multi-scale Embeddings. (arXiv:2107.01063v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00710",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cote_Allard_U/0/1/0/all/0/1\">Ulysse C&#xf4;t&#xe9;-Allard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jakobsen_P/0/1/0/all/0/1\">Petter Jakobsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stautland_A/0/1/0/all/0/1\">Andrea Stautland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nordgreen_T/0/1/0/all/0/1\">Tine Nordgreen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fasmer_O/0/1/0/all/0/1\">Ole Bernt Fasmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oedegaard_K/0/1/0/all/0/1\">Ketil Joachim Oedegaard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torresen_J/0/1/0/all/0/1\">Jim Torresen</a>",
          "description": "Manic episodes of bipolar disorder can lead to uncritical behaviour and\ndelusional psychosis, often with destructive consequences for those affected\nand their surroundings. Early detection and intervention of a manic episode are\ncrucial to prevent escalation, hospital admission and premature death. However,\npeople with bipolar disorder may not recognize that they are experiencing a\nmanic episode and symptoms such as euphoria and increased productivity can also\ndeter affected individuals from seeking help. This work proposes to perform\nuser-independent, automatic mood-state detection based on actigraphy and\nelectrodermal activity acquired from a wrist-worn device during mania and after\nrecovery (euthymia). This paper proposes a new deep learning-based ensemble\nmethod leveraging long (20h) and short (5 minutes) time-intervals to\ndiscriminate between the mood-states. When tested on 47 bipolar patients, the\nproposed classification scheme achieves an average accuracy of 91.59% in\neuthymic/manic mood-state recognition.",
          "link": "http://arxiv.org/abs/2107.00710",
          "publishedOn": "2021-07-05T01:54:57.615Z",
          "wordCount": 609,
          "title": "Long-Short Ensemble Network for Bipolar Manic-Euthymic State Recognition Based on Wrist-worn Sensors. (arXiv:2107.00710v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00887",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hampali_S/0/1/0/all/0/1\">Shreyas Hampali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarkar_S/0/1/0/all/0/1\">Sayan Deb Sarkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lepetit_V/0/1/0/all/0/1\">Vincent Lepetit</a>",
          "description": "HO-3D is a dataset providing image sequences of various hand-object\ninteraction scenarios annotated with the 3D pose of the hand and the object and\nwas originally introduced as HO-3D_v2. The annotations were obtained\nautomatically using an optimization method, 'HOnnotate', introduced in the\noriginal paper. HO-3D_v3 provides more accurate annotations for both the hand\nand object poses thus resulting in better estimates of contact regions between\nthe hand and the object. In this report, we elaborate on the improvements to\nthe HOnnotate method and provide evaluations to compare the accuracy of\nHO-3D_v2 and HO-3D_v3. HO-3D_v3 results in 4mm higher accuracy compared to\nHO-3D_v2 for hand poses while exhibiting higher contact regions with the object\nsurface.",
          "link": "http://arxiv.org/abs/2107.00887",
          "publishedOn": "2021-07-05T01:54:57.597Z",
          "wordCount": 559,
          "title": "HO-3D_v3: Improving the Accuracy of Hand-Object Annotations of the HO-3D Dataset. (arXiv:2107.00887v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00811",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ishikawa_S/0/1/0/all/0/1\">Shintaro Ishikawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sugiura_K/0/1/0/all/0/1\">Komei Sugiura</a>",
          "description": "Currently, domestic service robots have an insufficient ability to interact\nnaturally through language. This is because understanding human instructions is\ncomplicated by various ambiguities and missing information. In existing\nmethods, the referring expressions that specify the relationships between\nobjects are insufficiently modeled. In this paper, we propose Target-dependent\nUNITER, which learns the relationship between the target object and other\nobjects directly by focusing on the relevant regions within an image, rather\nthan the whole image. Our method is an extension of the UNITER-based\nTransformer that can be pretrained on general-purpose datasets. We extend the\nUNITER approach by introducing a new architecture for handling the target\ncandidates. Our model is validated on two standard datasets, and the results\nshow that Target-dependent UNITER outperforms the baseline method in terms of\nclassification accuracy.",
          "link": "http://arxiv.org/abs/2107.00811",
          "publishedOn": "2021-07-05T01:54:57.588Z",
          "wordCount": 580,
          "title": "Target-dependent UNITER: A Transformer-Based Multimodal Language Comprehension Model for Domestic Service Robots. (arXiv:2107.00811v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00789",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kambara_M/0/1/0/all/0/1\">Motonari Kambara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sugiura_K/0/1/0/all/0/1\">Komei Sugiura</a>",
          "description": "There have been many studies in robotics to improve the communication skills\nof domestic service robots. Most studies, however, have not fully benefited\nfrom recent advances in deep neural networks because the training datasets are\nnot large enough. In this paper, our aim is to augment the datasets based on a\ncrossmodal language generation model. We propose the Case Relation Transformer\n(CRT), which generates a fetching instruction sentence from an image, such as\n\"Move the blue flip-flop to the lower left box.\" Unlike existing methods, the\nCRT uses the Transformer to integrate the visual features and geometry features\nof objects in the image. The CRT can handle the objects because of the Case\nRelation Block. We conducted comparison experiments and a human evaluation. The\nexperimental results show the CRT outperforms baseline methods.",
          "link": "http://arxiv.org/abs/2107.00789",
          "publishedOn": "2021-07-05T01:54:57.581Z",
          "wordCount": 580,
          "title": "Case Relation Transformer: A Crossmodal Language Generation Model for Fetching Instructions. (arXiv:2107.00789v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00704",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Junqing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruzhansky_M/0/1/0/all/0/1\">Michael Ruzhansky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qianying Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haihui Wang</a>",
          "description": "This paper presents a novel intrinsic image transfer (IIT) algorithm for\nillumination manipulation, which creates a local image translation between two\nillumination surfaces. This model is built on an optimization-based framework\nconsisting of three photo-realistic losses defined on the sub-layers factorized\nby an intrinsic image decomposition. We illustrate that all losses can be\nreduced without the necessity of taking an intrinsic image decomposition under\nthe well-known spatial-varying illumination illumination-invariant reflectance\nprior knowledge. Moreover, with a series of relaxations, all of them can be\ndirectly defined on images, giving a closed-form solution for image\nillumination manipulation. This new paradigm differs from the prevailing\nRetinex-based algorithms, as it provides an implicit way to deal with the\nper-pixel image illumination. We finally demonstrate its versatility and\nbenefits to the illumination-related tasks such as illumination compensation,\nimage enhancement, and high dynamic range (HDR) image compression, and show the\nhigh-quality results on natural image datasets.",
          "link": "http://arxiv.org/abs/2107.00704",
          "publishedOn": "2021-07-05T01:54:57.572Z",
          "wordCount": 581,
          "title": "Intrinsic Image Transfer for Illumination Manipulation. (arXiv:2107.00704v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00808",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoni Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yucan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weiping Wang</a>",
          "description": "Hierarchical classification is significant for complex tasks by providing\nmulti-granular predictions and encouraging better mistakes. As the label\nstructure decides its performance, many existing approaches attempt to\nconstruct an excellent label structure for promoting the classification\nresults. In this paper, we consider that different label structures provide a\nvariety of prior knowledge for category recognition, thus fusing them is\nhelpful to achieve better hierarchical classification results. Furthermore, we\npropose a multi-task multi-structure fusion model to integrate different label\nstructures. It contains two kinds of branches: one is the traditional\nclassification branch to classify the common subclasses, the other is\nresponsible for identifying the heterogeneous superclasses defined by different\nlabel structures. Besides the effect of multiple label structures, we also\nexplore the architecture of the deep model for better hierachical\nclassification and adjust the hierarchical evaluation metrics for multiple\nlabel structures. Experimental results on CIFAR100 and Car196 show that our\nmethod obtains significantly better results than using a flat classifier or a\nhierarchical classifier with any single label structure.",
          "link": "http://arxiv.org/abs/2107.00808",
          "publishedOn": "2021-07-05T01:54:57.476Z",
          "wordCount": 609,
          "title": "MMF: Multi-Task Multi-Structure Fusion for Hierarchical Image Classification. (arXiv:2107.00808v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00727",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Shanu Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurmi_V/0/1/0/all/0/1\">Vinod Kumar Kurmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_P/0/1/0/all/0/1\">Praphul Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Namboodiri_V/0/1/0/all/0/1\">Vinay P Namboodiri</a>",
          "description": "Understanding unsupervised domain adaptation has been an important task that\nhas been well explored. However, the wide variety of methods have not analyzed\nthe role of a classifier's performance in detail. In this paper, we thoroughly\nexamine the role of a classifier in terms of matching source and target\ndistributions. We specifically investigate the classifier ability by matching\na) the distribution of features, b) probabilistic uncertainty for samples and\nc) certainty activation mappings. Our analysis suggests that using these three\ndistributions does result in a consistently improved performance on all the\ndatasets. Our work thus extends present knowledge on the role of the various\ndistributions obtained from the classifier towards solving unsupervised domain\nadaptation.",
          "link": "http://arxiv.org/abs/2107.00727",
          "publishedOn": "2021-07-05T01:54:57.469Z",
          "wordCount": 552,
          "title": "Mitigating Uncertainty of Classifier for Unsupervised Domain Adaptation. (arXiv:2107.00727v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00712",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rebol_M/0/1/0/all/0/1\">Manuel Rebol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gutl_C/0/1/0/all/0/1\">Christian G&#xfc;tl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pietroszek_K/0/1/0/all/0/1\">Krzysztof Pietroszek</a>",
          "description": "In real life, people communicate using both speech and non-verbal signals\nsuch as gestures, face expression or body pose. Non-verbal signals impact the\nmeaning of the spoken utterance in an abundance of ways. An absence of\nnon-verbal signals impoverishes the process of communication. Yet, when users\nare represented as avatars, it is difficult to translate non-verbal signals\nalong with the speech into the virtual world without specialized motion-capture\nhardware. In this paper, we propose a novel, data-driven technique for\ngenerating gestures directly from speech. Our approach is based on the\napplication of Generative Adversarial Neural Networks (GANs) to model the\ncorrelation rather than causation between speech and gestures. This approach\napproximates neuroscience findings on how non-verbal communication and speech\nare correlated. We create a large dataset which consists of speech and\ncorresponding gestures in a 3D human pose format from which our model learns\nthe speaker-specific correlation. We evaluate the proposed technique in a user\nstudy that is inspired by the Turing test. For the study, we animate the\ngenerated gestures on a virtual character. We find that users are not able to\ndistinguish between the generated and the recorded gestures. Moreover, users\nare able to identify our synthesized gestures as related or not related to a\ngiven utterance.",
          "link": "http://arxiv.org/abs/2107.00712",
          "publishedOn": "2021-07-05T01:54:57.461Z",
          "wordCount": 662,
          "title": "Passing a Non-verbal Turing Test: Evaluating Gesture Animations Generated from Speech. (arXiv:2107.00712v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00818",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Pengcheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_L/0/1/0/all/0/1\">Lingqiao Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_Z/0/1/0/all/0/1\">Zhilong Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yuan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>",
          "description": "In this technical report, we briefly introduce the solution of our team\n\"TAL-ai\" for (Semi-) supervised Face detection in the low light condition in\nUG2+ Challenge in CVPR 2021. By conducting several experiments with popular\nimage enhancement methods and image transfer methods, we pulled the low light\nimage and the normal image to a more closer domain. And it is observed that\nusing these data to training can achieve better performance. We also adapt\nseveral popular object detection frameworks, e.g., DetectoRS, Cascade-RCNN, and\nlarge backbone like Swin-transformer. Finally, we ensemble several models which\nachieved mAP 74.89 on the testing set, ranking 1st on the final leaderboard.",
          "link": "http://arxiv.org/abs/2107.00818",
          "publishedOn": "2021-07-05T01:54:57.455Z",
          "wordCount": 559,
          "title": "1st Place Solutions for UG2+ Challenge 2021 -- (Semi-)supervised Face detection in the low light condition. (arXiv:2107.00818v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00769",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Glaser_N/0/1/0/all/0/1\">Nathaniel Glaser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yen-Cheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_J/0/1/0/all/0/1\">Junjiao Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kira_Z/0/1/0/all/0/1\">Zsolt Kira</a>",
          "description": "In this paper, we address the multi-robot collaborative perception problem,\nspecifically in the context of multi-view infilling for distributed semantic\nsegmentation. This setting entails several real-world challenges, especially\nthose relating to unregistered multi-agent image data. Solutions must\neffectively leverage multiple, non-static, and intermittently-overlapping RGB\nperspectives. To this end, we propose the Multi-Agent Infilling Network: an\nextensible neural architecture that can be deployed (in a distributed manner)\nto each agent in a robotic swarm. Specifically, each robot is in charge of\nlocally encoding and decoding visual information, and an extensible neural\nmechanism allows for an uncertainty-aware and context-based exchange of\nintermediate features. We demonstrate improved performance on a realistic\nmulti-robot AirSim dataset.",
          "link": "http://arxiv.org/abs/2107.00769",
          "publishedOn": "2021-07-05T01:54:57.448Z",
          "wordCount": 572,
          "title": "Enhancing Multi-Robot Perception via Learned Data Association. (arXiv:2107.00769v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00717",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kothawade_S/0/1/0/all/0/1\">Suraj Kothawade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beck_N/0/1/0/all/0/1\">Nathan Beck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Killamsetty_K/0/1/0/all/0/1\">Krishnateja Killamsetty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyer_R/0/1/0/all/0/1\">Rishabh Iyer</a>",
          "description": "Active learning has proven to be useful for minimizing labeling costs by\nselecting the most informative samples. However, existing active learning\nmethods do not work well in realistic scenarios such as imbalance or rare\nclasses, out-of-distribution data in the unlabeled set, and redundancy. In this\nwork, we propose SIMILAR (Submodular Information Measures based actIve\nLeARning), a unified active learning framework using recently proposed\nsubmodular information measures (SIM) as acquisition functions. We argue that\nSIMILAR not only works in standard active learning, but also easily extends to\nthe realistic settings considered above and acts as a one-stop solution for\nactive learning that is scalable to large real-world datasets. Empirically, we\nshow that SIMILAR significantly outperforms existing active learning algorithms\nby as much as ~5% - 18% in the case of rare classes and ~5% - 10% in the case\nof out-of-distribution data on several image classification tasks like\nCIFAR-10, MNIST, and ImageNet.",
          "link": "http://arxiv.org/abs/2107.00717",
          "publishedOn": "2021-07-05T01:54:57.432Z",
          "wordCount": 592,
          "title": "SIMILAR: Submodular Information Measures Based Active Learning In Realistic Scenarios. (arXiv:2107.00717v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00691",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mirsadeghi_S/0/1/0/all/0/1\">S. Ehsan Mirsadeghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Royat_A/0/1/0/all/0/1\">Ali Royat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rezatofighi_H/0/1/0/all/0/1\">Hamid Rezatofighi</a>",
          "description": "Semantic segmentation is one of the basic, yet essential scene understanding\ntasks for an autonomous agent. The recent developments in supervised machine\nlearning and neural networks have enjoyed great success in enhancing the\nperformance of the state-of-the-art techniques for this task. However, their\nsuperior performance is highly reliant on the availability of a large-scale\nannotated dataset. In this paper, we propose a novel fully unsupervised\nsemantic segmentation method, the so-called Information Maximization and\nAdversarial Regularization Segmentation (InMARS). Inspired by human perception\nwhich parses a scene into perceptual groups, rather than analyzing each pixel\nindividually, our proposed approach first partitions an input image into\nmeaningful regions (also known as superpixels). Next, it utilizes\nMutual-Information-Maximization followed by an adversarial training strategy to\ncluster these regions into semantically meaningful classes. To customize an\nadversarial training scheme for the problem, we incorporate adversarial pixel\nnoise along with spatial perturbations to impose photometrical and geometrical\ninvariance on the deep neural network. Our experiments demonstrate that our\nmethod achieves the state-of-the-art performance on two commonly used\nunsupervised semantic segmentation datasets, COCO-Stuff, and Potsdam.",
          "link": "http://arxiv.org/abs/2107.00691",
          "publishedOn": "2021-07-05T01:54:57.425Z",
          "wordCount": 623,
          "title": "Unsupervised Image Segmentation by Mutual Information Maximization and Adversarial Regularization. (arXiv:2107.00691v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00781",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yunhe Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Mu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metaxas_D/0/1/0/all/0/1\">Dimitris Metaxas</a>",
          "description": "Transformer architecture has emerged to be successful in a number of natural\nlanguage processing tasks. However, its applications to medical vision remain\nlargely unexplored. In this study, we present UTNet, a simple yet powerful\nhybrid Transformer architecture that integrates self-attention into a\nconvolutional neural network for enhancing medical image segmentation. UTNet\napplies self-attention modules in both encoder and decoder for capturing\nlong-range dependency at different scales with minimal overhead. To this end,\nwe propose an efficient self-attention mechanism along with relative position\nencoding that reduces the complexity of self-attention operation significantly\nfrom $O(n^2)$ to approximate $O(n)$. A new self-attention decoder is also\nproposed to recover fine-grained details from the skipped connections in the\nencoder. Our approach addresses the dilemma that Transformer requires huge\namounts of data to learn vision inductive bias. Our hybrid layer design allows\nthe initialization of Transformer into convolutional networks without a need of\npre-training. We have evaluated UTNet on the multi-label, multi-vendor cardiac\nmagnetic resonance imaging cohort. UTNet demonstrates superior segmentation\nperformance and robustness against the state-of-the-art approaches, holding the\npromise to generalize well on other medical image segmentations.",
          "link": "http://arxiv.org/abs/2107.00781",
          "publishedOn": "2021-07-05T01:54:57.418Z",
          "wordCount": 623,
          "title": "UTNet: A Hybrid Transformer Architecture for Medical Image Segmentation. (arXiv:2107.00781v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00782",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Huajun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fuqiang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1\">Xinyi Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1\">Dong Huang</a>",
          "description": "Pixel-wise regression is probably the most common problem in fine-grained\ncomputer vision tasks, such as estimating keypoint heatmaps and segmentation\nmasks. These regression problems are very challenging particularly because they\nrequire, at low computation overheads, modeling long-range dependencies on\nhigh-resolution inputs/outputs to estimate the highly nonlinear pixel-wise\nsemantics. While attention mechanisms in Deep Convolutional Neural\nNetworks(DCNNs) has become popular for boosting long-range dependencies,\nelement-specific attention, such as Nonlocal blocks, is highly complex and\nnoise-sensitive to learn, and most of simplified attention hybrids try to reach\nthe best compromise among multiple types of tasks. In this paper, we present\nthe Polarized Self-Attention(PSA) block that incorporates two critical designs\ntowards high-quality pixel-wise regression: (1) Polarized filtering: keeping\nhigh internal resolution in both channel and spatial attention computation\nwhile completely collapsing input tensors along their counterpart dimensions.\n(2) Enhancement: composing non-linearity that directly fits the output\ndistribution of typical fine-grained regression, such as the 2D Gaussian\ndistribution (keypoint heatmaps), or the 2D Binormial distribution (binary\nsegmentation masks). PSA appears to have exhausted the representation capacity\nwithin its channel-only and spatial-only branches, such that there is only\nmarginal metric differences between its sequential and parallel layouts.\nExperimental results show that PSA boosts standard baselines by $2-4$ points,\nand boosts state-of-the-arts by $1-2$ points on 2D pose estimation and semantic\nsegmentation benchmarks.",
          "link": "http://arxiv.org/abs/2107.00782",
          "publishedOn": "2021-07-05T01:54:57.411Z",
          "wordCount": 648,
          "title": "Polarized Self-Attention: Towards High-quality Pixel-wise Regression. (arXiv:2107.00782v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00708",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiahui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Shijian Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_F/0/1/0/all/0/1\">Fangneng Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yingchen Yu</a>",
          "description": "Image super-resolution (SR) research has witnessed impressive progress thanks\nto the advance of convolutional neural networks (CNNs) in recent years.\nHowever, most existing SR methods are non-blind and assume that degradation has\na single fixed and known distribution (e.g., bicubic) which struggle while\nhandling degradation in real-world data that usually follows a multi-modal,\nspatially variant, and unknown distribution. The recent blind SR studies\naddress this issue via degradation estimation, but they do not generalize well\nto multi-source degradation and cannot handle spatially variant degradation. We\ndesign CRL-SR, a contrastive representation learning network that focuses on\nblind SR of images with multi-modal and spatially variant distributions. CRL-SR\naddresses the blind SR challenges from two perspectives. The first is\ncontrastive decoupling encoding which introduces contrastive learning to\nextract resolution-invariant embedding and discard resolution-variant embedding\nunder the guidance of a bidirectional contrastive loss. The second is\ncontrastive feature refinement which generates lost or corrupted high-frequency\ndetails under the guidance of a conditional contrastive loss. Extensive\nexperiments on synthetic datasets and real images show that the proposed CRL-SR\ncan handle multi-modal and spatially variant degradation effectively under\nblind settings and it also outperforms state-of-the-art SR methods\nqualitatively and quantitatively.",
          "link": "http://arxiv.org/abs/2107.00708",
          "publishedOn": "2021-07-05T01:54:57.395Z",
          "wordCount": 629,
          "title": "Blind Image Super-Resolution via Contrastive Representation Learning. (arXiv:2107.00708v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00771",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Glaser_N/0/1/0/all/0/1\">Nathaniel Glaser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yen-Cheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_J/0/1/0/all/0/1\">Junjiao Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kira_Z/0/1/0/all/0/1\">Zsolt Kira</a>",
          "description": "In this paper, we address bandwidth-limited and obstruction-prone\ncollaborative perception, specifically in the context of multi-agent semantic\nsegmentation. This setting presents several key challenges, including\nprocessing and exchanging unregistered robotic swarm imagery. To be successful,\nsolutions must effectively leverage multiple non-static and\nintermittently-overlapping RGB perspectives, while heeding bandwidth\nconstraints and overcoming unwanted foreground obstructions. As such, we\npropose an end-to-end learn-able Multi-Agent Spatial Handshaking network (MASH)\nto process, compress, and propagate visual information across a robotic swarm.\nOur distributed communication module operates directly (and exclusively) on raw\nimage data, without additional input requirements such as pose, depth, or\nwarping data. We demonstrate superior performance of our model compared against\nseveral baselines in a photo-realistic multi-robot AirSim environment,\nespecially in the presence of image occlusions. Our method achieves an absolute\n11% IoU improvement over strong baselines.",
          "link": "http://arxiv.org/abs/2107.00771",
          "publishedOn": "2021-07-05T01:54:57.385Z",
          "wordCount": 576,
          "title": "Overcoming Obstructions via Bandwidth-Limited Multi-Agent Spatial Handshaking. (arXiv:2107.00771v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00689",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Youngjoo Kim</a>",
          "description": "This paper proposes a novel approach to map-based navigation system for\nunmanned aircraft. The proposed system attempts label-to-label matching, not\nimage-to-image matching between aerial images and a map database. By using\nsemantic segmentation, the ground objects are labelled and the configuration of\nthe objects is used to find the corresponding location in the map database. The\nuse of the deep learning technique as a tool for extracting high-level features\nreduces the image-based localization problem to a pattern matching problem.\nThis paper proposes a pattern matching algorithm which does not require\naltitude information or a camera model to estimate the absolute horizontal\nposition. The feasibility analysis with simulated images shows the proposed\nmap-based navigation can be realized with the proposed pattern matching\nalgorithm and it is able to provide positions given the labelled objects.",
          "link": "http://arxiv.org/abs/2107.00689",
          "publishedOn": "2021-07-05T01:54:57.312Z",
          "wordCount": 572,
          "title": "Aerial Map-Based Navigation Using Semantic Segmentation and Pattern Matching. (arXiv:2107.00689v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2007.00062",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kansizoglou_I/0/1/0/all/0/1\">Ioannis Kansizoglou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bampis_L/0/1/0/all/0/1\">Loukas Bampis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gasteratos_A/0/1/0/all/0/1\">Antonios Gasteratos</a>",
          "description": "One of the most prominent attributes of Neural Networks (NNs) constitutes\ntheir capability of learning to extract robust and descriptive features from\nhigh dimensional data, like images. Hence, such an ability renders their\nexploitation as feature extractors particularly frequent in an abundant of\nmodern reasoning systems. Their application scope mainly includes complex\ncascade tasks, like multi-modal recognition and deep Reinforcement Learning\n(RL). However, NNs induce implicit biases that are difficult to avoid or to\ndeal with and are not met in traditional image descriptors. Moreover, the lack\nof knowledge for describing the intra-layer properties -- and thus their\ngeneral behavior -- restricts the further applicability of the extracted\nfeatures. With the paper at hand, a novel way of visualizing and understanding\nthe vector space before the NNs' output layer is presented, aiming to enlighten\nthe deep feature vectors' properties under classification tasks. Main attention\nis paid to the nature of overfitting in the feature space and its adverse\neffect on further exploitation. We present the findings that can be derived\nfrom our model's formulation, and we evaluate them on realistic recognition\nscenarios, proving its prominence by improving the obtained results.",
          "link": "http://arxiv.org/abs/2007.00062",
          "publishedOn": "2021-07-02T01:58:01.431Z",
          "wordCount": 671,
          "title": "Deep Feature Space: A Geometrical Perspective. (arXiv:2007.00062v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.01209",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hudson_D/0/1/0/all/0/1\">Drew A. Hudson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zitnick_C/0/1/0/all/0/1\">C. Lawrence Zitnick</a>",
          "description": "We introduce the GANformer, a novel and efficient type of transformer, and\nexplore it for the task of visual generative modeling. The network employs a\nbipartite structure that enables long-range interactions across the image,\nwhile maintaining computation of linear efficiency, that can readily scale to\nhigh-resolution synthesis. It iteratively propagates information from a set of\nlatent variables to the evolving visual features and vice versa, to support the\nrefinement of each in light of the other and encourage the emergence of\ncompositional representations of objects and scenes. In contrast to the classic\ntransformer architecture, it utilizes multiplicative integration that allows\nflexible region-based modulation, and can thus be seen as a generalization of\nthe successful StyleGAN network. We demonstrate the model's strength and\nrobustness through a careful evaluation over a range of datasets, from\nsimulated multi-object environments to rich real-world indoor and outdoor\nscenes, showing it achieves state-of-the-art results in terms of image quality\nand diversity, while enjoying fast learning and better data-efficiency. Further\nqualitative and quantitative experiments offer us an insight into the model's\ninner workings, revealing improved interpretability and stronger\ndisentanglement, and illustrating the benefits and efficacy of our approach. An\nimplementation of the model is available at\nhttps://github.com/dorarad/gansformer.",
          "link": "http://arxiv.org/abs/2103.01209",
          "publishedOn": "2021-07-02T01:58:01.304Z",
          "wordCount": 686,
          "title": "Generative Adversarial Transformers. (arXiv:2103.01209v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12900",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Shuyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1\">Xuelong Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_B/0/1/0/all/0/1\">Bin Xiao</a>",
          "description": "Meta-learning model can quickly adapt to new tasks using few-shot labeled\ndata. However, despite achieving good generalization on few-shot classification\ntasks, it is still challenging to improve the adversarial robustness of the\nmeta-learning model in few-shot learning. Although adversarial training (AT)\nmethods such as Adversarial Query (AQ) can improve the adversarially robust\nperformance of meta-learning models, AT is still computationally expensive\ntraining. On the other hand, meta-learning models trained with AT will drop\nsignificant accuracy on the original clean images. This paper proposed a\nmeta-learning method on the adversarially robust neural network called\nLong-term Cross Adversarial Training (LCAT). LCAT will update meta-learning\nmodel parameters cross along the natural and adversarial sample distribution\ndirection with long-term to improve both adversarial and clean few-shot\nclassification accuracy. Due to cross-adversarial training, LCAT only needs\nhalf of the adversarial training epoch than AQ, resulting in a low adversarial\ntraining computation. Experiment results show that LCAT achieves superior\nperformance both on the clean and adversarial few-shot classification accuracy\nthan SOTA adversarial training methods for meta-learning models.",
          "link": "http://arxiv.org/abs/2106.12900",
          "publishedOn": "2021-07-02T01:58:01.219Z",
          "wordCount": 668,
          "title": "Long-term Cross Adversarial Training: A Robust Meta-learning Method for Few-shot Classification Tasks. (arXiv:2106.12900v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.06900",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Selvan_R/0/1/0/all/0/1\">Raghavendra Selvan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dam_E/0/1/0/all/0/1\">Erik B Dam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petersen_J/0/1/0/all/0/1\">Jens Petersen</a>",
          "description": "Tensor networks provide an efficient approximation of operations involving\nhigh dimensional tensors and have been extensively used in modelling quantum\nmany-body systems. More recently, supervised learning has been attempted with\ntensor networks, primarily focused on tasks such as image classification. In\nthis work, we propose a novel formulation of tensor networks for supervised\nimage segmentation which allows them to operate on high resolution medical\nimages. We use the matrix product state (MPS) tensor network on non-overlapping\npatches of a given input image to predict the segmentation mask by learning a\npixel-wise linear classification rule in a high dimensional space. The proposed\nmodel is end-to-end trainable using backpropagation. It is implemented as a\nStrided Tensor Network to reduce the parameter complexity. The performance of\nthe proposed method is evaluated on two public medical imaging datasets and\ncompared to relevant baselines. The evaluation shows that the strided tensor\nnetwork yields competitive performance compared to CNN-based models while using\nfewer resources. Additionally, based on the experiments we discuss the\nfeasibility of using fully linear models for segmentation tasks.",
          "link": "http://arxiv.org/abs/2102.06900",
          "publishedOn": "2021-07-02T01:58:01.211Z",
          "wordCount": 673,
          "title": "Segmenting two-dimensional structures with strided tensor networks. (arXiv:2102.06900v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.14196",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Hosseini_S/0/1/0/all/0/1\">Seyed Mohsen Hosseini</a>",
          "description": "A novel method for feature fusion in convolutional neural networks is\nproposed in this paper. Different feature fusion techniques are suggested to\nfacilitate the flow of information and improve the training of deep neural\nnetworks. Some of these techniques as well as the proposed network can be\nconsidered a type of Directed Acyclic Graph (DAG) Network, where a layer can\nreceive inputs from other layers and have outputs to other layers. In the\nproposed general framework of Lattice Fusion Network (LFNet), feature maps of\neach convolutional layer are passed to other layers based on a lattice graph\nstructure, where nodes are convolutional layers. To evaluate the performance of\nthe proposed architecture, different designs based on the general framework of\nLFNet are implemented for the task of image denoising. This task is used as an\nexample where training deep convolutional networks is needed. Results are\ncompared with state of the art methods. The proposed network is able to achieve\nbetter results with far fewer learnable parameters, which shows the\neffectiveness of LFNets for training of deep neural networks.",
          "link": "http://arxiv.org/abs/2011.14196",
          "publishedOn": "2021-07-02T01:58:01.199Z",
          "wordCount": 643,
          "title": "Lattice Fusion Networks for Image Denoising. (arXiv:2011.14196v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00648",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Braman_N/0/1/0/all/0/1\">Nathaniel Braman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gordon_J/0/1/0/all/0/1\">Jacob W. H. Gordon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goossens_E/0/1/0/all/0/1\">Emery T. Goossens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Willis_C/0/1/0/all/0/1\">Caleb Willis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stumpe_M/0/1/0/all/0/1\">Martin C. Stumpe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venkataraman_J/0/1/0/all/0/1\">Jagadish Venkataraman</a>",
          "description": "Clinical decision-making in oncology involves multimodal data such as\nradiology scans, molecular profiling, histopathology slides, and clinical\nfactors. Despite the importance of these modalities individually, no deep\nlearning framework to date has combined them all to predict patient prognosis.\nHere, we predict the overall survival (OS) of glioma patients from diverse\nmultimodal data with a Deep Orthogonal Fusion (DOF) model. The model learns to\ncombine information from multiparametric MRI exams, biopsy-based modalities\n(such as H&E slide images and/or DNA sequencing), and clinical variables into a\ncomprehensive multimodal risk score. Prognostic embeddings from each modality\nare learned and combined via attention-gated tensor fusion. To maximize the\ninformation gleaned from each modality, we introduce a multimodal\northogonalization (MMO) loss term that increases model performance by\nincentivizing constituent embeddings to be more complementary. DOF predicts OS\nin glioma patients with a median C-index of 0.788 +/- 0.067, significantly\noutperforming (p=0.023) the best performing unimodal model with a median\nC-index of 0.718 +/- 0.064. The prognostic model significantly stratifies\nglioma patients by OS within clinical subsets, adding further granularity to\nprognostic clinical grading and molecular subtyping.",
          "link": "http://arxiv.org/abs/2107.00648",
          "publishedOn": "2021-07-02T01:58:01.192Z",
          "wordCount": 659,
          "title": "Deep Orthogonal Fusion: Multimodal Prognostic Biomarker Discovery Integrating Radiology, Pathology, Genomic, and Clinical Data. (arXiv:2107.00648v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2011.09831",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fumanal_Idocin_J/0/1/0/all/0/1\">Javier Fumanal-Idocin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takac_Z/0/1/0/all/0/1\">Zdenko Tak&#xe1;&#x10d;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanz_J/0/1/0/all/0/1\">Javier Fern&#xe1;ndez Jose Antonio Sanz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyena_H/0/1/0/all/0/1\">Harkaitz Goyena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Ching-Teng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu-Kai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bustince_H/0/1/0/all/0/1\">Humberto Bustince</a>",
          "description": "In this work we study the use of moderate deviation functions to measure\nsimilarity and dissimilarity among a set of given interval-valued data. To do\nso, we introduce the notion of interval-valued moderate deviation function and\nwe study in particular those interval-valued moderate deviation functions which\npreserve the width of the input intervals. Then, we study how to apply these\nfunctions to construct interval-valued aggregation functions. We have applied\nthem in the decision making phase of two Motor-Imagery Brain Computer Interface\nframeworks, obtaining better results than those obtained using other numerical\nand intervalar aggregations.",
          "link": "http://arxiv.org/abs/2011.09831",
          "publishedOn": "2021-07-02T01:58:01.160Z",
          "wordCount": 588,
          "title": "Interval-valued aggregation functions based on moderate deviations applied to Motor-Imagery-Based Brain Computer Interface. (arXiv:2011.09831v2 [cs.HC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00606",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mazzia_V/0/1/0/all/0/1\">Vittorio Mazzia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Angarano_S/0/1/0/all/0/1\">Simone Angarano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salvetti_F/0/1/0/all/0/1\">Francesco Salvetti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Angelini_F/0/1/0/all/0/1\">Federico Angelini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiaberge_M/0/1/0/all/0/1\">Marcello Chiaberge</a>",
          "description": "Deep neural networks based purely on attention have been successful across\nseveral domains, relying on minimal architectural priors from the designer. In\nHuman Action Recognition (HAR), attention mechanisms have been primarily\nadopted on top of standard convolutional or recurrent layers, improving the\noverall generalization capability. In this work, we introduce Action\nTransformer (AcT), a simple, fully self-attentional architecture that\nconsistently outperforms more elaborated networks that mix convolutional,\nrecurrent, and attentive layers. In order to limit computational and energy\nrequests, building on previous human action recognition research, the proposed\napproach exploits 2D pose representations over small temporal windows,\nproviding a low latency solution for accurate and effective real-time\nperformance. Moreover, we open-source MPOSE2021, a new large-scale dataset, as\nan attempt to build a formal training and evaluation benchmark for real-time\nshort-time human action recognition. Extensive experimentation on MPOSE2021\nwith our proposed methodology and several previous architectural solutions\nproves the effectiveness of the AcT model and poses the base for future work on\nHAR.",
          "link": "http://arxiv.org/abs/2107.00606",
          "publishedOn": "2021-07-02T01:58:01.145Z",
          "wordCount": 607,
          "title": "Action Transformer: A Self-Attention Model for Short-Time Human Action Recognition. (arXiv:2107.00606v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2104.08581",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dutta_U/0/1/0/all/0/1\">Ujjal Kr Dutta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Repakula_S/0/1/0/all/0/1\">Sandeep Repakula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parmar_M/0/1/0/all/0/1\">Maulik Parmar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravi_A/0/1/0/all/0/1\">Abhinav Ravi</a>",
          "description": "In this paper, we utilize deep visual Representation Learning to address an\nimportant problem in fashion e-commerce: color variants identification, i.e.,\nidentifying fashion products that match exactly in their design (or style), but\nonly to differ in their color. At first we attempt to tackle the problem by\nobtaining manual annotations (depicting whether two products are color\nvariants), and train a supervised triplet loss based neural network model to\nlearn representations of fashion products. However, for large scale real-world\nindustrial datasets such as addressed in our paper, it is infeasible to obtain\nannotations for the entire dataset, while capturing all the difficult corner\ncases. Interestingly, we observed that color variants are essentially\nmanifestations of color jitter based augmentations. Thus, we instead explore\nSelf-Supervised Learning (SSL) to solve this problem. We observed that existing\nstate-of-the-art SSL methods perform poor, for our problem. To address this, we\npropose a novel SSL based color variants model that simultaneously focuses on\ndifferent parts of an apparel. Quantitative and qualitative evaluation shows\nthat our method outperforms existing SSL methods, and at times, the supervised\nmodel.",
          "link": "http://arxiv.org/abs/2104.08581",
          "publishedOn": "2021-07-02T01:58:01.060Z",
          "wordCount": 663,
          "title": "Color Variants Identification in Fashion e-commerce via Contrastive Self-Supervised Representation Learning. (arXiv:2104.08581v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.11135",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pernus_M/0/1/0/all/0/1\">Martin Pernu&#x161;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Struc_V/0/1/0/all/0/1\">Vitomir &#x160;truc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dobrisek_S/0/1/0/all/0/1\">Simon Dobri&#x161;ek</a>",
          "description": "Face editing represents a popular research topic within the computer vision\nand image processing communities. While significant progress has been made\nrecently in this area, existing solutions: (i) are still largely focused on\nlow-resolution images, (ii) often generate editing results with visual\nartefacts, or (iii) lack fine-grained control and alter multiple (entangled)\nattributes at once, when trying to generate the desired facial semantics. In\nthis paper, we aim to address these issues though a novel attribute editing\napproach called MaskFaceGAN. The proposed approach is based on an optimization\nprocedure that directly optimizes the latent code of a pre-trained\n(state-of-the-art) Generative Adversarial Network (i.e., StyleGAN2) with\nrespect to several constraints that ensure: (i) preservation of relevant image\ncontent, (ii) generation of the targeted facial attributes, and (iii)\nspatially--selective treatment of local image areas. The constraints are\nenforced with the help of an (differentiable) attribute classifier and face\nparser that provide the necessary reference information for the optimization\nprocedure. MaskFaceGAN is evaluated in extensive experiments on the CelebA-HQ,\nHelen and SiblingsDB-HQf datasets and in comparison with several\nstate-of-the-art techniques from the literature, i.e., StarGAN, AttGAN, STGAN,\nand two versions of InterFaceGAN. Our experimental results show that the\nproposed approach is able to edit face images with respect to several facial\nattributes with unprecedented image quality and at high-resolutions\n(1024x1024), while exhibiting considerably less problems with attribute\nentanglement than competing solutions. The source code is made freely available\nfrom: https://github.com/MartinPernus/MaskFaceGAN.",
          "link": "http://arxiv.org/abs/2103.11135",
          "publishedOn": "2021-07-02T01:58:00.681Z",
          "wordCount": 738,
          "title": "High Resolution Face Editing with Masked GAN Latent Code Optimization. (arXiv:2103.11135v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09517",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ren_G/0/1/0/all/0/1\">Guangyu Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stathaki_T/0/1/0/all/0/1\">Tania Stathaki</a>",
          "description": "RGB-D salient object detection(SOD) demonstrates its superiority on detecting\nin complex environments due to the additional depth information introduced in\nthe data. Inevitably, an independent stream is introduced to extract features\nfrom depth images, leading to extra computation and parameters. This\nmethodology which sacrifices the model size to improve the detection accuracy\nmay impede the practical application of SOD problems. To tackle this dilemma,\nwe propose a dynamic distillation method along with a lightweight framework,\nwhich significantly reduces the parameters. This method considers the factors\nof both teacher and student performance within the training stage and\ndynamically assigns the distillation weight instead of applying a fixed weight\non the student model. Extensive experiments are conducted on five public\ndatasets to demonstrate that our method can achieve competitive performance\ncompared to 10 prior methods through a 78.2MB lightweight structure.",
          "link": "http://arxiv.org/abs/2106.09517",
          "publishedOn": "2021-07-02T01:58:00.655Z",
          "wordCount": 597,
          "title": "Dynamic Knowledge Distillation with A Single Stream Structure for RGB-D Salient Object Detection. (arXiv:2106.09517v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.10670",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Junwei Liang</a>",
          "description": "With the advancement in computer vision deep learning, systems now are able\nto analyze an unprecedented amount of rich visual information from videos to\nenable applications such as autonomous driving, socially-aware robot assistant\nand public safety monitoring. Deciphering human behaviors to predict their\nfuture paths/trajectories and what they would do from videos is important in\nthese applications. However, human trajectory prediction still remains a\nchallenging task, as scene semantics and human intent are difficult to model.\nMany systems do not provide high-level semantic attributes to reason about\npedestrian future. This design hinders prediction performance in video data\nfrom diverse domains and unseen scenarios. To enable optimal future human\nbehavioral forecasting, it is crucial for the system to be able to detect and\nanalyze human activities as well as scene semantics, passing informative\nfeatures to the subsequent prediction module for context understanding.",
          "link": "http://arxiv.org/abs/2011.10670",
          "publishedOn": "2021-07-02T01:58:00.605Z",
          "wordCount": 617,
          "title": "From Recognition to Prediction: Analysis of Human Action and Trajectory Prediction in Video. (arXiv:2011.10670v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.03244",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Saha_A/0/1/0/all/0/1\">Anindo Saha</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hosseinzadeh_M/0/1/0/all/0/1\">Matin Hosseinzadeh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huisman_H/0/1/0/all/0/1\">Henkjan Huisman</a>",
          "description": "We present a multi-stage 3D computer-aided detection and diagnosis (CAD)\nmodel for automated localization of clinically significant prostate cancer\n(csPCa) in bi-parametric MR imaging (bpMRI). Deep attention mechanisms drive\nits detection network, targeting salient structures and highly discriminative\nfeature dimensions across multiple resolutions. Its goal is to accurately\nidentify csPCa lesions from indolent cancer and the wide range of benign\npathology that can afflict the prostate gland. Simultaneously, a decoupled\nresidual classifier is used to achieve consistent false positive reduction,\nwithout sacrificing high sensitivity or computational efficiency. In order to\nguide model generalization with domain-specific clinical knowledge, a\nprobabilistic anatomical prior is used to encode the spatial prevalence and\nzonal distinction of csPCa. Using a large dataset of 1950 prostate bpMRI paired\nwith radiologically-estimated annotations, we hypothesize that such CNN-based\nmodels can be trained to detect biopsy-confirmed malignancies in an independent\ncohort.\n\nFor 486 institutional testing scans, the 3D CAD system achieves\n83.69$\\pm$5.22% and 93.19$\\pm$2.96% detection sensitivity at 0.50 and 1.46\nfalse positive(s) per patient, respectively, with 0.882$\\pm$0.030 AUROC in\npatient-based diagnosis $-$significantly outperforming four state-of-the-art\nbaseline architectures (U-SEResNet, UNet++, nnU-Net, Attention U-Net) from\nrecent literature. For 296 external biopsy-confirmed testing scans, the\nensembled CAD system shares moderate agreement with a consensus of expert\nradiologists (76.69%; $kappa$ $=$ 0.51$\\pm$0.04) and independent pathologists\n(81.08%; $kappa$ $=$ 0.56$\\pm$0.06); demonstrating strong generalization to\nhistologically-confirmed csPCa diagnosis.",
          "link": "http://arxiv.org/abs/2101.03244",
          "publishedOn": "2021-07-02T01:58:00.583Z",
          "wordCount": 802,
          "title": "End-to-end Prostate Cancer Detection in bpMRI via 3D CNNs: Effects of Attention Mechanisms, Clinical Priori and Decoupled False Positive Reduction. (arXiv:2101.03244v10 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.12247",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nag_S/0/1/0/all/0/1\">Sayan Nag</a>",
          "description": "Self-supervised learning and pre-training strategies have developed over the\nlast few years especially for Convolutional Neural Networks (CNNs). Recently\napplication of such methods can also be noticed for Graph Neural Networks\n(GNNs) . In this paper, we have used a graph based self-supervised learning\nstrategy with different loss functions (Barlow Twins[Zbontar et al., 2021],\nHSIC[Tsai et al., 2021], VICReg[Bardes et al., 2021]) which have shown\npromising results when applied with CNNs previously. We have also proposed a\nhybrid loss function combining the advantages of VICReg and HSIC and called it\nas VICRegHSIC. The performance of these aforementioned methods have been\ncompared when applied to different datasets such as MUTAG, PROTEINS and\nIMDB-Binary. Moreover, the impact of different batch sizes, projector\ndimensions and data augmentation strategies have also been explored",
          "link": "http://arxiv.org/abs/2105.12247",
          "publishedOn": "2021-07-02T01:58:00.556Z",
          "wordCount": 626,
          "title": "Graph Self Supervised Learning: the BT, the HSIC, and the VICReg. (arXiv:2105.12247v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.06654",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Evtimov_I/0/1/0/all/0/1\">Ivan Evtimov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Covert_I/0/1/0/all/0/1\">Ian Covert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kusupati_A/0/1/0/all/0/1\">Aditya Kusupati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kohno_T/0/1/0/all/0/1\">Tadayoshi Kohno</a>",
          "description": "When data is publicly released for human consumption, it is unclear how to\nprevent its unauthorized usage for machine learning purposes. Successful model\ntraining may be preventable with carefully designed dataset modifications, and\nwe present a proof-of-concept approach for the image classification setting. We\npropose methods based on the notion of adversarial shortcuts, which encourage\nmodels to rely on non-robust signals rather than semantic features, and our\nexperiments demonstrate that these measures successfully prevent deep learning\nmodels from achieving high accuracy on real, unmodified data examples.",
          "link": "http://arxiv.org/abs/2106.06654",
          "publishedOn": "2021-07-02T01:58:00.546Z",
          "wordCount": 544,
          "title": "Disrupting Model Training with Adversarial Shortcuts. (arXiv:2106.06654v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00646",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yen_Chen_L/0/1/0/all/0/1\">Lin Yen-Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_A/0/1/0/all/0/1\">Andy Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Shuran Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Isola_P/0/1/0/all/0/1\">Phillip Isola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1\">Tsung-Yi Lin</a>",
          "description": "Does having visual priors (e.g. the ability to detect objects) facilitate\nlearning to perform vision-based manipulation (e.g. picking up objects)? We\nstudy this problem under the framework of transfer learning, where the model is\nfirst trained on a passive vision task, and adapted to perform an active\nmanipulation task. We find that pre-training on vision tasks significantly\nimproves generalization and sample efficiency for learning to manipulate\nobjects. However, realizing these gains requires careful selection of which\nparts of the model to transfer. Our key insight is that outputs of standard\nvision models highly correlate with affordance maps commonly used in\nmanipulation. Therefore, we explore directly transferring model parameters from\nvision networks to affordance prediction networks, and show that this can\nresult in successful zero-shot adaptation, where a robot can pick up certain\nobjects with zero robotic experience. With just a small amount of robotic\nexperience, we can further fine-tune the affordance model to achieve better\nresults. With just 10 minutes of suction experience or 1 hour of grasping\nexperience, our method achieves ~80% success rate at picking up novel objects.",
          "link": "http://arxiv.org/abs/2107.00646",
          "publishedOn": "2021-07-02T01:58:00.539Z",
          "wordCount": 633,
          "title": "Learning to See before Learning to Act: Visual Pre-training for Manipulation. (arXiv:2107.00646v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00652",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xiaoyi Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1\">Jianmin Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dongdong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weiming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_N/0/1/0/all/0/1\">Nenghai Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lu Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_B/0/1/0/all/0/1\">Baining Guo</a>",
          "description": "We present CSWin Transformer, an efficient and effective Transformer-based\nbackbone for general-purpose vision tasks. A challenging issue in Transformer\ndesign is that global self-attention is very expensive to compute whereas local\nself-attention often limits the field of interactions of each token. To address\nthis issue, we develop the Cross-Shaped Window self-attention mechanism for\ncomputing self-attention in the horizontal and vertical stripes in parallel\nthat form a cross-shaped window, with each stripe obtained by splitting the\ninput feature into stripes of equal width. We provide a detailed mathematical\nanalysis of the effect of the stripe width and vary the stripe width for\ndifferent layers of the Transformer network which achieves strong modeling\ncapability while limiting the computation cost. We also introduce\nLocally-enhanced Positional Encoding (LePE), which handles the local positional\ninformation better than existing encoding schemes. LePE naturally supports\narbitrary input resolutions, and is thus especially effective and friendly for\ndownstream tasks. Incorporated with these designs and a hierarchical structure,\nCSWin Transformer demonstrates competitive performance on common vision tasks.\nSpecifically, it achieves 85.4% Top-1 accuracy on ImageNet-1K without any extra\ntraining data or label, 53.9 box AP and 46.4 mask AP on the COCO detection\ntask, and 51.7 mIOU on the ADE20K semantic segmentation task, surpassing\nprevious state-of-the-art Swin Transformer backbone by +1.2, +2.0, +1.4, and\n+2.0 respectively under the similar FLOPs setting. By further pretraining on\nthe larger dataset ImageNet-21K, we achieve 87.5% Top-1 accuracy on ImageNet-1K\nand state-of-the-art segmentation performance on ADE20K with 55.2 mIoU. The\ncode and models will be available at\nhttps://github.com/microsoft/CSWin-Transformer.",
          "link": "http://arxiv.org/abs/2107.00652",
          "publishedOn": "2021-07-02T01:58:00.520Z",
          "wordCount": 721,
          "title": "CSWin Transformer: A General Vision Transformer Backbone with Cross-Shaped Windows. (arXiv:2107.00652v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2010.00824",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lirui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_Y/0/1/0/all/0/1\">Yu Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1\">Wei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mousavian_A/0/1/0/all/0/1\">Arsalan Mousavian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fox_D/0/1/0/all/0/1\">Dieter Fox</a>",
          "description": "6D robotic grasping beyond top-down bin-picking scenarios is a challenging\ntask. Previous solutions based on 6D grasp synthesis with robot motion planning\nusually operate in an open-loop setting, which are sensitive to grasp synthesis\nerrors. In this work, we propose a new method for learning closed-loop control\npolicies for 6D grasping. Our policy takes a segmented point cloud of an object\nfrom an egocentric camera as input, and outputs continuous 6D control actions\nof the robot gripper for grasping the object. We combine imitation learning and\nreinforcement learning and introduce a goal-auxiliary actor-critic algorithm\nfor policy learning. We demonstrate that our learned policy can be integrated\ninto a tabletop 6D grasping system and a human-robot handover system to improve\nthe grasping performance of unseen objects. Our videos and code can be found at\nhttps://sites.google.com/view/gaddpg .",
          "link": "http://arxiv.org/abs/2010.00824",
          "publishedOn": "2021-07-02T01:58:00.513Z",
          "wordCount": 621,
          "title": "Goal-Auxiliary Actor-Critic for 6D Robotic Grasping with Point Clouds. (arXiv:2010.00824v4 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00592",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1\">Changlin Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_R/0/1/0/all/0/1\">Rongjun Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xiao Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xu Huang</a>",
          "description": "Individual tree detection and crown delineation (ITDD) are critical in forest\ninventory management and remote sensing based forest surveys are largely\ncarried out through satellite images. However, most of these surveys only use\n2D spectral information which normally has not enough clues for ITDD. To fully\nexplore the satellite images, we propose a ITDD method using the orthophoto and\ndigital surface model (DSM) derived from the multi-view satellite data. Our\nalgorithm utilizes the top-hat morphological operation to efficiently extract\nthe local maxima from DSM as treetops, and then feed them to a modi-fied\nsuperpixel segmentation that combines both 2D and 3D information for tree crown\ndelineation. In subsequent steps, our method incorporates the biological\ncharacteristics of the crowns through plant allometric equation to falsify\npotential outliers. Experiments against manually marked tree plots on three\nrepresentative regions have demonstrated promising results - the best overall\ndetection accuracy can be 89%.",
          "link": "http://arxiv.org/abs/2107.00592",
          "publishedOn": "2021-07-02T01:58:00.506Z",
          "wordCount": 594,
          "title": "Individual Tree Detection and Crown Delineation with 3D Information from Multi-view Satellite Images. (arXiv:2107.00592v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2003.06148",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qi_L/0/1/0/all/0/1\">Lu Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yukang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yingcong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jian Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1\">Jiaya Jia</a>",
          "description": "In this paper, we explore the mask representation in instance segmentation\nwith Point-of-Interest (PoI) features. Differentiating multiple potential\ninstances within a single PoI feature is challenging because learning a\nhigh-dimensional mask feature for each instance using vanilla convolution\ndemands a heavy computing burden. To address this challenge, we propose an\ninstance-aware convolution. It decomposes this mask representation learning\ntask into two tractable modules as instance-aware weights and instance-agnostic\nfeatures. The former is to parametrize convolution for producing mask features\ncorresponding to different instances, improving mask learning efficiency by\navoiding employing several independent convolutions. Meanwhile, the latter\nserves as mask templates in a single point. Together, instance-aware mask\nfeatures are computed by convolving the template with dynamic weights, used for\nthe mask prediction. Along with instance-aware convolution, we propose\nPointINS, a simple and practical instance segmentation approach, building upon\ndense one-stage detectors. Through extensive experiments, we evaluated the\neffectiveness of our framework built upon RetinaNet and FCOS. PointINS in\nResNet101 backbone achieves a 38.3 mask mean average precision (mAP) on COCO\ndataset, outperforming existing point-based methods by a large margin. It gives\na comparable performance to the region-based Mask R-CNN with faster inference.",
          "link": "http://arxiv.org/abs/2003.06148",
          "publishedOn": "2021-07-02T01:58:00.500Z",
          "wordCount": 668,
          "title": "PointINS: Point-based Instance Segmentation. (arXiv:2003.06148v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00637",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dittadi_A/0/1/0/all/0/1\">Andrea Dittadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papa_S/0/1/0/all/0/1\">Samuele Papa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vita_M/0/1/0/all/0/1\">Michele De Vita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1\">Bernhard Sch&#xf6;lkopf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Winther_O/0/1/0/all/0/1\">Ole Winther</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Locatello_F/0/1/0/all/0/1\">Francesco Locatello</a>",
          "description": "The idea behind object-centric representation learning is that natural scenes\ncan better be modeled as compositions of objects and their relations as opposed\nto distributed representations. This inductive bias can be injected into neural\nnetworks to potentially improve systematic generalization and learning\nefficiency of downstream tasks in scenes with multiple objects. In this paper,\nwe train state-of-the-art unsupervised models on five common multi-object\ndatasets and evaluate segmentation accuracy and downstream object property\nprediction. In addition, we study systematic generalization and robustness by\ninvestigating the settings where either single objects are out-of-distribution\n-- e.g., having unseen colors, textures, and shapes -- or global properties of\nthe scene are altered -- e.g., by occlusions, cropping, or increasing the\nnumber of objects. From our experimental study, we find object-centric\nrepresentations to be generally useful for downstream tasks and robust to\nshifts in the data distribution, especially if shifts affect single objects.",
          "link": "http://arxiv.org/abs/2107.00637",
          "publishedOn": "2021-07-02T01:58:00.493Z",
          "wordCount": 593,
          "title": "Generalization and Robustness Implications in Object-Centric Learning. (arXiv:2107.00637v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2007.08194",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liang_H/0/1/0/all/0/1\">Haoyu Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_Z/0/1/0/all/0/1\">Zhihao Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1\">Yuyuan Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hang Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zihao He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_S/0/1/0/all/0/1\">Shu-Tao Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bo Zhang</a>",
          "description": "Convolutional neural networks (CNNs) have been successfully used in a range\nof tasks. However, CNNs are often viewed as \"black-box\" and lack of\ninterpretability. One main reason is due to the filter-class entanglement -- an\nintricate many-to-many correspondence between filters and classes. Most\nexisting works attempt post-hoc interpretation on a pre-trained model, while\nneglecting to reduce the entanglement underlying the model. In contrast, we\nfocus on alleviating filter-class entanglement during training. Inspired by\ncellular differentiation, we propose a novel strategy to train interpretable\nCNNs by encouraging class-specific filters, among which each filter responds to\nonly one (or few) class. Concretely, we design a learnable sparse\nClass-Specific Gate (CSG) structure to assign each filter with one (or few)\nclass in a flexible way. The gate allows a filter's activation to pass only\nwhen the input samples come from the specific class. Extensive experiments\ndemonstrate the fabulous performance of our method in generating a sparse and\nhighly class-related representation of the input, which leads to stronger\ninterpretability. Moreover, comparing with the standard training strategy, our\nmodel displays benefits in applications like object localization and\nadversarial sample detection. Code link: https://github.com/hyliang96/CSGCNN.",
          "link": "http://arxiv.org/abs/2007.08194",
          "publishedOn": "2021-07-02T01:58:00.486Z",
          "wordCount": 693,
          "title": "Training Interpretable Convolutional Neural Networks by Differentiating Class-specific Filters. (arXiv:2007.08194v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.14005",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ali_S/0/1/0/all/0/1\">Sk Aziz Ali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kahraman_K/0/1/0/all/0/1\">Kerem Kahraman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1\">Christian Theobalt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stricker_D/0/1/0/all/0/1\">Didier Stricker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Golyanik_V/0/1/0/all/0/1\">Vladislav Golyanik</a>",
          "description": "This article introduces a new physics-based method for rigid point set\nalignment called Fast Gravitational Approach (FGA). In FGA, the source and\ntarget point sets are interpreted as rigid particle swarms with masses\ninteracting in a globally multiply-linked manner while moving in a simulated\ngravitational force field. The optimal alignment is obtained by explicit\nmodeling of forces acting on the particles as well as their velocities and\ndisplacements with second-order ordinary differential equations of motion.\nAdditional alignment cues (point-based or geometric features, and other\nboundary conditions) can be integrated into FGA through particle masses. We\npropose a smooth-particle mass function for point mass initialization, which\nimproves robustness to noise and structural discontinuities. To avoid\nprohibitive quadratic complexity of all-to-all point interactions, we adapt a\nBarnes-Hut tree for accelerated force computation and achieve quasilinear\ncomputational complexity. We show that the new method class has characteristics\nnot found in previous alignment methods such as efficient handling of partial\noverlaps, inhomogeneous point sampling densities, and coping with large point\nclouds with reduced runtime compared to the state of the art. Experiments show\nthat our method performs on par with or outperforms all compared competing\nnon-deep-learning-based and general-purpose techniques (which do not assume the\navailability of training data and a scene prior) in resolving transformations\nfor LiDAR data and gains state-of-the-art accuracy and speed when coping with\ndifferent types of data disturbances.",
          "link": "http://arxiv.org/abs/2009.14005",
          "publishedOn": "2021-07-02T01:58:00.478Z",
          "wordCount": 730,
          "title": "Fast Gravitational Approach for Rigid Point Set Registration with Ordinary Differential Equations. (arXiv:2009.14005v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.14100",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">W. Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">H. Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">P. Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_L/0/1/0/all/0/1\">L. Han</a>",
          "description": "The goal of convective storm nowcasting is local prediction of severe and\nimminent convective storms. Here, we consider the convective storm nowcasting\nproblem from the perspective of machine learning. First, we use a pixel-wise\nsampling method to construct spatiotemporal features for nowcasting, and\nflexibly adjust the proportions of positive and negative samples in the\ntraining set to mitigate class-imbalance issues. Second, we employ a concise\ntwo-stream convolutional neural network to extract spatial and temporal cues\nfor nowcasting. This simplifies the network structure, reduces the training\ntime requirement, and improves classification accuracy. The two-stream network\nused both radar and satellite data. In the resulting two-stream, fused\nconvolutional neural network, some of the parameters are entered into a\nsingle-stream convolutional neural network, but it can learn the features of\nmany data. Further, considering the relevance of classification and regression\ntasks, we develop a multi-task learning strategy that predicts the labels used\nin such tasks. We integrate two-stream multi-task learning into a single\nconvolutional neural network. Given the compact architecture, this network is\nmore efficient and easier to optimize than existing recurrent neural networks.",
          "link": "http://arxiv.org/abs/2010.14100",
          "publishedOn": "2021-07-02T01:58:00.470Z",
          "wordCount": 683,
          "title": "A Multi-task Two-stream Spatiotemporal Convolutional Neural Network for Convective Storm Nowcasting. (arXiv:2010.14100v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12387",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Puyol_Anton_E/0/1/0/all/0/1\">Esther Puyol-Anton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruijsink_B/0/1/0/all/0/1\">Bram Ruijsink</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piechnik_S/0/1/0/all/0/1\">Stefan K. Piechnik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubauer_S/0/1/0/all/0/1\">Stefan Neubauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petersen_S/0/1/0/all/0/1\">Steffen E. Petersen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Razavi_R/0/1/0/all/0/1\">Reza Razavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+King_A/0/1/0/all/0/1\">Andrew P. King</a>",
          "description": "The subject of \"fairness\" in artificial intelligence (AI) refers to assessing\nAI algorithms for potential bias based on demographic characteristics such as\nrace and gender, and the development of algorithms to address this bias. Most\napplications to date have been in computer vision, although some work in\nhealthcare has started to emerge. The use of deep learning (DL) in cardiac MR\nsegmentation has led to impressive results in recent years, and such techniques\nare starting to be translated into clinical practice. However, no work has yet\ninvestigated the fairness of such models. In this work, we perform such an\nanalysis for racial/gender groups, focusing on the problem of training data\nimbalance, using a nnU-Net model trained and evaluated on cine short axis\ncardiac MR data from the UK Biobank dataset, consisting of 5,903 subjects from\n6 different racial groups. We find statistically significant differences in\nDice performance between different racial groups. To reduce the racial bias, we\ninvestigated three strategies: (1) stratified batch sampling, in which batch\nsampling is stratified to ensure balance between racial groups; (2) fair\nmeta-learning for segmentation, in which a DL classifier is trained to classify\nrace and jointly optimized with the segmentation model; and (3) protected group\nmodels, in which a different segmentation model is trained for each racial\ngroup. We also compared the results to the scenario where we have a perfectly\nbalanced database. To assess fairness we used the standard deviation (SD) and\nskewed error ratio (SER) of the average Dice values. Our results demonstrate\nthat the racial bias results from the use of imbalanced training data, and that\nall proposed bias mitigation strategies improved fairness, with the best SD and\nSER resulting from the use of protected group models.",
          "link": "http://arxiv.org/abs/2106.12387",
          "publishedOn": "2021-07-02T01:58:00.437Z",
          "wordCount": 779,
          "title": "Fairness in Cardiac MR Image Analysis: An Investigation of Bias Due to Data Imbalance in Deep Learning Based Segmentation. (arXiv:2106.12387v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11516",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_X/0/1/0/all/0/1\">Xin Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xiangrui Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wanlong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_F/0/1/0/all/0/1\">Feng Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongbo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yong Liu</a>",
          "description": "LiDAR-based SLAM system is admittedly more accurate and stable than others,\nwhile its loop closure detection is still an open issue. With the development\nof 3D semantic segmentation for point cloud, semantic information can be\nobtained conveniently and steadily, essential for high-level intelligence and\nconductive to SLAM. In this paper, we present a novel semantic-aided LiDAR SLAM\nwith loop closure based on LOAM, named SA-LOAM, which leverages semantics in\nodometry as well as loop closure detection. Specifically, we propose a\nsemantic-assisted ICP, including semantically matching, downsampling and plane\nconstraint, and integrates a semantic graph-based place recognition method in\nour loop closure detection module. Benefitting from semantics, we can improve\nthe localization accuracy, detect loop closures effectively, and construct a\nglobal consistent semantic map even in large-scale scenes. Extensive\nexperiments on KITTI and Ford Campus dataset show that our system significantly\nimproves baseline performance, has generalization ability to unseen data and\nachieves competitive results compared with state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2106.11516",
          "publishedOn": "2021-07-02T01:58:00.415Z",
          "wordCount": 622,
          "title": "SA-LOAM: Semantic-aided LiDAR SLAM with Loop Closure. (arXiv:2106.11516v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.07085",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pan_H/0/1/0/all/0/1\">Hongyi Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dabawi_D/0/1/0/all/0/1\">Diaa Dabawi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cetin_A/0/1/0/all/0/1\">Ahmet Enis Cetin</a>",
          "description": "In this paper, we propose a novel layer based on fast Walsh-Hadamard\ntransform (WHT) and smooth-thresholding to replace $1\\times 1$ convolution\nlayers in deep neural networks. In the WHT domain, we denoise the transform\ndomain coefficients using the new smooth-thresholding non-linearity, a smoothed\nversion of the well-known soft-thresholding operator. We also introduce a\nfamily of multiplication-free operators from the basic 2$\\times$2 Hadamard\ntransform to implement $3\\times 3$ depthwise separable convolution layers.\nUsing these two types of layers, we replace the bottleneck layers in\nMobileNet-V2 to reduce the network's number of parameters with a slight loss in\naccuracy. For example, by replacing the final third bottleneck layers, we\nreduce the number of parameters from 2.270M to 540K. This reduces the accuracy\nfrom 95.21\\% to 92.98\\% on the CIFAR-10 dataset. Our approach significantly\nimproves the speed of data processing. The fast Walsh-Hadamard transform has a\ncomputational complexity of $O(m\\log_2 m)$. As a result, it is computationally\nmore efficient than the $1\\times1$ convolution layer. The fast Walsh-Hadamard\nlayer processes a tensor in $\\mathbb{R}^{10\\times32\\times32\\times1024}$ about 2\ntimes faster than $1\\times1$ convolution layer on NVIDIA Jetson Nano computer\nboard.",
          "link": "http://arxiv.org/abs/2104.07085",
          "publishedOn": "2021-07-02T01:58:00.397Z",
          "wordCount": 714,
          "title": "Fast Walsh-Hadamard Transform and Smooth-Thresholding Based Binary Layers in Deep Neural Networks. (arXiv:2104.07085v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.01563",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1\">Zhenyue Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_P/0/1/0/all/0/1\">Pan Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Dongwoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McKay_B/0/1/0/all/0/1\">Bob McKay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anwar_S/0/1/0/all/0/1\">Saeed Anwar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gedeon_T/0/1/0/all/0/1\">Tom Gedeon</a>",
          "description": "Skeleton sequences are lightweight and compact, thus are ideal candidates for\naction recognition on edge devices. Recent skeleton-based action recognition\nmethods extract features from 3D joint coordinates as spatial-temporal cues,\nusing these representations in a graph neural network for feature fusion to\nboost recognition performance. The use of first- and second-order features,\n\\ie{} joint and bone representations, has led to high accuracy. Nonetheless,\nmany models are still confused by actions that have similar motion\ntrajectories. To address these issues, we propose fusing third-order features\nin the form of angular encoding into modern architectures to robustly capture\nthe relationships between joints and body parts. This simple fusion with\npopular spatial-temporal graph neural networks achieves new state-of-the-art\naccuracy in two large benchmarks, including NTU60 and NTU120, while employing\nfewer parameters and reduced run time. Our source code is publicly available\nat: https://github.com/ZhenyueQin/Angular-Skeleton-Encoding.",
          "link": "http://arxiv.org/abs/2105.01563",
          "publishedOn": "2021-07-02T01:58:00.389Z",
          "wordCount": 632,
          "title": "Fusing Higher-Order Features in Graph Neural Networks for Skeleton-Based Action Recognition. (arXiv:2105.01563v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12570",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Joy_T/0/1/0/all/0/1\">Tom Joy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yuge Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1\">Philip H.S. Torr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rainforth_T/0/1/0/all/0/1\">Tom Rainforth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmon_S/0/1/0/all/0/1\">Sebastian M. Schmon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siddharth_N/0/1/0/all/0/1\">N. Siddharth</a>",
          "description": "Multimodal VAEs seek to model the joint distribution over heterogeneous data\n(e.g.\\ vision, language), whilst also capturing a shared representation across\nsuch modalities. Prior work has typically combined information from the\nmodalities by reconciling idiosyncratic representations directly in the\nrecognition model through explicit products, mixtures, or other such\nfactorisations. Here we introduce a novel alternative, the MEME, that avoids\nsuch explicit combinations by repurposing semi-supervised VAEs to combine\ninformation between modalities implicitly through mutual supervision. This\nformulation naturally allows learning from partially-observed data where some\nmodalities can be entirely missing -- something that most existing approaches\neither cannot handle, or do so to a limited extent. We demonstrate that MEME\noutperforms baselines on standard metrics across both partial and complete\nobservation schemes on the MNIST-SVHN (image-image) and CUB (image-text)\ndatasets. We also contrast the quality of the representations learnt by mutual\nsupervision against standard approaches and observe interesting trends in its\nability to capture relatedness between data.",
          "link": "http://arxiv.org/abs/2106.12570",
          "publishedOn": "2021-07-02T01:58:00.381Z",
          "wordCount": 613,
          "title": "Learning Multimodal VAEs through Mutual Supervision. (arXiv:2106.12570v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11562",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cha_S/0/1/0/all/0/1\">Sungmin Cha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1\">Beomyoung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoo_Y/0/1/0/all/0/1\">Youngjoon Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moon_T/0/1/0/all/0/1\">Taesup Moon</a>",
          "description": "We consider a class-incremental semantic segmentation (CISS) problem. While\nsome recently proposed algorithms utilized variants of knowledge distillation\n(KD) technique to tackle the problem, they only partially addressed the key\nadditional challenges in CISS that causes the catastrophic forgetting; i.e.,\nthe semantic drift of the background class and multi-label prediction issue. To\nbetter address these challenges, we propose a new method, dubbed as SSUL-M\n(Semantic Segmentation with Unknown Label with Memory), by carefully combining\nseveral techniques tailored for semantic segmentation. More specifically, we\nmake three main contributions; (1) modeling unknown class within the background\nclass to help learning future classes (help plasticity), (2) freezing backbone\nnetwork and past classifiers with binary cross-entropy loss and pseudo-labeling\nto overcome catastrophic forgetting (help stability), and (3) utilizing tiny\nexemplar memory for the first time in CISS to improve both plasticity and\nstability. As a result, we show our method achieves significantly better\nperformance than the recent state-of-the-art baselines on the standard\nbenchmark datasets. Furthermore, we justify our contributions with thorough and\nextensive ablation analyses and discuss different natures of the CISS problem\ncompared to the standard class-incremental learning for classification.",
          "link": "http://arxiv.org/abs/2106.11562",
          "publishedOn": "2021-07-02T01:58:00.361Z",
          "wordCount": 648,
          "title": "SSUL: Semantic Segmentation with Unknown Label for Exemplar-based Class-Incremental Learning. (arXiv:2106.11562v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00644",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hansen_N/0/1/0/all/0/1\">Nicklas Hansen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hao Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaolong Wang</a>",
          "description": "While agents trained by Reinforcement Learning (RL) can solve increasingly\nchallenging tasks directly from visual observations, generalizing learned\nskills to novel environments remains very challenging. Extensive use of data\naugmentation is a promising technique for improving generalization in RL, but\nit is often found to decrease sample efficiency and can even lead to\ndivergence. In this paper, we investigate causes of instability when using data\naugmentation in common off-policy RL algorithms. We identify two problems, both\nrooted in high-variance Q-targets. Based on our findings, we propose a simple\nyet effective technique for stabilizing this class of algorithms under\naugmentation. We perform extensive empirical evaluation of image-based RL using\nboth ConvNets and Vision Transformers (ViT) on a family of benchmarks based on\nDeepMind Control Suite, as well as in robotic manipulation tasks. Our method\ngreatly improves stability and sample efficiency of ConvNets under\naugmentation, and achieves generalization results competitive with\nstate-of-the-art methods for image-based RL. We further show that our method\nscales to RL with ViT-based architectures, and that data augmentation may be\nespecially important in this setting.",
          "link": "http://arxiv.org/abs/2107.00644",
          "publishedOn": "2021-07-02T01:58:00.355Z",
          "wordCount": 630,
          "title": "Stabilizing Deep Q-Learning with ConvNets and Vision Transformers under Data Augmentation. (arXiv:2107.00644v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00651",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Minghao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Houwen Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jianlong Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_H/0/1/0/all/0/1\">Haibin Ling</a>",
          "description": "Recently, pure transformer-based models have shown great potentials for\nvision tasks such as image classification and detection. However, the design of\ntransformer networks is challenging. It has been observed that the depth,\nembedding dimension, and number of heads can largely affect the performance of\nvision transformers. Previous models configure these dimensions based upon\nmanual crafting. In this work, we propose a new one-shot architecture search\nframework, namely AutoFormer, dedicated to vision transformer search.\nAutoFormer entangles the weights of different blocks in the same layers during\nsupernet training. Benefiting from the strategy, the trained supernet allows\nthousands of subnets to be very well-trained. Specifically, the performance of\nthese subnets with weights inherited from the supernet is comparable to those\nretrained from scratch. Besides, the searched models, which we refer to\nAutoFormers, surpass the recent state-of-the-arts such as ViT and DeiT. In\nparticular, AutoFormer-tiny/small/base achieve 74.7%/81.7%/82.4% top-1 accuracy\non ImageNet with 5.7M/22.9M/53.7M parameters, respectively. Lastly, we verify\nthe transferability of AutoFormer by providing the performance on downstream\nbenchmarks and distillation experiments. Code and models are available at\nhttps://github.com/microsoft/AutoML.",
          "link": "http://arxiv.org/abs/2107.00651",
          "publishedOn": "2021-07-02T01:58:00.336Z",
          "wordCount": 615,
          "title": "AutoFormer: Searching Transformers for Visual Recognition. (arXiv:2107.00651v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00650",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Narasimhan_M/0/1/0/all/0/1\">Medhini Narasimhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rohrbach_A/0/1/0/all/0/1\">Anna Rohrbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1\">Trevor Darrell</a>",
          "description": "A generic video summary is an abridged version of a video that conveys the\nwhole story and features the most important scenes. Yet the importance of\nscenes in a video is often subjective, and users should have the option of\ncustomizing the summary by using natural language to specify what is important\nto them. Further, existing models for fully automatic generic summarization\nhave not exploited available language models, which can serve as an effective\nprior for saliency. This work introduces CLIP-It, a single framework for\naddressing both generic and query-focused video summarization, typically\napproached separately in the literature. We propose a language-guided\nmultimodal transformer that learns to score frames in a video based on their\nimportance relative to one another and their correlation with a user-defined\nquery (for query-focused summarization) or an automatically generated dense\nvideo caption (for generic video summarization). Our model can be extended to\nthe unsupervised setting by training without ground-truth supervision. We\noutperform baselines and prior work by a significant margin on both standard\nvideo summarization datasets (TVSum and SumMe) and a query-focused video\nsummarization dataset (QFVS). Particularly, we achieve large improvements in\nthe transfer setting, attesting to our method's strong generalization\ncapabilities.",
          "link": "http://arxiv.org/abs/2107.00650",
          "publishedOn": "2021-07-02T01:58:00.325Z",
          "wordCount": 635,
          "title": "CLIP-It! Language-Guided Video Summarization. (arXiv:2107.00650v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2012.01204",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Castellanos_F/0/1/0/all/0/1\">Francisco J. Castellanos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gallego_A/0/1/0/all/0/1\">Antonio-Javier Gallego</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Calvo_Zaragoza_J/0/1/0/all/0/1\">Jorge Calvo-Zaragoza</a>",
          "description": "Binarization is a well-known image processing task, whose objective is to\nseparate the foreground of an image from the background. One of the many tasks\nfor which it is useful is that of preprocessing document images in order to\nidentify relevant information, such as text or symbols. The wide variety of\ndocument types, alphabets, and formats makes binarization challenging. There\nare multiple proposals with which to solve this problem, from classical\nmanually-adjusted methods, to more recent approaches based on machine learning.\nThe latter techniques require a large amount of training data in order to\nobtain good results; however, labeling a portion of each existing collection of\ndocuments is not feasible in practice. This is a common problem in supervised\nlearning, which can be addressed by using the so-called Domain Adaptation (DA)\ntechniques. These techniques take advantage of the knowledge learned in one\ndomain, for which labeled data are available, to apply it to other domains for\nwhich there are no labeled data. This paper proposes a method that combines\nneural networks and DA in order to carry out unsupervised document\nbinarization. However, when both the source and target domains are very\nsimilar, this adaptation could be detrimental. Our methodology, therefore,\nfirst measures the similarity between domains in an innovative manner in order\nto determine whether or not it is appropriate to apply the adaptation process.\nThe results reported in the experimentation, when evaluating up to 20 possible\ncombinations among five different domains, show that our proposal successfully\ndeals with the binarization of new document domains without the need for\nlabeled data.",
          "link": "http://arxiv.org/abs/2012.01204",
          "publishedOn": "2021-07-02T01:58:00.317Z",
          "wordCount": 734,
          "title": "Unsupervised Neural Domain Adaptation for Document Image Binarization. (arXiv:2012.01204v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00598",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ling_X/0/1/0/all/0/1\">Xiao Ling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xu Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_R/0/1/0/all/0/1\">Rongjun Qin</a>",
          "description": "Bundle adjustment (BA) is a technique for refining sensor orientations of\nsatellite images, while adjustment accuracy is correlated with feature matching\nresults. Feature match-ing often contains high uncertainties in weak/repeat\ntextures, while BA results are helpful in reducing these uncertainties. To\ncompute more accurate orientations, this article incorpo-rates BA and feature\nmatching in a unified framework and formulates the union as the optimization of\na global energy function so that the solutions of the BA and feature matching\nare constrained with each other. To avoid a degeneracy in the optimization, we\npropose a comprised solution by breaking the optimization of the global energy\nfunction into two-step suboptimizations and compute the local minimums of each\nsuboptimization in an incremental manner. Experiments on multi-view\nhigh-resolution satellite images show that our proposed method outperforms\nstate-of-the-art orientation techniques with or without accurate least-squares\nmatching.",
          "link": "http://arxiv.org/abs/2107.00598",
          "publishedOn": "2021-07-02T01:58:00.296Z",
          "wordCount": 583,
          "title": "A Unified Framework of Bundle Adjustment and Feature Matching for High-Resolution Satellite Images. (arXiv:2107.00598v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.07197",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tuli_S/0/1/0/all/0/1\">Shikhar Tuli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dasgupta_I/0/1/0/all/0/1\">Ishita Dasgupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grant_E/0/1/0/all/0/1\">Erin Grant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Griffiths_T/0/1/0/all/0/1\">Thomas L. Griffiths</a>",
          "description": "Modern machine learning models for computer vision exceed humans in accuracy\non specific visual recognition tasks, notably on datasets like ImageNet.\nHowever, high accuracy can be achieved in many ways. The particular decision\nfunction found by a machine learning system is determined not only by the data\nto which the system is exposed, but also the inductive biases of the model,\nwhich are typically harder to characterize. In this work, we follow a recent\ntrend of in-depth behavioral analyses of neural network models that go beyond\naccuracy as an evaluation metric by looking at patterns of errors. Our focus is\non comparing a suite of standard Convolutional Neural Networks (CNNs) and a\nrecently-proposed attention-based network, the Vision Transformer (ViT), which\nrelaxes the translation-invariance constraint of CNNs and therefore represents\na model with a weaker set of inductive biases. Attention-based networks have\npreviously been shown to achieve higher accuracy than CNNs on vision tasks, and\nwe demonstrate, using new metrics for examining error consistency with more\ngranularity, that their errors are also more consistent with those of humans.\nThese results have implications both for building more human-like vision\nmodels, as well as for understanding visual object recognition in humans.",
          "link": "http://arxiv.org/abs/2105.07197",
          "publishedOn": "2021-07-02T01:58:00.289Z",
          "wordCount": 684,
          "title": "Are Convolutional Neural Networks or Transformers more like human vision?. (arXiv:2105.07197v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00627",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Junqing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haihui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xuechao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruzhansky_M/0/1/0/all/0/1\">Michael Ruzhansky</a>",
          "description": "In this paper, we propose an interesting semi-sparsity smoothing algorithm\nbased on a novel sparsity-inducing optimization framework. This method is\nderived from the multiple observations, that is, semi-sparsity prior knowledge\nis more universally applicable, especially in areas where sparsity is not fully\nadmitted, such as polynomial-smoothing surfaces. We illustrate that this\nsemi-sparsity can be identified into a generalized $L_0$-norm minimization in\nhigher-order gradient domains, thereby giving rise to a new ``feature-aware''\nfiltering method with a powerful simultaneous-fitting ability in both sparse\nfeatures (singularities and sharpening edges) and non-sparse regions\n(polynomial-smoothing surfaces). Notice that a direct solver is always\nunavailable due to the non-convexity and combinatorial nature of $L_0$-norm\nminimization. Instead, we solve the model based on an efficient half-quadratic\nsplitting minimization with fast Fourier transforms (FFTs) for acceleration. We\nfinally demonstrate its versatility and many benefits to a series of\nsignal/image processing and computer vision applications.",
          "link": "http://arxiv.org/abs/2107.00627",
          "publishedOn": "2021-07-02T01:58:00.282Z",
          "wordCount": 573,
          "title": "Semi-Sparsity for Smoothing Filters. (arXiv:2107.00627v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14879",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xiang_D/0/1/0/all/0/1\">Donglai Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prada_F/0/1/0/all/0/1\">Fabian Andres Prada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bagautdinov_T/0/1/0/all/0/1\">Timur Bagautdinov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Weipeng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yuan Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_H/0/1/0/all/0/1\">He Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hodgins_J/0/1/0/all/0/1\">Jessica Hodgins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chenglei Wu</a>",
          "description": "Recent work has shown great progress in building photorealistic animatable\nfull-body codec avatars, but these avatars still face difficulties in\ngenerating high-fidelity animation of clothing. To address the difficulties, we\npropose a method to build an animatable clothed body avatar with an explicit\nrepresentation of the clothing on the upper body from multi-view captured\nvideos. We use a two-layer mesh representation to separately register the 3D\nscans with templates. In order to improve the photometric correspondence across\ndifferent frames, texture alignment is then performed through inverse rendering\nof the clothing geometry and texture predicted by a variational autoencoder. We\nthen train a new two-layer codec avatar with separate modeling of the upper\nclothing and the inner body layer. To learn the interaction between the body\ndynamics and clothing states, we use a temporal convolution network to predict\nthe clothing latent code based on a sequence of input skeletal poses. We show\nphotorealistic animation output for three different actors, and demonstrate the\nadvantage of our clothed-body avatars over single-layer avatars in the previous\nwork. We also show the benefit of an explicit clothing model which allows the\nclothing texture to be edited in the animation output.",
          "link": "http://arxiv.org/abs/2106.14879",
          "publishedOn": "2021-07-02T01:58:00.275Z",
          "wordCount": 659,
          "title": "Explicit Clothing Modeling for an Animatable Full-Body Avatar. (arXiv:2106.14879v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00649",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Postels_J/0/1/0/all/0/1\">Janis Postels</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Segu_M/0/1/0/all/0/1\">Mattia Segu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1\">Tao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1\">Luc Van Gool</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1\">Fisher Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tombari_F/0/1/0/all/0/1\">Federico Tombari</a>",
          "description": "A set of novel approaches for estimating epistemic uncertainty in deep neural\nnetworks with a single forward pass has recently emerged as a valid alternative\nto Bayesian Neural Networks. On the premise of informative representations,\nthese deterministic uncertainty methods (DUMs) achieve strong performance on\ndetecting out-of-distribution (OOD) data while adding negligible computational\ncosts at inference time. However, it remains unclear whether DUMs are well\ncalibrated and can seamlessly scale to real-world applications - both\nprerequisites for their practical deployment. To this end, we first provide a\ntaxonomy of DUMs, evaluate their calibration under continuous distributional\nshifts and their performance on OOD detection for image classification tasks.\nThen, we extend the most promising approaches to semantic segmentation. We find\nthat, while DUMs scale to realistic vision tasks and perform well on OOD\ndetection, the practicality of current methods is undermined by poor\ncalibration under realistic distributional shifts.",
          "link": "http://arxiv.org/abs/2107.00649",
          "publishedOn": "2021-07-02T01:58:00.268Z",
          "wordCount": 585,
          "title": "On the Practicality of Deterministic Epistemic Uncertainty. (arXiv:2107.00649v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00641",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jianwei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chunyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pengchuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1\">Xiyang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_B/0/1/0/all/0/1\">Bin Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lu Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>",
          "description": "Recently, Vision Transformer and its variants have shown great promise on\nvarious computer vision tasks. The ability of capturing short- and long-range\nvisual dependencies through self-attention is arguably the main source for the\nsuccess. But it also brings challenges due to quadratic computational overhead,\nespecially for the high-resolution vision tasks (e.g., object detection). In\nthis paper, we present focal self-attention, a new mechanism that incorporates\nboth fine-grained local and coarse-grained global interactions. Using this new\nmechanism, each token attends the closest surrounding tokens at fine\ngranularity but the tokens far away at coarse granularity, and thus can capture\nboth short- and long-range visual dependencies efficiently and effectively.\nWith focal self-attention, we propose a new variant of Vision Transformer\nmodels, called Focal Transformer, which achieves superior performance over the\nstate-of-the-art vision Transformers on a range of public image classification\nand object detection benchmarks. In particular, our Focal Transformer models\nwith a moderate size of 51.1M and a larger size of 89.8M achieve 83.5 and 83.8\nTop-1 accuracy, respectively, on ImageNet classification at 224x224 resolution.\nUsing Focal Transformers as the backbones, we obtain consistent and substantial\nimprovements over the current state-of-the-art Swin Transformers for 6\ndifferent object detection methods trained with standard 1x and 3x schedules.\nOur largest Focal Transformer yields 58.7/58.9 box mAPs and 50.9/51.3 mask mAPs\non COCO mini-val/test-dev, and 55.4 mIoU on ADE20K for semantic segmentation,\ncreating new SoTA on three of the most challenging computer vision tasks.",
          "link": "http://arxiv.org/abs/2107.00641",
          "publishedOn": "2021-07-02T01:58:00.249Z",
          "wordCount": 689,
          "title": "Focal Self-attention for Local-Global Interactions in Vision Transformers. (arXiv:2107.00641v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00067",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mazumder_P/0/1/0/all/0/1\">Pratik Mazumder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_P/0/1/0/all/0/1\">Pravendra Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Namboodiri_V/0/1/0/all/0/1\">Vinay P. Namboodiri</a>",
          "description": "Deep learning models generally learn the biases present in the training data.\nResearchers have proposed several approaches to mitigate such biases and make\nthe model fair. Bias mitigation techniques assume that a sufficiently large\nnumber of training examples are present. However, we observe that if the\ntraining data is limited, then the effectiveness of bias mitigation methods is\nseverely degraded. In this paper, we propose a novel approach to address this\nproblem. Specifically, we adapt self-supervision and self-distillation to\nreduce the impact of biases on the model in this setting. Self-supervision and\nself-distillation are not used for bias mitigation. However, through this work,\nwe demonstrate for the first time that these techniques are very effective in\nbias mitigation. We empirically show that our approach can significantly reduce\nthe biases learned by the model. Further, we experimentally demonstrate that\nour approach is complementary to other bias mitigation strategies. Our approach\nsignificantly improves their performance and further reduces the model biases\nin the limited data regime. Specifically, on the L-CIFAR-10S skewed dataset,\nour approach significantly reduces the bias score of the baseline model by\n78.22% and outperforms it in terms of accuracy by a significant absolute margin\nof 8.89%. It also significantly reduces the bias score for the state-of-the-art\ndomain independent bias mitigation method by 59.26% and improves its\nperformance by a significant absolute margin of 7.08%.",
          "link": "http://arxiv.org/abs/2107.00067",
          "publishedOn": "2021-07-02T01:58:00.241Z",
          "wordCount": 669,
          "title": "Fair Visual Recognition in Limited Data Regime using Self-Supervision and Self-Distillation. (arXiv:2107.00067v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00645",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rao_Y/0/1/0/all/0/1\">Yongming Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wenliang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zheng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiwen Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>",
          "description": "Recent advances in self-attention and pure multi-layer perceptrons (MLP)\nmodels for vision have shown great potential in achieving promising performance\nwith fewer inductive biases. These models are generally based on learning\ninteraction among spatial locations from raw data. The complexity of\nself-attention and MLP grows quadratically as the image size increases, which\nmakes these models hard to scale up when high-resolution features are required.\nIn this paper, we present the Global Filter Network (GFNet), a conceptually\nsimple yet computationally efficient architecture, that learns long-term\nspatial dependencies in the frequency domain with log-linear complexity. Our\narchitecture replaces the self-attention layer in vision transformers with\nthree key operations: a 2D discrete Fourier transform, an element-wise\nmultiplication between frequency-domain features and learnable global filters,\nand a 2D inverse Fourier transform. We exhibit favorable accuracy/complexity\ntrade-offs of our models on both ImageNet and downstream tasks. Our results\ndemonstrate that GFNet can be a very competitive alternative to\ntransformer-style models and CNNs in efficiency, generalization ability and\nrobustness. Code is available at https://github.com/raoyongming/GFNet",
          "link": "http://arxiv.org/abs/2107.00645",
          "publishedOn": "2021-07-02T01:58:00.234Z",
          "wordCount": 616,
          "title": "Global Filter Networks for Image Classification. (arXiv:2107.00645v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00418",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Suh_S/0/1/0/all/0/1\">Sungho Suh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cheon_S/0/1/0/all/0/1\">Sojeong Cheon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Choi_W/0/1/0/all/0/1\">Wonseo Choi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chung_Y/0/1/0/all/0/1\">Yeon Woong Chung</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cho_W/0/1/0/all/0/1\">Won-Kyung Cho</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Paik_J/0/1/0/all/0/1\">Ji-Sun Paik</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_S/0/1/0/all/0/1\">Sung Eun Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chang_D/0/1/0/all/0/1\">Dong-Jin Chang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_Y/0/1/0/all/0/1\">Yong Oh Lee</a>",
          "description": "Deep neural networks (DNNs) have been widely used for medical image analysis.\nHowever, the lack of access a to large-scale annotated dataset poses a great\nchallenge, especially in the case of rare diseases, or new domains for the\nresearch society. Transfer of pre-trained features, from the relatively large\ndataset is a considerable solution. In this paper, we have explored supervised\nsegmentation using domain adaptation for optic nerve and orbital tumor, when\nonly small sampled CT images are given. Even the lung image database consortium\nimage collection (LIDC-IDRI) is a cross-domain to orbital CT, but the proposed\ndomain adaptation method improved the performance of attention U-Net for the\nsegmentation in public optic nerve dataset and our clinical orbital tumor\ndataset. The code and dataset are available at https://github.com/cmcbigdata.",
          "link": "http://arxiv.org/abs/2107.00418",
          "publishedOn": "2021-07-02T01:58:00.226Z",
          "wordCount": 595,
          "title": "Supervised Segmentation with Domain Adaptation for Small Sampled Orbital CT Images. (arXiv:2107.00418v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00590",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Albanwan_H/0/1/0/all/0/1\">Hessah Albanwan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_R/0/1/0/all/0/1\">Rongjun Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xiaohu Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Desheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guldmann_J/0/1/0/all/0/1\">Jean-Michel Guldmann</a>",
          "description": "The current practice in land cover/land use change analysis relies heavily on\nthe individually classified maps of the multitemporal data set. Due to varying\nacquisition conditions (e.g., illumination, sensors, seasonal differences), the\nclassification maps yielded are often inconsistent through time for robust\nstatistical analysis. 3D geometric features have been shown to be stable for\nassessing differences across the temporal data set. Therefore, in this article\nwe investigate he use of a multitemporal orthophoto and digital surface model\nderived from satellite data for spatiotemporal classification. Our approach\nconsists of two major steps: generating per-class probability distribution maps\nusing the random-forest classifier with limited training samples, and making\nspatiotemporal inferences using an iterative 3D spatiotemporal filter operating\non per-class probability maps. Our experimental results demonstrate that the\nproposed methods can consistently improve the individual classification results\nby 2%-6% and thus can be an important postclassification refinement approach.",
          "link": "http://arxiv.org/abs/2107.00590",
          "publishedOn": "2021-07-02T01:58:00.218Z",
          "wordCount": 590,
          "title": "3D Iterative Spatiotemporal Filtering for Classification of Multitemporal Satellite Data Sets. (arXiv:2107.00590v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00239",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hong_D/0/1/0/all/0/1\">Dexiang Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Congcong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_L/0/1/0/all/0/1\">Longyin Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinyao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Libo Zhang</a>",
          "description": "This report presents the approach used in the submission of Generic Event\nBoundary Detection (GEBD) Challenge at CVPR21. In this work, we design a\nCascaded Temporal Attention Network (CASTANET) for GEBD, which is formed by\nthree parts, the backbone network, the temporal attention module, and the\nclassification module. Specifically, the Channel-Separated Convolutional\nNetwork (CSN) is used as the backbone network to extract features, and the\ntemporal attention module is designed to enforce the network to focus on the\ndiscriminative features. After that, the cascaded architecture is used in the\nclassification module to generate more accurate boundaries. In addition, the\nensemble strategy is used to further improve the performance of the proposed\nmethod. The proposed method achieves 83.30% F1 score on Kinetics-GEBD test set,\nwhich improves 20.5% F1 score compared to the baseline method. Code is\navailable at https://github.com/DexiangHong/Cascade-PC.",
          "link": "http://arxiv.org/abs/2107.00239",
          "publishedOn": "2021-07-02T01:58:00.193Z",
          "wordCount": 591,
          "title": "Generic Event Boundary Detection Challenge at CVPR 2021 Technical Report: Cascaded Temporal Attention Network (CASTANET). (arXiv:2107.00239v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00559",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kerkouri_M/0/1/0/all/0/1\">Mohamed Amine Kerkouri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tliba_M/0/1/0/all/0/1\">Marouane Tliba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chetouani_A/0/1/0/all/0/1\">Aladine Chetouani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harba_R/0/1/0/all/0/1\">Rachid Harba</a>",
          "description": "Human vision is naturally more attracted by some regions within their field\nof view than others. This intrinsic selectivity mechanism, so-called visual\nattention, is influenced by both high- and low-level factors; such as the\nglobal environment (illumination, background texture, etc.), stimulus\ncharacteristics (color, intensity, orientation, etc.), and some prior visual\ninformation. Visual attention is useful for many computer vision applications\nsuch as image compression, recognition, and captioning. In this paper, we\npropose an end-to-end deep-based method, so-called SALYPATH (SALiencY and\nscanPATH), that efficiently predicts the scanpath of an image through features\nof a saliency model. The idea is predict the scanpath by exploiting the\ncapacity of a deep-based model to predict the saliency. The proposed method was\nevaluated through 2 well-known datasets. The results obtained showed the\nrelevance of the proposed framework comparing to state-of-the-art models.",
          "link": "http://arxiv.org/abs/2107.00559",
          "publishedOn": "2021-07-02T01:58:00.186Z",
          "wordCount": 584,
          "title": "SALYPATH: A Deep-Based Architecture for visual attention prediction. (arXiv:2107.00559v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00070",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1\">Shihao Ji</a>",
          "description": "Training deep neural networks with an $L_0$ regularization is one of the\nprominent approaches for network pruning or sparsification. The method prunes\nthe network during training by encouraging weights to become exactly zero.\nHowever, recent work of Gale et al. reveals that although this method yields\nhigh compression rates on smaller datasets, it performs inconsistently on\nlarge-scale learning tasks, such as ResNet50 on ImageNet. We analyze this\nphenomenon through the lens of variational inference and find that it is likely\ndue to the independent modeling of binary gates, the mean-field approximation,\nwhich is known in Bayesian statistics for its poor performance due to the crude\napproximation. To mitigate this deficiency, we propose a dependency modeling of\nbinary gates, which can be modeled effectively as a multi-layer perceptron\n(MLP). We term our algorithm Dep-$L_0$ as it prunes networks via a\ndependency-enabled $L_0$ regularization. Extensive experiments on CIFAR10,\nCIFAR100 and ImageNet with VGG16, ResNet50, ResNet56 show that our Dep-$L_0$\noutperforms the original $L_0$-HC algorithm of Louizos et al. by a significant\nmargin, especially on ImageNet. Compared with the state-of-the-arts network\nsparsification algorithms, our dependency modeling makes the $L_0$-based\nsparsification once again very competitive on large-scale learning tasks. Our\nsource code is available at https://github.com/leo-yangli/dep-l0.",
          "link": "http://arxiv.org/abs/2107.00070",
          "publishedOn": "2021-07-02T01:58:00.180Z",
          "wordCount": 646,
          "title": "Dep-$L_0$: Improving $L_0$-based Network Sparsification via Dependency Modeling. (arXiv:2107.00070v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00228",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fischer_M/0/1/0/all/0/1\">Marc Fischer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baader_M/0/1/0/all/0/1\">Maximilian Baader</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vechev_M/0/1/0/all/0/1\">Martin Vechev</a>",
          "description": "We present a new certification method for image and point cloud segmentation\nbased on randomized smoothing. The method leverages a novel scalable algorithm\nfor prediction and certification that correctly accounts for multiple testing,\nnecessary for ensuring statistical guarantees. The key to our approach is\nreliance on established multiple-testing correction mechanisms as well as the\nability to abstain from classifying single pixels or points while still\nrobustly segmenting the overall input. Our experimental evaluation on synthetic\ndata and challenging datasets, such as Pascal Context, Cityscapes, and\nShapeNet, shows that our algorithm can achieve, for the first time, competitive\naccuracy and certification guarantees on real-world segmentation tasks. We\nprovide an implementation at https://github.com/eth-sri/segmentation-smoothing.",
          "link": "http://arxiv.org/abs/2107.00228",
          "publishedOn": "2021-07-02T01:58:00.172Z",
          "wordCount": 545,
          "title": "Scalable Certified Segmentation via Randomized Smoothing. (arXiv:2107.00228v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00400",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Nguyen_D/0/1/0/all/0/1\">Dat Thanh Nguyen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Quach_M/0/1/0/all/0/1\">Maurice Quach</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Valenzise_G/0/1/0/all/0/1\">Giuseppe Valenzise</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Duhamel_P/0/1/0/all/0/1\">Pierre Duhamel</a>",
          "description": "This paper proposes a lossless point cloud (PC) geometry compression method\nthat uses neural networks to estimate the probability distribution of voxel\noccupancy. First, to take into account the PC sparsity, our method adaptively\npartitions a point cloud into multiple voxel block sizes. This partitioning is\nsignalled via an octree. Second, we employ a deep auto-regressive generative\nmodel to estimate the occupancy probability of each voxel given the previously\nencoded ones. We then employ the estimated probabilities to code efficiently a\nblock using a context-based arithmetic coder. Our context has variable size and\ncan expand beyond the current block to learn more accurate probabilities. We\nalso consider using data augmentation techniques to increase the generalization\ncapability of the learned probability models, in particular in the presence of\nnoise and lower-density point clouds. Experimental evaluation, performed on a\nvariety of point clouds from four different datasets and with diverse\ncharacteristics, demonstrates that our method reduces significantly (by up to\n30%) the rate for lossless coding compared to the state-of-the-art MPEG codec.",
          "link": "http://arxiv.org/abs/2107.00400",
          "publishedOn": "2021-07-02T01:58:00.163Z",
          "wordCount": 652,
          "title": "Lossless Coding of Point Cloud Geometry using a Deep Generative Model. (arXiv:2107.00400v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00436",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Larsen_E/0/1/0/all/0/1\">Erik Larsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noever_D/0/1/0/all/0/1\">David Noever</a>, <a href=\"http://arxiv.org/find/cs/1/au:+MacVittie_K/0/1/0/all/0/1\">Korey MacVittie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lilly_J/0/1/0/all/0/1\">John Lilly</a>",
          "description": "Twenty-three machine learning algorithms were trained then scored to\nestablish baseline comparison metrics and to select an image classification\nalgorithm worthy of embedding into mission-critical satellite imaging systems.\nThe Overhead-MNIST dataset is a collection of satellite images similar in style\nto the ubiquitous MNIST hand-written digits found in the machine learning\nliterature. The CatBoost classifier, Light Gradient Boosting Machine, and\nExtreme Gradient Boosting models produced the highest accuracies, Areas Under\nthe Curve (AUC), and F1 scores in a PyCaret general comparison. Separate\nevaluations showed that a deep convolutional architecture was the most\npromising. We present results for the overall best performing algorithm as a\nbaseline for edge deployability and future performance improvement: a\nconvolutional neural network (CNN) scoring 0.965 categorical accuracy on unseen\ntest data.",
          "link": "http://arxiv.org/abs/2107.00436",
          "publishedOn": "2021-07-02T01:58:00.143Z",
          "wordCount": 570,
          "title": "Overhead-MNIST: Machine Learning Baselines for Image Classification. (arXiv:2107.00436v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00283",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Thambawita_V/0/1/0/all/0/1\">Vajira Thambawita</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hicks_S/0/1/0/all/0/1\">Steven A. Hicks</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Halvorsen_P/0/1/0/all/0/1\">P&#xe5;l Halvorsen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Riegler_M/0/1/0/all/0/1\">Michael A. Riegler</a>",
          "description": "Detection of colon polyps has become a trending topic in the intersecting\nfields of machine learning and gastrointestinal endoscopy. The focus has mainly\nbeen on per-frame classification. More recently, polyp segmentation has gained\nattention in the medical community. Segmentation has the advantage of being\nmore accurate than per-frame classification or object detection as it can show\nthe affected area in greater detail. For our contribution to the EndoCV 2021\nsegmentation challenge, we propose two separate approaches. First, a\nsegmentation model named TriUNet composed of three separate UNet models.\nSecond, we combine TriUNet with an ensemble of well-known segmentation models,\nnamely UNet++, FPN, DeepLabv3, and DeepLabv3+, into a model called\nDivergentNets to produce more generalizable medical image segmentation masks.\nIn addition, we propose a modified Dice loss that calculates loss only for a\nsingle class when performing multiclass segmentation, forcing the model to\nfocus on what is most important. Overall, the proposed methods achieved the\nbest average scores for each respective round in the challenge, with TriUNet\nbeing the winning model in Round I and DivergentNets being the winning model in\nRound II of the segmentation generalization challenge at EndoCV 2021. The\nimplementation of our approach is made publicly available on GitHub.",
          "link": "http://arxiv.org/abs/2107.00283",
          "publishedOn": "2021-07-02T01:58:00.136Z",
          "wordCount": 691,
          "title": "DivergentNets: Medical Image Segmentation by Network Ensemble. (arXiv:2107.00283v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00500",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xufeng Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chang-Tsun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanchez_V/0/1/0/all/0/1\">Victor Sanchez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maple_C/0/1/0/all/0/1\">Carsten Maple</a>",
          "description": "Driven by recent advances in object detection with deep neural networks, the\ntracking-by-detection paradigm has gained increasing prevalence in the research\ncommunity of multi-object tracking (MOT). It has long been known that\nappearance information plays an essential role in the detection-to-track\nassociation, which lies at the core of the tracking-by-detection paradigm.\nWhile most existing works consider the appearance distances between the\ndetections and the tracks, they ignore the statistical information implied by\nthe historical appearance distance records in the tracks, which can be\nparticularly useful when a detection has similar distances with two or more\ntracks. In this work, we propose a hybrid track association (HTA) algorithm\nthat models the historical appearance distances of a track with an incremental\nGaussian mixture model (IGMM) and incorporates the derived statistical\ninformation into the calculation of the detection-to-track association cost.\nExperimental results on three MOT benchmarks confirm that HTA effectively\nimproves the target identification performance with a small compromise to the\ntracking speed. Additionally, compared to many state-of-the-art trackers, the\nDeepSORT tracker equipped with HTA achieves better or comparable performance in\nterms of the balance of tracking quality and speed.",
          "link": "http://arxiv.org/abs/2107.00500",
          "publishedOn": "2021-07-02T01:58:00.128Z",
          "wordCount": 632,
          "title": "On the detection-to-track association for online multi-object tracking. (arXiv:2107.00500v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00360",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schaaf_N/0/1/0/all/0/1\">Nina Schaaf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitri_O/0/1/0/all/0/1\">Omar de Mitri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hang Beom Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Windberger_A/0/1/0/all/0/1\">Alexander Windberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huber_M/0/1/0/all/0/1\">Marco F. Huber</a>",
          "description": "Convolutional Neural Networks (CNN) have become de fact state-of-the-art for\nthe main computer vision tasks. However, due to the complex underlying\nstructure their decisions are hard to understand which limits their use in some\ncontext of the industrial world. A common and hard to detect challenge in\nmachine learning (ML) tasks is data bias. In this work, we present a systematic\napproach to uncover data bias by means of attribution maps. For this purpose,\nfirst an artificial dataset with a known bias is created and used to train\nintentionally biased CNNs. The networks' decisions are then inspected using\nattribution maps. Finally, meaningful metrics are used to measure the\nattribution maps' representativeness with respect to the known bias. The\nproposed study shows that some attribution map techniques highlight the\npresence of bias in the data better than others and metrics can support the\nidentification of bias.",
          "link": "http://arxiv.org/abs/2107.00360",
          "publishedOn": "2021-07-02T01:58:00.121Z",
          "wordCount": 606,
          "title": "Towards Measuring Bias in Image Classification. (arXiv:2107.00360v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00395",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yunxin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_B/0/1/0/all/0/1\">Baotian Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qingcai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_Y/0/1/0/all/0/1\">Yang Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaolong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yuxin Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lin Ma</a>",
          "description": "Previous works indicate that the glyph of Chinese characters contains rich\nsemantic information and has the potential to enhance the representation of\nChinese characters. The typical method to utilize the glyph features is by\nincorporating them into the character embedding space. Inspired by previous\nmethods, we innovatively propose a Chinese pre-trained representation model\nnamed as GlyphCRM, which abandons the ID-based character embedding method yet\nsolely based on sequential character images. We render each character into a\nbinary grayscale image and design two-channel position feature maps for it.\nFormally, we first design a two-layer residual convolutional neural network,\nnamely HanGlyph to generate the initial glyph representation of Chinese\ncharacters, and subsequently adopt multiple bidirectional encoder Transformer\nblocks as the superstructure to capture the context-sensitive information.\nMeanwhile, we feed the glyph features extracted from each layer of the HanGlyph\nmodule into the underlying Transformer blocks by skip-connection method to\nfully exploit the glyph features of Chinese characters. As the HanGlyph module\ncan obtain a sufficient glyph representation of any Chinese character, the\nlong-standing out-of-vocabulary problem could be effectively solved. Extensive\nexperimental results indicate that GlyphCRM substantially outperforms the\nprevious BERT-based state-of-the-art model on 9 fine-tuning tasks, and it has\nstrong transferability and generalization on specialized fields and\nlow-resource tasks. We hope this work could spark further research beyond the\nrealms of well-established representation of Chinese texts.",
          "link": "http://arxiv.org/abs/2107.00395",
          "publishedOn": "2021-07-02T01:58:00.109Z",
          "wordCount": 683,
          "title": "GlyphCRM: Bidirectional Encoder Representation for Chinese Character with its Glyph. (arXiv:2107.00395v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00456",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xiaotian Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tolmachev_A/0/1/0/all/0/1\">Arseny Tolmachev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamamoto_T/0/1/0/all/0/1\">Tatsuya Yamamoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takeuchi_K/0/1/0/all/0/1\">Koh Takeuchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Okajima_S/0/1/0/all/0/1\">Seiji Okajima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takebayashi_T/0/1/0/all/0/1\">Tomoyoshi Takebayashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maruhashi_K/0/1/0/all/0/1\">Koji Maruhashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kashima_H/0/1/0/all/0/1\">Hisashi Kashima</a>",
          "description": "Understanding the reasons behind the predictions made by deep neural networks\nis critical for gaining human trust in many important applications, which is\nreflected in the increasing demand for explainability in AI (XAI) in recent\nyears. Saliency-based feature attribution methods, which highlight important\nparts of images that contribute to decisions by classifiers, are often used as\nXAI methods, especially in the field of computer vision. In order to compare\nvarious saliency-based XAI methods quantitatively, several approaches for\nautomated evaluation schemes have been proposed; however, there is no guarantee\nthat such automated evaluation metrics correctly evaluate explainability, and a\nhigh rating by an automated evaluation scheme does not necessarily mean a high\nexplainability for humans. In this study, instead of the automated evaluation,\nwe propose a new human-based evaluation scheme using crowdsourcing to evaluate\nXAI methods. Our method is inspired by a human computation game, \"Peek-a-boom\",\nand can efficiently compare different XAI methods by exploiting the power of\ncrowds. We evaluate the saliency maps of various XAI methods on two datasets\nwith automated and crowd-based evaluation schemes. Our experiments show that\nthe result of our crowd-based evaluation scheme is different from those of\nautomated evaluation schemes. In addition, we regard the crowd-based evaluation\nresults as ground truths and provide a quantitative performance measure to\ncompare different automated evaluation schemes. We also discuss the impact of\ncrowd workers on the results and show that the varying ability of crowd workers\ndoes not significantly impact the results.",
          "link": "http://arxiv.org/abs/2107.00456",
          "publishedOn": "2021-07-02T01:58:00.009Z",
          "wordCount": 705,
          "title": "Crowdsourcing Evaluation of Saliency-based XAI Methods. (arXiv:2107.00456v1 [cs.HC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00434",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1\">Zicong Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spurr_A/0/1/0/all/0/1\">Adrian Spurr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kocabas_M/0/1/0/all/0/1\">Muhammed Kocabas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1\">Siyu Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Black_M/0/1/0/all/0/1\">Michael J. Black</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hilliges_O/0/1/0/all/0/1\">Otmar Hilliges</a>",
          "description": "In natural conversation and interaction, our hands often overlap or are in\ncontact with each other. Due to the homogeneous appearance of hands, this makes\nestimating the 3D pose of interacting hands from images difficult. In this\npaper we demonstrate that self-similarity, and the resulting ambiguities in\nassigning pixel observations to the respective hands and their parts, is a\nmajor cause of the final 3D pose error. Motivated by this insight, we propose\nDIGIT, a novel method for estimating the 3D poses of two interacting hands from\na single monocular image. The method consists of two interwoven branches that\nprocess the input imagery into a per-pixel semantic part segmentation mask and\na visual feature volume. In contrast to prior work, we do not decouple the\nsegmentation from the pose estimation stage, but rather leverage the per-pixel\nprobabilities directly in the downstream pose estimation task. To do so, the\npart probabilities are merged with the visual features and processed via\nfully-convolutional layers. We experimentally show that the proposed approach\nachieves new state-of-the-art performance on the InterHand2.6M dataset for both\nsingle and interacting hands across all metrics. We provide detailed ablation\nstudies to demonstrate the efficacy of our method and to provide insights into\nhow the modelling of pixel ownership affects single and interacting hand pose\nestimation. Our code will be released for research purposes.",
          "link": "http://arxiv.org/abs/2107.00434",
          "publishedOn": "2021-07-02T01:57:59.988Z",
          "wordCount": 669,
          "title": "Learning to Disambiguate Strongly Interacting Hands via Probabilistic Per-pixel Part Segmentation. (arXiv:2107.00434v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00249",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xinxin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_L/0/1/0/all/0/1\">Longteng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zijia Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Mingzhen Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weining Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jinqiao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Hanqing Lu</a>",
          "description": "In this paper, we propose an Omni-perception Pre-Trainer (OPT) for\ncross-modal understanding and generation, by jointly modeling visual, text and\naudio resources. OPT is constructed in an encoder-decoder framework, including\nthree single-modal encoders to generate token-based embeddings for each\nmodality, a cross-modal encoder to encode the correlations among the three\nmodalities, and two cross-modal decoders to generate text and image\nrespectively. For the OPT's pre-training, we design a multi-task pretext\nlearning scheme to model multi-modal resources from three different data\ngranularities, \\ie, token-, modality-, and sample-level modeling, through which\nOPT learns to align and translate among different modalities. The pre-training\ntask is carried out on a large amount of image-text-audio triplets from Open\nImages. Experimental results show that OPT can learn strong image-text-audio\nmulti-modal representations and achieve promising results on a variety of\ncross-modal understanding and generation tasks.",
          "link": "http://arxiv.org/abs/2107.00249",
          "publishedOn": "2021-07-02T01:57:59.951Z",
          "wordCount": 583,
          "title": "OPT: Omni-Perception Pre-Trainer for Cross-Modal Understanding and Generation. (arXiv:2107.00249v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00544",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yasar_M/0/1/0/all/0/1\">Mohammad Samin Yasar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iqbal_T/0/1/0/all/0/1\">Tariq Iqbal</a>",
          "description": "Human motion prediction is an essential component for enabling closer\nhuman-robot collaboration. The task of accurately predicting human motion is\nnon-trivial. It is compounded by the variability of human motion, both at a\nskeletal level due to the varying size of humans and at a motion level due to\nindividual movement's idiosyncrasies. These variables make it challenging for\nlearning algorithms to obtain a general representation that is robust to the\ndiverse spatio-temporal patterns of human motion. In this work, we propose a\nmodular sequence learning approach that allows end-to-end training while also\nhaving the flexibility of being fine-tuned. Our approach relies on the\ndiversity of training samples to first learn a robust representation, which can\nthen be fine-tuned in a continual learning setup to predict the motion of new\nsubjects. We evaluated the proposed approach by comparing its performance\nagainst state-of-the-art baselines. The results suggest that our approach\noutperforms other methods over all the evaluated temporal horizons, using a\nsmall amount of data for fine-tuning. The improved performance of our approach\nopens up the possibility of using continual learning for personalized and\nreliable motion prediction.",
          "link": "http://arxiv.org/abs/2107.00544",
          "publishedOn": "2021-07-02T01:57:59.944Z",
          "wordCount": 615,
          "title": "Improving Human Motion Prediction Through Continual Learning. (arXiv:2107.00544v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00583",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dorent_R/0/1/0/all/0/1\">Reuben Dorent</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joutard_S/0/1/0/all/0/1\">Samuel Joutard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shapey_J/0/1/0/all/0/1\">Jonathan Shapey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kujawa_A/0/1/0/all/0/1\">Aaron Kujawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Modat_M/0/1/0/all/0/1\">Marc Modat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ourselin_S/0/1/0/all/0/1\">Sebastien Ourselin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vercauteren_T/0/1/0/all/0/1\">Tom Vercauteren</a>",
          "description": "We introduce $\\textit{InExtremIS}$, a weakly supervised 3D approach to train\na deep image segmentation network using particularly weak train-time\nannotations: only 6 extreme clicks at the boundary of the objects of interest.\nOur fully-automatic method is trained end-to-end and does not require any\ntest-time annotations. From the extreme points, 3D bounding boxes are extracted\naround objects of interest. Then, deep geodesics connecting extreme points are\ngenerated to increase the amount of \"annotated\" voxels within the bounding\nboxes. Finally, a weakly supervised regularised loss derived from a Conditional\nRandom Field formulation is used to encourage prediction consistency over\nhomogeneous regions. Extensive experiments are performed on a large open\ndataset for Vestibular Schwannoma segmentation. $\\textit{InExtremIS}$ obtained\ncompetitive performance, approaching full supervision and outperforming\nsignificantly other weakly supervised techniques based on bounding boxes.\nMoreover, given a fixed annotation time budget, $\\textit{InExtremIS}$\noutperforms full supervision. Our code and data are available online.",
          "link": "http://arxiv.org/abs/2107.00583",
          "publishedOn": "2021-07-02T01:57:59.914Z",
          "wordCount": 603,
          "title": "Inter Extreme Points Geodesics for Weakly Supervised Segmentation. (arXiv:2107.00583v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00233",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yoon_T/0/1/0/all/0/1\">Tehrim Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_S/0/1/0/all/0/1\">Sumin Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1\">Sung Ju Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_E/0/1/0/all/0/1\">Eunho Yang</a>",
          "description": "Federated learning (FL) allows edge devices to collectively learn a model\nwithout directly sharing data within each device, thus preserving privacy and\neliminating the need to store data globally. While there are promising results\nunder the assumption of independent and identically distributed (iid) local\ndata, current state-of-the-art algorithms suffer from performance degradation\nas the heterogeneity of local data across clients increases. To resolve this\nissue, we propose a simple framework, Mean Augmented Federated Learning (MAFL),\nwhere clients send and receive averaged local data, subject to the privacy\nrequirements of target applications. Under our framework, we propose a new\naugmentation algorithm, named FedMix, which is inspired by a phenomenal yet\nsimple data augmentation method, Mixup, but does not require local raw data to\nbe directly shared among devices. Our method shows greatly improved performance\nin the standard benchmark datasets of FL, under highly non-iid federated\nsettings, compared to conventional algorithms.",
          "link": "http://arxiv.org/abs/2107.00233",
          "publishedOn": "2021-07-02T01:57:59.904Z",
          "wordCount": 604,
          "title": "FedMix: Approximation of Mixup under Mean Augmented Federated Learning. (arXiv:2107.00233v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00328",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shurun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shiqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1\">Yan Ye</a>",
          "description": "The research of visual signal compression has a long history. Fueled by deep\nlearning, exciting progress has been made recently. Despite achieving better\ncompression performance, existing end-to-end compression algorithms are still\ndesigned towards better signal quality in terms of rate-distortion\noptimization. In this paper, we show that the design and optimization of\nnetwork architecture could be further improved for compression towards machine\nvision. We propose an inverted bottleneck structure for end-to-end compression\ntowards machine vision, which specifically accounts for efficient\nrepresentation of the semantic information. Moreover, we quest the capability\nof optimization by incorporating the analytics accuracy into the optimization\nprocess, and the optimality is further explored with generalized rate-accuracy\noptimization in an iterative manner. We use object detection as a showcase for\nend-to-end compression towards machine vision, and extensive experiments show\nthat the proposed scheme achieves significant BD-rate savings in terms of\nanalysis performance. Moreover, the promise of the scheme is also demonstrated\nwith strong generalization capability towards other machine vision tasks, due\nto the enabling of signal-level reconstruction.",
          "link": "http://arxiv.org/abs/2107.00328",
          "publishedOn": "2021-07-02T01:57:59.897Z",
          "wordCount": 619,
          "title": "End-to-end Compression Towards Machine Vision: Network Architecture Design and Optimization. (arXiv:2107.00328v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00296",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Niu_Y/0/1/0/all/0/1\">Yuhao Niu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gu_L/0/1/0/all/0/1\">Lin Gu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_Y/0/1/0/all/0/1\">Yitian Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lu_F/0/1/0/all/0/1\">Feng Lu</a>",
          "description": "Though deep learning has shown successful performance in classifying the\nlabel and severity stage of certain diseases, most of them give few\nexplanations on how to make predictions. Inspired by Koch's Postulates, the\nfoundation in evidence-based medicine (EBM) to identify the pathogen, we\npropose to exploit the interpretability of deep learning application in medical\ndiagnosis. By determining and isolating the neuron activation patterns on which\ndiabetic retinopathy (DR) detector relies to make decisions, we demonstrate the\ndirect relation between the isolated neuron activation and lesions for a\npathological explanation. To be specific, we first define novel pathological\ndescriptors using activated neurons of the DR detector to encode both spatial\nand appearance information of lesions. Then, to visualize the symptom encoded\nin the descriptor, we propose Patho-GAN, a new network to synthesize medically\nplausible retinal images. By manipulating these descriptors, we could even\narbitrarily control the position, quantity, and categories of generated\nlesions. We also show that our synthesized images carry the symptoms directly\nrelated to diabetic retinopathy diagnosis. Our generated images are both\nqualitatively and quantitatively superior to the ones by previous methods.\nBesides, compared to existing methods that take hours to generate an image, our\nsecond level speed endows the potential to be an effective solution for data\naugmentation.",
          "link": "http://arxiv.org/abs/2107.00296",
          "publishedOn": "2021-07-02T01:57:59.888Z",
          "wordCount": 667,
          "title": "Explainable Diabetic Retinopathy Detection and Retinal Image Generation. (arXiv:2107.00296v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00451",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Koot_R/0/1/0/all/0/1\">Raivo Koot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Haiping Lu</a>",
          "description": "Efficient video action recognition remains a challenging problem. One large\nmodel after another takes the place of the state-of-the-art on the Kinetics\ndataset, but real-world efficiency evaluations are often lacking. In this work,\nwe fill this gap and investigate the use of transformers for efficient action\nrecognition. We propose a novel, lightweight action recognition architecture,\nVideoLightFormer. In a factorized fashion, we carefully extend the 2D\nconvolutional Temporal Segment Network with transformers, while maintaining\nspatial and temporal video structure throughout the entire model. Existing\nmethods often resort to one of the two extremes, where they either apply huge\ntransformers to video features, or minimal transformers on highly pooled video\nfeatures. Our method differs from them by keeping the transformer models small,\nbut leveraging full spatiotemporal feature structure. We evaluate\nVideoLightFormer in a high-efficiency setting on the temporally-demanding\nEPIC-KITCHENS-100 and Something-Something-V2 (SSV2) datasets and find that it\nachieves a better mix of efficiency and accuracy than existing state-of-the-art\nmodels, apart from the Temporal Shift Module on SSV2.",
          "link": "http://arxiv.org/abs/2107.00451",
          "publishedOn": "2021-07-02T01:57:59.881Z",
          "wordCount": 597,
          "title": "VideoLightFormer: Lightweight Action Recognition using Transformers. (arXiv:2107.00451v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00382",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_X/0/1/0/all/0/1\">Xin Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xiangrui Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Tianxin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yong Liu</a>",
          "description": "Place recognition gives a SLAM system the ability to correct cumulative\nerrors. Unlike images that contain rich texture features, point clouds are\nalmost pure geometric information which makes place recognition based on point\nclouds challenging. Existing works usually encode low-level features such as\ncoordinate, normal, reflection intensity, etc., as local or global descriptors\nto represent scenes. Besides, they often ignore the translation between point\nclouds when matching descriptors. Different from most existing methods, we\nexplore the use of high-level features, namely semantics, to improve the\ndescriptor's representation ability. Also, when matching descriptors, we try to\ncorrect the translation between point clouds to improve accuracy. Concretely,\nwe propose a novel global descriptor, Semantic Scan Context, which explores\nsemantic information to represent scenes more effectively. We also present a\ntwo-step global semantic ICP to obtain the 3D pose (x, y, yaw) used to align\nthe point cloud to improve matching performance. Our experiments on the KITTI\ndataset show that our approach outperforms the state-of-the-art methods with a\nlarge margin. Our code is available at: https://github.com/lilin-hitcrt/SSC.",
          "link": "http://arxiv.org/abs/2107.00382",
          "publishedOn": "2021-07-02T01:57:59.874Z",
          "wordCount": 621,
          "title": "SSC: Semantic Scan Context for Large-Scale Place Recognition. (arXiv:2107.00382v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00396",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bulatov_K/0/1/0/all/0/1\">Konstantin Bulatov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Emelianova_E/0/1/0/all/0/1\">Ekaterina Emelianova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tropin_D/0/1/0/all/0/1\">Daniil Tropin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skoryukina_N/0/1/0/all/0/1\">Natalya Skoryukina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chernyshova_Y/0/1/0/all/0/1\">Yulia Chernyshova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheshkus_A/0/1/0/all/0/1\">Alexander Sheshkus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Usilin_S/0/1/0/all/0/1\">Sergey Usilin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ming_Z/0/1/0/all/0/1\">Zuheng Ming</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burie_J/0/1/0/all/0/1\">Jean-Christophe Burie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luqman_M/0/1/0/all/0/1\">Muhammad Muzzamil Luqman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arlazarov_V/0/1/0/all/0/1\">Vladimir V. Arlazarov</a>",
          "description": "Identity documents recognition is an important sub-field of document\nanalysis, which deals with tasks of robust document detection, type\nidentification, text fields recognition, as well as identity fraud prevention\nand document authenticity validation given photos, scans, or video frames of an\nidentity document capture. Significant amount of research has been published on\nthis topic in recent years, however a chief difficulty for such research is\nscarcity of datasets, due to the subject matter being protected by security\nrequirements. A few datasets of identity documents which are available lack\ndiversity of document types, capturing conditions, or variability of document\nfield values. In addition, the published datasets were typically designed only\nfor a subset of document recognition problems, not for a complex identity\ndocument analysis. In this paper, we present a dataset MIDV-2020 which consists\nof 1000 video clips, 2000 scanned images, and 1000 photos of 1000 unique mock\nidentity documents, each with unique text field values and unique artificially\ngenerated faces, with rich annotation. For the presented benchmark dataset\nbaselines are provided for such tasks as document location and identification,\ntext fields recognition, and face detection. With 72409 annotated images in\ntotal, to the date of publication the proposed dataset is the largest publicly\navailable identity documents dataset with variable artificially generated data,\nand we believe that it will prove invaluable for advancement of the field of\ndocument analysis and recognition. The dataset is available for download at\nthis ftp URL and this http URL .",
          "link": "http://arxiv.org/abs/2107.00396",
          "publishedOn": "2021-07-02T01:57:59.854Z",
          "wordCount": 705,
          "title": "MIDV-2020: A Comprehensive Benchmark Dataset for Identity Document Analysis. (arXiv:2107.00396v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00285",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Henglin Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Haoyu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zitong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaobai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhaoz%3F_G/0/1/0/all/0/1\">Guoying Zhaoz?</a>",
          "description": "We introduce a new dataset for the emotional artificial intelligence\nresearch: identity-free video dataset for Micro-Gesture Understanding and\nEmotion analysis (iMiGUE). Different from existing public datasets, iMiGUE\nfocuses on nonverbal body gestures without using any identity information,\nwhile the predominant researches of emotion analysis concern sensitive\nbiometric data, like face and speech. Most importantly, iMiGUE focuses on\nmicro-gestures, i.e., unintentional behaviors driven by inner feelings, which\nare different from ordinary scope of gestures from other gesture datasets which\nare mostly intentionally performed for illustrative purposes. Furthermore,\niMiGUE is designed to evaluate the ability of models to analyze the emotional\nstates by integrating information of recognized micro-gesture, rather than just\nrecognizing prototypes in the sequences separately (or isolatedly). This is\nbecause the real need for emotion AI is to understand the emotional states\nbehind gestures in a holistic way. Moreover, to counter for the challenge of\nimbalanced sample distribution of this dataset, an unsupervised learning method\nis proposed to capture latent representations from the micro-gesture sequences\nthemselves. We systematically investigate representative methods on this\ndataset, and comprehensive experimental results reveal several interesting\ninsights from the iMiGUE, e.g., micro-gesture-based analysis can promote\nemotion understanding. We confirm that the new iMiGUE dataset could advance\nstudies of micro-gesture and emotion AI.",
          "link": "http://arxiv.org/abs/2107.00285",
          "publishedOn": "2021-07-02T01:57:59.846Z",
          "wordCount": 655,
          "title": "iMiGUE: An Identity-free Video Dataset for Micro-Gesture Understanding and Emotion Analysis. (arXiv:2107.00285v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00206",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1\">Shuai Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zhenfeng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhizhe Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Zhenyu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yao Zhao</a>",
          "description": "Benefiting from the powerful expressive capability of graphs, graph-based\napproaches have achieved impressive performance in various biomedical\napplications. Most existing methods tend to define the adjacency matrix among\nsamples manually based on meta-features, and then obtain the node embeddings\nfor downstream tasks by Graph Representation Learning (GRL). However, it is not\neasy for these approaches to generalize to unseen samples. Meanwhile, the\ncomplex correlation between modalities is also ignored. As a result, these\nfactors inevitably yield the inadequacy of providing valid information about\nthe patient's condition for a reliable diagnosis. In this paper, we propose an\nend-to-end Multimodal Graph Learning framework (MMGL) for disease prediction.\nTo effectively exploit the rich information across multi-modality associated\nwith diseases, amodal-attentional multi-modal fusion is proposed to integrate\nthe features of each modality by leveraging the correlation and complementarity\nbetween the modalities. Furthermore, instead of defining the adjacency matrix\nmanually as existing methods, the latent graph structure can be captured\nthrough a novel way of adaptive graph learning. It could be jointly optimized\nwith the prediction model, thus revealing the intrinsic connections among\nsamples. Unlike the previous transductive methods, our model is also applicable\nto the scenario of inductive learning for those unseen data. An extensive group\nof experiments on two disease prediction problems is then carefully designed\nand presented, demonstrating that MMGL obtains more favorable performances. In\naddition, we also visualize and analyze the learned graph structure to provide\nmore reliable decision support for doctors in real medical applications and\ninspiration for disease research.",
          "link": "http://arxiv.org/abs/2107.00206",
          "publishedOn": "2021-07-02T01:57:59.834Z",
          "wordCount": 693,
          "title": "Multi-modal Graph Learning for Disease Prediction. (arXiv:2107.00206v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00471",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Thambawita_V/0/1/0/all/0/1\">Vajira Thambawita</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Salehi_P/0/1/0/all/0/1\">Pegah Salehi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sheshkal_S/0/1/0/all/0/1\">Sajad Amouei Sheshkal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hicks_S/0/1/0/all/0/1\">Steven A. Hicks</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hammer_H/0/1/0/all/0/1\">Hugo L.Hammer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Parasa_S/0/1/0/all/0/1\">Sravanthi Parasa</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lange_T/0/1/0/all/0/1\">Thomas de Lange</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Halvorsen_P/0/1/0/all/0/1\">P&#xe5;l Halvorsen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Riegler_M/0/1/0/all/0/1\">Michael A. Riegler</a>",
          "description": "Processing medical data to find abnormalities is a time-consuming and costly\ntask, requiring tremendous efforts from medical experts. Therefore, Ai has\nbecome a popular tool for the automatic processing of medical data, acting as a\nsupportive tool for doctors. AI tools highly depend on data for training the\nmodels. However, there are several constraints to access to large amounts of\nmedical data to train machine learning algorithms in the medical domain, e.g.,\ndue to privacy concerns and the costly, time-consuming medical data annotation\nprocess. To address this, in this paper we present a novel synthetic data\ngeneration pipeline called SinGAN-Seg to produce synthetic medical data with\nthe corresponding annotated ground truth masks. We show that these synthetic\ndata generation pipelines can be used as an alternative to bypass privacy\nconcerns and as an alternative way to produce artificial segmentation datasets\nwith corresponding ground truth masks to avoid the tedious medical data\nannotation process. As a proof of concept, we used an open polyp segmentation\ndataset. By training UNet++ using both the real polyp segmentation dataset and\nthe corresponding synthetic dataset generated from the SinGAN-Seg pipeline, we\nshow that the synthetic data can achieve a very close performance to the real\ndata when the real segmentation datasets are large enough. In addition, we show\nthat synthetic data generated from the SinGAN-Seg pipeline improving the\nperformance of segmentation algorithms when the training dataset is very small.\nSince our SinGAN-Seg pipeline is applicable for any medical dataset, this\npipeline can be used with any other segmentation datasets.",
          "link": "http://arxiv.org/abs/2107.00471",
          "publishedOn": "2021-07-02T01:57:59.826Z",
          "wordCount": 718,
          "title": "SinGAN-Seg: Synthetic Training Data Generation for Medical Image Segmentation. (arXiv:2107.00471v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00135",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nagrani_A/0/1/0/all/0/1\">Arsha Nagrani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arnab_A/0/1/0/all/0/1\">Anurag Arnab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jansen_A/0/1/0/all/0/1\">Aren Jansen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_C/0/1/0/all/0/1\">Cordelia Schmid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Chen Sun</a>",
          "description": "Humans perceive the world by concurrently processing and fusing\nhigh-dimensional inputs from multiple modalities such as vision and audio.\nMachine perception models, in stark contrast, are typically modality-specific\nand optimised for unimodal benchmarks, and hence late-stage fusion of final\nrepresentations or predictions from each modality (`late-fusion') is still a\ndominant paradigm for multimodal video classification. Instead, we introduce a\nnovel transformer based architecture that uses `fusion bottlenecks' for\nmodality fusion at multiple layers. Compared to traditional pairwise\nself-attention, our model forces information between different modalities to\npass through a small number of bottleneck latents, requiring the model to\ncollate and condense the most relevant information in each modality and only\nshare what is necessary. We find that such a strategy improves fusion\nperformance, at the same time reducing computational cost. We conduct thorough\nablation studies, and achieve state-of-the-art results on multiple audio-visual\nclassification benchmarks including Audioset, Epic-Kitchens and VGGSound. All\ncode and models will be released.",
          "link": "http://arxiv.org/abs/2107.00135",
          "publishedOn": "2021-07-02T01:57:59.818Z",
          "wordCount": 590,
          "title": "Attention Bottlenecks for Multimodal Fusion. (arXiv:2107.00135v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00315",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Varshney_N/0/1/0/all/0/1\">Neeraj Varshney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Swaroop Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1\">Chitta Baral</a>",
          "description": "Standard NLP tasks do not incorporate several common real-world scenarios\nsuch as seeking clarifications about the question, taking advantage of clues,\nabstaining in order to avoid incorrect answers, etc. This difference in task\nformulation hinders the adoption of NLP systems in real-world settings. In this\nwork, we take a step towards bridging this gap and present a multi-stage task\nthat simulates a typical human-human questioner-responder interaction such as\nan interview. Specifically, the system is provided with question\nsimplifications, knowledge statements, examples, etc. at various stages to\nimprove its prediction when it is not sufficiently confident. We instantiate\nthe proposed task in Natural Language Inference setting where a system is\nevaluated on both in-domain and out-of-domain (OOD) inputs. We conduct\ncomprehensive experiments and find that the multi-stage formulation of our task\nleads to OOD generalization performance improvement up to 2.29% in Stage 1,\n1.91% in Stage 2, 54.88% in Stage 3, and 72.02% in Stage 4 over the standard\nunguided prediction. However, our task leaves a significant challenge for NLP\nresearchers to further improve OOD performance at each stage.",
          "link": "http://arxiv.org/abs/2107.00315",
          "publishedOn": "2021-07-02T01:57:59.798Z",
          "wordCount": 626,
          "title": "Interviewer-Candidate Role Play: Towards Developing Real-World NLP Systems. (arXiv:2107.00315v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00415",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Marchisio_A/0/1/0/all/0/1\">Alberto Marchisio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pira_G/0/1/0/all/0/1\">Giacomo Pira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martina_M/0/1/0/all/0/1\">Maurizio Martina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masera_G/0/1/0/all/0/1\">Guido Masera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shafique_M/0/1/0/all/0/1\">Muhammad Shafique</a>",
          "description": "Spiking Neural Networks (SNNs), despite being energy-efficient when\nimplemented on neuromorphic hardware and coupled with event-based Dynamic\nVision Sensors (DVS), are vulnerable to security threats, such as adversarial\nattacks, i.e., small perturbations added to the input for inducing a\nmisclassification. Toward this, we propose DVS-Attacks, a set of stealthy yet\nefficient adversarial attack methodologies targeted to perturb the event\nsequences that compose the input of the SNNs. First, we show that noise filters\nfor DVS can be used as defense mechanisms against adversarial attacks.\nAfterwards, we implement several attacks and test them in the presence of two\ntypes of noise filters for DVS cameras. The experimental results show that the\nfilters can only partially defend the SNNs against our proposed DVS-Attacks.\nUsing the best settings for the noise filters, our proposed Mask Filter-Aware\nDash Attack reduces the accuracy by more than 20% on the DVS-Gesture dataset\nand by more than 65% on the MNIST dataset, compared to the original clean\nframes. The source code of all the proposed DVS-Attacks and noise filters is\nreleased at https://github.com/albertomarchisio/DVS-Attacks.",
          "link": "http://arxiv.org/abs/2107.00415",
          "publishedOn": "2021-07-02T01:57:59.790Z",
          "wordCount": 633,
          "title": "DVS-Attacks: Adversarial Attacks on Dynamic Vision Sensors for Spiking Neural Networks. (arXiv:2107.00415v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00181",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhen Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1\">Xu Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_J/0/1/0/all/0/1\">Jun Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tongliang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_X/0/1/0/all/0/1\">Xinmei Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Houqiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_B/0/1/0/all/0/1\">Bing Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jianqiang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_X/0/1/0/all/0/1\">Xian-Sheng Hua</a>",
          "description": "Knowledge Distillation (KD) is a popular technique to transfer knowledge from\na teacher model or ensemble to a student model. Its success is generally\nattributed to the privileged information on similarities/consistency between\nthe class distributions or intermediate feature representations of the teacher\nmodel and the student model. However, directly pushing the student model to\nmimic the probabilities/features of the teacher model to a large extent limits\nthe student model in learning undiscovered knowledge/features. In this paper,\nwe propose a novel inheritance and exploration knowledge distillation framework\n(IE-KD), in which a student model is split into two parts - inheritance and\nexploration. The inheritance part is learned with a similarity loss to transfer\nthe existing learned knowledge from the teacher model to the student model,\nwhile the exploration part is encouraged to learn representations different\nfrom the inherited ones with a dis-similarity loss. Our IE-KD framework is\ngeneric and can be easily combined with existing distillation or mutual\nlearning methods for training deep neural networks. Extensive experiments\ndemonstrate that these two parts can jointly push the student model to learn\nmore diversified and effective representations, and our IE-KD can be a general\ntechnique to improve the student network to achieve SOTA performance.\nFurthermore, by applying our IE-KD to the training of two networks, the\nperformance of both can be improved w.r.t. deep mutual learning. The code and\nmodels of IE-KD will be make publicly available at\nhttps://github.com/yellowtownhz/IE-KD.",
          "link": "http://arxiv.org/abs/2107.00181",
          "publishedOn": "2021-07-02T01:57:59.783Z",
          "wordCount": 693,
          "title": "Revisiting Knowledge Distillation: An Inheritance and Exploration Framework. (arXiv:2107.00181v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00235",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Pavlov_S/0/1/0/all/0/1\">Stoyan Pavlov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Momcheva_G/0/1/0/all/0/1\">Galina Momcheva</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Burlakova_P/0/1/0/all/0/1\">Pavlina Burlakova</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Atanasov_S/0/1/0/all/0/1\">Simeon Atanasov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Stoyanov_D/0/1/0/all/0/1\">Dimo Stoyanov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ivanov_M/0/1/0/all/0/1\">Martin Ivanov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tonchev_A/0/1/0/all/0/1\">Anton Tonchev</a>",
          "description": "This paper presents a proof of concept for the usefulness of second-order\ntexture features for the qualitative analysis and classification of chromogenic\nin-situ hybridization whole slide images in high-throughput imaging\nexperiments. The challenge is that currently, the gold standard for gene\nexpression grading in such images is expert assessment. The idea of the\nresearch team is to use different approaches in the analysis of these images\nthat will be used for structural segmentation and functional analysis in gene\nexpression. The article presents such perspective idea to select a number of\ntextural features that are going to be used for classification. In our\nexperiment, natural grouping of image samples (tiles) depending on their local\ntexture properties was explored in an unsupervised classification procedure.\nThe features are reduced to two dimensions with fuzzy c-means clustering. The\noverall conclusion of this experiment is that Haralick features are a viable\nchoice for classification and analysis of chromogenic in-situ hybridization\nimage data. The principal component analysis approach produced slightly more\n\"understandable\" from an annotator's point of view classes.",
          "link": "http://arxiv.org/abs/2107.00235",
          "publishedOn": "2021-07-02T01:57:59.776Z",
          "wordCount": 659,
          "title": "Feasibility of Haralick's Texture Features for the Classification of Chromogenic In-situ Hybridization Images. (arXiv:2107.00235v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00362",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Piciarelli_C/0/1/0/all/0/1\">Claudio Piciarelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foresti_G/0/1/0/all/0/1\">Gian Luca Foresti</a>",
          "description": "Swarms of drones are being more and more used in many practical scenarios,\nsuch as surveillance, environmental monitoring, search and rescue in\nhardly-accessible areas, etc.. While a single drone can be guided by a human\noperator, the deployment of a swarm of multiple drones requires proper\nalgorithms for automatic task-oriented control. In this paper, we focus on\nvisual coverage optimization with drone-mounted camera sensors. In particular,\nwe consider the specific case in which the coverage requirements are uneven,\nmeaning that different parts of the environment have different coverage\npriorities. We model these coverage requirements with relevance maps and\npropose a deep reinforcement learning algorithm to guide the swarm. The paper\nfirst defines a proper learning model for a single drone, and then extends it\nto the case of multiple drones both with greedy and cooperative strategies.\nExperimental results show the performance of the proposed method, also compared\nwith a standard patrolling algorithm.",
          "link": "http://arxiv.org/abs/2107.00362",
          "publishedOn": "2021-07-02T01:57:59.767Z",
          "wordCount": 610,
          "title": "Drone swarm patrolling with uneven coverage requirements. (arXiv:2107.00362v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00166",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiaolong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_G/0/1/0/all/0/1\">Geng Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1\">Xuan Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tianlong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xuxi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaohan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1\">Ning Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_M/0/1/0/all/0/1\">Minghai Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Sijia Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanzhi Wang</a>",
          "description": "There have been long-standing controversies and inconsistencies over the\nexperiment setup and criteria for identifying the \"winning ticket\" in\nliterature. To reconcile such, we revisit the definition of lottery ticket\nhypothesis, with comprehensive and more rigorous conditions. Under our new\ndefinition, we show concrete evidence to clarify whether the winning ticket\nexists across the major DNN architectures and/or applications. Through\nextensive experiments, we perform quantitative analysis on the correlations\nbetween winning tickets and various experimental factors, and empirically study\nthe patterns of our observations. We find that the key training\nhyperparameters, such as learning rate and training epochs, as well as the\narchitecture characteristics such as capacities and residual connections, are\nall highly correlated with whether and when the winning tickets can be\nidentified. Based on our analysis, we summarize a guideline for parameter\nsettings in regards of specific architecture characteristics, which we hope to\ncatalyze the research progress on the topic of lottery ticket hypothesis.",
          "link": "http://arxiv.org/abs/2107.00166",
          "publishedOn": "2021-07-02T01:57:59.746Z",
          "wordCount": 620,
          "title": "Sanity Checks for Lottery Tickets: Does Your Winning Ticket Really Win the Jackpot?. (arXiv:2107.00166v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00191",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_W/0/1/0/all/0/1\">Wonju Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Byun_S/0/1/0/all/0/1\">Seok-Yong Byun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jooeun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_M/0/1/0/all/0/1\">Minje Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chechil_K/0/1/0/all/0/1\">Kirill Chechil</a>",
          "description": "While many real-world data streams imply that they change frequently in a\nnonstationary way, most of deep learning methods optimize neural networks on\ntraining data, and this leads to severe performance degradation when dataset\nshift happens. However, it is less possible to annotate or inspect newly\nstreamed data by humans, and thus it is desired to measure model drift at\ninference time in an unsupervised manner. In this paper, we propose a novel\nmethod of model drift estimation by exploiting statistics of batch\nnormalization layer on unlabeled test data. To remedy possible sampling error\nof streamed input data, we adopt low-rank approximation to each\nrepresentational layer. We show the effectiveness of our method not only on\ndataset shift detection but also on model selection when there are multiple\ncandidate models among model zoo or training trajectories in an unsupervised\nway. We further demonstrate the consistency of our method by comparing model\ndrift scores between different network architectures.",
          "link": "http://arxiv.org/abs/2107.00191",
          "publishedOn": "2021-07-02T01:57:59.739Z",
          "wordCount": 616,
          "title": "Unsupervised Model Drift Estimation with Batch Normalization Statistics for Dataset Shift Detection and Model Selection. (arXiv:2107.00191v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00254",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Niu_S/0/1/0/all/0/1\">Shuaicheng Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiaxiang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Guanghui Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yifan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yong Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1\">Peilin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1\">Mingkui Tan</a>",
          "description": "In real-world applications, data often come in a growing manner, where the\ndata volume and the number of classes may increase dynamically. This will bring\na critical challenge for learning: given the increasing data volume or the\nnumber of classes, one has to instantaneously adjust the neural model capacity\nto obtain promising performance. Existing methods either ignore the growing\nnature of data or seek to independently search an optimal architecture for a\ngiven dataset, and thus are incapable of promptly adjusting the architectures\nfor the changed data. To address this, we present a neural architecture\nadaptation method, namely Adaptation eXpert (AdaXpert), to efficiently adjust\nprevious architectures on the growing data. Specifically, we introduce an\narchitecture adjuster to generate a suitable architecture for each data\nsnapshot, based on the previous architecture and the different extent between\ncurrent and previous data distributions. Furthermore, we propose an adaptation\ncondition to determine the necessity of adjustment, thereby avoiding\nunnecessary and time-consuming adjustments. Extensive experiments on two growth\nscenarios (increasing data volume and number of classes) demonstrate the\neffectiveness of the proposed method.",
          "link": "http://arxiv.org/abs/2107.00254",
          "publishedOn": "2021-07-02T01:57:59.731Z",
          "wordCount": 626,
          "title": "AdaXpert: Adapting Neural Architecture for Growing Data. (arXiv:2107.00254v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00462",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wurster_S/0/1/0/all/0/1\">Skylar W. Wurster</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shen_H/0/1/0/all/0/1\">Han-Wei Shen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guo_H/0/1/0/all/0/1\">Hanqi Guo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Peterka_T/0/1/0/all/0/1\">Thomas Peterka</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Raj_M/0/1/0/all/0/1\">Mukund Raj</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_J/0/1/0/all/0/1\">Jiayi Xu</a>",
          "description": "We present an approach for hierarchical super resolution (SR) using neural\nnetworks on an octree data representation. We train a hierarchy of neural\nnetworks, each capable of 2x upscaling in each spatial dimension between two\nlevels of detail, and use these networks in tandem to facilitate large scale\nfactor super resolution, scaling with the number of trained networks. We\nutilize these networks in a hierarchical super resolution algorithm that\nupscales multiresolution data to a uniform high resolution without introducing\nseam artifacts on octree node boundaries. We evaluate application of this\nalgorithm in a data reduction framework by dynamically downscaling input data\nto an octree-based data structure to represent the multiresolution data before\ncompressing for additional storage reduction. We demonstrate that our approach\navoids seam artifacts common to multiresolution data formats, and show how\nneural network super resolution assisted data reduction can preserve global\nfeatures better than compressors alone at the same compression ratios.",
          "link": "http://arxiv.org/abs/2107.00462",
          "publishedOn": "2021-07-02T01:57:59.724Z",
          "wordCount": 609,
          "title": "Deep Hierarchical Super-Resolution for Scientific Data Reduction and Visualization. (arXiv:2107.00462v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00337",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Plizzari_C/0/1/0/all/0/1\">Chiara Plizzari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Planamente_M/0/1/0/all/0/1\">Mirco Planamente</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alberti_E/0/1/0/all/0/1\">Emanuele Alberti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caputo_B/0/1/0/all/0/1\">Barbara Caputo</a>",
          "description": "In this report, we describe the technical details of our submission to the\nEPIC-Kitchens-100 Unsupervised Domain Adaptation (UDA) Challenge in Action\nRecognition. To tackle the domain-shift which exists under the UDA setting, we\nfirst exploited a recent Domain Generalization (DG) technique, called Relative\nNorm Alignment (RNA). It consists in designing a model able to generalize well\nto any unseen domain, regardless of the possibility to access target data at\ntraining time. Then, in a second phase, we extended the approach to work on\nunlabelled target data, allowing the model to adapt to the target distribution\nin an unsupervised fashion. For this purpose, we included in our framework\nexisting UDA algorithms, such as Temporal Attentive Adversarial Adaptation\nNetwork (TA3N), jointly with new multi-stream consistency losses, namely\nTemporal Hard Norm Alignment (T-HNA) and Min-Entropy Consistency (MEC). Our\nsubmission (entry 'plnet') is visible on the leaderboard and it achieved the\n1st position for 'verb', and the 3rd position for both 'noun' and 'action'.",
          "link": "http://arxiv.org/abs/2107.00337",
          "publishedOn": "2021-07-02T01:57:59.717Z",
          "wordCount": 618,
          "title": "PoliTO-IIT Submission to the EPIC-KITCHENS-100 Unsupervised Domain Adaptation Challenge for Action Recognition. (arXiv:2107.00337v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00358",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wei-Hong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xialei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bilen_H/0/1/0/all/0/1\">Hakan Bilen</a>",
          "description": "In this paper, we look at the problem of cross-domain few-shot classification\nthat aims to learn a classifier from previously unseen classes and domains with\nfew labeled samples. We study several strategies including various adapter\ntopologies and operations in terms of their performance and efficiency that can\nbe easily attached to existing methods with different meta-training strategies\nand adapt them for a given task during meta-test phase. We show that parametric\nadapters attached to convolutional layers with residual connections performs\nthe best, and significantly improves the performance of the state-of-the-art\nmodels in the Meta-Dataset benchmark with minor additional cost. Our code will\nbe available at https://github.com/VICO-UoE/URL.",
          "link": "http://arxiv.org/abs/2107.00358",
          "publishedOn": "2021-07-02T01:57:59.697Z",
          "wordCount": 549,
          "title": "Improving Task Adaptation for Cross-domain Few-shot Learning. (arXiv:2107.00358v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00420",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liang_T/0/1/0/all/0/1\">Tingting Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_X/0/1/0/all/0/1\">Xiaojie Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yudong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yongtao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1\">Zhi Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_W/0/1/0/all/0/1\">Wei Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jingdong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_H/0/1/0/all/0/1\">Haibing Ling</a>",
          "description": "Modern top-performing object detectors depend heavily on backbone networks,\nwhose advances bring consistent performance gains through exploring more\neffective network structures. However, designing or searching for a new\nbackbone and pre-training it on ImageNet may require a large number of\ncomputational resources, making it costly to obtain better detection\nperformance. In this paper, we propose a novel backbone network, namely\nCBNetV2, by constructing compositions of existing open-sourced pre-trained\nbackbones. In particular, CBNetV2 architecture groups multiple identical\nbackbones, which are connected through composite connections. We also propose a\nbetter training strategy with the Assistant Supervision for CBNet-based\ndetectors. Without additional pre-training, CBNetV2 can be integrated into\nmainstream detectors, including one-stage and two-stage detectors, as well as\nanchor-based and anchor-free-based ones, and significantly improve their\nperformance by more than 3.0% AP over the baseline on COCO. Also, experiments\nprovide strong evidence showing that composite backbones are more efficient and\nresource-friendly than pre-trained wider and deeper networks, including\nmanual-based and NAS-based, as well as CNN-based and Transformer-based ones.\nParticularly, with single-model and single-scale testing, our HTC Dual-Swin-B\nachieves 58.6% box AP and 51.1% mask AP on COCO test-dev, which is\nsignificantly better than the state-of-the-art result (i.e., 57.7% box AP and\n50.2% mask AP) achieved by a stronger baseline HTC++ with a larger backbone\nSwin-L. Code will be released at https://github.com/VDIGPKU/CBNetV2.",
          "link": "http://arxiv.org/abs/2107.00420",
          "publishedOn": "2021-07-02T01:57:59.689Z",
          "wordCount": 670,
          "title": "CBNetV2: A Composite Backbone Network Architecture for Object Detection. (arXiv:2107.00420v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00229",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Long_Y/0/1/0/all/0/1\">Yonghao Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhaoshuo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yee_C/0/1/0/all/0/1\">Chi Hang Yee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_C/0/1/0/all/0/1\">Chi Fai Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taylor_R/0/1/0/all/0/1\">Russell H. Taylor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Unberath_M/0/1/0/all/0/1\">Mathias Unberath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_Q/0/1/0/all/0/1\">Qi Dou</a>",
          "description": "Reconstructing the scene of robotic surgery from the stereo endoscopic video\nis an important and promising topic in surgical data science, which potentially\nsupports many applications such as surgical visual perception, robotic surgery\neducation and intra-operative context awareness. However, current methods are\nmostly restricted to reconstructing static anatomy assuming no tissue\ndeformation, tool occlusion and de-occlusion, and camera movement. However,\nthese assumptions are not always satisfied in minimal invasive robotic\nsurgeries. In this work, we present an efficient reconstruction pipeline for\nhighly dynamic surgical scenes that runs at 28 fps. Specifically, we design a\ntransformer-based stereoscopic depth perception for efficient depth estimation\nand a light-weight tool segmentor to handle tool occlusion. After that, a\ndynamic reconstruction algorithm which can estimate the tissue deformation and\ncamera movement, and aggregate the information over time is proposed for\nsurgical scene reconstruction. We evaluate the proposed pipeline on two\ndatasets, the public Hamlyn Centre Endoscopic Video Dataset and our in-house\nDaVinci robotic surgery dataset. The results demonstrate that our method can\nrecover the scene obstructed by the surgical tool and handle the movement of\ncamera in realistic surgical scenarios effectively at real-time speed.",
          "link": "http://arxiv.org/abs/2107.00229",
          "publishedOn": "2021-07-02T01:57:59.676Z",
          "wordCount": 649,
          "title": "E-DSSR: Efficient Dynamic Surgical Scene Reconstruction with Transformer-based Stereoscopic Depth Perception. (arXiv:2107.00229v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00327",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Ming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhe_X/0/1/0/all/0/1\">Xuefei Zhe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1\">Hong Yan</a>",
          "description": "Recently, deep hashing with Hamming distance metric has drawn increasing\nattention for face image retrieval tasks. However, its counterpart deep\nquantization methods, which learn binary code representations with\ndictionary-related distance metrics, have seldom been explored for the task.\nThis paper makes the first attempt to integrate product quantization into an\nend-to-end deep learning framework for face image retrieval. Unlike prior deep\nquantization methods where the codewords for quantization are learned from\ndata, we propose a novel scheme using predefined orthonormal vectors as\ncodewords, which aims to enhance the quantization informativeness and reduce\nthe codewords' redundancy. To make the most of the discriminative information,\nwe design a tailored loss function that maximizes the identity discriminability\nin each quantization subspace for both the quantized and the original features.\nFurthermore, an entropy-based regularization term is imposed to reduce the\nquantization error. We conduct experiments on three commonly-used datasets\nunder the settings of both single-domain and cross-domain retrieval. It shows\nthat the proposed method outperforms all the compared deep hashing/quantization\nmethods under both settings with significant superiority. The proposed\ncodewords scheme consistently improves both regular model performance and model\ngeneralization ability, verifying the importance of codewords' distribution for\nthe quantization quality. Besides, our model's better generalization ability\nthan deep hashing models indicates that it is more suitable for scalable face\nimage retrieval tasks.",
          "link": "http://arxiv.org/abs/2107.00327",
          "publishedOn": "2021-07-02T01:57:59.664Z",
          "wordCount": 654,
          "title": "Orthonormal Product Quantization Network for Scalable Face Image Retrieval. (arXiv:2107.00327v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00430",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Q/0/1/0/all/0/1\">Qiulei Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhanyi Hu</a>",
          "description": "This work is to tackle the problem of point cloud semantic segmentation for\n3D hybrid scenes under the framework of zero-shot learning. Here by hybrid, we\nmean the scene consists of both seen-class and unseen-class 3D objects, a more\ngeneral and realistic setting in application. To our knowledge, this problem\nhas not been explored in the literature. To this end, we propose a network to\nsynthesize point features for various classes of objects by leveraging the\nsemantic features of both seen and unseen object classes, called PFNet. The\nproposed PFNet employs a GAN architecture to synthesize point features, where\nthe semantic relationship between seen-class and unseen-class features is\nconsolidated by adapting a new semantic regularizer, and the synthesized\nfeatures are used to train a classifier for predicting the labels of the\ntesting 3D scene points. Besides we also introduce two benchmarks for\nalgorithmic evaluation by re-organizing the public S3DIS and ScanNet datasets\nunder six different data splits. Experimental results on the two benchmarks\nvalidate our proposed method, and we hope our introduced two benchmarks and\nmethodology could be of help for more research on this new direction.",
          "link": "http://arxiv.org/abs/2107.00430",
          "publishedOn": "2021-07-02T01:57:59.656Z",
          "wordCount": 623,
          "title": "Segmenting 3D Hybrid Scenes via Zero-Shot Learning. (arXiv:2107.00430v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00057",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Du_X/0/1/0/all/0/1\">Xianzhi Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zoph_B/0/1/0/all/0/1\">Barret Zoph</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hung_W/0/1/0/all/0/1\">Wei-Chih Hung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1\">Tsung-Yi Lin</a>",
          "description": "The speed-accuracy Pareto curve of object detection systems have advanced\nthrough a combination of better model architectures, training and inference\nmethods. In this paper, we methodically evaluate a variety of these techniques\nto understand where most of the improvements in modern detection systems come\nfrom. We benchmark these improvements on the vanilla ResNet-FPN backbone with\nRetinaNet and RCNN detectors. The vanilla detectors are improved by 7.7% in\naccuracy while being 30% faster in speed. We further provide simple scaling\nstrategies to generate family of models that form two Pareto curves, named\nRetinaNet-RS and Cascade RCNN-RS. These simple rescaled detectors explore the\nspeed-accuracy trade-off between the one-stage RetinaNet detectors and\ntwo-stage RCNN detectors. Our largest Cascade RCNN-RS models achieve 52.9% AP\nwith a ResNet152-FPN backbone and 53.6% with a SpineNet143L backbone. Finally,\nwe show the ResNet architecture, with three minor architectural changes,\noutperforms EfficientNet as the backbone for object detection and instance\nsegmentation systems.",
          "link": "http://arxiv.org/abs/2107.00057",
          "publishedOn": "2021-07-02T01:57:59.648Z",
          "wordCount": 597,
          "title": "Simple Training Strategies and Model Scaling for Object Detection. (arXiv:2107.00057v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00197",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Han-Jia Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ming_L/0/1/0/all/0/1\">Lu Ming</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_D/0/1/0/all/0/1\">De-Chuan Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_W/0/1/0/all/0/1\">Wei-Lun Chao</a>",
          "description": "Few-shot learning (FSL) aims to train a strong classifier using limited\nlabeled examples. Many existing works take the meta-learning approach, sampling\nfew-shot tasks in turn and optimizing the few-shot learner's performance on\nclassifying the query examples. In this paper, we point out two potential\nweaknesses of this approach. First, the sampled query examples may not provide\nsufficient supervision for the few-shot learner. Second, the effectiveness of\nmeta-learning diminishes sharply with increasing shots (i.e., the number of\ntraining examples per class). To resolve these issues, we propose a novel\nobjective to directly train the few-shot learner to perform like a strong\nclassifier. Concretely, we associate each sampled few-shot task with a strong\nclassifier, which is learned with ample labeled examples. The strong classifier\nhas a better generalization ability and we use it to supervise the few-shot\nlearner. We present an efficient way to construct the strong classifier, making\nour proposed objective an easily plug-and-play term to existing meta-learning\nbased FSL methods. We validate our approach in combinations with many\nrepresentative meta-learning methods. On several benchmark datasets including\nminiImageNet and tiredImageNet, our approach leads to a notable improvement\nacross a variety of tasks. More importantly, with our approach, meta-learning\nbased FSL methods can consistently outperform non-meta-learning based ones,\neven in a many-shot setting, greatly strengthening their applicability.",
          "link": "http://arxiv.org/abs/2107.00197",
          "publishedOn": "2021-07-02T01:57:59.628Z",
          "wordCount": 654,
          "title": "Few-Shot Learning with a Strong Teacher. (arXiv:2107.00197v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00223",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Uchizawa_K/0/1/0/all/0/1\">Kei Uchizawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abe_H/0/1/0/all/0/1\">Haruki Abe</a>",
          "description": "We study computational hardness of feature and conjunction search through the\nlens of circuit complexity. Let $x = (x_1, ... , x_n)$ (resp., $y = (y_1, ... ,\ny_n)$) be Boolean variables each of which takes the value one if and only if a\nneuron at place $i$ detects a feature (resp., another feature). We then simply\nformulate the feature and conjunction search as Boolean functions ${\\rm\nFTR}_n(x) = \\bigvee_{i=1}^n x_i$ and ${\\rm CONJ}_n(x, y) = \\bigvee_{i=1}^n x_i\n\\wedge y_i$, respectively. We employ a threshold circuit or a discretized\ncircuit (such as a sigmoid circuit or a ReLU circuit with discretization) as\nour models of neural networks, and consider the following four computational\nresources: [i] the number of neurons (size), [ii] the number of levels (depth),\n[iii] the number of active neurons outputting non-zero values (energy), and\n[iv] synaptic weight resolution (weight).\n\nWe first prove that any threshold circuit $C$ of size $s$, depth $d$, energy\n$e$ and weight $w$ satisfies $\\log rk(M_C) \\le ed (\\log s + \\log w + \\log n)$,\nwhere $rk(M_C)$ is the rank of the communication matrix $M_C$ of a\n$2n$-variable Boolean function that $C$ computes. Since ${\\rm CONJ}_n$ has rank\n$2^n$, we have $n \\le ed (\\log s + \\log w + \\log n)$. Thus, an exponential\nlower bound on the size of even sublinear-depth threshold circuits exists if\nthe energy and weight are sufficiently small. Since ${\\rm FTR}_n$ is computable\nindependently of $n$, our result suggests that computational capacity for the\nfeature and conjunction search are different. We also show that the inequality\nis tight up to a constant factor if $ed = o(n/ \\log n)$. We next show that a\nsimilar inequality holds for any discretized circuit. Thus, if we regard the\nnumber of gates outputting non-zero values as a measure for sparse activity,\nour results suggest that larger depth helps neural networks to acquire sparse\nactivity.",
          "link": "http://arxiv.org/abs/2107.00223",
          "publishedOn": "2021-07-02T01:57:59.570Z",
          "wordCount": 752,
          "title": "Circuit Complexity of Visual Search. (arXiv:2107.00223v1 [cs.CC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00372",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_J/0/1/0/all/0/1\">Jianing Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lo_F/0/1/0/all/0/1\">Frank P.-W. Lo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_X/0/1/0/all/0/1\">Xiao Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jobarteh_M/0/1/0/all/0/1\">Modou L. Jobarteh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_W/0/1/0/all/0/1\">Wenyan Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baranowski_T/0/1/0/all/0/1\">Tom Baranowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steiner_Asiedu_M/0/1/0/all/0/1\">Matilda Steiner-Asiedu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anderson_A/0/1/0/all/0/1\">Alex K. Anderson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCrory_M/0/1/0/all/0/1\">Megan A McCrory</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sazonov_E/0/1/0/all/0/1\">Edward Sazonov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Mingui Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frost_G/0/1/0/all/0/1\">Gary Frost</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lo_B/0/1/0/all/0/1\">Benny Lo</a>",
          "description": "Camera-based passive dietary intake monitoring is able to continuously\ncapture the eating episodes of a subject, recording rich visual information,\nsuch as the type and volume of food being consumed, as well as the eating\nbehaviours of the subject. However, there currently is no method that is able\nto incorporate these visual clues and provide a comprehensive context of\ndietary intake from passive recording (e.g., is the subject sharing food with\nothers, what food the subject is eating, and how much food is left in the\nbowl). On the other hand, privacy is a major concern while egocentric wearable\ncameras are used for capturing. In this paper, we propose a privacy-preserved\nsecure solution (i.e., egocentric image captioning) for dietary assessment with\npassive monitoring, which unifies food recognition, volume estimation, and\nscene understanding. By converting images into rich text descriptions,\nnutritionists can assess individual dietary intake based on the captions\ninstead of the original images, reducing the risk of privacy leakage from\nimages. To this end, an egocentric dietary image captioning dataset has been\nbuilt, which consists of in-the-wild images captured by head-worn and\nchest-worn cameras in field studies in Ghana. A novel transformer-based\narchitecture is designed to caption egocentric dietary images. Comprehensive\nexperiments have been conducted to evaluate the effectiveness and to justify\nthe design of the proposed architecture for egocentric dietary image\ncaptioning. To the best of our knowledge, this is the first work that applies\nimage captioning to dietary intake assessment in real life settings.",
          "link": "http://arxiv.org/abs/2107.00372",
          "publishedOn": "2021-07-02T01:57:59.561Z",
          "wordCount": 706,
          "title": "Egocentric Image Captioning for Privacy-Preserved Passive Dietary Intake Monitoring. (arXiv:2107.00372v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00272",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ahmedt_Aristizabal_D/0/1/0/all/0/1\">David Ahmedt-Aristizabal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Armin_M/0/1/0/all/0/1\">Mohammad Ali Armin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denman_S/0/1/0/all/0/1\">Simon Denman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fookes_C/0/1/0/all/0/1\">Clinton Fookes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petersson_L/0/1/0/all/0/1\">Lars Petersson</a>",
          "description": "With the remarkable success of representation learning for prediction\nproblems, we have witnessed a rapid expansion of the use of machine learning\nand deep learning for the analysis of digital pathology and biopsy image\npatches. However, traditional learning over patch-wise features using\nconvolutional neural networks limits the model when attempting to capture\nglobal contextual information. The phenotypical and topological distribution of\nconstituent histological entities play a critical role in tissue diagnosis. As\nsuch, graph data representations and deep learning have attracted significant\nattention for encoding tissue representations, and capturing intra- and inter-\nentity level interactions. In this review, we provide a conceptual grounding of\ngraph-based deep learning and discuss its current success for tumor\nlocalization and classification, tumor invasion and staging, image retrieval,\nand survival prediction. We provide an overview of these methods in a\nsystematic manner organized by the graph representation of the input image\nincluding whole slide images and tissue microarrays. We also outline the\nlimitations of existing techniques, and suggest potential future advances in\nthis domain.",
          "link": "http://arxiv.org/abs/2107.00272",
          "publishedOn": "2021-07-02T01:57:59.542Z",
          "wordCount": 617,
          "title": "A Survey on Graph-Based Deep Learning for Computational Histopathology. (arXiv:2107.00272v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00085",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Ankit Singh</a>",
          "description": "Unsupervised Domain Adaptation (UDA) aims to align the labeled source\ndistribution with the unlabeled target distribution to obtain domain invariant\npredictive models. However, the application of well-known UDA approaches does\nnot generalize well in Semi-Supervised Domain Adaptation (SSDA) scenarios where\nfew labeled samples from the target domain are available. In this paper, we\npropose a simple Contrastive Learning framework for semi-supervised Domain\nAdaptation (CLDA) that attempts to bridge the intra-domain gap between the\nlabeled and unlabeled target distributions and inter-domain gap between source\nand unlabeled target distribution in SSDA. We suggest employing class-wise\ncontrastive learning to reduce the inter-domain gap and instance-level\ncontrastive alignment between the original (input image) and strongly augmented\nunlabeled target images to minimize the intra-domain discrepancy. We have shown\nempirically that both of these modules complement each other to achieve\nsuperior performance. Experiments on three well-known domain adaptation\nbenchmark datasets namely DomainNet, Office-Home, and Office31 demonstrate the\neffectiveness of our approach. CLDA achieves state-of-the-art results on all\nthe above datasets.",
          "link": "http://arxiv.org/abs/2107.00085",
          "publishedOn": "2021-07-02T01:57:59.534Z",
          "wordCount": 591,
          "title": "CLDA: Contrastive Learning for Semi-Supervised Domain Adaptation. (arXiv:2107.00085v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00005",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sasmal_P/0/1/0/all/0/1\">Pradipta Sasmal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paul_A/0/1/0/all/0/1\">Avinash Paul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhuyan_M/0/1/0/all/0/1\">M.K. Bhuyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iwahori_Y/0/1/0/all/0/1\">Yuji Iwahori</a>",
          "description": "A deep learning-based monocular depth estimation (MDE) technique is proposed\nfor selection of most informative frames (key frames) of an endoscopic video.\nIn most of the cases, ground truth depth maps of polyps are not readily\navailable and that is why the transfer learning approach is adopted in our\nmethod. An endoscopic modalities generally capture thousands of frames. In this\nscenario, it is quite important to discard low-quality and clinically\nirrelevant frames of an endoscopic video while the most informative frames\nshould be retained for clinical diagnosis. In this view, a key-frame selection\nstrategy is proposed by utilizing the depth information of polyps. In our\nmethod, image moment, edge magnitude, and key-points are considered for\nadaptively selecting the key frames. One important application of our proposed\nmethod could be the 3D reconstruction of polyps with the help of extracted key\nframes. Also, polyps are localized with the help of extracted depth maps.",
          "link": "http://arxiv.org/abs/2107.00005",
          "publishedOn": "2021-07-02T01:57:59.518Z",
          "wordCount": 591,
          "title": "Extraction of Key-frames of Endoscopic Videos by using Depth Information. (arXiv:2107.00005v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00115",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Lakshminarayanan_V/0/1/0/all/0/1\">Vasudevan Lakshminarayanan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kherdfallah_H/0/1/0/all/0/1\">Hoda Kherdfallah</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sarkar_A/0/1/0/all/0/1\">Arya Sarkar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Balaji_J/0/1/0/all/0/1\">J. Jothi Balaji</a>",
          "description": "Diabetic Retinopathy (DR) is a leading cause of vision loss in the world,. In\nthe past few Diabetic Retinopathy (DR) is a leading cause of vision loss in the\nworld. In the past few years, Artificial Intelligence (AI) based approaches\nhave been used to detect and grade DR. Early detection enables appropriate\ntreatment and thus prevents vision loss, Both fundus and optical coherence\ntomography (OCT) images are used to image the retina. With deep\nlearning/machine learning apprroaches it is possible to extract features from\nthe images and detect the presence of DR. Multiple strategies are implemented\nto detect and grade the presence of DR using classification, segmentation, and\nhybrid techniques. This review covers the literature dealing with AI approaches\nto DR that have been published in the open literature over a five year span\n(2016-2021). In addition a comprehensive list of available DR datasets is\nreported. Both the PICO (P-patient, I-intervention, C-control O-outcome) and\nPreferred Reporting Items for Systematic Review and Meta-Analysis (PRISMA)2009\nsearch strategies were employed. We summarize a total of 114 published articles\nwhich conformed to the scope of the review. In addition a list of 43 major\ndatasets is presented.",
          "link": "http://arxiv.org/abs/2107.00115",
          "publishedOn": "2021-07-02T01:57:59.484Z",
          "wordCount": 660,
          "title": "Automated Detection and Diagnosis of Diabetic Retinopathy: A Comprehensive Survey. (arXiv:2107.00115v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00222",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tian_M/0/1/0/all/0/1\">Mi Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_Q/0/1/0/all/0/1\">Qiong Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1\">Hao Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_X/0/1/0/all/0/1\">Xiahua Xia</a>",
          "description": "Visual localization is one of the most important components for robotics and\nautonomous driving. Recently, inspiring results have been shown with CNN-based\nmethods which provide a direct formulation to end-to-end regress 6-DoF absolute\npose. Additional information like geometric or semantic constraints is\ngenerally introduced to improve performance. Especially, the latter can\naggregate high-level semantic information into localization task, but it\nusually requires enormous manual annotations. To this end, we propose a novel\nauxiliary learning strategy for camera localization by introducing\nscene-specific high-level semantics from self-supervised representation\nlearning task. Viewed as a powerful proxy task, image colorization task is\nchosen as complementary task that outputs pixel-wise color version of grayscale\nphotograph without extra annotations. In our work, feature representations from\ncolorization network are embedded into localization network by design to\nproduce discriminative features for pose regression. Meanwhile an attention\nmechanism is introduced for the benefit of localization performance. Extensive\nexperiments show that our model significantly improve localization accuracy\nover state-of-the-arts on both indoor and outdoor datasets.",
          "link": "http://arxiv.org/abs/2107.00222",
          "publishedOn": "2021-07-02T01:57:59.435Z",
          "wordCount": 606,
          "title": "Deep auxiliary learning for visual localization using colorization task. (arXiv:2107.00222v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00143",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yasuno_T/0/1/0/all/0/1\">Takato Yasuno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fujii_J/0/1/0/all/0/1\">Junichiro Fujii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fukami_S/0/1/0/all/0/1\">Sakura Fukami</a>",
          "description": "For steel product manufacturing in indoor factories, steel defect detection\nis important for quality control. For example, a steel sheet is extremely\ndelicate, and must be accurately inspected. However, to maintain the painted\nsteel parts of the infrastructure around a severe outdoor environment,\ncorrosion detection is critical for predictive maintenance. In this paper, we\npropose a general-purpose application for steel anomaly detection that consists\nof the following four components. The first, a learner, is a unit image\nclassification network to determine whether the region of interest or\nbackground has been recognised, after dividing the original large sized image\ninto 256 square unit images. The second, an extractor, is a discriminator\nfeature encoder based on a pre-trained steel generator with a patch generative\nadversarial network discriminator(GAN). The third, an anomaly detector, is a\none-class support vector machine(SVM) to predict the anomaly score using the\ndiscriminator feature. The fourth, an indicator, is an anomalous probability\nmap used to visually explain the anomalous features. Furthermore, we\ndemonstrated our method through the inspection of steel sheet defects with\n13,774 unit images using high-speed cameras, and painted steel corrosion with\n19,766 unit images based on an eye inspection of the photographs. Finally, we\nvisualise anomalous feature maps of steel using a strip and painted steel\ninspection dataset",
          "link": "http://arxiv.org/abs/2107.00143",
          "publishedOn": "2021-07-02T01:57:59.427Z",
          "wordCount": 671,
          "title": "One-class Steel Detector Using Patch GAN Discriminator for Visualising Anomalous Feature Map. (arXiv:2107.00143v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00346",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Peng_K/0/1/0/all/0/1\">Kunyu Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_J/0/1/0/all/0/1\">Juncong Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kailun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roitberg_A/0/1/0/all/0/1\">Alina Roitberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jiaming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bieder_F/0/1/0/all/0/1\">Frank Bieder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heidenreich_P/0/1/0/all/0/1\">Philipp Heidenreich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stiller_C/0/1/0/all/0/1\">Christoph Stiller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stiefelhagen_R/0/1/0/all/0/1\">Rainer Stiefelhagen</a>",
          "description": "At the heart of all automated driving systems is the ability to sense the\nsurroundings, e.g., through semantic segmentation of LiDAR sequences, which\nexperienced a remarkable progress due to the release of large datasets such as\nSemanticKITTI and nuScenes-LidarSeg. While most previous works focus on sparse\nsegmentation of the LiDAR input, dense output masks provide self-driving cars\nwith almost complete environment information. In this paper, we introduce MASS\n- a Multi-Attentional Semantic Segmentation model specifically built for dense\ntop-view understanding of the driving scenes. Our framework operates on pillar-\nand occupancy features and comprises three attention-based building blocks: (1)\na keypoint-driven graph attention, (2) an LSTM-based attention computed from a\nvector embedding of the spatial input, and (3) a pillar-based attention,\nresulting in a dense 360-degree segmentation mask. With extensive experiments\non both, SemanticKITTI and nuScenes-LidarSeg, we quantitatively demonstrate the\neffectiveness of our model, outperforming the state of the art by 19.0% on\nSemanticKITTI and reaching 32.7% in mIoU on nuScenes-LidarSeg, where MASS is\nthe first work addressing the dense segmentation task. Furthermore, our\nmulti-attention model is shown to be very effective for 3D object detection\nvalidated on the KITTI-3D dataset, showcasing its high generalizability to\nother tasks related to 3D vision.",
          "link": "http://arxiv.org/abs/2107.00346",
          "publishedOn": "2021-07-02T01:57:59.418Z",
          "wordCount": 674,
          "title": "MASS: Multi-Attentional Semantic Segmentation of LiDAR Data for Dense Top-View Understanding. (arXiv:2107.00346v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00422",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Becker_S/0/1/0/all/0/1\">Stefan Becker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hug_R/0/1/0/all/0/1\">Ronny Hug</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hubner_W/0/1/0/all/0/1\">Wolfgang H&#xfc;bner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arens_M/0/1/0/all/0/1\">Michael Arens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morris_B/0/1/0/all/0/1\">Brendan T. Morris</a>",
          "description": "Deep learning-based models, such as recurrent neural networks (RNNs), have\nbeen applied to various sequence learning tasks with great success. Following\nthis, these models are increasingly replacing classic approaches in object\ntracking applications for motion prediction. On the one hand, these models can\ncapture complex object dynamics with less modeling required, but on the other\nhand, they depend on a large amount of training data for parameter tuning.\nTowards this end, we present an approach for generating synthetic trajectory\ndata of unmanned-aerial-vehicles (UAVs) in image space. Since UAVs, or rather\nquadrotors are dynamical systems, they can not follow arbitrary trajectories.\nWith the prerequisite that UAV trajectories fulfill a smoothness criterion\ncorresponding to a minimal change of higher-order motion, methods for planning\naggressive quadrotors flights can be utilized to generate optimal trajectories\nthrough a sequence of 3D waypoints. By projecting these maneuver trajectories,\nwhich are suitable for controlling quadrotors, to image space, a versatile\ntrajectory data set is realized. To demonstrate the applicability of the\nsynthetic trajectory data, we show that an RNN-based prediction model solely\ntrained on the generated data can outperform classic reference models on a\nreal-world UAV tracking dataset. The evaluation is done on the publicly\navailable ANTI-UAV dataset.",
          "link": "http://arxiv.org/abs/2107.00422",
          "publishedOn": "2021-07-02T01:57:59.353Z",
          "wordCount": 644,
          "title": "Generating Synthetic Training Data for Deep Learning-Based UAV Trajectory Prediction. (arXiv:2107.00422v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15793",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yao_X/0/1/0/all/0/1\">Xingxu Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Sicheng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1\">Pengfei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jufeng Yang</a>",
          "description": "To reduce annotation labor associated with object detection, an increasing\nnumber of studies focus on transferring the learned knowledge from a labeled\nsource domain to another unlabeled target domain. However, existing methods\nassume that the labeled data are sampled from a single source domain, which\nignores a more generalized scenario, where labeled data are from multiple\nsource domains. For the more challenging task, we propose a unified Faster\nR-CNN based framework, termed Divide-and-Merge Spindle Network (DMSN), which\ncan simultaneously enhance domain invariance and preserve discriminative power.\nSpecifically, the framework contains multiple source subnets and a pseudo\ntarget subnet. First, we propose a hierarchical feature alignment strategy to\nconduct strong and weak alignments for low- and high-level features,\nrespectively, considering their different effects for object detection. Second,\nwe develop a novel pseudo subnet learning algorithm to approximate optimal\nparameters of pseudo target subset by weighted combination of parameters in\ndifferent source subnets. Finally, a consistency regularization for region\nproposal network is proposed to facilitate each subnet to learn more abstract\ninvariances. Extensive experiments on different adaptation scenarios\ndemonstrate the effectiveness of the proposed model.",
          "link": "http://arxiv.org/abs/2106.15793",
          "publishedOn": "2021-07-01T01:59:34.067Z",
          "wordCount": 623,
          "title": "Multi-Source Domain Adaptation for Object Detection. (arXiv:2106.15793v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15765",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhihong Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Deng_C/0/1/0/all/0/1\">Chao Deng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yuan_X/0/1/0/all/0/1\">Xin Yuan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Suo_J/0/1/0/all/0/1\">Jinli Suo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dai_Q/0/1/0/all/0/1\">Qionghai Dai</a>",
          "description": "High resolution images are widely used in our daily life, whereas high-speed\nvideo capture is challenging due to the low frame rate of cameras working at\nthe high resolution mode. Digging deeper, the main bottleneck lies in the low\nthroughput of existing imaging systems. Towards this end, snapshot compressive\nimaging (SCI) was proposed as a promising solution to improve the throughput of\nimaging systems by compressive sampling and computational reconstruction.\nDuring acquisition, multiple high-speed images are encoded and collapsed to a\nsingle measurement. After this, algorithms are employed to retrieve the video\nframes from the coded snapshot. Recently developed Plug-and-Play (PnP)\nalgorithms make it possible for SCI reconstruction in large-scale problems.\nHowever, the lack of high-resolution encoding systems still precludes SCI's\nwide application. In this paper, we build a novel hybrid coded aperture\nsnapshot compressive imaging (HCA-SCI) system by incorporating a dynamic liquid\ncrystal on silicon and a high-resolution lithography mask. We further implement\na PnP reconstruction algorithm with cascaded denoisers for high quality\nreconstruction. Based on the proposed HCA-SCI system and algorithm, we achieve\na 10-mega pixel SCI system to capture high-speed scenes, leading to a high\nthroughput of 4.6G voxels per second. Both simulation and real data experiments\nverify the feasibility and performance of our proposed HCA-SCI scheme.",
          "link": "http://arxiv.org/abs/2106.15765",
          "publishedOn": "2021-07-01T01:59:34.045Z",
          "wordCount": 666,
          "title": "10-mega pixel snapshot compressive imaging with a hybrid coded aperture. (arXiv:2106.15765v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16126",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Momin_R/0/1/0/all/0/1\">Rauf Momin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Momin_A/0/1/0/all/0/1\">Ali Shan Momin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rasheed_K/0/1/0/all/0/1\">Khalid Rasheed</a>",
          "description": "Facial expressions are the most universal forms of body language and\nautomatic facial expression recognition is one of the challenging tasks due to\ndifferent uncertainties. However, it has been an active field of research for\nmany years. Nevertheless, efficiency and performance are yet essential aspects\nfor building robust systems. We proposed two models, EmoXNet which is an\nensemble learning technique for learning convoluted facial representations, and\nEmoXNetLite which is a distillation technique that is useful for transferring\nthe knowledge from our ensemble model to an efficient deep neural network using\nlabel-smoothen soft labels for able to effectively detect expressions in\nreal-time. Both of the techniques performed quite well, where the ensemble\nmodel (EmoXNet) helped to achieve 85.07% test accuracy on FER2013 with FER+\nannotations and 86.25% test accuracy on RAF-DB. Moreover, the distilled model\n(EmoXNetLite) showed 82.07% test accuracy on FER2013 with FER+ annotations and\n81.78% test accuracy on RAF-DB.",
          "link": "http://arxiv.org/abs/2106.16126",
          "publishedOn": "2021-07-01T01:59:33.706Z",
          "wordCount": 603,
          "title": "Recognizing Facial Expressions in the Wild using Multi-Architectural Representations based Ensemble Learning with Distillation. (arXiv:2106.16126v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.05953",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Spurr_A/0/1/0/all/0/1\">Adrian Spurr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dahiya_A/0/1/0/all/0/1\">Aneesh Dahiya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xucong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hilliges_O/0/1/0/all/0/1\">Otmar Hilliges</a>",
          "description": "Acquiring accurate 3D annotated data for hand pose estimation is a\nnotoriously difficult problem. This typically requires complex multi-camera\nsetups and controlled conditions, which in turn creates a domain gap that is\nhard to bridge to fully unconstrained settings. Encouraged by the success of\ncontrastive learning on image classification tasks, we propose a new\nself-supervised method for the structured regression task of 3D hand pose\nestimation. Contrastive learning makes use of unlabeled data for the purpose of\nrepresentation learning via a loss formulation that encourages the learned\nfeature representations to be invariant under any image transformation. For 3D\nhand pose estimation, it too is desirable to have invariance to appearance\ntransformation such as color jitter. However, the task requires equivariance\nunder affine transformations, such as rotation and translation. To address this\nissue, we propose an equivariant contrastive objective and demonstrate its\neffectiveness in the context of 3D hand pose estimation. We experimentally\ninvestigate the impact of invariant and equivariant contrastive objectives and\nshow that learning equivariant features leads to better representations for the\ntask of 3D hand pose estimation. Furthermore, we show that a standard\nResNet-152, trained on additional unlabeled data, attains an improvement of\n$7.6\\%$ in PA-EPE on FreiHAND and thus achieves state-of-the-art performance\nwithout any task specific, specialized architectures.",
          "link": "http://arxiv.org/abs/2106.05953",
          "publishedOn": "2021-07-01T01:59:33.603Z",
          "wordCount": 671,
          "title": "Self-Supervised 3D Hand Pose Estimation from monocular RGB via Contrastive Learning. (arXiv:2106.05953v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15788",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">Di Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Siyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zang_Z/0/1/0/all/0/1\">Zelin Zang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kai Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_L/0/1/0/all/0/1\">Lei Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1\">Baigui Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Stan Z. Li</a>",
          "description": "Self-supervised contrastive learning has demonstrated great potential in\nlearning visual representations. Despite their success on various downstream\ntasks such as image classification and object detection, self-supervised\npre-training for fine-grained scenarios is not fully explored. In this paper,\nwe first point out that current contrastive methods are prone to memorizing\nbackground/foreground texture and therefore have a limitation in localizing the\nforeground object. Analysis suggests that learning to extract discriminative\ntexture information and localization are equally crucial for self-supervised\npre-training under fine-grained scenarios. Based on our findings, we introduce\nCross-view Saliency Alignment (CVSA), a contrastive learning framework that\nfirst crops and swaps saliency regions of images as a novel view generation and\nthen guides the model to localize on the foreground object via a cross-view\nalignment loss. Extensive experiments on four popular fine-grained\nclassification benchmarks show that CVSA significantly improves the learned\nrepresentation.",
          "link": "http://arxiv.org/abs/2106.15788",
          "publishedOn": "2021-07-01T01:59:33.560Z",
          "wordCount": 590,
          "title": "Align Yourself: Self-supervised Pre-training for Fine-grained Recognition via Saliency Alignment. (arXiv:2106.15788v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2006.09199",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rouditchenko_A/0/1/0/all/0/1\">Andrew Rouditchenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boggust_A/0/1/0/all/0/1\">Angie Boggust</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harwath_D/0/1/0/all/0/1\">David Harwath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Brian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joshi_D/0/1/0/all/0/1\">Dhiraj Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thomas_S/0/1/0/all/0/1\">Samuel Thomas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Audhkhasi_K/0/1/0/all/0/1\">Kartik Audhkhasi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuehne_H/0/1/0/all/0/1\">Hilde Kuehne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panda_R/0/1/0/all/0/1\">Rameswar Panda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feris_R/0/1/0/all/0/1\">Rogerio Feris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kingsbury_B/0/1/0/all/0/1\">Brian Kingsbury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Picheny_M/0/1/0/all/0/1\">Michael Picheny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torralba_A/0/1/0/all/0/1\">Antonio Torralba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glass_J/0/1/0/all/0/1\">James Glass</a>",
          "description": "Current methods for learning visually grounded language from videos often\nrely on text annotation, such as human generated captions or machine generated\nautomatic speech recognition (ASR) transcripts. In this work, we introduce the\nAudio-Video Language Network (AVLnet), a self-supervised network that learns a\nshared audio-visual embedding space directly from raw video inputs. To\ncircumvent the need for text annotation, we learn audio-visual representations\nfrom randomly segmented video clips and their raw audio waveforms. We train\nAVLnet on HowTo100M, a large corpus of publicly available instructional videos,\nand evaluate on image retrieval and video retrieval tasks, achieving\nstate-of-the-art performance. We perform analysis of AVLnet's learned\nrepresentations, showing our model utilizes speech and natural sounds to learn\naudio-visual concepts. Further, we propose a tri-modal model that jointly\nprocesses raw audio, video, and text captions from videos to learn a\nmulti-modal semantic embedding space useful for text-video retrieval. Our code,\ndata, and trained models will be released at avlnet.csail.mit.edu",
          "link": "http://arxiv.org/abs/2006.09199",
          "publishedOn": "2021-07-01T01:59:33.537Z",
          "wordCount": 675,
          "title": "AVLnet: Learning Audio-Visual Language Representations from Instructional Videos. (arXiv:2006.09199v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.00327",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nishi_S/0/1/0/all/0/1\">Shintaro Nishi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kadota_T/0/1/0/all/0/1\">Takeaki Kadota</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uchida_S/0/1/0/all/0/1\">Seiichi Uchida</a>",
          "description": "This paper analyzes a large number of logo images from the LLD-logo dataset,\nby recent deep learning-based techniques, to understand not only design trends\nof logo images and but also the correlation to their owner company. Especially,\nwe focus on three correlations between logo images and their text areas,\nbetween the text areas and the number of followers on Twitter, and between the\nlogo images and the number of followers. Various findings include the weak\npositive correlation between the text area ratio and the number of followers of\nthe company. In addition, deep regression and deep ranking methods can catch\ncorrelations between the logo images and the number of followers.",
          "link": "http://arxiv.org/abs/2104.00327",
          "publishedOn": "2021-07-01T01:59:33.518Z",
          "wordCount": 592,
          "title": "Famous Companies Use More Letters in Logo:A Large-Scale Analysis of Text Area in Logo. (arXiv:2104.00327v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12407",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Xu_J/0/1/0/all/0/1\">Junshen Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Turk_E/0/1/0/all/0/1\">Esra Abaci Turk</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Grant_P/0/1/0/all/0/1\">P. Ellen Grant</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Golland_P/0/1/0/all/0/1\">Polina Golland</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Adalsteinsson_E/0/1/0/all/0/1\">Elfar Adalsteinsson</a>",
          "description": "Fetal motion is unpredictable and rapid on the scale of conventional MR scan\ntimes. Therefore, dynamic fetal MRI, which aims at capturing fetal motion and\ndynamics of fetal function, is limited to fast imaging techniques with\ncompromises in image quality and resolution. Super-resolution for dynamic fetal\nMRI is still a challenge, especially when multi-oriented stacks of image slices\nfor oversampling are not available and high temporal resolution for recording\nthe dynamics of the fetus or placenta is desired. Further, fetal motion makes\nit difficult to acquire high-resolution images for supervised learning methods.\nTo address this problem, in this work, we propose STRESS (Spatio-Temporal\nResolution Enhancement with Simulated Scans), a self-supervised\nsuper-resolution framework for dynamic fetal MRI with interleaved slice\nacquisitions. Our proposed method simulates an interleaved slice acquisition\nalong the high-resolution axis on the originally acquired data to generate\npairs of low- and high-resolution images. Then, it trains a super-resolution\nnetwork by exploiting both spatial and temporal correlations in the MR time\nseries, which is used to enhance the resolution of the original data.\nEvaluations on both simulated and in utero data show that our proposed method\noutperforms other self-supervised super-resolution methods and improves image\nquality, which is beneficial to other downstream tasks and evaluations.",
          "link": "http://arxiv.org/abs/2106.12407",
          "publishedOn": "2021-07-01T01:59:33.459Z",
          "wordCount": 673,
          "title": "STRESS: Super-Resolution for Dynamic Fetal MRI using Self-Supervised Learning. (arXiv:2106.12407v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11396",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Huang_F/0/1/0/all/0/1\">Feihu Huang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Huang_H/0/1/0/all/0/1\">Heng Huang</a>",
          "description": "Bilevel optimization recently has attracted increased interest in machine\nlearning due to its many applications such as hyper-parameter optimization and\npolicy optimization. Although some methods recently have been proposed to solve\nthe bilevel problems, these methods do not consider using adaptive learning\nrates. To fill this gap, in the paper, we propose a class of fast and effective\nadaptive methods for solving bilevel optimization problems that the outer\nproblem is possibly nonconvex and the inner problem is strongly-convex.\nSpecifically, we propose a fast single-loop BiAdam algorithm based on the basic\nmomentum technique, which achieves a sample complexity of\n$\\tilde{O}(\\epsilon^{-4})$ for finding an $\\epsilon$-stationary point. At the\nsame time, we propose an accelerated version of BiAdam algorithm (VR-BiAdam) by\nusing variance reduced technique, which reaches the best known sample\ncomplexity of $\\tilde{O}(\\epsilon^{-3})$. To further reduce computation in\nestimating derivatives, we propose a fast single-loop stochastic approximated\nBiAdam algorithm (saBiAdam) by avoiding the Hessian inverse, which still\nachieves a sample complexity of $\\tilde{O}(\\epsilon^{-4})$ without large\nbatches. We further present an accelerated version of saBiAdam algorithm\n(VR-saBiAdam), which also reaches the best known sample complexity of\n$\\tilde{O}(\\epsilon^{-3})$. We apply the unified adaptive matrices to our\nmethods as the SUPER-ADAM \\citep{huang2021super}, which including many types of\nadaptive learning rates. Moreover, our framework can flexibly use the momentum\nand variance reduced techniques. In particular, we provide a useful convergence\nanalysis framework for both the constrained and unconstrained bilevel\noptimization. To the best of our knowledge, we first study the adaptive bilevel\noptimization methods with adaptive learning rates.",
          "link": "http://arxiv.org/abs/2106.11396",
          "publishedOn": "2021-07-01T01:59:33.376Z",
          "wordCount": 714,
          "title": "BiAdam: Fast Adaptive Bilevel Optimization Methods. (arXiv:2106.11396v2 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1911.07192",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zou_Q/0/1/0/all/0/1\">Qin Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_L/0/1/0/all/0/1\">Ling Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Long Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Song Wang</a>",
          "description": "Hash coding has been widely used in approximate nearest neighbor search for\nlarge-scale image retrieval. Given semantic annotations such as class labels\nand pairwise similarities of the training data, hashing methods can learn and\ngenerate effective and compact binary codes. While some newly introduced images\nmay contain undefined semantic labels, which we call unseen images, zeor-shot\nhashing techniques have been studied. However, existing zeor-shot hashing\nmethods focus on the retrieval of single-label images, and cannot handle\nmulti-label images. In this paper, for the first time, a novel transductive\nzero-shot hashing method is proposed for multi-label unseen image retrieval. In\norder to predict the labels of the unseen/target data, a visual-semantic bridge\nis built via instance-concept coherence ranking on the seen/source data. Then,\npairwise similarity loss and focal quantization loss are constructed for\ntraining a hashing model using both the seen/source and unseen/target data.\nExtensive evaluations on three popular multi-label datasets demonstrate that,\nthe proposed hashing method achieves significantly better results than the\ncompeting methods.",
          "link": "http://arxiv.org/abs/1911.07192",
          "publishedOn": "2021-07-01T01:59:33.183Z",
          "wordCount": 648,
          "title": "Transductive Zero-Shot Hashing for Multilabel Image Retrieval. (arXiv:1911.07192v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.02451",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zheheng Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_F/0/1/0/all/0/1\">Feixiang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_A/0/1/0/all/0/1\">Aite Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Ling Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xuelong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Huiyu Zhou</a>",
          "description": "Home-cage social behaviour analysis of mice is an invaluable tool to assess\ntherapeutic efficacy of neurodegenerative diseases. Despite tremendous efforts\nmade within the research community, single-camera video recordings are mainly\nused for such analysis. Because of the potential to create rich descriptions of\nmouse social behaviors, the use of multi-view video recordings for rodent\nobservations is increasingly receiving much attention. However, identifying\nsocial behaviours from various views is still challenging due to the lack of\ncorrespondence across data sources. To address this problem, we here propose a\nnovel multiview latent-attention and dynamic discriminative model that jointly\nlearns view-specific and view-shared sub-structures, where the former captures\nunique dynamics of each view whilst the latter encodes the interaction between\nthe views. Furthermore, a novel multi-view latent-attention variational\nautoencoder model is introduced in learning the acquired features, enabling us\nto learn discriminative features in each view. Experimental results on the\nstandard CRMI13 and our multi-view Parkinson's Disease Mouse Behaviour (PDMB)\ndatasets demonstrate that our model outperforms the other state of the arts\ntechnologies and effectively deals with the imbalanced data problem.",
          "link": "http://arxiv.org/abs/2011.02451",
          "publishedOn": "2021-07-01T01:59:33.160Z",
          "wordCount": 660,
          "title": "Muti-view Mouse Social Behaviour Recognition with Deep Graphical Model. (arXiv:2011.02451v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.00564",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Martini_M/0/1/0/all/0/1\">Mauro Martini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mazzia_V/0/1/0/all/0/1\">Vittorio Mazzia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khaliq_A/0/1/0/all/0/1\">Aleem Khaliq</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiaberge_M/0/1/0/all/0/1\">Marcello Chiaberge</a>",
          "description": "The increasing availability of large-scale remote sensing labeled data has\nprompted researchers to develop increasingly precise and accurate data-driven\nmodels for land cover and crop classification (LC&CC). Moreover, with the\nintroduction of self-attention and introspection mechanisms, deep learning\napproaches have shown promising results in processing long temporal sequences\nin the multi-spectral domain with a contained computational request.\nNevertheless, most practical applications cannot rely on labeled data, and in\nthe field, surveys are a time consuming solution that poses strict limitations\nto the number of collected samples. Moreover, atmospheric conditions and\nspecific geographical region characteristics constitute a relevant domain gap\nthat does not allow direct applicability of a trained model on the available\ndataset to the area of interest. In this paper, we investigate adversarial\ntraining of deep neural networks to bridge the domain discrepancy between\ndistinct geographical zones. In particular, we perform a thorough analysis of\ndomain adaptation applied to challenging multi-spectral, multi-temporal data,\naccurately highlighting the advantages of adapting state-of-the-art\nself-attention based models for LC&CC to different target zones where labeled\ndata are not available. Extensive experimentation demonstrated significant\nperformance and generalization gain in applying domain-adversarial training to\nsource and target regions with marked dissimilarities between the distribution\nof extracted features.",
          "link": "http://arxiv.org/abs/2104.00564",
          "publishedOn": "2021-07-01T01:59:33.154Z",
          "wordCount": 679,
          "title": "Domain-Adversarial Training of Self-Attention Based Networks for Land Cover Classification using Multi-temporal Sentinel-2 Satellite Imagery. (arXiv:2104.00564v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.16162",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vats_A/0/1/0/all/0/1\">Anuja Vats</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pedersen_M/0/1/0/all/0/1\">Marius Pedersen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohammed_A/0/1/0/all/0/1\">Ahmed Mohammed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hovde_O/0/1/0/all/0/1\">&#xd8;istein Hovde</a>",
          "description": "The progress in Computer Aided Diagnosis (CADx) of Wireless Capsule Endoscopy\n(WCE) is thwarted by the lack of data. The inadequacy in richly representative\nhealthy and abnormal conditions results in isolated analyses of pathologies,\nthat can not handle realistic multi-pathology scenarios. In this work, we\nexplore how to learn more for free, from limited data through solving a WCE\nmulticentric, multi-pathology classification problem. Learning more implies to\nlearning more than full supervision would allow with the same data. This is\ndone by combining self supervision with full supervision, under multi task\nlearning. Additionally, we draw inspiration from the Human Visual System (HVS)\nin designing self supervision tasks and investigate if seemingly ineffectual\nsignals within the data itself can be exploited to gain performance, if so,\nwhich signals would be better than others. Further, we present our analysis of\nthe high level features as a stepping stone towards more robust multi-pathology\nCADx in WCE.",
          "link": "http://arxiv.org/abs/2106.16162",
          "publishedOn": "2021-07-01T01:59:33.142Z",
          "wordCount": 612,
          "title": "Learning More for Free - A Multi Task Learning Approach for Improved Pathology Classification in Capsule Endoscopy. (arXiv:2106.16162v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2102.08871",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tautkute_I/0/1/0/all/0/1\">Ivona Tautkute</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trzcinski_T/0/1/0/all/0/1\">Tomasz Trzcinski</a>",
          "description": "This paper addresses the problem of media retrieval using a multimodal query\n(a query which combines visual input with additional semantic information in\nnatural language feedback). We propose a SynthTriplet GAN framework which\nresolves this task by expanding the multimodal query with a synthetically\ngenerated image that captures semantic information from both image and text\ninput. We introduce a novel triplet mining method that uses a synthetic image\nas an anchor to directly optimize for embedding distances of generated and\ntarget images. We demonstrate that apart from the added value of retrieval\nillustration with synthetic image with the focus on customization and user\nfeedback, the proposed method greatly surpasses other multimodal generation\nmethods and achieves state of the art results in the multimodal retrieval task.\nWe also show that in contrast to other retrieval methods, our method provides\nexplainable embeddings.",
          "link": "http://arxiv.org/abs/2102.08871",
          "publishedOn": "2021-07-01T01:59:33.136Z",
          "wordCount": 619,
          "title": "I Want This Product but Different : Multimodal Retrieval with Synthetic Query Expansion. (arXiv:2102.08871v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15796",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yunsong Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yuan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Hongzi Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Cheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hongyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Q/0/1/0/all/0/1\">Qinhong Jiang</a>",
          "description": "Monocular 3D object detection is an important task in autonomous driving. It\ncan be easily intractable where there exists ego-car pose change w.r.t. ground\nplane. This is common due to the slight fluctuation of road smoothness and\nslope. Due to the lack of insight in industrial application, existing methods\non open datasets neglect the camera pose information, which inevitably results\nin the detector being susceptible to camera extrinsic parameters. The\nperturbation of objects is very popular in most autonomous driving cases for\nindustrial products. To this end, we propose a novel method to capture camera\npose to formulate the detector free from extrinsic perturbation. Specifically,\nthe proposed framework predicts camera extrinsic parameters by detecting\nvanishing point and horizon change. A converter is designed to rectify\nperturbative features in the latent space. By doing so, our 3D detector works\nindependent of the extrinsic parameter variations and produces accurate results\nin realistic cases, e.g., potholed and uneven roads, where almost all existing\nmonocular detectors fail to handle. Experiments demonstrate our method yields\nthe best performance compared with the other state-of-the-arts by a large\nmargin on both KITTI 3D and nuScenes datasets.",
          "link": "http://arxiv.org/abs/2106.15796",
          "publishedOn": "2021-07-01T01:59:33.129Z",
          "wordCount": 631,
          "title": "Monocular 3D Object Detection: An Extrinsic Parameter Free Approach. (arXiv:2106.15796v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16093",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Scalbert_M/0/1/0/all/0/1\">Marin Scalbert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vakalopoulou_M/0/1/0/all/0/1\">Maria Vakalopoulou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Couzinie_Devy_F/0/1/0/all/0/1\">Florent Couzini&#xe9;-Devy</a>",
          "description": "Multi-Source Unsupervised Domain Adaptation (multi-source UDA) aims to learn\na model from several labeled source domains while performing well on a\ndifferent target domain where only unlabeled data are available at training\ntime. To align source and target features distributions, several recent works\nuse source and target explicit statistics matching such as features moments or\nclass centroids. Yet, these approaches do not guarantee class conditional\ndistributions alignment across domains. In this work, we propose a new\nframework called Contrastive Multi-Source Domain Adaptation (CMSDA) for\nmulti-source UDA that addresses this limitation. Discriminative features are\nlearned from interpolated source examples via cross entropy minimization and\nfrom target examples via consistency regularization and hard pseudo-labeling.\nSimultaneously, interpolated source examples are leveraged to align source\nclass conditional distributions through an interpolated version of the\nsupervised contrastive loss. This alignment leads to more general and\ntransferable features which further improve the generalization on the target\ndomain. Extensive experiments have been carried out on three standard\nmulti-source UDA datasets where our method reports state-of-the-art results.",
          "link": "http://arxiv.org/abs/2106.16093",
          "publishedOn": "2021-07-01T01:59:33.106Z",
          "wordCount": 609,
          "title": "Multi-Source domain adaptation via supervised contrastive learning and confident consistency regularization. (arXiv:2106.16093v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15944",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1\">Jingang Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Su_R/0/1/0/all/0/1\">Runmu Su</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ren_W/0/1/0/all/0/1\">Wenqi Ren</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fu_Q/0/1/0/all/0/1\">Qiang Fu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nie_Y/0/1/0/all/0/1\">Yunfeng Nie</a>",
          "description": "Hyperspectral imaging enables versatile applications due to its competence in\ncapturing abundant spatial and spectral information, which are crucial for\nidentifying substances. However, the devices for acquiring hyperspectral images\nare expensive and complicated. Therefore, many alternative spectral imaging\nmethods have been proposed by directly reconstructing the hyperspectral\ninformation from lower-cost, more available RGB images. We present a thorough\ninvestigation of these state-of-the-art spectral reconstruction methods from\nthe widespread RGB images. A systematic study and comparison of more than 25\nmethods has revealed that most of the data-driven deep learning methods are\nsuperior to prior-based methods in terms of reconstruction accuracy and quality\ndespite lower speeds. This comprehensive review can serve as a fruitful\nreference source for peer researchers, thus further inspiring future\ndevelopment directions in related domains.",
          "link": "http://arxiv.org/abs/2106.15944",
          "publishedOn": "2021-07-01T01:59:33.073Z",
          "wordCount": 580,
          "title": "Learnable Reconstruction Methods from RGB Images to Hyperspectral Imaging: A Survey. (arXiv:2106.15944v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16128",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shubao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Ke-Yue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_T/0/1/0/all/0/1\">Taiping Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheng_K/0/1/0/all/0/1\">Kekai Sheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1\">Shouhong Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tai_Y/0/1/0/all/0/1\">Ying Tai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jilin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yuan Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1\">Lizhuang Ma</a>",
          "description": "Face anti-spoofing approaches based on domain generalization (DG) have drawn\ngrowing attention due to their robustness for unseen scenarios. Previous\nmethods treat each sample from multiple domains indiscriminately during the\ntraining process, and endeavor to extract a common feature space to improve the\ngeneralization. However, due to complex and biased data distribution, directly\ntreating them equally will corrupt the generalization ability. To settle the\nissue, we propose a novel Dual Reweighting Domain Generalization (DRDG)\nframework which iteratively reweights the relative importance between samples\nto further improve the generalization. Concretely, Sample Reweighting Module is\nfirst proposed to identify samples with relatively large domain bias, and\nreduce their impact on the overall optimization. Afterwards, Feature\nReweighting Module is introduced to focus on these samples and extract more\ndomain-irrelevant features via a self-distilling mechanism. Combined with the\ndomain discriminator, the iteration of the two modules promotes the extraction\nof generalized features. Extensive experiments and visualizations are presented\nto demonstrate the effectiveness and interpretability of our method against the\nstate-of-the-art competitors.",
          "link": "http://arxiv.org/abs/2106.16128",
          "publishedOn": "2021-07-01T01:59:33.066Z",
          "wordCount": 620,
          "title": "Dual Reweighting Domain Generalization for Face Presentation Attack Detection. (arXiv:2106.16128v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16056",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Roy_W/0/1/0/all/0/1\">William Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kelly_G/0/1/0/all/0/1\">Glen Kelly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leer_R/0/1/0/all/0/1\">Robert Leer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ricardo_F/0/1/0/all/0/1\">Frederick Ricardo</a>",
          "description": "Generative Adversarial Networks (GANs) have been extremely successful in\nvarious application domains. Adversarial image synthesis has drawn increasing\nattention and made tremendous progress in recent years because of its wide\nrange of applications in many computer vision and image processing problems.\nAmong the many applications of GAN, image synthesis is the most well-studied\none, and research in this area has already demonstrated the great potential of\nusing GAN in image synthesis. In this paper, we provide a taxonomy of methods\nused in image synthesis, review different models for text-to-image synthesis\nand image-to-image translation, and discuss some evaluation metrics as well as\npossible future research directions in image synthesis with GAN.",
          "link": "http://arxiv.org/abs/2106.16056",
          "publishedOn": "2021-07-01T01:59:33.054Z",
          "wordCount": 556,
          "title": "A Survey on Adversarial Image Synthesis. (arXiv:2106.16056v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15753",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wu_L/0/1/0/all/0/1\">Liming Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Han_S/0/1/0/all/0/1\">Shuo Han</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_A/0/1/0/all/0/1\">Alain Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Salama_P/0/1/0/all/0/1\">Paul Salama</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dunn_K/0/1/0/all/0/1\">Kenneth W. Dunn</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Delp_E/0/1/0/all/0/1\">Edward J. Delp</a>",
          "description": "Robust and accurate nuclei centroid detection is important for the\nunderstanding of biological structures in fluorescence microscopy images.\nExisting automated nuclei localization methods face three main challenges: (1)\nMost of object detection methods work only on 2D images and are difficult to\nextend to 3D volumes; (2) Segmentation-based models can be used on 3D volumes\nbut it is computational expensive for large microscopy volumes and they have\ndifficulty distinguishing different instances of objects; (3) Hand annotated\nground truth is limited for 3D microscopy volumes. To address these issues, we\npresent a scalable approach for nuclei centroid detection of 3D microscopy\nvolumes. We describe the RCNN-SliceNet to detect 2D nuclei centroids for each\nslice of the volume from different directions and 3D agglomerative hierarchical\nclustering (AHC) is used to estimate the 3D centroids of nuclei in a volume.\nThe model was trained with the synthetic microscopy data generated using\nSpatially Constrained Cycle-Consistent Adversarial Networks (SpCycleGAN) and\ntested on different types of real 3D microscopy data. Extensive experimental\nresults demonstrate that our proposed method can accurately count and detect\nthe nuclei centroids in a 3D microscopy volume.",
          "link": "http://arxiv.org/abs/2106.15753",
          "publishedOn": "2021-07-01T01:59:33.047Z",
          "wordCount": 657,
          "title": "RCNN-SliceNet: A Slice and Cluster Approach for Nuclei Centroid Detection in Three-Dimensional Fluorescence Microscopy Images. (arXiv:2106.15753v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2104.12345",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rodriguez_Diaz_N/0/1/0/all/0/1\">Nuria Rodriguez-Diaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aspandi_D/0/1/0/all/0/1\">Decky Aspandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sukno_F/0/1/0/all/0/1\">Federico Sukno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Binefa_X/0/1/0/all/0/1\">Xavier Binefa</a>",
          "description": "Lie detection is considered a concern for everyone in their day to day life\ngiven its impact on human interactions. Thus, people normally pay attention to\nboth what their interlocutors are saying and also to their visual appearances,\nincluding faces, to try to find any signs that indicate whether the person is\ntelling the truth or not. While automatic lie detection may help us to\nunderstand this lying characteristics, current systems are still fairly\nlimited, partly due to lack of adequate datasets to evaluate their performance\nin realistic scenarios. In this work, we have collected an annotated dataset of\nfacial images, comprising both 2D and 3D information of several participants\nduring a card game that encourages players to lie. Using our collected dataset,\nWe evaluated several types of machine learning-based lie detectors in terms of\ntheir generalization, person-specific and cross-domain experiments. Our results\nshow that models based on deep learning achieve the best accuracy, reaching up\nto 57\\% for the generalization task and 63\\% when dealing with a single\nparticipant. Finally, we also highlight the limitation of the deep learning\nbased lie detector when dealing with cross-domain lie detection tasks.",
          "link": "http://arxiv.org/abs/2104.12345",
          "publishedOn": "2021-07-01T01:59:33.040Z",
          "wordCount": 661,
          "title": "Machine Learning-based Lie Detector applied to a Novel Annotated Game Dataset. (arXiv:2104.12345v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.16118",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kollar_T/0/1/0/all/0/1\">Thomas Kollar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laskey_M/0/1/0/all/0/1\">Michael Laskey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stone_K/0/1/0/all/0/1\">Kevin Stone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thananjeyan_B/0/1/0/all/0/1\">Brijen Thananjeyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tjersland_M/0/1/0/all/0/1\">Mark Tjersland</a>",
          "description": "Robot manipulation of unknown objects in unstructured environments is a\nchallenging problem due to the variety of shapes, materials, arrangements and\nlighting conditions. Even with large-scale real-world data collection, robust\nperception and manipulation of transparent and reflective objects across\nvarious lighting conditions remain challenging. To address these challenges we\npropose an approach to performing sim-to-real transfer of robotic perception.\nThe underlying model, SimNet, is trained as a single multi-headed neural\nnetwork using simulated stereo data as input and simulated object segmentation\nmasks, 3D oriented bounding boxes (OBBs), object keypoints, and disparity as\noutput. A key component of SimNet is the incorporation of a learned stereo\nsub-network that predicts disparity. SimNet is evaluated on 2D car detection,\nunknown object detection, and deformable object keypoint detection and\nsignificantly outperforms a baseline that uses a structured light RGB-D sensor.\nBy inferring grasp positions using the OBB and keypoint predictions, SimNet can\nbe used to perform end-to-end manipulation of unknown objects in both easy and\nhard scenarios using our fleet of Toyota HSR robots in four home environments.\nIn unknown object grasping experiments, the predictions from the baseline RGB-D\nnetwork and SimNet enable successful grasps of most of the easy objects.\nHowever, the RGB-D baseline only grasps 35% of the hard (e.g., transparent)\nobjects, while SimNet grasps 95%, suggesting that SimNet can enable robust\nmanipulation of unknown objects, including transparent objects, in unknown\nenvironments.",
          "link": "http://arxiv.org/abs/2106.16118",
          "publishedOn": "2021-07-01T01:59:32.988Z",
          "wordCount": 683,
          "title": "SimNet: Enabling Robust Unknown Object Manipulation from Pure Synthetic Data via Stereo. (arXiv:2106.16118v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15941",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yehui Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1\">Kai Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_A/0/1/0/all/0/1\">An Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1\">Yiping Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunhe Wang</a>",
          "description": "Transformer models have achieved great progress on computer vision tasks\nrecently. The rapid development of vision transformers is mainly contributed by\ntheir high representation ability for extracting informative features from\ninput images. However, the mainstream transformer models are designed with deep\narchitectures, and the feature diversity will be continuously reduced as the\ndepth increases, i.e., feature collapse. In this paper, we theoretically\nanalyze the feature collapse phenomenon and study the relationship between\nshortcuts and feature diversity in these transformer models. Then, we present\nan augmented shortcut scheme, which inserts additional paths with learnable\nparameters in parallel on the original shortcuts. To save the computational\ncosts, we further explore an efficient approach that uses the block-circulant\nprojection to implement augmented shortcuts. Extensive experiments conducted on\nbenchmark datasets demonstrate the effectiveness of the proposed method, which\nbrings about 1% accuracy increase of the state-of-the-art visual transformers\nwithout obviously increasing their parameters and FLOPs.",
          "link": "http://arxiv.org/abs/2106.15941",
          "publishedOn": "2021-07-01T01:59:32.909Z",
          "wordCount": 591,
          "title": "Augmented Shortcuts for Vision Transformers. (arXiv:2106.15941v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16060",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sansone_E/0/1/0/all/0/1\">Emanuele Sansone</a>",
          "description": "This work considers the problem of learning structured representations from\nraw images using self-supervised learning. We propose a principled framework\nbased on a mutual information objective, which integrates self-supervised and\nstructure learning. Furthermore, we devise a post-hoc procedure to interpret\nthe meaning of the learnt representations. Preliminary experiments on CIFAR-10\nshow that the proposed framework achieves higher generalization performance in\ndownstream classification tasks and provides more interpretable representations\ncompared to the ones learnt through traditional self-supervised learning.",
          "link": "http://arxiv.org/abs/2106.16060",
          "publishedOn": "2021-07-01T01:59:32.900Z",
          "wordCount": 503,
          "title": "Leveraging Hidden Structure in Self-Supervised Learning. (arXiv:2106.16060v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15754",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1\">Lei Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1\">Dong Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Shaofu Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_X/0/1/0/all/0/1\">Xiaojie Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuebin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Hao Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bruzzone_L/0/1/0/all/0/1\">Lorenzo Bruzzone</a>",
          "description": "Long-range context information is crucial for the semantic segmentation of\nHigh-Resolution (HR) Remote Sensing Images (RSIs). The image cropping\noperations, commonly used for training neural networks, limit the perception of\nlong-range context information in large RSIs. To break this limitation, we\npropose a Wider-Context Network (WiCNet) for the semantic segmentation of HR\nRSIs. In the WiCNet, apart from a conventional feature extraction network to\naggregate the local information, an extra context branch is designed to\nexplicitly model the context information in a larger image area. The\ninformation between the two branches is communicated through a Context\nTransformer, which is a novel design derived from the Vision Transformer to\nmodel the long-range context correlations. Ablation studies and comparative\nexperiments conducted on several benchmark datasets prove the effectiveness of\nthe proposed method. Additionally, we present a new Beijing Land-Use (BLU)\ndataset. This is a large-scale HR satellite dataset provided with high-quality\nand fine-grained reference labels, which we hope will boost future studies in\nthis field.",
          "link": "http://arxiv.org/abs/2106.15754",
          "publishedOn": "2021-07-01T01:59:32.830Z",
          "wordCount": 621,
          "title": "Looking Outside the Window: Wider-Context Transformer for the Semantic Segmentation of High-Resolution Remote Sensing Images. (arXiv:2106.15754v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15989",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Maruyama_M/0/1/0/all/0/1\">Mizuki Maruyama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghose_S/0/1/0/all/0/1\">Shuvozit Ghose</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Inoue_K/0/1/0/all/0/1\">Katsufumi Inoue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_P/0/1/0/all/0/1\">Partha Pratim Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iwamura_M/0/1/0/all/0/1\">Masakazu Iwamura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoshioka_M/0/1/0/all/0/1\">Michifumi Yoshioka</a>",
          "description": "In recent years, Word-level Sign Language Recognition (WSLR) research has\ngained popularity in the computer vision community, and thus various approaches\nhave been proposed. Among these approaches, the method using I3D network\nachieves the highest recognition accuracy on large public datasets for WSLR.\nHowever, the method with I3D only utilizes appearance information of the upper\nbody of the signers to recognize sign language words. On the other hand, in\nWSLR, the information of local regions, such as the hand shape and facial\nexpression, and the positional relationship among the body and both hands are\nimportant. Thus in this work, we utilized local region images of both hands and\nface, along with skeletal information to capture local information and the\npositions of both hands relative to the body, respectively. In other words, we\npropose a novel multi-stream WSLR framework, in which a stream with local\nregion images and a stream with skeletal information are introduced by\nextending I3D network to improve the recognition accuracy of WSLR. From the\nexperimental results on WLASL dataset, it is evident that the proposed method\nhas achieved about 15% improvement in the Top-1 accuracy than the existing\nconventional methods.",
          "link": "http://arxiv.org/abs/2106.15989",
          "publishedOn": "2021-07-01T01:59:32.811Z",
          "wordCount": 645,
          "title": "Word-level Sign Language Recognition with Multi-stream Neural Networks Focusing on Local Regions. (arXiv:2106.15989v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.14734",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mao_M/0/1/0/all/0/1\">Mingyuan Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Renrui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Honghui Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1\">Peng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1\">Teli Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yan Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_E/0/1/0/all/0/1\">Errui Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Baochang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Shumin Han</a>",
          "description": "Transformers with remarkable global representation capacities achieve\ncompetitive results for visual tasks, but fail to consider high-level local\npattern information in input images. In this paper, we present a generic\nDual-stream Network (DS-Net) to fully explore the representation capacity of\nlocal and global pattern features for image classification. Our DS-Net can\nsimultaneously calculate fine-grained and integrated features and efficiently\nfuse them. Specifically, we propose an Intra-scale Propagation module to\nprocess two different resolutions in each block and an Inter-Scale Alignment\nmodule to perform information interaction across features at dual scales.\nBesides, we also design a Dual-stream FPN (DS-FPN) to further enhance\ncontextual information for downstream dense predictions. Without bells and\nwhistles, the propsed DS-Net outperforms Deit-Small by 2.4% in terms of top-1\naccuracy on ImageNet-1k and achieves state-of-the-art performance over other\nVision Transformers and ResNets. For object detection and instance\nsegmentation, DS-Net-Small respectively outperforms ResNet-50 by 6.4% and 5.5 %\nin terms of mAP on MSCOCO 2017, and surpasses the previous state-of-the-art\nscheme, which significantly demonstrates its potential to be a general backbone\nin vision tasks. The code will be released soon.",
          "link": "http://arxiv.org/abs/2105.14734",
          "publishedOn": "2021-07-01T01:59:32.694Z",
          "wordCount": 639,
          "title": "Dual-stream Network for Visual Recognition. (arXiv:2105.14734v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10479",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tan_Y/0/1/0/all/0/1\">Yang Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shao-Lun Huang</a>",
          "description": "Transferability estimation is an essential problem in transfer learning to\npredict how good the performance is when transferring a source model (or source\ntask) to a target task. Recent analytical transferability metrics have been\nwidely used for source model selection and multi-task learning. A major\nchallenge is how to make transfereability estimation robust under the\ncross-domain cross-task settings. The recently proposed OTCE score solves this\nproblem by considering both domain and task differences, with the help of\ntransfer experiences on auxiliary tasks, which causes an efficiency overhead.\nIn this work, we propose a practical transferability metric called JC-NCE score\nthat dramatically improves the robustness of the task difference estimation in\nOTCE, thus removing the need for auxiliary tasks. Specifically, we build the\njoint correspondences between source and target data via solving an optimal\ntransport problem with a ground cost considering both the sample distance and\nlabel distance, and then compute the transferability score as the negative\nconditional entropy of the matched labels. Extensive validations under the\nintra-dataset and inter-dataset transfer settings demonstrate that our JC-NCE\nscore outperforms the auxiliary-task free version of OTCE for 7% and 12%,\nrespectively, and is also more robust than other existing transferability\nmetrics on average.",
          "link": "http://arxiv.org/abs/2106.10479",
          "publishedOn": "2021-07-01T01:59:32.668Z",
          "wordCount": 660,
          "title": "Practical Transferability Estimation for Image Classification Tasks. (arXiv:2106.10479v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.16209",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schmarje_L/0/1/0/all/0/1\">Lars Schmarje</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santarossa_M/0/1/0/all/0/1\">Monty Santarossa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schroder_S/0/1/0/all/0/1\">Simon-Martin Schr&#xf6;der</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zelenka_C/0/1/0/all/0/1\">Claudius Zelenka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiko_R/0/1/0/all/0/1\">Rainer Kiko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stracke_J/0/1/0/all/0/1\">Jenny Stracke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Volkmann_N/0/1/0/all/0/1\">Nina Volkmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koch_R/0/1/0/all/0/1\">Reinhard Koch</a>",
          "description": "Semi-Supervised Learning (SSL) can decrease the amount of required labeled\nimage data and thus the cost for deep learning. Most SSL methods only consider\na clear distinction between classes but in many real-world datasets, this clear\ndistinction is not given due to intra- or interobserver variability. This\nvariability can lead to different annotations per image. Thus many images have\nambiguous annotations and their label needs to be considered \"fuzzy\". This\nfuzziness of labels must be addressed as it will limit the performance of\nSemi-Supervised Learning (SSL) and deep learning in general. We propose\nSemi-Supervised Classification & Clustering (S2C2) which can extend many deep\nSSL algorithms. S2C2 can estimate the fuzziness of a label and applies SSL as a\nclassification to certainly labeled data while creating distinct clusters for\nimages with similar but fuzzy labels. We show that S2C2 results in median 7.4%\nbetter F1-score for classifications and 5.4% lower inner distance of clusters\nacross multiple SSL algorithms and datasets while being more interpretable due\nto the fuzziness estimation of our method. Overall, a combination of\nSemi-Supervised Learning with our method S2C2 leads to better handling of the\nfuzziness of labels and thus real-world datasets.",
          "link": "http://arxiv.org/abs/2106.16209",
          "publishedOn": "2021-07-01T01:59:32.652Z",
          "wordCount": 644,
          "title": "S2C2 - An orthogonal method for Semi-Supervised Learning on fuzzy labels. (arXiv:2106.16209v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16139",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yilmaz_A/0/1/0/all/0/1\">Abdurrahim Yilmaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varol_R/0/1/0/all/0/1\">Rahmetullah Varol</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goktay_F/0/1/0/all/0/1\">Fatih Goktay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gencoglan_G/0/1/0/all/0/1\">Gulsum Gencoglan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demircali_A/0/1/0/all/0/1\">Ali Anil Demircali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dilsizoglu_B/0/1/0/all/0/1\">Berk Dilsizoglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uvet_H/0/1/0/all/0/1\">Huseyin Uvet</a>",
          "description": "Clinical dermatology, still relies heavily on manual introspection of fungi\nwithin a Potassium Hydroxide (KOH) solution using a brightfield microscope.\nHowever, this method takes a long time, is based on the experience of the\nclinician, and has a low accuracy. With the increase of neural network\napplications in the field of clinical microscopy it is now possible to automate\nsuch manual processes increasing both efficiency and accuracy. This study\npresents a deep neural network structure that enables the rapid solutions for\nthese problems and can perform automatic fungi detection in grayscale images\nwithout colorants. Microscopic images of 81 fungi and 235 ceratine were\ncollected. Then, smaller patches were extracted containing 2062 fungi and 2142\nceratine. In order to detect fungus and ceratine, two models were created one\nof which was a custom neural network and the other was based on the VGG16\narchitecture. The developed custom model had 99.84% accuracy, and an area under\nthe curve (AUC) value of 1.00, while the VGG16 model had 98.89% accuracy and an\nAUC value of 0.99. However, average accuracy and AUC value of clinicians is\n72.8% and 0.87 respectively. This deep learning model allows the development of\nan automated system that can detect fungi within microscopic images.",
          "link": "http://arxiv.org/abs/2106.16139",
          "publishedOn": "2021-07-01T01:59:32.643Z",
          "wordCount": 649,
          "title": "Automated Onychomycosis Detection Using Deep Neural Networks. (arXiv:2106.16139v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16136",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuechen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jiajun Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wengang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Houqiang Li</a>",
          "description": "Temporal language grounding (TLG) is a fundamental and challenging problem\nfor vision and language understanding. Existing methods mainly focus on fully\nsupervised setting with temporal boundary labels for training, which, however,\nsuffers expensive cost of annotation. In this work, we are dedicated to weakly\nsupervised TLG, where multiple description sentences are given to an untrimmed\nvideo without temporal boundary labels. In this task, it is critical to learn a\nstrong cross-modal semantic alignment between sentence semantics and visual\ncontent. To this end, we introduce a novel weakly supervised temporal adjacent\nnetwork (WSTAN) for temporal language grounding. Specifically, WSTAN learns\ncross-modal semantic alignment by exploiting temporal adjacent network in a\nmultiple instance learning (MIL) paradigm, with a whole description paragraph\nas input. Moreover, we integrate a complementary branch into the framework,\nwhich explicitly refines the predictions with pseudo supervision from the MIL\nstage. An additional self-discriminating loss is devised on both the MIL branch\nand the complementary branch, aiming to enhance semantic discrimination by\nself-supervising. Extensive experiments are conducted on three widely used\nbenchmark datasets, \\emph{i.e.}, ActivityNet-Captions, Charades-STA, and\nDiDeMo, and the results demonstrate the effectiveness of our approach.",
          "link": "http://arxiv.org/abs/2106.16136",
          "publishedOn": "2021-07-01T01:59:32.625Z",
          "wordCount": 632,
          "title": "Weakly Supervised Temporal Adjacent Network for Language Grounding. (arXiv:2106.16136v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15827",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hanbin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_X/0/1/0/all/0/1\">Xin Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_S/0/1/0/all/0/1\">Shihao Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zibo Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xi Li</a>",
          "description": "With the rapid development of social media, tremendous videos with new\nclasses are generated daily, which raise an urgent demand for video\nclassification methods that can continuously update new classes while\nmaintaining the knowledge of old videos with limited storage and computing\nresources. In this paper, we summarize this task as \\textit{Class-Incremental\nVideo Classification (CIVC)} and propose a novel framework to address it. As a\nsubarea of incremental learning tasks, the challenge of \\textit{catastrophic\nforgetting} is unavoidable in CIVC. To better alleviate it, we utilize some\ncharacteristics of videos. First, we decompose the spatio-temporal knowledge\nbefore distillation rather than treating it as a whole in the knowledge\ntransfer process; trajectory is also used to refine the decomposition. Second,\nwe propose a dual granularity exemplar selection method to select and store\nrepresentative video instances of old classes and key-frames inside videos\nunder a tight storage budget. We benchmark our method and previous SOTA\nclass-incremental learning methods on Something-Something V2 and Kinetics\ndatasets, and our method outperforms previous methods significantly.",
          "link": "http://arxiv.org/abs/2106.15827",
          "publishedOn": "2021-07-01T01:59:32.619Z",
          "wordCount": 602,
          "title": "When Video Classification Meets Incremental Classes. (arXiv:2106.15827v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2012.10133",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zuo_X/0/1/0/all/0/1\">Xingxing Zuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Merrill_N/0/1/0/all/0/1\">Nathaniel Merrill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pollefeys_M/0/1/0/all/0/1\">Marc Pollefeys</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1\">Guoquan Huang</a>",
          "description": "In this work, we present a lightweight, tightly-coupled deep depth network\nand visual-inertial odometry (VIO) system, which can provide accurate state\nestimates and dense depth maps of the immediate surroundings. Leveraging the\nproposed lightweight Conditional Variational Autoencoder (CVAE) for depth\ninference and encoding, we provide the network with previously marginalized\nsparse features from VIO to increase the accuracy of initial depth prediction\nand generalization capability. The compact encoded depth maps are then updated\njointly with navigation states in a sliding window estimator in order to\nprovide the dense local scene geometry. We additionally propose a novel method\nto obtain the CVAE's Jacobian which is shown to be more than an order of\nmagnitude faster than previous works, and we additionally leverage\nFirst-Estimate Jacobian (FEJ) to avoid recalculation. As opposed to previous\nworks relying on completely dense residuals, we propose to only provide sparse\nmeasurements to update the depth code and show through careful experimentation\nthat our choice of sparse measurements and FEJs can still significantly improve\nthe estimated depth maps. Our full system also exhibits state-of-the-art pose\nestimation accuracy, and we show that it can run in real-time with\nsingle-thread execution while utilizing GPU acceleration only for the network\nand code Jacobian.",
          "link": "http://arxiv.org/abs/2012.10133",
          "publishedOn": "2021-07-01T01:59:32.614Z",
          "wordCount": 675,
          "title": "CodeVIO: Visual-Inertial Odometry with Learned Optimizable Dense Depth. (arXiv:2012.10133v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.03412",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pintea_S/0/1/0/all/0/1\">Silvia L.Pintea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tomen_N/0/1/0/all/0/1\">Nergis Tomen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goes_S/0/1/0/all/0/1\">Stanley F. Goes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loog_M/0/1/0/all/0/1\">Marco Loog</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gemert_J/0/1/0/all/0/1\">Jan C. van Gemert</a>",
          "description": "Resolution in deep convolutional neural networks (CNNs) is typically bounded\nby the receptive field size through filter sizes, and subsampling layers or\nstrided convolutions on feature maps. The optimal resolution may vary\nsignificantly depending on the dataset. Modern CNNs hard-code their resolution\nhyper-parameters in the network architecture which makes tuning such\nhyper-parameters cumbersome. We propose to do away with hard-coded resolution\nhyper-parameters and aim to learn the appropriate resolution from data. We use\nscale-space theory to obtain a self-similar parametrization of filters and make\nuse of the N-Jet: a truncated Taylor series to approximate a filter by a\nlearned combination of Gaussian derivative filters. The parameter sigma of the\nGaussian basis controls both the amount of detail the filter encodes and the\nspatial extent of the filter. Since sigma is a continuous parameter, we can\noptimize it with respect to the loss. The proposed N-Jet layer achieves\ncomparable performance when used in state-of-the art architectures, while\nlearning the correct resolution in each layer automatically. We evaluate our\nN-Jet layer on both classification and segmentation, and we show that learning\nsigma is especially beneficial for inputs at multiple sizes.",
          "link": "http://arxiv.org/abs/2106.03412",
          "publishedOn": "2021-07-01T01:59:32.606Z",
          "wordCount": 647,
          "title": "Resolution learning in deep convolutional networks using scale-space theory. (arXiv:2106.03412v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.16028",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1\">Zhihang Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Ye Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yinqiang Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_B/0/1/0/all/0/1\">Bo Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sato_I/0/1/0/all/0/1\">Imari Sato</a>",
          "description": "Real-time video deblurring still remains a challenging task due to the\ncomplexity of spatially and temporally varying blur itself and the requirement\nof low computational cost. To improve the network efficiency, we adopt residual\ndense blocks into RNN cells, so as to efficiently extract the spatial features\nof the current frame. Furthermore, a global spatio-temporal attention module is\nproposed to fuse the effective hierarchical features from past and future\nframes to help better deblur the current frame. Another issue needs to be\naddressed urgently is the lack of a real-world benchmark dataset. Thus, we\ncontribute a novel dataset (BSD) to the community, by collecting paired\nblurry/sharp video clips using a co-axis beam splitter acquisition system.\nExperimental results show that the proposed method (ESTRNN) can achieve better\ndeblurring performance both quantitatively and qualitatively with less\ncomputational cost against state-of-the-art video deblurring methods. In\naddition, cross-validation experiments between datasets illustrate the high\ngenerality of BSD over the synthetic datasets. The code and dataset are\nreleased at https://github.com/zzh-tech/ESTRNN.",
          "link": "http://arxiv.org/abs/2106.16028",
          "publishedOn": "2021-07-01T01:59:32.594Z",
          "wordCount": 615,
          "title": "Efficient Spatio-Temporal Recurrent Neural Network for Video Deblurring. (arXiv:2106.16028v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16129",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Stoian_M/0/1/0/all/0/1\">Mihaela C&#x103;t&#x103;lina Stoian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cavallari_T/0/1/0/all/0/1\">Tommaso Cavallari</a>",
          "description": "Many man-made objects are characterised by a shape that is symmetric along\none or more planar directions. Estimating the location and orientation of such\nsymmetry planes can aid many tasks such as estimating the overall orientation\nof an object of interest or performing shape completion, where a partial scan\nof an object is reflected across the estimated symmetry plane in order to\nobtain a more detailed shape. Many methods processing 3D data rely on expensive\n3D convolutions. In this paper we present an alternative novel encoding that\ninstead slices the data along the height dimension and passes it sequentially\nto a 2D convolutional recurrent regression scheme. The method also comprises a\ndifferentiable least squares step, allowing for end-to-end accurate and fast\nprocessing of both full and partial scans of symmetric objects. We use this\napproach to efficiently handle 3D inputs to design a method to estimate planar\nreflective symmetries. We show that our approach has an accuracy comparable to\nstate-of-the-art techniques on the task of planar reflective symmetry\nestimation on full synthetic objects. Additionally, we show that it can be\ndeployed on partial scans of objects in a real-world pipeline to improve the\noutputs of a 3D object detector.",
          "link": "http://arxiv.org/abs/2106.16129",
          "publishedOn": "2021-07-01T01:59:32.570Z",
          "wordCount": 644,
          "title": "Recurrently Estimating Reflective Symmetry Planes from Partial Pointclouds. (arXiv:2106.16129v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15716",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Scott_C/0/1/0/all/0/1\">Cory Braker Scott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mjolsness_E/0/1/0/all/0/1\">Eric Mjolsness</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oyen_D/0/1/0/all/0/1\">Diane Oyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kodera_C/0/1/0/all/0/1\">Chie Kodera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouchez_D/0/1/0/all/0/1\">David Bouchez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uyttewaal_M/0/1/0/all/0/1\">Magalie Uyttewaal</a>",
          "description": "We present a method for learning \"spectrally descriptive\" edge weights for\ngraphs. We generalize a previously known distance measure on graphs (Graph\nDiffusion Distance), thereby allowing it to be tuned to minimize an arbitrary\nloss function. Because all steps involved in calculating this modified GDD are\ndifferentiable, we demonstrate that it is possible for a small neural network\nmodel to learn edge weights which minimize loss. GDD alone does not effectively\ndiscriminate between graphs constructed from shoot apical meristem images of\nwild-type vs. mutant \\emph{Arabidopsis thaliana} specimens. However, training\nedge weights and kernel parameters with contrastive loss produces a learned\ndistance metric with large margins between these graph categories. We\ndemonstrate this by showing improved performance of a simple\nk-nearest-neighbors classifier on the learned distance matrix. We also\ndemonstrate a further application of this method to biological image analysis:\nonce trained, we use our model to compute the distance between the biological\ngraphs and a set of graphs output by a cell division simulator. This allows us\nto identify simulation parameter regimes which are similar to each class of\ngraph in our original dataset.",
          "link": "http://arxiv.org/abs/2106.15716",
          "publishedOn": "2021-07-01T01:59:32.560Z",
          "wordCount": 638,
          "title": "Diff2Dist: Learning Spectrally Distinct Edge Functions, with Applications to Cell Morphology Analysis. (arXiv:2106.15716v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15693",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pereira_T/0/1/0/all/0/1\">Tiago de C. G. Pereira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Campos_T/0/1/0/all/0/1\">Teofilo E. de Campos</a>",
          "description": "In the world where big data reigns and there is plenty of hardware prepared\nto gather a huge amount of non structured data, data acquisition is no longer a\nproblem. Surveillance cameras are ubiquitous and they capture huge numbers of\npeople walking across different scenes. However, extracting value from this\ndata is challenging, specially for tasks that involve human images, such as\nface recognition and person re-identification. Annotation of this kind of data\nis a challenging and expensive task. In this work we propose a domain\nadaptation workflow to allow CNNs that were trained in one domain to be applied\nto another domain without the need for new annotation of the target data. Our\nmethod uses AlignedReID++ as the baseline, trained using a Triplet loss with\nbatch hard. Domain adaptation is done by using pseudo-labels generated using an\nunsupervised learning strategy. Our results show that domain adaptation\ntechniques really improve the performance of the CNN when applied in the target\ndomain.",
          "link": "http://arxiv.org/abs/2106.15693",
          "publishedOn": "2021-07-01T01:59:32.530Z",
          "wordCount": 633,
          "title": "Domain adaptation for person re-identification on new unlabeled data using AlignedReID++. (arXiv:2106.15693v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15991",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zernetsch_S/0/1/0/all/0/1\">Stefan Zernetsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trupp_O/0/1/0/all/0/1\">Oliver Trupp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kress_V/0/1/0/all/0/1\">Viktor Kress</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doll_K/0/1/0/all/0/1\">Konrad Doll</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sick_B/0/1/0/all/0/1\">Bernhard Sick</a>",
          "description": "This article presents a novel approach to incorporate visual cues from\nvideo-data from a wide-angle stereo camera system mounted at an urban\nintersection into the forecast of cyclist trajectories. We extract features\nfrom image and optical flow (OF) sequences using 3D convolutional neural\nnetworks (3D-ConvNet) and combine them with features extracted from the\ncyclist's past trajectory to forecast future cyclist positions. By the use of\nadditional information, we are able to improve positional accuracy by about 7.5\n% for our test dataset and by up to 22 % for specific motion types compared to\na method solely based on past trajectories. Furthermore, we compare the use of\nimage sequences to the use of OF sequences as additional information, showing\nthat OF alone leads to significant improvements in positional accuracy. By\ntraining and testing our methods using a real-world dataset recorded at a\nheavily frequented public intersection and evaluating the methods' runtimes, we\ndemonstrate the applicability in real traffic scenarios. Our code and parts of\nour dataset are made publicly available.",
          "link": "http://arxiv.org/abs/2106.15991",
          "publishedOn": "2021-07-01T01:59:32.512Z",
          "wordCount": 613,
          "title": "Cyclist Trajectory Forecasts by Incorporation of Multi-View Video Information. (arXiv:2106.15991v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15648",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Georgakis_G/0/1/0/all/0/1\">Georgios Georgakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bucher_B/0/1/0/all/0/1\">Bernadette Bucher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmeckpeper_K/0/1/0/all/0/1\">Karl Schmeckpeper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Siddharth Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daniilidis_K/0/1/0/all/0/1\">Kostas Daniilidis</a>",
          "description": "We consider the problem of object goal navigation in unseen environments. In\nour view, solving this problem requires learning of contextual semantic priors,\na challenging endeavour given the spatial and semantic variability of indoor\nenvironments. Current methods learn to implicitly encode these priors through\ngoal-oriented navigation policy functions operating on spatial representations\nthat are limited to the agent's observable areas. In this work, we propose a\nnovel framework that actively learns to generate semantic maps outside the\nfield of view of the agent and leverages the uncertainty over the semantic\nclasses in the unobserved areas to decide on long term goals. We demonstrate\nthat through this spatial prediction strategy, we are able to learn semantic\npriors in scenes that can be leveraged in unknown environments. Additionally,\nwe show how different objectives can be defined by balancing exploration with\nexploitation during searching for semantic targets. Our method is validated in\nthe visually realistic environments offered by the Matterport3D dataset and\nshow state of the art results on the object goal navigation task.",
          "link": "http://arxiv.org/abs/2106.15648",
          "publishedOn": "2021-07-01T01:59:32.462Z",
          "wordCount": 612,
          "title": "Learning to Map for Active Semantic Goal Navigation. (arXiv:2106.15648v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08208",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Huang_F/0/1/0/all/0/1\">Feihu Huang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Li_J/0/1/0/all/0/1\">Junyi Li</a>, <a href=\"http://arxiv.org/find/math/1/au:+Huang_H/0/1/0/all/0/1\">Heng Huang</a>",
          "description": "Adaptive gradient methods have shown excellent performance for solving many\nmachine learning problems. Although multiple adaptive methods were recently\nstudied, they mainly focus on either empirical or theoretical aspects and also\nonly work for specific problems by using specific adaptive learning rates. It\nis desired to design a universal framework for practical algorithms of adaptive\ngradients with theoretical guarantee to solve general problems. To fill this\ngap, we propose a faster and universal framework of adaptive gradients (i.e.,\nSUPER-ADAM) by introducing a universal adaptive matrix that includes most\nexisting adaptive gradient forms. Moreover, our framework can flexibly\nintegrates the momentum and variance reduced techniques. In particular, our\nnovel framework provides the convergence analysis support for adaptive gradient\nmethods under the nonconvex setting. In theoretical analysis, we prove that our\nnew algorithm can achieve the best known complexity of\n$\\tilde{O}(\\epsilon^{-3})$ for finding an $\\epsilon$-stationary point of\nnonconvex optimization, which matches the lower bound for stochastic smooth\nnonconvex optimization. In numerical experiments, we employ various deep\nlearning tasks to validate that our algorithm consistently outperforms the\nexisting adaptive algorithms.",
          "link": "http://arxiv.org/abs/2106.08208",
          "publishedOn": "2021-07-01T01:59:32.418Z",
          "wordCount": 648,
          "title": "SUPER-ADAM: Faster and Universal Framework of Adaptive Gradients. (arXiv:2106.08208v2 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10587",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Conde_M/0/1/0/all/0/1\">Marcos V. Conde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turgutlu_K/0/1/0/all/0/1\">Kerem Turgutlu</a>",
          "description": "Existing computer vision research in categorization struggles with\nfine-grained attributes recognition due to the inherently high intra-class\nvariances and low inter-class variances. SOTA methods tackle this challenge by\nlocating the most informative image regions and rely on them to classify the\ncomplete image. The most recent work, Vision Transformer (ViT), shows its\nstrong performance in both traditional and fine-grained classification tasks.\nIn this work, we propose a multi-stage ViT framework for fine-grained image\nclassification tasks, which localizes the informative image regions without\nrequiring architectural changes using the inherent multi-head self-attention\nmechanism. We also introduce attention-guided augmentations for improving the\nmodel's capabilities. We demonstrate the value of our approach by experimenting\nwith four popular fine-grained benchmarks: CUB-200-2011, Stanford Cars,\nStanford Dogs, and FGVC7 Plant Pathology. We also prove our model's\ninterpretability via qualitative results.",
          "link": "http://arxiv.org/abs/2106.10587",
          "publishedOn": "2021-07-01T01:59:32.401Z",
          "wordCount": 621,
          "title": "Exploring Vision Transformers for Fine-grained Classification. (arXiv:2106.10587v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08049",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Drozdowski_P/0/1/0/all/0/1\">Pawel Drozdowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rathgeb_C/0/1/0/all/0/1\">Christian Rathgeb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Busch_C/0/1/0/all/0/1\">Christoph Busch</a>",
          "description": "Recently, different researchers have found that the gallery composition of a\nface database can induce performance differentials to facial identification\nsystems in which a probe image is compared against up to all stored reference\nimages to reach a biometric decision. This negative effect is referred to as\n\"watchlist imbalance effect\". In this work, we present a method to\ntheoretically estimate said effect for a biometric identification system given\nits verification performance across demographic groups and the composition of\nthe used gallery. Further, we report results for identification experiments on\ndifferently composed demographic subsets, i.e. females and males, of the public\nacademic MORPH database using the open-source ArcFace face recognition system.\nIt is shown that the database composition has a huge impact on performance\ndifferentials in biometric identification systems, even if performance\ndifferentials are less pronounced in the verification scenario. This study\nrepresents the first detailed analysis of the watchlist imbalance effect which\nis expected to be of high interest for future research in the field of facial\nrecognition.",
          "link": "http://arxiv.org/abs/2106.08049",
          "publishedOn": "2021-07-01T01:59:32.394Z",
          "wordCount": 646,
          "title": "Demographic Fairness in Face Identification: The Watchlist Imbalance Effect. (arXiv:2106.08049v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.10013",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vendramini_M/0/1/0/all/0/1\">Marcos Vendramini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliveira_H/0/1/0/all/0/1\">Hugo Oliveira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Machado_A/0/1/0/all/0/1\">Alexei Machado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santos_J/0/1/0/all/0/1\">Jefersson A. dos Santos</a>",
          "description": "Image classification methods are usually trained to perform predictions\ntaking into account a predefined group of known classes. Real-world problems,\nhowever, may not allow for a full knowledge of the input and label spaces,\nmaking failures in recognition a hazard to deep visual learning. Open set\nrecognition methods are characterized by the ability to correctly identify\ninputs of known and unknown classes. In this context, we propose GeMOS: simple\nand plug-and-play open set recognition modules that can be attached to\npretrained Deep Neural Networks for visual recognition. The GeMOS framework\npairs pre-trained Convolutional Neural Networks with generative models for open\nset recognition to extract open set scores for each sample, allowing for\nfailure recognition in object recognition tasks. We conduct a thorough\nevaluation of the proposed method in comparison with state-of-the-art open set\nalgorithms, finding that GeMOS either outperforms or is statistically\nindistinguishable from more complex and costly models.",
          "link": "http://arxiv.org/abs/2105.10013",
          "publishedOn": "2021-07-01T01:59:32.387Z",
          "wordCount": 626,
          "title": "Opening Deep Neural Networks with Generative Models. (arXiv:2105.10013v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.10159",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Nguyen_D/0/1/0/all/0/1\">Du Nguyen</a>",
          "description": "We provide an explicit formula for the Levi-Civita connection and Riemannian\nHessian for a Riemannian manifold that is a quotient of a manifold embedded in\nan inner product space with a non-constant metric function. Together with a\nclassical formula for projection, this allows us to evaluate Riemannian\ngradient and Hessian for several families of metrics on classical manifolds,\nincluding a family of metrics on Stiefel manifolds connecting both the constant\nand canonical ambient metrics with closed-form geodesics. Using these formulas,\nwe derive Riemannian optimization frameworks on quotients of Stiefel manifolds,\nincluding flag manifolds, and a new family of complete quotient metrics on the\nmanifold of positive-semidefinite matrices of fixed rank, considered as a\nquotient of a product of Stiefel and positive-definite matrix manifold with\naffine-invariant metrics. The method is procedural, and in many instances, the\nRiemannian gradient and Hessian formulas could be derived by symbolic calculus.\nThe method extends the list of potential metrics that could be used in manifold\noptimization and machine learning.",
          "link": "http://arxiv.org/abs/2009.10159",
          "publishedOn": "2021-07-01T01:59:32.375Z",
          "wordCount": 639,
          "title": "Operator-valued formulas for Riemannian Gradient and Hessian and families of tractable metrics. (arXiv:2009.10159v2 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.05575",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+VanRullen_R/0/1/0/all/0/1\">Rufin VanRullen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alamia_A/0/1/0/all/0/1\">Andrea Alamia</a>",
          "description": "Transformer attention architectures, similar to those developed for natural\nlanguage processing, have recently proved efficient also in vision, either in\nconjunction with or as a replacement for convolutional layers. Typically,\nvisual attention is inserted in the network architecture as a (series of)\nfeedforward self-attention module(s), with mutual key-query agreement as the\nmain selection and routing operation. However efficient, this strategy is only\nvaguely compatible with the way that attention is implemented in biological\nbrains: as a separate and unified network of attentional selection regions,\nreceiving inputs from and exerting modulatory influence on the entire hierarchy\nof visual regions. Here, we report experiments with a simple such attention\nsystem that can improve the performance of standard convolutional networks,\nwith relatively few additional parameters. Each spatial position in each layer\nof the network produces a key-query vector pair; all queries are then pooled\ninto a global attention query. On the next iteration, the match between each\nkey and the global attention query modulates the network's activations --\nemphasizing or silencing the locations that agree or disagree (respectively)\nwith the global attention system. We demonstrate the usefulness of this\nbrain-inspired Global Attention Agreement network (GAttANet) for various\nconvolutional backbones (from a simple 5-layer toy model to a standard ResNet50\narchitecture) and datasets (CIFAR10, CIFAR100, Imagenet-1k). Each time, our\nglobal attention system improves accuracy over the corresponding baseline.",
          "link": "http://arxiv.org/abs/2104.05575",
          "publishedOn": "2021-07-01T01:59:32.355Z",
          "wordCount": 704,
          "title": "GAttANet: Global attention agreement for convolutional neural networks. (arXiv:2104.05575v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11776",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tahir_G/0/1/0/all/0/1\">Ghalib Tahir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loo_C/0/1/0/all/0/1\">Chu Kiong Loo</a>",
          "description": "Last ten years have witnessed the growth of many computer vision applications\nfor food recognition. Dietary studies showed that dietary-related problem such\nas obesity is associated with other chronic diseases like hypertension,\nirregular blood sugar levels, and increased risk of heart attacks. The primary\ncause of these problems is poor lifestyle choices and unhealthy dietary habits,\nwhich are manageable by using interactive mHealth apps that use automatic\nvisual-based methods to assess dietary intake. This review discusses the most\nperforming methodologies that have been developed so far for automatic food\nrecognition. First, we will present the rationale of visual-based methods for\nfood recognition. The core of the paper is the presentation, discussion and\nevaluation of these methods on popular food image databases. We also discussed\nthe mobile applications that are implementing these methods. The review ends\nwith a discussion of research gaps and future challenges in this area.",
          "link": "http://arxiv.org/abs/2106.11776",
          "publishedOn": "2021-07-01T01:59:32.345Z",
          "wordCount": 605,
          "title": "A Review of the Vision-based Approaches for Dietary Assessment. (arXiv:2106.11776v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.16108",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kousha_S/0/1/0/all/0/1\">Shayan Kousha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brubaker_M/0/1/0/all/0/1\">Marcus A. Brubaker</a>",
          "description": "The purpose of generative Zero-shot learning (ZSL) is to learning from seen\nclasses, transfer the learned knowledge, and create samples of unseen classes\nfrom the description of these unseen categories. To achieve better ZSL\naccuracies, models need to better understand the descriptions of unseen\nclasses. We introduce a novel form of regularization that encourages generative\nZSL models to pay more attention to the description of each category. Our\nempirical results demonstrate improvements over the performance of multiple\nstate-of-the-art models on the task of generalized zero-shot recognition and\nclassification when trained on textual description-based datasets like CUB and\nNABirds and attribute-based datasets like AWA2, aPY and SUN.",
          "link": "http://arxiv.org/abs/2106.16108",
          "publishedOn": "2021-07-01T01:59:32.310Z",
          "wordCount": 534,
          "title": "Zero-shot Learning with Class Description Regularization. (arXiv:2106.16108v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2104.07636",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Saharia_C/0/1/0/all/0/1\">Chitwan Saharia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ho_J/0/1/0/all/0/1\">Jonathan Ho</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chan_W/0/1/0/all/0/1\">William Chan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Salimans_T/0/1/0/all/0/1\">Tim Salimans</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fleet_D/0/1/0/all/0/1\">David J. Fleet</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Norouzi_M/0/1/0/all/0/1\">Mohammad Norouzi</a>",
          "description": "We present SR3, an approach to image Super-Resolution via Repeated\nRefinement. SR3 adapts denoising diffusion probabilistic models to conditional\nimage generation and performs super-resolution through a stochastic denoising\nprocess. Inference starts with pure Gaussian noise and iteratively refines the\nnoisy output using a U-Net model trained on denoising at various noise levels.\nSR3 exhibits strong performance on super-resolution tasks at different\nmagnification factors, on faces and natural images. We conduct human evaluation\non a standard 8X face super-resolution task on CelebA-HQ, comparing with SOTA\nGAN methods. SR3 achieves a fool rate close to 50%, suggesting photo-realistic\noutputs, while GANs do not exceed a fool rate of 34%. We further show the\neffectiveness of SR3 in cascaded image generation, where generative models are\nchained with super-resolution models, yielding a competitive FID score of 11.3\non ImageNet.",
          "link": "http://arxiv.org/abs/2104.07636",
          "publishedOn": "2021-07-01T01:59:32.295Z",
          "wordCount": 600,
          "title": "Image Super-Resolution via Iterative Refinement. (arXiv:2104.07636v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.09396",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Borji_A/0/1/0/all/0/1\">Ali Borji</a>",
          "description": "This work is an update of a previous paper on the same topic published a few\nyears ago. With the dramatic progress in generative modeling, a suite of new\nquantitative and qualitative techniques to evaluate models has emerged.\nAlthough some measures such as Inception Score, Frechet Inception Distance,\nPrecision-Recall, and Perceptual Path Length are relatively more popular, GAN\nevaluation is not a settled issue and there is still room for improvement.\nHere, I describe new dimensions that are becoming important in assessing models\n(e.g. bias and fairness) and discuss the connection between GAN evaluation and\ndeepfakes. These are important areas of concern in the machine learning\ncommunity today and progress in GAN evaluation can help mitigate them.",
          "link": "http://arxiv.org/abs/2103.09396",
          "publishedOn": "2021-07-01T01:59:32.288Z",
          "wordCount": 581,
          "title": "Pros and Cons of GAN Evaluation Measures: New Developments. (arXiv:2103.09396v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.16091",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Leeb_F/0/1/0/all/0/1\">Felix Leeb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bauer_S/0/1/0/all/0/1\">Stefan Bauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1\">Bernhard Sch&#xf6;lkopf</a>",
          "description": "The encoders and decoders of autoencoders effectively project the input onto\nlearned manifolds in the latent space and data space respectively. We propose a\nframework, called latent responses, for probing the learned data manifold using\ninterventions in the latent space. Using this framework, we investigate \"holes\"\nin the representation to quantitatively ascertain to what extent the latent\nspace of a trained VAE is consistent with the chosen prior. Furthermore, we use\nthe identified structure to improve interpolation between latent vectors. We\nevaluate how our analyses improve the quality of the generated samples using\nthe VAE on a variety of benchmark datasets.",
          "link": "http://arxiv.org/abs/2106.16091",
          "publishedOn": "2021-07-01T01:59:32.268Z",
          "wordCount": 541,
          "title": "Interventional Assays for the Latent Space of Autoencoders. (arXiv:2106.16091v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2012.00650",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lu_M/0/1/0/all/0/1\">Ming Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_z/0/1/0/all/0/1\">zhenyu Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_D/0/1/0/all/0/1\">Dandan Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zhan Ma</a>",
          "description": "This paper proposes a decoder-side Cross Resolution Synthesis (CRS) module to\npursue better compression efficiency beyond the latest Versatile Video Coding\n(VVC), where we encode intra frames at original high resolution (HR), compress\ninter frames at a lower resolution (LR), and then super-resolve decoded LR\ninter frames with the help from preceding HR intra and neighboring LR inter\nframes. For a LR inter frame, a motion alignment and aggregation network (MAN)\nis devised to produce temporally aggregated motion representation (AMR) for the\nguarantee of temporal smoothness; Another texture compensation network (TCN)\ninputs decoded HR intra frame, re-sampled HR intra frame, and this LR inter\nframe to generate multiscale affinity map (MAM) and multiscale texture\nrepresentation (MTR) for better augmenting spatial details; Finally,\nsimilarity-driven fusion synthesizes AMR, MTR, MAM to upscale LR inter frame\nfor the removal of compression and resolution re-sampling noises. We enhance\nthe VVC using proposed CRS, showing averaged 8.76% and 11.93% Bj{\\o}ntegaard\nDelta Rate (BD-Rate) gains against the latest VVC anchor in Random Access (RA)\nand Low-delay P (LDP) settings respectively. In addition, experimental\ncomparisons to the state-of-the-art super-resolution (SR) based VVC enhancement\nmethods, and ablation studies are conducted to further report superior\nefficiency and generalization of proposed algorithm. All materials will be made\nto public at https://njuvision.github.io/CRS for reproducible research.",
          "link": "http://arxiv.org/abs/2012.00650",
          "publishedOn": "2021-07-01T01:59:32.263Z",
          "wordCount": 705,
          "title": "Decoder-side Cross Resolution Synthesis for Video Compression Enhancement. (arXiv:2012.00650v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.16009",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Becker_S/0/1/0/all/0/1\">Stefan Becker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hug_R/0/1/0/all/0/1\">Ronny Hug</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hubner_W/0/1/0/all/0/1\">Wolfgang H&#xfc;bner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arens_M/0/1/0/all/0/1\">Michael Arens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morris_B/0/1/0/all/0/1\">Brendan T. Morris</a>",
          "description": "In applications such as object tracking, time-series data inevitably carry\nmissing observations. Following the success of deep learning-based models for\nvarious sequence learning tasks, these models increasingly replace classic\napproaches in object tracking applications for inferring the object motions\nstate. While traditional tracking approaches can deal with missing\nobservations, most of their deep counterparts are, by default, not suited for\nthis.\n\nTowards this end, this paper introduces a transformer-based approach for\nhandling missing observations in variable input length trajectory data. The\nmodel is formed indirectly by successively increasing the complexity of the\ndemanded inference tasks. Starting from reproducing noise-free trajectories,\nthe model then learns to infer trajectories from noisy inputs. By providing\nmissing tokens, binary-encoded missing events, the model learns to in-attend to\nmissing data and infers a complete trajectory conditioned on the remaining\ninputs. In the case of a sequence of successive missing events, the model then\nacts as a pure prediction model. The model's abilities are demonstrated on\nsynthetic data and real-world data reflecting prototypical object tracking\nscenarios.",
          "link": "http://arxiv.org/abs/2106.16009",
          "publishedOn": "2021-07-01T01:59:32.256Z",
          "wordCount": 615,
          "title": "MissFormer: (In-)attention-based handling of missing observations for trajectory filtering and prediction. (arXiv:2106.16009v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16000",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiawei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuexiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1\">Kai Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yefeng Zheng</a>",
          "description": "Convolutional neural network (CNN) have proven its success for semantic\nsegmentation, which is a core task of emerging industrial applications such as\nautonomous driving. However, most progress in semantic segmentation of urban\nscenes is reported on standard scenarios, i.e., daytime scenes with favorable\nillumination conditions. In practical applications, the outdoor weather and\nillumination are changeable, e.g., cloudy and nighttime, which results in a\nsignificant drop of semantic segmentation accuracy of CNN only trained with\ndaytime data. In this paper, we propose a novel generative adversarial network\n(namely Mutual-GAN) to alleviate the accuracy decline when daytime-trained\nneural network is applied to videos captured under adverse weather conditions.\nThe proposed Mutual-GAN adopts mutual information constraint to preserve\nimage-objects during cross-weather adaptation, which is an unsolved problem for\nmost unsupervised image-to-image translation approaches (e.g., CycleGAN). The\nproposed Mutual-GAN is evaluated on two publicly available driving video\ndatasets (i.e., CamVid and SYNTHIA). The experimental results demonstrate that\nour Mutual-GAN can yield visually plausible translated images and significantly\nimprove the semantic segmentation accuracy of daytime-trained deep learning\nnetwork while processing videos under challenging weathers.",
          "link": "http://arxiv.org/abs/2106.16000",
          "publishedOn": "2021-07-01T01:59:32.248Z",
          "wordCount": 627,
          "title": "Mutual-GAN: Towards Unsupervised Cross-Weather Adaptation with Mutual Information Constraint. (arXiv:2106.16000v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15893",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wilm_F/0/1/0/all/0/1\">Frauke Wilm</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Benz_M/0/1/0/all/0/1\">Michaela Benz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bruns_V/0/1/0/all/0/1\">Volker Bruns</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Baghdadlian_S/0/1/0/all/0/1\">Serop Baghdadlian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dexl_J/0/1/0/all/0/1\">Jakob Dexl</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hartmann_D/0/1/0/all/0/1\">David Hartmann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kuritcyn_P/0/1/0/all/0/1\">Petr Kuritcyn</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Weidenfeller_M/0/1/0/all/0/1\">Martin Weidenfeller</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wittenberg_T/0/1/0/all/0/1\">Thomas Wittenberg</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Merkel_S/0/1/0/all/0/1\">Susanne Merkel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hartmann_A/0/1/0/all/0/1\">Arndt Hartmann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Eckstein_M/0/1/0/all/0/1\">Markus Eckstein</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Geppert_C/0/1/0/all/0/1\">Carol I. Geppert</a>",
          "description": "Whole-slide-image cartography is the process of automatically detecting and\noutlining different tissue types in digitized histological specimen. This\nsemantic segmentation provides a basis for many follow-up analyses and can\npotentially guide subsequent medical decisions. Due to their large size,\nwhole-slide-images typically have to be divided into smaller patches which are\nthen analyzed individually using machine learning-based approaches. Thereby,\nlocal dependencies of image regions get lost and since a whole-slide-image\ncomprises many thousands of such patches this process is inherently slow. We\npropose to subdivide the image into coherent regions prior to classification by\ngrouping visually similar adjacent image pixels into larger segments, i.e.\nsuperpixels. Afterwards, only a random subset of patches per superpixel is\nclassified and patch labels are combined into a single superpixel label. The\nalgorithm has been developed and validated on a dataset of 159 hand-annotated\nwhole-slide-images of colon resections and its performance has been compared to\na standard patch-based approach. The algorithm shows an average speed-up of 41%\non the test data and the overall accuracy is increased from 93.8% to 95.7%. We\nadditionally propose a metric for identifying superpixels with an uncertain\nclassification so they can be excluded from further analysis. Finally, we\nevaluate two potential medical applications, namely tumor area estimation\nincluding tumor invasive margin generation and tumor composition analysis.",
          "link": "http://arxiv.org/abs/2106.15893",
          "publishedOn": "2021-07-01T01:59:32.242Z",
          "wordCount": 700,
          "title": "Fast whole-slide cartography in colon cancer histology using superpixels and CNN classification. (arXiv:2106.15893v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15889",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Berger_C/0/1/0/all/0/1\">Christian Berger</a>",
          "description": "ML-enabled software systems have been incorporated in many public\ndemonstrations for automated driving (AD) systems. Such solutions have also\nbeen considered as a crucial approach to aim at SAE Level 5 systems, where the\npassengers in such vehicles do not have to interact with the system at all\nanymore. Already in 2016, Nvidia demonstrated a complete end-to-end approach\nfor training the complete software stack covering perception, planning and\ndecision making, and the actual vehicle control. While such approaches show the\ngreat potential of such ML-enabled systems, there have also been demonstrations\nwhere already changes to single pixels in a video frame can potentially lead to\ncompletely different decisions with dangerous consequences. In this paper, a\nstructured analysis has been conducted to explore video degradation effects on\nthe performance of an ML-enabled pedestrian detector. Firstly, a baseline of\napplying YOLO to 1,026 frames with pedestrian annotations in the KITTI Vision\nBenchmark Suite has been established. Next, video degradation candidates for\neach of these frames were generated using the leading video codecs libx264,\nlibx265, Nvidia HEVC, and AV1: 52 frames for the various compression presets\nfor color and gray-scale frames resulting in 104 degradation candidates per\noriginal KITTI frame and 426,816 images in total. YOLO was applied to each\nimage to compute the intersection-over-union (IoU) metric to compare the\nperformance with the original baseline. While aggressively lossy compression\nsettings result in significant performance drops as expected, it was also\nobserved that some configurations actually result in slightly better IoU\nresults compared to the baseline. The findings show that carefully chosen lossy\nvideo configurations preserve a decent performance of particular ML-enabled\nsystems while allowing for substantial savings when storing or transmitting\ndata.",
          "link": "http://arxiv.org/abs/2106.15889",
          "publishedOn": "2021-07-01T01:59:32.233Z",
          "wordCount": 726,
          "title": "A Structured Analysis of the Video Degradation Effects on the Performance of a Machine Learning-enabled Pedestrian Detector. (arXiv:2106.15889v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15711",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1\">Christopher Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mousavian_A/0/1/0/all/0/1\">Arsalan Mousavian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_Y/0/1/0/all/0/1\">Yu Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fox_D/0/1/0/all/0/1\">Dieter Fox</a>",
          "description": "Segmenting unseen object instances in cluttered environments is an important\ncapability that robots need when functioning in unstructured environments.\nWhile previous methods have exhibited promising results, they still tend to\nprovide incorrect results in highly cluttered scenes. We postulate that a\nnetwork architecture that encodes relations between objects at a high-level can\nbe beneficial. Thus, in this work, we propose a novel framework that refines\nthe output of such methods by utilizing a graph-based representation of\ninstance masks. We train deep networks capable of sampling smart perturbations\nto the segmentations, and a graph neural network, which can encode relations\nbetween objects, to evaluate the perturbed segmentations. Our proposed method\nis orthogonal to previous works and achieves state-of-the-art performance when\ncombined with them. We demonstrate an application that uses uncertainty\nestimates generated by our method to guide a manipulator, leading to efficient\nunderstanding of cluttered scenes. Code, models, and video can be found at\nhttps://github.com/chrisdxie/rice .",
          "link": "http://arxiv.org/abs/2106.15711",
          "publishedOn": "2021-07-01T01:59:32.203Z",
          "wordCount": 601,
          "title": "RICE: Refining Instance Masks in Cluttered Environments with Graph Neural Networks. (arXiv:2106.15711v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.03423",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yu Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_G/0/1/0/all/0/1\">Guansong Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fengbei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+chen_Y/0/1/0/all/0/1\">Yuanhong chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_S/0/1/0/all/0/1\">Seon Ho Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verjans_J/0/1/0/all/0/1\">Johan W. Verjans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1\">Rajvinder Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carneiro_G/0/1/0/all/0/1\">Gustavo Carneiro</a>",
          "description": "Unsupervised anomaly detection (UAD) learns one-class classifiers exclusively\nwith normal (i.e., healthy) images to detect any abnormal (i.e., unhealthy)\nsamples that do not conform to the expected normal patterns. UAD has two main\nadvantages over its fully supervised counterpart. Firstly, it is able to\ndirectly leverage large datasets available from health screening programs that\ncontain mostly normal image samples, avoiding the costly manual labelling of\nabnormal samples and the subsequent issues involved in training with extremely\nclass-imbalanced data. Further, UAD approaches can potentially detect and\nlocalise any type of lesions that deviate from the normal patterns. One\nsignificant challenge faced by UAD methods is how to learn effective\nlow-dimensional image representations to detect and localise subtle\nabnormalities, generally consisting of small lesions. To address this\nchallenge, we propose a novel self-supervised representation learning method,\ncalled Constrained Contrastive Distribution learning for anomaly detection\n(CCD), which learns fine-grained feature representations by simultaneously\npredicting the distribution of augmented data and image contexts using\ncontrastive learning with pretext constraints. The learned representations can\nbe leveraged to train more anomaly-sensitive detection models. Extensive\nexperiment results show that our method outperforms current state-of-the-art\nUAD approaches on three different colonoscopy and fundus screening datasets.\nOur code is available at https://github.com/tianyu0207/CCD.",
          "link": "http://arxiv.org/abs/2103.03423",
          "publishedOn": "2021-07-01T01:59:32.187Z",
          "wordCount": 694,
          "title": "Constrained Contrastive Distribution Learning for Unsupervised Anomaly Detection and Localisation in Medical Images. (arXiv:2103.03423v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.09866",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bhat_P/0/1/0/all/0/1\">Prashant Bhat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arani_E/0/1/0/all/0/1\">Elahe Arani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zonooz_B/0/1/0/all/0/1\">Bahram Zonooz</a>",
          "description": "Self-supervised learning solves pretext prediction tasks that do not require\nannotations to learn feature representations. For vision tasks, pretext tasks\nsuch as predicting rotation, solving jigsaw are solely created from the input\ndata. Yet, predicting this known information helps in learning representations\nuseful for downstream tasks. However, recent works have shown that wider and\ndeeper models benefit more from self-supervised learning than smaller models.\nTo address the issue of self-supervised pre-training of smaller models, we\npropose Distill-on-the-Go (DoGo), a self-supervised learning paradigm using\nsingle-stage online knowledge distillation to improve the representation\nquality of the smaller models. We employ deep mutual learning strategy in which\ntwo models collaboratively learn from each other to improve one another.\nSpecifically, each model is trained using self-supervised learning along with\ndistillation that aligns each model's softmax probabilities of similarity\nscores with that of the peer model. We conduct extensive experiments on\nmultiple benchmark datasets, learning objectives, and architectures to\ndemonstrate the potential of our proposed method. Our results show significant\nperformance gain in the presence of noisy and limited labels and generalization\nto out-of-distribution data.",
          "link": "http://arxiv.org/abs/2104.09866",
          "publishedOn": "2021-07-01T01:59:32.169Z",
          "wordCount": 663,
          "title": "Distill on the Go: Online knowledge distillation in self-supervised learning. (arXiv:2104.09866v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.16198",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Madan_S/0/1/0/all/0/1\">Spandan Madan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sasaki_T/0/1/0/all/0/1\">Tomotake Sasaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tzu-Mao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boix_X/0/1/0/all/0/1\">Xavier Boix</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfister_H/0/1/0/all/0/1\">Hanspeter Pfister</a>",
          "description": "Neural networks are susceptible to small transformations including 2D\nrotations and shifts, image crops, and even changes in object colors. This is\noften attributed to biases in the training dataset, and the lack of 2D\nshift-invariance due to not respecting the sampling theorem. In this paper, we\nchallenge this hypothesis by training and testing on unbiased datasets, and\nshowing that networks are brittle to both small 3D perspective changes and\nlighting variations which cannot be explained by dataset bias or lack of\nshift-invariance. To find these in-distribution errors, we introduce an\nevolution strategies (ES) based approach, which we call CMA-Search. Despite\ntraining with a large-scale (0.5 million images), unbiased dataset of camera\nand light variations, in over 71% cases CMA-Search can find camera parameters\nin the vicinity of a correctly classified image which lead to in-distribution\nmisclassifications with < 3.6% change in parameters. With lighting changes,\nCMA-Search finds misclassifications in 33% cases with < 11.6% change in\nparameters. Finally, we extend this method to find misclassifications in the\nvicinity of ImageNet images for both ResNet and OpenAI's CLIP model.",
          "link": "http://arxiv.org/abs/2106.16198",
          "publishedOn": "2021-07-01T01:59:32.146Z",
          "wordCount": 630,
          "title": "Small in-distribution changes in 3D perspective and lighting fool both CNNs and Transformers. (arXiv:2106.16198v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2104.05704",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hassani_A/0/1/0/all/0/1\">Ali Hassani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walton_S/0/1/0/all/0/1\">Steven Walton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_N/0/1/0/all/0/1\">Nikhil Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abuduweili_A/0/1/0/all/0/1\">Abulikemu Abuduweili</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiachen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Humphrey Shi</a>",
          "description": "With the rise of Transformers as the standard for language processing, and\ntheir advancements in computer vision, along with their unprecedented size and\namounts of training data, many have come to believe that they are not suitable\nfor small sets of data. This trend leads to great concerns, including but not\nlimited to: limited availability of data in certain scientific domains and the\nexclusion of those with limited resource from research in the field. In this\npaper, we dispel the myth that transformers are \"data hungry\" and therefore can\nonly be applied to large sets of data. We show for the first time that with the\nright size and tokenization, transformers can perform head-to-head with\nstate-of-the-art CNNs on small datasets. Our model eliminates the requirement\nfor class token and positional embeddings through a novel sequence pooling\nstrategy and the use of convolutions. We show that compared to CNNs, our\ncompact transformers have fewer parameters and MACs, while obtaining similar\naccuracies. Our method is flexible in terms of model size, and can have as\nlittle as 0.28M parameters and achieve reasonable results. It can reach an\naccuracy of 95.29 % when training from scratch on CIFAR-10, which is comparable\nwith modern CNN based approaches, and a significant improvement over previous\nTransformer based models. Our simple and compact design democratizes\ntransformers by making them accessible to those equipped with basic computing\nresources and/or dealing with important small datasets. Our method works on\nlarger datasets, such as ImageNet (80.28% accuracy with 29% parameters of ViT),\nand NLP tasks as well. Our code and pre-trained models are publicly available\nat https://github.com/SHI-Labs/Compact-Transformers.",
          "link": "http://arxiv.org/abs/2104.05704",
          "publishedOn": "2021-07-01T01:59:32.140Z",
          "wordCount": 748,
          "title": "Escaping the Big Data Paradigm with Compact Transformers. (arXiv:2104.05704v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.16245",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Han-Jia Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_W/0/1/0/all/0/1\">Wei-Lun Chao</a>",
          "description": "Model-agnostic meta-learning (MAML) is arguably the most popular\nmeta-learning algorithm nowadays, given its flexibility to incorporate various\nmodel architectures and to be applied to different problems. Nevertheless, its\nperformance on few-shot classification is far behind many recent algorithms\ndedicated to the problem. In this paper, we point out several key facets of how\nto train MAML to excel in few-shot classification. First, we find that a large\nnumber of gradient steps are needed for the inner loop update, which\ncontradicts the common usage of MAML for few-shot classification. Second, we\nfind that MAML is sensitive to the permutation of class assignments in\nmeta-testing: for a few-shot task of $N$ classes, there are exponentially many\nways to assign the learned initialization of the $N$-way classifier to the $N$\nclasses, leading to an unavoidably huge variance. Third, we investigate several\nways for permutation invariance and find that learning a shared classifier\ninitialization for all the classes performs the best. On benchmark datasets\nsuch as MiniImageNet and TieredImageNet, our approach, which we name\nUNICORN-MAML, performs on a par with or even outperforms state-of-the-art\nalgorithms, while keeping the simplicity of MAML without adding any extra\nsub-networks.",
          "link": "http://arxiv.org/abs/2106.16245",
          "publishedOn": "2021-07-01T01:59:32.133Z",
          "wordCount": 632,
          "title": "How to Train Your MAML to Excel in Few-Shot Classification. (arXiv:2106.16245v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15787",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Liyu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yuexian Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Can Zhang</a>",
          "description": "Efficient long-short temporal modeling is key for enhancing the performance\nof action recognition task. In this paper, we propose a new two-stream action\nrecognition network, termed as MENet, consisting of a Motion Enhancement (ME)\nmodule and a Video-level Aggregation (VLA) module to achieve long-short\ntemporal modeling. Specifically, motion representations have been proved\neffective in capturing short-term and high-frequency action. However, current\nmotion representations are calculated from adjacent frames, which may have poor\ninterpretation and bring useless information (noisy or blank). Thus, for\nshort-term motions, we design an efficient ME module to enhance the short-term\nmotions by mingling the motion saliency among neighboring segments. As for\nlong-term aggregations, VLA is adopted at the top of the appearance branch to\nintegrate the long-term dependencies across all segments. The two components of\nMENet are complementary in temporal modeling. Extensive experiments are\nconducted on UCF101 and HMDB51 benchmarks, which verify the effectiveness and\nefficiency of our proposed MENet.",
          "link": "http://arxiv.org/abs/2106.15787",
          "publishedOn": "2021-07-01T01:59:32.113Z",
          "wordCount": 591,
          "title": "Long-Short Temporal Modeling for Efficient Action Recognition. (arXiv:2106.15787v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15828",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tapia_J/0/1/0/all/0/1\">Juan Tapia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Droguett_E/0/1/0/all/0/1\">Enrique Lopez Droguett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valenzuela_A/0/1/0/all/0/1\">Andres Valenzuela</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benalcazar_D/0/1/0/all/0/1\">Daniel Benalcazar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Causa_L/0/1/0/all/0/1\">Leonardo Causa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Busch_C/0/1/0/all/0/1\">Christoph Busch</a>",
          "description": "This paper proposes a new framework to detect, segment, and estimate the\nlocalization of the eyes from a periocular Near-Infra-Red iris image under\nalcohol consumption. The purpose of the system is to measure the fitness for\nduty. Fitness systems allow us to determine whether a person is physically or\npsychologically able to perform their tasks. Our framework is based on an\nobject detector trained from scratch to detect both eyes from a single image.\nThen, two efficient networks were used for semantic segmentation; a Criss-Cross\nattention network and DenseNet10, with only 122,514 and 210,732 parameters,\nrespectively. These networks can find the pupil, iris, and sclera. In the end,\nthe binary output eye mask is used for pupil and iris diameter estimation with\nhigh precision. Five state-of-the-art algorithms were used for this purpose. A\nmixed proposal reached the best results. A second contribution is establishing\nan alcohol behavior curve to detect the alcohol presence utilizing a stream of\nimages captured from an iris instance. Also, a manually labeled database with\nmore than 20k images was created. Our best method obtains a mean\nIntersection-over-Union of 94.54% with DenseNet10 with only 210,732 parameters\nand an error of only 1-pixel on average.",
          "link": "http://arxiv.org/abs/2106.15828",
          "publishedOn": "2021-07-01T01:59:32.106Z",
          "wordCount": 643,
          "title": "Semantic Segmentation of Periocular Near-Infra-Red Eye Images Under Alcohol Effects. (arXiv:2106.15828v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15947",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinlong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rufeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chunhua Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_T/0/1/0/all/0/1\">Tao Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>",
          "description": "Compared to many other dense prediction tasks, e.g., semantic segmentation,\nit is the arbitrary number of instances that has made instance segmentation\nmuch more challenging. In order to predict a mask for each instance, mainstream\napproaches either follow the 'detect-then-segment' strategy (e.g., Mask R-CNN),\nor predict embedding vectors first then cluster pixels into individual\ninstances. In this paper, we view the task of instance segmentation from a\ncompletely new perspective by introducing the notion of \"instance categories\",\nwhich assigns categories to each pixel within an instance according to the\ninstance's location. With this notion, we propose segmenting objects by\nlocations (SOLO), a simple, direct, and fast framework for instance\nsegmentation with strong performance. We derive a few SOLO variants (e.g.,\nVanilla SOLO, Decoupled SOLO, Dynamic SOLO) following the basic principle. Our\nmethod directly maps a raw input image to the desired object categories and\ninstance masks, eliminating the need for the grouping post-processing or the\nbounding box detection. Our approach achieves state-of-the-art results for\ninstance segmentation in terms of both speed and accuracy, while being\nconsiderably simpler than the existing methods. Besides instance segmentation,\nour method yields state-of-the-art results in object detection (from our mask\nbyproduct) and panoptic segmentation. We further demonstrate the flexibility\nand high-quality segmentation of SOLO by extending it to perform one-stage\ninstance-level image matting. Code is available at: https://git.io/AdelaiDet",
          "link": "http://arxiv.org/abs/2106.15947",
          "publishedOn": "2021-07-01T01:59:32.089Z",
          "wordCount": 672,
          "title": "SOLO: A Simple Framework for Instance Segmentation. (arXiv:2106.15947v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15918",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zipei Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_F/0/1/0/all/0/1\">Fengqian Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiwen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_C/0/1/0/all/0/1\">Chuyang Ye</a>",
          "description": "Cell detection in histopathology images is of great value in clinical\npractice. \\textit{Convolutional neural networks} (CNNs) have been applied to\ncell detection to improve the detection accuracy, where cell annotations are\nrequired for network training. However, due to the variety and large number of\ncells, complete annotations that include every cell of interest in the training\nimages can be challenging. Usually, incomplete annotations can be achieved,\nwhere positive labeling results are carefully examined to ensure their\nreliability but there can be other positive instances, i.e., cells of interest,\nthat are not included in the annotations. This annotation strategy leads to a\nlack of knowledge about true negative samples. Most existing methods simply\ntreat instances that are not labeled as positive as truly negative during\nnetwork training, which can adversely affect the network performance. In this\nwork, to address the problem of incomplete annotations, we formulate the\ntraining of detection networks as a positive-unlabeled learning problem.\nSpecifically, the classification loss in network training is revised to take\ninto account incomplete annotations, where the terms corresponding to negative\nsamples are approximated with the true positive samples and the other samples\nof which the labels are unknown. To evaluate the proposed method, experiments\nwere performed on a publicly available dataset for mitosis detection in breast\ncancer cells, and the experimental results show that our method improves the\nperformance of cell detection given incomplete annotations for training.",
          "link": "http://arxiv.org/abs/2106.15918",
          "publishedOn": "2021-07-01T01:59:32.068Z",
          "wordCount": 680,
          "title": "Positive-unlabeled Learning for Cell Detection in Histopathology Images with Incomplete Annotations. (arXiv:2106.15918v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16237",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Arora_H/0/1/0/all/0/1\">Himanshu Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Saurabh Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1\">Shichong Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Ke Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahdavi_Amiri_A/0/1/0/all/0/1\">Ali Mahdavi-Amiri</a>",
          "description": "Shape completion is the problem of completing partial input shapes such as\npartial scans. This problem finds important applications in computer vision and\nrobotics due to issues such as occlusion or sparsity in real-world data.\nHowever, most of the existing research related to shape completion has been\nfocused on completing shapes by learning a one-to-one mapping which limits the\ndiversity and creativity of the produced results. We propose a novel multimodal\nshape completion technique that is effectively able to learn a one-to-many\nmapping and generates diverse complete shapes. Our approach is based on the\nconditional Implicit MaximumLikelihood Estimation (IMLE) technique wherein we\ncondition our inputs on partial 3D point clouds. We extensively evaluate our\napproach by comparing it to various baselines both quantitatively and\nqualitatively. We show that our method is superior to alternatives in terms of\ncompleteness and diversity of shapes",
          "link": "http://arxiv.org/abs/2106.16237",
          "publishedOn": "2021-07-01T01:59:32.031Z",
          "wordCount": 571,
          "title": "Shape Completion via IMLE. (arXiv:2106.16237v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16174",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Chen_P/0/1/0/all/0/1\">Pingjun Chen</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Aminu_M/0/1/0/all/0/1\">Muhammad Aminu</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Hussein_S/0/1/0/all/0/1\">Siba El Hussein</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Khoury_J/0/1/0/all/0/1\">Joseph Khoury</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Wu_J/0/1/0/all/0/1\">Jia Wu</a>",
          "description": "The cells and their spatial patterns in the tumor microenvironment (TME) play\na key role in tumor evolution, and yet remains an understudied topic in\ncomputational pathology. This study, to the best of our knowledge, is among the\nfirst to hybrid local and global graph methods to profile orchestration and\ninteraction of cellular components. To address the challenge in hematolymphoid\ncancers where the cell classes in TME are unclear, we first implemented cell\nlevel unsupervised learning and identified two new cell subtypes. Local cell\ngraphs or supercells were built for each image by considering the individual\ncell's geospatial location and classes. Then, we applied supercell level\nclustering and identified two new cell communities. In the end, we built global\ngraphs to abstract spatial interaction patterns and extract features for\ndisease diagnosis. We evaluate the proposed algorithm on H\\&E slides of 60\nhematolymphoid neoplasm patients and further compared it with three cell level\ngraph-based algorithms, including the global cell graph, cluster cell graph,\nand FLocK. The proposed algorithm achieves a mean diagnosis accuracy of 0.703\nwith the repeated 5-fold cross-validation scheme. In conclusion, our algorithm\nshows superior performance over the existing methods and can be potentially\napplied to other cancer types.",
          "link": "http://arxiv.org/abs/2106.16174",
          "publishedOn": "2021-07-01T01:59:31.999Z",
          "wordCount": 661,
          "title": "Hierarchical Phenotyping and Graph Modeling of Spatial Architecture in Lymphoid Neoplasms. (arXiv:2106.16174v1 [q-bio.QM])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16031",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Dalmaz_O/0/1/0/all/0/1\">Onat Dalmaz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yurt_M/0/1/0/all/0/1\">Mahmut Yurt</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cukur_T/0/1/0/all/0/1\">Tolga &#xc7;ukur</a>",
          "description": "Multi-modal imaging is a key healthcare technology in the diagnosis and\nmanagement of disease, but it is often underutilized due to costs associated\nwith multiple separate scans. This limitation yields the need for synthesis of\nunacquired modalities from the subset of available modalities. In recent years,\ngenerative adversarial network (GAN) models with superior depiction of\nstructural details have been established as state-of-the-art in numerous\nmedical image synthesis tasks. However, GANs are characteristically based on\nconvolutional neural network (CNN) backbones that perform local processing with\ncompact filters. This inductive bias, in turn, compromises learning of\nlong-range spatial dependencies. While attention maps incorporated in GANs can\nmultiplicatively modulate CNN features to emphasize critical image regions,\ntheir capture of global context is mostly implicit. Here, we propose a novel\ngenerative adversarial approach for medical image synthesis, ResViT, to combine\nlocal precision of convolution operators with contextual sensitivity of vision\ntransformers. Based on an encoder-decoder architecture, ResViT employs a\ncentral bottleneck comprising novel aggregated residual transformer (ART)\nblocks that synergistically combine convolutional and transformer modules.\nComprehensive demonstrations are performed for synthesizing missing sequences\nin multi-contrast MRI and CT images from MRI. Our results indicate the\nsuperiority of ResViT against competing methods in terms of qualitative\nobservations and quantitative metrics.",
          "link": "http://arxiv.org/abs/2106.16031",
          "publishedOn": "2021-07-01T01:59:31.980Z",
          "wordCount": 650,
          "title": "ResViT: Residual vision transformers for multi-modal medical image synthesis. (arXiv:2106.16031v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15797",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yong Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yaofo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1\">Mingkui Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_K/0/1/0/all/0/1\">Kui Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jingdong Wang</a>",
          "description": "Convolutional Neural Networks (CNNs) have achieved great success due to the\npowerful feature learning ability of convolution layers. Specifically, the\nstandard convolution traverses the input images/features using a sliding window\nscheme to extract features. However, not all the windows contribute equally to\nthe prediction results of CNNs. In practice, the convolutional operation on\nsome of the windows (e.g., smooth windows that contain very similar pixels) can\nbe very redundant and may introduce noises into the computation. Such\nredundancy may not only deteriorate the performance but also incur the\nunnecessary computational cost. Thus, it is important to reduce the\ncomputational redundancy of convolution to improve the performance. To this\nend, we propose a Content-aware Convolution (CAC) that automatically detects\nthe smooth windows and applies a 1x1 convolutional kernel to replace the\noriginal large kernel. In this sense, we are able to effectively avoid the\nredundant computation on similar pixels. By replacing the standard convolution\nin CNNs with our CAC, the resultant models yield significantly better\nperformance and lower computational cost than the baseline models with the\nstandard convolution. More critically, we are able to dynamically allocate\nsuitable computation resources according to the data smoothness of different\nimages, making it possible for content-aware computation. Extensive experiments\non various computer vision tasks demonstrate the superiority of our method over\nexisting methods.",
          "link": "http://arxiv.org/abs/2106.15797",
          "publishedOn": "2021-07-01T01:59:31.970Z",
          "wordCount": 656,
          "title": "Content-Aware Convolutional Neural Networks. (arXiv:2106.15797v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16100",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuchi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhongdao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xiangxin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1\">Liang Zheng</a>",
          "description": "Association, aiming to link bounding boxes of the same identity in a video\nsequence, is a central component in multi-object tracking (MOT). To train\nassociation modules, e.g., parametric networks, real video data are usually\nused. However, annotating person tracks in consecutive video frames is\nexpensive, and such real data, due to its inflexibility, offer us limited\nopportunities to evaluate the system performance w.r.t changing tracking\nscenarios. In this paper, we study whether 3D synthetic data can replace\nreal-world videos for association training. Specifically, we introduce a\nlarge-scale synthetic data engine named MOTX, where the motion characteristics\nof cameras and objects are manually configured to be similar to those in\nreal-world datasets. We show that compared with real data, association\nknowledge obtained from synthetic data can achieve very similar performance on\nreal-world test sets without domain adaption techniques. Our intriguing\nobservation is credited to two factors. First and foremost, 3D engines can well\nsimulate motion factors such as camera movement, camera view and object\nmovement, so that the simulated videos can provide association modules with\neffective motion features. Second, experimental results show that the\nappearance domain gap hardly harms the learning of association knowledge. In\naddition, the strong customization ability of MOTX allows us to quantitatively\nassess the impact of motion factors on MOT, which brings new insights to the\ncommunity.",
          "link": "http://arxiv.org/abs/2106.16100",
          "publishedOn": "2021-07-01T01:59:31.963Z",
          "wordCount": 670,
          "title": "Synthetic Data Are as Good as the Real for Association Knowledge Learning in Multi-object Tracking. (arXiv:2106.16100v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16125",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Sicheng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_X/0/1/0/all/0/1\">Xingxu Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jufeng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_G/0/1/0/all/0/1\">Guoli Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_G/0/1/0/all/0/1\">Guiguang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1\">Tat-Seng Chua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schuller_B/0/1/0/all/0/1\">Bj&#xf6;rn W. Schuller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keutzer_K/0/1/0/all/0/1\">Kurt Keutzer</a>",
          "description": "Images can convey rich semantics and induce various emotions in viewers.\nRecently, with the rapid advancement of emotional intelligence and the\nexplosive growth of visual data, extensive research efforts have been dedicated\nto affective image content analysis (AICA). In this survey, we will\ncomprehensively review the development of AICA in the recent two decades,\nespecially focusing on the state-of-the-art methods with respect to three main\nchallenges -- the affective gap, perception subjectivity, and label noise and\nabsence. We begin with an introduction to the key emotion representation models\nthat have been widely employed in AICA and description of available datasets\nfor performing evaluation with quantitative comparison of label noise and\ndataset bias. We then summarize and compare the representative approaches on\n(1) emotion feature extraction, including both handcrafted and deep features,\n(2) learning methods on dominant emotion recognition, personalized emotion\nprediction, emotion distribution learning, and learning from noisy data or few\nlabels, and (3) AICA based applications. Finally, we discuss some challenges\nand promising research directions in the future, such as image content and\ncontext understanding, group emotion clustering, and viewer-image interaction.",
          "link": "http://arxiv.org/abs/2106.16125",
          "publishedOn": "2021-07-01T01:59:31.957Z",
          "wordCount": 643,
          "title": "Affective Image Content Analysis: Two Decades Review and New Perspectives. (arXiv:2106.16125v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15998",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wiens_D/0/1/0/all/0/1\">Daniel Wiens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hammer_B/0/1/0/all/0/1\">Barbara Hammer</a>",
          "description": "Even though deep neural networks succeed on many different tasks including\nsemantic segmentation, they lack on robustness against adversarial examples. To\ncounteract this exploit, often adversarial training is used. However, it is\nknown that adversarial training with weak adversarial attacks (e.g. using the\nFast Gradient Method) does not improve the robustness against stronger attacks.\nRecent research shows that it is possible to increase the robustness of such\nsingle-step methods by choosing an appropriate step size during the training.\nFinding such a step size, without increasing the computational effort of\nsingle-step adversarial training, is still an open challenge. In this work we\naddress the computationally particularly demanding task of semantic\nsegmentation and propose a new step size control algorithm that increases the\nrobustness of single-step adversarial training. The proposed algorithm does not\nincrease the computational effort of single-step adversarial training\nconsiderably and also simplifies training, because it is free of\nmeta-parameter. We show that the robustness of our approach can compete with\nmulti-step adversarial training on two popular benchmarks for semantic\nsegmentation.",
          "link": "http://arxiv.org/abs/2106.15998",
          "publishedOn": "2021-07-01T01:59:31.936Z",
          "wordCount": 609,
          "title": "Single-Step Adversarial Training for Semantic Segmentation. (arXiv:2106.15998v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15953",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wei_X/0/1/0/all/0/1\">Xinxu Wei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1\">Xianshi Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1\">Shisen Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cheng_C/0/1/0/all/0/1\">Cheng Cheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_Y/0/1/0/all/0/1\">Yanlin Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_K/0/1/0/all/0/1\">Kaifu Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1\">Yongjie Li</a>",
          "description": "Images obtained in real-world low-light conditions are not only low in\nbrightness, but they also suffer from many other types of degradation, such as\ncolor bias, unknown noise, detail loss and halo artifacts. In this paper, we\npropose a very fast deep learning framework called Bringing the Lightness\n(denoted as BLNet) that consists of two U-Nets with a series of well-designed\nloss functions to tackle all of the above degradations. Based on Retinex\nTheory, the decomposition net in our model can decompose low-light images into\nreflectance and illumination and remove noise in the reflectance during the\ndecomposition phase. We propose a Noise and Color Bias Control module (NCBC\nModule) that contains a convolutional neural network and two loss functions\n(noise loss and color loss). This module is only used to calculate the loss\nfunctions during the training phase, so our method is very fast during the test\nphase. This module can smooth the reflectance to achieve the purpose of noise\nremoval while preserving details and edge information and controlling color\nbias. We propose a network that can be trained to learn the mapping between\nlow-light and normal-light illumination and enhance the brightness of images\ntaken in low-light illumination. We train and evaluate the performance of our\nproposed model over the real-world Low-Light (LOL) dataset), and we also test\nour model over several other frequently used datasets (LIME, DICM and MEF\ndatasets). We conduct extensive experiments to demonstrate that our approach\nachieves a promising effect with good rubustness and generalization and\noutperforms many other state-of-the-art methods qualitatively and\nquantitatively. Our method achieves high speed because we use loss functions\ninstead of introducing additional denoisers for noise removal and color\ncorrection. The code and model are available at\nhttps://github.com/weixinxu666/BLNet.",
          "link": "http://arxiv.org/abs/2106.15953",
          "publishedOn": "2021-07-01T01:59:31.910Z",
          "wordCount": 765,
          "title": "BLNet: A Fast Deep Learning Framework for Low-Light Image Enhancement with Noise Removal and Color Restoration. (arXiv:2106.15953v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15681",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_B/0/1/0/all/0/1\">Bohao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1\">Xiong Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bradbury_K/0/1/0/all/0/1\">Kyle Bradbury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malof_J/0/1/0/all/0/1\">Jordan M. Malof</a>",
          "description": "Recently deep neural networks (DNNs) have achieved tremendous success for\nobject detection in overhead (e.g., satellite) imagery. One ongoing challenge\nhowever is the acquisition of training data, due to high costs of obtaining\nsatellite imagery and annotating objects in it. In this work we present a\nsimple approach - termed Synthetic object IMPLantation (SIMPL) - to easily and\nrapidly generate large quantities of synthetic overhead training data for\ncustom target objects. We demonstrate the effectiveness of using SIMPL\nsynthetic imagery for training DNNs in zero-shot scenarios where no real\nimagery is available; and few-shot learning scenarios, where limited real-world\nimagery is available. We also conduct experiments to study the sensitivity of\nSIMPL's effectiveness to some key design parameters, providing users for\ninsights when designing synthetic imagery for custom objects. We release a\nsoftware implementation of our SIMPL approach so that others can build upon it,\nor use it for their own custom problems.",
          "link": "http://arxiv.org/abs/2106.15681",
          "publishedOn": "2021-07-01T01:59:31.903Z",
          "wordCount": 599,
          "title": "SIMPL: Generating Synthetic Overhead Imagery to Address Zero-shot and Few-Shot Detection Problems. (arXiv:2106.15681v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16006",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tabani_H/0/1/0/all/0/1\">Hamid Tabani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balasubramaniam_A/0/1/0/all/0/1\">Ajay Balasubramaniam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marzban_S/0/1/0/all/0/1\">Shabbir Marzban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arani_E/0/1/0/all/0/1\">Elahe Arani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zonooz_B/0/1/0/all/0/1\">Bahram Zonooz</a>",
          "description": "Transformers provide promising accuracy and have become popular and used in\nvarious domains such as natural language processing and computer vision.\nHowever, due to their massive number of model parameters, memory and\ncomputation requirements, they are not suitable for resource-constrained\nlow-power devices. Even with high-performance and specialized devices, the\nmemory bandwidth can become a performance-limiting bottleneck. In this paper,\nwe present a performance analysis of state-of-the-art vision transformers on\nseveral devices. We propose to reduce the overall memory footprint and memory\ntransfers by clustering the model parameters. We show that by using only 64\nclusters to represent model parameters, it is possible to reduce the data\ntransfer from the main memory by more than 4x, achieve up to 22% speedup and\n39% energy savings on mobile devices with less than 0.1% accuracy loss.",
          "link": "http://arxiv.org/abs/2106.16006",
          "publishedOn": "2021-07-01T01:59:31.887Z",
          "wordCount": 589,
          "title": "Improving the Efficiency of Transformers for Resource-Constrained Devices. (arXiv:2106.16006v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15778",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_W/0/1/0/all/0/1\">Wenming Tang Guoping Qiu</a>",
          "description": "This paper presents new designs of graph convolutional neural networks (GCNs)\non 3D meshes for 3D object segmentation and classification. We use the faces of\nthe mesh as basic processing units and represent a 3D mesh as a graph where\neach node corresponds to a face. To enhance the descriptive power of the graph,\nwe introduce a 1-ring face neighbourhood structure to derive novel\nmulti-dimensional spatial and structure features to represent the graph nodes.\nBased on this new graph representation, we then design a densely connected\ngraph convolutional block which aggregates local and regional features as the\nkey construction component to build effective and efficient practical GCN\nmodels for 3D object classification and segmentation. We will present\nexperimental results to show that our new technique outperforms state of the\nart where our models are shown to have the smallest number of parameters and\nconsietently achieve the highest accuracies across a number of benchmark\ndatasets. We will also present ablation studies to demonstrate the soundness of\nour design principles and the effectiveness of our practical models.",
          "link": "http://arxiv.org/abs/2106.15778",
          "publishedOn": "2021-07-01T01:59:31.848Z",
          "wordCount": 621,
          "title": "Dense Graph Convolutional Neural Networks on 3D Meshes for 3D Object Segmentation and Classification. (arXiv:2106.15778v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15831",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Andreassen_A/0/1/0/all/0/1\">Anders Andreassen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bahri_Y/0/1/0/all/0/1\">Yasaman Bahri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neyshabur_B/0/1/0/all/0/1\">Behnam Neyshabur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roelofs_R/0/1/0/all/0/1\">Rebecca Roelofs</a>",
          "description": "Although machine learning models typically experience a drop in performance\non out-of-distribution data, accuracies on in- versus out-of-distribution data\nare widely observed to follow a single linear trend when evaluated across a\ntestbed of models. Models that are more accurate on the out-of-distribution\ndata relative to this baseline exhibit \"effective robustness\" and are\nexceedingly rare. Identifying such models, and understanding their properties,\nis key to improving out-of-distribution performance. We conduct a thorough\nempirical investigation of effective robustness during fine-tuning and\nsurprisingly find that models pre-trained on larger datasets exhibit effective\nrobustness during training that vanishes at convergence. We study how\nproperties of the data influence effective robustness, and we show that it\nincreases with the larger size, more diversity, and higher example difficulty\nof the dataset. We also find that models that display effective robustness are\nable to correctly classify 10% of the examples that no other current testbed\nmodel gets correct. Finally, we discuss several strategies for scaling\neffective robustness to the high-accuracy regime to improve the\nout-of-distribution accuracy of state-of-the-art models.",
          "link": "http://arxiv.org/abs/2106.15831",
          "publishedOn": "2021-07-01T01:59:31.828Z",
          "wordCount": 617,
          "title": "The Evolution of Out-of-Distribution Robustness Throughout Fine-Tuning. (arXiv:2106.15831v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15686",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aghdaie_P/0/1/0/all/0/1\">Poorya Aghdaie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhary_B/0/1/0/all/0/1\">Baaria Chaudhary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soleymani_S/0/1/0/all/0/1\">Sobhan Soleymani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dawson_J/0/1/0/all/0/1\">Jeremy Dawson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nasrabadi_N/0/1/0/all/0/1\">Nasser M. Nasrabadi</a>",
          "description": "Morphed images have exploited loopholes in the face recognition checkpoints,\ne.g., Credential Authentication Technology (CAT), used by Transportation\nSecurity Administration (TSA), which is a non-trivial security concern. To\novercome the risks incurred due to morphed presentations, we propose a\nwavelet-based morph detection methodology which adopts an end-to-end trainable\nsoft attention mechanism . Our attention-based deep neural network (DNN)\nfocuses on the salient Regions of Interest (ROI) which have the most spatial\nsupport for morph detector decision function, i.e, morph class binary softmax\noutput. A retrospective of morph synthesizing procedure aids us to speculate\nthe ROI as regions around facial landmarks , particularly for the case of\nlandmark-based morphing techniques. Moreover, our attention-based DNN is\nadapted to the wavelet space, where inputs of the network are coarse-to-fine\nspectral representations, 48 stacked wavelet sub-bands to be exact. We evaluate\nperformance of the proposed framework using three datasets, VISAPP17, LMA, and\nMorGAN. In addition, as attention maps can be a robust indicator whether a\nprobe image under investigation is genuine or counterfeit, we analyze the\nestimated attention maps for both a bona fide image and its corresponding\nmorphed image. Finally, we present an ablation study on the efficacy of\nutilizing attention mechanism for the sake of morph detection.",
          "link": "http://arxiv.org/abs/2106.15686",
          "publishedOn": "2021-07-01T01:59:31.806Z",
          "wordCount": 647,
          "title": "Attention Aware Wavelet-based Detection of Morphed Face Images. (arXiv:2106.15686v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15707",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wu_Y/0/1/0/all/0/1\">Yinzhe Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tang_Z/0/1/0/all/0/1\">Zeyu Tang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_B/0/1/0/all/0/1\">Binghuan Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Firmin_D/0/1/0/all/0/1\">David Firmin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_G/0/1/0/all/0/1\">Guang Yang</a>",
          "description": "Segmentation of cardiac fibrosis and scar are essential for clinical\ndiagnosis and can provide invaluable guidance for the treatment of cardiac\ndiseases. Late Gadolinium enhancement (LGE) cardiovascular magnetic resonance\n(CMR) has been successful for its efficacy in guiding the clinical diagnosis\nand treatment reliably. For LGE CMR, many methods have demonstrated success in\naccurately segmenting scarring regions. Co-registration with other\nnon-contrast-agent (non-CA) modalities, balanced steady-state free precession\n(bSSFP) and cine magnetic resonance imaging (MRI) for example, can further\nenhance the efficacy of automated segmentation of cardiac anatomies. Many\nconventional methods have been proposed to provide automated or semi-automated\nsegmentation of scars. With the development of deep learning in recent years,\nwe can also see more advanced methods that are more efficient in providing more\naccurate segmentations. This paper conducts a state-of-the-art review of\nconventional and current state-of-the-art approaches utilising different\nmodalities for accurate cardiac fibrosis and scar segmentation.",
          "link": "http://arxiv.org/abs/2106.15707",
          "publishedOn": "2021-07-01T01:59:31.800Z",
          "wordCount": 622,
          "title": "Recent Advances in Fibrosis and Scar Segmentation from Cardiac MRI: A State-of-the-Art Review and Future Perspectives. (arXiv:2106.15707v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2008.10796",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Yue_Z/0/1/0/all/0/1\">Zongsheng Yue</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yong_H/0/1/0/all/0/1\">Hongwei Yong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_Q/0/1/0/all/0/1\">Qian Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Meng_D/0/1/0/all/0/1\">Deyu Meng</a>",
          "description": "Deep neural networks (DNNs) have achieved significant success in image\nrestoration tasks by directly learning a powerful non-linear mapping from\ncorrupted images to their latent clean ones. However, there still exist two\nmajor limitations for these deep learning (DL)-based methods. Firstly, the\nnoises contained in real corrupted images are very complex, usually neglected\nand largely under-estimated in most current methods. Secondly, existing DL\nmethods are mostly trained on one pre-assumed degradation process for all of\nthe training image pairs, such as the widely used bicubic downsampling\nassumption in the image super-resolution task, inevitably leading to poor\ngeneralization performance when the true degradation does not match with such\nassumed one. To address these issues, we propose a unified generative model for\nthe image restoration, which elaborately configures the degradation process\nfrom the latent clean image to the observed corrupted one. Specifically,\ndifferent from most of current methods, the pixel-wisely non-i.i.d. Gaussian\ndistribution, being with more flexibility, is adopted in our method to fit the\ncomplex real noises. Furthermore, the method is built on the general image\ndegradation process, making it capable of adapting diverse degradations under\none single model. Besides, we design a variational inference algorithm to learn\nall parameters involved in the proposed model with explicit form of objective\nloss. Specifically, beyond traditional variational methodology, two DNNs are\nemployed to parameterize the posteriori distributions, one to infer the\ndistribution of the latent clean image, and another to infer the distribution\nof the image noise. Extensive experiments demonstrate the superiority of the\nproposed method on three classical image restoration tasks, including image\ndenoising, image super-resolution and JPEG image deblocking.",
          "link": "http://arxiv.org/abs/2008.10796",
          "publishedOn": "2021-06-30T02:01:02.194Z",
          "wordCount": 733,
          "title": "Variational Image Restoration Network. (arXiv:2008.10796v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11756",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Iyer_C/0/1/0/all/0/1\">C.V.Krishnakumar Iyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_F/0/1/0/all/0/1\">Feili Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Henry Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yonghong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_K/0/1/0/all/0/1\">Kay Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganguli_S/0/1/0/all/0/1\">Swetava Ganguli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pandey_V/0/1/0/all/0/1\">Vipul Pandey</a>",
          "description": "We present a no-code Artificial Intelligence (AI) platform called Trinity\nwith the main design goal of enabling both machine learning researchers and\nnon-technical geospatial domain experts to experiment with domain-specific\nsignals and datasets for solving a variety of complex problems on their own.\nThis versatility to solve diverse problems is achieved by transforming complex\nSpatio-temporal datasets to make them consumable by standard deep learning\nmodels, in this case, Convolutional Neural Networks (CNNs), and giving the\nability to formulate disparate problems in a standard way, eg. semantic\nsegmentation. With an intuitive user interface, a feature store that hosts\nderivatives of complex feature engineering, a deep learning kernel, and a\nscalable data processing mechanism, Trinity provides a powerful platform for\ndomain experts to share the stage with scientists and engineers in solving\nbusiness-critical problems. It enables quick prototyping, rapid experimentation\nand reduces the time to production by standardizing model building and\ndeployment. In this paper, we present our motivation behind Trinity and its\ndesign along with showcasing sample applications to motivate the idea of\nlowering the bar to using AI.",
          "link": "http://arxiv.org/abs/2106.11756",
          "publishedOn": "2021-06-30T02:01:02.188Z",
          "wordCount": 666,
          "title": "Trinity: A No-Code AI platform for complex spatial datasets. (arXiv:2106.11756v4 [cs.SE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.14202",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hanting Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunhe Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chunjing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tong Zhang</a>",
          "description": "Compared with cheap addition operation, multiplication operation is of much\nhigher computation complexity. The widely-used convolutions in deep neural\nnetworks are exactly cross-correlation to measure the similarity between input\nfeature and convolution filters, which involves massive multiplications between\nfloat values. In this paper, we present adder networks (AdderNets) to trade\nthese massive multiplications in deep neural networks, especially convolutional\nneural networks (CNNs), for much cheaper additions to reduce computation costs.\nIn AdderNets, we take the $\\ell_1$-norm distance between filters and input\nfeature as the output response. We first develop a theoretical foundation for\nAdderNets, by showing that both the single hidden layer AdderNet and the\nwidth-bounded deep AdderNet with ReLU activation functions are universal\nfunction approximators. An approximation bound for AdderNets with a single\nhidden layer is also presented. We further analyze the influence of this new\nsimilarity measure on the optimization of neural network and develop a special\ntraining scheme for AdderNets. Based on the gradient magnitude, an adaptive\nlearning rate strategy is proposed to enhance the training procedure of\nAdderNets. AdderNets can achieve a 75.7% Top-1 accuracy and a 92.3% Top-5\naccuracy using ResNet-50 on the ImageNet dataset without any multiplication in\nthe convolutional layer.",
          "link": "http://arxiv.org/abs/2105.14202",
          "publishedOn": "2021-06-30T02:01:02.169Z",
          "wordCount": 682,
          "title": "Universal Adder Neural Networks. (arXiv:2105.14202v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.03488",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nanni_L/0/1/0/all/0/1\">Loris Nanni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghidoni_S/0/1/0/all/0/1\">Stefano Ghidoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brahnam_S/0/1/0/all/0/1\">Sheryl Brahnam</a>",
          "description": "Features play a crucial role in computer vision. Initially designed to detect\nsalient elements by means of handcrafted algorithms, features are now often\nlearned by different layers in Convolutional Neural Networks (CNNs). This paper\ndevelops a generic computer vision system based on features extracted from\ntrained CNNs. Multiple learned features are combined into a single structure to\nwork on different image classification tasks. The proposed system was\nexperimentally derived by testing several approaches for extracting features\nfrom the inner layers of CNNs and using them as inputs to SVMs that are then\ncombined by sum rule. Dimensionality reduction techniques are used to reduce\nthe high dimensionality of inner layers. The resulting vision system is shown\nto significantly boost the performance of standard CNNs across a large and\ndiverse collection of image data sets. An ensemble of different topologies\nusing the same approach obtains state-of-the-art results on a virus data set.",
          "link": "http://arxiv.org/abs/2104.03488",
          "publishedOn": "2021-06-30T02:01:02.086Z",
          "wordCount": 612,
          "title": "Deep Features for training Support Vector Machine. (arXiv:2104.03488v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14591",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yixin Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yang Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lin_Z/0/1/0/all/0/1\">Zihao Lin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tian_J/0/1/0/all/0/1\">Jiang Tian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhong_C/0/1/0/all/0/1\">Cheng Zhong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shi_Z/0/1/0/all/0/1\">Zhongchao Shi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fan_J/0/1/0/all/0/1\">Jianping Fan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+He_Z/0/1/0/all/0/1\">Zhiqiang He</a>",
          "description": "Accurate segmentation of brain tumors from magnetic resonance imaging (MRI)\nis clinically relevant in diagnoses, prognoses and surgery treatment, which\nrequires multiple modalities to provide complementary morphological and\nphysiopathologic information. However, missing modality commonly occurs due to\nimage corruption, artifacts, different acquisition protocols or allergies to\ncertain contrast agents in clinical practice. Though existing efforts\ndemonstrate the possibility of a unified model for all missing situations, most\nof them perform poorly when more than one modality is missing. In this paper,\nwe propose a novel Adversarial Co-training Network (ACN) to solve this issue,\nin which a series of independent yet related models are trained dedicated to\neach missing situation with significantly better results. Specifically, ACN\nadopts a novel co-training network, which enables a coupled learning process\nfor both full modality and missing modality to supplement each other's domain\nand feature representations, and more importantly, to recover the `missing'\ninformation of absent modalities. Then, two unsupervised modules, i.e., entropy\nand knowledge adversarial learning modules are proposed to minimize the domain\ngap while enhancing prediction reliability and encouraging the alignment of\nlatent representations, respectively. We also adapt modality-mutual information\nknowledge transfer learning to ACN to retain the rich mutual information among\nmodalities. Extensive experiments on BraTS2018 dataset show that our proposed\nmethod significantly outperforms all state-of-the-art methods under any missing\nsituation.",
          "link": "http://arxiv.org/abs/2106.14591",
          "publishedOn": "2021-06-30T02:01:02.045Z",
          "wordCount": 700,
          "title": "ACN: Adversarial Co-training Network for Brain Tumor Segmentation with Missing Modalities. (arXiv:2106.14591v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.09808",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huan_L/0/1/0/all/0/1\">Linxi Huan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_N/0/1/0/all/0/1\">Nan Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1\">Xianwei Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_W/0/1/0/all/0/1\">Wei He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_J/0/1/0/all/0/1\">Jianya Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_G/0/1/0/all/0/1\">Gui-Song Xia</a>",
          "description": "This paper presents a context-aware tracing strategy (CATS) for crisp edge\ndetection with deep edge detectors, based on an observation that the\nlocalization ambiguity of deep edge detectors is mainly caused by the mixing\nphenomenon of convolutional neural networks: feature mixing in edge\nclassification and side mixing during fusing side predictions. The CATS\nconsists of two modules: a novel tracing loss that performs feature unmixing by\ntracing boundaries for better side edge learning, and a context-aware fusion\nblock that tackles the side mixing by aggregating the complementary merits of\nlearned side edges. Experiments demonstrate that the proposed CATS can be\nintegrated into modern deep edge detectors to improve localization accuracy.\nWith the vanilla VGG16 backbone, in terms of BSDS500 dataset, our CATS improves\nthe F-measure (ODS) of the RCF and BDCN deep edge detectors by 12% and 6%\nrespectively when evaluating without using the morphological non-maximal\nsuppression scheme for edge detection.",
          "link": "http://arxiv.org/abs/2011.09808",
          "publishedOn": "2021-06-30T02:01:02.040Z",
          "wordCount": 620,
          "title": "Unmixing Convolutional Features for Crisp Edge Detection. (arXiv:2011.09808v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.05535",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Perov_I/0/1/0/all/0/1\">Ivan Perov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_D/0/1/0/all/0/1\">Daiheng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chervoniy_N/0/1/0/all/0/1\">Nikolay Chervoniy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1\">Kunlin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marangonda_S/0/1/0/all/0/1\">Sugasa Marangonda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ume_C/0/1/0/all/0/1\">Chris Um&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dpfks_M/0/1/0/all/0/1\">Mr. Dpfks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Facenheim_C/0/1/0/all/0/1\">Carl Shift Facenheim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+RP_L/0/1/0/all/0/1\">Luis RP</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jian Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Sheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_P/0/1/0/all/0/1\">Pingyu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Bo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weiming Zhang</a>",
          "description": "Deepfake defense not only requires the research of detection but also\nrequires the efforts of generation methods. However, current deepfake methods\nsuffer the effects of obscure workflow and poor performance. To solve this\nproblem, we present DeepFaceLab, the current dominant deepfake framework for\nface-swapping. It provides the necessary tools as well as an easy-to-use way to\nconduct high-quality face-swapping. It also offers a flexible and loose\ncoupling structure for people who need to strengthen their pipeline with other\nfeatures without writing complicated boilerplate code. We detail the principles\nthat drive the implementation of DeepFaceLab and introduce its pipeline,\nthrough which every aspect of the pipeline can be modified painlessly by users\nto achieve their customization purpose. It is noteworthy that DeepFaceLab could\nachieve cinema-quality results with high fidelity. We demonstrate the advantage\nof our system by comparing our approach with other face-swapping methods.For\nmore information, please visit:https://github.com/iperov/DeepFaceLab/.",
          "link": "http://arxiv.org/abs/2005.05535",
          "publishedOn": "2021-06-30T02:01:02.035Z",
          "wordCount": 674,
          "title": "DeepFaceLab: Integrated, flexible and extensible face-swapping framework. (arXiv:2005.05535v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.03746",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Huangjie Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1\">Jiangchao Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hongxia Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chunyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Ya Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsang_I/0/1/0/all/0/1\">Ivor Tsang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jingren Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Mingyuan Zhou</a>",
          "description": "Contrastive learning (CL) is effective in learning data representations\nwithout label supervision, where the encoder needs to contrast each positive\nsample over multiple negative samples via a one-vs-many softmax cross-entropy\nloss. However, conventional CL is sensitive to how many negative samples are\nincluded and how they are selected. Proposed in this paper is a doubly CL\nstrategy that contrasts positive samples and negative ones within themselves\nseparately. We realize this strategy with contrastive attraction and\ncontrastive repulsion (CACR) makes the query not only exert a greater force to\nattract more distant positive samples but also do so to repel closer negative\nsamples. Theoretical analysis reveals the connection between CACR and CL from\nthe perspectives of both positive attraction and negative repulsion and shows\nthe benefits in both efficiency and robustness brought by separately\ncontrasting within the sampled positive and negative pairs. Extensive\nlarge-scale experiments on standard vision tasks show that CACR not only\nconsistently outperforms existing CL methods on benchmark datasets in\nrepresentation learning, but also provides interpretable contrastive weights,\ndemonstrating the efficacy of the proposed doubly contrastive strategy.",
          "link": "http://arxiv.org/abs/2105.03746",
          "publishedOn": "2021-06-30T02:01:02.024Z",
          "wordCount": 664,
          "title": "Contrastive Attraction and Contrastive Repulsion for Representation Learning. (arXiv:2105.03746v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.14240",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1\">Qi Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuang_K/0/1/0/all/0/1\">Kun Kuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_K/0/1/0/all/0/1\">Kelu Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yisen Wang</a>",
          "description": "Adversarial training is one of the most effective approaches to improve model\nrobustness against adversarial examples. However, previous works mainly focus\non the overall robustness of the model, and the in-depth analysis on the role\nof each class involved in adversarial training is still missing. In this paper,\nwe propose to analyze the class-wise robustness in adversarial training. First,\nwe provide a detailed diagnosis of adversarial training on six benchmark\ndatasets, i.e., MNIST, CIFAR-10, CIFAR-100, SVHN, STL-10 and ImageNet.\nSurprisingly, we find that there are remarkable robustness discrepancies among\nclasses, leading to unbalance/unfair class-wise robustness in the robust\nmodels. Furthermore, we keep investigating the relations between classes and\nfind that the unbalanced class-wise robustness is pretty consistent among\ndifferent attack and defense methods. Moreover, we observe that the stronger\nattack methods in adversarial learning achieve performance improvement mainly\nfrom a more successful attack on the vulnerable classes (i.e., classes with\nless robustness). Inspired by these interesting findings, we design a simple\nbut effective attack method based on the traditional PGD attack, named\nTemperature-PGD attack, which proposes to enlarge the robustness disparity\namong classes with a temperature factor on the confidence distribution of each\nimage. Experiments demonstrate our method can achieve a higher attack rate than\nthe PGD attack. Furthermore, from the defense perspective, we also make some\nmodifications in the training and inference phase to improve the robustness of\nthe most vulnerable class, so as to mitigate the large difference in class-wise\nrobustness. We believe our work can contribute to a more comprehensive\nunderstanding of adversarial training as well as rethinking the class-wise\nproperties in robust models.",
          "link": "http://arxiv.org/abs/2105.14240",
          "publishedOn": "2021-06-30T02:01:02.009Z",
          "wordCount": 754,
          "title": "Analysis and Applications of Class-wise Robustness in Adversarial Training. (arXiv:2105.14240v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.06115",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Barenboim_G/0/1/0/all/0/1\">Gabriela Barenboim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hirn_J/0/1/0/all/0/1\">Johannes Hirn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanz_V/0/1/0/all/0/1\">Veronica Sanz</a>",
          "description": "We explore whether Neural Networks (NNs) can {\\it discover} the presence of\nsymmetries as they learn to perform a task. For this, we train hundreds of NNs\non a {\\it decoy task} based on well-controlled Physics templates, where no\ninformation on symmetry is provided. We use the output from the last hidden\nlayer of all these NNs, projected to fewer dimensions, as the input for a\nsymmetry classification task, and show that information on symmetry had indeed\nbeen identified by the original NN without guidance. As an interdisciplinary\napplication of this procedure, we identify the presence and level of symmetry\nin artistic paintings from different styles such as those of Picasso, Pollock\nand Van Gogh.",
          "link": "http://arxiv.org/abs/2103.06115",
          "publishedOn": "2021-06-30T02:01:01.980Z",
          "wordCount": 579,
          "title": "Symmetry meets AI. (arXiv:2103.06115v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08917",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khan_N/0/1/0/all/0/1\">Numair Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Min H. Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tompkin_J/0/1/0/all/0/1\">James Tompkin</a>",
          "description": "We present a method to estimate dense depth by optimizing a sparse set of\npoints such that their diffusion into a depth map minimizes a multi-view\nreprojection error from RGB supervision. We optimize point positions, depths,\nand weights with respect to the loss by differential splatting that models\npoints as Gaussians with analytic transmittance. Further, we develop an\nefficient optimization routine that can simultaneously optimize the 50k+ points\nrequired for complex scene reconstruction. We validate our routine using ground\ntruth data and show high reconstruction quality. Then, we apply this to light\nfield and wider baseline images via self supervision, and show improvements in\nboth average and outlier error for depth maps diffused from inaccurate sparse\npoints. Finally, we compare qualitative and quantitative results to image\nprocessing and deep learning methods. this http URL",
          "link": "http://arxiv.org/abs/2106.08917",
          "publishedOn": "2021-06-30T02:01:01.953Z",
          "wordCount": 592,
          "title": "Differentiable Diffusion for Dense Depth Estimation from Multi-view Images. (arXiv:2106.08917v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.05562",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bao_L/0/1/0/all/0/1\">Linchao Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xiangkai Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yajing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haoxian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhe_X/0/1/0/all/0/1\">Xuefei Zhe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_D/0/1/0/all/0/1\">Di Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Haozhi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xinwei Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhengyou Zhang</a>",
          "description": "We present a fully automatic system that can produce high-fidelity,\nphoto-realistic 3D digital human heads with a consumer RGB-D selfie camera. The\nsystem only needs the user to take a short selfie RGB-D video while rotating\nhis/her head, and can produce a high quality head reconstruction in less than\n30 seconds. Our main contribution is a new facial geometry modeling and\nreflectance synthesis procedure that significantly improves the\nstate-of-the-art. Specifically, given the input video a two-stage frame\nselection procedure is first employed to select a few high-quality frames for\nreconstruction. Then a differentiable renderer based 3D Morphable Model (3DMM)\nfitting algorithm is applied to recover facial geometries from multiview RGB-D\ndata, which takes advantages of a powerful 3DMM basis constructed with\nextensive data generation and perturbation. Our 3DMM has much larger expressive\ncapacities than conventional 3DMM, allowing us to recover more accurate facial\ngeometry using merely linear basis. For reflectance synthesis, we present a\nhybrid approach that combines parametric fitting and CNNs to synthesize\nhigh-resolution albedo/normal maps with realistic hair/pore/wrinkle details.\nResults show that our system can produce faithful 3D digital human faces with\nextremely realistic details. The main code and the newly constructed 3DMM basis\nis publicly available.",
          "link": "http://arxiv.org/abs/2010.05562",
          "publishedOn": "2021-06-30T02:01:01.935Z",
          "wordCount": 692,
          "title": "High-Fidelity 3D Digital Human Head Creation from RGB-D Selfies. (arXiv:2010.05562v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.00916",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1\">Yuhu Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yingying Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_M/0/1/0/all/0/1\">Mingzhi Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yujiang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yutian Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_Q/0/1/0/all/0/1\">Qin Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dick_R/0/1/0/all/0/1\">Robert P. Dick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1\">Tun Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_N/0/1/0/all/0/1\">Ning Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_L/0/1/0/all/0/1\">Li Shang</a>",
          "description": "This work presents MemX: a biologically-inspired attention-aware eyewear\nsystem developed with the goal of pursuing the long-awaited vision of a\npersonalized visual Memex. MemX captures human visual attention on the fly,\nanalyzes the salient visual content, and records moments of personal interest\nin the form of compact video snippets. Accurate attentive scene detection and\nanalysis on resource-constrained platforms is challenging because these tasks\nare computation and energy intensive. We propose a new temporal visual\nattention network that unifies human visual attention tracking and salient\nvisual content analysis. Attention tracking focuses computation-intensive video\nanalysis on salient regions, while video analysis makes human attention\ndetection and tracking more accurate. Using the YouTube-VIS dataset and 30\nparticipants, we experimentally show that MemX significantly improves the\nattention tracking accuracy over the eye-tracking-alone method, while\nmaintaining high system energy efficiency. We have also conducted 11 in-field\npilot studies across a range of daily usage scenarios, which demonstrate the\nfeasibility and potential benefits of MemX.",
          "link": "http://arxiv.org/abs/2105.00916",
          "publishedOn": "2021-06-30T02:01:01.913Z",
          "wordCount": 680,
          "title": "MemX: An Attention-Aware Smart Eyewear System for Personalized Moment Auto-capture. (arXiv:2105.00916v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15610",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gabbay_A/0/1/0/all/0/1\">Aviv Gabbay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_N/0/1/0/all/0/1\">Niv Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoshen_Y/0/1/0/all/0/1\">Yedid Hoshen</a>",
          "description": "Unsupervised disentanglement has been shown to be theoretically impossible\nwithout inductive biases on the models and the data. As an alternative\napproach, recent methods rely on limited supervision to disentangle the factors\nof variation and allow their identifiability. While annotating the true\ngenerative factors is only required for a limited number of observations, we\nargue that it is infeasible to enumerate all the factors of variation that\ndescribe a real-world image distribution. To this end, we propose a method for\ndisentangling a set of factors which are only partially labeled, as well as\nseparating the complementary set of residual factors that are never explicitly\nspecified. Our success in this challenging setting, demonstrated on synthetic\nbenchmarks, gives rise to leveraging off-the-shelf image descriptors to\npartially annotate a subset of attributes in real image domains (e.g. of human\nfaces) with minimal manual effort. Specifically, we use a recent language-image\nembedding model (CLIP) to annotate a set of attributes of interest in a\nzero-shot manner and demonstrate state-of-the-art disentangled image\nmanipulation results.",
          "link": "http://arxiv.org/abs/2106.15610",
          "publishedOn": "2021-06-30T02:01:01.891Z",
          "wordCount": 625,
          "title": "An Image is Worth More Than a Thousand Words: Towards Disentanglement in the Wild. (arXiv:2106.15610v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2104.13826",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Raffy_P/0/1/0/all/0/1\">Philippe Raffy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pambrun_J/0/1/0/all/0/1\">Jean-Fran&#xe7;ois Pambrun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kumar_A/0/1/0/all/0/1\">Ashish Kumar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dubois_D/0/1/0/all/0/1\">David Dubois</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Patti_J/0/1/0/all/0/1\">Jay Waldron Patti</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cairns_R/0/1/0/all/0/1\">Robyn Alexandra Cairns</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Young_R/0/1/0/all/0/1\">Ryan Young</a>",
          "description": "Standardized body region labelling of individual images provides data that\ncan improve human and computer use of medical images. A CNN-based classifier\nwas developed to identify body regions in CT and MRI. 17 CT (18 MRI) body\nregions covering the entire human body were defined for the classification\ntask. Three retrospective databases were built for the AI model training,\nvalidation, and testing, with a balanced distribution of studies per body\nregion. The test databases originated from a different healthcare network.\nAccuracy, recall and precision of the classifier was evaluated for patient age,\npatient gender, institution, scanner manufacturer, contrast, slice thickness,\nMRI sequence, and CT kernel. The data included a retrospective cohort of 2,934\nanonymized CT cases (training: 1,804 studies, validation: 602 studies, test:\n528 studies) and 3,185 anonymized MRI cases (training: 1,911 studies,\nvalidation: 636 studies, test: 638 studies). 27 institutions from primary care\nhospitals, community hospitals and imaging centers contributed to the test\ndatasets. The data included cases of all genders in equal proportions and\nsubjects aged from a few months old to +90 years old. An image-level prediction\naccuracy of 91.9% (90.2 - 92.1) for CT, and 94.2% (92.0 - 95.6) for MRI was\nachieved. The classification results were robust across all body regions and\nconfounding factors. Due to limited data, performance results for subjects\nunder 10 years-old could not be reliably evaluated. We show that deep learning\nmodels can classify CT and MRI images by body region including lower and upper\nextremities with high accuracy.",
          "link": "http://arxiv.org/abs/2104.13826",
          "publishedOn": "2021-06-30T02:01:01.885Z",
          "wordCount": 732,
          "title": "Deep Learning Body Region Classification of MRI and CT examinations. (arXiv:2104.13826v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.09576",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Saem Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_D/0/1/0/all/0/1\">Donghoon Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwak_N/0/1/0/all/0/1\">Nojun Kwak</a>",
          "description": "Video frame interpolation is the task of creating an interframe between two\nadjacent frames along the time axis. So, instead of simply averaging two\nadjacent frames to create an intermediate image, this operation should maintain\nsemantic continuity with the adjacent frames. Most conventional methods use\noptical flow, and various tools such as occlusion handling and object smoothing\nare indispensable. Since the use of these various tools leads to complex\nproblems, we tried to tackle the video interframe generation problem without\nusing problematic optical flow . To enable this , we have tried to use a deep\nneural network with an invertible structure, and developed an U-Net based\nGenerative Flow which is a modified normalizing flow. In addition, we propose a\nlearning method with a new consistency loss in the latent space to maintain\nsemantic temporal consistency between frames. The resolution of the generated\nimage is guaranteed to be identical to that of the original images by using an\ninvertible network. Furthermore, as it is not a random image like the ones by\ngenerative models, our network guarantees stable outputs without flicker.\nThrough experiments, we \\sam {confirmed the feasibility of the proposed\nalgorithm and would like to suggest the U-Net based Generative Flow as a new\npossibility for baseline in video frame interpolation. This paper is meaningful\nin that it is the world's first attempt to use invertible networks instead of\noptical flows for video interpolation.",
          "link": "http://arxiv.org/abs/2103.09576",
          "publishedOn": "2021-06-30T02:01:01.879Z",
          "wordCount": 718,
          "title": "The U-Net based GLOW for Optical-Flow-free Video Interframe Generation. (arXiv:2103.09576v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.00561",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tianxiao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1\">Wenchi Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guanghui Wang</a>",
          "description": "Most deep learning models are data-driven and the excellent performance is\nhighly dependent on the abundant and diverse datasets. However, it is very hard\nto obtain and label the datasets of some specific scenes or applications. If we\ntrain the detector using the data from one domain, it cannot perform well on\nthe data from another domain due to domain shift, which is one of the big\nchallenges of most object detection models. To address this issue, some\nimage-to-image translation techniques have been employed to generate some fake\ndata of some specific scenes to train the models. With the advent of Generative\nAdversarial Networks (GANs), we could realize unsupervised image-to-image\ntranslation in both directions from a source to a target domain and from the\ntarget to the source domain. In this study, we report a new approach to making\nuse of the generated images. We propose to concatenate the original 3-channel\nimages and their corresponding GAN-generated fake images to form 6-channel\nrepresentations of the dataset, hoping to address the domain shift problem\nwhile exploiting the success of available detection models. The idea of\naugmented data representation may inspire further study on object detection and\nother applications.",
          "link": "http://arxiv.org/abs/2101.00561",
          "publishedOn": "2021-06-30T02:01:01.863Z",
          "wordCount": 655,
          "title": "Six-channel Image Representation for Cross-domain Object Detection. (arXiv:2101.00561v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.09008",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shyh_Yaw_J/0/1/0/all/0/1\">Jou Shyh-Yaw</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_Yen_S/0/1/0/all/0/1\">Su Chung-Yen</a>",
          "description": "In the paper of ExquisiteNetV1, the ability of classification of\nExquisiteNetV1 is worse than DenseNet. In this article, we propose a faster and\nbetter model ExquisiteNetV2. We conduct many experiments to evaluate its\nperformance. We test ExquisiteNetV2, ExquisiteNetV1 and other 9 well-known\nmodels on 15 credible datasets under the same condition. According to the\nexperimental results, ExquisiteNetV2 gets the highest classification accuracy\nover half of the datasets. Important of all, ExquisiteNetV2 has fewest amounts\nof parameters. Besides, in most instances, ExquisiteNetV2 has fastest computing\nspeed.",
          "link": "http://arxiv.org/abs/2105.09008",
          "publishedOn": "2021-06-30T02:01:01.841Z",
          "wordCount": 561,
          "title": "A Novel lightweight Convolutional Neural Network, ExquisiteNetV2. (arXiv:2105.09008v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.11530",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_P/0/1/0/all/0/1\">Pranay Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_D/0/1/0/all/0/1\">Divyanshu Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarvadevabhatla_R/0/1/0/all/0/1\">Ravi Kiran Sarvadevabhatla</a>",
          "description": "We introduce SynSE, a novel syntactically guided generative approach for\nZero-Shot Learning (ZSL). Our end-to-end approach learns progressively refined\ngenerative embedding spaces constrained within and across the involved\nmodalities (visual, language). The inter-modal constraints are defined between\naction sequence embedding and embeddings of Parts of Speech (PoS) tagged words\nin the corresponding action description. We deploy SynSE for the task of\nskeleton-based action sequence recognition. Our design choices enable SynSE to\ngeneralize compositionally, i.e., recognize sequences whose action descriptions\ncontain words not encountered during training. We also extend our approach to\nthe more challenging Generalized Zero-Shot Learning (GZSL) problem via a\nconfidence-based gating mechanism. We are the first to present zero-shot\nskeleton action recognition results on the large-scale NTU-60 and NTU-120\nskeleton action datasets with multiple splits. Our results demonstrate SynSE's\nstate of the art performance in both ZSL and GZSL settings compared to strong\nbaselines on the NTU-60 and NTU-120 datasets. The code and pretrained models\nare available at https://github.com/skelemoa/synse-zsl",
          "link": "http://arxiv.org/abs/2101.11530",
          "publishedOn": "2021-06-30T02:01:01.808Z",
          "wordCount": 646,
          "title": "Syntactically Guided Generative Embeddings for Zero-Shot Skeleton Action Recognition. (arXiv:2101.11530v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14844",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Lu_Y/0/1/0/all/0/1\">Yucheng Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jung_S/0/1/0/all/0/1\">Seung-Won Jung</a>",
          "description": "Low-light imaging on mobile devices is typically challenging due to\ninsufficient incident light coming through the relatively small aperture,\nresulting in a low signal-to-noise ratio. Most of the previous works on\nlow-light image processing focus either only on a single task such as\nillumination adjustment, color enhancement, or noise removal; or on a joint\nillumination adjustment and denoising task that heavily relies on short-long\nexposure image pairs collected from specific camera models, and thus these\napproaches are less practical and generalizable in real-world settings where\ncamera-specific joint enhancement and restoration is required. To tackle this\nproblem, in this paper, we propose a low-light image processing framework that\nperforms joint illumination adjustment, color enhancement, and denoising.\nConsidering the difficulty in model-specific data collection and the ultra-high\ndefinition of the captured images, we design two branches: a coefficient\nestimation branch as well as a joint enhancement and denoising branch. The\ncoefficient estimation branch works in a low-resolution space and predicts the\ncoefficients for enhancement via bilateral learning, whereas the joint\nenhancement and denoising branch works in a full-resolution space and performs\njoint enhancement and denoising in a progressive manner. In contrast to\nexisting methods, our framework does not need to recollect massive data when\nbeing adapted to another camera model, which significantly reduces the efforts\nrequired to fine-tune our approach for practical usage. Through extensive\nexperiments, we demonstrate its great potential in real-world low-light imaging\napplications when compared with current state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2106.14844",
          "publishedOn": "2021-06-30T02:01:01.801Z",
          "wordCount": 702,
          "title": "Progressive Joint Low-light Enhancement and Noise Removal for Raw Images. (arXiv:2106.14844v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.07287",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Savarese_P/0/1/0/all/0/1\">Pedro Savarese</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sunnie S. Y. Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maire_M/0/1/0/all/0/1\">Michael Maire</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shakhnarovich_G/0/1/0/all/0/1\">Greg Shakhnarovich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McAllester_D/0/1/0/all/0/1\">David McAllester</a>",
          "description": "We study image segmentation from an information-theoretic perspective,\nproposing a novel adversarial method that performs unsupervised segmentation by\npartitioning images into maximally independent sets. More specifically, we\ngroup image pixels into foreground and background, with the goal of minimizing\npredictability of one set from the other. An easily computed loss drives a\ngreedy search process to maximize inpainting error over these partitions. Our\nmethod does not involve training deep networks, is computationally cheap,\nclass-agnostic, and even applicable in isolation to a single unlabeled image.\nExperiments demonstrate that it achieves a new state-of-the-art in unsupervised\nsegmentation quality, while being substantially faster and more general than\ncompeting approaches.",
          "link": "http://arxiv.org/abs/2012.07287",
          "publishedOn": "2021-06-30T02:01:01.783Z",
          "wordCount": 589,
          "title": "Information-Theoretic Segmentation by Inpainting Error Maximization. (arXiv:2012.07287v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.01619",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Long_Y/0/1/0/all/0/1\">Yonghao Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jie Ying Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_B/0/1/0/all/0/1\">Bo Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Yueming Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Unberath_M/0/1/0/all/0/1\">Mathias Unberath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yun-Hui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heng_P/0/1/0/all/0/1\">Pheng Ann Heng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_Q/0/1/0/all/0/1\">Qi Dou</a>",
          "description": "Automatic surgical gesture recognition is fundamentally important to enable\nintelligent cognitive assistance in robotic surgery. With recent advancement in\nrobot-assisted minimally invasive surgery, rich information including surgical\nvideos and robotic kinematics can be recorded, which provide complementary\nknowledge for understanding surgical gestures. However, existing methods either\nsolely adopt uni-modal data or directly concatenate multi-modal\nrepresentations, which can not sufficiently exploit the informative\ncorrelations inherent in visual and kinematics data to boost gesture\nrecognition accuracies. In this regard, we propose a novel online approach of\nmulti-modal relational graph network (i.e., MRG-Net) to dynamically integrate\nvisual and kinematics information through interactive message propagation in\nthe latent feature space. In specific, we first extract embeddings from video\nand kinematics sequences with temporal convolutional networks and LSTM units.\nNext, we identify multi-relations in these multi-modal embeddings and leverage\nthem through a hierarchical relational graph learning module. The effectiveness\nof our method is demonstrated with state-of-the-art results on the public\nJIGSAWS dataset, outperforming current uni-modal and multi-modal methods on\nboth suturing and knot typing tasks. Furthermore, we validated our method on\nin-house visual-kinematics datasets collected with da Vinci Research Kit (dVRK)\nplatforms in two centers, with consistent promising performance achieved.",
          "link": "http://arxiv.org/abs/2011.01619",
          "publishedOn": "2021-06-30T02:01:01.777Z",
          "wordCount": 701,
          "title": "Relational Graph Learning on Visual and Kinematics Embeddings for Accurate Gesture Recognition in Robotic Surgery. (arXiv:2011.01619v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1912.13200",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hanting Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunhe Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chunjing Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1\">Boxin Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1\">Qi Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chang Xu</a>",
          "description": "Compared with cheap addition operation, multiplication operation is of much\nhigher computation complexity. The widely-used convolutions in deep neural\nnetworks are exactly cross-correlation to measure the similarity between input\nfeature and convolution filters, which involves massive multiplications between\nfloat values. In this paper, we present adder networks (AdderNets) to trade\nthese massive multiplications in deep neural networks, especially convolutional\nneural networks (CNNs), for much cheaper additions to reduce computation costs.\nIn AdderNets, we take the $\\ell_1$-norm distance between filters and input\nfeature as the output response. The influence of this new similarity measure on\nthe optimization of neural network have been thoroughly analyzed. To achieve a\nbetter performance, we develop a special back-propagation approach for\nAdderNets by investigating the full-precision gradient. We then propose an\nadaptive learning rate strategy to enhance the training procedure of AdderNets\naccording to the magnitude of each neuron's gradient. As a result, the proposed\nAdderNets can achieve 74.9% Top-1 accuracy 91.7% Top-5 accuracy using ResNet-50\non the ImageNet dataset without any multiplication in convolution layer. The\ncodes are publicly available at: https://github.com/huaweinoah/AdderNet.",
          "link": "http://arxiv.org/abs/1912.13200",
          "publishedOn": "2021-06-30T02:01:01.771Z",
          "wordCount": 685,
          "title": "AdderNet: Do We Really Need Multiplications in Deep Learning?. (arXiv:1912.13200v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15537",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rajput_G/0/1/0/all/0/1\">Gaurav Rajput</a>, <a href=\"http://arxiv.org/find/cs/1/au:+punn_N/0/1/0/all/0/1\">Narinder Singh punn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sonbhadra_S/0/1/0/all/0/1\">Sanjay Kumar Sonbhadra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1\">Sonali Agarwal</a>",
          "description": "With increasing popularity of social media platforms hate speech is emerging\nas a major concern, where it expresses abusive speech that targets specific\ngroup characteristics, such as gender, religion or ethnicity to spread\nviolence. Earlier people use to verbally deliver hate speeches but now with the\nexpansion of technology, some people are deliberately using social media\nplatforms to spread hate by posting, sharing, commenting, etc. Whether it is\nChristchurch mosque shootings or hate crimes against Asians in west, it has\nbeen observed that the convicts are very much influenced from hate text present\nonline. Even though AI systems are in place to flag such text but one of the\nkey challenges is to reduce the false positive rate (marking non hate as hate),\nso that these systems can detect hate speech without undermining the freedom of\nexpression. In this paper, we use ETHOS hate speech detection dataset and\nanalyze the performance of hate speech detection classifier by replacing or\nintegrating the word embeddings (fastText (FT), GloVe (GV) or FT + GV) with\nstatic BERT embeddings (BE). With the extensive experimental trails it is\nobserved that the neural network performed better with static BE compared to\nusing FT, GV or FT + GV as word embeddings. In comparison to fine-tuned BERT,\none metric that significantly improved is specificity.",
          "link": "http://arxiv.org/abs/2106.15537",
          "publishedOn": "2021-06-30T02:01:01.765Z",
          "wordCount": 655,
          "title": "Hate speech detection using static BERT embeddings. (arXiv:2106.15537v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2006.07034",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Weis_M/0/1/0/all/0/1\">Marissa A. Weis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chitta_K/0/1/0/all/0/1\">Kashyap Chitta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_Y/0/1/0/all/0/1\">Yash Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brendel_W/0/1/0/all/0/1\">Wieland Brendel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bethge_M/0/1/0/all/0/1\">Matthias Bethge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geiger_A/0/1/0/all/0/1\">Andreas Geiger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ecker_A/0/1/0/all/0/1\">Alexander S. Ecker</a>",
          "description": "Perceiving the world in terms of objects and tracking them through time is a\ncrucial prerequisite for reasoning and scene understanding. Recently, several\nmethods have been proposed for unsupervised learning of object-centric\nrepresentations. However, since these models were evaluated on different\ndownstream tasks, it remains unclear how they compare in terms of basic\nperceptual abilities such as detection, figure-ground segmentation and tracking\nof objects. To close this gap, we design a benchmark with four data sets of\nvarying complexity and seven additional test sets featuring challenging\ntracking scenarios relevant for natural videos. Using this benchmark, we\ncompare the perceptual abilities of four object-centric approaches: ViMON, a\nvideo-extension of MONet, based on recurrent spatial attention, OP3, which\nexploits clustering via spatial mixture models, as well as TBA and SCALOR,\nwhich use explicit factorization via spatial transformers. Our results suggest\nthat the architectures with unconstrained latent representations learn more\npowerful representations in terms of object detection, segmentation and\ntracking than the spatial transformer based architectures. We also observe that\nnone of the methods are able to gracefully handle the most challenging tracking\nscenarios despite their synthetic nature, suggesting that our benchmark may\nprovide fruitful guidance towards learning more robust object-centric video\nrepresentations.",
          "link": "http://arxiv.org/abs/2006.07034",
          "publishedOn": "2021-06-30T02:01:01.759Z",
          "wordCount": 674,
          "title": "Benchmarking Unsupervised Object Representations for Video Sequences. (arXiv:2006.07034v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.14924",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Maanpaa_J/0/1/0/all/0/1\">Jyri Maanp&#xe4;&#xe4;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taher_J/0/1/0/all/0/1\">Josef Taher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manninen_P/0/1/0/all/0/1\">Petri Manninen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pakola_L/0/1/0/all/0/1\">Leo Pakola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Melekhov_I/0/1/0/all/0/1\">Iaroslav Melekhov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hyyppa_J/0/1/0/all/0/1\">Juha Hyypp&#xe4;</a>",
          "description": "Autonomous driving is challenging in adverse road and weather conditions in\nwhich there might not be lane lines, the road might be covered in snow and the\nvisibility might be poor. We extend the previous work on end-to-end learning\nfor autonomous steering to operate in these adverse real-life conditions with\nmultimodal data. We collected 28 hours of driving data in several road and\nweather conditions and trained convolutional neural networks to predict the car\nsteering wheel angle from front-facing color camera images and lidar range and\nreflectance data. We compared the CNN model performances based on the different\nmodalities and our results show that the lidar modality improves the\nperformances of different multimodal sensor-fusion models. We also performed\non-road tests with different models and they support this observation.",
          "link": "http://arxiv.org/abs/2010.14924",
          "publishedOn": "2021-06-30T02:01:01.743Z",
          "wordCount": 638,
          "title": "Multimodal End-to-End Learning for Autonomous Steering in Adverse Road and Weather Conditions. (arXiv:2010.14924v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15599",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Thakur_N/0/1/0/all/0/1\">Nirmalya Thakur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_C/0/1/0/all/0/1\">Chia Y. Han</a>",
          "description": "The population of elderly people has been increasing at a rapid rate over the\nlast few decades and their population is expected to further increase in the\nupcoming future. Their increasing population is associated with their\nincreasing needs due to problems like physical disabilities, cognitive issues,\nweakened memory and disorganized behavior, that elderly people face with\nincreasing age. To reduce their financial burden on the world economy and to\nenhance their quality of life, it is essential to develop technology-based\nsolutions that are adaptive, assistive and intelligent in nature. Intelligent\nAffect Aware Systems that can not only analyze but also predict the behavior of\nelderly people in the context of their day to day interactions with technology\nin an IoT-based environment, holds immense potential for serving as a long-term\nsolution for improving the user experience of elderly in smart homes. This work\ntherefore proposes the framework for an Intelligent Affect Aware environment\nfor elderly people that can not only analyze the affective components of their\ninteractions but also predict their likely user experience even before they\nstart engaging in any activity in the given smart home environment. This\nforecasting of user experience would provide scope for enhancing the same,\nthereby increasing the assistive and adaptive nature of such intelligent\nsystems. To uphold the efficacy of this proposed framework for improving the\nquality of life of elderly people in smart homes, it has been tested on three\ndatasets and the results are presented and discussed.",
          "link": "http://arxiv.org/abs/2106.15599",
          "publishedOn": "2021-06-30T02:01:01.737Z",
          "wordCount": 723,
          "title": "Framework for an Intelligent Affect Aware Smart Home Environment for Elderly People. (arXiv:2106.15599v1 [cs.HC])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15409",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Symeonidis_C/0/1/0/all/0/1\">C. Symeonidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nousi_P/0/1/0/all/0/1\">P. Nousi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tosidis_P/0/1/0/all/0/1\">P. Tosidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsampazis_K/0/1/0/all/0/1\">K. Tsampazis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Passalis_N/0/1/0/all/0/1\">N. Passalis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tefas_A/0/1/0/all/0/1\">A. Tefas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nikolaidis_N/0/1/0/all/0/1\">N. Nikolaidis</a>",
          "description": "The performance of supervised deep learning algorithms depends significantly\non the scale, quality and diversity of the data used for their training.\nCollecting and manually annotating large amount of data can be both\ntime-consuming and costly tasks to perform. In the case of tasks related to\nvisual human-centric perception, the collection and distribution of such data\nmay also face restrictions due to legislation regarding privacy. In addition,\nthe design and testing of complex systems, e.g., robots, which often employ\ndeep learning-based perception models, may face severe difficulties as even\nstate-of-the-art methods trained on real and large-scale datasets cannot always\nperform adequately as they have not adapted to the visual differences between\nthe virtual and the real world data. As an attempt to tackle and mitigate the\neffect of these issues, we present a method that automatically generates\nrealistic synthetic data with annotations for a) person detection, b) face\nrecognition, and c) human pose estimation. The proposed method takes as input\nreal background images and populates them with human figures in various poses.\nInstead of using hand-made 3D human models, we propose the use of models\ngenerated through deep learning methods, further reducing the dataset creation\ncosts, while maintaining a high level of realism. In addition, we provide\nopen-source and easy to use tools that implement the proposed pipeline,\nallowing for generating highly-realistic synthetic datasets for a variety of\ntasks. A benchmarking and evaluation in the corresponding tasks shows that\nsynthetic data can be effectively used as a supplement to real data.",
          "link": "http://arxiv.org/abs/2106.15409",
          "publishedOn": "2021-06-30T02:01:01.585Z",
          "wordCount": 700,
          "title": "Efficient Realistic Data Generation Framework leveraging Deep Learning-based Human Digitization. (arXiv:2106.15409v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15553",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hoppe_A/0/1/0/all/0/1\">Anett Hoppe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morris_D/0/1/0/all/0/1\">David Morris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ewerth_R/0/1/0/all/0/1\">Ralph Ewerth</a>",
          "description": "Illustrations are widely used in education, and sometimes, alternatives are\nnot available for visually impaired students. Therefore, those students would\nbenefit greatly from an automatic illustration description system, but only if\nthose descriptions were complete, correct, and easily understandable using a\nscreenreader. In this paper, we report on a study for the assessment of\nautomated image descriptions. We interviewed experts to establish evaluation\ncriteria, which we then used to create an evaluation questionnaire for sighted\nnon-expert raters, and description templates. We used this questionnaire to\nevaluate the quality of descriptions which could be generated with a\ntemplate-based automatic image describer. We present evidence that these\ntemplates have the potential to generate useful descriptions, and that the\nquestionnaire identifies problems with description templates.",
          "link": "http://arxiv.org/abs/2106.15553",
          "publishedOn": "2021-06-30T02:01:01.578Z",
          "wordCount": 635,
          "title": "Evaluation of Automated Image Descriptions for Visually Impaired Students. (arXiv:2106.15553v1 [cs.HC])"
        },
        {
          "id": "http://arxiv.org/abs/1908.01931",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1\">Qian Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1\">Yuhua Qian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xinyan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+She_Y/0/1/0/all/0/1\">Yanhong She</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Deyu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Jiye Liang</a>",
          "description": "Logic reasoning is a significant ability of human intelligence and also an\nimportant task in artificial intelligence. The existing logic reasoning\nmethods, quite often, need to design some reasoning patterns beforehand. This\nhas led to an interesting question: can logic reasoning patterns be directly\nlearned from given data? The problem is termed as a data concept logic. In this\nstudy, a learning logic task from images, called a LiLi task, first is\nproposed. This task is to learn and reason the logic relation from images,\nwithout presetting any reasoning patterns. As a preliminary exploration, we\ndesign six LiLi data sets (Bitwise And, Bitwise Or, Bitwise Xor, Addition,\nSubtraction and Multiplication), in which each image is embedded with a n-digit\nnumber. It is worth noting that a learning model beforehand does not know the\nmeaning of the n-digit numbers embedded in images and the relation between the\ninput images and the output image. In order to tackle the task, in this work we\nuse many typical neural network models and produce fruitful results. However,\nthese models have the poor performances on the difficult logic task. For\nfurthermore addressing this task, a novel network framework called a divide and\nconquer model by adding some label information is designed, achieving a high\ntesting accuracy.",
          "link": "http://arxiv.org/abs/1908.01931",
          "publishedOn": "2021-06-30T02:01:01.546Z",
          "wordCount": 676,
          "title": "Logic could be learned from images. (arXiv:1908.01931v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.09787",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Robinson_C/0/1/0/all/0/1\">Caleb Robinson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ortiz_A/0/1/0/all/0/1\">Anthony Ortiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferres_J/0/1/0/all/0/1\">Juan M. Lavista Ferres</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anderson_B/0/1/0/all/0/1\">Brandon Anderson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ho_D/0/1/0/all/0/1\">Daniel E. Ho</a>",
          "description": "Longitudinal studies are vital to understanding dynamic changes of the\nplanet, but labels (e.g., buildings, facilities, roads) are often available\nonly for a single point in time. We propose a general model, Temporal Cluster\nMatching (TCM), for detecting building changes in time series of remotely\nsensed imagery when footprint labels are observed only once. The intuition\nbehind the model is that the relationship between spectral values inside and\noutside of building's footprint will change when a building is constructed (or\ndemolished). For instance, in rural settings, the pre-construction area may\nlook similar to the surrounding environment until the building is constructed.\nSimilarly, in urban settings, the pre-construction areas will look different\nfrom the surrounding environment until construction. We further propose a\nheuristic method for selecting the parameters of our model which allows it to\nbe applied in novel settings without requiring data labeling efforts (to fit\nthe parameters). We apply our model over a dataset of poultry barns from\n2016/2017 high-resolution aerial imagery in the Delmarva Peninsula and a\ndataset of solar farms from a 2020 mosaic of Sentinel 2 imagery in India. Our\nresults show that our model performs as well when fit using the proposed\nheuristic as it does when fit with labeled data, and further, that supervised\nversions of our model perform the best among all the baselines we test against.\nFinally, we show that our proposed approach can act as an effective data\naugmentation strategy -- it enables researchers to augment existing structure\nfootprint labels along the time dimension and thus use imagery from multiple\npoints in time to train deep learning models. We show that this improves the\nspatial generalization of such models when evaluated on the same change\ndetection task.",
          "link": "http://arxiv.org/abs/2103.09787",
          "publishedOn": "2021-06-30T02:01:01.521Z",
          "wordCount": 765,
          "title": "Temporal Cluster Matching for Change Detection of Structures from Satellite Imagery. (arXiv:2103.09787v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15328",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1\">Qingtian Zhu</a>",
          "description": "3D reconstruction has lately attracted increasing attention due to its wide\napplication in many areas, such as autonomous driving, robotics and virtual\nreality. As a dominant technique in artificial intelligence, deep learning has\nbeen successfully adopted to solve various computer vision problems. However,\ndeep learning for 3D reconstruction is still at its infancy due to its unique\nchallenges and varying pipelines. To stimulate future research, this paper\npresents a review of recent progress in deep learning methods for Multi-view\nStereo (MVS), which is considered as a crucial task of image-based 3D\nreconstruction. It also presents comparative results on several publicly\navailable datasets, with insightful observations and inspiring future research\ndirections.",
          "link": "http://arxiv.org/abs/2106.15328",
          "publishedOn": "2021-06-30T02:01:01.499Z",
          "wordCount": 543,
          "title": "Deep Learning for Multi-View Stereo via Plane Sweep: A Survey. (arXiv:2106.15328v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15453",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kong_Y/0/1/0/all/0/1\">Yan San Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suresh_V/0/1/0/all/0/1\">Varsha Suresh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soh_J/0/1/0/all/0/1\">Jonathan Soh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ong_D/0/1/0/all/0/1\">Desmond C. Ong</a>",
          "description": "Facial Expression Recognition is a commercially important application, but\none common limitation is that applications often require making predictions on\nout-of-sample distributions, where target images may have very different\nproperties from the images that the model was trained on. How well, or badly,\ndo these models do on unseen target domains? In this paper, we provide a\nsystematic evaluation of domain adaptation in facial expression recognition.\nUsing state-of-the-art transfer learning techniques and six commonly-used\nfacial expression datasets (three collected in the lab and three\n\"in-the-wild\"), we conduct extensive round-robin experiments to examine the\nclassification accuracies for a state-of-the-art CNN model. We also perform\nmulti-source experiments where we examine a model's ability to transfer from\nmultiple source datasets, including (i) within-setting (e.g., lab to lab), (ii)\ncross-setting (e.g., in-the-wild to lab), (iii) mixed-setting (e.g., lab and\nwild to lab) transfer learning experiments. We find sobering results that the\naccuracy of transfer learning is not high, and varies idiosyncratically with\nthe target dataset, and to a lesser extent the source dataset. Generally, the\nbest settings for transfer include fine-tuning the weights of a pre-trained\nmodel, and we find that training with more datasets, regardless of setting,\nimproves transfer performance. We end with a discussion of the need for more --\nand regular -- systematic investigations into the generalizability of FER\nmodels, especially for deployed applications.",
          "link": "http://arxiv.org/abs/2106.15453",
          "publishedOn": "2021-06-30T02:01:01.475Z",
          "wordCount": 668,
          "title": "A Systematic Evaluation of Domain Adaptation in Facial Expression Recognition. (arXiv:2106.15453v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15331",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Juan Liu</a>",
          "description": "Recently, deep learning methods have been proposed for quantitative\nsusceptibility mapping (QSM) data processing: background field removal,\nfield-to-source inversion, and single-step QSM reconstruction. However, the\nconventional padding mechanism used in convolutional neural networks (CNNs) can\nintroduce spatial artifacts, especially in QSM background field removal and\nsingle-step QSM which requires inference from total fields with extreme large\nvalues at the edge boundaries of volume of interest. To address this issue, we\npropose an improved padding technique which utilizes the neighboring valid\nvoxels to estimate the invalid voxels of feature maps at volume boundaries in\nthe neural networks. Studies using simulated and in-vivo data show that the\nproposed padding greatly improves estimation accuracy and reduces artifacts in\nthe results in the tasks of background field removal, field-to-source\ninversion, and single-step QSM reconstruction.",
          "link": "http://arxiv.org/abs/2106.15331",
          "publishedOn": "2021-06-30T02:01:01.470Z",
          "wordCount": 566,
          "title": "Improved Padding in CNNs for Quantitative Susceptibility Mapping. (arXiv:2106.15331v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15597",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_D/0/1/0/all/0/1\">Dewen Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mingqi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yukun Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiaowei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_Q/0/1/0/all/0/1\">Qiu Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Ruixue Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_H/0/1/0/all/0/1\">Hongwen Fei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Meiping Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_J/0/1/0/all/0/1\">Jian Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yiyu Shi</a>",
          "description": "Most existing deep learning-based frameworks for image segmentation assume\nthat a unique ground truth is known and can be used for performance evaluation.\nThis is true for many applications, but not all. Myocardial segmentation of\nMyocardial Contrast Echocardiography (MCE), a critical task in automatic\nmyocardial perfusion analysis, is an example. Due to the low resolution and\nserious artifacts in MCE data, annotations from different cardiologists can\nvary significantly, and it is hard to tell which one is the best. In this case,\nhow can we find a good way to evaluate segmentation performance and how do we\ntrain the neural network? In this paper, we address the first problem by\nproposing a new extended Dice to effectively evaluate the segmentation\nperformance when multiple accepted ground truth is available. Then based on our\nproposed metric, we solve the second problem by further incorporating the new\nmetric into a loss function that enables neural networks to flexibly learn\ngeneral features of myocardium. Experiment results on our clinical MCE data set\ndemonstrate that the neural network trained with the proposed loss function\noutperforms those existing ones that try to obtain a unique ground truth from\nmultiple annotations, both quantitatively and qualitatively. Finally, our\ngrading study shows that using extended Dice as an evaluation metric can better\nidentify segmentation results that need manual correction compared with using\nDice.",
          "link": "http://arxiv.org/abs/2106.15597",
          "publishedOn": "2021-06-30T02:01:01.464Z",
          "wordCount": 687,
          "title": "Segmentation with Multiple Acceptable Annotations: A Case Study of Myocardial Segmentation in Contrast Echocardiography. (arXiv:2106.15597v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15402",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhuo_W/0/1/0/all/0/1\">Wei Zhuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1\">Kunchi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_T/0/1/0/all/0/1\">Taofeng Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_B/0/1/0/all/0/1\">Beihong Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Beibei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xinzhou Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">He Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_W/0/1/0/all/0/1\">Wenhai Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xuejian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shuo Zhou</a>",
          "description": "Interactions between users and videos are the major data source of performing\nvideo recommendation. Despite lots of existing recommendation methods, user\nbehaviors on videos, which imply the complex relations between users and\nvideos, are still far from being fully explored. In the paper, we present a\nmodel named Sagittarius. Sagittarius adopts a graph convolutional neural\nnetwork to capture the influence between users and videos. In particular,\nSagittarius differentiates between different user behaviors by weighting and\nfuses the semantics of user behaviors into the embeddings of users and videos.\nMoreover, Sagittarius combines multiple optimization objectives to learn user\nand video embeddings and then achieves the video recommendation by the learned\nuser and video embeddings. The experimental results on multiple datasets show\nthat Sagittarius outperforms several state-of-the-art models in terms of\nrecall, unique recall and NDCG.",
          "link": "http://arxiv.org/abs/2106.15402",
          "publishedOn": "2021-06-30T02:01:01.459Z",
          "wordCount": 584,
          "title": "A Behavior-aware Graph Convolution Network Model for Video Recommendation. (arXiv:2106.15402v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15448",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Robinson_C/0/1/0/all/0/1\">Caleb Robinson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ortiz_A/0/1/0/all/0/1\">Anthony Ortiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hughey_L/0/1/0/all/0/1\">Lacey Hughey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stabach_J/0/1/0/all/0/1\">Jared A. Stabach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferres_J/0/1/0/all/0/1\">Juan M. Lavista Ferres</a>",
          "description": "Localizing and counting large ungulates -- hoofed mammals like cows and elk\n-- in very high-resolution satellite imagery is an important task for\nsupporting ecological studies. Prior work has shown that this is feasible with\ndeep learning based methods and sub-meter multi-spectral satellite imagery. We\nextend this line of work by proposing a baseline method, CowNet, that\nsimultaneously estimates the number of animals in an image (counts), as well as\npredicts their location at a pixel level (localizes). We also propose an\nmethodology for evaluating such models on counting and localization tasks\nacross large scenes that takes the uncertainty of noisy labels and the\ninformation needed by stakeholders in ecological monitoring tasks into account.\nFinally, we benchmark our baseline method with state of the art vision methods\nfor counting objects in scenes. We specifically test the temporal\ngeneralization of the resulting models over a large landscape in Point Reyes\nSeashore, CA. We find that the LC-FCN model performs the best and achieves an\naverage precision between 0.56 and 0.61 and an average recall between 0.78 and\n0.92 over three held out test scenes.",
          "link": "http://arxiv.org/abs/2106.15448",
          "publishedOn": "2021-06-30T02:01:01.454Z",
          "wordCount": 638,
          "title": "Detecting Cattle and Elk in the Wild from Space. (arXiv:2106.15448v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15413",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1\">Laiyan Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_R/0/1/0/all/0/1\">Rui Huang</a>",
          "description": "3D semantic scene completion and 2D semantic segmentation are two tightly\ncorrelated tasks that are both essential for indoor scene understanding,\nbecause they predict the same semantic classes, using positively correlated\nhigh-level features. Current methods use 2D features extracted from early-fused\nRGB-D images for 2D segmentation to improve 3D scene completion. We argue that\nthis sequential scheme does not ensure these two tasks fully benefit each\nother, and present an Iterative Mutual Enhancement Network (IMENet) to solve\nthem jointly, which interactively refines the two tasks at the late prediction\nstage. Specifically, two refinement modules are developed under a unified\nframework for the two tasks. The first is a 2D Deformable Context Pyramid (DCP)\nmodule, which receives the projection from the current 3D predictions to refine\nthe 2D predictions. In turn, a 3D Deformable Depth Attention (DDA) module is\nproposed to leverage the reprojected results from 2D predictions to update the\ncoarse 3D predictions. This iterative fusion happens to the stable high-level\nfeatures of both tasks at a late stage. Extensive experiments on NYU and NYUCAD\ndatasets verify the effectiveness of the proposed iterative late fusion scheme,\nand our approach outperforms the state of the art on both 3D semantic scene\ncompletion and 2D semantic segmentation.",
          "link": "http://arxiv.org/abs/2106.15413",
          "publishedOn": "2021-06-30T02:01:01.449Z",
          "wordCount": 656,
          "title": "IMENet: Joint 3D Semantic Scene Completion and 2D Semantic Segmentation through Iterative Mutual Enhancement. (arXiv:2106.15413v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15550",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Matsumori_S/0/1/0/all/0/1\">Shoya Matsumori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shingyouchi_K/0/1/0/all/0/1\">Kosuke Shingyouchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abe_Y/0/1/0/all/0/1\">Yuki Abe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fukuchi_Y/0/1/0/all/0/1\">Yosuke Fukuchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sugiura_K/0/1/0/all/0/1\">Komei Sugiura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Imai_M/0/1/0/all/0/1\">Michita Imai</a>",
          "description": "Building an interactive artificial intelligence that can ask questions about\nthe real world is one of the biggest challenges for vision and language\nproblems. In particular, goal-oriented visual dialogue, where the aim of the\nagent is to seek information by asking questions during a turn-taking dialogue,\nhas been gaining scholarly attention recently. While several existing models\nbased on the GuessWhat?! dataset have been proposed, the Questioner typically\nasks simple category-based questions or absolute spatial questions. This might\nbe problematic for complex scenes where the objects share attributes or in\ncases where descriptive questions are required to distinguish objects. In this\npaper, we propose a novel Questioner architecture, called Unified Questioner\nTransformer (UniQer), for descriptive question generation with referring\nexpressions. In addition, we build a goal-oriented visual dialogue task called\nCLEVR Ask. It synthesizes complex scenes that require the Questioner to\ngenerate descriptive questions. We train our model with two variants of CLEVR\nAsk datasets. The results of the quantitative and qualitative evaluations show\nthat UniQer outperforms the baseline.",
          "link": "http://arxiv.org/abs/2106.15550",
          "publishedOn": "2021-06-30T02:01:01.444Z",
          "wordCount": 614,
          "title": "Unified Questioner Transformer for Descriptive Question Generation in Goal-Oriented Visual Dialogue. (arXiv:2106.15550v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15510",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Kai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1\">Bo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yingjie Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_Z/0/1/0/all/0/1\">Zhiquan Qi</a>",
          "description": "Numerous detection problems in computer vision, including road crack\ndetection, suffer from exceedingly foreground-background imbalance.\nFortunately, modification of loss function appears to solve this puzzle once\nand for all. In this paper, we propose a pixel-based adaptive weighted\ncross-entropy loss in conjunction with Jaccard distance to facilitate\nhigh-quality pixel-level road crack detection. Our work profoundly demonstrates\nthe influence of loss functions on detection outcomes, and sheds light on the\nsophisticated consecutive improvements in the realm of crack detection.\nSpecifically, to verify the effectiveness of the proposed loss, we conduct\nextensive experiments on four public databases, i.e., CrackForest, AigleRN,\nCrack360, and BJN260. Compared with the vanilla weighted cross-entropy, the\nproposed loss significantly speeds up the training process while retaining the\ntest accuracy.",
          "link": "http://arxiv.org/abs/2106.15510",
          "publishedOn": "2021-06-30T02:01:01.426Z",
          "wordCount": 564,
          "title": "Fast and Accurate Road Crack Detection Based on Adaptive Cost-Sensitive Loss Function. (arXiv:2106.15510v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15475",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khanal_B/0/1/0/all/0/1\">Bidur Khanal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanan_C/0/1/0/all/0/1\">Christopher Kanan</a>",
          "description": "Incorrectly labeled examples, or label noise, is common in real-world\ncomputer vision datasets. While the impact of label noise on learning in deep\nneural networks has been studied in prior work, these studies have exclusively\nfocused on homogeneous label noise, i.e., the degree of label noise is the same\nacross all categories. However, in the real-world, label noise is often\nheterogeneous, with some categories being affected to a greater extent than\nothers. Here, we address this gap in the literature. We hypothesized that\nheterogeneous label noise would only affect the classes that had label noise\nunless there was transfer from those classes to the classes without label\nnoise. To test this hypothesis, we designed a series of computer vision studies\nusing MNIST, CIFAR-10, CIFAR-100, and MS-COCO where we imposed heterogeneous\nlabel noise during the training of multi-class, multi-task, and multi-label\nsystems. Our results provide evidence in support of our hypothesis: label noise\nonly affects the class affected by it unless there is transfer.",
          "link": "http://arxiv.org/abs/2106.15475",
          "publishedOn": "2021-06-30T02:01:01.407Z",
          "wordCount": 598,
          "title": "How Does Heterogeneous Label Noise Impact Generalization in Neural Nets?. (arXiv:2106.15475v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/1903.04855",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gou_C/0/1/0/all/0/1\">Chao Gou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_T/0/1/0/all/0/1\">Tianyu Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1\">Wenbo Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1\">Huadan Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hui Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_Q/0/1/0/all/0/1\">Qiang Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1\">Zhengyu Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fei-Yue Wang</a>",
          "description": "There has been much progress in data-driven artificial intelligence\ntechnology for medical image analysis in the last decades. However, it still\nremains challenging due to its distinctive complexity of acquiring and\nannotating image data, extracting medical domain knowledge, and explaining the\ndiagnostic decision for medical image analysis. In this paper, we propose a\ndata-knowledge-driven framework termed as Parallel Medical Imaging (PMI) for\nintelligent medical image analysis based on the methodology of interactive\nACP-based parallel intelligence. In the PMI framework, computational\nexperiments with predictive learning in a data-driven way are conducted to\nextract medical knowledge for diagnostic decision support. Artificial imaging\nsystems are introduced to select and prescriptively generate medical image data\nin a knowledge-driven way to utilize medical domain knowledge. Through the\nclosed-loop optimization based on parallel execution, our proposed PMI\nframework can boost the generalization ability and alleviate the limitation of\nmedical interpretation for diagnostic decisions. Furthermore, we illustrate the\npreliminary implementation of PMI method through the case studies of mammogram\nanalysis and skin lesion image analysis. Experimental results on several public\nmedical image datasets demonstrate the effectiveness of proposed PMI.",
          "link": "http://arxiv.org/abs/1903.04855",
          "publishedOn": "2021-06-30T02:01:01.396Z",
          "wordCount": 672,
          "title": "Parallel Medical Imaging for Intelligent Medical Image Analysis: Concepts, Methods, and Applications. (arXiv:1903.04855v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15575",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Upadhyay_U/0/1/0/all/0/1\">Uddeshya Upadhyay</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Awate_S/0/1/0/all/0/1\">Suyash Awate</a>",
          "description": "Deep neural networks for image quality enhancement typically need large\nquantities of highly-curated training data comprising pairs of low-quality\nimages and their corresponding high-quality images. While high-quality image\nacquisition is typically expensive and time-consuming, medium-quality images\nare faster to acquire, at lower equipment costs, and available in larger\nquantities. Thus, we propose a novel generative adversarial network (GAN) that\ncan leverage training data at multiple levels of quality (e.g., high and medium\nquality) to improve performance while limiting costs of data curation. We apply\nour mixed-supervision GAN to (i) super-resolve histopathology images and (ii)\nenhance laparoscopy images by combining super-resolution and surgical smoke\nremoval. Results on large clinical and pre-clinical datasets show the benefits\nof our mixed-supervision GAN over the state of the art.",
          "link": "http://arxiv.org/abs/2106.15575",
          "publishedOn": "2021-06-30T02:01:01.380Z",
          "wordCount": 573,
          "title": "A Mixed-Supervision Multilevel GAN Framework for Image Quality Enhancement. (arXiv:2106.15575v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15379",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Ghojogh_B/0/1/0/all/0/1\">Benyamin Ghojogh</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ghodsi_A/0/1/0/all/0/1\">Ali Ghodsi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Karray_F/0/1/0/all/0/1\">Fakhri Karray</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Crowley_M/0/1/0/all/0/1\">Mark Crowley</a>",
          "description": "This is a tutorial and survey paper on unification of spectral dimensionality\nreduction methods, kernel learning by Semidefinite Programming (SDP), Maximum\nVariance Unfolding (MVU) or Semidefinite Embedding (SDE), and its variants. We\nfirst explain how the spectral dimensionality reduction methods can be unified\nas kernel Principal Component Analysis (PCA) with different kernels. This\nunification can be interpreted as eigenfunction learning or representation of\nkernel in terms of distance matrix. Then, since the spectral methods are\nunified as kernel PCA, we say let us learn the best kernel for unfolding the\nmanifold of data to its maximum variance. We first briefly introduce kernel\nlearning by SDP for the transduction task. Then, we explain MVU in detail.\nVarious versions of supervised MVU using nearest neighbors graph, by class-wise\nunfolding, by Fisher criterion, and by colored MVU are explained. We also\nexplain out-of-sample extension of MVU using eigenfunctions and kernel mapping.\nFinally, we introduce other variants of MVU including action respecting\nembedding, relaxed MVU, and landmark MVU for big data.",
          "link": "http://arxiv.org/abs/2106.15379",
          "publishedOn": "2021-06-30T02:01:01.367Z",
          "wordCount": 644,
          "title": "Unified Framework for Spectral Dimensionality Reduction, Maximum Variance Unfolding, and Kernel Learning By Semidefinite Programming: Tutorial and Survey. (arXiv:2106.15379v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15542",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Upadhyay_U/0/1/0/all/0/1\">Uddeshya Upadhyay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yanbei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hepp_T/0/1/0/all/0/1\">Tobias Hepp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gatidis_S/0/1/0/all/0/1\">Sergios Gatidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akata_Z/0/1/0/all/0/1\">Zeynep Akata</a>",
          "description": "Image-to-image translation plays a vital role in tackling various medical\nimaging tasks such as attenuation correction, motion correction, undersampled\nreconstruction, and denoising. Generative adversarial networks have been shown\nto achieve the state-of-the-art in generating high fidelity images for these\ntasks. However, the state-of-the-art GAN-based frameworks do not estimate the\nuncertainty in the predictions made by the network that is essential for making\ninformed medical decisions and subsequent revision by medical experts and has\nrecently been shown to improve the performance and interpretability of the\nmodel. In this work, we propose an uncertainty-guided progressive learning\nscheme for image-to-image translation. By incorporating aleatoric uncertainty\nas attention maps for GANs trained in a progressive manner, we generate images\nof increasing fidelity progressively. We demonstrate the efficacy of our model\non three challenging medical image translation tasks, including PET to CT\ntranslation, undersampled MRI reconstruction, and MRI motion artefact\ncorrection. Our model generalizes well in three different tasks and improves\nperformance over state of the art under full-supervision and weak-supervision\nwith limited data. Code is released here:\nhttps://github.com/ExplainableML/UncerGuidedI2I",
          "link": "http://arxiv.org/abs/2106.15542",
          "publishedOn": "2021-06-30T02:01:01.361Z",
          "wordCount": 634,
          "title": "Uncertainty-Guided Progressive GANs for Medical Image Translation. (arXiv:2106.15542v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/1904.03936",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fatras_K/0/1/0/all/0/1\">Kilian Fatras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Damodaran_B/0/1/0/all/0/1\">Bharath Bhushan Damodaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lobry_S/0/1/0/all/0/1\">Sylvain Lobry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Flamary_R/0/1/0/all/0/1\">R&#xe9;mi Flamary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tuia_D/0/1/0/all/0/1\">Devis Tuia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Courty_N/0/1/0/all/0/1\">Nicolas Courty</a>",
          "description": "Noisy labels often occur in vision datasets, especially when they are\nobtained from crowdsourcing or Web scraping. We propose a new regularization\nmethod, which enables learning robust classifiers in presence of noisy data. To\nachieve this goal, we propose a new adversarial regularization scheme based on\nthe Wasserstein distance. Using this distance allows taking into account\nspecific relations between classes by leveraging the geometric properties of\nthe labels space. Our Wasserstein Adversarial Regularization (WAR) encodes a\nselective regularization, which promotes smoothness of the classifier between\nsome classes, while preserving sufficient complexity of the decision boundary\nbetween others. We first discuss how and why adversarial regularization can be\nused in the context of label noise and then show the effectiveness of our\nmethod on five datasets corrupted with noisy labels: in both benchmarks and\nreal datasets, WAR outperforms the state-of-the-art competitors.",
          "link": "http://arxiv.org/abs/1904.03936",
          "publishedOn": "2021-06-30T02:01:01.347Z",
          "wordCount": 638,
          "title": "Wasserstein Adversarial Regularization (WAR) on label noise. (arXiv:1904.03936v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15332",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yixuan Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yihao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1\">Xianbin Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Ziliang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1\">Xianbiao Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1\">Peng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_G/0/1/0/all/0/1\">Guotong Xie</a>",
          "description": "TextVQA requires models to read and reason about text in images to answer\nquestions about them. Specifically, models need to incorporate a new modality\nof text present in the images and reason over it to answer TextVQA questions.\nIn this challenge, we use generative model T5 for TextVQA task. Based on\npre-trained checkpoint T5-3B from HuggingFace repository, two other\npre-training tasks including masked language modeling(MLM) and relative\nposition prediction(RPP) are designed to better align object feature and scene\ntext. In the stage of pre-training, encoder is dedicate to handle the fusion\namong multiple modalities: question text, object text labels, scene text\nlabels, object visual features, scene visual features. After that decoder\ngenerates the text sequence step-by-step, cross entropy loss is required by\ndefault. We use a large-scale scene text dataset in pre-training and then\nfine-tune the T5-3B with the TextVQA dataset only.",
          "link": "http://arxiv.org/abs/2106.15332",
          "publishedOn": "2021-06-30T02:01:01.341Z",
          "wordCount": 604,
          "title": "Winner Team Mia at TextVQA Challenge 2021: Vision-and-Language Representation Learning with Pre-trained Sequence-to-Sequence Model. (arXiv:2106.15332v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15395",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Lu_Z/0/1/0/all/0/1\">Zhiyang Lu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Z/0/1/0/all/0/1\">Zheng Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1\">Jun Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+shi_J/0/1/0/all/0/1\">Jun shi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shen_D/0/1/0/all/0/1\">Dinggang Shen</a>",
          "description": "The thick-slice magnetic resonance (MR) images are often structurally blurred\nin coronal and sagittal views, which causes harm to diagnosis and image\npost-processing. Deep learning (DL) has shown great potential to re-construct\nthe high-resolution (HR) thin-slice MR images from those low-resolution (LR)\ncases, which we refer to as the slice interpolation task in this work. However,\nsince it is generally difficult to sample abundant paired LR-HR MR images, the\nclassical fully supervised DL-based models cannot be effectively trained to get\nrobust performance. To this end, we propose a novel Two-stage Self-supervised\nCycle-consistency Network (TSCNet) for MR slice interpolation, in which a\ntwo-stage self-supervised learning (SSL) strategy is developed for unsupervised\nDL network training. The paired LR-HR images are synthesized along the sagittal\nand coronal directions of input LR images for network pretraining in the\nfirst-stage SSL, and then a cyclic in-terpolation procedure based on triplet\naxial slices is designed in the second-stage SSL for further refinement. More\ntraining samples with rich contexts along all directions are exploited as\nguidance to guarantee the improved in-terpolation performance. Moreover, a new\ncycle-consistency constraint is proposed to supervise this cyclic procedure,\nwhich encourages the network to reconstruct more realistic HR images. The\nexperimental results on a real MRI dataset indicate that TSCNet achieves\nsuperior performance over the conventional and other SSL-based algorithms, and\nobtains competitive quali-tative and quantitative results compared with the\nfully supervised algorithm.",
          "link": "http://arxiv.org/abs/2106.15395",
          "publishedOn": "2021-06-30T02:01:01.336Z",
          "wordCount": 681,
          "title": "Two-Stage Self-Supervised Cycle-Consistency Network for Reconstruction of Thin-Slice MR Images. (arXiv:2106.15395v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15382",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1\">Tianyu Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Q/0/1/0/all/0/1\">Quanxue Gao</a>",
          "description": "Graph-based multi-view clustering has become an active topic due to the\nefficiency in characterizing both the complex structure and relationship\nbetween multimedia data. However, existing methods have the following\nshortcomings: (1) They are inefficient or even fail for graph learning in large\nscale due to the graph construction and eigen-decomposition. (2) They cannot\nwell exploit both the complementary information and spatial structure embedded\nin graphs of different views. To well exploit complementary information and\ntackle the scalability issue plaguing graph-based multi-view clustering, we\npropose an efficient multiple graph learning model via a small number of anchor\npoints and tensor Schatten p-norm minimization. Specifically, we construct a\nhidden and tractable large graph by anchor graph for each view and well exploit\ncomplementary information embedded in anchor graphs of different views by\ntensor Schatten p-norm regularizer. Finally, we develop an efficient algorithm,\nwhich scales linearly with the data size, to solve our proposed model.\nExtensive experimental results on several datasets indicate that our proposed\nmethod outperforms some state-of-the-art multi-view clustering algorithms.",
          "link": "http://arxiv.org/abs/2106.15382",
          "publishedOn": "2021-06-30T02:01:01.328Z",
          "wordCount": 603,
          "title": "Multiple Graph Learning for Scalable Multi-view Clustering. (arXiv:2106.15382v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15366",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1\">Cong Ma</a>",
          "description": "TANet is one of state-of-the-art 3D object detection method on KITTI and JRDB\nbenchmark, the network contains a Triple Attention module and Coarse-to-Fine\nRegression module to improve the robustness and accuracy of 3D Detection.\nHowever, since the original input data (point clouds) contains a lot of noise\nduring collecting the data, which will further affect the training of the\nmodel. For example, the object is far from the robot, the sensor is difficult\nto obtain enough pointcloud. If the objects only contains few point clouds, and\nthe samples are fed into model with the normal samples together during\ntraining, the detector will be difficult to distinguish the individual with few\npointcloud belong to object or background. In this paper, we propose TANet++ to\nimprove the performance on 3D Detection, which adopt a novel training strategy\non training the TANet. In order to reduce the negative impact by the weak\nsamples, the training strategy previously filtered the training data, and then\nthe TANet++ is trained by the rest of data. The experimental results shows that\nAP score of TANet++ is 8.98\\% higher than TANet on JRDB benchmark.",
          "link": "http://arxiv.org/abs/2106.15366",
          "publishedOn": "2021-06-30T02:01:01.313Z",
          "wordCount": 623,
          "title": "TANet++: Triple Attention Network with Filtered Pointcloud on 3D Detection. (arXiv:2106.15366v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15345",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yuanqi Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quan_Q/0/1/0/all/0/1\">Quan Quan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_H/0/1/0/all/0/1\">Hu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">S. Kevin Zhou</a>",
          "description": "Pseudo-normality synthesis, which computationally generates a pseudo-normal\nimage from an abnormal one (e.g., with lesions), is critical in many\nperspectives, from lesion detection, data augmentation to clinical surgery\nsuggestion. However, it is challenging to generate high-quality pseudo-normal\nimages in the absence of the lesion information. Thus, expensive lesion\nsegmentation data have been introduced to provide lesion information for the\ngenerative models and improve the quality of the synthetic images. In this\npaper, we aim to alleviate the need of a large amount of lesion segmentation\ndata when generating pseudo-normal images. We propose a Semi-supervised Medical\nImage generative LEarning network (SMILE) which not only utilizes limited\nmedical images with segmentation masks, but also leverages massive medical\nimages without segmentation masks to generate realistic pseudo-normal images.\nExtensive experiments show that our model outperforms the best state-of-the-art\nmodel by up to 6% for data augmentation task and 3% in generating high-quality\nimages. Moreover, the proposed semi-supervised learning achieves comparable\nmedical image synthesis quality with supervised learning model, using only 50\nof segmentation data.",
          "link": "http://arxiv.org/abs/2106.15345",
          "publishedOn": "2021-06-30T02:01:01.307Z",
          "wordCount": 623,
          "title": "Where is the disease? Semi-supervised pseudo-normality synthesis from an abnormal image. (arXiv:2106.15345v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15420",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kotariya_V/0/1/0/all/0/1\">Vineet Kotariya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganguly_U/0/1/0/all/0/1\">Udayan Ganguly</a>",
          "description": "Spiking Neural Networks (SNNs) have shown great potential in solving deep\nlearning problems in an energy-efficient manner. However, they are still\nlimited to simple classification tasks. In this paper, we propose Spiking-GAN,\nthe first spike-based Generative Adversarial Network (GAN). It employs a kind\nof temporal coding scheme called time-to-first-spike coding. We train it using\napproximate backpropagation in the temporal domain. We use simple\nintegrate-and-fire (IF) neurons with very high refractory period for our\nnetwork which ensures a maximum of one spike per neuron. This makes the model\nmuch sparser than a spike rate-based system. Our modified temporal loss\nfunction called 'Aggressive TTFS' improves the inference time of the network by\nover 33% and reduces the number of spikes in the network by more than 11%\ncompared to previous works. Our experiments show that on training the network\non the MNIST dataset using this approach, we can generate high quality samples.\nThereby demonstrating the potential of this framework for solving such problems\nin the spiking domain.",
          "link": "http://arxiv.org/abs/2106.15420",
          "publishedOn": "2021-06-30T02:01:01.302Z",
          "wordCount": 610,
          "title": "Spiking-GAN: A Spiking Generative Adversarial Network Using Time-To-First-Spike Coding. (arXiv:2106.15420v1 [cs.NE])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15357",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaosen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1\">Chuanbiao Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liwei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_K/0/1/0/all/0/1\">Kun He</a>",
          "description": "In the field of adversarial robustness, there is a common practice that\nadopts the single-step adversarial training for quickly developing\nadversarially robust models. However, the single-step adversarial training is\nmost likely to cause catastrophic overfitting, as after a few training epochs\nit will be hard to generate strong adversarial examples to continuously boost\nthe adversarial robustness. In this work, we aim to avoid the catastrophic\noverfitting by introducing multi-step adversarial examples during the\nsingle-step adversarial training. Then, to balance the large training overhead\nof generating multi-step adversarial examples, we propose a Multi-stage\nOptimization based Adversarial Training (MOAT) method that periodically trains\nthe model on mixed benign examples, single-step adversarial examples, and\nmulti-step adversarial examples stage by stage. In this way, the overall\ntraining overhead is reduced significantly, meanwhile, the model could avoid\ncatastrophic overfitting. Extensive experiments on CIFAR-10 and CIFAR-100\ndatasets demonstrate that under similar amount of training overhead, the\nproposed MOAT exhibits better robustness than either single-step or multi-step\nadversarial training methods.",
          "link": "http://arxiv.org/abs/2106.15357",
          "publishedOn": "2021-06-30T02:01:01.286Z",
          "wordCount": 597,
          "title": "Multi-stage Optimization based Adversarial Training. (arXiv:2106.15357v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15350",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dogaru_R/0/1/0/all/0/1\">Radu Dogaru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dogaru_I/0/1/0/all/0/1\">Ioana Dogaru</a>",
          "description": "Light binary convolutional neural networks (LB-CNN) are particularly useful\nwhen implemented in low-energy computing platforms as required in many\nindustrial applications. Herein, a framework for optimizing compact LB-CNN is\nintroduced and its effectiveness is evaluated. The framework is freely\navailable and may run on free-access cloud platforms, thus requiring no major\ninvestments. The optimized model is saved in the standardized .h5 format and\ncan be used as input to specialized tools for further deployment into specific\ntechnologies, thus enabling the rapid development of various intelligent image\nsensors. The main ingredient in accelerating the optimization of our model,\nparticularly the selection of binary convolution kernels, is the Chainer/Cupy\nmachine learning library offering significant speed-ups for training the output\nlayer as an extreme-learning machine. Additional training of the output layer\nusing Keras/Tensorflow is included, as it allows an increase in accuracy.\nResults for widely used datasets including MNIST, GTSRB, ORL, VGG show very\ngood compromise between accuracy and complexity. Particularly, for face\nrecognition problems a carefully optimized LB-CNN model provides up to 100%\naccuracies. Such TinyML solutions are well suited for industrial applications\nrequiring image recognition with low energy consumption.",
          "link": "http://arxiv.org/abs/2106.15350",
          "publishedOn": "2021-06-30T02:01:01.276Z",
          "wordCount": 657,
          "title": "LB-CNN: An Open Source Framework for Fast Training of Light Binary Convolutional Neural Networks using Chainer and Cupy. (arXiv:2106.15350v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15338",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gabbur_P/0/1/0/all/0/1\">Prasad Gabbur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bilkhu_M/0/1/0/all/0/1\">Manjot Bilkhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Movellan_J/0/1/0/all/0/1\">Javier Movellan</a>",
          "description": "We provide a probabilistic interpretation of attention and show that the\nstandard dot-product attention in transformers is a special case of Maximum A\nPosteriori (MAP) inference. The proposed approach suggests the use of\nExpectation Maximization algorithms for online adaptation of key and value\nmodel parameters. This approach is useful for cases in which external agents,\ne.g., annotators, provide inference-time information about the correct values\nof some tokens, e.g, the semantic category of some pixels, and we need for this\nnew information to propagate to other tokens in a principled manner. We\nillustrate the approach on an interactive semantic segmentation task in which\nannotators and models collaborate online to improve annotation efficiency.\nUsing standard benchmarks, we observe that key adaptation boosts model\nperformance ($\\sim10\\%$ mIoU) in the low feedback regime and value propagation\nimproves model responsiveness in the high feedback regime. A PyTorch layer\nimplementation of our probabilistic attention model will be made publicly\navailable.",
          "link": "http://arxiv.org/abs/2106.15338",
          "publishedOn": "2021-06-30T02:01:01.260Z",
          "wordCount": 602,
          "title": "Probabilistic Attention for Interactive Segmentation. (arXiv:2106.15338v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15507",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chatterjee_A/0/1/0/all/0/1\">Amit Chatterjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dhanotiya_J/0/1/0/all/0/1\">Jitendra Dhanotiya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhatia_V/0/1/0/all/0/1\">Vimal Bhatia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prakash_S/0/1/0/all/0/1\">Shashi Prakash</a>",
          "description": "Main visual techniques used to obtain information from speckle patterns are\nFujii method, generalized difference, weighted generalized difference, mean\nwindowed difference, structural function (SF), modified SF, etc. In this work,\na comparative analysis of major visual techniques for natural gum sample is\ncarried out. Obtained results conclusively establish SF based method as an\noptimum tool for visual inspection of dynamic speckle data.",
          "link": "http://arxiv.org/abs/2106.15507",
          "publishedOn": "2021-06-30T02:01:01.254Z",
          "wordCount": 508,
          "title": "Study of visual processing techniques for dynamic speckles: a comparative analysis. (arXiv:2106.15507v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15368",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jianqi Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1\">Shi Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>",
          "description": "Scene text image super-resolution (STISR) aims to improve the resolution and\nvisual quality of low-resolution (LR) scene text images, and consequently boost\nthe performance of text recognition. However, most of existing STISR methods\nregard text images as natural scene images, ignoring the categorical\ninformation of text. In this paper, we make an inspiring attempt to embed\ncategorical text prior into STISR model training. Specifically, we adopt the\ncharacter probability sequence as the text prior, which can be obtained\nconveniently from a text recognition model. The text prior provides categorical\nguidance to recover high-resolution (HR) text images. On the other hand, the\nreconstructed HR image can refine the text prior in return. Finally, we present\na multi-stage text prior guided super-resolution (TPGSR) framework for STISR.\nOur experiments on the benchmark TextZoom dataset show that TPGSR can not only\neffectively improve the visual quality of scene text images, but also\nsignificantly improve the text recognition accuracy over existing STISR\nmethods. Our model trained on TextZoom also demonstrates certain generalization\ncapability to the LR images in other datasets.",
          "link": "http://arxiv.org/abs/2106.15368",
          "publishedOn": "2021-06-30T02:01:01.232Z",
          "wordCount": 607,
          "title": "Text Prior Guided Scene Text Image Super-resolution. (arXiv:2106.15368v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15361",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kumakoshi_Y/0/1/0/all/0/1\">Yusuke Kumakoshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Onoda_S/0/1/0/all/0/1\">Shigeaki Onoda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takahashi_T/0/1/0/all/0/1\">Tetsuya Takahashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoshimura_Y/0/1/0/all/0/1\">Yuji Yoshimura</a>",
          "description": "The disorder of urban streetscapes would negatively affect people's\nperception of their aesthetic quality. The presence of billboards on building\nfacades has been regarded as an important factor of the disorder, but its\nquantification methodology has not yet been developed in a scalable manner. To\nfill the gap, this paper reports the performance of our deep learning model on\na unique data set prepared in Tokyo to recognize the areas covered by facades\nand billboards in streetscapes, respectively. The model achieved 63.17 % of\naccuracy, measured by Intersection-over-Union (IoU), thus enabling researchers\nand practitioners to obtain insights on urban streetscape design by combining\ndata of people's preferences.",
          "link": "http://arxiv.org/abs/2106.15361",
          "publishedOn": "2021-06-30T02:01:01.226Z",
          "wordCount": 556,
          "title": "Quantifying urban streetscapes with deep learning: focus on aesthetic evaluation. (arXiv:2106.15361v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15341",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vasata_D/0/1/0/all/0/1\">Daniel Va&#x161;ata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Halama_T/0/1/0/all/0/1\">Tom&#xe1;&#x161; Halama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Friedjungova_M/0/1/0/all/0/1\">Magda Friedjungov&#xe1;</a>",
          "description": "Image inpainting is one of the important tasks in computer vision which\nfocuses on the reconstruction of missing regions in an image. The aim of this\npaper is to introduce an image inpainting model based on Wasserstein Generative\nAdversarial Imputation Network. The generator network of the model uses\nbuilding blocks of convolutional layers with different dilation rates, together\nwith skip connections that help the model reproduce fine details of the output.\nThis combination yields a universal imputation model that is able to handle\nvarious scenarios of missingness with sufficient quality. To show this\nexperimentally, the model is simultaneously trained to deal with three\nscenarios given by missing pixels at random, missing various smaller square\nregions, and one missing square placed in the center of the image. It turns out\nthat our model achieves high-quality inpainting results on all scenarios.\nPerformance is evaluated using peak signal-to-noise ratio and structural\nsimilarity index on two real-world benchmark datasets, CelebA faces and Paris\nStreetView. The results of our model are compared to biharmonic imputation and\nto some of the other state-of-the-art image inpainting methods.",
          "link": "http://arxiv.org/abs/2106.15341",
          "publishedOn": "2021-06-30T02:01:01.195Z",
          "wordCount": 634,
          "title": "Image Inpainting Using Wasserstein Generative Adversarial Imputation Network. (arXiv:2106.15341v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15326",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_Z/0/1/0/all/0/1\">Zhen Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yifan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Hongbin Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_S/0/1/0/all/0/1\">Shuaicheng Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yanxia Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Q/0/1/0/all/0/1\">Qing Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1\">Mingkui Tan</a>",
          "description": "We study a practical domain adaptation task, called source-free unsupervised\ndomain adaptation (UDA) problem, in which we cannot access source domain data\ndue to data privacy issues but only a pre-trained source model and unlabeled\ntarget data are available. This task, however, is very difficult due to one key\nchallenge: the lack of source data and target domain labels makes model\nadaptation very challenging. To address this, we propose to mine the hidden\nknowledge in the source model and exploit it to generate source avatar\nprototypes (i.e., representative features for each source class) as well as\ntarget pseudo labels for domain alignment. To this end, we propose a\nContrastive Prototype Generation and Adaptation (CPGA) method. Specifically,\nCPGA consists of two stages: (1) prototype generation: by exploring the\nclassification boundary information of the source model, we train a prototype\ngenerator to generate avatar prototypes via contrastive learning. (2) prototype\nadaptation: based on the generated source prototypes and target pseudo labels,\nwe develop a new robust contrastive prototype adaptation strategy to align each\npseudo-labeled target data to the corresponding source prototypes. Extensive\nexperiments on three UDA benchmark datasets demonstrate the effectiveness and\nsuperiority of the proposed method.",
          "link": "http://arxiv.org/abs/2106.15326",
          "publishedOn": "2021-06-30T02:01:01.179Z",
          "wordCount": 644,
          "title": "Source-free Domain Adaptation via Avatar Prototype Generation and Adaptation. (arXiv:2106.15326v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15324",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Beck_N/0/1/0/all/0/1\">Nathan Beck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sivasubramanian_D/0/1/0/all/0/1\">Durga Sivasubramanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dani_A/0/1/0/all/0/1\">Apurva Dani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramakrishnan_G/0/1/0/all/0/1\">Ganesh Ramakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyer_R/0/1/0/all/0/1\">Rishabh Iyer</a>",
          "description": "With the goal of making deep learning more label-efficient, a growing number\nof papers have been studying active learning (AL) for deep models. However,\nthere are a number of issues in the prevalent experimental settings, mainly\nstemming from a lack of unified implementation and benchmarking. Issues in the\ncurrent literature include sometimes contradictory observations on the\nperformance of different AL algorithms, unintended exclusion of important\ngeneralization approaches such as data augmentation and SGD for optimization, a\nlack of study of evaluation facets like the labeling efficiency of AL, and\nlittle or no clarity on the scenarios in which AL outperforms random sampling\n(RS). In this work, we present a unified re-implementation of state-of-the-art\nAL algorithms in the context of image classification, and we carefully study\nthese issues as facets of effective evaluation. On the positive side, we show\nthat AL techniques are 2x to 4x more label-efficient compared to RS with the\nuse of data augmentation. Surprisingly, when data augmentation is included,\nthere is no longer a consistent gain in using BADGE, a state-of-the-art\napproach, over simple uncertainty sampling. We then do a careful analysis of\nhow existing approaches perform with varying amounts of redundancy and number\nof examples per class. Finally, we provide several insights for AL\npractitioners to consider in future work, such as the effect of the AL batch\nsize, the effect of initialization, the importance of retraining a new model at\nevery round, and other insights.",
          "link": "http://arxiv.org/abs/2106.15324",
          "publishedOn": "2021-06-30T02:01:01.173Z",
          "wordCount": 726,
          "title": "Effective Evaluation of Deep Active Learning on Image Classification Tasks. (arXiv:2106.15324v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15327",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yao_D/0/1/0/all/0/1\">Dan Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McLaughlin_S/0/1/0/all/0/1\">Stephen McLaughlin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Altmann_Y/0/1/0/all/0/1\">Yoann Altmann</a>",
          "description": "This paper presents a new Expectation Propagation (EP) framework for image\nrestoration using patch-based prior distributions. While Monte Carlo techniques\nare classically used to sample from intractable posterior distributions, they\ncan suffer from scalability issues in high-dimensional inference problems such\nas image restoration. To address this issue, EP is used here to approximate the\nposterior distributions using products of multivariate Gaussian densities.\nMoreover, imposing structural constraints on the covariance matrices of these\ndensities allows for greater scalability and distributed computation. While the\nmethod is naturally suited to handle additive Gaussian observation noise, it\ncan also be extended to non-Gaussian noise. Experiments conducted for\ndenoising, inpainting and deconvolution problems with Gaussian and Poisson\nnoise illustrate the potential benefits of such flexible approximate Bayesian\nmethod for uncertainty quantification in imaging problems, at a reduced\ncomputational cost compared to sampling techniques.",
          "link": "http://arxiv.org/abs/2106.15327",
          "publishedOn": "2021-06-30T02:01:01.168Z",
          "wordCount": 571,
          "title": "Patch-Based Image Restoration using Expectation Propagation. (arXiv:2106.15327v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15329",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yaseen_M/0/1/0/all/0/1\">Muhammad Usman Yaseen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anjum_A/0/1/0/all/0/1\">Ashiq Anjum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fortino_G/0/1/0/all/0/1\">Giancarlo Fortino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liotta_A/0/1/0/all/0/1\">Antonio Liotta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hussain_A/0/1/0/all/0/1\">Amir Hussain</a>",
          "description": "Object recognition from live video streams comes with numerous challenges\nsuch as the variation in illumination conditions and poses. Convolutional\nneural networks (CNNs) have been widely used to perform intelligent visual\nobject recognition. Yet, CNNs still suffer from severe accuracy degradation,\nparticularly on illumination-variant datasets. To address this problem, we\npropose a new CNN method based on orientation fusion for visual object\nrecognition. The proposed cloud-based video analytics system pioneers the use\nof bi-dimensional empirical mode decomposition to split a video frame into\nintrinsic mode functions (IMFs). We further propose these IMFs to endure Reisz\ntransform to produce monogenic object components, which are in turn used for\nthe training of CNNs. Past works have demonstrated how the object orientation\ncomponent may be used to pursue accuracy levels as high as 93\\%. Herein we\ndemonstrate how a feature-fusion strategy of the orientation components leads\nto further improving visual recognition accuracy to 97\\%. We also assess the\nscalability of our method, looking at both the number and the size of the video\nstreams under scrutiny. We carry out extensive experimentation on the publicly\navailable Yale dataset, including also a self generated video datasets, finding\nsignificant improvements (both in accuracy and scale), in comparison to\nAlexNet, LeNet and SE-ResNeXt, which are the three most commonly used deep\nlearning models for visual object recognition and classification.",
          "link": "http://arxiv.org/abs/2106.15329",
          "publishedOn": "2021-06-30T02:01:01.163Z",
          "wordCount": 675,
          "title": "Cloud based Scalable Object Recognition from Video Streams using Orientation Fusion and Convolutional Neural Networks. (arXiv:2106.15329v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15322",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Abdallah_A/0/1/0/all/0/1\">Abdelrahman Abdallah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berendeyev_A/0/1/0/all/0/1\">Alexander Berendeyev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nuradin_I/0/1/0/all/0/1\">Islam Nuradin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nurseitov_D/0/1/0/all/0/1\">Daniyar Nurseitov</a>",
          "description": "We present TNCR, a new table dataset with varying image quality collected\nfrom free websites. The TNCR dataset can be used for table detection in scanned\ndocument images and their classification into 5 different classes. TNCR\ncontains 9428 high-quality labeled images. In this paper, we have implemented\nstate-of-the-art deep learning-based methods for table detection to create\nseveral strong baselines. Cascade Mask R-CNN with ResNeXt-101-64x4d Backbone\nNetwork achieves the highest performance compared to other methods with a\nprecision of 79.7%, recall of 89.8%, and f1 score of 84.4% on the TNCR dataset.\nWe have made TNCR open source in the hope of encouraging more deep learning\napproaches to table detection, classification, and structure recognition. The\ndataset and trained model checkpoints are available at\nhttps://github.com/abdoelsayed2016/TNCR_Dataset.",
          "link": "http://arxiv.org/abs/2106.15322",
          "publishedOn": "2021-06-30T02:01:01.156Z",
          "wordCount": 562,
          "title": "TNCR: Table Net Detection and Classification Dataset. (arXiv:2106.15322v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15325",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hafiz_A/0/1/0/all/0/1\">Abdul Mueed Hafiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhat_R/0/1/0/all/0/1\">Rouf Ul Alam Bhat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parah_S/0/1/0/all/0/1\">Shabir Ahmad Parah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassaballah_M/0/1/0/all/0/1\">M. Hassaballah</a>",
          "description": "3D model generation from single 2D RGB images is a challenging and actively\nresearched computer vision task. Various techniques using conventional network\narchitectures have been proposed for the same. However, the body of research\nwork is limited and there are various issues like using inefficient 3D\nrepresentation formats, weak 3D model generation backbones, inability to\ngenerate dense point clouds, dependence of post-processing for generation of\ndense point clouds, and dependence on silhouettes in RGB images. In this paper,\na novel 2D RGB image to point cloud conversion technique is proposed, which\nimproves the state of art in the field due to its efficient, robust and simple\nmodel by using the concept of parallelization in network architecture. It not\nonly uses the efficient and rich 3D representation of point clouds, but also\nuses a novel and robust point cloud generation backbone in order to address the\nprevalent issues. This involves using a single-encoder multiple-decoder deep\nnetwork architecture wherein each decoder generates certain fixed viewpoints.\nThis is followed by fusing all the viewpoints to generate a dense point cloud.\nVarious experiments are conducted on the technique and its performance is\ncompared with those of other state of the art techniques and impressive gains\nin performance are demonstrated. Code is available at\nhttps://github.com/mueedhafiz1982/",
          "link": "http://arxiv.org/abs/2106.15325",
          "publishedOn": "2021-06-30T02:01:01.138Z",
          "wordCount": 672,
          "title": "SE-MD: A Single-encoder multiple-decoder deep network for point cloud generation from 2D images. (arXiv:2106.15325v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15319",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_F/0/1/0/all/0/1\">Fan Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marti_Puig_P/0/1/0/all/0/1\">Pere Marti-Puig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caiafa_C/0/1/0/all/0/1\">Cesar F. Caiafa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zhe Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_F/0/1/0/all/0/1\">Feng Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sole_Casals_J/0/1/0/all/0/1\">Jordi Sol&#xe9;-Casals</a>",
          "description": "Empirical mode decomposition (EMD) has developed into a prominent tool for\nadaptive, scale-based signal analysis in various fields like robotics, security\nand biomedical engineering. Since the dramatic increase in amount of data puts\nforward higher requirements for the capability of real-time signal analysis, it\nis difficult for existing EMD and its variants to trade off the growth of data\ndimension and the speed of signal analysis. In order to decompose\nmulti-dimensional signals at a faster speed, we present a novel\nsignal-serialization method (serial-EMD), which concatenates multi-variate or\nmulti-dimensional signals into a one-dimensional signal and uses various\none-dimensional EMD algorithms to decompose it. To verify the effects of the\nproposed method, synthetic multi-variate time series, artificial 2D images with\nvarious textures and real-world facial images are tested. Compared with\nexisting multi-EMD algorithms, the decomposition time becomes significantly\nreduced. In addition, the results of facial recognition with Intrinsic Mode\nFunctions (IMFs) extracted using our method can achieve a higher accuracy than\nthose obtained by existing multi-EMD algorithms, which demonstrates the\nsuperior performance of our method in terms of the quality of IMFs.\nFurthermore, this method can provide a new perspective to optimize the existing\nEMD algorithms, that is, transforming the structure of the input signal rather\nthan being constrained by developing envelope computation techniques or signal\ndecomposition methods. In summary, the study suggests that the serial-EMD\ntechnique is a highly competitive and fast alternative for multi-dimensional\nsignal analysis.",
          "link": "http://arxiv.org/abs/2106.15319",
          "publishedOn": "2021-06-30T02:01:01.133Z",
          "wordCount": 692,
          "title": "Serial-EMD: Fast Empirical Mode Decomposition Method for Multi-dimensional Signals Based on Serialization. (arXiv:2106.15319v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15306",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ruijters_D/0/1/0/all/0/1\">Daniel Ruijters</a>",
          "description": "Minimally invasive image guided treatment procedures often employ advanced\nimage processing algorithms. The recent developments of artificial intelligence\nalgorithms harbor potential to further enhance this domain. In this article we\nexplore several application areas within the minimally invasive treatment space\nand discuss the deployment of artificial intelligence within these areas.",
          "link": "http://arxiv.org/abs/2106.15306",
          "publishedOn": "2021-06-30T02:01:01.128Z",
          "wordCount": 496,
          "title": "Artificial Intelligence in Minimally Invasive Interventional Treatment. (arXiv:2106.15306v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15323",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jeckeln_G/0/1/0/all/0/1\">G&#xe9;raldine Jeckeln</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Ying Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cavazos_J/0/1/0/all/0/1\">Jacqueline G. Cavazos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yates_A/0/1/0/all/0/1\">Amy N. Yates</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hahn_C/0/1/0/all/0/1\">Carina A. Hahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_L/0/1/0/all/0/1\">Larry Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phillips_J/0/1/0/all/0/1\">Jonathon Phillips</a>, <a href=\"http://arxiv.org/find/cs/1/au:+OToole_A/0/1/0/all/0/1\">Alice J. O&#x27;Toole</a>",
          "description": "Measures of face identification proficiency are essential to ensure accurate\nand consistent performance by professional forensic face examiners and others\nwho perform face identification tasks in applied scenarios. Current proficiency\ntests rely on static sets of stimulus items, and so, cannot be administered\nvalidly to the same individual multiple times. To create a proficiency test, a\nlarge number of items of \"known\" difficulty must be assembled. Multiple tests\nof equal difficulty can be constructed then using subsets of items. Here, we\nintroduce a proficiency test, the Triad Identity Matching (TIM) test, based on\nstimulus difficulty measures based on Item Response Theory (IRT). Participants\nview face-image \"triads\" (N=225) (two images of one identity and one image of a\ndifferent identity) and select the different identity. In Experiment 1,\nuniversity students (N=197) showed wide-ranging accuracy on the TIM test.\nFurthermore, IRT modeling demonstrated that the TIM test produces items of\nvarious difficulty levels. In Experiment 2, IRT-based item difficulty measures\nwere used to partition the TIM test into three equally \"easy\" and three equally\n\"difficult\" subsets. Simulation results indicated that the full set, as well as\ncurated subsets, of the TIM items yielded reliable estimates of subject\nability. In summary, the TIM test can provide a starting point for developing a\nframework that is flexible, calibrated, and adaptive to measure proficiency\nacross various ability levels (e.g., professionals or populations with face\nprocessing deficits)",
          "link": "http://arxiv.org/abs/2106.15323",
          "publishedOn": "2021-06-30T02:01:01.122Z",
          "wordCount": 692,
          "title": "Face Identification Proficiency Test Designed Using Item Response Theory. (arXiv:2106.15323v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15315",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_N/0/1/0/all/0/1\">Neil Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Netravali_R/0/1/0/all/0/1\">Ravi Netravali</a>",
          "description": "Delivering fast responses to retrospective queries on video datasets is\ndifficult due to the large number of frames to consider and the high costs of\nrunning convolutional neural networks (CNNs) on each one. A natural solution is\nto perform a subset of the necessary computations ahead of time, as video is\ningested. However, existing ingest-time systems require knowledge of the\nspecific CNN that will be used in future queries -- a challenging requisite\ngiven the evergrowing space of CNN architectures and training\ndatasets/methodologies.\n\nThis paper presents Boggart, a retrospective video analytics system that\ndelivers ingest-time speedups in a model-agnostic manner. Our underlying\ninsight is that traditional computer vision (CV) algorithms are capable of\nperforming computations that can be used to accelerate diverse queries with\nwide-ranging CNNs. Building on this, at ingest-time, Boggart carefully employs\na variety of motion tracking algorithms to identify potential objects and their\ntrajectories across frames. Then, at query-time, Boggart uses several novel\ntechniques to collect the smallest sample of CNN results required to meet the\ntarget accuracy: (1) a clustering strategy to efficiently unearth the\ninevitable discrepancies between CV- and CNN-generated outputs, and (2) a set\nof accuracy-preserving propagation techniques to safely extend sampled results\nalong each trajectory. Across many videos, CNNs, and queries Boggart\nconsistently meets accuracy targets while using CNNs sparingly (on 3-54% of\nframes).",
          "link": "http://arxiv.org/abs/2106.15315",
          "publishedOn": "2021-06-30T02:01:01.117Z",
          "wordCount": 665,
          "title": "Boggart: Accelerating Retrospective Video Analytics via Model-Agnostic Ingest Processing. (arXiv:2106.15315v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15283",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chenglin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_C/0/1/0/all/0/1\">Carrie Lu Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_D/0/1/0/all/0/1\">Di Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_B/0/1/0/all/0/1\">Bei Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuo_X/0/1/0/all/0/1\">Xiao Zuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1\">Lei Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_J/0/1/0/all/0/1\">Jian Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jianming Yang</a>",
          "description": "Deep learning models for human activity recognition (HAR) based on sensor\ndata have been heavily studied recently. However, the generalization ability of\ndeep models on complex real-world HAR data is limited by the availability of\nhigh-quality labeled activity data, which are hard to obtain. In this paper, we\ndesign a similarity embedding neural network that maps input sensor signals\nonto real vectors through carefully designed convolutional and LSTM layers. The\nembedding network is trained with a pairwise similarity loss, encouraging the\nclustering of samples from the same class in the embedded real space, and can\nbe effectively trained on a small dataset and even on a noisy dataset with\nmislabeled samples. Based on the learned embeddings, we further propose both\nnonparametric and parametric approaches for activity recognition. Extensive\nevaluation based on two public datasets has shown that the proposed similarity\nembedding network significantly outperforms state-of-the-art deep models on HAR\nclassification tasks, is robust to mislabeled samples in the training set, and\ncan also be used to effectively denoise a noisy dataset.",
          "link": "http://arxiv.org/abs/2106.15283",
          "publishedOn": "2021-06-30T02:01:01.111Z",
          "wordCount": 626,
          "title": "Similarity Embedding Networks for Robust Human Activity Recognition. (arXiv:2106.15283v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15288",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Boutros_F/0/1/0/all/0/1\">Fadi Boutros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Damer_N/0/1/0/all/0/1\">Naser Damer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolf_J/0/1/0/all/0/1\">Jan Niklas Kolf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raja_K/0/1/0/all/0/1\">Kiran Raja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirchbuchner_F/0/1/0/all/0/1\">Florian Kirchbuchner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramachandra_R/0/1/0/all/0/1\">Raghavendra Ramachandra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuijper_A/0/1/0/all/0/1\">Arjan Kuijper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_P/0/1/0/all/0/1\">Pengcheng Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1\">Fei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Montero_D/0/1/0/all/0/1\">David Montero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aginako_N/0/1/0/all/0/1\">Naiara Aginako</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sierra_B/0/1/0/all/0/1\">Basilio Sierra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nieto_M/0/1/0/all/0/1\">Marcos Nieto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erakin_M/0/1/0/all/0/1\">Mustafa Ekrem Erakin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demir_U/0/1/0/all/0/1\">Ugur Demir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kemal_H/0/1/0/all/0/1\">Hazim Kemal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ekenel/0/1/0/all/0/1\">Ekenel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kataoka_A/0/1/0/all/0/1\">Asaki Kataoka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ichikawa_K/0/1/0/all/0/1\">Kohei Ichikawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kubo_S/0/1/0/all/0/1\">Shizuma Kubo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_M/0/1/0/all/0/1\">Mingjie He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_D/0/1/0/all/0/1\">Dan Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_S/0/1/0/all/0/1\">Shiguang Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grm_K/0/1/0/all/0/1\">Klemen Grm</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Struc_V/0/1/0/all/0/1\">Vitomir &#x160;truc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seneviratne_S/0/1/0/all/0/1\">Sachith Seneviratne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kasthuriarachchi_N/0/1/0/all/0/1\">Nuran Kasthuriarachchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rasnayaka_S/0/1/0/all/0/1\">Sanka Rasnayaka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neto_P/0/1/0/all/0/1\">Pedro C. Neto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sequeira_A/0/1/0/all/0/1\">Ana F. Sequeira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pinto_J/0/1/0/all/0/1\">Joao Ribeiro Pinto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saffari_M/0/1/0/all/0/1\">Mohsen Saffari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cardoso_J/0/1/0/all/0/1\">Jaime S. Cardoso</a>",
          "description": "This paper presents a summary of the Masked Face Recognition Competitions\n(MFR) held within the 2021 International Joint Conference on Biometrics (IJCB\n2021). The competition attracted a total of 10 participating teams with valid\nsubmissions. The affiliations of these teams are diverse and associated with\nacademia and industry in nine different countries. These teams successfully\nsubmitted 18 valid solutions. The competition is designed to motivate solutions\naiming at enhancing the face recognition accuracy of masked faces. Moreover,\nthe competition considered the deployability of the proposed solutions by\ntaking the compactness of the face recognition models into account. A private\ndataset representing a collaborative, multi-session, real masked, capture\nscenario is used to evaluate the submitted solutions. In comparison to one of\nthe top-performing academic face recognition solutions, 10 out of the 18\nsubmitted solutions did score higher masked face verification accuracy.",
          "link": "http://arxiv.org/abs/2106.15288",
          "publishedOn": "2021-06-30T02:01:01.095Z",
          "wordCount": 649,
          "title": "MFR 2021: Masked Face Recognition Competition. (arXiv:2106.15288v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15320",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kahu_S/0/1/0/all/0/1\">Sampanna Yashwant Kahu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ingram_W/0/1/0/all/0/1\">William A. Ingram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fox_E/0/1/0/all/0/1\">Edward A. Fox</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jian Wu</a>",
          "description": "We focus on electronic theses and dissertations (ETDs), aiming to improve\naccess and expand their utility, since more than 6 million are publicly\navailable, and they constitute an important corpus to aid research and\neducation across disciplines. The corpus is growing as new born-digital\ndocuments are included, and since millions of older theses and dissertations\nhave been converted to digital form to be disseminated electronically in\ninstitutional repositories. In ETDs, as with other scholarly works, figures and\ntables can communicate a large amount of information in a concise way. Although\nmethods have been proposed for extracting figures and tables from born-digital\nPDFs, they do not work well with scanned ETDs. Considering this problem, our\nassessment of state-of-the-art figure extraction systems is that the reason\nthey do not function well on scanned PDFs is that they have only been trained\non born-digital documents. To address this limitation, we present ScanBank, a\nnew dataset containing 10 thousand scanned page images, manually labeled by\nhumans as to the presence of the 3.3 thousand figures or tables found therein.\nWe use this dataset to train a deep neural network model based on YOLOv5 to\naccurately extract figures and tables from scanned ETDs. We pose and answer\nimportant research questions aimed at finding better methods for figure\nextraction from scanned documents. One of those concerns the value for\ntraining, of data augmentation techniques applied to born-digital documents\nwhich are used to train models better suited for figure extraction from scanned\ndocuments. To the best of our knowledge, ScanBank is the first manually\nannotated dataset for figure and table extraction for scanned ETDs. A\nYOLOv5-based model, trained on ScanBank, outperforms existing comparable\nopen-source and freely available baseline methods by a considerable margin.",
          "link": "http://arxiv.org/abs/2106.15320",
          "publishedOn": "2021-06-30T02:01:01.089Z",
          "wordCount": 756,
          "title": "ScanBank: A Benchmark Dataset for Figure Extraction from Scanned Electronic Theses and Dissertations. (arXiv:2106.15320v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15287",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Douillard_A/0/1/0/all/0/1\">Arthur Douillard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yifu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dapogny_A/0/1/0/all/0/1\">Arnaud Dapogny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cord_M/0/1/0/all/0/1\">Matthieu Cord</a>",
          "description": "Deep learning approaches are nowadays ubiquitously used to tackle computer\nvision tasks such as semantic segmentation, requiring large datasets and\nsubstantial computational power. Continual learning for semantic segmentation\n(CSS) is an emerging trend that consists in updating an old model by\nsequentially adding new classes. However, continual learning methods are\nusually prone to catastrophic forgetting. This issue is further aggravated in\nCSS where, at each step, old classes from previous iterations are collapsed\ninto the background. In this paper, we propose Local POD, a multi-scale pooling\ndistillation scheme that preserves long- and short-range spatial relationships\nat feature level. Furthermore, we design an entropy-based pseudo-labelling of\nthe background w.r.t. classes predicted by the old model to deal with\nbackground shift and avoid catastrophic forgetting of the old classes. Finally,\nwe introduce a novel rehearsal method that is particularly suited for\nsegmentation. Our approach, called PLOP, significantly outperforms\nstate-of-the-art methods in existing CSS scenarios, as well as in newly\nproposed challenging benchmarks.",
          "link": "http://arxiv.org/abs/2106.15287",
          "publishedOn": "2021-06-30T02:01:01.083Z",
          "wordCount": 609,
          "title": "Tackling Catastrophic Forgetting and Background Shift in Continual Semantic Segmentation. (arXiv:2106.15287v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15321",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Boucaud_L/0/1/0/all/0/1\">Laurent Boucaud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aloise_D/0/1/0/all/0/1\">Daniel Aloise</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saunier_N/0/1/0/all/0/1\">Nicolas Saunier</a>",
          "description": "We consider the problem of predicting the future path of a pedestrian using\nits motion history and the motion history of the surrounding pedestrians,\ncalled social information. Since the seminal paper on Social-LSTM,\ndeep-learning has become the main tool used to model the impact of social\ninteractions on a pedestrian's motion. The demonstration that these models can\nlearn social interactions relies on an ablative study of these models. The\nmodels are compared with and without their social interactions module on two\nstandard metrics, the Average Displacement Error and Final Displacement Error.\nYet, these complex models were recently outperformed by a simple\nconstant-velocity approach. This questions if they actually allow to model\nsocial interactions as well as the validity of the proof. In this paper, we\nfocus on the deep-learning models with a soft-attention mechanism for social\ninteraction modeling and study whether they use social information at\nprediction time. We conduct two experiments across four state-of-the-art\napproaches on the ETH and UCY datasets, which were also used in previous work.\nFirst, the models are trained by replacing the social information with random\nnoise and compared to model trained with actual social information. Second, we\nuse a gating mechanism along with a $L_0$ penalty, allowing models to shut down\ntheir inner components. The models consistently learn to prune their\nsoft-attention mechanism. For both experiments, neither the course of the\nconvergence nor the prediction performance were altered. This demonstrates that\nthe soft-attention mechanism and therefore the social information are ignored\nby the models.",
          "link": "http://arxiv.org/abs/2106.15321",
          "publishedOn": "2021-06-30T02:01:01.078Z",
          "wordCount": 726,
          "title": "Soft Attention: Does it Actually Help to Learn Social Interactions in Pedestrian Trajectory Prediction?. (arXiv:2106.15321v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15312",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_C/0/1/0/all/0/1\">Chao Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tiesong Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwong_S/0/1/0/all/0/1\">Sam Kwong</a>",
          "description": "Automatically evaluating the quality of image captions can be very\nchallenging since human language is quite flexible that there can be various\nexpressions for the same meaning. Most of the current captioning metrics rely\non token level matching between candidate caption and the ground truth label\nsentences. It usually neglects the sentence-level information. Motivated by the\nauto-encoder mechanism and contrastive representation learning advances, we\npropose a learning-based metric for image captioning, which we call Intrinsic\nImage Captioning Evaluation($I^2CE$). We develop three progressive model\nstructures to learn the sentence level representations--single branch model,\ndual branches model, and triple branches model. Our empirical tests show that\n$I^2CE$ trained with dual branches structure achieves better consistency with\nhuman judgments to contemporary image captioning evaluation metrics.\nFurthermore, We select several state-of-the-art image captioning models and\ntest their performances on the MS COCO dataset concerning both contemporary\nmetrics and the proposed $I^2CE$. Experiment results show that our proposed\nmethod can align well with the scores generated from other contemporary\nmetrics. On this concern, the proposed metric could serve as a novel indicator\nof the intrinsic information between captions, which may be complementary to\nthe existing ones.",
          "link": "http://arxiv.org/abs/2106.15312",
          "publishedOn": "2021-06-30T02:01:01.071Z",
          "wordCount": 631,
          "title": "Contrastive Semantic Similarity Learning for Image Captioning Evaluation with Intrinsic Auto-encoder. (arXiv:2106.15312v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15309",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ozsoy_E/0/1/0/all/0/1\">Ege &#xd6;zsoy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ornek_E/0/1/0/all/0/1\">Evin P&#x131;nar &#xd6;rnek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eck_U/0/1/0/all/0/1\">Ulrich Eck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tombari_F/0/1/0/all/0/1\">Federico Tombari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navab_N/0/1/0/all/0/1\">Nassir Navab</a>",
          "description": "From a computer science viewpoint, a surgical domain model needs to be a\nconceptual one incorporating both behavior and data. It should therefore model\nactors, devices, tools, their complex interactions and data flow. To capture\nand model these, we take advantage of the latest computer vision methodologies\nfor generating 3D scene graphs from camera views. We then introduce the\nMultimodal Semantic Scene Graph (MSSG) which aims at providing a unified\nsymbolic, spatiotemporal and semantic representation of surgical procedures.\nThis methodology aims at modeling the relationship between different components\nin surgical domain including medical staff, imaging systems, and surgical\ndevices, opening the path towards holistic understanding and modeling of\nsurgical procedures. We then use MSSG to introduce a dynamically generated\ngraphical user interface tool for surgical procedure analysis which could be\nused for many applications including process optimization, OR design and\nautomatic report generation. We finally demonstrate that the proposed MSSGs\ncould also be used for synchronizing different complex surgical procedures.\nWhile the system still needs to be integrated into real operating rooms before\ngetting validated, this conference paper aims mainly at providing the community\nwith the basic principles of this novel concept through a first prototypal\npartial realization based on MVOR dataset.",
          "link": "http://arxiv.org/abs/2106.15309",
          "publishedOn": "2021-06-30T02:01:01.056Z",
          "wordCount": 645,
          "title": "Multimodal Semantic Scene Graphs for Holistic Modeling of Surgical Procedures. (arXiv:2106.15309v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15304",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1\">Xuan Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_G/0/1/0/all/0/1\">Geng Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_W/0/1/0/all/0/1\">Wei Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiaolong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_J/0/1/0/all/0/1\">Jiexiong Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhengang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_B/0/1/0/all/0/1\">Bin Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanzhi Wang</a>",
          "description": "The rapid development of autonomous driving, abnormal behavior detection, and\nbehavior recognition makes an increasing demand for multi-person pose\nestimation-based applications, especially on mobile platforms. However, to\nachieve high accuracy, state-of-the-art methods tend to have a large model size\nand complex post-processing algorithm, which costs intense computation and long\nend-to-end latency. To solve this problem, we propose an architecture\noptimization and weight pruning framework to accelerate inference of\nmulti-person pose estimation on mobile devices. With our optimization\nframework, we achieve up to 2.51x faster model inference speed with higher\naccuracy compared to representative lightweight multi-person pose estimator.",
          "link": "http://arxiv.org/abs/2106.15304",
          "publishedOn": "2021-06-30T02:01:01.051Z",
          "wordCount": 544,
          "title": "Towards Fast and Accurate Multi-Person Pose Estimation on Mobile Devices. (arXiv:2106.15304v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15318",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kollias_D/0/1/0/all/0/1\">Dimitrios Kollias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kotsia_I/0/1/0/all/0/1\">Irene Kotsia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajiyev_E/0/1/0/all/0/1\">Elnar Hajiyev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zafeiriou_S/0/1/0/all/0/1\">Stefanos Zafeiriou</a>",
          "description": "The Affective Behavior Analysis in-the-wild (ABAW2) 2021 Competition is the\nsecond -- following the first very successful ABAW Competition held in\nconjunction with IEEE FG 2020- Competition that aims at automatically analyzing\naffect. ABAW2 is split into three Challenges, each one addressing one of the\nthree main behavior tasks of valence-arousal estimation, basic expression\nclassification and action unit detection. All three Challenges are based on a\ncommon benchmark database, Aff-Wild2, which is a large scale in-the-wild\ndatabase and the first one to be annotated for all these three tasks. In this\npaper, we describe this Competition, to be held in conjunction with ICCV 2021.\nWe present the three Challenges, with the utilized Competition corpora. We\noutline the evaluation metrics and present the baseline system with its\nresults. More information regarding the Competition is provided in the\nCompetition site: https://ibug.doc.ic.ac.uk/resources/iccv-2021-2nd-abaw.",
          "link": "http://arxiv.org/abs/2106.15318",
          "publishedOn": "2021-06-30T02:01:01.045Z",
          "wordCount": 577,
          "title": "Analysing Affective Behavior in the second ABAW2 Competition. (arXiv:2106.15318v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15206",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1\">Kaiwen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_X/0/1/0/all/0/1\">Xinmei Tian</a>",
          "description": "Domain generalization in person re-identification is a highly important\nmeaningful and practical task in which a model trained with data from several\nsource domains is expected to generalize well to unseen target domains. Domain\nadversarial learning is a promising domain generalization method that aims to\nremove domain information in the latent representation through adversarial\ntraining. However, in person re-identification, the domain and class are\ncorrelated, and we theoretically show that domain adversarial learning will\nlose certain information about class due to this domain-class correlation.\nInspired by casual inference, we propose to perform interventions to the domain\nfactor $d$, aiming to decompose the domain-class correlation. To achieve this\ngoal, we proposed estimating the resulting representation $z^{*}$ caused by the\nintervention through first- and second-order statistical characteristic\nmatching. Specifically, we build a memory bank to restore the statistical\ncharacteristics of each domain. Then, we use the newly generated samples\n$\\{z^{*},y,d^{*}\\}$ to compute the loss function. These samples are\ndomain-class correlation decomposed; thus, we can learn a domain-invariant\nrepresentation that can capture more class-related features. Extensive\nexperiments show that our model outperforms the state-of-the-art methods on the\nlarge-scale domain generalization Re-ID benchmark.",
          "link": "http://arxiv.org/abs/2106.15206",
          "publishedOn": "2021-06-30T02:01:01.040Z",
          "wordCount": 624,
          "title": "Domain-Class Correlation Decomposition for Generalizable Person Re-Identification. (arXiv:2106.15206v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15294",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sugihara_K/0/1/0/all/0/1\">Kenichi Sugihara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wallace_M/0/1/0/all/0/1\">Martin Wallace</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kongwen/0/1/0/all/0/1\">Kongwen</a> (Frank) <a href=\"http://arxiv.org/find/cs/1/au:+Zhang/0/1/0/all/0/1\">Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khmelevsky_Y/0/1/0/all/0/1\">Youry Khmelevsky</a>",
          "description": "The 3D building modelling is important in urban planning and related domains\nthat draw upon the content of 3D models of urban scenes. Such 3D models can be\nused to visualize city images at multiple scales from individual buildings to\nentire cities prior to and after a change has occurred. This ability is of\ngreat importance in day-to-day work and special projects undertaken by\nplanners, geo-designers, and architects. In this research, we implemented a\nnovel approach to 3D building models for such matter, which included the\nintegration of geographic information systems (GIS) and 3D Computer Graphics\n(3DCG) components that generate 3D house models from building footprints\n(polygons), and the automated generation of simple and complex roof geometries\nfor rapid roof area damage reporting. These polygons (footprints) are usually\northogonal. A complicated orthogonal polygon can be partitioned into a set of\nrectangles. The proposed GIS and 3DCG integrated system partitions orthogonal\nbuilding polygons into a set of rectangles and places rectangular roofs and\nbox-shaped building bodies on these rectangles. Since technicians are drawing\nthese polygons manually with digitizers, depending on aerial photos, not all\nbuilding polygons are precisely orthogonal. But, when placing a set of boxes as\nbuilding bodies for creating the buildings, there may be gaps or overlaps\nbetween these boxes if building polygons are not precisely orthogonal. In our\nproposal, after approximately orthogonal building polygons are partitioned and\nrectified into a set of mutually orthogonal rectangles, each rectangle knows\nwhich rectangle is adjacent to and which edge of the rectangle is adjacent to,\nwhich will avoid unwanted intersection of windows and doors when building\nbodies combined.",
          "link": "http://arxiv.org/abs/2106.15294",
          "publishedOn": "2021-06-30T02:01:01.035Z",
          "wordCount": 701,
          "title": "Roof Damage Assessment from Automated 3D Building Models. (arXiv:2106.15294v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15292",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Patel_D/0/1/0/all/0/1\">Deep Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sastry_P/0/1/0/all/0/1\">P.S. Sastry</a>",
          "description": "Deep Neural Networks (DNNs) have been shown to be susceptible to memorization\nor overfitting in the presence of noisily labelled data. For the problem of\nrobust learning under such noisy data, several algorithms have been proposed. A\nprominent class of algorithms rely on sample selection strategies, motivated by\ncurriculum learning. For example, many algorithms use the `small loss trick'\nwherein a fraction of samples with loss values below a certain threshold are\nselected for training. These algorithms are sensitive to such thresholds, and\nit is difficult to fix or learn these thresholds. Often, these algorithms also\nrequire information such as label noise rates which are typically unavailable\nin practice. In this paper, we propose a data-dependent, adaptive sample\nselection strategy that relies only on batch statistics of a given mini-batch\nto provide robustness against label noise. The algorithm does not have any\nadditional hyperparameters for sample selection, does not need any information\non noise rates, and does not need access to separate data with clean labels. We\nempirically demonstrate the effectiveness of our algorithm on benchmark\ndatasets.",
          "link": "http://arxiv.org/abs/2106.15292",
          "publishedOn": "2021-06-30T02:01:01.020Z",
          "wordCount": 620,
          "title": "Adaptive Sample Selection for Robust Learning under Label Noise. (arXiv:2106.15292v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15308",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Homan_R/0/1/0/all/0/1\">Robert Homan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rijsselt_R/0/1/0/all/0/1\">Ren&#xe9; van Rijsselt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruijters_D/0/1/0/all/0/1\">Daniel Ruijters</a>",
          "description": "Fusing live fluoroscopy images with a 3D rotational reconstruction of the\nvasculature allows to navigate endovascular devices in minimally invasive\nneuro-vascular treatment, while reducing the usage of harmful iodine contrast\nmedium. The alignment of the fluoroscopy images and the 3D reconstruction is\ninitialized using the sensor information of the X-ray C-arm geometry. Patient\nmotion is then corrected by an image-based registration algorithm, based on a\ngradient difference similarity measure using digital reconstructed radiographs\nof the 3D reconstruction. This algorithm does not require the vessels in the\nfluoroscopy image to be filled with iodine contrast agent, but rather relies on\ngradients in the image (bone structures, sinuses) as landmark features. This\npaper investigates the accuracy, robustness and computation time aspects of the\nimage-based registration algorithm. Using phantom experiments 97% of the\nregistration attempts passed the success criterion of a residual registration\nerror of less than 1 mm translation and 3{\\deg} rotation. The paper establishes\na new method for validation of 2D-3D registration without requiring changes to\nthe clinical workflow, such as attaching fiducial markers. As a consequence,\nthis method can be retrospectively applied to pre-existing clinical data. For\nclinical data experiments, 87% of the registration attempts passed the\ncriterion of a residual translational error of < 1 mm, and 84% possessed a\nrotational error of < 3{\\deg}.",
          "link": "http://arxiv.org/abs/2106.15308",
          "publishedOn": "2021-06-30T02:01:01.014Z",
          "wordCount": 658,
          "title": "Automatic 2D-3D Registration without Contrast Agent during Neurovascular Interventions. (arXiv:2106.15308v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15305",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zehni_M/0/1/0/all/0/1\">Mona Zehni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Shaona Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sridhar_K/0/1/0/all/0/1\">Krishna Sridhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raman_S/0/1/0/all/0/1\">Sethu Raman</a>",
          "description": "Inverse rendering is the problem of decomposing an image into its intrinsic\ncomponents, i.e. albedo, normal and lighting. To solve this ill-posed problem\nfrom single image, state-of-the-art methods in shape from shading mostly resort\nto supervised training on all the components on either synthetic or real\ndatasets. Here, we propose a new self-supervised training paradigm that 1)\nreduces the need for full supervision on the decomposition task and 2) takes\ninto account the relighting task. We introduce new self-supervised loss terms\nthat leverage the consistencies between multi-lit images (images of the same\nscene under different illuminations). Our approach is applicable to multi-lit\ndatasets. We apply our training approach in two settings: 1) train on a mixture\nof synthetic and real data, 2) train on real datasets with limited supervision.\nWe show-case the effectiveness of our training paradigm on both intrinsic\ndecomposition and relighting and demonstrate how the model struggles in both\ntasks without the self-supervised loss terms in limited supervision settings.\nWe provide results of comprehensive experiments on SfSNet, CelebA and Photoface\ndatasets and verify the performance of our approach on images in the wild.",
          "link": "http://arxiv.org/abs/2106.15305",
          "publishedOn": "2021-06-30T02:01:01.009Z",
          "wordCount": 625,
          "title": "Joint Learning of Portrait Intrinsic Decomposition and Relighting. (arXiv:2106.15305v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15301",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_M/0/1/0/all/0/1\">Monami Banerjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_R/0/1/0/all/0/1\">Rudrasis Chakraborty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouza_J/0/1/0/all/0/1\">Jose Bouza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vemuri_B/0/1/0/all/0/1\">Baba C. Vemuri</a>",
          "description": "Convolutional neural networks have been highly successful in image-based\nlearning tasks due to their translation equivariance property. Recent work has\ngeneralized the traditional convolutional layer of a convolutional neural\nnetwork to non-Euclidean spaces and shown group equivariance of the generalized\nconvolution operation. In this paper, we present a novel higher order Volterra\nconvolutional neural network (VolterraNet) for data defined as samples of\nfunctions on Riemannian homogeneous spaces. Analagous to the result for\ntraditional convolutions, we prove that the Volterra functional convolutions\nare equivariant to the action of the isometry group admitted by the Riemannian\nhomogeneous spaces, and under some restrictions, any non-linear equivariant\nfunction can be expressed as our homogeneous space Volterra convolution,\ngeneralizing the non-linear shift equivariant characterization of Volterra\nexpansions in Euclidean space. We also prove that second order functional\nconvolution operations can be represented as cascaded convolutions which leads\nto an efficient implementation. Beyond this, we also propose a dilated\nVolterraNet model. These advances lead to large parameter reductions relative\nto baseline non-Euclidean CNNs. To demonstrate the efficacy of the VolterraNet\nperformance, we present several real data experiments involving classification\ntasks on spherical-MNIST, atomic energy, Shrec17 data sets, and group testing\non diffusion MRI data. Performance comparisons to the state-of-the-art are also\npresented.",
          "link": "http://arxiv.org/abs/2106.15301",
          "publishedOn": "2021-06-30T02:01:01.003Z",
          "wordCount": 665,
          "title": "VolterraNet: A higher order convolutional network with group equivariance for homogeneous manifolds. (arXiv:2106.15301v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15296",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pereg_D/0/1/0/all/0/1\">Deborah Pereg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_I/0/1/0/all/0/1\">Israel Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vassiliou_A/0/1/0/all/0/1\">Anthony A. Vassiliou</a>",
          "description": "In sparse coding, we attempt to extract features of input vectors, assuming\nthat the data is inherently structured as a sparse superposition of basic\nbuilding blocks. Similarly, neural networks perform a given task by learning\nfeatures of the training data set. Recently both data-driven and model-driven\nfeature extracting methods have become extremely popular and have achieved\nremarkable results. Nevertheless, practical implementations are often too slow\nto be employed in real-life scenarios, especially for real-time applications.\nWe propose a speed-up upgraded version of the classic iterative thresholding\nalgorithm, that produces a good approximation of the convolutional sparse code\nwithin 2-5 iterations. The speed advantage is gained mostly from the\nobservation that most solvers are slowed down by inefficient global\nthresholding. The main idea is to normalize each data point by the local\nreceptive field energy, before applying a threshold. This way, the natural\ninclination towards strong feature expressions is suppressed, so that one can\nrely on a global threshold that can be easily approximated, or learned during\ntraining. The proposed algorithm can be employed with a known predetermined\ndictionary, or with a trained dictionary. The trained version is implemented as\na neural net designed as the unfolding of the proposed solver. The performance\nof the proposed solution is demonstrated via the seismic inversion problem in\nboth synthetic and real data scenarios. We also provide theoretical guarantees\nfor a stable support recovery. Namely, we prove that under certain conditions\nthe true support is perfectly recovered within the first iteration.",
          "link": "http://arxiv.org/abs/2106.15296",
          "publishedOn": "2021-06-30T02:01:00.997Z",
          "wordCount": 697,
          "title": "Convolutional Sparse Coding Fast Approximation with Application to Seismic Reflectivity Estimation. (arXiv:2106.15296v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15268",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Soares_D/0/1/0/all/0/1\">Daniel de Barros Soares</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Andrieux_F/0/1/0/all/0/1\">Fran&#xe7;ois Andrieux</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Hell_B/0/1/0/all/0/1\">Bastien Hell</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Lenhardt_J/0/1/0/all/0/1\">Julien Lenhardt</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Badosa_J/0/1/0/all/0/1\">Jordi Badosa</a> (3), <a href=\"http://arxiv.org/find/cs/1/au:+Gavoille_S/0/1/0/all/0/1\">Sylvain Gavoille</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Gaiffas_S/0/1/0/all/0/1\">St&#xe9;phane Gaiffas</a> (1, 4 and 5), <a href=\"http://arxiv.org/find/cs/1/au:+Bacry_E/0/1/0/all/0/1\">Emmanuel Bacry</a> (1 and 6), ((1) namR, Paris, France, (2) ENSTA Paris, France, (3) LMD, Ecole polytechnique, IP Paris, Palaiseau, France, (4) LPSM, Universit&#xe9; de Paris, France, (5) DMA, Ecole normale sup&#xe9;rieure, Paris, France, (6) CEREMADE, Universit&#xe9; Paris Dauphine, Paris, France)",
          "description": "Estimating the amount of electricity that can be produced by rooftop\nphotovoltaic systems is a time-consuming process that requires on-site\nmeasurements, a difficult task to achieve on a large scale. In this paper, we\npresent an approach to estimate the solar potential of rooftops based on their\nlocation and architectural characteristics, as well as the amount of solar\nradiation they receive annually. Our technique uses computer vision to achieve\nsemantic segmentation of roof sections and roof objects on the one hand, and a\nmachine learning model based on structured building features to predict roof\npitch on the other hand. We then compute the azimuth and maximum number of\nsolar panels that can be installed on a rooftop with geometric approaches.\nFinally, we compute precise shading masks and combine them with solar\nirradiation data that enables us to estimate the yearly solar potential of a\nrooftop.",
          "link": "http://arxiv.org/abs/2106.15268",
          "publishedOn": "2021-06-30T02:01:00.982Z",
          "wordCount": 655,
          "title": "Predicting the Solar Potential of Rooftops using Image Segmentation and Structured Data. (arXiv:2106.15268v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15179",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Richter_H/0/1/0/all/0/1\">Hendrik Richter</a>",
          "description": "Color symmetry implies that the colors of geometrical objects are assigned\naccording to their symmetry properties. It is defined by associating the\nelements of the symmetry group with a color permutation. I use this concept for\ngenerative art and apply symmetry-consistent color distortions to images of\npaintings by Johannes Vermeer. The color permutations are realized as mappings\nof the HSV color space onto itself.",
          "link": "http://arxiv.org/abs/2106.15179",
          "publishedOn": "2021-06-30T02:01:00.966Z",
          "wordCount": 492,
          "title": "Wrong Colored Vermeer: Color-Symmetric Image Distortion. (arXiv:2106.15179v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15299",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zamanitajeddin_N/0/1/0/all/0/1\">Neda Zamanitajeddin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jahanifar_M/0/1/0/all/0/1\">Mostafa Jahanifar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajpoot_N/0/1/0/all/0/1\">Nasir Rajpoot</a>",
          "description": "Digitization of histology images and the advent of new computational methods,\nlike deep learning, have helped the automatic grading of colorectal\nadenocarcinoma cancer (CRA). Present automated CRA grading methods, however,\nusually use tiny image patches and thus fail to integrate the entire tissue\nmicro-architecture for grading purposes. To tackle these challenges, we propose\nto use a statistical network analysis method to describe the complex structure\nof the tissue micro-environment by modelling nuclei and their connections as a\nnetwork. We show that by analyzing only the interactions between the cells in a\nnetwork, we can extract highly discriminative statistical features for CRA\ngrading. Unlike other deep learning or convolutional graph-based approaches,\nour method is highly scalable (can be used for cell networks consist of\nmillions of nodes), completely explainable, and computationally inexpensive. We\ncreate cell networks on a broad CRC histology image dataset, experiment with\nour method, and report state-of-the-art performance for the prediction of\nthree-class CRA grading.",
          "link": "http://arxiv.org/abs/2106.15299",
          "publishedOn": "2021-06-30T02:01:00.937Z",
          "wordCount": 607,
          "title": "Cells are Actors: Social Network Analysis with Classical ML for SOTA Histology Image Classification. (arXiv:2106.15299v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15083",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kulits_P/0/1/0/all/0/1\">Peter Kulits</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wall_J/0/1/0/all/0/1\">Jake Wall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bedetti_A/0/1/0/all/0/1\">Anka Bedetti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henley_M/0/1/0/all/0/1\">Michelle Henley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beery_S/0/1/0/all/0/1\">Sara Beery</a>",
          "description": "African elephants are vital to their ecosystems, but their populations are\nthreatened by a rise in human-elephant conflict and poaching. Monitoring\npopulation dynamics is essential in conservation efforts; however, tracking\nelephants is a difficult task, usually relying on the invasive and sometimes\ndangerous placement of GPS collars. Although there have been many recent\nsuccesses in the use of computer vision techniques for automated identification\nof other species, identification of elephants is extremely difficult and\ntypically requires expertise as well as familiarity with elephants in the\npopulation. We have built and deployed a web-based platform and database for\nhuman-in-the-loop re-identification of elephants combining manual attribute\nlabeling and state-of-the-art computer vision algorithms, known as\nElephantBook. Our system is currently in use at the Mara Elephant Project,\nhelping monitor the protected and at-risk population of elephants in the\nGreater Maasai Mara ecosystem. ElephantBook makes elephant re-identification\nusable by non-experts and scalable for use by multiple conservation NGOs.",
          "link": "http://arxiv.org/abs/2106.15083",
          "publishedOn": "2021-06-30T02:01:00.930Z",
          "wordCount": 595,
          "title": "ElephantBook: A Semi-Automated Human-in-the-Loop System for Elephant Re-Identification. (arXiv:2106.15083v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15121",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1\">Xingqun Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Muyi Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weining Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xiaoxiao Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_C/0/1/0/all/0/1\">Caifeng Shan</a>",
          "description": "Face sketch synthesis has made significant progress with the development of\ndeep neural networks in these years. The delicate depiction of sketch portraits\nfacilitates a wide range of applications like digital entertainment and law\nenforcement. However, accurate and realistic face sketch generation is still a\nchallenging task due to the illumination variations and complex backgrounds in\nthe real scenes. To tackle these challenges, we propose a novel Semantic-Driven\nGenerative Adversarial Network (SDGAN) which embeds global structure-level\nstyle injection and local class-level knowledge re-weighting. Specifically, we\nconduct facial saliency detection on the input face photos to provide overall\nfacial texture structure, which could be used as a global type of prior\ninformation. In addition, we exploit face parsing layouts as the semantic-level\nspatial prior to enforce globally structural style injection in the generator\nof SDGAN. Furthermore, to enhance the realistic effect of the details, we\npropose a novel Adaptive Re-weighting Loss (ARLoss) which dedicates to balance\nthe contributions of different semantic classes. Experimentally, our extensive\nexperiments on CUFS and CUFSF datasets show that our proposed algorithm\nachieves state-of-the-art performance.",
          "link": "http://arxiv.org/abs/2106.15121",
          "publishedOn": "2021-06-30T02:01:00.900Z",
          "wordCount": 618,
          "title": "Face Sketch Synthesis via Semantic-Driven Generative Adversarial Network. (arXiv:2106.15121v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15130",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Conti_M/0/1/0/all/0/1\">Mauro Conti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milani_S/0/1/0/all/0/1\">Simone Milani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nowroozi_E/0/1/0/all/0/1\">Ehsan Nowroozi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orazi_G/0/1/0/all/0/1\">Gabriele Orazi</a>",
          "description": "The last-generation video conferencing software allows users to utilize a\nvirtual background to conceal their personal environment due to privacy\nconcerns, especially in official meetings with other employers. On the other\nhand, users maybe want to fool people in the meeting by considering the virtual\nbackground to conceal where they are. In this case, developing tools to\nunderstand the virtual background utilize for fooling people in meeting plays\nan important role. Besides, such detectors must prove robust against different\nkinds of attacks since a malicious user can fool the detector by applying a set\nof adversarial editing steps on the video to conceal any revealing footprint.\nIn this paper, we study the feasibility of an efficient tool to detect whether\na videoconferencing user background is real. In particular, we provide the\nfirst tool which computes pixel co-occurrences matrices and uses them to search\nfor inconsistencies among spectral and spatial bands. Our experiments confirm\nthat cross co-occurrences matrices improve the robustness of the detector\nagainst different kinds of attacks. This work's performance is especially\nnoteworthy with regard to color SPAM features. Moreover, the performance\nespecially is significant with regard to robustness versus post-processing,\nlike geometric transformations, filtering, contrast enhancement, and JPEG\ncompression with different quality factors.",
          "link": "http://arxiv.org/abs/2106.15130",
          "publishedOn": "2021-06-30T02:01:00.884Z",
          "wordCount": 669,
          "title": "Do Not Deceive Your Employer with a Virtual Background: A Video Conferencing Manipulation-Detection System. (arXiv:2106.15130v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15286",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kirsten_L/0/1/0/all/0/1\">Lucas N. Kirsten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piccoli_R/0/1/0/all/0/1\">Ricardo Piccoli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ribani_R/0/1/0/all/0/1\">Ricardo Ribani</a>",
          "description": "This work evaluates six state-of-the-art deep neural network (DNN)\narchitectures applied to the problem of enhancing camera-captured document\nimages. The results from each network were evaluated both qualitatively and\nquantitatively using Image Quality Assessment (IQA) metrics, and also compared\nwith an existing approach based on traditional computer vision techniques. The\nbest performing architectures generally produced good enhancement compared to\nthe existing algorithm, showing that it is possible to use DNNs for document\nimage enhancement. Furthermore, the best performing architectures could work as\na baseline for future investigations on document enhancement using deep\nlearning techniques. The main contributions of this paper are: a baseline of\ndeep learning techniques that can be further improved to provide better\nresults, and a evaluation methodology using IQA metrics for quantitatively\ncomparing the produced images from the neural networks to a ground truth.",
          "link": "http://arxiv.org/abs/2106.15286",
          "publishedOn": "2021-06-30T02:01:00.879Z",
          "wordCount": 597,
          "title": "Evaluating Deep Neural Networks for Image Document Enhancement. (arXiv:2106.15286v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15258",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ning_R/0/1/0/all/0/1\">Ranyu Ning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Can Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yuexian Zou</a>",
          "description": "Temporal action detection (TAD) is a challenging task which aims to\ntemporally localize and recognize the human action in untrimmed videos. Current\nmainstream one-stage TAD approaches localize and classify action proposals\nrelying on pre-defined anchors, where the location and scale for action\ninstances are set by designers. Obviously, such an anchor-based TAD method\nlimits its generalization capability and will lead to performance degradation\nwhen videos contain rich action variation. In this study, we explore to remove\nthe requirement of pre-defined anchors for TAD methods. A novel TAD model\ntermed as Selective Receptive Field Network (SRF-Net) is developed, in which\nthe location offsets and classification scores at each temporal location can be\ndirectly estimated in the feature map and SRF-Net is trained in an end-to-end\nmanner. Innovatively, a building block called Selective Receptive Field\nConvolution (SRFC) is dedicatedly designed which is able to adaptively adjust\nits receptive field size according to multiple scales of input information at\neach temporal location in the feature map. Extensive experiments are conducted\non the THUMOS14 dataset, and superior results are reported comparing to\nstate-of-the-art TAD approaches.",
          "link": "http://arxiv.org/abs/2106.15258",
          "publishedOn": "2021-06-30T02:01:00.862Z",
          "wordCount": 624,
          "title": "SRF-Net: Selective Receptive Field Network for Anchor-Free Temporal Action Detection. (arXiv:2106.15258v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15282",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ho_J/0/1/0/all/0/1\">Jonathan Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saharia_C/0/1/0/all/0/1\">Chitwan Saharia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_W/0/1/0/all/0/1\">William Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fleet_D/0/1/0/all/0/1\">David J. Fleet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Norouzi_M/0/1/0/all/0/1\">Mohammad Norouzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salimans_T/0/1/0/all/0/1\">Tim Salimans</a>",
          "description": "We show that cascaded diffusion models are capable of generating high\nfidelity images on the class-conditional ImageNet generation challenge, without\nany assistance from auxiliary image classifiers to boost sample quality. A\ncascaded diffusion model comprises a pipeline of multiple diffusion models that\ngenerate images of increasing resolution, beginning with a standard diffusion\nmodel at the lowest resolution, followed by one or more super-resolution\ndiffusion models that successively upsample the image and add higher resolution\ndetails. We find that the sample quality of a cascading pipeline relies\ncrucially on conditioning augmentation, our proposed method of data\naugmentation of the lower resolution conditioning inputs to the\nsuper-resolution models. Our experiments show that conditioning augmentation\nprevents compounding error during sampling in a cascaded model, helping us to\ntrain cascading pipelines achieving FID scores of 1.48 at 64x64, 3.52 at\n128x128 and 4.88 at 256x256 resolutions, outperforming BigGAN-deep.",
          "link": "http://arxiv.org/abs/2106.15282",
          "publishedOn": "2021-06-30T02:01:00.857Z",
          "wordCount": 593,
          "title": "Cascaded Diffusion Models for High Fidelity Image Generation. (arXiv:2106.15282v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15277",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_Z/0/1/0/all/0/1\">Zhuangwei Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Rong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuanqing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_K/0/1/0/all/0/1\">Kui Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qicheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1\">Mingkui Tan</a>",
          "description": "3D LiDAR (light detection and ranging) based semantic segmentation is\nimportant in scene understanding for many applications, such as auto-driving\nand robotics. For example, for autonomous cars equipped with RGB cameras and\nLiDAR, it is crucial to fuse complementary information from different sensors\nfor robust and accurate segmentation. Existing fusion-based methods, however,\nmay not achieve promising performance due to the vast difference between two\nmodalities. In this work, we investigate a collaborative fusion scheme called\nperception-aware multi-sensor fusion (PMF) to exploit perceptual information\nfrom two modalities, namely, appearance information from RGB images and\nspatio-depth information from point clouds. To this end, we first project point\nclouds to the camera coordinates to provide spatio-depth information for RGB\nimages. Then, we propose a two-stream network to extract features from the two\nmodalities, separately, and fuse the features by effective residual-based\nfusion modules. Moreover, we propose additional perception-aware losses to\nmeasure the great perceptual difference between the two modalities. Extensive\nexperiments on two benchmark data sets show the superiority of our method. For\nexample, on nuScenes, our PMF outperforms the state-of-the-art method by 0.8%\nin mIoU.",
          "link": "http://arxiv.org/abs/2106.15277",
          "publishedOn": "2021-06-30T02:01:00.849Z",
          "wordCount": 627,
          "title": "Perception-aware Multi-sensor Fusion for 3D LiDAR Semantic Segmentation. (arXiv:2106.15277v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15280",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yiqin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_T/0/1/0/all/0/1\">Tian Guo</a>",
          "description": "Omnidirectional lighting provides the foundation for achieving\nspatially-variant photorealistic 3D rendering, a desirable property for mobile\naugmented reality applications. However, in practice, estimating\nomnidirectional lighting can be challenging due to limitations such as partial\npanoramas of the rendering positions, and the inherent environment lighting and\nmobile user dynamics. A new opportunity arises recently with the advancements\nin mobile 3D vision, including built-in high-accuracy depth sensors and deep\nlearning-powered algorithms, which provide the means to better sense and\nunderstand the physical surroundings. Centering the key idea of 3D vision, in\nthis work, we design an edge-assisted framework called Xihe to provide mobile\nAR applications the ability to obtain accurate omnidirectional lighting\nestimation in real time. Specifically, we develop a novel sampling technique\nthat efficiently compresses the raw point cloud input generated at the mobile\ndevice. This technique is derived based on our empirical analysis of a recent\n3D indoor dataset and plays a key role in our 3D vision-based lighting\nestimator pipeline design. To achieve the real-time goal, we develop a tailored\nGPU pipeline for on-device point cloud processing and use an encoding technique\nthat reduces network transmitted bytes. Finally, we present an adaptive\ntriggering strategy that allows Xihe to skip unnecessary lighting estimations\nand a practical way to provide temporal coherent rendering integration with the\nmobile AR ecosystem. We evaluate both the lighting estimation accuracy and time\nof Xihe using a reference mobile application developed with Xihe's APIs. Our\nresults show that Xihe takes as fast as 20.67ms per lighting estimation and\nachieves 9.4% better estimation accuracy than a state-of-the-art neural\nnetwork.",
          "link": "http://arxiv.org/abs/2106.15280",
          "publishedOn": "2021-06-30T02:01:00.832Z",
          "wordCount": 705,
          "title": "Xihe: A 3D Vision-based Lighting Estimation Framework for Mobile Augmented Reality. (arXiv:2106.15280v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15257",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kashi_M/0/1/0/all/0/1\">Mohammad Amin Kashi</a>",
          "description": "Depth perception is fundamental for robots to understand the surrounding\nenvironment. As the view of cognitive neuroscience, visual depth perception\nmethods are divided into three categories, namely binocular, active, and\npictorial. The first two categories have been studied for decades in detail.\nHowever, research for the exploration of the third category is still in its\ninfancy and has got momentum by the advent of deep learning methods in recent\nyears. In cognitive neuroscience, it is known that pictorial depth perception\nmechanisms are dependent on the perception of seen objects. Inspired by this\nfact, in this thesis, we investigated the relation of perception of objects and\ndepth estimation convolutional neural networks. For this purpose, we developed\nnew network structures based on a simple depth estimation network that only\nused a single image at its input. Our proposed structures use both an image and\na semantic label of the image as their input. We used semantic labels as the\noutput of object perception. The obtained results of performance comparison\nbetween the developed network and original network showed that our novel\nstructures can improve the performance of depth estimation by 52\\% of relative\nerror of distance in the examined cases. Most of the experimental studies were\ncarried out on synthetic datasets that were generated by game engines to\nisolate the performance comparison from the effect of inaccurate depth and\nsemantic labels of non-synthetic datasets. It is shown that particular\nsynthetic datasets may be used for training of depth networks in cases that an\nappropriate dataset is not available. Furthermore, we showed that in these\ncases, usage of semantic labels improves the robustness of the network against\ndomain shift from synthetic training data to non-synthetic test data.",
          "link": "http://arxiv.org/abs/2106.15257",
          "publishedOn": "2021-06-30T02:01:00.826Z",
          "wordCount": 748,
          "title": "Predicting Depth from Semantic Segmentation using Game Engine Dataset. (arXiv:2106.15257v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15113",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Ziquan Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1\">Shenghua Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiuli Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_S/0/1/0/all/0/1\">Shaoqun Zeng</a>",
          "description": "Digital gigapixel whole slide image (WSI) is widely used in clinical\ndiagnosis, and automated WSI analysis is key for computer-aided diagnosis.\nCurrently, analyzing the integrated descriptor of probabilities or feature maps\nfrom massive local patches encoded by ResNet classifier is the main manner for\nWSI-level prediction. Feature representations of the sparse and tiny lesion\ncells in cervical slides, however, are still challengeable for the\nunder-promoted upstream encoders, while the unused spatial representations of\ncervical cells are the available features to supply the semantics analysis. As\nwell as patches sampling with overlap and repetitive processing incur the\ninefficiency and the unpredictable side effect. This study designs a novel\ninline connection network (InCNet) by enriching the multi-scale connectivity to\nbuild the lightweight model named You Only Look Cytopathology Once (YOLCO) with\nthe additional supervision of spatial information. The proposed model allows\nthe input size enlarged to megapixel that can stitch the WSI without any\noverlap by the average repeats decreased from $10^3\\sim10^4$ to $10^1\\sim10^2$\nfor collecting features and predictions at two scales. Based on Transformer for\nclassifying the integrated multi-scale multi-task features, the experimental\nresults appear $0.872$ AUC score better and $2.51\\times$ faster than the best\nconventional method in WSI classification on multicohort datasets of 2,019\nslides from four scanning devices.",
          "link": "http://arxiv.org/abs/2106.15113",
          "publishedOn": "2021-06-30T02:01:00.820Z",
          "wordCount": 672,
          "title": "An Efficient Cervical Whole Slide Image Analysis Framework Based on Multi-scale Semantic and Spatial Features using Deep Learning. (arXiv:2106.15113v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15097",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wu_Q/0/1/0/all/0/1\">Qing Wu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1\">Yuwei Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_L/0/1/0/all/0/1\">Lan Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Feng_R/0/1/0/all/0/1\">Ruiming Feng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wei_H/0/1/0/all/0/1\">Hongjiang Wei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_Q/0/1/0/all/0/1\">Qing Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_B/0/1/0/all/0/1\">Boliang Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_X/0/1/0/all/0/1\">Xiaozhao Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_J/0/1/0/all/0/1\">Jingyi Yu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuyao Zhang</a>",
          "description": "For collecting high-quality high-resolution (HR) MR image, we propose a novel\nimage reconstruction network named IREM, which is trained on multiple\nlow-resolution (LR) MR images and achieve an arbitrary up-sampling rate for HR\nimage reconstruction. In this work, we suppose the desired HR image as an\nimplicit continuous function of the 3D image spatial coordinate and the\nthick-slice LR images as several sparse discrete samplings of this function.\nThen the super-resolution (SR) task is to learn the continuous volumetric\nfunction from a limited observations using an fully-connected neural network\ncombined with Fourier feature positional encoding. By simply minimizing the\nerror between the network prediction and the acquired LR image intensity across\neach imaging plane, IREM is trained to represent a continuous model of the\nobserved tissue anatomy. Experimental results indicate that IREM succeeds in\nrepresenting high frequency image feature, and in real scene data collection,\nIREM reduces scan time and achieves high-quality high-resolution MR imaging in\nterms of SNR and local image detail.",
          "link": "http://arxiv.org/abs/2106.15097",
          "publishedOn": "2021-06-30T02:01:00.815Z",
          "wordCount": 631,
          "title": "IREM: High-Resolution Magnetic Resonance (MR) Image Reconstruction via Implicit Neural Representation. (arXiv:2106.15097v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15176",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pucci_R/0/1/0/all/0/1\">Rita Pucci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martinel_N/0/1/0/all/0/1\">Niki Martinel</a>",
          "description": "Automatic image colourisation is the computer vision research path that\nstudies how to colourise greyscale images (for restoration). Deep learning\ntechniques improved image colourisation yielding astonishing results. These\ndiffer by various factors, such as structural differences, input types, user\nassistance, etc. Most of them, base the architectural structure on\nconvolutional layers with no emphasis on layers specialised in object features\nextraction. We introduce a novel downsampling upsampling architecture named\nTUCaN (Tiny UCapsNet) that exploits the collaboration of convolutional layers\nand capsule layers to obtain a neat colourisation of entities present in every\nsingle image. This is obtained by enforcing collaboration among such layers by\nskip and residual connections. We pose the problem as a per pixel colour\nclassification task that identifies colours as a bin in a quantized space. To\ntrain the network, in contrast with the standard end to end learning method, we\npropose the progressive learning scheme to extract the context of objects by\nonly manipulating the learning process without changing the model. In this\nscheme, the upsampling starts from the reconstruction of low resolution images\nand progressively grows to high resolution images throughout the training\nphase. Experimental results on three benchmark datasets show that our approach\nwith ImageNet10k dataset outperforms existing methods on standard quality\nmetrics and achieves state of the art performances on image colourisation. We\nperformed a user study to quantify the perceptual realism of the colourisation\nresults demonstrating: that progressive learning let the TUCaN achieve better\ncolours than the end to end scheme; and pointing out the limitations of the\nexisting evaluation metrics.",
          "link": "http://arxiv.org/abs/2106.15176",
          "publishedOn": "2021-06-30T02:01:00.808Z",
          "wordCount": 691,
          "title": "TUCaN: Progressively Teaching Colourisation to Capsules. (arXiv:2106.15176v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15117",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Truong_S/0/1/0/all/0/1\">Son Nguyen Truong</a>",
          "description": "We present a novel data generation tool for document processing. The tool\nfocuses on providing a maximal level of visual information in a normal type\ndocument, ranging from character position to paragraph-level position. It also\nenables working with a large dataset on low-resource languages as well as\nproviding a mean of processing thorough full-level information of the\ndocumented text. The data generation tools come with a dataset of 320000\nVietnamese synthetic document images and an instruction to generate a dataset\nof similar size in other languages. The repository can be found at:\nhttps://github.com/tson1997/SDL-Document-Image-Generation",
          "link": "http://arxiv.org/abs/2106.15117",
          "publishedOn": "2021-06-30T02:01:00.793Z",
          "wordCount": 529,
          "title": "SDL: New data generation tools for full-level annotated document layout. (arXiv:2106.15117v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15064",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tu_P/0/1/0/all/0/1\">Peng Tu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yawen Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1\">Rongrong Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_F/0/1/0/all/0/1\">Feng Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>",
          "description": "Semi-supervised learning is a challenging problem which aims to construct a\nmodel by learning from a limited number of labeled examples. Numerous methods\nhave been proposed to tackle this problem, with most focusing on utilizing the\npredictions of unlabeled instances consistency alone to regularize networks.\nHowever, treating labeled and unlabeled data separately often leads to the\ndiscarding of mass prior knowledge learned from the labeled examples, and\nfailure to mine the feature interaction between the labeled and unlabeled image\npairs. In this paper, we propose a novel method for semi-supervised semantic\nsegmentation named GuidedMix-Net, by leveraging labeled information to guide\nthe learning of unlabeled instances. Specifically, we first introduce a feature\nalignment objective between labeled and unlabeled data to capture potentially\nsimilar image pairs and then generate mixed inputs from them. The proposed\nmutual information transfer (MITrans), based on the cluster assumption, is\nshown to be a powerful knowledge module for further progressive refining\nfeatures of unlabeled data in the mixed data space. To take advantage of the\nlabeled examples and guide unlabeled data learning, we further propose a mask\ngeneration module to generate high-quality pseudo masks for the unlabeled data.\nAlong with supervised learning for labeled data, the prediction of unlabeled\ndata is jointly learned with the generated pseudo masks from the mixed data.\nExtensive experiments on PASCAL VOC 2012, PASCAL-Context and Cityscapes\ndemonstrate the effectiveness of our GuidedMix-Net, which achieves competitive\nsegmentation accuracy and significantly improves the mIoU by +7$\\%$ compared to\nprevious state-of-the-art approaches.",
          "link": "http://arxiv.org/abs/2106.15064",
          "publishedOn": "2021-06-30T02:01:00.788Z",
          "wordCount": 694,
          "title": "GuidedMix-Net: Learning to Improve Pseudo Masks Using Labeled Images as Reference. (arXiv:2106.15064v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15125",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yi-Fan Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_C/0/1/0/all/0/1\">Caifeng Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liang Wang</a>",
          "description": "One essential problem in skeleton-based action recognition is how to extract\ndiscriminative features over all skeleton joints. However, the complexity of\nthe recent State-Of-The-Art (SOTA) models for this task tends to be exceedingly\nsophisticated and over-parameterized. The low efficiency in model training and\ninference has increased the validation costs of model architectures in\nlarge-scale datasets. To address the above issue, recent advanced separable\nconvolutional layers are embedded into an early fused Multiple Input Branches\n(MIB) network, constructing an efficient Graph Convolutional Network (GCN)\nbaseline for skeleton-based action recognition. In addition, based on such the\nbaseline, we design a compound scaling strategy to expand the model's width and\ndepth synchronously, and eventually obtain a family of efficient GCN baselines\nwith high accuracies and small amounts of trainable parameters, termed\nEfficientGCN-Bx, where ''x'' denotes the scaling coefficient. On two\nlarge-scale datasets, i.e., NTU RGB+D 60 and 120, the proposed EfficientGCN-B4\nbaseline outperforms other SOTA methods, e.g., achieving 91.7% accuracy on the\ncross-subject benchmark of NTU 60 dataset, while being 3.15x smaller and 3.21x\nfaster than MS-G3D, which is one of the best SOTA methods. The source code in\nPyTorch version and the pretrained models are available at\nhttps://github.com/yfsong0709/EfficientGCNv1.",
          "link": "http://arxiv.org/abs/2106.15125",
          "publishedOn": "2021-06-30T02:01:00.783Z",
          "wordCount": 654,
          "title": "Constructing Stronger and Faster Baselines for Skeleton-based Action Recognition. (arXiv:2106.15125v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15278",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1\">Geeho Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1\">Bohyung Han</a>",
          "description": "Visual recognition tasks are often limited to dealing with a small subset of\nclasses simply because the labels for the remaining classes are unavailable. We\nare interested in identifying novel concepts in a dataset through\nrepresentation learning based on the examples in both labeled and unlabeled\nclasses, and extending the horizon of recognition to both known and novel\nclasses. To address this challenging task, we propose a combinatorial learning\napproach, which naturally clusters the examples in unseen classes using the\ncompositional knowledge given by multiple supervised meta-classifiers on\nheterogeneous label spaces. We also introduce a metric learning strategy to\nestimate pairwise pseudo-labels for improving representations of unlabeled\nexamples, which preserves semantic relations across known and novel classes\neffectively. The proposed algorithm discovers novel concepts via a joint\noptimization of enhancing the discrimitiveness of unseen classes as well as\nlearning the representations of known classes generalizable to novel ones. Our\nextensive experiments demonstrate remarkable performance gains by the proposed\napproach in multiple image retrieval and novel class discovery benchmarks.",
          "link": "http://arxiv.org/abs/2106.15278",
          "publishedOn": "2021-06-30T02:01:00.777Z",
          "wordCount": 609,
          "title": "Open-Set Representation Learning through Combinatorial Embedding. (arXiv:2106.15278v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15087",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mo_K/0/1/0/all/0/1\">Kaichun Mo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1\">Yuzhe Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_F/0/1/0/all/0/1\">Fanbo Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hao Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guibas_L/0/1/0/all/0/1\">Leonidas Guibas</a>",
          "description": "Contrary to the vast literature in modeling, perceiving, and understanding\nagent-object (e.g., human-object, hand-object, robot-object) interaction in\ncomputer vision and robotics, very few past works have studied the task of\nobject-object interaction, which also plays an important role in robotic\nmanipulation and planning tasks. There is a rich space of object-object\ninteraction scenarios in our daily life, such as placing an object on a messy\ntabletop, fitting an object inside a drawer, pushing an object using a tool,\netc. In this paper, we propose a unified affordance learning framework to learn\nobject-object interaction for various tasks. By constructing four object-object\ninteraction task environments using physical simulation (SAPIEN) and thousands\nof ShapeNet models with rich geometric diversity, we are able to conduct\nlarge-scale object-object affordance learning without the need for human\nannotations or demonstrations. At the core of technical contribution, we\npropose an object-kernel point convolution network to reason about detailed\ninteraction between two objects. Experiments on large-scale synthetic data and\nreal-world data prove the effectiveness of the proposed approach. Please refer\nto the project webpage for code, data, video, and more materials:\nhttps://cs.stanford.edu/~kaichun/o2oafford",
          "link": "http://arxiv.org/abs/2106.15087",
          "publishedOn": "2021-06-30T02:01:00.772Z",
          "wordCount": 622,
          "title": "O2O-Afford: Annotation-Free Large-Scale Object-Object Affordance Learning. (arXiv:2106.15087v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15202",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bai_T/0/1/0/all/0/1\">Tao Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jinqi Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jun Zhao</a>",
          "description": "Deep learning based image recognition systems have been widely deployed on\nmobile devices in today's world. In recent studies, however, deep learning\nmodels are shown vulnerable to adversarial examples. One variant of adversarial\nexamples, called adversarial patch, draws researchers' attention due to its\nstrong attack abilities. Though adversarial patches achieve high attack success\nrates, they are easily being detected because of the visual inconsistency\nbetween the patches and the original images. Besides, it usually requires a\nlarge amount of data for adversarial patch generation in the literature, which\nis computationally expensive and time-consuming. To tackle these challenges, we\npropose an approach to generate inconspicuous adversarial patches with one\nsingle image. In our approach, we first decide the patch locations basing on\nthe perceptual sensitivity of victim models, then produce adversarial patches\nin a coarse-to-fine way by utilizing multiple-scale generators and\ndiscriminators. The patches are encouraged to be consistent with the background\nimages with adversarial training while preserving strong attack abilities. Our\napproach shows the strong attack abilities in white-box settings and the\nexcellent transferability in black-box settings through extensive experiments\non various models with different architectures and training methods. Compared\nto other adversarial patches, our adversarial patches hold the most negligible\nrisks to be detected and can evade human observations, which is supported by\nthe illustrations of saliency maps and results of user evaluations. Lastly, we\nshow that our adversarial patches can be applied in the physical world.",
          "link": "http://arxiv.org/abs/2106.15202",
          "publishedOn": "2021-06-30T02:01:00.756Z",
          "wordCount": 691,
          "title": "Inconspicuous Adversarial Patches for Fooling Image Recognition Systems on Mobile Devices. (arXiv:2106.15202v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15252",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1\">Kai Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rebuffi_S/0/1/0/all/0/1\">Sylvestre-Alvise Rebuffi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ehrhardt_S/0/1/0/all/0/1\">S&#xe9;bastien Ehrhardt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vedaldi_A/0/1/0/all/0/1\">Andrea Vedaldi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zisserman_A/0/1/0/all/0/1\">Andrew Zisserman</a>",
          "description": "We tackle the problem of discovering novel classes in an image collection\ngiven labelled examples of other classes. We present a new approach called\nAutoNovel to address this problem by combining three ideas: (1) we suggest that\nthe common approach of bootstrapping an image representation using the labelled\ndata only introduces an unwanted bias, and that this can be avoided by using\nself-supervised learning to train the representation from scratch on the union\nof labelled and unlabelled data; (2) we use ranking statistics to transfer the\nmodel's knowledge of the labelled classes to the problem of clustering the\nunlabelled images; and, (3) we train the data representation by optimizing a\njoint objective function on the labelled and unlabelled subsets of the data,\nimproving both the supervised classification of the labelled data, and the\nclustering of the unlabelled data. Moreover, we propose a method to estimate\nthe number of classes for the case where the number of new categories is not\nknown a priori. We evaluate AutoNovel on standard classification benchmarks and\nsubstantially outperform current methods for novel category discovery. In\naddition, we also show that AutoNovel can be used for fully unsupervised image\nclustering, achieving promising results.",
          "link": "http://arxiv.org/abs/2106.15252",
          "publishedOn": "2021-06-30T02:01:00.750Z",
          "wordCount": 652,
          "title": "AutoNovel: Automatically Discovering and Learning Novel Visual Categories. (arXiv:2106.15252v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15281",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rosso_M/0/1/0/all/0/1\">Maria Pia Del Rosso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sebastianelli_A/0/1/0/all/0/1\">Alessandro Sebastianelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spiller_D/0/1/0/all/0/1\">Dario Spiller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathieu_P/0/1/0/all/0/1\">Pierre Philippe Mathieu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ullo_S/0/1/0/all/0/1\">Silvia Liberata Ullo</a>",
          "description": "In recent years, the growth of Machine Learning algorithms in a variety of\ndifferent applications has raised numerous studies on the applicability of\nthese algorithms in real scenarios. Among all, one of the hardest scenarios,\ndue to its physical requirements, is the aerospace one. In this context, the\nauthors of this work aim to propose a first prototype and a study of\nfeasibility for an AI model to be 'loaded' on board. As a case study, the\nauthors decided to investigate the detection of volcanic eruptions as a method\nto swiftly produce alerts. Two Convolutional Neural Networks have been proposed\nand created, also showing how to correctly implement them on real hardware and\nhow the complexity of a CNN can be adapted to fit computational requirements.",
          "link": "http://arxiv.org/abs/2106.15281",
          "publishedOn": "2021-06-30T02:01:00.745Z",
          "wordCount": 597,
          "title": "On-board Volcanic Eruption Detection through CNNs and Satellite Multispectral Imagery. (arXiv:2106.15281v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15183",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bakhtiarnia_A/0/1/0/all/0/1\">Arian Bakhtiarnia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iosifidis_A/0/1/0/all/0/1\">Alexandros Iosifidis</a>",
          "description": "Deep neural networks can be converted to multi-exit architectures by\ninserting early exit branches after some of their intermediate layers. This\nallows their inference process to become dynamic, which is useful for time\ncritical IoT applications with stringent latency requirements, but with\ntime-variant communication and computation resources. In particular, in edge\ncomputing systems and IoT networks where the exact computation time budget is\nvariable and not known beforehand. Vision Transformer is a recently proposed\narchitecture which has since found many applications across various domains of\ncomputer vision. In this work, we propose seven different architectures for\nearly exit branches that can be used for dynamic inference in Vision\nTransformer backbones. Through extensive experiments involving both\nclassification and regression problems, we show that each one of our proposed\narchitectures could prove useful in the trade-off between accuracy and speed.",
          "link": "http://arxiv.org/abs/2106.15183",
          "publishedOn": "2021-06-30T02:01:00.739Z",
          "wordCount": 568,
          "title": "Multi-Exit Vision Transformer for Dynamic Inference. (arXiv:2106.15183v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15171",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Caldero_M/0/1/0/all/0/1\">Manuel Sarmiento Calder&#xf3;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varas_D/0/1/0/all/0/1\">David Varas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bou_Balust_E/0/1/0/all/0/1\">Elisenda Bou-Balust</a>",
          "description": "Research in action detection has grown in the recentyears, as it plays a key\nrole in video understanding. Modelling the interactions (either spatial or\ntemporal) between actors and their context has proven to be essential for this\ntask. While recent works use spatial features with aggregated temporal\ninformation, this work proposes to use non-aggregated temporal information.\nThis is done by adding an attention based method that leverages spatio-temporal\ninteractions between elements in the scene along the clip.The main contribution\nof this work is the introduction of two cross attention blocks to effectively\nmodel the spatial relations and capture short range temporal\ninteractions.Experiments on the AVA dataset show the advantages of the proposed\napproach that models spatio-temporal relations between relevant elements in the\nscene, outperforming other methods that model actor interactions with their\ncontext by +0.31 mAP.",
          "link": "http://arxiv.org/abs/2106.15171",
          "publishedOn": "2021-06-30T02:01:00.734Z",
          "wordCount": 571,
          "title": "Spatio-Temporal Context for Action Detection. (arXiv:2106.15171v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15274",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aliyev_N/0/1/0/all/0/1\">Namig Aliyev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sezer_O/0/1/0/all/0/1\">Oguzhan Sezer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guzel_M/0/1/0/all/0/1\">Mehmet Turan Guzel</a>",
          "description": "Autonomous systems require identifying the environment and it has a long way\nto go before putting it safely into practice. In autonomous driving systems,\nthe detection of obstacles and traffic lights are of importance as well as lane\ntracking. In this study, an autonomous driving system is developed and tested\nin the experimental environment designed for this purpose. In this system, a\nmodel vehicle having a camera is used to trace the lanes and avoid obstacles to\nexperimentally study autonomous driving behavior. Convolutional Neural Network\nmodels were trained for Lane tracking. For the vehicle to avoid obstacles,\ncorner detection, optical flow, focus of expansion, time to collision, balance\ncalculation, and decision mechanism were created, respectively.",
          "link": "http://arxiv.org/abs/2106.15274",
          "publishedOn": "2021-06-30T02:01:00.687Z",
          "wordCount": 573,
          "title": "Autonomous Driving Implementation in an Experimental Environment. (arXiv:2106.15274v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15232",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tsuji_K/0/1/0/all/0/1\">Kaigen Tsuji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haraguchi_D/0/1/0/all/0/1\">Daichi Haraguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uchida_S/0/1/0/all/0/1\">Seiichi Uchida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iwana_B/0/1/0/all/0/1\">Brian Kenji Iwana</a>",
          "description": "Fonts have had trends throughout their history, not only in when they were\ninvented but also in their usage and popularity. In this paper, we attempt to\nspecifically find the trends in font usage using robust regression on a large\ncollection of text images. We utilize movie posters as the source of fonts for\nthis task because movie posters can represent time periods by using their\nrelease date. In addition, movie posters are documents that are carefully\ndesigned and represent a wide range of fonts. To understand the relationship\nbetween the fonts of movie posters and time, we use a regression Convolutional\nNeural Network (CNN) to estimate the release year of a movie using an isolated\ntitle text image. Due to the difficulty of the task, we propose to use of a\nhybrid training regimen that uses a combination of Mean Squared Error (MSE) and\nTukey's biweight loss. Furthermore, we perform a thorough analysis on the\ntrends of fonts through time.",
          "link": "http://arxiv.org/abs/2106.15232",
          "publishedOn": "2021-06-30T02:01:00.682Z",
          "wordCount": 615,
          "title": "Using Robust Regression to Find Font Usage Trends. (arXiv:2106.15232v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15067",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1\">Xiang Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zihang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Heng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yong Li</a>",
          "description": "Attention Mechanism is a widely used method for improving the performance of\nconvolutional neural networks (CNNs) on computer vision tasks. Despite its\npervasiveness, we have a poor understanding of what its effectiveness stems\nfrom. It is popularly believed that its effectiveness stems from the visual\nattention explanation, advocating focusing on the important part of input data\nrather than ingesting the entire input. In this paper, we find that there is\nonly a weak consistency between the attention weights of features and their\nimportance. Instead, we verify the crucial role of feature map multiplication\nin attention mechanism and uncover a fundamental impact of feature map\nmultiplication on the learned landscapes of CNNs: with the high order\nnon-linearity brought by the feature map multiplication, it played a\nregularization role on CNNs, which made them learn smoother and more stable\nlandscapes near real samples compared to vanilla CNNs. This smoothness and\nstability induce a more predictive and stable behavior in-between real samples,\nand make CNNs generate better. Moreover, motivated by the proposed\neffectiveness of feature map multiplication, we design feature map\nmultiplication network (FMMNet) by simply replacing the feature map addition in\nResNet with feature map multiplication. FMMNet outperforms ResNet on various\ndatasets, and this indicates that feature map multiplication plays a vital role\nin improving the performance even without finely designed attention mechanism\nin existing methods.",
          "link": "http://arxiv.org/abs/2106.15067",
          "publishedOn": "2021-06-30T02:01:00.585Z",
          "wordCount": 662,
          "title": "Towards Understanding the Effectiveness of Attention Mechanism. (arXiv:2106.15067v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15004",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Deo_N/0/1/0/all/0/1\">Nachiket Deo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolff_E/0/1/0/all/0/1\">Eric M. Wolff</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beijbom_O/0/1/0/all/0/1\">Oscar Beijbom</a>",
          "description": "Accurately predicting the future motion of surrounding vehicles requires\nreasoning about the inherent uncertainty in goals and driving behavior. This\nuncertainty can be loosely decoupled into lateral (e.g., keeping lane, turning)\nand longitudinal (e.g., accelerating, braking). We present a novel method that\ncombines learned discrete policy rollouts with a focused decoder on subsets of\nthe lane graph. The policy rollouts explore different goals given our current\nobservations, ensuring that the model captures lateral variability. The\nlongitudinal variability is captured by our novel latent variable model decoder\nthat is conditioned on various subsets of the lane graph. Our model achieves\nstate-of-the-art performance on the nuScenes motion prediction dataset, and\nqualitatively demonstrates excellent scene compliance. Detailed ablations\nhighlight the importance of both the policy rollouts and the decoder\narchitecture.",
          "link": "http://arxiv.org/abs/2106.15004",
          "publishedOn": "2021-06-30T02:01:00.579Z",
          "wordCount": 562,
          "title": "Multimodal Trajectory Prediction Conditioned on Lane-Graph Traversals. (arXiv:2106.15004v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15011",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Boulahbal_H/0/1/0/all/0/1\">Houssem-eddine Boulahbal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Voicila_A/0/1/0/all/0/1\">Adrian Voicila</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Comport_A/0/1/0/all/0/1\">Andrew Comport</a>",
          "description": "This paper proposes two important contributions for conditional Generative\nAdversarial Networks (cGANs) to improve the wide variety of applications that\nexploit this architecture. The first main contribution is an analysis of cGANs\nto show that they are not explicitly conditional. In particular, it will be\nshown that the discriminator and subsequently the cGAN does not automatically\nlearn the conditionality between inputs. The second contribution is a new\nmethod, called acontrario, that explicitly models conditionality for both parts\nof the adversarial architecture via a novel acontrario loss that involves\ntraining the discriminator to learn unconditional (adverse) examples. This\nleads to a novel type of data augmentation approach for GANs (acontrario\nlearning) which allows to restrict the search space of the generator to\nconditional outputs using adverse examples. Extensive experimentation is\ncarried out to evaluate the conditionality of the discriminator by proposing a\nprobability distribution analysis. Comparisons with the cGAN architecture for\ndifferent applications show significant improvements in performance on well\nknown datasets including, semantic image synthesis, image segmentation and\nmonocular depth prediction using different metrics including Fr\\'echet\nInception Distance(FID), mean Intersection over Union (mIoU), Root Mean Square\nError log (RMSE log) and Number of statistically-Different Bins (NDB)",
          "link": "http://arxiv.org/abs/2106.15011",
          "publishedOn": "2021-06-30T02:01:00.563Z",
          "wordCount": 628,
          "title": "Are conditional GANs explicitly conditional?. (arXiv:2106.15011v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15058",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Z/0/1/0/all/0/1\">Zihao Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1\">Chilin Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yinpeng Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1\">Wei Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaolu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jun Zhu</a>",
          "description": "Face recognition is greatly improved by deep convolutional neural networks\n(CNNs). Recently, these face recognition models have been used for identity\nauthentication in security sensitive applications. However, deep CNNs are\nvulnerable to adversarial patches, which are physically realizable and\nstealthy, raising new security concerns on the real-world applications of these\nmodels. In this paper, we evaluate the robustness of face recognition models\nusing adversarial patches based on transferability, where the attacker has\nlimited accessibility to the target models. First, we extend the existing\ntransfer-based attack techniques to generate transferable adversarial patches.\nHowever, we observe that the transferability is sensitive to initialization and\ndegrades when the perturbation magnitude is large, indicating the overfitting\nto the substitute models. Second, we propose to regularize the adversarial\npatches on the low dimensional data manifold. The manifold is represented by\ngenerative models pre-trained on legitimate human face images. Using face-like\nfeatures as adversarial perturbations through optimization on the manifold, we\nshow that the gaps between the responses of substitute models and the target\nmodels dramatically decrease, exhibiting a better transferability. Extensive\ndigital world experiments are conducted to demonstrate the superiority of the\nproposed method in the black-box setting. We apply the proposed method in the\nphysical world as well.",
          "link": "http://arxiv.org/abs/2106.15058",
          "publishedOn": "2021-06-30T02:01:00.558Z",
          "wordCount": 673,
          "title": "Improving Transferability of Adversarial Patches on Face Recognition with Generative Models. (arXiv:2106.15058v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15020",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bjork_S/0/1/0/all/0/1\">Sara Bj&#xf6;rk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anfinsen_S/0/1/0/all/0/1\">Stian Normann Anfinsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naesset_E/0/1/0/all/0/1\">Erik N&#xe6;sset</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gobakken_T/0/1/0/all/0/1\">Terje Gobakken</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zahabu_E/0/1/0/all/0/1\">Eliakimu Zahabu</a>",
          "description": "This paper studies construction of above-ground biomass (AGB) prediction maps\nfrom synthetic aperture radar (SAR) intensity images. The purpose is to improve\ntraditional regression models based on SAR intensity, trained with a limited\namount of AGB in situ measurements. Although it is costly to collect, data from\nairborne laser scanning (ALS) sensors are highly correlated with AGB.\nTherefore, we propose using AGB predictions based on ALS data as surrogate\nresponse variables for SAR data in a sequential modelling fashion. This\nincreases the amount of training data dramatically. To model the regression\nfunction between SAR intensity and ALS-predicted AGB we propose to utilise a\nconditional generative adversarial network (cGAN), i.e. the Pix2Pix\nconvolutional neural network. This enables the recreation of existing ALS-based\nAGB prediction maps. The generated synthesised ALS-based AGB predictions are\nevaluated qualitatively and quantitatively against ALS-based AGB predictions\nretrieved from a traditional non-sequential regression model trained in the\nsame area. Results show that the proposed architecture manages to capture\ncharacteristics of the actual data. This suggests that the use of ALS-guided\ngenerative models is a promising avenue for AGB prediction from SAR intensity.\nFurther research on this area has the potential of providing both large-scale\nand low-cost predictions of AGB.",
          "link": "http://arxiv.org/abs/2106.15020",
          "publishedOn": "2021-06-30T02:01:00.551Z",
          "wordCount": 658,
          "title": "Constructing Forest Biomass Prediction Maps from Radar Backscatter by Sequential Regression with a Conditional Generative Adversarial Network. (arXiv:2106.15020v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15021",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Venieris_S/0/1/0/all/0/1\">Stylianos I. Venieris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panopoulos_I/0/1/0/all/0/1\">Ioannis Panopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leontiadis_I/0/1/0/all/0/1\">Ilias Leontiadis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venieris_I/0/1/0/all/0/1\">Iakovos S. Venieris</a>",
          "description": "The unprecedented performance of deep neural networks (DNNs) has led to large\nstrides in various Artificial Intelligence (AI) inference tasks, such as object\nand speech recognition. Nevertheless, deploying such AI models across commodity\ndevices faces significant challenges: large computational cost, multiple\nperformance objectives, hardware heterogeneity and a common need for high\naccuracy, together pose critical problems to the deployment of DNNs across the\nvarious embedded and mobile devices in the wild. As such, we have yet to\nwitness the mainstream usage of state-of-the-art deep learning algorithms\nacross consumer devices. In this paper, we provide preliminary answers to this\npotentially game-changing question by presenting an array of design techniques\nfor efficient AI systems. We start by examining the major roadblocks when\ntargeting both programmable processors and custom accelerators. Then, we\npresent diverse methods for achieving real-time performance following a\ncross-stack approach. These span model-, system- and hardware-level techniques,\nand their combination. Our findings provide illustrative examples of AI systems\nthat do not overburden mobile hardware, while also indicating how they can\nimprove inference accuracy. Moreover, we showcase how custom ASIC- and\nFPGA-based accelerators can be an enabling factor for next-generation AI\napplications, such as multi-DNN systems. Collectively, these results highlight\nthe critical need for further exploration as to how the various cross-stack\nsolutions can be best combined in order to bring the latest advances in deep\nlearning close to users, in a robust and efficient manner.",
          "link": "http://arxiv.org/abs/2106.15021",
          "publishedOn": "2021-06-30T02:01:00.544Z",
          "wordCount": 707,
          "title": "How to Reach Real-Time AI on Consumer Devices? Solutions for Programmable and Custom Architectures. (arXiv:2106.15021v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14989",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yuli Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yucheng Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_S/0/1/0/all/0/1\">Suting Miao</a>",
          "description": "We present an object detection based approach to localize handwritten regions\nfrom documents, which initially aims to enhance the anonymization during the\ndata transmission. The concatenated fusion of original and preprocessed images\ncontaining both printed texts and handwritten notes or signatures are fed into\nthe convolutional neural network, where the bounding boxes are learned to\ndetect the handwriting. Afterwards, the handwritten regions can be processed\n(e.g. replaced with redacted signatures) to conceal the personally identifiable\ninformation (PII). This processing pipeline based on the deep learning network\nCascade R-CNN works at 10 fps on a GPU during the inference, which ensures the\nenhanced anonymization with minimal computational overheads. Furthermore, the\nimpressive generalizability has been empirically showcased: the trained model\nbased on the English-dominant dataset works well on the fictitious unseen\ninvoices, even in Chinese. The proposed approach is also expected to facilitate\nother tasks such as handwriting recognition and signature verification.",
          "link": "http://arxiv.org/abs/2106.14989",
          "publishedOn": "2021-06-30T02:01:00.538Z",
          "wordCount": 588,
          "title": "Object Detection Based Handwriting Localization. (arXiv:2106.14989v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15045",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sanket_N/0/1/0/all/0/1\">Nitin J. Sanket</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_C/0/1/0/all/0/1\">Chahat Deep Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parameshwara_C/0/1/0/all/0/1\">Chethan M. Parameshwara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fermuller_C/0/1/0/all/0/1\">Cornelia Ferm&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Croon_G/0/1/0/all/0/1\">Guido C.H.E. de Croon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aloimonos_Y/0/1/0/all/0/1\">Yiannis Aloimonos</a>",
          "description": "The rapid rise of accessibility of unmanned aerial vehicles or drones pose a\nthreat to general security and confidentiality. Most of the commercially\navailable or custom-built drones are multi-rotors and are comprised of multiple\npropellers. Since these propellers rotate at a high-speed, they are generally\nthe fastest moving parts of an image and cannot be directly \"seen\" by a\nclassical camera without severe motion blur. We utilize a class of sensors that\nare particularly suitable for such scenarios called event cameras, which have a\nhigh temporal resolution, low-latency, and high dynamic range.\n\nIn this paper, we model the geometry of a propeller and use it to generate\nsimulated events which are used to train a deep neural network called EVPropNet\nto detect propellers from the data of an event camera. EVPropNet directly\ntransfers to the real world without any fine-tuning or retraining. We present\ntwo applications of our network: (a) tracking and following an unmarked drone\nand (b) landing on a near-hover drone. We successfully evaluate and demonstrate\nthe proposed approach in many real-world experiments with different propeller\nshapes and sizes. Our network can detect propellers at a rate of 85.1% even\nwhen 60% of the propeller is occluded and can run at upto 35Hz on a 2W power\nbudget. To our knowledge, this is the first deep learning-based solution for\ndetecting propellers (to detect drones). Finally, our applications also show an\nimpressive success rate of 92% and 90% for the tracking and landing tasks\nrespectively.",
          "link": "http://arxiv.org/abs/2106.15045",
          "publishedOn": "2021-06-30T02:01:00.532Z",
          "wordCount": 719,
          "title": "EVPropNet: Detecting Drones By Finding Propellers For Mid-Air Landing And Following. (arXiv:2106.15045v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15069",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Leyuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kunbo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunlong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zhenan Sun</a>",
          "description": "For distant iris recognition, a long focal length lens is generally used to\nensure the resolution ofiris images, which reduces the depth of field and leads\nto potential defocus blur. To accommodate users at different distances, it is\nnecessary to control focus quickly and accurately. While for users in motion,\nit is expected to maintain the correct focus on the iris area continuously. In\nthis paper, we introduced a novel rapid autofocus camera for active refocusing\nofthe iris area ofthe moving objects using a focus-tunable lens. Our end-to-end\ncomputational algorithm can predict the best focus position from one single\nblurred image and generate a lens diopter control signal automatically. This\nscene-based active manipulation method enables real-time focus tracking of the\niris area ofa moving object. We built a testing bench to collect real-world\nfocal stacks for evaluation of the autofocus methods. Our camera has reached an\nautofocus speed ofover 50 fps. The results demonstrate the advantages of our\nproposed camera for biometric perception in static and dynamic scenes. The code\nis available at https://github.com/Debatrix/AquulaCam.",
          "link": "http://arxiv.org/abs/2106.15069",
          "publishedOn": "2021-06-30T02:01:00.509Z",
          "wordCount": 624,
          "title": "An End-to-End Autofocus Camera for Iris on the Move. (arXiv:2106.15069v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15007",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Z/0/1/0/all/0/1\">Zongyao Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gutierrez_N/0/1/0/all/0/1\">Nolan B. Gutierrez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beksi_W/0/1/0/all/0/1\">William J. Beksi</a>",
          "description": "In this paper, we introduce a new technique that combines two popular methods\nto estimate uncertainty in object detection. Quantifying uncertainty is\ncritical in real-world robotic applications. Traditional detection models can\nbe ambiguous even when they provide a high-probability output. Robot actions\nbased on high-confidence, yet unreliable predictions, may result in serious\nrepercussions. Our framework employs deep ensembles and Monte Carlo dropout for\napproximating predictive uncertainty, and it improves upon the uncertainty\nestimation quality of the baseline method. The proposed approach is evaluated\non publicly available synthetic image datasets captured from sequences of\nvideo.",
          "link": "http://arxiv.org/abs/2106.15007",
          "publishedOn": "2021-06-30T02:01:00.498Z",
          "wordCount": 544,
          "title": "An Uncertainty Estimation Framework for Probabilistic Object Detection. (arXiv:2106.15007v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14922",
          "author": "<a href=\"http://arxiv.org/find/astro-ph/1/au:+Xu_C/0/1/0/all/0/1\">Chengyuan Xu</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+McCully_C/0/1/0/all/0/1\">Curtis McCully</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Dong_B/0/1/0/all/0/1\">Boning Dong</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Howell_D/0/1/0/all/0/1\">D. Andrew Howell</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Sen_P/0/1/0/all/0/1\">Pradeep Sen</a>",
          "description": "Rejecting cosmic rays (CRs) is essential for scientific interpretation of\nCCD-captured data, but detecting CRs in single-exposure images has remained\nchallenging. Conventional CR-detection algorithms require tuning multiple\nparameters experimentally making it hard to automate across different\ninstruments or observation requests. Recent work using deep learning to train\nCR-detection models has demonstrated promising results. However,\ninstrument-specific models suffer from performance loss on images from\nground-based facilities not included in the training data. In this work, we\npresent Cosmic-CoNN, a deep-learning framework designed to produce generic\nCR-detection models. We build a large, diverse ground-based CR dataset\nleveraging thousands of images from the Las Cumbres Observatory global\ntelescope network to produce a generic CR-detection model which achieves a\n99.91% true-positive detection rate and maintains over 96.40% true-positive\nrates on unseen data from Gemini GMOS-N/S, with a false-positive rate of 0.01%.\nApart from the open-source framework and dataset, we also build a suite of\ntools including console commands, a web-based application, and Python APIs to\nmake automatic, robust CR detection widely accessible by the community of\nastronomers.",
          "link": "http://arxiv.org/abs/2106.14922",
          "publishedOn": "2021-06-30T02:01:00.485Z",
          "wordCount": 649,
          "title": "Cosmic-CoNN: A Cosmic Ray Detection Deep-Learning Framework, Dataset, and Toolkit. (arXiv:2106.14922v1 [astro-ph.IM])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14917",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tian_J/0/1/0/all/0/1\">Junjiao Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mithun_N/0/1/0/all/0/1\">Niluthpol Mithun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seymour_Z/0/1/0/all/0/1\">Zach Seymour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiu_H/0/1/0/all/0/1\">Han-Pang Chiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kira_Z/0/1/0/all/0/1\">Zsolt Kira</a>",
          "description": "Class imbalance is a fundamental problem in computer vision applications such\nas semantic segmentation. Specifically, uneven class distributions in a\ntraining dataset often result in unsatisfactory performance on\nunder-represented classes. Many works have proposed to weight the standard\ncross entropy loss function with pre-computed weights based on class\nstatistics, such as the number of samples and class margins. There are two\nmajor drawbacks to these methods: 1) constantly up-weighting minority classes\ncan introduce excessive false positives in semantic segmentation; 2) a minority\nclass is not necessarily a hard class. The consequence is low precision due to\nexcessive false positives. In this regard, we propose a hard-class mining loss\nby reshaping the vanilla cross entropy loss such that it weights the loss for\neach class dynamically based on instantaneous recall performance. We show that\nthe novel recall loss changes gradually between the standard cross entropy loss\nand the inverse frequency weighted loss. Recall loss also leads to improved\nmean accuracy while offering competitive mean Intersection over Union (IoU)\nperformance. On Synthia dataset, recall loss achieves 9% relative improvement\non mean accuracy with competitive mean IoU using DeepLab-ResNet18 compared to\nthe cross entropy loss. Code available at\nhttps://github.com/PotatoTian/recall-semseg.",
          "link": "http://arxiv.org/abs/2106.14917",
          "publishedOn": "2021-06-30T02:01:00.479Z",
          "wordCount": 637,
          "title": "Striking the Right Balance: Recall Loss for Semantic Segmentation. (arXiv:2106.14917v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14948",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zitong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1\">Yunxiao Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaobai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1\">Chenxu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_Z/0/1/0/all/0/1\">Zhen Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1\">Guoying Zhao</a>",
          "description": "Face anti-spoofing (FAS) has lately attracted increasing attention due to its\nvital role in securing face recognition systems from presentation attacks\n(PAs). As more and more realistic PAs with novel types spring up, traditional\nFAS methods based on handcrafted features become unreliable due to their\nlimited representation capacity. With the emergence of large-scale academic\ndatasets in the recent decade, deep learning based FAS achieves remarkable\nperformance and dominates this area. However, existing reviews in this field\nmainly focus on the handcrafted features, which are outdated and uninspiring\nfor the progress of FAS community. In this paper, to stimulate future research,\nwe present the first comprehensive review of recent advances in deep learning\nbased FAS. It covers several novel and insightful components: 1) besides\nsupervision with binary label (e.g., '0' for bonafide vs. '1' for PAs), we also\ninvestigate recent methods with pixel-wise supervision (e.g., pseudo depth\nmap); 2) in addition to traditional intra-dataset evaluation, we collect and\nanalyze the latest methods specially designed for domain generalization and\nopen-set FAS; and 3) besides commercial RGB camera, we summarize the deep\nlearning applications under multi-modal (e.g., depth and infrared) or\nspecialized (e.g., light field and flash) sensors. We conclude this survey by\nemphasizing current open issues and highlighting potential prospects.",
          "link": "http://arxiv.org/abs/2106.14948",
          "publishedOn": "2021-06-30T02:01:00.474Z",
          "wordCount": 658,
          "title": "Deep Learning for Face Anti-Spoofing: A Survey. (arXiv:2106.14948v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15009",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jaiswal_A/0/1/0/all/0/1\">Ashish Jaiswal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Babu_A/0/1/0/all/0/1\">Ashwin Ramesh Babu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zadeh_M/0/1/0/all/0/1\">Mohammad Zaki Zadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Makedon_F/0/1/0/all/0/1\">Fillia Makedon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wylie_G/0/1/0/all/0/1\">Glenn Wylie</a>",
          "description": "Functional magnetic resonance imaging (fMRI) is a neuroimaging technique that\nrecords neural activations in the brain by capturing the blood oxygen level in\ndifferent regions based on the task performed by a subject. Given fMRI data,\nthe problem of predicting the state of cognitive fatigue in a person has not\nbeen investigated to its full extent. This paper proposes tackling this issue\nas a multi-class classification problem by dividing the state of cognitive\nfatigue into six different levels, ranging from no-fatigue to extreme fatigue\nconditions. We built a spatio-temporal model that uses convolutional neural\nnetworks (CNN) for spatial feature extraction and a long short-term memory\n(LSTM) network for temporal modeling of 4D fMRI scans. We also applied a\nself-supervised method called MoCo to pre-train our model on a public dataset\nBOLD5000 and fine-tuned it on our labeled dataset to classify cognitive\nfatigue. Our novel dataset contains fMRI scans from Traumatic Brain Injury\n(TBI) patients and healthy controls (HCs) while performing a series of\ncognitive tasks. This method establishes a state-of-the-art technique to\nanalyze cognitive fatigue from fMRI data and beats previous approaches to solve\nthis problem.",
          "link": "http://arxiv.org/abs/2106.15009",
          "publishedOn": "2021-06-30T02:01:00.469Z",
          "wordCount": 634,
          "title": "Understanding Cognitive Fatigue from fMRI Scans with Self-supervised Learning. (arXiv:2106.15009v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14943",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1\">Pu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_W/0/1/0/all/0/1\">Wei Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_G/0/1/0/all/0/1\">Geng Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1\">Yuxuan Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_B/0/1/0/all/0/1\">Bin Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanzhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1\">Xue Lin</a>",
          "description": "Object detection plays an important role in self-driving cars for security\ndevelopment. However, mobile systems on self-driving cars with limited\ncomputation resources lead to difficulties for object detection. To facilitate\nthis, we propose a compiler-aware neural pruning search framework to achieve\nhigh-speed inference on autonomous vehicles for 2D and 3D object detection. The\nframework automatically searches the pruning scheme and rate for each layer to\nfind a best-suited pruning for optimizing detection accuracy and speed\nperformance under compiler optimization. Our experiments demonstrate that for\nthe first time, the proposed method achieves (close-to) real-time, 55ms and\n99ms inference times for YOLOv4 based 2D object detection and PointPillars\nbased 3D detection, respectively, on an off-the-shelf mobile phone with minor\n(or no) accuracy loss.",
          "link": "http://arxiv.org/abs/2106.14943",
          "publishedOn": "2021-06-30T02:01:00.457Z",
          "wordCount": 579,
          "title": "Achieving Real-Time Object Detection on MobileDevices with Neural Pruning Search. (arXiv:2106.14943v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14947",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Fabian_Z/0/1/0/all/0/1\">Zalan Fabian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Heckel_R/0/1/0/all/0/1\">Reinhard Heckel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Soltanolkotabi_M/0/1/0/all/0/1\">Mahdi Soltanolkotabi</a>",
          "description": "Deep neural networks have emerged as very successful tools for image\nrestoration and reconstruction tasks. These networks are often trained\nend-to-end to directly reconstruct an image from a noisy or corrupted\nmeasurement of that image. To achieve state-of-the-art performance, training on\nlarge and diverse sets of images is considered critical. However, it is often\ndifficult and/or expensive to collect large amounts of training images.\nInspired by the success of Data Augmentation (DA) for classification problems,\nin this paper, we propose a pipeline for data augmentation for accelerated MRI\nreconstruction and study its effectiveness at reducing the required training\ndata in a variety of settings. Our DA pipeline, MRAugment, is specifically\ndesigned to utilize the invariances present in medical imaging measurements as\nnaive DA strategies that neglect the physics of the problem fail. Through\nextensive studies on multiple datasets we demonstrate that in the low-data\nregime DA prevents overfitting and can match or even surpass the state of the\nart while using significantly fewer training data, whereas in the high-data\nregime it has diminishing returns. Furthermore, our findings show that DA can\nimprove the robustness of the model against various shifts in the test\ndistribution.",
          "link": "http://arxiv.org/abs/2106.14947",
          "publishedOn": "2021-06-30T02:01:00.452Z",
          "wordCount": 663,
          "title": "Data augmentation for deep learning based accelerated MRI reconstruction with limited data. (arXiv:2106.14947v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14942",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bergman_A/0/1/0/all/0/1\">Alexander W. Bergman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kellnhofer_P/0/1/0/all/0/1\">Petr Kellnhofer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wetzstein_G/0/1/0/all/0/1\">Gordon Wetzstein</a>",
          "description": "Novel view synthesis is a long-standing problem in machine learning and\ncomputer vision. Significant progress has recently been made in developing\nneural scene representations and rendering techniques that synthesize\nphotorealistic images from arbitrary views. These representations, however, are\nextremely slow to train and often also slow to render. Inspired by neural\nvariants of image-based rendering, we develop a new neural rendering approach\nwith the goal of quickly learning a high-quality representation which can also\nbe rendered in real-time. Our approach, MetaNLR++, accomplishes this by using a\nunique combination of a neural shape representation and 2D CNN-based image\nfeature extraction, aggregation, and re-projection. To push representation\nconvergence times down to minutes, we leverage meta learning to learn neural\nshape and image feature priors which accelerate training. The optimized shape\nand image features can then be extracted using traditional graphics techniques\nand rendered in real time. We show that MetaNLR++ achieves similar or better\nnovel view synthesis results in a fraction of the time that competing methods\nrequire.",
          "link": "http://arxiv.org/abs/2106.14942",
          "publishedOn": "2021-06-30T02:01:00.413Z",
          "wordCount": 616,
          "title": "Fast Training of Neural Lumigraph Representations using Meta Learning. (arXiv:2106.14942v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2008.10271",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Comandur_B/0/1/0/all/0/1\">Bharath Comandur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kak_A/0/1/0/all/0/1\">Avinash C. Kak</a>",
          "description": "We present a novel multi-view training framework and CNN architecture for\ncombining information from multiple overlapping satellite images and noisy\ntraining labels derived from OpenStreetMap (OSM) to semantically label\nbuildings and roads across large geographic regions (100 km$^2$). Our approach\nto multi-view semantic segmentation yields a 4-7% improvement in the per-class\nIoU scores compared to the traditional approaches that use the views\nindependently of one another. A unique (and, perhaps, surprising) property of\nour system is that modifications that are added to the tail-end of the CNN for\nlearning from the multi-view data can be discarded at the time of inference\nwith a relatively small penalty in the overall performance. This implies that\nthe benefits of training using multiple views are absorbed by all the layers of\nthe network. Additionally, our approach only adds a small overhead in terms of\nthe GPU-memory consumption even when training with as many as 32 views per\nscene. The system we present is end-to-end automated, which facilitates\ncomparing the classifiers trained directly on true orthophotos vis-a-vis first\ntraining them on the off-nadir images and subsequently translating the\npredicted labels to geographical coordinates. With no human supervision, our\nIoU scores for the buildings and roads classes are 0.8 and 0.64 respectively\nwhich are better than state-of-the-art approaches that use OSM labels and that\nare not completely automated.",
          "link": "http://arxiv.org/abs/2008.10271",
          "publishedOn": "2021-06-29T01:55:17.839Z",
          "wordCount": 774,
          "title": "Semantic Labeling of Large-Area Geographic Regions Using Multi-View and Multi-Date Satellite Images and Noisy OSM Training Labels. (arXiv:2008.10271v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.14758",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Xu_L/0/1/0/all/0/1\">Lu Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuwei Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1\">Ying Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_D/0/1/0/all/0/1\">Daoye Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_M/0/1/0/all/0/1\">Mu Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ren_J/0/1/0/all/0/1\">Jimmy Ren</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wei_J/0/1/0/all/0/1\">Jingwei Wei</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ye_Z/0/1/0/all/0/1\">Zhaoxiang Ye</a>",
          "description": "Low-dose CT has been a key diagnostic imaging modality to reduce the\npotential risk of radiation overdose to patient health. Despite recent\nadvances, CNN-based approaches typically apply filters in a spatially invariant\nway and adopt similar pixel-level losses, which treat all regions of the CT\nimage equally and can be inefficient when fine-grained structures coexist with\nnon-uniformly distributed noises. To address this issue, we propose a\nStructure-preserving Kernel Prediction Network (StructKPN) that combines the\nkernel prediction network with a structure-aware loss function that utilizes\nthe pixel gradient statistics and guides the model towards spatially-variant\nfilters that enhance noise removal, prevent over-smoothing and preserve\ndetailed structures for different regions in CT imaging. Extensive experiments\ndemonstrated that our approach achieved superior performance on both synthetic\nand non-synthetic datasets, and better preserves structures that are highly\ndesired in clinical screening and low-dose protocol optimization.",
          "link": "http://arxiv.org/abs/2105.14758",
          "publishedOn": "2021-06-29T01:55:17.641Z",
          "wordCount": 614,
          "title": "Low-Dose CT Denoising Using a Structure-Preserving Kernel Prediction Network. (arXiv:2105.14758v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.10510",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Nasim_M/0/1/0/all/0/1\">M Quamer Nasim</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Maiti_T/0/1/0/all/0/1\">Tannistha Maiti</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Srivastava_A/0/1/0/all/0/1\">Ayush Srivastava</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Singh_T/0/1/0/all/0/1\">Tarry Singh</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Mei_J/0/1/0/all/0/1\">Jie Mei</a>",
          "description": "Deep neural networks (DNNs) can learn accurately from large quantities of\nlabeled input data, but DNNs sometimes fail to generalize to test data sampled\nfrom different input distributions. Unsupervised Deep Domain Adaptation (DDA)\nproves useful when no input labels are available, and distribution shifts are\nobserved in the target domain (TD). Experiments are performed on seismic images\nof the F3 block 3D dataset from offshore Netherlands (source domain; SD) and\nPenobscot 3D survey data from Canada (target domain; TD). Three geological\nclasses from SD and TD that have similar reflection patterns are considered. In\nthe present study, an improved deep neural network architecture named\nEarthAdaptNet (EAN) is proposed to semantically segment the seismic images. We\nspecifically use a transposed residual unit to replace the traditional dilated\nconvolution in the decoder block. The EAN achieved a pixel-level accuracy >84%\nand an accuracy of ~70% for the minority classes, showing improved performance\ncompared to existing architectures. In addition, we introduced the CORAL\n(Correlation Alignment) method to the EAN to create an unsupervised deep domain\nadaptation network (EAN-DDA) for the classification of seismic reflections\nfromF3 and Penobscot. Maximum class accuracy achieved was ~99% for class 2 of\nPenobscot with >50% overall accuracy. Taken together, EAN-DDA has the potential\nto classify target domain seismic facies classes with high accuracy.",
          "link": "http://arxiv.org/abs/2011.10510",
          "publishedOn": "2021-06-29T01:55:17.568Z",
          "wordCount": 701,
          "title": "Seismic Facies Analysis: A Deep Domain Adaptation Approach. (arXiv:2011.10510v2 [physics.geo-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.00250",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_K/0/1/0/all/0/1\">Kshitij Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gautam_D/0/1/0/all/0/1\">Devansh Gautam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mamidi_R/0/1/0/all/0/1\">Radhika Mamidi</a>",
          "description": "Multimodal Machine Translation (MMT) enriches the source text with visual\ninformation for translation. It has gained popularity in recent years, and\nseveral pipelines have been proposed in the same direction. Yet, the task lacks\nquality datasets to illustrate the contribution of visual modality in the\ntranslation systems. In this paper, we propose our system under the team name\nVolta for the Multimodal Translation Task of WAT 2021 from English to Hindi. We\nalso participate in the textual-only subtask of the same language pair for\nwhich we use mBART, a pretrained multilingual sequence-to-sequence model. For\nmultimodal translation, we propose to enhance the textual input by bringing the\nvisual information to a textual domain by extracting object tags from the\nimage. We also explore the robustness of our system by systematically degrading\nthe source text. Finally, we achieve a BLEU score of 44.6 and 51.6 on the test\nset and challenge set of the multimodal task.",
          "link": "http://arxiv.org/abs/2106.00250",
          "publishedOn": "2021-06-29T01:55:17.561Z",
          "wordCount": 625,
          "title": "ViTA: Visual-Linguistic Translation by Aligning Object Tags. (arXiv:2106.00250v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.07566",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wang_F/0/1/0/all/0/1\">Fanyi Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hu_H/0/1/0/all/0/1\">Haotian Hu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shen_C/0/1/0/all/0/1\">Cheng Shen</a>",
          "description": "Attention mechanism has shown enormous potential for single image\nsuper-resolution (SISR). However, existing works only proposed some attention\nmechanism for a specific network. A universal attention mechanism for SISR,\nwhich could further improve the performance of networks without attention and\nprovide a baseline for networks with attention, is still lacking. To fit this\ngap, we propose a lightweight and efficient Balanced Attention Mechanism (BAM),\nwhich consists of Avgpool Channel Attention Module (ACAM) and Maxpool Spatial\nAttention Module (MSAM) in parallel. The information extraction mechanism of\nACAM and MSAM effectively filters redundant information, making the overall\nstructure of BAM very lightweight. Owing to the parallel structure, during the\ngradient backpropagation process of BAM, ACAM and MSAM not only conduct\nself-optimization, but also mutual optimization so as to generate more balanced\nattention information. To verify the effectiveness and robustness of BAM, we\napplied it to 12 state-ofthe-art SISR networks. The results on 4 benchmark\ndatasets demonstrate that BAM can efficiently improve the networks'\nperformance, and for those with attention, the substitution with BAM further\nreduces the amount of parameters and increase the inference speed. Moreover,\nablation experiments were conducted to prove the minimalism of BAM.",
          "link": "http://arxiv.org/abs/2104.07566",
          "publishedOn": "2021-06-29T01:55:17.539Z",
          "wordCount": 670,
          "title": "BAM: A Lightweight and Efficient Balanced Attention Mechanism for Single Image Super Resolution. (arXiv:2104.07566v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.10762",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Murphy_R/0/1/0/all/0/1\">Robert A. Murphy</a>",
          "description": "Random field and random cluster theory are used to prove certain mathematical\nresults concerning the probability distribution of image pixel intensities\ncharacterized as generic $2D$ integer arrays. The size of the smallest bounded\nregion within an image is estimated for segmenting an image, from which, the\nequilibrium distribution of intensities can be recovered. From the estimated\nbounded regions, properties of the sub-optimal and equilibrium distributions of\nintensities are derived, which leads to an image compression methodology\nwhereby only slightly more than half of all pixels are required for a\nworst-case reconstruction of the original image. An example in unsupervised\nobject detection illustrates the mathematical results.",
          "link": "http://arxiv.org/abs/2104.10762",
          "publishedOn": "2021-06-29T01:55:17.533Z",
          "wordCount": 635,
          "title": "Image Segmentation, Compression and Reconstruction from Edge Distribution Estimation with Random Field and Random Cluster Theories. (arXiv:2104.10762v8 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09003",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zha_J/0/1/0/all/0/1\">Jiajun Zha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1\">Yiran Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hartley_R/0/1/0/all/0/1\">Richard Hartley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1\">Liang Zheng</a>",
          "description": "Attention has been proved to be an efficient mechanism to capture long-range\ndependencies. However, so far it has not been deployed in invertible networks.\nThis is due to the fact that in order to make a network invertible, every\ncomponent within the network needs to be a bijective transformation, but a\nnormal attention block is not. In this paper, we propose invertible attention\nthat can be plugged into existing invertible models. We mathematically and\nexperimentally prove that the invertibility of an attention model can be\nachieved by carefully constraining its Lipschitz constant. We validate the\ninvertibility of our invertible attention on image reconstruction task with 3\npopular datasets: CIFAR-10, SVHN, and CelebA. We also show that our invertible\nattention achieves similar performance in comparison with normal non-invertible\nattention on dense prediction tasks. The code is available at\nhttps://github.com/Schwartz-Zha/InvertibleAttention",
          "link": "http://arxiv.org/abs/2106.09003",
          "publishedOn": "2021-06-29T01:55:17.527Z",
          "wordCount": 594,
          "title": "Invertible Attention. (arXiv:2106.09003v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.08913",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Do_T/0/1/0/all/0/1\">Tuong Do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_B/0/1/0/all/0/1\">Binh X. Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tjiputra_E/0/1/0/all/0/1\">Erman Tjiputra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_M/0/1/0/all/0/1\">Minh Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_Q/0/1/0/all/0/1\">Quang D. Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1\">Anh Nguyen</a>",
          "description": "Transfer learning is an important step to extract meaningful features and\novercome the data limitation in the medical Visual Question Answering (VQA)\ntask. However, most of the existing medical VQA methods rely on external data\nfor transfer learning, while the meta-data within the dataset is not fully\nutilized. In this paper, we present a new multiple meta-model quantifying\nmethod that effectively learns meta-annotation and leverages meaningful\nfeatures to the medical VQA task. Our proposed method is designed to increase\nmeta-data by auto-annotation, deal with noisy labels, and output meta-models\nwhich provide robust features for medical VQA tasks. Extensively experimental\nresults on two public medical VQA datasets show that our approach achieves\nsuperior accuracy in comparison with other state-of-the-art methods, while does\nnot require external data to train meta-models.",
          "link": "http://arxiv.org/abs/2105.08913",
          "publishedOn": "2021-06-29T01:55:17.521Z",
          "wordCount": 604,
          "title": "Multiple Meta-model Quantifying for Medical Visual Question Answering. (arXiv:2105.08913v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.08697",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lu_D/0/1/0/all/0/1\">Dongchen Lu</a>",
          "description": "Rotated object detection is a challenging issue of computer vision field.\nLoss of spatial information and confusion of parametric order have been the\nbottleneck for rotated detection accuracy. In this paper, we propose an\norientation-sensitive keypoint based rotated detector OSKDet. We adopt a set of\nkeypoints to characterize the target and predict the keypoint heatmap on ROI to\nform a rotated target. By proposing the orientation-sensitive heatmap, OSKDet\ncould learn the shape and direction of rotated target implicitly and has\nstronger modeling capabilities for target representation, which improves the\nlocalization accuracy and acquires high quality detection results. To extract\nhighly effective features at border areas, we design a rotation-aware\ndeformable convolution module. Furthermore, we explore a new keypoint reorder\nalgorithm and feature fusion module based on the angle distribution to\neliminate the confusion of keypoint order. Experimental results on several\npublic benchmarks show the state-of-the-art performance of OSKDet.\nSpecifically, we achieve an AP of 77.81% on DOTA, 89.91% on HRSC2016, and\n97.18% on UCAS-AOD, respectively.",
          "link": "http://arxiv.org/abs/2104.08697",
          "publishedOn": "2021-06-29T01:55:17.488Z",
          "wordCount": 621,
          "title": "OSKDet: Towards Orientation-sensitive Keypoint Localization for Rotated Object Detection. (arXiv:2104.08697v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.06963",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fenglin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_S/0/1/0/all/0/1\">Shen Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_W/0/1/0/all/0/1\">Wei Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yuexian Zou</a>",
          "description": "Automatically generating radiology reports can improve current clinical\npractice in diagnostic radiology. On one hand, it can relieve radiologists from\nthe heavy burden of report writing; On the other hand, it can remind\nradiologists of abnormalities and avoid the misdiagnosis and missed diagnosis.\nYet, this task remains a challenging job for data-driven neural networks, due\nto the serious visual and textual data biases. To this end, we propose a\nPosterior-and-Prior Knowledge Exploring-and-Distilling approach (PPKED) to\nimitate the working patterns of radiologists, who will first examine the\nabnormal regions and assign the disease topic tags to the abnormal regions, and\nthen rely on the years of prior medical knowledge and prior working experience\naccumulations to write reports. Thus, the PPKED includes three modules:\nPosterior Knowledge Explorer (PoKE), Prior Knowledge Explorer (PrKE) and\nMulti-domain Knowledge Distiller (MKD). In detail, PoKE explores the posterior\nknowledge, which provides explicit abnormal visual regions to alleviate visual\ndata bias; PrKE explores the prior knowledge from the prior medical knowledge\ngraph (medical knowledge) and prior radiology reports (working experience) to\nalleviate textual data bias. The explored knowledge is distilled by the MKD to\ngenerate the final reports. Evaluated on MIMIC-CXR and IU-Xray datasets, our\nmethod is able to outperform previous state-of-the-art models on these two\ndatasets.",
          "link": "http://arxiv.org/abs/2106.06963",
          "publishedOn": "2021-06-29T01:55:17.464Z",
          "wordCount": 689,
          "title": "Exploring and Distilling Posterior and Prior Knowledge for Radiology Report Generation. (arXiv:2106.06963v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.10441",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bagautdinov_T/0/1/0/all/0/1\">Timur Bagautdinov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chenglei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simon_T/0/1/0/all/0/1\">Tomas Simon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prada_F/0/1/0/all/0/1\">Fabian Prada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shiratori_T/0/1/0/all/0/1\">Takaaki Shiratori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_S/0/1/0/all/0/1\">Shih-En Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Weipeng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheikh_Y/0/1/0/all/0/1\">Yaser Sheikh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saragih_J/0/1/0/all/0/1\">Jason Saragih</a>",
          "description": "We present a learning-based method for building driving-signal aware\nfull-body avatars. Our model is a conditional variational autoencoder that can\nbe animated with incomplete driving signals, such as human pose and facial\nkeypoints, and produces a high-quality representation of human geometry and\nview-dependent appearance. The core intuition behind our method is that better\ndrivability and generalization can be achieved by disentangling the driving\nsignals and remaining generative factors, which are not available during\nanimation. To this end, we explicitly account for information deficiency in the\ndriving signal by introducing a latent space that exclusively captures the\nremaining information, thus enabling the imputation of the missing factors\nrequired during full-body animation, while remaining faithful to the driving\nsignal. We also propose a learnable localized compression for the driving\nsignal which promotes better generalization, and helps minimize the influence\nof global chance-correlations often found in real datasets. For a given driving\nsignal, the resulting variational model produces a compact space of uncertainty\nfor missing factors that allows for an imputation strategy best suited to a\nparticular application. We demonstrate the efficacy of our approach on the\nchallenging problem of full-body animation for virtual telepresence with\ndriving signals acquired from minimal sensors placed in the environment and\nmounted on a VR-headset.",
          "link": "http://arxiv.org/abs/2105.10441",
          "publishedOn": "2021-06-29T01:55:17.458Z",
          "wordCount": 686,
          "title": "Driving-Signal Aware Full-Body Avatars. (arXiv:2105.10441v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.10127",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1\">Yuxin Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_Z/0/1/0/all/0/1\">Zhexiong Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1\">Yuchao Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1\">Aixuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_Y/0/1/0/all/0/1\">Yunqiu Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_X/0/1/0/all/0/1\">Xinyu Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1\">Deng-Ping Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barnes_N/0/1/0/all/0/1\">Nick Barnes</a>",
          "description": "The transformer networks are particularly good at modeling long-range\ndependencies within a long sequence. In this paper, we conduct research on\napplying the transformer networks for salient object detection (SOD). We adopt\nthe dense transformer backbone for fully supervised RGB image based SOD, RGB-D\nimage pair based SOD, and weakly supervised SOD within a unified framework\nbased on the observation that the transformer backbone can provide accurate\nstructure modeling, which makes it powerful in learning from weak labels with\nless structure information. Further, we find that the vision transformer\narchitectures do not offer direct spatial supervision, instead encoding\nposition as a feature. Therefore, we investigate the contributions of two\nstrategies to provide stronger spatial supervision through the transformer\nlayers within our unified framework, namely deep supervision and\ndifficulty-aware learning. We find that deep supervision can get gradients back\ninto the higher level features, thus leads to uniform activation within the\nsame semantic object. Difficulty-aware learning on the other hand is capable of\nidentifying the hard pixels for effective hard negative mining. We also\nvisualize features of conventional backbone and transformer backbone before and\nafter fine-tuning them for SOD, and find that transformer backbone encodes more\naccurate object structure information and more distinct semantic information\nwithin the lower and higher level features respectively. We also apply our\nmodel to camouflaged object detection (COD) and achieve similar observations as\nthe above three SOD tasks. Extensive experimental results on various SOD and\nCOD tasks illustrate that transformer networks can transform SOD and COD,\nleading to new benchmarks for each related task. The source code and\nexperimental results are available via our project page:\nhttps://github.com/fupiao1998/TrasformerSOD.",
          "link": "http://arxiv.org/abs/2104.10127",
          "publishedOn": "2021-06-29T01:55:17.443Z",
          "wordCount": 757,
          "title": "Transformer Transforms Salient Object Detection and Camouflaged Object Detection. (arXiv:2104.10127v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.02581",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Batra_H/0/1/0/all/0/1\">Himanshu Batra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Punn_N/0/1/0/all/0/1\">Narinder Singh Punn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sonbhadra_S/0/1/0/all/0/1\">Sanjay Kumar Sonbhadra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1\">Sonali Agarwal</a>",
          "description": "Sentiment analysis can provide a suitable lead for the tools used in software\nengineering along with the API recommendation systems and relevant libraries to\nbe used. In this context, the existing tools like SentiCR, SentiStrength-SE,\netc. exhibited low f1-scores that completely defeats the purpose of deployment\nof such strategies, thereby there is enough scope for performance improvement.\nRecent advancements show that transformer based pre-trained models (e.g., BERT,\nRoBERTa, ALBERT, etc.) have displayed better results in the text classification\ntask. Following this context, the present research explores different\nBERT-based models to analyze the sentences in GitHub comments, Jira comments,\nand Stack Overflow posts. The paper presents three different strategies to\nanalyse BERT based model for sentiment analysis, where in the first strategy\nthe BERT based pre-trained models are fine-tuned; in the second strategy an\nensemble model is developed from BERT variants, and in the third strategy a\ncompressed model (Distil BERT) is used. The experimental results show that the\nBERT based ensemble approach and the compressed BERT model attain improvements\nby 6-12% over prevailing tools for the F1 measure on all three datasets.",
          "link": "http://arxiv.org/abs/2106.02581",
          "publishedOn": "2021-06-29T01:55:17.436Z",
          "wordCount": 636,
          "title": "BERT based sentiment analysis: A software engineering perspective. (arXiv:2106.02581v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10309",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Akiva_P/0/1/0/all/0/1\">Peri Akiva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dana_K/0/1/0/all/0/1\">Kristin Dana</a>",
          "description": "The costly process of obtaining semantic segmentation labels has driven\nresearch towards weakly supervised semantic segmentation (WSSS) methods, using\nonly image-level, point, or box labels. The lack of dense scene representation\nrequires methods to increase complexity to obtain additional semantic\ninformation about the scene, often done through multiple stages of training and\nrefinement. Current state-of-the-art (SOTA) models leverage image-level labels\nto produce class activation maps (CAMs) which go through multiple stages of\nrefinement before they are thresholded to make pseudo-masks for supervision.\nThe multi-stage approach is computationally expensive, and dependency on\nimage-level labels for CAMs generation lacks generalizability to more complex\nscenes. In contrary, our method offers a single-stage approach generalizable to\narbitrary dataset, that is trainable from scratch, without any dependency on\npre-trained backbones, classification, or separate refinement tasks. We utilize\npoint annotations to generate reliable, on-the-fly pseudo-masks through refined\nand filtered features. While our method requires point annotations that are\nonly slightly more expensive than image-level annotations, we are to\ndemonstrate SOTA performance on benchmark datasets (PascalVOC 2012), as well as\nsignificantly outperform other SOTA WSSS methods on recent real-world datasets\n(CRAID, CityPersons, IAD).",
          "link": "http://arxiv.org/abs/2106.10309",
          "publishedOn": "2021-06-29T01:55:17.398Z",
          "wordCount": 637,
          "title": "Towards Single Stage Weakly Supervised Semantic Segmentation. (arXiv:2106.10309v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.11883",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuxin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1\">Mingbao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chia-Wen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Feiyue Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yongjian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yonghong Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1\">Rongrong Ji</a>",
          "description": "Channel Pruning has been long studied to compress CNNs for efficient image\nclassification. Prior works implement channel pruning in an unexplainable\nmanner, which tends to reduce the final classification errors while failing to\nconsider the internal influence of each channel. In this paper, we conduct\nchannel pruning in a white box. Through deep visualization of feature maps\nactivated by different channels, we observe that different channels have a\nvarying contribution to different categories in image classification. Inspired\nby this, we choose to preserve channels contributing to most categories.\nSpecifically, to model the contribution of each channel to differentiating\ncategories, we develop a class-wise mask for each channel, implemented in a\ndynamic training manner w.r.t. the input image's category. On the basis of the\nlearned class-wise mask, we perform a global voting mechanism to remove\nchannels with less category discrimination. Lastly, a fine-tuning process is\nconducted to recover the performance of the pruned model. To our best\nknowledge, it is the first time that CNN interpretability theory is considered\nto guide channel pruning. Extensive experiments on representative image\nclassification tasks demonstrate the superiority of our White-Box over many\nstate-of-the-arts. For instance, on CIFAR-10, it reduces 65.23% FLOPs with even\n0.62% accuracy improvement for ResNet-110. On ILSVRC-2012, White-Box achieves a\n45.6% FLOPs reduction with only a small loss of 0.83% in the top-1 accuracy for\nResNet-50.",
          "link": "http://arxiv.org/abs/2104.11883",
          "publishedOn": "2021-06-29T01:55:17.096Z",
          "wordCount": 707,
          "title": "Channel Pruning in a White Box for Efficient Image Classification. (arXiv:2104.11883v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07409",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_X/0/1/0/all/0/1\">Xiaofei Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jiangtao Xie</a>",
          "description": "This is a short technical report introducing the solution of Team Rat for\nShort-video Parsing Face Parsing Track of The 3rd Person in Context (PIC)\nWorkshop and Challenge at CVPR 2021.\n\nIn this report, we propose an Edge-Aware Network (EANet) that uses edge\ninformation to refine the segmentation edge. To further obtain the finer edge\nresults, we introduce edge attention loss that only compute cross entropy on\nthe edges, it can effectively reduce the classification error around edge and\nget more smooth boundary. Benefiting from the edge information and edge\nattention loss, the proposed EANet achieves 86.16\\% accuracy in the Short-video\nFace Parsing track of the 3rd Person in Context (PIC) Workshop and Challenge,\nranked the third place.",
          "link": "http://arxiv.org/abs/2106.07409",
          "publishedOn": "2021-06-29T01:55:17.090Z",
          "wordCount": 569,
          "title": "3rd Place Solution for Short-video Face Parsing Challenge. (arXiv:2106.07409v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.09165",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1\">Ji Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Graham_B/0/1/0/all/0/1\">Benjamin Graham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niessner_M/0/1/0/all/0/1\">Matthias Nie&#xdf;ner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1\">Saining Xie</a>",
          "description": "The rapid progress in 3D scene understanding has come with growing demand for\ndata; however, collecting and annotating 3D scenes (e.g. point clouds) are\nnotoriously hard. For example, the number of scenes (e.g. indoor rooms) that\ncan be accessed and scanned might be limited; even given sufficient data,\nacquiring 3D labels (e.g. instance masks) requires intensive human labor. In\nthis paper, we explore data-efficient learning for 3D point cloud. As a first\nstep towards this direction, we propose Contrastive Scene Contexts, a 3D\npre-training method that makes use of both point-level correspondences and\nspatial contexts in a scene. Our method achieves state-of-the-art results on a\nsuite of benchmarks where training data or labels are scarce. Our study reveals\nthat exhaustive labelling of 3D point clouds might be unnecessary; and\nremarkably, on ScanNet, even using 0.1% of point labels, we still achieve 89%\n(instance segmentation) and 96% (semantic segmentation) of the baseline\nperformance that uses full annotations.",
          "link": "http://arxiv.org/abs/2012.09165",
          "publishedOn": "2021-06-29T01:55:17.067Z",
          "wordCount": 635,
          "title": "Exploring Data-Efficient 3D Scene Understanding with Contrastive Scene Contexts. (arXiv:2012.09165v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.08655",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khose_S/0/1/0/all/0/1\">Sahil Khose</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tiwari_A/0/1/0/all/0/1\">Abhiraj Tiwari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_A/0/1/0/all/0/1\">Ankita Ghosh</a>",
          "description": "FloodNet is a high-resolution image dataset acquired by a small UAV platform,\nDJI Mavic Pro quadcopters, after Hurricane Harvey. The dataset presents a\nunique challenge of advancing the damage assessment process for post-disaster\nscenarios using unlabeled and limited labeled dataset. We propose a solution to\naddress their classification and semantic segmentation challenge. We approach\nthis problem by generating pseudo labels for both classification and\nsegmentation during training and slowly incrementing the amount by which the\npseudo label loss affects the final loss. Using this semi-supervised method of\ntraining helped us improve our baseline supervised loss by a huge margin for\nclassification, allowing the model to generalize and perform better on the\nvalidation and test splits of the dataset. In this paper, we compare and\ncontrast the various methods and models for image classification and semantic\nsegmentation on the FloodNet dataset.",
          "link": "http://arxiv.org/abs/2105.08655",
          "publishedOn": "2021-06-29T01:55:17.060Z",
          "wordCount": 611,
          "title": "Semi-Supervised Classification and Segmentation on High Resolution Aerial Images. (arXiv:2105.08655v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09229",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chaves_L/0/1/0/all/0/1\">Levy Chaves</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bissoto_A/0/1/0/all/0/1\">Alceu Bissoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valle_E/0/1/0/all/0/1\">Eduardo Valle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avila_S/0/1/0/all/0/1\">Sandra Avila</a>",
          "description": "Self-supervised pre-training appears as an advantageous alternative to\nsupervised pre-trained for transfer learning. By synthesizing annotations on\npretext tasks, self-supervision allows to pre-train models on large amounts of\npseudo-labels before fine-tuning them on the target task. In this work, we\nassess self-supervision for the diagnosis of skin lesions, comparing three\nself-supervised pipelines to a challenging supervised baseline, on five test\ndatasets comprising in- and out-of-distribution samples. Our results show that\nself-supervision is competitive both in improving accuracies and in reducing\nthe variability of outcomes. Self-supervision proves particularly useful for\nlow training data scenarios ($<1\\,500$ and $<150$ samples), where its ability\nto stabilize the outcomes is essential to provide sound results.",
          "link": "http://arxiv.org/abs/2106.09229",
          "publishedOn": "2021-06-29T01:55:17.041Z",
          "wordCount": 568,
          "title": "An Evaluation of Self-Supervised Pre-Training for Skin-Lesion Analysis. (arXiv:2106.09229v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.03072",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Gu_J/0/1/0/all/0/1\">Jinjin Gu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cai_H/0/1/0/all/0/1\">Haoming Cai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dong_C/0/1/0/all/0/1\">Chao Dong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ren_J/0/1/0/all/0/1\">Jimmy S. Ren</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gu_S/0/1/0/all/0/1\">Shuhang Gu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Timofte_R/0/1/0/all/0/1\">Radu Timofte</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cheon_M/0/1/0/all/0/1\">Manri Cheon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yoon_S/0/1/0/all/0/1\">Sungjun Yoon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kang_B/0/1/0/all/0/1\">Byungyeon Kang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lee_J/0/1/0/all/0/1\">Junwoo Lee</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Q/0/1/0/all/0/1\">Qing Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guo_H/0/1/0/all/0/1\">Haiyang Guo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bin_Y/0/1/0/all/0/1\">Yi Bin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hou_Y/0/1/0/all/0/1\">Yuqing Hou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Luo_H/0/1/0/all/0/1\">Hengliang Luo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guo_J/0/1/0/all/0/1\">Jingyu Guo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1\">Zirui Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_H/0/1/0/all/0/1\">Hai Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_W/0/1/0/all/0/1\">Wenming Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bai_Q/0/1/0/all/0/1\">Qingyan Bai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shi_S/0/1/0/all/0/1\">Shuwei Shi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xia_W/0/1/0/all/0/1\">Weihao Xia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cao_M/0/1/0/all/0/1\">Mingdeng Cao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1\">Jiahao Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1\">Yifan Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_Y/0/1/0/all/0/1\">Yujiu Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1\">Yang Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_T/0/1/0/all/0/1\">Tao Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Feng_L/0/1/0/all/0/1\">Longtao Feng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liao_Y/0/1/0/all/0/1\">Yiting Liao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1\">Junlin Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Thong_W/0/1/0/all/0/1\">William Thong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pereira_J/0/1/0/all/0/1\">Jose Costa Pereira</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Leonardis_A/0/1/0/all/0/1\">Ales Leonardis</a>, <a href=\"http://arxiv.org/find/eess/1/au:+McDonagh_S/0/1/0/all/0/1\">Steven McDonagh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_K/0/1/0/all/0/1\">Kele Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_L/0/1/0/all/0/1\">Lehan Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cai_H/0/1/0/all/0/1\">Hengxing Cai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_P/0/1/0/all/0/1\">Pengfei Sun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ayyoubzadeh_S/0/1/0/all/0/1\">Seyed Mehdi Ayyoubzadeh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Royat_A/0/1/0/all/0/1\">Ali Royat</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fezza_S/0/1/0/all/0/1\">Sid Ahmed Fezza</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hammou_D/0/1/0/all/0/1\">Dounia Hammou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hamidouche_W/0/1/0/all/0/1\">Wassim Hamidouche</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ahn_S/0/1/0/all/0/1\">Sewoong Ahn</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yoon_G/0/1/0/all/0/1\">Gwangjin Yoon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tsubota_K/0/1/0/all/0/1\">Koki Tsubota</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Akutsu_H/0/1/0/all/0/1\">Hiroaki Akutsu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Aizawa_K/0/1/0/all/0/1\">Kiyoharu Aizawa</a>",
          "description": "This paper reports on the NTIRE 2021 challenge on perceptual image quality\nassessment (IQA), held in conjunction with the New Trends in Image Restoration\nand Enhancement workshop (NTIRE) workshop at CVPR 2021. As a new type of image\nprocessing technology, perceptual image processing algorithms based on\nGenerative Adversarial Networks (GAN) have produced images with more realistic\ntextures. These output images have completely different characteristics from\ntraditional distortions, thus pose a new challenge for IQA methods to evaluate\ntheir visual quality. In comparison with previous IQA challenges, the training\nand testing datasets in this challenge include the outputs of perceptual image\nprocessing algorithms and the corresponding subjective scores. Thus they can be\nused to develop and evaluate IQA methods on GAN-based distortions. The\nchallenge has 270 registered participants in total. In the final testing stage,\n13 participating teams submitted their models and fact sheets. Almost all of\nthem have achieved much better results than existing IQA methods, while the\nwinning method can demonstrate state-of-the-art performance.",
          "link": "http://arxiv.org/abs/2105.03072",
          "publishedOn": "2021-06-29T01:55:17.004Z",
          "wordCount": 730,
          "title": "NTIRE 2021 Challenge on Perceptual Image Quality Assessment. (arXiv:2105.03072v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14459",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ngo_T/0/1/0/all/0/1\">Trung Tan Ngo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1\">Hung Tuan Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ly_N/0/1/0/all/0/1\">Nam Tuan Ly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakagawa_M/0/1/0/all/0/1\">Masaki Nakagawa</a>",
          "description": "In this paper, we propose an RNN-Transducer model for recognizing Japanese\nand Chinese offline handwritten text line images. As far as we know, it is the\nfirst approach that adopts the RNN-Transducer model for offline handwritten\ntext recognition. The proposed model consists of three main components: a\nvisual feature encoder that extracts visual features from an input image by CNN\nand then encodes the visual features by BLSTM; a linguistic context encoder\nthat extracts and encodes linguistic features from the input image by embedded\nlayers and LSTM; and a joint decoder that combines and then decodes the visual\nfeatures and the linguistic features into the final label sequence by fully\nconnected and softmax layers. The proposed model takes advantage of both visual\nand linguistic information from the input image. In the experiments, we\nevaluated the performance of the proposed model on the two datasets: Kuzushiji\nand SCUT-EPT. Experimental results show that the proposed model achieves\nstate-of-the-art performance on all datasets.",
          "link": "http://arxiv.org/abs/2106.14459",
          "publishedOn": "2021-06-29T01:55:16.982Z",
          "wordCount": 607,
          "title": "Recurrent neural network transducer for Japanese and Chinese offline handwritten text recognition. (arXiv:2106.14459v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10598",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xue_W/0/1/0/all/0/1\">Wenyuan Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1\">Baosheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qingyong Li</a>",
          "description": "A table arranging data in rows and columns is a very effective data\nstructure, which has been widely used in business and scientific research.\nConsidering large-scale tabular data in online and offline documents, automatic\ntable recognition has attracted increasing attention from the document analysis\ncommunity. Though human can easily understand the structure of tables, it\nremains a challenge for machines to understand that, especially due to a\nvariety of different table layouts and styles. Existing methods usually model a\ntable as either the markup sequence or the adjacency matrix between different\ntable cells, failing to address the importance of the logical location of table\ncells, e.g., a cell is located in the first row and the second column of the\ntable. In this paper, we reformulate the problem of table structure recognition\nas the table graph reconstruction, and propose an end-to-end trainable table\ngraph reconstruction network (TGRNet) for table structure recognition.\nSpecifically, the proposed method has two main branches, a cell detection\nbranch and a cell logical location branch, to jointly predict the spatial\nlocation and the logical location of different cells. Experimental results on\nthree popular table recognition datasets and a new dataset with table graph\nannotations (TableGraph-350K) demonstrate the effectiveness of the proposed\nTGRNet for table structure recognition. Code and annotations will be made\npublicly available.",
          "link": "http://arxiv.org/abs/2106.10598",
          "publishedOn": "2021-06-29T01:55:16.953Z",
          "wordCount": 681,
          "title": "TGRNet: A Table Graph Reconstruction Network for Table Structure Recognition. (arXiv:2106.10598v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10026",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Lijin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yifei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sugano_Y/0/1/0/all/0/1\">Yusuke Sugano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sato_Y/0/1/0/all/0/1\">Yoichi Sato</a>",
          "description": "In this report, we describe the technical details of our submission to the\n2021 EPIC-KITCHENS-100 Unsupervised Domain Adaptation Challenge for Action\nRecognition. Leveraging multiple modalities has been proved to benefit the\nUnsupervised Domain Adaptation (UDA) task. In this work, we present Multi-Modal\nMutual Enhancement Module (M3EM), a deep module for jointly considering\ninformation from multiple modalities to find the most transferable\nrepresentations across domains. We achieve this by implementing two sub-modules\nfor enhancing each modality using the context of other modalities. The first\nsub-module exchanges information across modalities through the semantic space,\nwhile the second sub-module finds the most transferable spatial region based on\nthe consensus of all modalities.",
          "link": "http://arxiv.org/abs/2106.10026",
          "publishedOn": "2021-06-29T01:55:16.946Z",
          "wordCount": 586,
          "title": "EPIC-KITCHENS-100 Unsupervised Domain Adaptation Challenge for Action Recognition 2021: Team M3EM Technical Report. (arXiv:2106.10026v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.07175",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Su_L/0/1/0/all/0/1\">Lang Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1\">Chuqing Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guofa Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_D/0/1/0/all/0/1\">Dongpu Cao</a>",
          "description": "Multimodal learning mimics the reasoning process of the human multi-sensory\nsystem, which is used to perceive the surrounding world. While making a\nprediction, the human brain tends to relate crucial cues from multiple sources\nof information. In this work, we propose a novel multimodal fusion module that\nlearns to emphasize more contributive features across all modalities.\nSpecifically, the proposed Multimodal Split Attention Fusion (MSAF) module\nsplits each modality into channel-wise equal feature blocks and creates a joint\nrepresentation that is used to generate soft attention for each channel across\nthe feature blocks. Further, the MSAF module is designed to be compatible with\nfeatures of various spatial dimensions and sequence lengths, suitable for both\nCNNs and RNNs. Thus, MSAF can be easily added to fuse features of any unimodal\nnetworks and utilize existing pretrained unimodal model weights. To demonstrate\nthe effectiveness of our fusion module, we design three multimodal networks\nwith MSAF for emotion recognition, sentiment analysis, and action recognition\ntasks. Our approach achieves competitive results in each task and outperforms\nother application-specific networks and multimodal fusion benchmarks.",
          "link": "http://arxiv.org/abs/2012.07175",
          "publishedOn": "2021-06-29T01:55:16.924Z",
          "wordCount": 637,
          "title": "MSAF: Multimodal Split Attention Fusion. (arXiv:2012.07175v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.14870",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Henriques_L/0/1/0/all/0/1\">Luis Felipe M.O. Henriques</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morgan_E/0/1/0/all/0/1\">Eduardo Morgan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Colcher_S/0/1/0/all/0/1\">Sergio Colcher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milidiu_R/0/1/0/all/0/1\">Ruy Luiz Milidi&#xfa;</a>",
          "description": "Non-Intrusive Load Monitoring (NILM) is a computational technique to estimate\nthe power loads' appliance-by-appliance from the whole consumption measured by\na single meter. In this paper, we propose a conditional density estimation\nmodel, based on deep neural networks, that joins a Conditional Variational\nAutoencoder with a Conditional Invertible Normalizing Flow model to estimate\nthe individual appliance's power demand. The resulting model is called Prior\nFlow Variational Autoencoder or, for simplicity PFVAE. Thus, instead of having\none model per appliance, the resulting model is responsible for estimating the\npower demand, appliance-by-appliance, at once. We train and evaluate our\nproposed model in a publicly available dataset composed of power demand\nmeasures from a poultry feed factory located in Brazil. The proposed model's\nquality is evaluated by comparing the obtained normalized disaggregation error\n(NDE) and signal aggregated error (SAE) with the previous work values on the\nsame dataset. Our proposal achieves highly competitive results, and for six of\nthe eight machines belonging to the dataset, we observe consistent improvements\nthat go from 28% up to 81% in NDE and from 27% up to 86% in SAE.",
          "link": "http://arxiv.org/abs/2011.14870",
          "publishedOn": "2021-06-29T01:55:16.900Z",
          "wordCount": 663,
          "title": "Prior Flow Variational Autoencoder: A density estimation model for Non-Intrusive Load Monitoring. (arXiv:2011.14870v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.03316",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1\">Ying Dai</a>",
          "description": "To establish an appropriate model for photo aesthetic assessment, in this\npaper, a D-measure which reflects the disentanglement degree of the final layer\nFC nodes of CNN is introduced. By combining F-measure with D-measure to obtain\na FD measure, an algorithm of determining the optimal model from the multiple\nphoto score prediction models generated by CNN-based repetitively self-revised\nlearning(RSRL) is proposed. Furthermore, the first fixation perspective(FFP)\nand the assessment interest region(AIR) of the models are defined and\ncalculated. The experimental results show that the FD measure is effective for\nestablishing the appropriate model from the multiple score prediction models\nwith different CNN structures. Moreover, the FD-determined optimal models with\nthe comparatively high FD always have the FFP an AIR which are close to the\nhuman's aesthetic perception when enjoying photos.",
          "link": "http://arxiv.org/abs/2106.03316",
          "publishedOn": "2021-06-29T01:55:16.876Z",
          "wordCount": 591,
          "title": "Exploring to establish an appropriate model for image aesthetic assessment via CNN-based RSRL: An empirical study. (arXiv:2106.03316v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.02800",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Q/0/1/0/all/0/1\">Qian Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sampani_K/0/1/0/all/0/1\">Konstantina Sampani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_M/0/1/0/all/0/1\">Mengjia Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cai_S/0/1/0/all/0/1\">Shengze Cai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Deng_Y/0/1/0/all/0/1\">Yixiang Deng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1\">He Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_J/0/1/0/all/0/1\">Jennifer K. Sun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Karniadakis_G/0/1/0/all/0/1\">George Em Karniadakis</a>",
          "description": "Microaneurysms (MAs) are one of the earliest signs of diabetic retinopathy\n(DR), a frequent complication of diabetes that can lead to visual impairment\nand blindness. Adaptive optics scanning laser ophthalmoscopy (AOSLO) provides\nreal-time retinal images with resolution down to 2 $\\mu m$ and thus allows\ndetection of the morphologies of individual MAs, a potential marker that might\ndictate MA pathology and affect the progression of DR. In contrast to the\nnumerous automatic models developed for assessing the number of MAs on fundus\nphotographs, currently there is no high throughput image protocol available for\nautomatic analysis of AOSLO photographs. To address this urgency, we introduce\nAOSLO-net, a deep neural network framework with customized training policies to\nautomatically segment MAs from AOSLO images. We evaluate the performance of\nAOSLO-net using 87 DR AOSLO images and our results demonstrate that the\nproposed model outperforms the state-of-the-art segmentation model both in\naccuracy and cost and enables correct MA morphological classification.",
          "link": "http://arxiv.org/abs/2106.02800",
          "publishedOn": "2021-06-29T01:55:16.861Z",
          "wordCount": 651,
          "title": "AOSLO-net: A deep learning-based method for automatic segmentation of retinal microaneurysms from adaptive optics scanning laser ophthalmoscope images. (arXiv:2106.02800v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.04430",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenxuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1\">Meng Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiangyun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zha_S/0/1/0/all/0/1\">Sen Zha</a>",
          "description": "Transformer, which can benefit from global (long-range) information modeling\nusing self-attention mechanisms, has been successful in natural language\nprocessing and 2D image classification recently. However, both local and global\nfeatures are crucial for dense prediction tasks, especially for 3D medical\nimage segmentation. In this paper, we for the first time exploit Transformer in\n3D CNN for MRI Brain Tumor Segmentation and propose a novel network named\nTransBTS based on the encoder-decoder structure. To capture the local 3D\ncontext information, the encoder first utilizes 3D CNN to extract the\nvolumetric spatial feature maps. Meanwhile, the feature maps are reformed\nelaborately for tokens that are fed into Transformer for global feature\nmodeling. The decoder leverages the features embedded by Transformer and\nperforms progressive upsampling to predict the detailed segmentation map.\nExtensive experimental results on both BraTS 2019 and 2020 datasets show that\nTransBTS achieves comparable or higher results than previous state-of-the-art\n3D methods for brain tumor segmentation on 3D MRI scans. The source code is\navailable at https://github.com/Wenxuan-1119/TransBTS",
          "link": "http://arxiv.org/abs/2103.04430",
          "publishedOn": "2021-06-29T01:55:16.749Z",
          "wordCount": 640,
          "title": "TransBTS: Multimodal Brain Tumor Segmentation Using Transformer. (arXiv:2103.04430v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.06930",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1\">Qingyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zixuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adeli_E/0/1/0/all/0/1\">Ehsan Adeli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pohl_K/0/1/0/all/0/1\">Kilian M. Pohl</a>",
          "description": "Machine learning analysis of longitudinal neuroimaging data is typically\nbased on supervised learning, which requires a large number of ground-truth\nlabels to be informative. As ground-truth labels are often missing or expensive\nto obtain in neuroscience, we avoid them in our analysis by combing factor\ndisentanglement with self-supervised learning to identify changes and\nconsistencies across the multiple MRIs acquired of each individual over time.\nSpecifically, we propose a new definition of disentanglement by formulating a\nmultivariate mapping between factors (e.g., brain age) associated with an MRI\nand a latent image representation. Then, factors that evolve across\nacquisitions of longitudinal sequences are disentangled from that mapping by\nself-supervised learning in such a way that changes in a single factor induce\nchange along one direction in the representation space. We implement this\nmodel, named Longitudinal Self-Supervised Learning (LSSL), via a standard\nautoencoding structure with a cosine loss to disentangle brain age from the\nimage representation. We apply LSSL to two longitudinal neuroimaging studies to\nhighlight its strength in extracting the brain-age information from MRI and\nrevealing informative characteristics associated with neurodegenerative and\nneuropsychological disorders. Moreover, the representations learned by LSSL\nfacilitate supervised classification by recording faster convergence and higher\n(or similar) prediction accuracy compared to several other representation\nlearning techniques.",
          "link": "http://arxiv.org/abs/2006.06930",
          "publishedOn": "2021-06-29T01:55:16.698Z",
          "wordCount": 670,
          "title": "Longitudinal Self-Supervised Learning. (arXiv:2006.06930v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.12026",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Skorokhodov_I/0/1/0/all/0/1\">Ivan Skorokhodov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ignatyev_S/0/1/0/all/0/1\">Savva Ignatyev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elhoseiny_M/0/1/0/all/0/1\">Mohamed Elhoseiny</a>",
          "description": "In most existing learning systems, images are typically viewed as 2D pixel\narrays. However, in another paradigm gaining popularity, a 2D image is\nrepresented as an implicit neural representation (INR) - an MLP that predicts\nan RGB pixel value given its (x,y) coordinate. In this paper, we propose two\nnovel architectural techniques for building INR-based image decoders:\nfactorized multiplicative modulation and multi-scale INRs, and use them to\nbuild a state-of-the-art continuous image GAN. Previous attempts to adapt INRs\nfor image generation were limited to MNIST-like datasets and do not scale to\ncomplex real-world data. Our proposed INR-GAN architecture improves the\nperformance of continuous image generators by several times, greatly reducing\nthe gap between continuous image GANs and pixel-based ones. Apart from that, we\nexplore several exciting properties of the INR-based decoders, like\nout-of-the-box superresolution, meaningful image-space interpolation,\naccelerated inference of low-resolution images, an ability to extrapolate\noutside of image boundaries, and strong geometric prior. The project page is\nlocated at https://universome.github.io/inr-gan.",
          "link": "http://arxiv.org/abs/2011.12026",
          "publishedOn": "2021-06-29T01:55:16.644Z",
          "wordCount": 629,
          "title": "Adversarial Generation of Continuous Images. (arXiv:2011.12026v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.07354",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Do_T/0/1/0/all/0/1\">Thao Do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Daeyoung Kim</a>",
          "description": "There are many real-life use cases such as barcode scanning or billboard\nreading where people need to detect objects and read the object contents.\nCommonly existing methods are first trying to localize object regions, then\ndetermine layout and lastly classify content units. However, for simple fixed\nstructured objects like license plates, this approach becomes overkill and\nlengthy to run. This work aims to solve this detect-and-read problem in a\nlightweight way by integrating multi-digit recognition into a one-stage object\ndetection model. Our unified method not only eliminates the duplication in\nfeature extraction (one for localizing, one again for classifying) but also\nprovides useful contextual information around object regions for\nclassification. Additionally, our choice of backbones and modifications in\narchitecture, loss function, data augmentation and training make the method\nrobust, efficient and speedy. Secondly, we made a public benchmark dataset of\ndiverse real-life 1D barcodes for a reliable evaluation, which we collected,\nannotated and checked carefully. Eventually, experimental results prove the\nmethod's efficiency on the barcode problem by outperforming industrial tools in\nboth detecting and decoding rates with a real-time fps at a VGA-similar\nresolution. It also did a great job expectedly on the license-plate recognition\ntask (on the AOLP dataset) by outperforming the current state-of-the-art method\nsignificantly in terms of recognition rate and inference time.",
          "link": "http://arxiv.org/abs/2102.07354",
          "publishedOn": "2021-06-29T01:55:16.303Z",
          "wordCount": 698,
          "title": "QuickBrowser: A Unified Model to Detect and Read Simple Object in Real-time. (arXiv:2102.07354v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14195",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Macke_J/0/1/0/all/0/1\">J. Macke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sedlar_J/0/1/0/all/0/1\">J. Sedlar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olsak_M/0/1/0/all/0/1\">M. Olsak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Urban_J/0/1/0/all/0/1\">J. Urban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sivic_J/0/1/0/all/0/1\">J. Sivic</a>",
          "description": "We describe a purely image-based method for finding geometric constructions\nwith a ruler and compass in the Euclidea geometric game. The method is based on\nadapting the Mask R-CNN state-of-the-art image processing neural architecture\nand adding a tree-based search procedure to it. In a supervised setting, the\nmethod learns to solve all 68 kinds of geometric construction problems from the\nfirst six level packs of Euclidea with an average 92% accuracy. When evaluated\non new kinds of problems, the method can solve 31 of the 68 kinds of Euclidea\nproblems. We believe that this is the first time that a purely image-based\nlearning has been trained to solve geometric construction problems of this\ndifficulty.",
          "link": "http://arxiv.org/abs/2106.14195",
          "publishedOn": "2021-06-29T01:55:16.269Z",
          "wordCount": 576,
          "title": "Learning to solve geometric construction problems from images. (arXiv:2106.14195v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/1906.02944",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Han-Jia Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Hexiang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_D/0/1/0/all/0/1\">De-Chuan Zhan</a>",
          "description": "Object recognition in the real-world requires handling long-tailed or even\nopen-ended data. An ideal visual system needs to recognize the populated head\nvisual concepts reliably and meanwhile efficiently learn about emerging new\ntail categories with a few training instances. Class-balanced many-shot\nlearning and few-shot learning tackle one side of this problem, by either\nlearning strong classifiers for head or learning to learn few-shot classifiers\nfor the tail. In this paper, we investigate the problem of generalized few-shot\nlearning (GFSL) -- a model during the deployment is required to learn about\ntail categories with few shots and simultaneously classify the head classes. We\npropose the ClAssifier SynThesis LEarning (CASTLE), a learning framework that\nlearns how to synthesize calibrated few-shot classifiers in addition to the\nmulti-class classifiers of head classes with a shared neural dictionary,\nshedding light upon the inductive GFSL. Furthermore, we propose an adaptive\nversion of CASTLE (ACASTLE) that adapts the head classifiers conditioned on the\nincoming tail training examples, yielding a framework that allows effective\nbackward knowledge transfer. As a consequence, ACASTLE can handle GFSL with\nclasses from heterogeneous domains effectively. CASTLE and ACASTLE demonstrate\nsuperior performances than existing GFSL algorithms and strong baselines on\nMiniImageNet as well as TieredImageNet datasets. More interestingly, they\noutperform previous state-of-the-art methods when evaluated with standard\nfew-shot learning criteria.",
          "link": "http://arxiv.org/abs/1906.02944",
          "publishedOn": "2021-06-29T01:55:16.216Z",
          "wordCount": 722,
          "title": "Learning Adaptive Classifiers Synthesis for Generalized Few-Shot Learning. (arXiv:1906.02944v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.10189",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1\">Jiawei Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1\">Songhao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Z/0/1/0/all/0/1\">Zhiwei Liang</a>",
          "description": "Facial Expression Recognition (FER) is a classification task that points to\nface variants. Hence, there are certain affinity features between facial\nexpressions, receiving little attention in the FER literature. Convolution\npadding, despite helping capture the edge information, causes erosion of the\nfeature map simultaneously. After multi-layer filling convolution, the output\nfeature map named albino feature definitely weakens the representation of the\nexpression. To tackle these challenges, we propose a novel architecture named\nAmending Representation Module (ARM). ARM is a substitute for the pooling\nlayer. Theoretically, it can be embedded in the back end of any network to deal\nwith the Padding Erosion. ARM efficiently enhances facial expression\nrepresentation from two different directions: 1) reducing the weight of eroded\nfeatures to offset the side effect of padding, and 2) sharing affinity features\nover mini-batch to strengthen the representation learning. Experiments on\npublic benchmarks prove that our ARM boosts the performance of FER remarkably.\nThe validation accuracies are respectively 92.05% on RAF-DB, 65.2% on\nAffect-Net, and 58.71% on SFEW, exceeding current state-of-the-art methods. Our\nimplementation and trained models are available at\nhttps://github.com/JiaweiShiCV/Amend-Representation-Module.",
          "link": "http://arxiv.org/abs/2103.10189",
          "publishedOn": "2021-06-29T01:55:16.181Z",
          "wordCount": 647,
          "title": "Learning to Amend Facial Expression Representation via De-albino and Affinity. (arXiv:2103.10189v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14166",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Cheng Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsiao_C/0/1/0/all/0/1\">Chi-Wei Hsiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1\">Ning-Hsu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Min Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hwann-Tzong Chen</a>",
          "description": "Indoor panorama typically consists of human-made structures parallel or\nperpendicular to gravity. We leverage this phenomenon to approximate the scene\nin a 360-degree image with (H)orizontal-planes and (V)ertical-planes. To this\nend, we propose an effective divide-and-conquer strategy that divides pixels\nbased on their plane orientation estimation; then, the succeeding instance\nsegmentation module conquers the task of planes clustering more easily in each\nplane orientation group. Besides, parameters of V-planes depend on camera yaw\nrotation, but translation-invariant CNNs are less aware of the yaw change. We\nthus propose a yaw-invariant V-planar reparameterization for CNNs to learn. We\ncreate a benchmark for indoor panorama planar reconstruction by extending\nexisting 360 depth datasets with ground truth H\\&V-planes (referred to as\nPanoH&V dataset) and adopt state-of-the-art planar reconstruction methods to\npredict H\\&V-planes as our baselines. Our method outperforms the baselines by a\nlarge margin on the proposed dataset.",
          "link": "http://arxiv.org/abs/2106.14166",
          "publishedOn": "2021-06-29T01:55:16.149Z",
          "wordCount": 584,
          "title": "Indoor Panorama Planar 3D Reconstruction via Divide and Conquer. (arXiv:2106.14166v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/1810.01256",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_G/0/1/0/all/0/1\">Guanxiong Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_B/0/1/0/all/0/1\">Bo Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Shan Yu</a>",
          "description": "Deep neural networks (DNNs) are powerful tools in learning sophisticated but\nfixed mapping rules between inputs and outputs, thereby limiting their\napplication in more complex and dynamic situations in which the mapping rules\nare not kept the same but changing according to different contexts. To lift\nsuch limits, we developed a novel approach involving a learning algorithm,\ncalled orthogonal weights modification (OWM), with the addition of a\ncontext-dependent processing (CDP) module. We demonstrated that with OWM to\novercome the problem of catastrophic forgetting, and the CDP module to learn\nhow to reuse a feature representation and a classifier for different contexts,\na single network can acquire numerous context-dependent mapping rules in an\nonline and continual manner, with as few as $\\sim$10 samples to learn each.\nThis should enable highly compact systems to gradually learn myriad\nregularities of the real world and eventually behave appropriately within it.",
          "link": "http://arxiv.org/abs/1810.01256",
          "publishedOn": "2021-06-29T01:55:16.106Z",
          "wordCount": 634,
          "title": "Continual Learning of Context-dependent Processing in Neural Networks. (arXiv:1810.01256v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.00442",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rajeswar_S/0/1/0/all/0/1\">Sai Rajeswar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ibrahim_C/0/1/0/all/0/1\">Cyril Ibrahim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Surya_N/0/1/0/all/0/1\">Nitin Surya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Golemo_F/0/1/0/all/0/1\">Florian Golemo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vazquez_D/0/1/0/all/0/1\">David Vazquez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Courville_A/0/1/0/all/0/1\">Aaron Courville</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pinheiro_P/0/1/0/all/0/1\">Pedro O. Pinheiro</a>",
          "description": "Robots in many real-world settings have access to force/torque sensors in\ntheir gripper and tactile sensing is often necessary in tasks that involve\ncontact-rich motion. In this work, we leverage surprise from mismatches in\ntouch feedback to guide exploration in hard sparse-reward reinforcement\nlearning tasks. Our approach, Touch-based Curiosity (ToC), learns what visible\nobjects interactions are supposed to \"feel\" like. We encourage exploration by\nrewarding interactions where the expectation and the experience don't match. In\nour proposed method, an initial task-independent exploration phase is followed\nby an on-task learning phase, in which the original interactions are relabeled\nwith on-task rewards. We test our approach on a range of touch-intensive robot\narm tasks (e.g. pushing objects, opening doors), which we also release as part\nof this work. Across multiple experiments in a simulated setting, we\ndemonstrate that our method is able to learn these difficult tasks through\nsparse reward and curiosity alone. We compare our cross-modal approach to\nsingle-modality (touch- or vision-only) approaches as well as other\ncuriosity-based methods and find that our method performs better and is more\nsample-efficient.",
          "link": "http://arxiv.org/abs/2104.00442",
          "publishedOn": "2021-06-29T01:55:16.057Z",
          "wordCount": 655,
          "title": "Touch-based Curiosity for Sparse-Reward Tasks. (arXiv:2104.00442v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.13392",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gominski_D/0/1/0/all/0/1\">Dimitri Gominski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gouet_Brunet_V/0/1/0/all/0/1\">Val&#xe9;rie Gouet-Brunet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liming Chen</a>",
          "description": "Advances in high resolution remote sensing image analysis are currently\nhampered by the difficulty of gathering enough annotated data for training deep\nlearning methods, giving rise to a variety of small datasets and associated\ndataset-specific methods. Moreover, typical tasks such as classification and\nretrieval lack a systematic evaluation on standard benchmarks and training\ndatasets, which make it hard to identify durable and generalizable scientific\ncontributions. We aim at unifying remote sensing image retrieval and\nclassification with a new large-scale training and testing dataset, SF300,\nincluding both vertical and oblique aerial images and made available to the\nresearch community, and an associated fine-tuning method. We additionally\npropose a new adversarial fine-tuning method for global descriptors. We show\nthat our framework systematically achieves a boost of retrieval and\nclassification performance on nine different datasets compared to an ImageNet\npretrained baseline, with currently no other method to compare to.",
          "link": "http://arxiv.org/abs/2102.13392",
          "publishedOn": "2021-06-29T01:55:16.045Z",
          "wordCount": 646,
          "title": "Unifying Remote Sensing Image Retrieval and Classification with Robust Fine-tuning. (arXiv:2102.13392v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.00731",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Law_H/0/1/0/all/0/1\">Ho Law</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_G/0/1/0/all/0/1\">Gary P. T. Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_K/0/1/0/all/0/1\">Ka Chun Lam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lui_L/0/1/0/all/0/1\">Lok Ming Lui</a>",
          "description": "Image registration has been widely studied over the past several decades,\nwith numerous applications in science, engineering and medicine. Most of the\nconventional mathematical models for large deformation image registration rely\non prescribed landmarks, which usually require tedious manual labeling and are\nprone to error. In recent years, there has been a surge of interest in the use\nof machine learning for image registration. In this paper, we develop a novel\nmethod for large deformation image registration by a fusion of quasiconformal\ntheory and convolutional neural network (CNN). More specifically, we propose a\nquasiconformal energy model with a novel fidelity term that incorporates the\nfeatures extracted using a pre-trained CNN, thereby allowing us to obtain\nmeaningful registration results without any guidance of prescribed landmarks.\nMoreover, unlike many prior image registration methods, the bijectivity of our\nmethod is guaranteed by quasiconformal theory. Experimental results are\npresented to demonstrate the effectiveness of the proposed method. More\nbroadly, our work sheds light on how rigorous mathematical theories and\npractical machine learning approaches can be integrated for developing\ncomputational methods with improved performance.",
          "link": "http://arxiv.org/abs/2011.00731",
          "publishedOn": "2021-06-29T01:55:16.038Z",
          "wordCount": 681,
          "title": "Quasiconformal model with CNN features for large deformation image registration. (arXiv:2011.00731v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.06957",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ferianc_M/0/1/0/all/0/1\">Martin Ferianc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manocha_D/0/1/0/all/0/1\">Divyansh Manocha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1\">Hongxiang Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rodrigues_M/0/1/0/all/0/1\">Miguel Rodrigues</a>",
          "description": "Fully convolutional U-shaped neural networks have largely been the dominant\napproach for pixel-wise image segmentation. In this work, we tackle two defects\nthat hinder their deployment in real-world applications: 1) Predictions lack\nuncertainty quantification that may be crucial to many decision-making systems;\n2) Large memory storage and computational consumption demanding extensive\nhardware resources. To address these issues and improve their practicality we\ndemonstrate a few-parameter compact Bayesian convolutional architecture, that\nachieves a marginal improvement in accuracy in comparison to related work using\nsignificantly fewer parameters and compute operations. The architecture\ncombines parameter-efficient operations such as separable convolutions,\nbilinear interpolation, multi-scale feature propagation and Bayesian inference\nfor per-pixel uncertainty quantification through Monte Carlo Dropout. The best\nperforming configurations required fewer than 2.5 million parameters on diverse\nchallenging datasets with few observations.",
          "link": "http://arxiv.org/abs/2104.06957",
          "publishedOn": "2021-06-29T01:55:16.016Z",
          "wordCount": 610,
          "title": "ComBiNet: Compact Convolutional Bayesian Neural Network for Image Segmentation. (arXiv:2104.06957v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14403",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Tan_W/0/1/0/all/0/1\">Weijun Tan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1\">Jingfeng Liu</a>",
          "description": "We present an automatic COVID1-19 diagnosis framework from lung CT-scan slice\nimages. In this framework, the slice images of a CT-scan volume are first\nproprocessed using segmentation techniques to filter out images of closed lung,\nand to remove the useless background. Then a resampling method is used to\nselect one or multiple sets of a fixed number of slice images for training and\nvalidation. A 3D CNN network with BERT is used to classify this set of selected\nslice images. In this network, an embedding feature is also extracted. In cases\nwhere there are more than one set of slice images in a volume, the features of\nall sets are extracted and pooled into a global feature vector for the whole\nCT-scan volume. A simple multiple-layer perceptron (MLP) network is used to\nfurther classify the aggregated feature vector. The models are trained and\nevaluated on the provided training and validation datasets. On the validation\ndataset, the accuracy is 0.9278 and the F1 score is 0.9261.",
          "link": "http://arxiv.org/abs/2106.14403",
          "publishedOn": "2021-06-29T01:55:16.009Z",
          "wordCount": 660,
          "title": "A 3D CNN Network with BERT For Automatic COVID-19 Diagnosis From CT-Scan Images. (arXiv:2106.14403v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2012.01030",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Terhorst_P/0/1/0/all/0/1\">Philipp Terh&#xf6;rst</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fahrmann_D/0/1/0/all/0/1\">Daniel F&#xe4;hrmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolf_J/0/1/0/all/0/1\">Jan Niklas Kolf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Damer_N/0/1/0/all/0/1\">Naser Damer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirchbuchner_F/0/1/0/all/0/1\">Florian Kirchbuchner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuijper_A/0/1/0/all/0/1\">Arjan Kuijper</a>",
          "description": "Soft-biometrics play an important role in face biometrics and related fields\nsince these might lead to biased performances, threatens the user's privacy, or\nare valuable for commercial aspects. Current face databases are specifically\nconstructed for the development of face recognition applications. Consequently,\nthese databases contain large amount of face images but lack in the number of\nattribute annotations and the overall annotation correctness. In this work, we\npropose MAADFace, a new face annotations database that is characterized by the\nlarge number of its high-quality attribute annotations. MAADFace is build on\nthe VGGFace2 database and thus, consists of 3.3M faces of over 9k individuals.\nUsing a novel annotation transfer-pipeline that allows an accurate\nlabel-transfer from multiple source-datasets to a target-dataset, MAAD-Face\nconsists of 123.9M attribute annotations of 47 different binary attributes.\nConsequently, it provides 15 and 137 times more attribute labels than CelebA\nand LFW. Our investigation on the annotation quality by three human evaluators\ndemonstrated the superiority of the MAAD-Face annotations over existing\ndatabases. Additionally, we make use of the large amount of high-quality\nannotations from MAAD-Face to study the viability of soft-biometrics for\nrecognition, providing insights about which attributes support genuine and\nimposter decisions. The MAAD-Face annotations dataset is publicly available.",
          "link": "http://arxiv.org/abs/2012.01030",
          "publishedOn": "2021-06-29T01:55:15.962Z",
          "wordCount": 680,
          "title": "MAAD-Face: A Massively Annotated Attribute Dataset for Face Images. (arXiv:2012.01030v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14133",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lai_X/0/1/0/all/0/1\">Xin Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Z/0/1/0/all/0/1\">Zhuotao Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1\">Li Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hengshuang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liwei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1\">Jiaya Jia</a>",
          "description": "Semantic segmentation has made tremendous progress in recent years. However,\nsatisfying performance highly depends on a large number of pixel-level\nannotations. Therefore, in this paper, we focus on the semi-supervised\nsegmentation problem where only a small set of labeled data is provided with a\nmuch larger collection of totally unlabeled images. Nevertheless, due to the\nlimited annotations, models may overly rely on the contexts available in the\ntraining data, which causes poor generalization to the scenes unseen before. A\npreferred high-level representation should capture the contextual information\nwhile not losing self-awareness. Therefore, we propose to maintain the\ncontext-aware consistency between features of the same identity but with\ndifferent contexts, making the representations robust to the varying\nenvironments. Moreover, we present the Directional Contrastive Loss (DC Loss)\nto accomplish the consistency in a pixel-to-pixel manner, only requiring the\nfeature with lower quality to be aligned towards its counterpart. In addition,\nto avoid the false-negative samples and filter the uncertain positive samples,\nwe put forward two sampling strategies. Extensive experiments show that our\nsimple yet effective method surpasses current state-of-the-art methods by a\nlarge margin and also generalizes well with extra image-level annotations.",
          "link": "http://arxiv.org/abs/2106.14133",
          "publishedOn": "2021-06-29T01:55:15.955Z",
          "wordCount": 643,
          "title": "Semi-supervised Semantic Segmentation with Directional Context-aware Consistency. (arXiv:2106.14133v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2009.12931",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ahmed_T/0/1/0/all/0/1\">Tashin Ahmed</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sabab_N/0/1/0/all/0/1\">Noor Hossain Nuri Sabab</a>",
          "description": "Climate change has been a common interest and the forefront of crucial\npolitical discussion and decision-making for many years. Shallow clouds play a\nsignificant role in understanding the Earth's climate, but they are challenging\nto interpret and represent in a climate model. By classifying these cloud\nstructures, there is a better possibility of understanding the physical\nstructures of the clouds, which would improve the climate model generation,\nresulting in a better prediction of climate change or forecasting weather\nupdate. Clouds organise in many forms, which makes it challenging to build\ntraditional rule-based algorithms to separate cloud features. In this paper,\nclassification of cloud organization patterns was performed using a new\nscaled-up version of Convolutional Neural Network (CNN) named as EfficientNet\nas the encoder and UNet as decoder where they worked as feature extractor and\nreconstructor of fine grained feature map and was used as a classifier, which\nwill help experts to understand how clouds will shape the future climate. By\nusing a segmentation model in a classification task, it was shown that with a\ngood encoder alongside UNet, it is possible to obtain good performance from\nthis dataset. Dice coefficient has been used for the final evaluation metric,\nwhich gave the score of 66.26\\% and 66.02\\% for public and private (test set)\nleaderboard on Kaggle competition respectively.",
          "link": "http://arxiv.org/abs/2009.12931",
          "publishedOn": "2021-06-29T01:55:15.798Z",
          "wordCount": 699,
          "title": "Classification and understanding of cloud structures via satellite images with EfficientUNet. (arXiv:2009.12931v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14248",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Feng_C/0/1/0/all/0/1\">Chun-Mei Feng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yan_Y/0/1/0/all/0/1\">Yunlu Yan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_G/0/1/0/all/0/1\">Geng Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fu_H/0/1/0/all/0/1\">Huazhu Fu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_Y/0/1/0/all/0/1\">Yong Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shao_L/0/1/0/all/0/1\">Ling Shao</a>",
          "description": "Accelerating multi-modal magnetic resonance (MR) imaging is a new and\neffective solution for fast MR imaging, providing superior performance in\nrestoring the target modality from its undersampled counterpart with guidance\nfrom an auxiliary modality. However, existing works simply introduce the\nauxiliary modality as prior information, lacking in-depth investigations on the\npotential mechanisms for fusing two modalities. Further, they usually rely on\nthe convolutional neural networks (CNNs), which focus on local information and\nprevent them from fully capturing the long-distance dependencies of global\nknowledge. To this end, we propose a multi-modal transformer (MTrans), which is\ncapable of transferring multi-scale features from the target modality to the\nauxiliary modality, for accelerated MR imaging. By restructuring the\ntransformer architecture, our MTrans gains a powerful ability to capture deep\nmulti-modal information. More specifically, the target modality and the\nauxiliary modality are first split into two branches and then fused using a\nmulti-modal transformer module. This module is based on an improved multi-head\nattention mechanism, named the cross attention module, which absorbs features\nfrom the auxiliary modality that contribute to the target modality. Our\nframework provides two appealing benefits: (i) MTrans is the first attempt at\nusing improved transformers for multi-modal MR imaging, affording more global\ninformation compared with CNN-based methods. (ii) A new cross attention module\nis proposed to exploit the useful information in each branch at different\nscales. It affords both distinct structural information and subtle pixel-level\ninformation, which supplement the target modality effectively.",
          "link": "http://arxiv.org/abs/2106.14248",
          "publishedOn": "2021-06-29T01:55:15.773Z",
          "wordCount": 687,
          "title": "MTrans: Multi-Modal Transformer for Accelerated MR Imaging. (arXiv:2106.14248v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2102.05228",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jung_H/0/1/0/all/0/1\">Hyungsik Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_Y/0/1/0/all/0/1\">Youngrock Oh</a>",
          "description": "Increasing demands for understanding the internal behavior of convolutional\nneural networks (CNNs) have led to remarkable improvements in explanation\nmethods. Particularly, several class activation mapping (CAM) based methods,\nwhich generate visual explanation maps by a linear combination of activation\nmaps from CNNs, have been proposed. However, the majority of the methods lack a\nclear theoretical basis on how they assign the coefficients of the linear\ncombination. In this paper, we revisit the intrinsic linearity of CAM with\nrespect to the activation maps; we construct an explanation model of CNN as a\nlinear function of binary variables that denote the existence of the\ncorresponding activation maps. With this approach, the explanation model can be\ndetermined by additive feature attribution methods in an analytic manner. We\nthen demonstrate the adequacy of SHAP values, which is a unique solution for\nthe explanation model with a set of desirable properties, as the coefficients\nof CAM. Since the exact SHAP values are unattainable, we introduce an efficient\napproximation method, LIFT-CAM, based on DeepLIFT. Our proposed LIFT-CAM can\nestimate the SHAP values of the activation maps with high speed and accuracy.\nFurthermore, it greatly outperforms other previous CAM-based methods in both\nqualitative and quantitative aspects.",
          "link": "http://arxiv.org/abs/2102.05228",
          "publishedOn": "2021-06-29T01:55:15.744Z",
          "wordCount": 653,
          "title": "Towards Better Explanations of Class Activation Mapping. (arXiv:2102.05228v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14307",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qin_R/0/1/0/all/0/1\">Rongjun Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xu Huang</a>",
          "description": "Image-based 3D object modeling refers to the process of converting raw\noptical images to 3D digital representations of the objects. Very often, such\nmodels are desired to be dimensionally true, semantically labeled with\nphotorealistic appearance (reality-based modeling). Laser scanning was deemed\nas the standard (and direct) way to obtaining highly accurate 3D measurements\nof objects, while one would have to abide the high acquisition cost and its\nunavailability on some of the platforms. Nowadays the image-based methods\nbackboned by the recently developed advanced dense image matching algorithms\nand geo-referencing paradigms, are becoming the dominant approaches, due to its\nhigh flexibility, availability and low cost. The largely automated geometric\nprocessing of images in a 3D object reconstruction workflow, from\nordered/unordered raw imagery to textured meshes, is becoming a critical part\nof the reality-based 3D modeling. This article summarizes the overall geometric\nprocessing workflow, with focuses on introducing the state-of-the-art methods\nof three major components of geometric processing: 1) geo-referencing; 2) Image\ndense matching 3) texture mapping. Finally, we will draw conclusions and share\nour outlooks of the topics discussed in this article.",
          "link": "http://arxiv.org/abs/2106.14307",
          "publishedOn": "2021-06-29T01:55:15.738Z",
          "wordCount": 610,
          "title": "Geometric Processing for Image-based 3D Object Modeling. (arXiv:2106.14307v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14474",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Maag_K/0/1/0/all/0/1\">Kira Maag</a>",
          "description": "Instance segmentation of images is an important tool for automated scene\nunderstanding. Neural networks are usually trained to optimize their overall\nperformance in terms of accuracy. Meanwhile, in applications such as automated\ndriving, an overlooked pedestrian seems more harmful than a falsely detected\none. In this work, we present a false negative detection method for image\nsequences based on inconsistencies in time series of tracked instances given\nthe availability of image sequences in online applications. As the number of\ninstances can be greatly increased by this algorithm, we apply a false positive\npruning using uncertainty estimates aggregated over instances. To this end,\ninstance-wise metrics are constructed which characterize uncertainty and\ngeometry of a given instance or are predicated on depth estimation. The\nproposed method serves as a post-processing step applicable to any neural\nnetwork that can also be trained on single frames only. In our tests, we obtain\nan improved trade-off between false negative and false positive instances by\nour fused detection approach in comparison to the use of an ordinary score\nvalue provided by the instance segmentation network during inference.",
          "link": "http://arxiv.org/abs/2106.14474",
          "publishedOn": "2021-06-29T01:55:15.732Z",
          "wordCount": 615,
          "title": "False Negative Reduction in Video Instance Segmentation using Uncertainty Estimates. (arXiv:2106.14474v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/1907.12727",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1\">Qingyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adeli_E/0/1/0/all/0/1\">Ehsan Adeli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfefferbaum_A/0/1/0/all/0/1\">Adolf Pfefferbaum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sullivan_E/0/1/0/all/0/1\">Edith V. Sullivan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pohl_K/0/1/0/all/0/1\">Kilian M. Pohl</a>",
          "description": "With recent advances in deep learning, neuroimaging studies increasingly rely\non convolutional networks (ConvNets) to predict diagnosis based on MR images.\nTo gain a better understanding of how a disease impacts the brain, the studies\nvisualize the salience maps of the ConvNet highlighting voxels within the brain\nmajorly contributing to the prediction. However, these salience maps are\ngenerally confounded, i.e., some salient regions are more predictive of\nconfounding variables (such as age) than the diagnosis. To avoid such\nmisinterpretation, we propose in this paper an approach that aims to visualize\nconfounder-free saliency maps that only highlight voxels predictive of the\ndiagnosis. The approach incorporates univariate statistical tests to identify\nconfounding effects within the intermediate features learned by ConvNet. The\ninfluence from the subset of confounded features is then removed by a novel\npartial back-propagation procedure. We use this two-step approach to visualize\nconfounder-free saliency maps extracted from synthetic and two real datasets.\nThese experiments reveal the potential of our visualization in producing\nunbiased model-interpretation.",
          "link": "http://arxiv.org/abs/1907.12727",
          "publishedOn": "2021-06-29T01:55:15.697Z",
          "wordCount": 638,
          "title": "Confounder-Aware Visualization of ConvNets. (arXiv:1907.12727v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2004.06297",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Do_T/0/1/0/all/0/1\">Thao Do</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tolcha_Y/0/1/0/all/0/1\">Yalew Tolcha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jun_T/0/1/0/all/0/1\">Tae Joon Jun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Daeyoung Kim</a>",
          "description": "Barcodes are ubiquitous and have been used in most of critical daily\nactivities for decades. However, most of traditional decoders require\nwell-founded barcode under a relatively standard condition. While wilder\nconditioned barcodes such as underexposed, occluded, blurry, wrinkled and\nrotated are commonly captured in reality, those traditional decoders show\nweakness of recognizing. Several works attempted to solve those challenging\nbarcodes, but many limitations still exist. This work aims to solve the\ndecoding problem using deep convolutional neural network with the possibility\nof running on portable devices. Firstly, we proposed a special modification of\ninference based on the feature of having checksum and test-time augmentation,\nnamed as Smart Inference (SI) in prediction phase of a trained model. SI\nconsiderably boosts accuracy and reduces the false prediction for trained\nmodels. Secondly, we have created a large practical evaluation dataset of real\ncaptured 1D barcode under various challenging conditions to test our methods\nvigorously, which is publicly available for other researchers. The experiments'\nresults demonstrated the SI effectiveness with the highest accuracy of 95.85%\nwhich outperformed many existing decoders on the evaluation set. Finally, we\nsuccessfully minimized the best model by knowledge distillation to a shallow\nmodel which is shown to have high accuracy (90.85%) with good inference speed\nof 34.2 ms per image on a real edge device.",
          "link": "http://arxiv.org/abs/2004.06297",
          "publishedOn": "2021-06-29T01:55:15.479Z",
          "wordCount": 706,
          "title": "Smart Inference for Multidigit Convolutional Neural Network based Barcode Decoding. (arXiv:2004.06297v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.02018",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Doyeon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joo_D/0/1/0/all/0/1\">Donggyu Joo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Junmo Kim</a>",
          "description": "Advances in technology have led to the development of methods that can create\ndesired visual multimedia. In particular, image generation using deep learning\nhas been extensively studied across diverse fields. In comparison, video\ngeneration, especially on conditional inputs, remains a challenging and less\nexplored area. To narrow this gap, we aim to train our model to produce a video\ncorresponding to a given text description. We propose a novel training\nframework, Text-to-Image-to-Video Generative Adversarial Network (TiVGAN),\nwhich evolves frame-by-frame and finally produces a full-length video. In the\nfirst phase, we focus on creating a high-quality single video frame while\nlearning the relationship between the text and an image. As the steps proceed,\nour model is trained gradually on more number of consecutive frames.This\nstep-by-step learning process helps stabilize the training and enables the\ncreation of high-resolution video based on conditional text descriptions.\nQualitative and quantitative experimental results on various datasets\ndemonstrate the effectiveness of the proposed method.",
          "link": "http://arxiv.org/abs/2009.02018",
          "publishedOn": "2021-06-29T01:55:15.417Z",
          "wordCount": 628,
          "title": "TiVGAN: Text to Image to Video Generation with Step-by-Step Evolutionary Generator. (arXiv:2009.02018v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2001.05846",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongxin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huatian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jiannan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1\">Cheng Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1\">Jigen Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yue_S/0/1/0/all/0/1\">Shigang Yue</a>",
          "description": "Discriminating small moving objects within complex visual environments is a\nsignificant challenge for autonomous micro robots that are generally limited in\ncomputational power. By exploiting their highly evolved visual systems, flying\ninsects can effectively detect mates and track prey during rapid pursuits, even\nthough the small targets equate to only a few pixels in their visual field. The\nhigh degree of sensitivity to small target movement is supported by a class of\nspecialized neurons called small target motion detectors (STMDs). Existing\nSTMD-based computational models normally comprise four sequentially arranged\nneural layers interconnected via feedforward loops to extract information on\nsmall target motion from raw visual inputs. However, feedback, another\nimportant regulatory circuit for motion perception, has not been investigated\nin the STMD pathway and its functional roles for small target motion detection\nare not clear. In this paper, we propose an STMD-based neural network with\nfeedback connection (Feedback STMD), where the network output is temporally\ndelayed, then fed back to the lower layers to mediate neural responses. We\ncompare the properties of the model with and without the time-delay feedback\nloop, and find it shows preference for high-velocity objects. Extensive\nexperiments suggest that the Feedback STMD achieves superior detection\nperformance for fast-moving small targets, while significantly suppressing\nbackground false positive movements which display lower velocities. The\nproposed feedback model provides an effective solution in robotic visual\nsystems for detecting fast-moving small targets that are always salient and\npotentially threatening.",
          "link": "http://arxiv.org/abs/2001.05846",
          "publishedOn": "2021-06-29T01:55:15.002Z",
          "wordCount": 753,
          "title": "A Time-Delay Feedback Neural Network for Discriminating Small, Fast-Moving Targets in Complex Dynamic Environments. (arXiv:2001.05846v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.04216",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yin_W/0/1/0/all/0/1\">Wei Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yifan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chunhua Shen</a>",
          "description": "Monocular depth prediction plays a crucial role in understanding 3D scene\ngeometry. Although recent methods have achieved impressive progress in terms of\nevaluation metrics such as the pixel-wise relative error, most methods neglect\nthe geometric constraints in the 3D space. In this work, we show the importance\nof the high-order 3D geometric constraints for depth prediction. By designing a\nloss term that enforces a simple geometric constraint, namely, virtual normal\ndirections determined by randomly sampled three points in the reconstructed 3D\nspace, we significantly improve the accuracy and robustness of monocular depth\nestimation. Significantly, the virtual normal loss can not only improve the\nperformance of learning metric depth, but also disentangle the scale\ninformation and enrich the model with better shape information. Therefore, when\nnot having access to absolute metric depth training data, we can use virtual\nnormal to learn a robust affine-invariant depth generated on diverse scenes. In\nexperiments, We show state-of-the-art results of learning metric depth on NYU\nDepth-V2 and KITTI. From the high-quality predicted depth, we are now able to\nrecover good 3D structures of the scene such as the point cloud and surface\nnormal directly, eliminating the necessity of relying on additional models as\nwas previously done. To demonstrate the excellent generalizability of learning\naffine-invariant depth on diverse data with the virtual normal loss, we\nconstruct a large-scale and diverse dataset for training affine-invariant\ndepth, termed Diverse Scene Depth dataset (DiverseDepth), and test on five\ndatasets with the zero-shot test setting. Code is available at:\nhttps://git.io/Depth",
          "link": "http://arxiv.org/abs/2103.04216",
          "publishedOn": "2021-06-29T01:55:14.996Z",
          "wordCount": 774,
          "title": "Virtual Normal: Enforcing Geometric Constraints for Accurate and Robust Depth Prediction. (arXiv:2103.04216v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14087",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nobis_F/0/1/0/all/0/1\">Felix Nobis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shafiei_E/0/1/0/all/0/1\">Ehsan Shafiei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karle_P/0/1/0/all/0/1\">Phillip Karle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Betz_J/0/1/0/all/0/1\">Johannes Betz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lienkamp_M/0/1/0/all/0/1\">Markus Lienkamp</a>",
          "description": "Automotive traffic scenes are complex due to the variety of possible\nscenarios, objects, and weather conditions that need to be handled. In contrast\nto more constrained environments, such as automated underground trains,\nautomotive perception systems cannot be tailored to a narrow field of specific\ntasks but must handle an ever-changing environment with unforeseen events. As\ncurrently no single sensor is able to reliably perceive all relevant activity\nin the surroundings, sensor data fusion is applied to perceive as much\ninformation as possible. Data fusion of different sensors and sensor modalities\non a low abstraction level enables the compensation of sensor weaknesses and\nmisdetections among the sensors before the information-rich sensor data are\ncompressed and thereby information is lost after a sensor-individual object\ndetection. This paper develops a low-level sensor fusion network for 3D object\ndetection, which fuses lidar, camera, and radar data. The fusion network is\ntrained and evaluated on the nuScenes data set. On the test set, fusion of\nradar data increases the resulting AP (Average Precision) detection score by\nabout 5.1% in comparison to the baseline lidar network. The radar sensor fusion\nproves especially beneficial in inclement conditions such as rain and night\nscenes. Fusing additional camera data contributes positively only in\nconjunction with the radar fusion, which shows that interdependencies of the\nsensors are important for the detection result. Additionally, the paper\nproposes a novel loss to handle the discontinuity of a simple yaw\nrepresentation for object detection. Our updated loss increases the detection\nand orientation estimation performance for all sensor input configurations. The\ncode for this research has been made available on GitHub.",
          "link": "http://arxiv.org/abs/2106.14087",
          "publishedOn": "2021-06-29T01:55:14.991Z",
          "wordCount": 705,
          "title": "Radar Voxel Fusion for 3D Object Detection. (arXiv:2106.14087v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14269",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Shuo Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jianxi Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jie Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Magee_C/0/1/0/all/0/1\">Christopher L. Magee</a>",
          "description": "In large technology companies, the requirements for managing and organizing\ntechnical documents created by engineers and managers in supporting relevant\ndecision making have increased dramatically in recent years, which has led to a\nhigher demand for more scalable, accurate, and automated document\nclassification. Prior studies have primarily focused on processing text for\nclassification and small-scale databases. This paper describes a novel\nmultimodal deep learning architecture, called TechDoc, for technical document\nclassification, which utilizes both natural language and descriptive images to\ntrain hierarchical classifiers. The architecture synthesizes convolutional\nneural networks and recurrent neural networks through an integrated training\nprocess. We applied the architecture to a large multimodal technical document\ndatabase and trained the model for classifying documents based on the\nhierarchical International Patent Classification system. Our results show that\nthe trained neural network presents a greater classification accuracy than\nthose using a single modality and several earlier text classification methods.\nThe trained model can potentially be scaled to millions of real-world technical\ndocuments with both text and figures, which is useful for data and knowledge\nmanagement in large technology companies and organizations.",
          "link": "http://arxiv.org/abs/2106.14269",
          "publishedOn": "2021-06-29T01:55:14.984Z",
          "wordCount": 625,
          "title": "Deep Learning for Technical Document Classification. (arXiv:2106.14269v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14385",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1\">Yuanfeng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruimao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huijie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Lingyun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shaoting Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1\">Ping Luo</a>",
          "description": "The recent vision transformer(i.e.for image classification) learns non-local\nattentive interaction of different patch tokens. However, prior arts miss\nlearning the cross-scale dependencies of different pixels, the semantic\ncorrespondence of different labels, and the consistency of the feature\nrepresentations and semantic embeddings, which are critical for biomedical\nsegmentation. In this paper, we tackle the above issues by proposing a unified\ntransformer network, termed Multi-Compound Transformer (MCTrans), which\nincorporates rich feature learning and semantic structure mining into a unified\nframework. Specifically, MCTrans embeds the multi-scale convolutional features\nas a sequence of tokens and performs intra- and inter-scale self-attention,\nrather than single-scale attention in previous works. In addition, a learnable\nproxy embedding is also introduced to model semantic relationship and feature\nenhancement by using self-attention and cross-attention, respectively. MCTrans\ncan be easily plugged into a UNet-like network and attains a significant\nimprovement over the state-of-the-art methods in biomedical image segmentation\nin six standard benchmarks. For example, MCTrans outperforms UNet by 3.64%,\n3.71%, 4.34%, 2.8%, 1.88%, 1.57% in Pannuke, CVC-Clinic, CVC-Colon, Etis,\nKavirs, ISIC2018 dataset, respectively. Code is available at\nhttps://github.com/JiYuanFeng/MCTrans.",
          "link": "http://arxiv.org/abs/2106.14385",
          "publishedOn": "2021-06-29T01:55:14.978Z",
          "wordCount": 623,
          "title": "Multi-Compound Transformer for Accurate Biomedical Image Segmentation. (arXiv:2106.14385v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14124",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Junyang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1\">Changxing Ding</a>",
          "description": "The past few years have witnessed great progress in the domain of face\nrecognition thanks to advances in deep learning. However, cross pose face\nrecognition remains a significant challenge. It is difficult for many deep\nlearning algorithms to narrow the performance gap caused by pose variations;\nthe main reasons for this relate to the intra-class discrepancy between face\nimages in different poses and the pose imbalances of training datasets.\nLearning pose-robust features by traversing to the feature space of frontal\nfaces provides an effective and cheap way to alleviate this problem. In this\npaper, we present a method for progressively transforming profile face\nrepresentations to the canonical pose with an attentive pair-wise loss.\nFirstly, to reduce the difficulty of directly transforming the profile face\nfeatures into a frontal pose, we propose to learn the feature residual between\nthe source pose and its nearby pose in a block-byblock fashion, and thus\ntraversing to the feature space of a smaller pose by adding the learned\nresidual. Secondly, we propose an attentive pair-wise loss to guide the feature\ntransformation progressing in the most effective direction. Finally, our\nproposed progressive module and attentive pair-wise loss are light-weight and\neasy to implement, adding only about 7:5% extra parameters. Evaluations on the\nCFP and CPLFW datasets demonstrate the superiority of our proposed method. Code\nis available at https://github.com/hjy1312/AGPM.",
          "link": "http://arxiv.org/abs/2106.14124",
          "publishedOn": "2021-06-29T01:55:14.970Z",
          "wordCount": 662,
          "title": "Attention-guided Progressive Mapping for Profile Face Recognition. (arXiv:2106.14124v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14467",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Sheng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1\">Xi Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Dan Yang</a>",
          "description": "Data augmentation is a powerful technique for improving the performance of\nthe few-shot classification task. It generates more samples as supplements, and\nthen this task can be transformed into a common supervised learning issue for\nsolution. However, most mainstream data augmentation based approaches only\nconsider the single modality information, which leads to the low diversity and\nquality of generated features. In this paper, we present a novel multi-modal\ndata augmentation approach named Dizygotic Conditional Variational AutoEncoder\n(DCVAE) for addressing the aforementioned issue. DCVAE conducts feature\nsynthesis via pairing two Conditional Variational AutoEncoders (CVAEs) with the\nsame seed but different modality conditions in a dizygotic symbiosis manner.\nSubsequently, the generated features of two CVAEs are adaptively combined to\nyield the final feature, which can be converted back into its paired conditions\nwhile ensuring these conditions are consistent with the original conditions not\nonly in representation but also in function. DCVAE essentially provides a new\nidea of data augmentation in various multi-modal scenarios by exploiting the\ncomplement of different modality prior information. Extensive experimental\nresults demonstrate our work achieves state-of-the-art performances on\nminiImageNet, CIFAR-FS and CUB datasets, and is able to work well in the\npartial modality absence case.",
          "link": "http://arxiv.org/abs/2106.14467",
          "publishedOn": "2021-06-29T01:55:14.955Z",
          "wordCount": 647,
          "title": "Dizygotic Conditional Variational AutoEncoder for Multi-Modal and Partial Modality Absent Few-Shot Learning. (arXiv:2106.14467v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2012.04830",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaoqing Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hu_Y/0/1/0/all/0/1\">Yan Hu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fang_J/0/1/0/all/0/1\">Jiansheng Fang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xiao_Z/0/1/0/all/0/1\">Zunjie Xiao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Higashita_R/0/1/0/all/0/1\">Risa Higashita</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1\">Jiang Liu</a>",
          "description": "Cataract is one of the leading causes of reversible visual impairment and\nblindness globally. Over the years, researchers have achieved significant\nprogress in developing state-of-the-art artificial intelligence techniques for\nautomatic cataract classification and grading, helping clinicians prevent and\ntreat cataract in time. This paper provides a comprehensive survey of recent\nadvances in machine learning for cataract classification and grading based on\nophthalmic images. We summarize existing literature from two research\ndirections: conventional machine learning techniques and deep learning\ntechniques. This paper also provides insights into existing works of both\nmerits and limitations. In addition, we discuss several challenges of automatic\ncataract classification and grading based on machine learning techniques and\npresent possible solutions to these challenges for future research.",
          "link": "http://arxiv.org/abs/2012.04830",
          "publishedOn": "2021-06-29T01:55:14.946Z",
          "wordCount": 612,
          "title": "Machine Learning for Cataract Classification and Grading on Ophthalmic Imaging Modalities: A Survey. (arXiv:2012.04830v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14366",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zihao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Shaozuo Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Siwei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1\">Bingchen Zhao</a>",
          "description": "This paper presents the Rail-5k dataset for benchmarking the performance of\nvisual algorithms in a real-world application scenario, namely the rail surface\ndefects detection task. We collected over 5k high-quality images from railways\nacross China, and annotated 1100 images with the help from railway experts to\nidentify the most common 13 types of rail defects. The dataset can be used for\ntwo settings both with unique challenges, the first is the fully-supervised\nsetting using the 1k+ labeled images for training, fine-grained nature and\nlong-tailed distribution of defect classes makes it hard for visual algorithms\nto tackle. The second is the semi-supervised learning setting facilitated by\nthe 4k unlabeled images, these 4k images are uncurated containing possible\nimage corruptions and domain shift with the labeled images, which can not be\neasily tackle by previous semi-supervised learning methods. We believe our\ndataset could be a valuable benchmark for evaluating robustness and reliability\nof visual algorithms.",
          "link": "http://arxiv.org/abs/2106.14366",
          "publishedOn": "2021-06-29T01:55:14.938Z",
          "wordCount": 593,
          "title": "Rail-5k: a Real-World Dataset for Rail Surface Defects Detection. (arXiv:2106.14366v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14413",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cha_H/0/1/0/all/0/1\">Hyuntak Cha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jaeho Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1\">Jinwoo Shin</a>",
          "description": "Recent breakthroughs in self-supervised learning show that such algorithms\nlearn visual representations that can be transferred better to unseen tasks\nthan joint-training methods relying on task-specific supervision. In this\npaper, we found that the similar holds in the continual learning con-text:\ncontrastively learned representations are more robust against the catastrophic\nforgetting than jointly trained representations. Based on this novel\nobservation, we propose a rehearsal-based continual learning algorithm that\nfocuses on continually learning and maintaining transferable representations.\nMore specifically, the proposed scheme (1) learns representations using the\ncontrastive learning objective, and (2) preserves learned representations using\na self-supervised distillation step. We conduct extensive experimental\nvalidations under popular benchmark image classification datasets, where our\nmethod sets the new state-of-the-art performance.",
          "link": "http://arxiv.org/abs/2106.14413",
          "publishedOn": "2021-06-29T01:55:14.925Z",
          "wordCount": 550,
          "title": "Co$^2$L: Contrastive Continual Learning. (arXiv:2106.14413v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14292",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Jain_R/0/1/0/all/0/1\">Rohit Kumar Jain</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sharma_P/0/1/0/all/0/1\">Prasen Kumar Sharma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gaj_S/0/1/0/all/0/1\">Sibaji Gaj</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sur_A/0/1/0/all/0/1\">Arijit Sur</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ghosh_P/0/1/0/all/0/1\">Palash Ghosh</a>",
          "description": "Knee Osteoarthritis (OA) is a destructive joint disease identified by joint\nstiffness, pain, and functional disability concerning millions of lives across\nthe globe. It is generally assessed by evaluating physical symptoms, medical\nhistory, and other joint screening tests like radiographs, Magnetic Resonance\nImaging (MRI), and Computed Tomography (CT) scans. Unfortunately, the\nconventional methods are very subjective, which forms a barrier in detecting\nthe disease progression at an early stage. This paper presents a deep\nlearning-based framework, namely OsteoHRNet, that automatically assesses the\nKnee OA severity in terms of Kellgren and Lawrence (KL) grade classification\nfrom X-rays. As a primary novelty, the proposed approach is built upon one of\nthe most recent deep models, called the High-Resolution Network (HRNet), to\ncapture the multi-scale features of knee X-rays. In addition, we have also\nincorporated an attention mechanism to filter out the counterproductive\nfeatures and boost the performance further. Our proposed model has achieved the\nbest multiclass accuracy of 71.74% and MAE of 0.311 on the baseline cohort of\nthe OAI dataset, which is a remarkable gain over the existing best-published\nworks. We have also employed the Gradient-based Class Activation Maps\n(Grad-CAMs) visualization to justify the proposed network learning.",
          "link": "http://arxiv.org/abs/2106.14292",
          "publishedOn": "2021-06-29T01:55:14.920Z",
          "wordCount": 680,
          "title": "Knee Osteoarthritis Severity Prediction using an Attentive Multi-Scale Deep Convolutional Neural Network. (arXiv:2106.14292v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2104.04945",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Szandala_T/0/1/0/all/0/1\">Tomasz Szandala</a>",
          "description": "In this paper, an enhancement technique for the class activation mapping\nmethods such as gradient-weighted class activation maps or excitation\nbackpropagation is proposed to present the visual explanations of decisions\nfrom convolutional neural network-based models. The proposed idea, called\nGradual Extrapolation, can supplement any method that generates a heatmap\npicture by sharpening the output. Instead of producing a coarse localization\nmap that highlights the important predictive regions in the image, the proposed\nmethod outputs the specific shape that most contributes to the model output.\nThus, the proposed method improves the accuracy of saliency maps. The effect\nhas been achieved by the gradual propagation of the crude map obtained in the\ndeep layer through all preceding layers with respect to their activations. In\nvalidation tests conducted on a selected set of images, the faithfulness,\ninterpretability, and applicability of the method are evaluated. The proposed\ntechnique significantly improves the localization detection of the neural\nnetworks attention at low additional computational costs. Furthermore, the\nproposed method is applicable to a variety deep neural network models. The code\nfor the method can be found at\nhttps://github.com/szandala/gradual-extrapolation",
          "link": "http://arxiv.org/abs/2104.04945",
          "publishedOn": "2021-06-29T01:55:14.914Z",
          "wordCount": 653,
          "title": "Enhancing Deep Neural Network Saliency Visualizations with Gradual Extrapolation. (arXiv:2104.04945v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.08239",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Liu_Z/0/1/0/all/0/1\">Zixuan Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Adeli_E/0/1/0/all/0/1\">Ehsan Adeli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pohl_K/0/1/0/all/0/1\">Kilian M. Pohl</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_Q/0/1/0/all/0/1\">Qingyu Zhao</a>",
          "description": "Interpretability is a critical factor in applying complex deep learning\nmodels to advance the understanding of brain disorders in neuroimaging studies.\nTo interpret the decision process of a trained classifier, existing techniques\ntypically rely on saliency maps to quantify the voxel-wise or feature-level\nimportance for classification through partial derivatives. Despite providing\nsome level of localization, these maps are not human-understandable from the\nneuroscience perspective as they do not inform the specific meaning of the\nalteration linked to the brain disorder. Inspired by the image-to-image\ntranslation scheme, we propose to train simulator networks that can warp a\ngiven image to inject or remove patterns of the disease. These networks are\ntrained such that the classifier produces consistently increased or decreased\nprediction logits for the simulated images. Moreover, we propose to couple all\nthe simulators into a unified model based on conditional convolution. We\napplied our approach to interpreting classifiers trained on a synthetic dataset\nand two neuroimaging datasets to visualize the effect of the Alzheimer's\ndisease and alcohol use disorder. Compared to the saliency maps generated by\nbaseline approaches, our simulations and visualizations based on the Jacobian\ndeterminants of the warping field reveal meaningful and understandable patterns\nrelated to the diseases.",
          "link": "http://arxiv.org/abs/2102.08239",
          "publishedOn": "2021-06-29T01:55:14.908Z",
          "wordCount": 673,
          "title": "Going Beyond Saliency Maps: Training Deep Models to Interpret Deep Models. (arXiv:2102.08239v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14184",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Venkatesh_P/0/1/0/all/0/1\">Praveen Venkatesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rana_R/0/1/0/all/0/1\">Rwik Rana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_V/0/1/0/all/0/1\">Varun Jain</a>",
          "description": "In self driving car applications, there is a requirement to predict the\nlocation of the lane given an input RGB front facing image. In this paper, we\npropose an architecture that allows us to increase the speed and robustness of\nroad detection without a large hit in accuracy by introducing an underlying\nshared feature space that is propagated over time, which serves as a flowing\ndynamic memory. By utilizing the gist of previous frames, we train the network\nto predict the current road with a greater accuracy and lesser deviation from\nprevious frames.",
          "link": "http://arxiv.org/abs/2106.14184",
          "publishedOn": "2021-06-29T01:55:14.902Z",
          "wordCount": 518,
          "title": "Memory Guided Road Detection. (arXiv:2106.14184v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/1906.06013",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1\">Chunhua Shen</a>",
          "description": "Text spotting in natural scene images is of great importance for many image\nunderstanding tasks. It includes two sub-tasks: text detection and recognition.\nIn this work, we propose a unified network that simultaneously localizes and\nrecognizes text with a single forward pass, avoiding intermediate processes\nsuch as image cropping and feature re-calculation, word separation, and\ncharacter grouping.\n\nIn contrast to existing approaches that consider text detection and\nrecognition as two distinct tasks and tackle them one by one, the proposed\nframework settles these two tasks concurrently. The whole framework can be\ntrained end-to-end and is able to handle text of arbitrary shapes. The\nconvolutional features are calculated only once and shared by both detection\nand recognition modules. Through multi-task training, the learned features\nbecome more discriminate and improve the overall performance. By employing the\n$2$D attention model in word recognition, the irregularity of text can be\nrobustly addressed. It provides the spatial location for each character, which\nnot only helps local feature extraction in word recognition, but also indicates\nan orientation angle to refine text localization. Our proposed method has\nachieved state-of-the-art performance on several standard text spotting\nbenchmarks, including both regular and irregular ones.",
          "link": "http://arxiv.org/abs/1906.06013",
          "publishedOn": "2021-06-29T01:55:14.897Z",
          "wordCount": 703,
          "title": "Towards End-to-End Text Spotting in Natural Scenes. (arXiv:1906.06013v6 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14178",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wang_Q/0/1/0/all/0/1\">Quanziang Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_R/0/1/0/all/0/1\">Renzhen Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1\">Yuexiang Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ma_K/0/1/0/all/0/1\">Kai Ma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zheng_Y/0/1/0/all/0/1\">Yefeng Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Meng_D/0/1/0/all/0/1\">Deyu Meng</a>",
          "description": "Location information is proven to benefit the deep learning models on\ncapturing the manifold structure of target objects, and accordingly boosts the\naccuracy of medical image segmentation. However, most existing methods encode\nthe location information in an implicit way, e.g. the distance transform maps,\nwhich describe the relative distance from each pixel to the contour boundary,\nfor the network to learn. These implicit approaches do not fully exploit the\nposition information (i.e. absolute location) of targets. In this paper, we\npropose a novel loss function, namely residual moment (RM) loss, to explicitly\nembed the location information of segmentation targets during the training of\ndeep learning networks. Particularly, motivated by image moments, the\nsegmentation prediction map and ground-truth map are weighted by coordinate\ninformation. Then our RM loss encourages the networks to maintain the\nconsistency between the two weighted maps, which promotes the segmentation\nnetworks to easily locate the targets and extract manifold-structure-related\nfeatures. We validate the proposed RM loss by conducting extensive experiments\non two publicly available datasets, i.e., 2D optic cup and disk segmentation\nand 3D left atrial segmentation. The experimental results demonstrate the\neffectiveness of our RM loss, which significantly boosts the accuracy of\nsegmentation networks.",
          "link": "http://arxiv.org/abs/2106.14178",
          "publishedOn": "2021-06-29T01:55:14.887Z",
          "wordCount": 648,
          "title": "Residual Moment Loss for Medical Image Segmentation. (arXiv:2106.14178v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14208",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ahishali_M/0/1/0/all/0/1\">Mete Ahishali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamac_M/0/1/0/all/0/1\">Mehmet Yamac</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiranyaz_S/0/1/0/all/0/1\">Serkan Kiranyaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gabbouj_M/0/1/0/all/0/1\">Moncef Gabbouj</a>",
          "description": "In this study, we propose a novel approach to predict the distances of the\ndetected objects in an observed scene. The proposed approach modifies the\nrecently proposed Convolutional Support Estimator Networks (CSENs). CSENs are\ndesigned to compute a direct mapping for the Support Estimation (SE) task in a\nrepresentation-based classification problem. We further propose and demonstrate\nthat representation-based methods (sparse or collaborative representation) can\nbe used in well-designed regression problems. To the best of our knowledge,\nthis is the first representation-based method proposed for performing a\nregression task by utilizing the modified CSENs; and hence, we name this novel\napproach as Representation-based Regression (RbR). The initial version of CSENs\nhas a proxy mapping stage (i.e., a coarse estimation for the support set) that\nis required for the input. In this study, we improve the CSEN model by\nproposing Compressive Learning CSEN (CL-CSEN) that has the ability to jointly\noptimize the so-called proxy mapping stage along with convolutional layers. The\nexperimental evaluations using the KITTI 3D Object Detection distance\nestimation dataset show that the proposed method can achieve a significantly\nimproved distance estimation performance over all competing methods. Finally,\nthe software implementations of the methods are publicly shared at\nhttps://github.com/meteahishali/CSENDistance.",
          "link": "http://arxiv.org/abs/2106.14208",
          "publishedOn": "2021-06-29T01:55:14.881Z",
          "wordCount": 642,
          "title": "Representation Based Regression for Object Distance Estimation. (arXiv:2106.14208v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14102",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kalgaonkar_P/0/1/0/all/0/1\">Priyank Kalgaonkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+El_Sharkawy_M/0/1/0/all/0/1\">Mohamed El-Sharkawy</a>",
          "description": "In this paper, we demonstrate the implementation of our ultra-efficient deep\nconvolutional neural network architecture: CondenseNeXt on NXP BlueBox, an\nautonomous driving development platform developed for self-driving vehicles. We\nshow that CondenseNeXt is remarkably efficient in terms of FLOPs, designed for\nARM-based embedded computing platforms with limited computational resources and\ncan perform image classification without the need of a CUDA enabled GPU.\nCondenseNeXt utilizes the state-of-the-art depthwise separable convolution and\nmodel compression techniques to achieve a remarkable computational efficiency.\nExtensive analyses are conducted on CIFAR-10, CIFAR-100 and ImageNet datasets\nto verify the performance of CondenseNeXt Convolutional Neural Network (CNN)\narchitecture. It achieves state-of-the-art image classification performance on\nthree benchmark datasets including CIFAR-10 (4.79% top-1 error), CIFAR-100\n(21.98% top-1 error) and ImageNet (7.91% single model, single crop top-5\nerror). CondenseNeXt achieves final trained model size improvement of 2.9+ MB\nand up to 59.98% reduction in forward FLOPs compared to CondenseNet and can\nperform image classification on ARM-Based computing platforms without needing a\nCUDA enabled GPU support, with outstanding efficiency.",
          "link": "http://arxiv.org/abs/2106.14102",
          "publishedOn": "2021-06-29T01:55:14.875Z",
          "wordCount": 620,
          "title": "Image Classification with CondenseNeXt for ARM-Based Computing Platforms. (arXiv:2106.14102v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14256",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Liu_B/0/1/0/all/0/1\">Boing Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yinxi Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Weitz_P/0/1/0/all/0/1\">Philippe Weitz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lindberg_J/0/1/0/all/0/1\">Johan Lindberg</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Egevad_L/0/1/0/all/0/1\">Lars Egevad</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gronberg_H/0/1/0/all/0/1\">Henrik Gr&#xf6;nberg</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Eklund_M/0/1/0/all/0/1\">Martin Eklund</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rantalainen_M/0/1/0/all/0/1\">Mattias Rantalainen</a>",
          "description": "Background: Transrectal ultrasound guided systematic biopsies of the prostate\nis a routine procedure to establish a prostate cancer diagnosis. However, the\n10-12 prostate core biopsies only sample a relatively small volume of the\nprostate, and tumour lesions in regions between biopsy cores can be missed,\nleading to a well-known low sensitivity to detect clinically relevant cancer.\nAs a proof-of-principle, we developed and validated a deep convolutional neural\nnetwork model to distinguish between morphological patterns in benign prostate\nbiopsy whole slide images from men with and without established cancer.\nMethods: This study included 14,354 hematoxylin and eosin stained whole slide\nimages from benign prostate biopsies from 1,508 men in two groups: men without\nan established prostate cancer (PCa) diagnosis and men with at least one core\nbiopsy diagnosed with PCa. 80% of the participants were assigned as training\ndata and used for model optimization (1,211 men), and the remaining 20% (297\nmen) as a held-out test set used to evaluate model performance. An ensemble of\n10 deep convolutional neural network models was optimized for classification of\nbiopsies from men with and without established cancer. Hyperparameter\noptimization and model selection was performed by cross-validation in the\ntraining data . Results: Area under the receiver operating characteristic curve\n(ROC-AUC) was estimated as 0.727 (bootstrap 95% CI: 0.708-0.745) on biopsy\nlevel and 0.738 (bootstrap 95% CI: 0.682 - 0.796) on man level. At a\nspecificity of 0.9 the model had an estimated sensitivity of 0.348. Conclusion:\nThe developed model has the ability to detect men with risk of missed PCa due\nto under-sampling of the prostate. The proposed model has the potential to\nreduce the number of false negative cases in routine systematic prostate\nbiopsies and to indicate men who could benefit from MRI-guided re-biopsy.",
          "link": "http://arxiv.org/abs/2106.14256",
          "publishedOn": "2021-06-29T01:55:14.832Z",
          "wordCount": 764,
          "title": "Using deep learning to detect patients at risk for prostate cancer despite benign biopsies. (arXiv:2106.14256v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14127",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ge_S/0/1/0/all/0/1\">Songwei Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parikh_D/0/1/0/all/0/1\">Devi Parikh</a>",
          "description": "We ask the question: to what extent can recent large-scale language and image\ngeneration models blend visual concepts? Given an arbitrary object, we identify\na relevant object and generate a single-sentence description of the blend of\nthe two using a language model. We then generate a visual depiction of the\nblend using a text-based image generation model. Quantitative and qualitative\nevaluations demonstrate the superiority of language models over classical\nmethods for conceptual blending, and of recent large-scale image generation\nmodels over prior models for the visual depiction.",
          "link": "http://arxiv.org/abs/2106.14127",
          "publishedOn": "2021-06-29T01:55:14.822Z",
          "wordCount": 527,
          "title": "Visual Conceptual Blending with Large-scale Language and Vision Models. (arXiv:2106.14127v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14349",
          "author": "<a href=\"http://arxiv.org/find/astro-ph/1/au:+Jia_P/0/1/0/all/0/1\">Peng Jia</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Sun_Y/0/1/0/all/0/1\">Yongyang Sun</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Liu_Q/0/1/0/all/0/1\">Qiang Liu</a>",
          "description": "Wide field small aperture telescopes (WFSATs) are mainly used to obtain\nscientific information of point--like and streak--like celestial objects.\nHowever, qualities of images obtained by WFSATs are seriously affected by the\nbackground noise and variable point spread functions. Developing high speed and\nhigh efficiency data processing method is of great importance for further\nscientific research. In recent years, deep neural networks have been proposed\nfor detection and classification of celestial objects and have shown better\nperformance than classical methods. In this paper, we further extend abilities\nof the deep neural network based astronomical target detection framework to\nmake it suitable for photometry and astrometry. We add new branches into the\ndeep neural network to obtain types, magnitudes and positions of different\ncelestial objects at the same time. Tested with simulated data, we find that\nour neural network has better performance in photometry than classical methods.\nBecause photometry and astrometry are regression algorithms, which would obtain\nhigh accuracy measurements instead of rough classification results, the\naccuracy of photometry and astrometry results would be affected by different\nobservation conditions. To solve this problem, we further propose to use\nreference stars to train our deep neural network with transfer learning\nstrategy when observation conditions change. The photometry framework proposed\nin this paper could be used as an end--to--end quick data processing framework\nfor WFSATs, which can further increase response speed and scientific outputs of\nWFSATs.",
          "link": "http://arxiv.org/abs/2106.14349",
          "publishedOn": "2021-06-29T01:55:14.809Z",
          "wordCount": 715,
          "title": "The Deep Neural Network based Photometry Framework for Wide Field Small Aperture Telescopes. (arXiv:2106.14349v1 [astro-ph.IM])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14104",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fan_Q/0/1/0/all/0/1\">Quanfu Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chun-Fu/0/1/0/all/0/1\">Chun-Fu</a> (Richard) <a href=\"http://arxiv.org/find/cs/1/au:+Chen/0/1/0/all/0/1\">Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panda_R/0/1/0/all/0/1\">Rameswar Panda</a>",
          "description": "We propose a new perspective on video understanding by casting the video\nrecognition problem as an image recognition task. We show that an image\nclassifier alone can suffice for video understanding without temporal modeling.\nOur approach is simple and universal. It composes input frames into a super\nimage to train an image classifier to fulfill the task of action recognition,\nin exactly the same way as classifying an image. We prove the viability of such\nan idea by demonstrating strong and promising performance on four public\ndatasets including Kinetics400, Something-to-something (V2), MiT and Jester,\nusing a recently developed vision transformer. We also experiment with the\nprevalent ResNet image classifiers in computer vision to further validate our\nidea. The results on Kinetics400 are comparable to some of the best-performed\nCNN approaches based on spatio-temporal modeling. our code and models will be\nmade available at https://github.com/IBM/sifar-pytorch.",
          "link": "http://arxiv.org/abs/2106.14104",
          "publishedOn": "2021-06-29T01:55:14.793Z",
          "wordCount": 578,
          "title": "An Image Classifier Can Suffice Video Understanding. (arXiv:2106.14104v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14274",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhiqin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tagliasacchi_A/0/1/0/all/0/1\">Andrea Tagliasacchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>",
          "description": "Polygonal meshes are ubiquitous, but have only played a relatively minor role\nin the deep learning revolution. State-of-the-art neural generative models for\n3D shapes learn implicit functions and generate meshes via expensive\niso-surfacing. We overcome these challenges by employing a classical spatial\ndata structure from computer graphics, Binary Space Partitioning (BSP), to\nfacilitate 3D learning. The core operation of BSP involves recursive\nsubdivision of 3D space to obtain convex sets. By exploiting this property, we\ndevise BSP-Net, a network that learns to represent a 3D shape via convex\ndecomposition without supervision. The network is trained to reconstruct a\nshape using a set of convexes obtained from a BSP-tree built over a set of\nplanes, where the planes and convexes are both defined by learned network\nweights. BSP-Net directly outputs polygonal meshes from the inferred convexes.\nThe generated meshes are watertight, compact (i.e., low-poly), and well suited\nto represent sharp geometry. We show that the reconstruction quality by BSP-Net\nis competitive with those from state-of-the-art methods while using much fewer\nprimitives. We also explore variations to BSP-Net including using a more\ngeneric decoder for reconstruction, more general primitives than planes, as\nwell as training a generative model with variational auto-encoders. Code is\navailable at https://github.com/czq142857/BSP-NET-original.",
          "link": "http://arxiv.org/abs/2106.14274",
          "publishedOn": "2021-06-29T01:55:14.777Z",
          "wordCount": 663,
          "title": "Learning Mesh Representations via Binary Space Partitioning Tree Networks. (arXiv:2106.14274v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14440",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1\">Ruihai Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yan Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mo_K/0/1/0/all/0/1\">Kaichun Mo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Zizheng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yian Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tianhao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_Q/0/1/0/all/0/1\">Qingnan Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xuelin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guibas_L/0/1/0/all/0/1\">Leonidas Guibas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1\">Hao Dong</a>",
          "description": "Perceiving and manipulating 3D articulated objects (e.g., cabinets, doors) in\nhuman environments is an important yet challenging task for future\nhome-assistant robots. The space of 3D articulated objects is exceptionally\nrich in their myriad semantic categories, diverse shape geometry, and\ncomplicated part functionality. Previous works mostly abstract kinematic\nstructure with estimated joint parameters and part poses as the visual\nrepresentations for manipulating 3D articulated objects. In this paper, we\npropose object-centric actionable visual priors as a novel\nperception-interaction handshaking point that the perception system outputs\nmore actionable guidance than kinematic structure estimation, by predicting\ndense geometry-aware, interaction-aware, and task-aware visual action\naffordance and trajectory proposals. We design an interaction-for-perception\nframework VAT-Mart to learn such actionable visual representations by\nsimultaneously training a curiosity-driven reinforcement learning policy\nexploring diverse interaction trajectories and a perception module summarizing\nand generalizing the explored knowledge for pointwise predictions among diverse\nshapes. Experiments prove the effectiveness of the proposed approach using the\nlarge-scale PartNet-Mobility dataset in SAPIEN environment and show promising\ngeneralization capabilities to novel test shapes, unseen object categories, and\nreal-world data. Project page: https://hyperplane-lab.github.io/vat-mart",
          "link": "http://arxiv.org/abs/2106.14440",
          "publishedOn": "2021-06-29T01:55:14.751Z",
          "wordCount": 638,
          "title": "VAT-Mart: Learning Visual Action Trajectory Proposals for Manipulating 3D ARTiculated Objects. (arXiv:2106.14440v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14101",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Murhij_Y/0/1/0/all/0/1\">Youshaa Murhij</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yudin_D/0/1/0/all/0/1\">Dmitry Yudin</a>",
          "description": "In this paper, we present a real-time 3D detection approach considering\ntime-spatial feature map aggregation from different time steps of deep neural\nmodel inference (named feature map flow, FMF). Proposed approach improves the\nquality of 3D detection center-based baseline and provides real-time\nperformance on the nuScenes and Waymo benchmark. Code is available at\nhttps://github.com/YoushaaMurhij/FMFNet",
          "link": "http://arxiv.org/abs/2106.14101",
          "publishedOn": "2021-06-29T01:55:14.743Z",
          "wordCount": 502,
          "title": "Real-time 3D Object Detection using Feature Map Flow. (arXiv:2106.14101v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14447",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_L/0/1/0/all/0/1\">Le Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1\">Zhiyu Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_B/0/1/0/all/0/1\">Bo He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xin_J/0/1/0/all/0/1\">Jingyu Xin</a>",
          "description": "With rapidly evolving internet technologies and emerging tools, sports\nrelated videos generated online are increasing at an unprecedentedly fast pace.\nTo automate sports video editing/highlight generation process, a key task is to\nprecisely recognize and locate the events in the long untrimmed videos. In this\ntech report, we present a two-stage paradigm to detect what and when events\nhappen in soccer broadcast videos. Specifically, we fine-tune multiple action\nrecognition models on soccer data to extract high-level semantic features, and\ndesign a transformer based temporal detection module to locate the target\nevents. This approach achieved the state-of-the-art performance in both two\ntasks, i.e., action spotting and replay grounding, in the SoccerNet-v2\nChallenge, under CVPR 2021 ActivityNet workshop. Our soccer embedding features\nare released at https://github.com/baidu-research/vidpress-sports. By sharing\nthese features with the broader community, we hope to accelerate the research\ninto soccer video understanding.",
          "link": "http://arxiv.org/abs/2106.14447",
          "publishedOn": "2021-06-29T01:55:14.736Z",
          "wordCount": 611,
          "title": "Feature Combination Meets Attention: Baidu Soccer Embeddings and Transformer based Temporal Detection. (arXiv:2106.14447v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14160",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Junru Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1\">Qiao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hang Zhao</a>",
          "description": "In autonomous driving, goal-based multi-trajectory prediction methods are\nproved to be effective recently, where they first score goal candidates, then\nselect a final set of goals, and finally complete trajectories based on the\nselected goals. However, these methods usually involve goal predictions based\non sparse predefined anchors. In this work, we propose an anchor-free model,\nnamed DenseTNT, which performs dense goal probability estimation for trajectory\nprediction. Our model achieves state-of-the-art performance, and ranks 1st on\nthe Waymo Open Dataset Motion Prediction Challenge.",
          "link": "http://arxiv.org/abs/2106.14160",
          "publishedOn": "2021-06-29T01:55:14.730Z",
          "wordCount": 523,
          "title": "DenseTNT: Waymo Open Dataset Motion Prediction Challenge 1st Place Solution. (arXiv:2106.14160v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2003.04641",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1\">Yuhong Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_D/0/1/0/all/0/1\">Di Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1\">Xiaofeng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Naifu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Huaping Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_F/0/1/0/all/0/1\">Fuchun Sun</a>",
          "description": "In this paper, we propose a novel task, Manipulation Question Answering\n(MQA), where the robot performs manipulation actions to change the environment\nin order to answer a given question. To solve this problem, a framework\nconsisting of a QA module and a manipulation module is proposed. For the QA\nmodule, we adopt the method for the Visual Question Answering (VQA) task. For\nthe manipulation module, a Deep Q Network (DQN) model is designed to generate\nmanipulation actions for the robot to interact with the environment. We\nconsider the situation where the robot continuously manipulating objects inside\na bin until the answer to the question is found. Besides, a novel dataset that\ncontains a variety of object models, scenarios and corresponding\nquestion-answer pairs is established in a simulation environment. Extensive\nexperiments have been conducted to validate the effectiveness of the proposed\nframework.",
          "link": "http://arxiv.org/abs/2003.04641",
          "publishedOn": "2021-06-29T01:55:14.712Z",
          "wordCount": 631,
          "title": "MQA: Answering the Question via Robotic Manipulation. (arXiv:2003.04641v3 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1910.06961",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Piergiovanni_A/0/1/0/all/0/1\">AJ Piergiovanni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Angelova_A/0/1/0/all/0/1\">Anelia Angelova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ryoo_M/0/1/0/all/0/1\">Michael S. Ryoo</a>",
          "description": "Video understanding is a challenging problem with great impact on the\nabilities of autonomous agents working in the real-world. Yet, solutions so far\nhave been computationally intensive, with the fastest algorithms running for\nmore than half a second per video snippet on powerful GPUs. We propose a novel\nidea on video architecture learning - Tiny Video Networks - which automatically\ndesigns highly efficient models for video understanding. The tiny video models\nrun with competitive performance for as low as 37 milliseconds per video on a\nCPU and 10 milliseconds on a standard GPU.",
          "link": "http://arxiv.org/abs/1910.06961",
          "publishedOn": "2021-06-29T01:55:14.704Z",
          "wordCount": 548,
          "title": "Tiny Video Networks. (arXiv:1910.06961v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14192",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Yi_K/0/1/0/all/0/1\">Kai Yi</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Pang_J/0/1/0/all/0/1\">Jianye Pang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Zhang_Y/0/1/0/all/0/1\">Yungeng Zhang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Zeng_X/0/1/0/all/0/1\">Xiangrui Zeng</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Xu_M/0/1/0/all/0/1\">Min Xu</a>",
          "description": "Cryo-electron tomography (Cryo-ET) is a 3D imaging technique that enables the\nsystemic study of shape, abundance, and distribution of macromolecular\nstructures in single cells in near-atomic resolution. However, the systematic\nand efficient $\\textit{de novo}$ recognition and recovery of macromolecular\nstructures captured by Cryo-ET are very challenging due to the structural\ncomplexity and imaging limits. Even macromolecules with identical structures\nhave various appearances due to different orientations and imaging limits, such\nas noise and the missing wedge effect. Explicitly disentangling the semantic\nfeatures of macromolecules is crucial for performing several downstream\nanalyses on the macromolecules. This paper has addressed the problem by\nproposing a 3D Spatial Variational Autoencoder that explicitly disentangle the\nstructure, orientation, and shift of macromolecules. Extensive experiments on\nboth synthesized and real cryo-ET datasets and cross-domain evaluations\ndemonstrate the efficacy of our method.",
          "link": "http://arxiv.org/abs/2106.14192",
          "publishedOn": "2021-06-29T01:55:14.697Z",
          "wordCount": 572,
          "title": "Disentangling semantic features of macromolecules in Cryo-Electron Tomography. (arXiv:2106.14192v1 [q-bio.BM])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14386",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yulun Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1\">Yun Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arias_F/0/1/0/all/0/1\">Fernando Herrera Arias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nieto_Granda_C/0/1/0/all/0/1\">Carlos Nieto-Granda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+How_J/0/1/0/all/0/1\">Jonathan P. How</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carlone_L/0/1/0/all/0/1\">Luca Carlone</a>",
          "description": "This paper presents Kimera-Multi, the first multi-robot system that (i) is\nrobust and capable of identifying and rejecting incorrect inter and intra-robot\nloop closures resulting from perceptual aliasing, (ii) is fully distributed and\nonly relies on local (peer-to-peer) communication to achieve distributed\nlocalization and mapping, and (iii) builds a globally consistent\nmetric-semantic 3D mesh model of the environment in real-time, where faces of\nthe mesh are annotated with semantic labels. Kimera-Multi is implemented by a\nteam of robots equipped with visual-inertial sensors. Each robot builds a local\ntrajectory estimate and a local mesh using Kimera. When communication is\navailable, robots initiate a distributed place recognition and robust pose\ngraph optimization protocol based on a novel distributed graduated\nnon-convexity algorithm. The proposed protocol allows the robots to improve\ntheir local trajectory estimates by leveraging inter-robot loop closures while\nbeing robust to outliers. Finally, each robot uses its improved trajectory\nestimate to correct the local mesh using mesh deformation techniques.\n\nWe demonstrate Kimera-Multi in photo-realistic simulations, SLAM benchmarking\ndatasets, and challenging outdoor datasets collected using ground robots. Both\nreal and simulated experiments involve long trajectories (e.g., up to 800\nmeters per robot). The experiments show that Kimera-Multi (i) outperforms the\nstate of the art in terms of robustness and accuracy, (ii) achieves estimation\nerrors comparable to a centralized SLAM system while being fully distributed,\n(iii) is parsimonious in terms of communication bandwidth, (iv) produces\naccurate metric-semantic 3D meshes, and (v) is modular and can be also used for\nstandard 3D reconstruction (i.e., without semantic labels) or for trajectory\nestimation (i.e., without reconstructing a 3D mesh).",
          "link": "http://arxiv.org/abs/2106.14386",
          "publishedOn": "2021-06-29T01:55:14.690Z",
          "wordCount": 715,
          "title": "Kimera-Multi: Robust, Distributed, Dense Metric-Semantic SLAM for Multi-Robot Systems. (arXiv:2106.14386v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14412",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hanbin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xi Li</a>",
          "description": "In this paper, we propose a novel image process scheme called class-based\nexpansion learning for image classification, which aims at improving the\nsupervision-stimulation frequency for the samples of the confusing classes.\nClass-based expansion learning takes a bottom-up growing strategy in a\nclass-based expansion optimization fashion, which pays more attention to the\nquality of learning the fine-grained classification boundaries for the\npreferentially selected classes. Besides, we develop a class confusion\ncriterion to select the confusing class preferentially for training. In this\nway, the classification boundaries of the confusing classes are frequently\nstimulated, resulting in a fine-grained form. Experimental results demonstrate\nthe effectiveness of the proposed scheme on several benchmarks.",
          "link": "http://arxiv.org/abs/2106.14412",
          "publishedOn": "2021-06-29T01:55:14.683Z",
          "wordCount": 549,
          "title": "Progressive Class-based Expansion Learning For Image Classification. (arXiv:2106.14412v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14439",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yuhao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jiake Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1\">Yu Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+and_Y/0/1/0/all/0/1\">Yong Tang and</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xin Yang</a>",
          "description": "Image matting is an ill-posed problem that aims to estimate the opacity of\nforeground pixels in an image. However, most existing deep learning-based\nmethods still suffer from the coarse-grained details. In general, these\nalgorithms are incapable of felicitously distinguishing the degree of\nexploration between deterministic domains (certain FG and BG pixels) and\nundetermined domains (uncertain in-between pixels), or inevitably lose\ninformation in the continuous sampling process, leading to a sub-optimal\nresult. In this paper, we propose a novel network named Prior-Induced\nInformation Alignment Matting Network (PIIAMatting), which can efficiently\nmodel the distinction of pixel-wise response maps and the correlation of\nlayer-wise feature maps. It mainly consists of a Dynamic Gaussian Modulation\nmechanism (DGM) and an Information Alignment strategy (IA). Specifically, the\nDGM can dynamically acquire a pixel-wise domain response map learned from the\nprior distribution. The response map can present the relationship between the\nopacity variation and the convergence process during training. On the other\nhand, the IA comprises an Information Match Module (IMM) and an Information\nAggregation Module (IAM), jointly scheduled to match and aggregate the adjacent\nlayer-wise features adaptively. Besides, we also develop a Multi-Scale\nRefinement (MSR) module to integrate multi-scale receptive field information at\nthe refinement stage to recover the fluctuating appearance details. Extensive\nquantitative and qualitative evaluations demonstrate that the proposed\nPIIAMatting performs favourably against state-of-the-art image matting methods\non the Alphamatting.com, Composition-1K and Distinctions-646 dataset.",
          "link": "http://arxiv.org/abs/2106.14439",
          "publishedOn": "2021-06-29T01:55:14.667Z",
          "wordCount": 671,
          "title": "Prior-Induced Information Alignment for Image Matting. (arXiv:2106.14439v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14290",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Razzhigaev_A/0/1/0/all/0/1\">Anton Razzhigaev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kireev_K/0/1/0/all/0/1\">Klim Kireev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Udovichenko_I/0/1/0/all/0/1\">Igor Udovichenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petiushko_A/0/1/0/all/0/1\">Aleksandr Petiushko</a>",
          "description": "Several methods for inversion of face recognition models were recently\npresented, attempting to reconstruct a face from deep templates. Although some\nof these approaches work in a black-box setup using only face embeddings,\nusually, on the end-user side, only similarity scores are provided. Therefore,\nthese algorithms are inapplicable in such scenarios. We propose a novel\napproach that allows reconstructing the face querying only similarity scores of\nthe black-box model. While our algorithm operates in a more general setup,\nexperiments show that it is query efficient and outperforms the existing\nmethods.",
          "link": "http://arxiv.org/abs/2106.14290",
          "publishedOn": "2021-06-29T01:55:14.659Z",
          "wordCount": 526,
          "title": "Darker than Black-Box: Face Reconstruction from Similarity Queries. (arXiv:2106.14290v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14350",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kovalerchuk_B/0/1/0/all/0/1\">Boris Kovalerchuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalla_D/0/1/0/all/0/1\">Divya Chandrika Kalla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_B/0/1/0/all/0/1\">Bedant Agarwal</a>",
          "description": "Powerful deep learning algorithms open an opportunity for solving non-image\nMachine Learning (ML) problems by transforming these problems to into the image\nrecognition problems. The CPC-R algorithm presented in this chapter converts\nnon-image data into images by visualizing non-image data. Then deep learning\nCNN algorithms solve the learning problems on these images. The design of the\nCPC-R algorithm allows preserving all high-dimensional information in 2-D\nimages. The use of pair values mapping instead of single value mapping used in\nthe alternative approaches allows encoding each n-D point with 2 times fewer\nvisual elements. The attributes of an n-D point are divided into pairs of its\nvalues and each pair is visualized as 2-D points in the same 2-D Cartesian\ncoordinates. Next, grey scale or color intensity values are assigned to each\npair to encode the order of pairs. This is resulted in the heatmap image. The\ncomputational experiments with CPC-R are conducted for different CNN\narchitectures, and methods to optimize the CPC-R images showing that the\ncombined CPC-R and deep learning CNN algorithms are able to solve non-image ML\nproblems reaching high accuracy on the benchmark datasets. This chapter expands\nour prior work by adding more experiments to test accuracy of classification,\nexploring saliency and informativeness of discovered features to test their\ninterpretability, and generalizing the approach.",
          "link": "http://arxiv.org/abs/2106.14350",
          "publishedOn": "2021-06-29T01:55:14.653Z",
          "wordCount": 655,
          "title": "Deep Learning Image Recognition for Non-images. (arXiv:2106.14350v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14465",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Hossain_S/0/1/0/all/0/1\">Sk Imran Hossain</a> (LIMOS), <a href=\"http://arxiv.org/find/eess/1/au:+Herve_J/0/1/0/all/0/1\">Jocelyn de Go&#xeb;r de Herve</a> (INRAE), <a href=\"http://arxiv.org/find/eess/1/au:+Hassan_M/0/1/0/all/0/1\">Md Shahriar Hassan</a> (LIMOS), <a href=\"http://arxiv.org/find/eess/1/au:+Martineau_D/0/1/0/all/0/1\">Delphine Martineau</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Petrosyan_E/0/1/0/all/0/1\">Evelina Petrosyan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Corbain_V/0/1/0/all/0/1\">Violaine Corbain</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Beytout_J/0/1/0/all/0/1\">Jean Beytout</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lebert_I/0/1/0/all/0/1\">Isabelle Lebert</a> (INRAE), <a href=\"http://arxiv.org/find/eess/1/au:+Baux_E/0/1/0/all/0/1\">Elisabeth Baux</a> (CHRU Nancy), <a href=\"http://arxiv.org/find/eess/1/au:+Cazorla_C/0/1/0/all/0/1\">C&#xe9;line Cazorla</a> (CHU de Saint-Etienne), <a href=\"http://arxiv.org/find/eess/1/au:+Eldin_C/0/1/0/all/0/1\">Carole Eldin</a> (IHU M&#xe9;diterran&#xe9;e Infection), <a href=\"http://arxiv.org/find/eess/1/au:+Hansmann_Y/0/1/0/all/0/1\">Yves Hansmann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Patrat_Delon_S/0/1/0/all/0/1\">Solene Patrat-Delon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Prazuck_T/0/1/0/all/0/1\">Thierry Prazuck</a> (CHR), <a href=\"http://arxiv.org/find/eess/1/au:+Raffetin_A/0/1/0/all/0/1\">Alice Raffetin</a> (CHIV), <a href=\"http://arxiv.org/find/eess/1/au:+Tattevin_P/0/1/0/all/0/1\">Pierre Tattevin</a> (CHU Rennes), <a href=\"http://arxiv.org/find/eess/1/au:+VourcH_G/0/1/0/all/0/1\">Gwena&#xeb;l Vourc&#x27;H</a> (INRAE), <a href=\"http://arxiv.org/find/eess/1/au:+Lesens_O/0/1/0/all/0/1\">Olivier Lesens</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nguifo_E/0/1/0/all/0/1\">Engelbert Nguifo</a> (LIMOS)",
          "description": "Lyme disease is one of the most common infectious vector-borne diseases in\nthe world. In the early stage, the disease manifests itself in most cases with\nerythema migrans (EM) skin lesions. Better diagnosis of these early forms would\nallow improving the prognosis by preventing the transition to a severe late\nform thanks to appropriate antibiotic therapy. Recent studies show that\nconvolutional neural networks (CNNs) perform very well to identify skin lesions\nfrom the image but, there is not much work for Lyme disease prediction from EM\nlesion images. The main objective of this study is to extensively analyze the\neffectiveness of CNNs for diagnosing Lyme disease from images and to find out\nthe best CNN architecture for the purpose. There is no publicly available EM\nimage dataset for Lyme dis…",
          "link": "http://arxiv.org/abs/2106.14465",
          "publishedOn": "2021-06-29T01:55:14.648Z",
          "wordCount": 857,
          "title": "Benchmarking convolutional neural networks for diagnosing Lyme disease from images. (arXiv:2106.14465v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14150",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hosseini_S/0/1/0/all/0/1\">Samira Hosseini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahdavi_M/0/1/0/all/0/1\">Mojtaba Mahdavi</a>",
          "description": "Content-independent watermarks and block-wise independency can be considered\nas vulnerabilities in semi-fragile watermarking methods. In this paper to\nachieve the objectives of semi-fragile watermarking techniques, a method is\nproposed to not have the mentioned shortcomings. In the proposed method, the\nwatermark is generated by relying on image content and a key. Furthermore, the\nembedding scheme causes the watermarked blocks to become dependent on each\nother, using a key. In the embedding phase, the image is partitioned into\nnon-overlapping blocks. In order to detect and separate the different types of\nattacks more precisely, the proposed method embeds three copies of each\nwatermark bit into LWT coefficients of each 4x4 block. In the authentication\nphase, by voting between the extracted bits the error maps are created; these\nmaps indicate image authenticity and reveal the modified regions. Also, in\norder to automate the authentication, the images are classified into four\ncategories using seven features. Classification accuracy in the experiments is\n97.97 percent. It is noted that our experiments demonstrate that the proposed\nmethod is robust against JPEG compression and is competitive with a\nstate-of-the-art semi-fragile watermarking method, in terms of robustness and\nsemi-fragility.",
          "link": "http://arxiv.org/abs/2106.14150",
          "publishedOn": "2021-06-29T01:55:14.640Z",
          "wordCount": 633,
          "title": "Image content dependent semi-fragile watermarking with localized tamper detection. (arXiv:2106.14150v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14275",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_T/0/1/0/all/0/1\">Townim Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jalisha_M/0/1/0/all/0/1\">Mahira Jalisha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheraghian_A/0/1/0/all/0/1\">Ali Cheraghian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahman_S/0/1/0/all/0/1\">Shafin Rahman</a>",
          "description": "When we fine-tune a well-trained deep learning model for a new set of\nclasses, the network learns new concepts but gradually forgets the knowledge of\nold training. In some real-life applications, we may be interested in learning\nnew classes without forgetting the capability of previous experience. Such\nlearning without forgetting problem is often investigated using 2D image\nrecognition tasks. In this paper, considering the growth of depth camera\ntechnology, we address the same problem for the 3D point cloud object data.\nThis problem becomes more challenging in the 3D domain than 2D because of the\nunavailability of large datasets and powerful pretrained backbone models. We\ninvestigate knowledge distillation techniques on 3D data to reduce catastrophic\nforgetting of the previous training. Moreover, we improve the distillation\nprocess by using semantic word vectors of object classes. We observe that\nexploring the interrelation of old and new knowledge during training helps to\nlearn new concepts without forgetting old ones. Experimenting on three 3D point\ncloud recognition backbones (PointNet, DGCNN, and PointConv) and synthetic\n(ModelNet40, ModelNet10) and real scanned (ScanObjectNN) datasets, we establish\nnew baseline results on learning without forgetting for 3D data. This research\nwill instigate many future works in this area.",
          "link": "http://arxiv.org/abs/2106.14275",
          "publishedOn": "2021-06-29T01:55:14.621Z",
          "wordCount": 647,
          "title": "Learning without Forgetting for 3D Point Cloud Objects. (arXiv:2106.14275v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2102.06697",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Noormandipour_M/0/1/0/all/0/1\">Mohammadreza Noormandipour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hanchen Wang</a>",
          "description": "In this work, we propose a parameterised quantum circuit learning approach to\npoint set matching problem. In contrast to previous annealing-based methods, we\npropose a quantum circuit-based framework whose parameters are optimised via\ndescending the gradients w.r.t a kernel-based loss function. We formulate the\nshape matching problem into a distribution learning task; that is, to learn the\ndistribution of the optimal transformation parameters. We show that this\nframework is able to find multiple optimal solutions for symmetric shapes and\nis more accurate, scalable and robust than the previous annealing-based method.\nCode, data and pre-trained weights are available at the project page:\n\\href{https://hansen7.github.io/qKC}{https://hansen7.github.io/qKC}",
          "link": "http://arxiv.org/abs/2102.06697",
          "publishedOn": "2021-06-29T01:55:14.564Z",
          "wordCount": 579,
          "title": "Matching Point Sets with Quantum Circuit Learning. (arXiv:2102.06697v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14306",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qin_R/0/1/0/all/0/1\">Rongjun Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Shuang Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_X/0/1/0/all/0/1\">Xiao Ling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elhashash_M/0/1/0/all/0/1\">Mostafa Elhashash</a>",
          "description": "3D recovery from multi-stereo and stereo images, as an important application\nof the image-based perspective geometry, serves many applications in computer\nvision, remote sensing and Geomatics. In this chapter, the authors utilize the\nimaging geometry and present approaches that perform 3D reconstruction from\ncross-view images that are drastically different in their viewpoints. We\nintroduce our framework that takes ground-view images and satellite images for\nfull 3D recovery, which includes necessary methods in satellite and\nground-based point cloud generation from images, 3D data co-registration,\nfusion and mesh generation. We demonstrate our proposed framework on a dataset\nconsisting of twelve satellite images and 150k video frames acquired through a\nvehicle-mounted Go-pro camera and demonstrate the reconstruction results. We\nhave also compared our results with results generated from an intuitive\nprocessing pipeline that involves typical geo-registration and meshing methods.",
          "link": "http://arxiv.org/abs/2106.14306",
          "publishedOn": "2021-06-29T01:55:14.548Z",
          "wordCount": 569,
          "title": "3D Reconstruction through Fusion of Cross-View Images. (arXiv:2106.14306v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14336",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huo_D/0/1/0/all/0/1\">Dong Huo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masoumzadeh_A/0/1/0/all/0/1\">Abbas Masoumzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yee-Hong Yang</a>",
          "description": "Many deep learning based methods are designed to remove non-uniform\n(spatially variant) motion blur caused by object motion and camera shake\nwithout knowing the blur kernel. Some methods directly output the latent sharp\nimage in one stage, while others utilize a multi-stage strategy (\\eg\nmulti-scale, multi-patch, or multi-temporal) to gradually restore the sharp\nimage. However, these methods have the following two main issues: 1) The\ncomputational cost of multi-stage is high; 2) The same convolution kernel is\napplied in different regions, which is not an ideal choice for non-uniform\nblur. Hence, non-uniform motion deblurring is still a challenging and open\nproblem. In this paper, we propose a new architecture which consists of\nmultiple Atrous Spatial Pyramid Deformable Convolution (ASPDC) modules to\ndeblur an image end-to-end with more flexibility. Multiple ASPDC modules\nimplicitly learn the pixel-specific motion with different dilation rates in the\nsame layer to handle movements of different magnitude. To improve the training,\nwe also propose a reblurring network to map the deblurred output back to the\nblurred input, which constrains the solution space. Our experimental results\nshow that the proposed method outperforms state-of-the-art methods on the\nbenchmark datasets.",
          "link": "http://arxiv.org/abs/2106.14336",
          "publishedOn": "2021-06-29T01:55:14.542Z",
          "wordCount": 638,
          "title": "Blind Non-Uniform Motion Deblurring using Atrous Spatial Pyramid Deformable Convolution and Deblurring-Reblurring Consistency. (arXiv:2106.14336v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14207",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Khandakar_A/0/1/0/all/0/1\">Amith Khandakar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chowdhury_M/0/1/0/all/0/1\">Muhammad E. H. Chowdhury</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Reaz_M/0/1/0/all/0/1\">Mamun Bin Ibne Reaz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ali_S/0/1/0/all/0/1\">Sawal Hamid Md Ali</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hasan_M/0/1/0/all/0/1\">Md Anwarul Hasan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kiranyaz_S/0/1/0/all/0/1\">Serkan Kiranyaz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rahman_T/0/1/0/all/0/1\">Tawsifur Rahman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Alfkey_R/0/1/0/all/0/1\">Rashad Alfkey</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bakar_A/0/1/0/all/0/1\">Ahmad Ashrif A. Bakar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Malik_R/0/1/0/all/0/1\">Rayaz A. Malik</a>",
          "description": "Diabetes foot ulceration (DFU) and amputation are a cause of significant\nmorbidity. The prevention of DFU may be achieved by the identification of\npatients at risk of DFU and the institution of preventative measures through\neducation and offloading. Several studies have reported that thermogram images\nmay help to detect an increase in plantar temperature prior to DFU. However,\nthe distribution of plantar temperature may be heterogeneous, making it\ndifficult to quantify and utilize to predict outcomes. We have compared a\nmachine learning-based scoring technique with feature selection and\noptimization techniques and learning classifiers to several state-of-the-art\nConvolutional Neural Networks (CNNs) on foot thermogram images and propose a\nrobust solution to identify the diabetic foot. A comparatively shallow CNN\nmodel, MobilenetV2 achieved an F1 score of ~95% for a two-feet thermogram\nimage-based classification and the AdaBoost Classifier used 10 features and\nachieved an F1 score of 97 %. A comparison of the inference time for the\nbest-performing networks confirmed that the proposed algorithm can be deployed\nas a smartphone application to allow the user to monitor the progression of the\nDFU in a home setting.",
          "link": "http://arxiv.org/abs/2106.14207",
          "publishedOn": "2021-06-29T01:55:14.524Z",
          "wordCount": 670,
          "title": "A Machine Learning Model for Early Detection of Diabetic Foot using Thermogram Images. (arXiv:2106.14207v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14148",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alimi_R/0/1/0/all/0/1\">Roger Alimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ivry_A/0/1/0/all/0/1\">Amir Ivry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fisher_E/0/1/0/all/0/1\">Elad Fisher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weiss_E/0/1/0/all/0/1\">Eyal Weiss</a>",
          "description": "Modern magnetic sensor arrays conventionally utilize state of the art low\npower magnetometers such as parallel and orthogonal fluxgates. Low power\nfluxgates tend to have large Barkhausen jumps that appear as a dc jump in the\nfluxgate output. This phenomenon deteriorates the signal fidelity and\neffectively increases the internal sensor noise. Even if sensors that are more\nprone to dc jumps can be screened during production, the conventional noise\nmeasurement does not always catch the dc jump because of its sparsity.\nMoreover, dc jumps persist in almost all the sensor cores although at a slower\nbut still intolerable rate. Even if dc jumps can be easily detected in a\nshielded environment, when deployed in presence of natural noise and clutter,\nit can be hard to positively detect them. This work fills this gap and presents\nalgorithms that distinguish dc jumps embedded in natural magnetic field data.\nTo improve robustness to noise, we developed two machine learning algorithms\nthat employ temporal and statistical physical-based features of a pre-acquired\nand well-known experimental data set. The first algorithm employs a support\nvector machine classifier, while the second is based on a neural network\narchitecture. We compare these new approaches to a more classical kernel-based\nmethod. To that purpose, the receiver operating characteristic curve is\ngenerated, which allows diagnosis ability of the different classifiers by\ncomparing their performances across various operation points. The accuracy of\nthe machine learning-based algorithms over the classic method is highly\nemphasized. In addition, high generalization and robustness of the neural\nnetwork can be concluded, based on the rapid convergence of the corresponding\nreceiver operating characteristic curves.",
          "link": "http://arxiv.org/abs/2106.14148",
          "publishedOn": "2021-06-29T01:55:14.518Z",
          "wordCount": 733,
          "title": "Machine Learning Detection Algorithm for Large Barkhausen Jumps in Cluttered Environment. (arXiv:2106.14148v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14259",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nishimura_H/0/1/0/all/0/1\">Hitoshi Nishimura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Komorita_S/0/1/0/all/0/1\">Satoshi Komorita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kawanishi_Y/0/1/0/all/0/1\">Yasutomo Kawanishi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murase_H/0/1/0/all/0/1\">Hiroshi Murase</a>",
          "description": "Multiple human tracking is a fundamental problem for scene understanding.\nAlthough both accuracy and speed are required in real-world applications,\nrecent tracking methods based on deep learning have focused on accuracy and\nrequire substantial running time. This study aims to improve running speed by\nperforming human detection at a certain frame interval because it accounts for\nmost of the running time. The question is how to maintain accuracy while\nskipping human detection. In this paper, we propose a method that complements\nthe detection results with optical flow, based on the fact that someone's\nappearance does not change much between adjacent frames. To maintain the\ntracking accuracy, we introduce robust interest point selection within human\nregions and a tracking termination metric calculated by the distribution of the\ninterest points. On the MOT20 dataset in the MOTChallenge, the proposed\nSDOF-Tracker achieved the best performance in terms of the total running speed\nwhile maintaining the MOTA metric. Our code is available at\nhttps://anonymous.4open.science/r/sdof-tracker-75AE.",
          "link": "http://arxiv.org/abs/2106.14259",
          "publishedOn": "2021-06-29T01:55:14.511Z",
          "wordCount": 601,
          "title": "SDOF-Tracker: Fast and Accurate Multiple Human Tracking by Skipped-Detection and Optical-Flow. (arXiv:2106.14259v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14193",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Haitao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zichang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheang_C/0/1/0/all/0/1\">Chilam Cheang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lingwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yanwei Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_X/0/1/0/all/0/1\">Xiangyang Xue</a>",
          "description": "We propose a method of Category-level 6D Object Pose and Size Estimation\n(COPSE) from a single depth image, without external pose-annotated real-world\ntraining data. While previous works exploit visual cues in RGB(D) images, our\nmethod makes inferences based on the rich geometric information of the object\nin the depth channel alone. Essentially, our framework explores such geometric\ninformation by learning the unified 3D Orientation-Consistent Representations\n(3D-OCR) module, and further enforced by the property of Geometry-constrained\nReflection Symmetry (GeoReS) module. The magnitude information of object size\nand the center point is finally estimated by Mirror-Paired Dimensional\nEstimation (MPDE) module. Extensive experiments on the category-level NOCS\nbenchmark demonstrate that our framework competes with state-of-the-art\napproaches that require labeled real-world images. We also deploy our approach\nto a physical Baxter robot to perform manipulation tasks on unseen but\ncategory-known instances, and the results further validate the efficacy of our\nproposed model. Our videos are available in the supplementary material.",
          "link": "http://arxiv.org/abs/2106.14193",
          "publishedOn": "2021-06-29T01:55:14.465Z",
          "wordCount": 607,
          "title": "DONet: Learning Category-Level 6D Object Pose and Size Estimation from Depth Observation. (arXiv:2106.14193v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14162",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1\">Bowen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1\">Zhenfei Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_J/0/1/0/all/0/1\">Jing Shao</a>",
          "description": "Face anti-spoofing (FAS) is an indispensable and widely used module in face\nrecognition systems. Although high accuracy has been achieved, a FAS system\nwill never be perfect due to the non-stationary applied environments and the\npotential emergence of new types of presentation attacks in real-world\napplications. In practice, given a handful of labeled samples from a new\ndeployment scenario (target domain) and abundant labeled face images in the\nexisting source domain, the FAS system is expected to perform well in the new\nscenario without sacrificing the performance on the original domain. To this\nend, we identify and address a more practical problem: Few-Shot Domain\nExpansion for Face Anti-Spoofing (FSDE-FAS). This problem is challenging since\nwith insufficient target domain training samples, the model may suffer from\nboth overfitting to the target domain and catastrophic forgetting of the source\ndomain. To address the problem, this paper proposes a Style transfer-based\nAugmentation for Semantic Alignment (SASA) framework. We propose to augment the\ntarget data by generating auxiliary samples based on photorealistic style\ntransfer. With the assistant of the augmented data, we further propose a\ncarefully designed mechanism to align different domains from both\ninstance-level and distribution-level, and then stabilize the performance on\nthe source domain with a less-forgetting constraint. Two benchmarks are\nproposed to simulate the FSDE-FAS scenarios, and the experimental results show\nthat the proposed SASA method outperforms state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2106.14162",
          "publishedOn": "2021-06-29T01:55:14.458Z",
          "wordCount": 664,
          "title": "Few-Shot Domain Expansion for Face Anti-Spoofing. (arXiv:2106.14162v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14183",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1\">Jun Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Buyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jun Yu</a>",
          "description": "We propose a novel method on refining cross-person gaze prediction task with\neye/face images only by explicitly modelling the person-specific differences.\nSpecifically, we first assume that we can obtain some initial gaze prediction\nresults with existing method, which we refer to as InitNet, and then introduce\nthree modules, the Validity Module (VM), Self-Calibration (SC) and\nPerson-specific Transform (PT)) Module. By predicting the reliability of\ncurrent eye/face images, our VM is able to identify invalid samples, e.g. eye\nblinking images, and reduce their effects in our modelling process. Our SC and\nPT module then learn to compensate for the differences on valid samples only.\nThe former models the translation offsets by bridging the gap between initial\npredictions and dataset-wise distribution. And the later learns more general\nperson-specific transformation by incorporating the information from existing\ninitial predictions of the same person. We validate our ideas on three publicly\navailable datasets, EVE, XGaze and MPIIGaze and demonstrate that our proposed\nmethod outperforms the SOTA methods significantly on all of them, e.g.\nrespectively 21.7%, 36.0% and 32.9% relative performance improvements. We won\nthe GAZE 2021 Competition on the EVE dataset. Our code can be found here\nhttps://github.com/bjj9/EVE_SCPT.",
          "link": "http://arxiv.org/abs/2106.14183",
          "publishedOn": "2021-06-29T01:55:14.396Z",
          "wordCount": 636,
          "title": "The Story in Your Eyes: An Individual-difference-aware Model for Cross-person Gaze Estimation. (arXiv:2106.14183v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14156",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhenhua Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunhe Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1\">Kai Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Siwei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1\">Wen Gao</a>",
          "description": "Recently, transformer has achieved remarkable performance on a variety of\ncomputer vision applications. Compared with mainstream convolutional neural\nnetworks, vision transformers are often of sophisticated architectures for\nextracting powerful feature representations, which are more difficult to be\ndeveloped on mobile devices. In this paper, we present an effective\npost-training quantization algorithm for reducing the memory storage and\ncomputational costs of vision transformers. Basically, the quantization task\ncan be regarded as finding the optimal low-bit quantization intervals for\nweights and inputs, respectively. To preserve the functionality of the\nattention mechanism, we introduce a ranking loss into the conventional\nquantization objective that aims to keep the relative order of the\nself-attention results after quantization. Moreover, we thoroughly analyze the\nrelationship between quantization loss of different layers and the feature\ndiversity, and explore a mixed-precision quantization scheme by exploiting the\nnuclear norm of each attention map and output feature. The effectiveness of the\nproposed method is verified on several benchmark models and datasets, which\noutperforms the state-of-the-art post-training quantization algorithms. For\ninstance, we can obtain an 81.29\\% top-1 accuracy using DeiT-B model on\nImageNet dataset with about 8-bit quantization.",
          "link": "http://arxiv.org/abs/2106.14156",
          "publishedOn": "2021-06-29T01:55:14.390Z",
          "wordCount": 618,
          "title": "Post-Training Quantization for Vision Transformer. (arXiv:2106.14156v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14324",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zhou_W/0/1/0/all/0/1\">Weimin Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bhadra_S/0/1/0/all/0/1\">Sayantan Bhadra</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Brooks_F/0/1/0/all/0/1\">Frank J. Brooks</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1\">Hua Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Anastasio_M/0/1/0/all/0/1\">Mark A. Anastasio</a>",
          "description": "In order to objectively assess new medical imaging technologies via\ncomputer-simulations, it is important to account for all sources of variability\nthat contribute to image data. One important source of variability that can\nsignificantly limit observer performance is associated with the variability in\nthe ensemble of objects to-be-imaged. This source of variability can be\ndescribed by stochastic object models (SOMs), which are generative models that\ncan be employed to sample from a distribution of to-be-virtually-imaged\nobjects. It is generally desirable to establish SOMs from experimental imaging\nmeasurements acquired by use of a well-characterized imaging system, but this\ntask has remained challenging. Deep generative neural networks, such as\ngenerative adversarial networks (GANs) hold potential for such tasks. To\nestablish SOMs from imaging measurements, an AmbientGAN has been proposed that\naugments a GAN with a measurement operator. However, the original AmbientGAN\ncould not immediately benefit from modern training procedures and GAN\narchitectures, which limited its ability to be applied to realistically sized\nmedical image data. To circumvent this, in this work, a modified AmbientGAN\ntraining strategy is proposed that is suitable for modern progressive or\nmulti-resolution training approaches such as employed in the Progressive\nGrowing of GANs and Style-based GANs. AmbientGANs established by use of the\nproposed training procedure are systematically validated in a controlled way by\nuse of computer-simulated measurement data corresponding to a stylized imaging\nsystem. Finally, emulated single-coil experimental magnetic resonance imaging\ndata are employed to demonstrate the methods under less stylized conditions.",
          "link": "http://arxiv.org/abs/2106.14324",
          "publishedOn": "2021-06-29T01:55:14.384Z",
          "wordCount": 728,
          "title": "Learning stochastic object models from medical imaging measurements by use of advanced AmbientGANs. (arXiv:2106.14324v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13849",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Al_Battal_A/0/1/0/all/0/1\">Abdullah F. Al-Battal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yan Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Lu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morton_T/0/1/0/all/0/1\">Timothy Morton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_C/0/1/0/all/0/1\">Chen Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+1_Y/0/1/0/all/0/1\">Yifeng Bu 1</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lerman_I/0/1/0/all/0/1\">Imanuel R Lerman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madhavan_R/0/1/0/all/0/1\">Radhika Madhavan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Truong Q. Nguyen</a>",
          "description": "Ultrasound scanning is essential in several medical diagnostic and\ntherapeutic applications. It is used to visualize and analyze anatomical\nfeatures and structures that influence treatment plans. However, it is both\nlabor intensive, and its effectiveness is operator dependent. Real-time\naccurate and robust automatic detection and tracking of anatomical structures\nwhile scanning would significantly impact diagnostic and therapeutic procedures\nto be consistent and efficient. In this paper, we propose a deep learning\nframework to automatically detect and track a specific anatomical target\nstructure in ultrasound scans. Our framework is designed to be accurate and\nrobust across subjects and imaging devices, to operate in real-time, and to not\nrequire a large training set. It maintains a localization precision and recall\nhigher than 90% when trained on training sets that are as small as 20% in size\nof the original training set. The framework backbone is a weakly trained\nsegmentation neural network based on U-Net. We tested the framework on two\ndifferent ultrasound datasets with the aim to detect and track the Vagus nerve,\nwhere it outperformed current state-of-the-art real-time object detection\nnetworks.",
          "link": "http://arxiv.org/abs/2106.13849",
          "publishedOn": "2021-06-29T01:55:14.359Z",
          "wordCount": 671,
          "title": "A CNN Segmentation-Based Approach to Object Detection and Tracking in Ultrasound Scans with Application to the Vagus Nerve Detection. (arXiv:2106.13849v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14118",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bagchi_A/0/1/0/all/0/1\">Anurag Bagchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahmood_J/0/1/0/all/0/1\">Jazib Mahmood</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandes_D/0/1/0/all/0/1\">Dolton Fernandes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarvadevabhatla_R/0/1/0/all/0/1\">Ravi Kiran Sarvadevabhatla</a>",
          "description": "State of the art architectures for untrimmed video Temporal Action\nLocalization (TAL) have only considered RGB and Flow modalities, leaving the\ninformation-rich audio modality totally unexploited. Audio fusion has been\nexplored for the related but arguably easier problem of trimmed (clip-level)\naction recognition. However, TAL poses a unique set of challenges. In this\npaper, we propose simple but effective fusion-based approaches for TAL. To the\nbest of our knowledge, our work is the first to jointly consider audio and\nvideo modalities for supervised TAL. We experimentally show that our schemes\nconsistently improve performance for state of the art video-only TAL\napproaches. Specifically, they help achieve new state of the art performance on\nlarge-scale benchmark datasets - ActivityNet-1.3 (52.73 mAP@0.5) and THUMOS14\n(57.18 mAP@0.5). Our experiments include ablations involving multiple fusion\nschemes, modality combinations and TAL architectures. Our code, models and\nassociated data will be made available.",
          "link": "http://arxiv.org/abs/2106.14118",
          "publishedOn": "2021-06-29T01:55:14.344Z",
          "wordCount": 593,
          "title": "Hear Me Out: Fusional Approaches for Audio Augmented Temporal Action Localization. (arXiv:2106.14118v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13939",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shizhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tuo_H/0/1/0/all/0/1\">Hongya Tuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jian Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jing_Z/0/1/0/all/0/1\">Zhongliang Jing</a>",
          "description": "Domain shift is a major challenge for object detectors to generalize well to\nreal world applications. Emerging techniques of domain adaptation for two-stage\ndetectors help to tackle this problem. However, two-stage detectors are not the\nfirst choice for industrial applications due to its long time consumption. In\nthis paper, a novel Domain Adaptive YOLO (DA-YOLO) is proposed to improve\ncross-domain performance for one-stage detectors. Image level features\nalignment is used to strictly match for local features like texture, and\nloosely match for global features like illumination. Multi-scale instance level\nfeatures alignment is presented to reduce instance domain shift effectively ,\nsuch as variations in object appearance and viewpoint. A consensus\nregularization to these domain classifiers is employed to help the network\ngenerate domain-invariant detections. We evaluate our proposed method on\npopular datasets like Cityscapes, KITTI, SIM10K and etc.. The results\ndemonstrate significant improvement when tested under different cross-domain\nscenarios.",
          "link": "http://arxiv.org/abs/2106.13939",
          "publishedOn": "2021-06-29T01:55:14.323Z",
          "wordCount": 582,
          "title": "Domain Adaptive YOLO for One-Stage Cross-Domain Detection. (arXiv:2106.13939v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14186",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ferla_M/0/1/0/all/0/1\">Michele La Ferla</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Montebello_M/0/1/0/all/0/1\">Matthew Montebello</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Seychell_D/0/1/0/all/0/1\">Dylan Seychell</a>",
          "description": "During the last decade or so, there has been an insurgence in the deep\nlearning community to solve health-related issues, particularly breast cancer.\nFollowing the Camelyon-16 challenge in 2016, several researchers have dedicated\ntheir time to build Convolutional Neural Networks (CNNs) to help radiologists\nand other clinicians diagnose breast cancer. In particular, there has been an\nemphasis on Ductal Carcinoma in Situ (DCIS); the clinical term for early-stage\nbreast cancer. Large companies have given their fair share of research into\nthis subject, among these Google Deepmind who developed a model in 2020 that\nhas proven to be better than radiologists themselves to diagnose breast cancer\ncorrectly.\n\nWe found that among the issues which exist, there is a need for an\nexplanatory system that goes through the hidden layers of a CNN to highlight\nthose pixels that contributed to the classification of a mammogram. We then\nchose an open-source, reasonably successful project developed by Prof. Shen,\nusing the CBIS-DDSM image database to run our experiments on. It was later\nimproved using the Resnet-50 and VGG-16 patch-classifiers, analytically\ncomparing the outcome of both. The results showed that the Resnet-50 one\nconverged earlier in the experiments.\n\nFollowing the research by Montavon and Binder, we used the DeepTaylor\nLayer-wise Relevance Propagation (LRP) model to highlight those pixels and\nregions within a mammogram which contribute most to its classification. This is\nrepresented as a map of those pixels in the original image, which contribute to\nthe diagnosis and the extent to which they contribute to the final\nclassification. The most significant advantage of this algorithm is that it\nperforms exceptionally well with the Resnet-50 patch classifier architecture.",
          "link": "http://arxiv.org/abs/2106.14186",
          "publishedOn": "2021-06-29T01:55:14.277Z",
          "wordCount": 739,
          "title": "An XAI Approach to Deep Learning Models in the Detection of Ductal Carcinoma in Situ. (arXiv:2106.14186v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13913",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1\">Hongyu Guo</a>",
          "description": "Label Smoothing (LS) improves model generalization through penalizing models\nfrom generating overconfident output distributions. For each training sample\nthe LS strategy smooths the one-hot encoded training signal by distributing its\ndistribution mass over the non-ground truth classes. We extend this technique\nby considering example pairs, coined PLS. PLS first creates midpoint samples by\naveraging random sample pairs and then learns a smoothing distribution during\ntraining for each of these midpoint samples, resulting in midpoints with high\nuncertainty labels for training. We empirically show that PLS significantly\noutperforms LS, achieving up to 30% of relative classification error reduction.\nWe also visualize that PLS produces very low winning softmax scores for both in\nand out of distribution samples.",
          "link": "http://arxiv.org/abs/2106.13913",
          "publishedOn": "2021-06-29T01:55:14.271Z",
          "wordCount": 566,
          "title": "Midpoint Regularization: from High Uncertainty Training to Conservative Classification. (arXiv:2106.13913v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14309",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qin_R/0/1/0/all/0/1\">Rongjun Qin</a>",
          "description": "The geodatabase (vectorized data) nowadays becomes a rather standard digital\ncity infrastructure; however, updating geodatabase efficiently and economically\nremains a fundamental and practical issue in the geospatial industry. The cost\nof building a geodatabase is extremely high and labor intensive, and very often\nthe maps we use have several months and even years of latency. One solution is\nto develop more automated methods for (vectorized) geospatial data generation,\nwhich has been proven a difficult task in the past decades. An alternative\nsolution is to first detect the differences between the new data and the\nexisting geospatial data, and then only update the area identified as changes.\nThe second approach is becoming more favored due to its high practicality and\nflexibility. A highly relevant technique is change detection. This article aims\nto provide an overview the state-of-the-art change detection methods in the\nfield of Remote Sensing and Geomatics to support the task of updating\ngeodatabases. Data used for change detection are highly disparate, we therefore\nstructure our review intuitively based on the dimension of the data, being 1)\nchange detection with 2D data; 2) change detection with 3D data. Conclusions\nwill be drawn based on the reviewed efforts in the field, and we will share our\noutlooks of the topic of updating geodatabases.",
          "link": "http://arxiv.org/abs/2106.14309",
          "publishedOn": "2021-06-29T01:55:14.242Z",
          "wordCount": 642,
          "title": "Change Detection for Geodatabase Updating. (arXiv:2106.14309v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14049",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1\">Yue Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_N/0/1/0/all/0/1\">Nningchuan Xiao</a>",
          "description": "The growing number of real-time camera feeds in urban areas has made it\npossible to provide high-quality traffic data for effective transportation\nplanning, operations, and management. However, deriving reliable traffic\nmetrics from these camera feeds has been a challenge due to the limitations of\ncurrent vehicle detection techniques, as well as the various camera conditions\nsuch as height and resolution. In this work, a quadtree based algorithm is\ndeveloped to continuously partition the image extent until only regions with\nhigh detection accuracy are remained. These regions are referred to as the\nhigh-accuracy identification regions (HAIR) in this paper. We demonstrate how\nthe use of the HAIR can improve the accuracy of traffic density estimates using\nimages from traffic cameras at different heights and resolutions in Central\nOhio. Our experiments show that the proposed algorithm can be used to derive\nrobust HAIR where vehicle detection accuracy is 41 percent higher than that in\nthe original image extent. The use of the HAIR also significantly improves the\ntraffic density estimation with an overall decrease of 49 percent in root mean\nsquared error.",
          "link": "http://arxiv.org/abs/2106.14049",
          "publishedOn": "2021-06-29T01:55:14.227Z",
          "wordCount": 639,
          "title": "Identifying High Accuracy Regions in Traffic Camera Images to Enhance the Estimation of Road Traffic Metrics: A Quadtree Based Method. (arXiv:2106.14049v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13899",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Monteiro_J/0/1/0/all/0/1\">Joao Monteiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gibert_X/0/1/0/all/0/1\">Xavier Gibert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jianqiao Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dumoulin_V/0/1/0/all/0/1\">Vincent Dumoulin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dar-Shyang Lee</a>",
          "description": "Learning guarantees often rely on assumptions of i.i.d. data, which will\nlikely be violated in practice once predictors are deployed to perform\nreal-world tasks. Domain adaptation approaches thus appeared as a useful\nframework yielding extra flexibility in that distinct train and test data\ndistributions are supported, provided that other assumptions are satisfied such\nas covariate shift, which expects the conditional distributions over labels to\nbe independent of the underlying data distribution. Several approaches were\nintroduced in order to induce generalization across varying train and test data\nsources, and those often rely on the general idea of domain-invariance, in such\na way that the data-generating distributions are to be disregarded by the\nprediction model. In this contribution, we tackle the problem of generalizing\nacross data sources by approaching it from the opposite direction: we consider\na conditional modeling approach in which predictions, in addition to being\ndependent on the input data, use information relative to the underlying\ndata-generating distribution. For instance, the model has an explicit mechanism\nto adapt to changing environments and/or new data sources. We argue that such\nan approach is more generally applicable than current domain adaptation methods\nsince it does not require extra assumptions such as covariate shift and further\nyields simpler training algorithms that avoid a common source of training\ninstabilities caused by minimax formulations, often employed in\ndomain-invariant methods.",
          "link": "http://arxiv.org/abs/2106.13899",
          "publishedOn": "2021-06-29T01:55:14.219Z",
          "wordCount": 673,
          "title": "Domain Conditional Predictors for Domain Adaptation. (arXiv:2106.13899v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14069",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Ye Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1\">Yan Yan</a>",
          "description": "Current vision and language tasks usually take complete visual data (e.g.,\nraw images or videos) as input, however, practical scenarios may often consist\nthe situations where part of the visual information becomes inaccessible due to\nvarious reasons e.g., restricted view with fixed camera or intentional vision\nblock for security concerns. As a step towards the more practical application\nscenarios, we introduce a novel task that aims to describe a video using the\nnatural language dialog between two agents as a supplementary information\nsource given incomplete visual data. Different from most existing\nvision-language tasks where AI systems have full access to images or video\nclips, which may reveal sensitive information such as recognizable human faces\nor voices, we intentionally limit the visual input for AI systems and seek a\nmore secure and transparent information medium, i.e., the natural language\ndialog, to supplement the missing visual information. Specifically, one of the\nintelligent agents - Q-BOT - is given two semantic segmented frames from the\nbeginning and the end of the video, as well as a finite number of opportunities\nto ask relevant natural language questions before describing the unseen video.\nA-BOT, the other agent who has access to the entire video, assists Q-BOT to\naccomplish the goal by answering the asked questions. We introduce two\ndifferent experimental settings with either a generative (i.e., agents generate\nquestions and answers freely) or a discriminative (i.e., agents select the\nquestions and answers from candidates) internal dialog generation process. With\nthe proposed unified QA-Cooperative networks, we experimentally demonstrate the\nknowledge transfer process between the two dialog agents and the effectiveness\nof using the natural language dialog as a supplement for incomplete implicit\nvisions.",
          "link": "http://arxiv.org/abs/2106.14069",
          "publishedOn": "2021-06-29T01:55:14.202Z",
          "wordCount": 722,
          "title": "Saying the Unseen: Video Descriptions via Dialog Agents. (arXiv:2106.14069v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13992",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Djeraba_C/0/1/0/all/0/1\">Chaabane Djeraba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riedi_J/0/1/0/all/0/1\">J&#xe9;r&#xf4;me Riedi</a>",
          "description": "This paper overviews two interdependent issues important for mining remote\nsensing data (e.g. images) obtained from atmospheric monitoring missions. The\nfirst issue relates the building new public datasets and benchmarks, which are\nhot priority of the remote sensing community. The second issue is the\ninvestigation of deep learning methodologies for atmospheric data\nclassification based on vast amount of data without annotations and with\nlocalized annotated data provided by sparse observing networks at the surface.\nThe targeted application is air quality assessment and prediction. Air quality\nis defined as the pollution level linked with several atmospheric constituents\nsuch as gases and aerosols. There are dependency relationships between the bad\nair quality, caused by air pollution, and the public health. The target\napplication is the development of a fast prediction model for local and\nregional air quality assessment and tracking. The results of mining data will\nhave significant implication for citizen and decision makers by providing a\nfast prediction and reliable air quality monitoring system able to cover the\nlocal and regional scale through intelligent extrapolation of sparse\nground-based in situ measurement networks.",
          "link": "http://arxiv.org/abs/2106.13992",
          "publishedOn": "2021-06-29T01:55:14.196Z",
          "wordCount": 606,
          "title": "Mining atmospheric data. (arXiv:2106.13992v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13929",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Huafeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kaixiong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinxing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_G/0/1/0/all/0/1\">Guangming Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhengtao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">David Zhang</a>",
          "description": "Since human-labeled samples are free for the target set, unsupervised person\nre-identification (Re-ID) has attracted much attention in recent years, by\nadditionally exploiting the source set. However, due to the differences on\ncamera styles, illumination and backgrounds, there exists a large gap between\nsource domain and target domain, introducing a great challenge on cross-domain\nmatching. To tackle this problem, in this paper we propose a novel method named\nDual-stream Reciprocal Disentanglement Learning (DRDL), which is quite\nefficient in learning domain-invariant features. In DRDL, two encoders are\nfirst constructed for id-related and id-unrelated feature extractions, which\nare respectively measured by their associated classifiers. Furthermore,\nfollowed by an adversarial learning strategy, both streams reciprocally and\npositively effect each other, so that the id-related features and id-unrelated\nfeatures are completely disentangled from a given image, allowing the encoder\nto be powerful enough to obtain the discriminative but domain-invariant\nfeatures. In contrast to existing approaches, our proposed method is free from\nimage generation, which not only reduces the computational complexity\nremarkably, but also removes redundant information from id-related features.\nExtensive experiments substantiate the superiority of our proposed method\ncompared with the state-of-the-arts. The source code has been released in\nhttps://github.com/lhf12278/DRDL.",
          "link": "http://arxiv.org/abs/2106.13929",
          "publishedOn": "2021-06-29T01:55:14.191Z",
          "wordCount": 647,
          "title": "Dual-Stream Reciprocal Disentanglement Learning for Domain Adaption Person Re-Identification. (arXiv:2106.13929v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13883",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Afifi_M/0/1/0/all/0/1\">Mahmoud Afifi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abuolaim_A/0/1/0/all/0/1\">Abdullah Abuolaim</a>",
          "description": "The raw-RGB colors of a camera sensor vary due to the spectral sensitivity\ndifferences across different sensor makes and models. This paper focuses on the\ntask of mapping between different sensor raw-RGB color spaces. Prior work\naddressed this problem using a pairwise calibration to achieve accurate color\nmapping. Although being accurate, this approach is less practical as it\nrequires: (1) capturing pair of images by both camera devices with a color\ncalibration object placed in each new scene; (2) accurate image alignment or\nmanual annotation of the color calibration object. This paper aims to tackle\ncolor mapping in the raw space through a more practical setup. Specifically, we\npresent a semi-supervised raw-to-raw mapping method trained on a small set of\npaired images alongside an unpaired set of images captured by each camera\ndevice. Through extensive experiments, we show that our method achieves better\nresults compared to other domain adaptation alternatives in addition to the\nsingle-calibration solution. We have generated a new dataset of raw images from\ntwo different smartphone cameras as part of this effort. Our dataset includes\nunpaired and paired sets for our semi-supervised training and evaluation.",
          "link": "http://arxiv.org/abs/2106.13883",
          "publishedOn": "2021-06-29T01:55:14.184Z",
          "wordCount": 617,
          "title": "Semi-Supervised Raw-to-Raw Mapping. (arXiv:2106.13883v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13984",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Shuai Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_K/0/1/0/all/0/1\">Kai Qiao</a>",
          "description": "In this paper, we propose a novel encoder, called ShapeEditor, for\nhigh-resolution, realistic and high-fidelity face exchange. First of all, in\norder to ensure sufficient clarity and authenticity, our key idea is to use an\nadvanced pretrained high-quality random face image generator, i.e. StyleGAN, as\nbackbone. Secondly, we design ShapeEditor, a two-step encoder, to make the\nswapped face integrate the identity and attribute of the input faces. In the\nfirst step, we extract the identity vector of the source image and the\nattribute vector of the target image respectively; in the second step, we map\nthe concatenation of identity vector and attribute vector into the\n$\\mathcal{W+}$ potential space. In addition, for learning to map into the\nlatent space of StyleGAN, we propose a set of self-supervised loss functions\nwith which the training data do not need to be labeled manually. Extensive\nexperiments on the test dataset show that the results of our method not only\nhave a great advantage in clarity and authenticity than other state-of-the-art\nmethods, but also reflect the sufficient integration of identity and attribute.",
          "link": "http://arxiv.org/abs/2106.13984",
          "publishedOn": "2021-06-29T01:55:14.176Z",
          "wordCount": 616,
          "title": "ShapeEditer: a StyleGAN Encoder for Face Swapping. (arXiv:2106.13984v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14132",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yang-tian Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Hao-zhi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_Y/0/1/0/all/0/1\">Yu-kun Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Lin Gao</a>",
          "description": "Pose transfer of human videos aims to generate a high fidelity video of a\ntarget person imitating actions of a source person. A few studies have made\ngreat progress either through image translation with deep latent features or\nneural rendering with explicit 3D features. However, both of them rely on large\namounts of training data to generate realistic results, and the performance\ndegrades on more accessible internet videos due to insufficient training\nframes. In this paper, we demonstrate that the dynamic details can be preserved\neven trained from short monocular videos. Overall, we propose a neural video\nrendering framework coupled with an image-translation-based dynamic details\ngeneration network (D2G-Net), which fully utilizes both the stability of\nexplicit 3D features and the capacity of learning components. To be specific, a\nnovel texture representation is presented to encode both the static and\npose-varying appearance characteristics, which is then mapped to the image\nspace and rendered as a detail-rich frame in the neural rendering stage.\nMoreover, we introduce a concise temporal loss in the training stage to\nsuppress the detail flickering that is made more visible due to high-quality\ndynamic details generated by our method. Through extensive comparisons, we\ndemonstrate that our neural human video renderer is capable of achieving both\nclearer dynamic details and more robust performance even on accessible short\nvideos with only 2k - 4k frames.",
          "link": "http://arxiv.org/abs/2106.14132",
          "publishedOn": "2021-06-29T01:55:14.149Z",
          "wordCount": 677,
          "title": "Robust Pose Transfer with Dynamic Details using Neural Video Rendering. (arXiv:2106.14132v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14019",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hwanhee Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1\">Seunghyun Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dernoncourt_F/0/1/0/all/0/1\">Franck Dernoncourt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1\">Trung Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_K/0/1/0/all/0/1\">Kyomin Jung</a>",
          "description": "Despite the success of various text generation metrics such as BERTScore, it\nis still difficult to evaluate the image captions without enough reference\ncaptions due to the diversity of the descriptions. In this paper, we introduce\na new metric UMIC, an Unreferenced Metric for Image Captioning which does not\nrequire reference captions to evaluate image captions. Based on\nVision-and-Language BERT, we train UMIC to discriminate negative captions via\ncontrastive learning. Also, we observe critical problems of the previous\nbenchmark dataset (i.e., human annotations) on image captioning metric, and\nintroduce a new collection of human annotations on the generated captions. We\nvalidate UMIC on four datasets, including our new dataset, and show that UMIC\nhas a higher correlation than all previous metrics that require multiple\nreferences. We release the benchmark dataset and pre-trained models to compute\nthe UMIC.",
          "link": "http://arxiv.org/abs/2106.14019",
          "publishedOn": "2021-06-29T01:55:14.142Z",
          "wordCount": 585,
          "title": "UMIC: An Unreferenced Metric for Image Captioning via Contrastive Learning. (arXiv:2106.14019v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13963",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Anukriti Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_K/0/1/0/all/0/1\">Kartikeya Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sujit_P/0/1/0/all/0/1\">P.B. Sujit</a>",
          "description": "We present OffRoadTranSeg, the first end-to-end framework for semi-supervised\nsegmentation in unstructured outdoor environment using transformers and\nautomatic data selection for labelling. The offroad segmentation is a scene\nunderstanding approach that is widely used in autonomous driving. The popular\noffroad segmentation method is to use fully connected convolution layers and\nlarge labelled data, however, due to class imbalance, there will be several\nmismatches and also some classes may not be detected. Our approach is to do the\ntask of offroad segmentation in a semi-supervised manner. The aim is to provide\na model where self supervised vision transformer is used to fine-tune offroad\ndatasets with self-supervised data collection for labelling using depth\nestimation. The proposed method is validated on RELLIS-3D and RUGD offroad\ndatasets. The experiments show that OffRoadTranSeg outperformed other state of\nthe art models, and also solves the RELLIS-3D class imbalance problem.",
          "link": "http://arxiv.org/abs/2106.13963",
          "publishedOn": "2021-06-29T01:55:14.136Z",
          "wordCount": 581,
          "title": "OffRoadTranSeg: Semi-Supervised Segmentation using Transformers on OffRoad environments. (arXiv:2106.13963v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14082",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bendre_N/0/1/0/all/0/1\">Nihar Bendre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Desai_K/0/1/0/all/0/1\">Kevin Desai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Najafirad_P/0/1/0/all/0/1\">Peyman Najafirad</a>",
          "description": "With the ever-increasing amount of data, the central challenge in multimodal\nlearning involves limitations of labelled samples. For the task of\nclassification, techniques such as meta-learning, zero-shot learning, and\nfew-shot learning showcase the ability to learn information about novel classes\nbased on prior knowledge. Recent techniques try to learn a cross-modal mapping\nbetween the semantic space and the image space. However, they tend to ignore\nthe local and global semantic knowledge. To overcome this problem, we propose a\nMultimodal Variational Auto-Encoder (M-VAE) which can learn the shared latent\nspace of image features and the semantic space. In our approach we concatenate\nmultimodal data to a single embedding before passing it to the VAE for learning\nthe latent space. We propose the use of a multi-modal loss during the\nreconstruction of the feature embedding through the decoder. Our approach is\ncapable to correlating modalities and exploit the local and global semantic\nknowledge for novel sample predictions. Our experimental results using a MLP\nclassifier on four benchmark datasets show that our proposed model outperforms\nthe current state-of-the-art approaches for generalized zero-shot learning.",
          "link": "http://arxiv.org/abs/2106.14082",
          "publishedOn": "2021-06-29T01:55:14.121Z",
          "wordCount": 630,
          "title": "Generalized Zero-Shot Learning using Multimodal Variational Auto-Encoder with Semantic Concepts. (arXiv:2106.14082v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13920",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Afifi_M/0/1/0/all/0/1\">Mahmoud Afifi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abuolaim_A/0/1/0/all/0/1\">Abdullah Abuolaim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hussien_M/0/1/0/all/0/1\">Mostafa Hussien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brubaker_M/0/1/0/all/0/1\">Marcus A. Brubaker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brown_M/0/1/0/all/0/1\">Michael S. Brown</a>",
          "description": "Image style transfer aims to manipulate the appearance of a source image, or\n\"content\" image, to share similar texture and colors of a target \"style\" image.\nIdeally, the style transfer manipulation should also preserve the semantic\ncontent of the source image. A commonly used approach to assist in transferring\nstyles is based on Gram matrix optimization. One problem of Gram matrix-based\noptimization is that it does not consider the correlation between colors and\ntheir styles. Specifically, certain textures or structures should be associated\nwith specific colors. This is particularly challenging when the target style\nimage exhibits multiple style types. In this work, we propose a color-aware\nmulti-style transfer method that generates aesthetically pleasing results while\npreserving the style-color correlation between style and generated images. We\nachieve this desired outcome by introducing a simple but efficient modification\nto classic Gram matrix-based style transfer optimization. A nice feature of our\nmethod is that it enables the users to manually select the color associations\nbetween the target style and content image for more transfer flexibility. We\nvalidated our method with several qualitative comparisons, including a user\nstudy conducted with 30 participants. In comparison with prior work, our method\nis simple, easy to implement, and achieves visually appealing results when\ntargeting images that have multiple styles. Source code is available at\nhttps://github.com/mahmoudnafifi/color-aware-style-transfer.",
          "link": "http://arxiv.org/abs/2106.13920",
          "publishedOn": "2021-06-29T01:55:14.116Z",
          "wordCount": 652,
          "title": "CAMS: Color-Aware Multi-Style Transfer. (arXiv:2106.13920v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13933",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cao_A/0/1/0/all/0/1\">Ang Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johnson_J/0/1/0/all/0/1\">Justin Johnson</a>",
          "description": "As a core problem in computer vision, the performance of object detection has\nimproved drastically in the past few years. Despite their impressive\nperformance, object detectors suffer from a lack of interpretability.\nVisualization techniques have been developed and widely applied to introspect\nthe decisions made by other kinds of deep learning models; however, visualizing\nobject detectors has been underexplored. In this paper, we propose using\ninversion as a primary tool to understand modern object detectors and develop\nan optimization-based approach to layout inversion, allowing us to generate\nsynthetic images recognized by trained detectors as containing a desired\nconfiguration of objects. We reveal intriguing properties of detectors by\napplying our layout inversion technique to a variety of modern object\ndetectors, and further investigate them via validation experiments: they rely\non qualitatively different features for classification and regression; they\nlearn canonical motifs of commonly co-occurring objects; they use diff erent\nvisual cues to recognize objects of varying sizes. We hope our insights can\nhelp practitioners improve object detectors.",
          "link": "http://arxiv.org/abs/2106.13933",
          "publishedOn": "2021-06-29T01:55:14.109Z",
          "wordCount": 593,
          "title": "Inverting and Understanding Object Detectors. (arXiv:2106.13933v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13884",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tsimpoukelli_M/0/1/0/all/0/1\">Maria Tsimpoukelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menick_J/0/1/0/all/0/1\">Jacob Menick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cabi_S/0/1/0/all/0/1\">Serkan Cabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eslami_S/0/1/0/all/0/1\">S.M. Ali Eslami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vinyals_O/0/1/0/all/0/1\">Oriol Vinyals</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hill_F/0/1/0/all/0/1\">Felix Hill</a>",
          "description": "When trained at sufficient scale, auto-regressive language models exhibit the\nnotable ability to learn a new language task after being prompted with just a\nfew examples. Here, we present a simple, yet effective, approach for\ntransferring this few-shot learning ability to a multimodal setting (vision and\nlanguage). Using aligned image and caption data, we train a vision encoder to\nrepresent each image as a sequence of continuous embeddings, such that a\npre-trained, frozen language model prompted with this prefix generates the\nappropriate caption. The resulting system is a multimodal few-shot learner,\nwith the surprising ability to learn a variety of new tasks when conditioned on\nexamples, represented as a sequence of multiple interleaved image and text\nembeddings. We demonstrate that it can rapidly learn words for new objects and\nnovel visual categories, do visual question-answering with only a handful of\nexamples, and make use of outside knowledge, by measuring a single model on a\nvariety of established and new benchmarks.",
          "link": "http://arxiv.org/abs/2106.13884",
          "publishedOn": "2021-06-29T01:55:14.097Z",
          "wordCount": 608,
          "title": "Multimodal Few-Shot Learning with Frozen Language Models. (arXiv:2106.13884v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13863",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Melnyk_P/0/1/0/all/0/1\">Pavlo Melnyk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Felsberg_M/0/1/0/all/0/1\">Michael Felsberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wadenback_M/0/1/0/all/0/1\">M&#xe5;rten Wadenb&#xe4;ck</a>",
          "description": "Emerging from low-level vision theory, steerable filters found their\ncounterpart in deep learning. Earlier works used the steering theorems and\npresented convolutional networks equivariant to rigid transformations. In our\nwork, we propose a steerable feed-forward learning-based approach that consists\nof spherical decision surfaces and operates on point clouds. Due to the\ninherent geometric 3D structure of our theory, we derive a 3D steerability\nconstraint for its atomic parts, the hypersphere neurons. Exploiting the\nrotational equivariance, we show how the model parameters are fully steerable\nat inference time. The proposed spherical filter banks enable to make\nequivariant and, after online optimization, invariant class predictions for\nknown synthetic point sets in unknown orientations.",
          "link": "http://arxiv.org/abs/2106.13863",
          "publishedOn": "2021-06-29T01:55:14.063Z",
          "wordCount": 543,
          "title": "Fully Steerable 3D Spherical Neurons. (arXiv:2106.13863v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13880",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kang_Z/0/1/0/all/0/1\">Zhao Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hongfei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiangxin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaofeng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_L/0/1/0/all/0/1\">Ling Tian</a>",
          "description": "Principal Component Analysis (PCA) has been widely used for dimensionality\nreduction and feature extraction. Robust PCA (RPCA), under different robust\ndistance metrics, such as l1-norm and l2, p-norm, can deal with noise or\noutliers to some extent. However, real-world data may display structures that\ncan not be fully captured by these simple functions. In addition, existing\nmethods treat complex and simple samples equally. By contrast, a learning\npattern typically adopted by human beings is to learn from simple to complex\nand less to more. Based on this principle, we propose a novel method called\nSelf-paced PCA (SPCA) to further reduce the effect of noise and outliers.\nNotably, the complexity of each sample is calculated at the beginning of each\niteration in order to integrate samples from simple to more complex into\ntraining. Based on an alternating optimization, SPCA finds an optimal\nprojection matrix and filters out outliers iteratively. Theoretical analysis is\npresented to show the rationality of SPCA. Extensive experiments on popular\ndata sets demonstrate that the proposed method can improve the state of-the-art\nresults considerably.",
          "link": "http://arxiv.org/abs/2106.13880",
          "publishedOn": "2021-06-29T01:55:14.057Z",
          "wordCount": 617,
          "title": "Self-paced Principal Component Analysis. (arXiv:2106.13880v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13842",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nimi_S/0/1/0/all/0/1\">Sumaiya Tabassum Nimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arefeen_M/0/1/0/all/0/1\">Md Adnan Arefeen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uddin_M/0/1/0/all/0/1\">Md Yusuf Sarwar Uddin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yugyung Lee</a>",
          "description": "Collaborative inference enables resource-constrained edge devices to make\ninferences by uploading inputs (e.g., images) to a server (i.e., cloud) where\nthe heavy deep learning models run. While this setup works cost-effectively for\nsuccessful inferences, it severely underperforms when the model faces input\nsamples on which the model was not trained (known as Out-of-Distribution (OOD)\nsamples). If the edge devices could, at least, detect that an input sample is\nan OOD, that could potentially save communication and computation resources by\nnot uploading those inputs to the server for inference workload. In this paper,\nwe propose a novel lightweight OOD detection approach that mines important\nfeatures from the shallow layers of a pretrained CNN model and detects an input\nsample as ID (In-Distribution) or OOD based on a distance function defined on\nthe reduced feature space. Our technique (a) works on pretrained models without\nany retraining of those models, and (b) does not expose itself to any OOD\ndataset (all detection parameters are obtained from the ID training dataset).\nTo this end, we develop EARLIN (EARLy OOD detection for Collaborative\nINference) that takes a pretrained model and partitions the model at the OOD\ndetection layer and deploys the considerably small OOD part on an edge device\nand the rest on the cloud. By experimenting using real datasets and a prototype\nimplementation, we show that our technique achieves better results than other\napproaches in terms of overall accuracy and cost when tested against popular\nOOD datasets on top of popular deep learning models pretrained on benchmark\ndatasets.",
          "link": "http://arxiv.org/abs/2106.13842",
          "publishedOn": "2021-06-29T01:55:14.050Z",
          "wordCount": 702,
          "title": "EARLIN: Early Out-of-Distribution Detection for Resource-efficient Collaborative Inference. (arXiv:2106.13842v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14070",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Morgan_A/0/1/0/all/0/1\">Andrew S. Morgan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_B/0/1/0/all/0/1\">Bowen Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1\">Junchi Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boularias_A/0/1/0/all/0/1\">Abdeslam Boularias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dollar_A/0/1/0/all/0/1\">Aaron M. Dollar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bekris_K/0/1/0/all/0/1\">Kostas Bekris</a>",
          "description": "Highly constrained manipulation tasks continue to be challenging for\nautonomous robots as they require high levels of precision, typically less than\n1mm, which is often incompatible with what can be achieved by traditional\nperception systems. This paper demonstrates that the combination of\nstate-of-the-art object tracking with passively adaptive mechanical hardware\ncan be leveraged to complete precision manipulation tasks with tight,\nindustrially-relevant tolerances (0.25mm). The proposed control method closes\nthe loop through vision by tracking the relative 6D pose of objects in the\nrelevant workspace. It adjusts the control reference of both the compliant\nmanipulator and the hand to complete object insertion tasks via within-hand\nmanipulation. Contrary to previous efforts for insertion, our method does not\nrequire expensive force sensors, precision manipulators, or time-consuming,\nonline learning, which is data hungry. Instead, this effort leverages\nmechanical compliance and utilizes an object agnostic manipulation model of the\nhand learned offline, off-the-shelf motion planning, and an RGBD-based object\ntracker trained solely with synthetic data. These features allow the proposed\nsystem to easily generalize and transfer to new tasks and environments. This\npaper describes in detail the system components and showcases its efficacy with\nextensive experiments involving tight tolerance peg-in-hole insertion tasks of\nvarious geometries as well as open-world constrained placement tasks.",
          "link": "http://arxiv.org/abs/2106.14070",
          "publishedOn": "2021-06-29T01:55:14.043Z",
          "wordCount": 658,
          "title": "Vision-driven Compliant Manipulation for Reliable, High-Precision Assembly Tasks. (arXiv:2106.14070v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13948",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Francis_J/0/1/0/all/0/1\">Jonathan Francis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kitamura_N/0/1/0/all/0/1\">Nariaki Kitamura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Labelle_F/0/1/0/all/0/1\">Felix Labelle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xiaopeng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navarro_I/0/1/0/all/0/1\">Ingrid Navarro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1\">Jean Oh</a>",
          "description": "Recent advances in the areas of multimodal machine learning and artificial\nintelligence (AI) have led to the development of challenging tasks at the\nintersection of Computer Vision, Natural Language Processing, and Embodied AI.\nWhereas many approaches and previous survey pursuits have characterised one or\ntwo of these dimensions, there has not been a holistic analysis at the center\nof all three. Moreover, even when combinations of these topics are considered,\nmore focus is placed on describing, e.g., current architectural methods, as\nopposed to also illustrating high-level challenges and opportunities for the\nfield. In this survey paper, we discuss Embodied Vision-Language Planning\n(EVLP) tasks, a family of prominent embodied navigation and manipulation\nproblems that jointly use computer vision and natural language. We propose a\ntaxonomy to unify these tasks and provide an in-depth analysis and comparison\nof the new and current algorithmic approaches, metrics, simulated environments,\nas well as the datasets used for EVLP tasks. Finally, we present the core\nchallenges that we believe new EVLP works should seek to address, and we\nadvocate for task construction that enables model generalizability and furthers\nreal-world deployment.",
          "link": "http://arxiv.org/abs/2106.13948",
          "publishedOn": "2021-06-29T01:55:14.024Z",
          "wordCount": 628,
          "title": "Core Challenges in Embodied Vision-Language Planning. (arXiv:2106.13948v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13967",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vasileiou_V/0/1/0/all/0/1\">Vasiliki I. Vasileiou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kardaris_N/0/1/0/all/0/1\">Nikolaos Kardaris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maragos_P/0/1/0/all/0/1\">Petros Maragos</a>",
          "description": "Nowadays, the interaction between humans and robots is constantly expanding,\nrequiring more and more human motion recognition applications to operate in\nreal time. However, most works on temporal action detection and recognition\nperform these tasks in offline manner, i.e. temporally segmented videos are\nclassified as a whole. In this paper, based on the recently proposed framework\nof Temporal Recurrent Networks, we explore how temporal context and human\nmovement dynamics can be effectively employed for online action detection. Our\napproach uses various state-of-the-art architectures and appropriately combines\nthe extracted features in order to improve action detection. We evaluate our\nmethod on a challenging but widely used dataset for temporal action\nlocalization, THUMOS'14. Our experiments show significant improvement over the\nbaseline method, achieving state-of-the art results on THUMOS'14.",
          "link": "http://arxiv.org/abs/2106.13967",
          "publishedOn": "2021-06-29T01:55:14.017Z",
          "wordCount": 579,
          "title": "Exploring Temporal Context and Human Movement Dynamics for Online Action Detection in Videos. (arXiv:2106.13967v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14190",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gowdra_N/0/1/0/all/0/1\">Nidhi Gowdra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sinha_R/0/1/0/all/0/1\">Roopak Sinha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+MacDonell_S/0/1/0/all/0/1\">Stephen MacDonell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_W/0/1/0/all/0/1\">Wei Qi Yan</a>",
          "description": "Convolutional Neural Networks (CNNs) such as ResNet-50, DenseNet-40 and\nResNeXt-56 are severely over-parameterized, necessitating a consequent increase\nin the computational resources required for model training which scales\nexponentially for increments in model depth. In this paper, we propose an\nEntropy-Based Convolutional Layer Estimation (EBCLE) heuristic which is robust\nand simple, yet effective in resolving the problem of over-parameterization\nwith regards to network depth of CNN model. The EBCLE heuristic employs a\npriori knowledge of the entropic data distribution of input datasets to\ndetermine an upper bound for convolutional network depth, beyond which identity\ntransformations are prevalent offering insignificant contributions for\nenhancing model performance. Restricting depth redundancies by forcing feature\ncompression and abstraction restricts over-parameterization while decreasing\ntraining time by 24.99% - 78.59% without degradation in model performance. We\npresent empirical evidence to emphasize the relative effectiveness of broader,\nyet shallower models trained using the EBCLE heuristic, which maintains or\noutperforms baseline classification accuracies of narrower yet deeper models.\nThe EBCLE heuristic is architecturally agnostic and EBCLE based CNN models\nrestrict depth redundancies resulting in enhanced utilization of the available\ncomputational resources. The proposed EBCLE heuristic is a compelling technique\nfor researchers to analytically justify their HyperParameter (HP) choices for\nCNNs. Empirical validation of the EBCLE heuristic in training CNN models was\nestablished on five benchmarking datasets (ImageNet32, CIFAR-10/100, STL-10,\nMNIST) and four network architectures (DenseNet, ResNet, ResNeXt and\nEfficientNet B0-B2) with appropriate statistical tests employed to infer any\nconclusive claims presented in this paper.",
          "link": "http://arxiv.org/abs/2106.14190",
          "publishedOn": "2021-06-29T01:55:14.012Z",
          "wordCount": 721,
          "title": "Mitigating severe over-parameterization in deep convolutional neural networks through forced feature abstraction and compression with an entropy-based heuristic. (arXiv:2106.14190v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13953",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jo_C/0/1/0/all/0/1\">Changho Jo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Im_W/0/1/0/all/0/1\">Woobin Im</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1\">Sung-Eui Yoon</a>",
          "description": "In computer vision, recovering spatial information by filling in masked\nregions, e.g., inpainting, has been widely investigated for its usability and\nwide applicability to other various applications: image inpainting, image\nextrapolation, and environment map estimation. Most of them are studied\nseparately depending on the applications. Our focus, however, is on\naccommodating the opposite task, e.g., image outpainting, which would benefit\nthe target applications, e.g., image inpainting. Our self-supervision method,\nIn-N-Out, is summarized as a training approach that leverages the knowledge of\nthe opposite task into the target model. We empirically show that In-N-Out --\nwhich explores the complementary information -- effectively takes advantage\nover the traditional pipelines where only task-specific learning takes place in\ntraining. In experiments, we compare our method to the traditional procedure\nand analyze the effectiveness of our method on different applications: image\ninpainting, image extrapolation, and environment map estimation. For these\ntasks, we demonstrate that In-N-Out consistently improves the performance of\nthe recent works with In-N-Out self-supervision to their training procedure.\nAlso, we show that our approach achieves better results than an existing\ntraining approach for outpainting.",
          "link": "http://arxiv.org/abs/2106.13953",
          "publishedOn": "2021-06-29T01:55:14.003Z",
          "wordCount": 623,
          "title": "In-N-Out: Towards Good Initialization for Inpainting and Outpainting. (arXiv:2106.13953v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14033",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1\">Xinyi Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xiang_T/0/1/0/all/0/1\">Tiange Xiang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_C/0/1/0/all/0/1\">Chaoyi Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Song_Y/0/1/0/all/0/1\">Yang Song</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_D/0/1/0/all/0/1\">Dongnan Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_H/0/1/0/all/0/1\">Heng Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cai_W/0/1/0/all/0/1\">Weidong Cai</a>",
          "description": "The recurrent mechanism has recently been introduced into U-Net in various\nmedical image segmentation tasks. Existing studies have focused on promoting\nnetwork recursion via reusing building blocks. Although network parameters\ncould be greatly saved, computational costs still increase inevitably in\naccordance with the pre-set iteration time. In this work, we study a\nmulti-scale upgrade of a bi-directional skip connected network and then\nautomatically discover an efficient architecture by a novel two-phase Neural\nArchitecture Search (NAS) algorithm, namely BiX-NAS. Our proposed method\nreduces the network computational cost by sifting out ineffective multi-scale\nfeatures at different levels and iterations. We evaluate BiX-NAS on two\nsegmentation tasks using three different medical image datasets, and the\nexperimental results show that our BiX-NAS searched architecture achieves the\nstate-of-the-art performance with significantly lower computational cost.",
          "link": "http://arxiv.org/abs/2106.14033",
          "publishedOn": "2021-06-29T01:55:13.988Z",
          "wordCount": 584,
          "title": "BiX-NAS: Searching Efficient Bi-directional Architecture for Medical Image Segmentation. (arXiv:2106.14033v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14073",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1\">Zhicheng Cai</a>",
          "description": "Traditionally, CNN models possess hierarchical structures and utilize the\nfeature mapping of the last layer to obtain the prediction output. However, it\ncan be difficulty to settle the optimal network depth and make the middle\nlayers learn distinguished features. This paper proposes the Interflow\nalgorithm specially for traditional CNN models. Interflow divides CNNs into\nseveral stages according to the depth and makes predictions by the feature\nmappings in each stage. Subsequently, we input these prediction branches into a\nwell-designed attention module, which learns the weights of these prediction\nbranches, aggregates them and obtains the final output. Interflow weights and\nfuses the features learned in both shallower and deeper layers, making the\nfeature information at each stage processed reasonably and effectively,\nenabling the middle layers to learn more distinguished features, and enhancing\nthe model representation ability. In addition, Interflow can alleviate gradient\nvanishing problem, lower the difficulty of network depth selection, and lighten\npossible over-fitting problem by introducing attention mechanism. Besides, it\ncan avoid network degradation as a byproduct. Compared with the original model,\nthe CNN model with Interflow achieves higher test accuracy on multiple\nbenchmark datasets.",
          "link": "http://arxiv.org/abs/2106.14073",
          "publishedOn": "2021-06-29T01:55:13.979Z",
          "wordCount": 621,
          "title": "Interflow: Aggregating Multi-layer Feature Mappings with Attention Mechanism. (arXiv:2106.14073v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14008",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhihua Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dingquan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1\">Kede Ma</a>",
          "description": "Ensemble methods are generally regarded to be better than a single model if\nthe base learners are deemed to be \"accurate\" and \"diverse.\" Here we\ninvestigate a semi-supervised ensemble learning strategy to produce\ngeneralizable blind image quality assessment models. We train a multi-head\nconvolutional network for quality prediction by maximizing the accuracy of the\nensemble (as well as the base learners) on labeled data, and the disagreement\n(i.e., diversity) among them on unlabeled data, both implemented by the\nfidelity loss. We conduct extensive experiments to demonstrate the advantages\nof employing unlabeled data for BIQA, especially in model generalization and\nfailure identification.",
          "link": "http://arxiv.org/abs/2106.14008",
          "publishedOn": "2021-06-29T01:55:13.973Z",
          "wordCount": 552,
          "title": "Semi-Supervised Deep Ensembles for Blind Image Quality Assessment. (arXiv:2106.14008v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13870",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tsuei_S/0/1/0/all/0/1\">Stephanie Tsuei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Golatkar_A/0/1/0/all/0/1\">Aditya Golatkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1\">Stefano Soatto</a>",
          "description": "We propose a method to estimate the uncertainty of the outcome of an image\nclassifier on a given input datum. Deep neural networks commonly used for image\nclassification are deterministic maps from an input image to an output class.\nAs such, their outcome on a given datum involves no uncertainty, so we must\nspecify what variability we are referring to when defining, measuring and\ninterpreting \"confidence.\" To this end, we introduce the Wellington Posterior,\nwhich is the distribution of outcomes that would have been obtained in response\nto data that could have been generated by the same scene that produced the\ngiven image. Since there are infinitely many scenes that could have generated\nthe given image, the Wellington Posterior requires induction from scenes other\nthan the one portrayed. We explore alternate methods using data augmentation,\nensembling, and model linearization. Additional alternatives include generative\nadversarial networks, conditional prior networks, and supervised single-view\nreconstruction. We test these alternatives against the empirical posterior\nobtained by inferring the class of temporally adjacent frames in a video. These\ndevelopments are only a small step towards assessing the reliability of deep\nnetwork classifiers in a manner that is compatible with safety-critical\napplications.",
          "link": "http://arxiv.org/abs/2106.13870",
          "publishedOn": "2021-06-29T01:55:13.960Z",
          "wordCount": 649,
          "title": "Scene Uncertainty and the Wellington Posterior of Deterministic Image Classifiers. (arXiv:2106.13870v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13864",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Thao_N/0/1/0/all/0/1\">Nguyen Hieu Thao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soloviev_O/0/1/0/all/0/1\">Oleg Soloviev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noom_J/0/1/0/all/0/1\">Jacques Noom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verhaegen_M/0/1/0/all/0/1\">Michel Verhaegen</a>",
          "description": "We propose and study the single-frame anisoplanatic deconvolution problem\nassociated with image classification using machine learning algorithms, named\nthe nonuniform defocus removal (NDR) problem. Mathematical analysis of the NDR\nproblem is done and the so-called defocus removal (DR) algorithm for solving it\nis proposed. Global convergence of the DR algorithm is established without\nimposing any unverifiable assumption. Numerical results on simulation data show\nsignificant features of DR including solvability, noise robustness,\nconvergence, model insensitivity and computational efficiency. Physical\nrelevance of the NDR problem and practicability of the DR algorithm are tested\non experimental data. Back to the application that originally motivated the\ninvestigation of the NDR problem, we show that the DR algorithm can improve the\naccuracy of classifying distorted images using convolutional neural networks.\nThe key difference of this paper compared to most existing works on\nsingle-frame anisoplanatic deconvolution is that the new method does not\nrequire the data image to be decomposable into isoplanatic subregions.\nTherefore, solution approaches partitioning the image into isoplanatic zones\nare not applicable to the NDR problem and those handling the entire image such\nas the DR algorithm need to be developed and analyzed.",
          "link": "http://arxiv.org/abs/2106.13864",
          "publishedOn": "2021-06-29T01:55:13.936Z",
          "wordCount": 637,
          "title": "Nonuniform Defocus Removal for Image Classification. (arXiv:2106.13864v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13952",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Di Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_B/0/1/0/all/0/1\">Bo Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liangpei Zhang</a>",
          "description": "In this paper, we propose a spectral-spatial graph reasoning network (SSGRN)\nfor hyperspectral image (HSI) classification. Concretely, this network contains\ntwo parts that separately named spatial graph reasoning subnetwork (SAGRN) and\nspectral graph reasoning subnetwork (SEGRN) to capture the spatial and spectral\ngraph contexts, respectively. Different from the previous approaches\nimplementing superpixel segmentation on the original image or attempting to\nobtain the category features under the guide of label image, we perform the\nsuperpixel segmentation on intermediate features of the network to adaptively\nproduce the homogeneous regions to get the effective descriptors. Then, we\nadopt a similar idea in spectral part that reasonably aggregating the channels\nto generate spectral descriptors for spectral graph contexts capturing. All\ngraph reasoning procedures in SAGRN and SEGRN are achieved through graph\nconvolution. To guarantee the global perception ability of the proposed\nmethods, all adjacent matrices in graph reasoning are obtained with the help of\nnon-local self-attention mechanism. At last, by combining the extracted spatial\nand spectral graph contexts, we obtain the SSGRN to achieve a high accuracy\nclassification. Extensive quantitative and qualitative experiments on three\npublic HSI benchmarks demonstrate the competitiveness of the proposed methods\ncompared with other state-of-the-art approaches.",
          "link": "http://arxiv.org/abs/2106.13952",
          "publishedOn": "2021-06-29T01:55:13.930Z",
          "wordCount": 641,
          "title": "Spectral-Spatial Graph Reasoning Network for Hyperspectral Image Classification. (arXiv:2106.13952v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14060",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Abbad_Z/0/1/0/all/0/1\">Zakariae Abbad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maliani_A/0/1/0/all/0/1\">Ahmed Drissi El Maliani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alaoui_S/0/1/0/all/0/1\">Said Ouatik El Alaoui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassouni_M/0/1/0/all/0/1\">Mohammed El Hassouni</a>",
          "description": "In this paper, we leverage the properties of non-Euclidean Geometry to define\nthe Geodesic distance (GD) on the space of statistical manifolds. The Geodesic\ndistance is a real and intuitive similarity measure that is a good alternative\nto the purely statistical and extensively used Kullback-Leibler divergence\n(KLD). Despite the effectiveness of the GD, a closed-form does not exist for\nmany manifolds, since the geodesic equations are hard to solve. This explains\nthat the major studies have been content to use numerical approximations.\nNevertheless, most of those do not take account of the manifold properties,\nwhich leads to a loss of information and thus to low performances. We propose\nan approximation of the Geodesic distance through a graph-based method. This\nlatter permits to well represent the structure of the statistical manifold, and\nrespects its geometrical properties. Our main aim is to compare the graph-based\napproximation to the state of the art approximations. Thus, the proposed\napproach is evaluated for two statistical manifolds, namely the Weibull\nmanifold and the Gamma manifold, considering the Content-Based Texture\nRetrieval application on different databases.",
          "link": "http://arxiv.org/abs/2106.14060",
          "publishedOn": "2021-06-29T01:55:13.923Z",
          "wordCount": 638,
          "title": "A Graph-based approach to derive the geodesic distance on Statistical manifolds: Application to Multimedia Information Retrieval. (arXiv:2106.14060v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13959",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Chatterjee_A/0/1/0/all/0/1\">Avishek Chatterjee</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Mazumder_S/0/1/0/all/0/1\">Satyaki Mazumder</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Das_K/0/1/0/all/0/1\">Koel Das</a>",
          "description": "In recent times, functional data analysis (FDA) has been successfully applied\nin the field of high dimensional data classification. In this paper, we present\na novel classification framework using functional data and classwise Principal\nComponent Analysis (PCA). Our proposed method can be used in high dimensional\ntime series data which typically suffers from small sample size problem. Our\nmethod extracts a piece wise linear functional feature space and is\nparticularly suitable for hard classification problems.The proposed framework\nconverts time series data into functional data and uses classwise functional\nPCA for feature extraction followed by classification using a Bayesian linear\nclassifier. We demonstrate the efficacy of our proposed method by applying it\nto both synthetic data sets and real time series data from diverse fields\nincluding but not limited to neuroscience, food science, medical sciences and\nchemometrics.",
          "link": "http://arxiv.org/abs/2106.13959",
          "publishedOn": "2021-06-29T01:55:13.916Z",
          "wordCount": 577,
          "title": "Functional Classwise Principal Component Analysis: A Novel Classification Framework. (arXiv:2106.13959v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14137",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Suzuki_R/0/1/0/all/0/1\">Riko Suzuki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yanaka_H/0/1/0/all/0/1\">Hitomi Yanaka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mineshima_K/0/1/0/all/0/1\">Koji Mineshima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bekki_D/0/1/0/all/0/1\">Daisuke Bekki</a>",
          "description": "This paper introduces a new video-and-language dataset with human actions for\nmultimodal logical inference, which focuses on intentional and aspectual\nexpressions that describe dynamic human actions. The dataset consists of 200\nvideos, 5,554 action labels, and 1,942 action triplets of the form <subject,\npredicate, object> that can be translated into logical semantic\nrepresentations. The dataset is expected to be useful for evaluating multimodal\ninference systems between videos and semantically complicated sentences\nincluding negation and quantification.",
          "link": "http://arxiv.org/abs/2106.14137",
          "publishedOn": "2021-06-29T01:55:13.898Z",
          "wordCount": 522,
          "title": "Building a Video-and-Language Dataset with Human Actions for Multimodal Logical Inference. (arXiv:2106.14137v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13982",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mendoza_A/0/1/0/all/0/1\">Arturo Mendoza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trullo_R/0/1/0/all/0/1\">Roger Trullo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wielhorski_Y/0/1/0/all/0/1\">Yanneck Wielhorski</a>",
          "description": "In this work we propose a novel and fully automated method for extracting the\nyarn geometrical features in woven composites so that a direct parametrization\nof the textile reinforcement is achieved (e.g., FE mesh). Thus, our aim is not\nonly to perform yarn segmentation from tomographic images but rather to provide\na complete descriptive modeling of the fabric. As such, this direct approach\nimproves on previous methods that use voxel-wise masks as intermediate\nrepresentations followed by re-meshing operations (yarn envelope estimation).\nThe proposed approach employs two deep neural network architectures (U-Net and\nMask RCNN). First, we train the U-Net to generate synthetic CT images from the\ncorresponding FE simulations. This allows to generate large quantities of\nannotated data without requiring costly manual annotations. This data is then\nused to train the Mask R-CNN, which is focused on predicting contour points\naround each of the yarns in the image. Experimental results show that our\nmethod is accurate and robust for performing yarn instance segmentation on CT\nimages, this is further validated by quantitative and qualitative analyses.",
          "link": "http://arxiv.org/abs/2106.13982",
          "publishedOn": "2021-06-29T01:55:13.881Z",
          "wordCount": 630,
          "title": "Descriptive Modeling of Textiles using FE Simulations and Deep Learning. (arXiv:2106.13982v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13974",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cortinhal_T/0/1/0/all/0/1\">Tiago Cortinhal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurnaz_F/0/1/0/all/0/1\">Fatih Kurnaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aksoy_E/0/1/0/all/0/1\">Eren Aksoy</a>",
          "description": "In this work, we present a simple yet effective framework to address the\ndomain translation problem between different sensor modalities with unique data\nformats. By relying only on the semantics of the scene, our modular generative\nframework can, for the first time, synthesize a panoramic color image from a\ngiven full 3D LiDAR point cloud. The framework starts with semantic\nsegmentation of the point cloud, which is initially projected onto a spherical\nsurface. The same semantic segmentation is applied to the corresponding camera\nimage. Next, our new conditional generative model adversarially learns to\ntranslate the predicted LiDAR segment maps to the camera image counterparts.\nFinally, generated image segments are processed to render the panoramic scene\nimages. We provide a thorough quantitative evaluation on the SemanticKitti\ndataset and show that our proposed framework outperforms other strong baseline\nmodels.\n\nOur source code is available at\nhttps://github.com/halmstad-University/TITAN-NET",
          "link": "http://arxiv.org/abs/2106.13974",
          "publishedOn": "2021-06-29T01:55:13.874Z",
          "wordCount": 585,
          "title": "Semantics-aware Multi-modal Domain Translation:From LiDAR Point Clouds to Panoramic Color Images. (arXiv:2106.13974v1 [cs.CV])"
        }
      ]
    },
    {
      "title": "cs.LG updates on arXiv.org",
      "feedUrl": "http://export.arxiv.org/rss/cs.LG",
      "siteUrl": "http://arxiv.org/",
      "articles": [
        {
          "id": "http://arxiv.org/abs/2103.14187",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Sean Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Dongwoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qing Wang</a>",
          "description": "Graph neural networks (GNNs) have been extensively studied for prediction\ntasks on graphs. As pointed out by recent studies, most GNNs assume local\nhomophily, i.e., strong similarities in local neighborhoods. This assumption\nhowever limits the generalizability power of GNNs. To address this limitation,\nwe propose a flexible GNN model, which is capable of handling any graphs\nwithout being restricted by their underlying homophily. At its core, this model\nadopts a node attention mechanism based on multiple learnable spectral filters;\ntherefore, the aggregation scheme is learned adaptively for each graph in the\nspectral domain. We evaluated the proposed model on node classification tasks\nover eight benchmark datasets. The proposed model is shown to generalize well\nto both homophilic and heterophilic graphs. Further, it outperforms all\nstate-of-the-art baselines on heterophilic graphs and performs comparably with\nthem on homophilic graphs.",
          "link": "http://arxiv.org/abs/2103.14187",
          "publishedOn": "2021-07-05T01:55:00.439Z",
          "wordCount": 614,
          "title": "Beyond Low-Pass Filters: Adaptive Feature Propagation on Graphs. (arXiv:2103.14187v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.03409",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pereira_G/0/1/0/all/0/1\">Gabriel Henrique de Almeida Pereira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fusioka_A/0/1/0/all/0/1\">Andr&#xe9; Minoro Fusioka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nassu_B/0/1/0/all/0/1\">Bogdan Tomoyuki Nassu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Minetto_R/0/1/0/all/0/1\">Rodrigo Minetto</a>",
          "description": "Active fire detection in satellite imagery is of critical importance to the\nmanagement of environmental conservation policies, supporting decision-making\nand law enforcement. This is a well established field, with many techniques\nbeing proposed over the years, usually based on pixel or region-level\ncomparisons involving sensor-specific thresholds and neighborhood statistics.\nIn this paper, we address the problem of active fire detection using deep\nlearning techniques. In recent years, deep learning techniques have been\nenjoying an enormous success in many fields, but their use for active fire\ndetection is relatively new, with open questions and demand for datasets and\narchitectures for evaluation. This paper addresses these issues by introducing\na new large-scale dataset for active fire detection, with over 150,000 image\npatches (more than 200 GB of data) extracted from Landsat-8 images captured\naround the world in August and September 2020, containing wildfires in several\nlocations. The dataset was split in two parts, and contains 10-band spectral\nimages with associated outputs, produced by three well known handcrafted\nalgorithms for active fire detection in the first part, and manually annotated\nmasks in the second part. We also present a study on how different\nconvolutional neural network architectures can be used to approximate these\nhandcrafted algorithms, and how models trained on automatically segmented\npatches can be combined to achieve better performance than the original\nalgorithms - with the best combination having 87.2% precision and 92.4% recall\non our manually annotated dataset. The proposed dataset, source codes and\ntrained models are available on Github\n(https://github.com/pereira-gha/activefire), creating opportunities for further\nadvances in the field",
          "link": "http://arxiv.org/abs/2101.03409",
          "publishedOn": "2021-07-05T01:55:00.423Z",
          "wordCount": 747,
          "title": "Active Fire Detection in Landsat-8 Imagery: a Large-Scale Dataset and a Deep-Learning Study. (arXiv:2101.03409v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.11743",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lavastida_T/0/1/0/all/0/1\">Thomas Lavastida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moseley_B/0/1/0/all/0/1\">Benjamin Moseley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravi_R/0/1/0/all/0/1\">R. Ravi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chenyang Xu</a>",
          "description": "We propose a new model for augmenting algorithms with predictions by\nrequiring that they are formally learnable and instance robust. Learnability\nensures that predictions can be efficiently constructed from a reasonable\namount of past data. Instance robustness ensures that the prediction is robust\nto modest changes in the problem input, where the measure of the change may be\nproblem specific. Instance robustness insists on a smooth degradation in\nperformance as a function of the change. Ideally, the performance is never\nworse than worst-case bounds. This also allows predictions to be objectively\ncompared.\n\nWe design online algorithms with predictions for a network flow allocation\nproblem and restricted assignment makespan minimization. For both problems, two\nkey properties are established: high quality predictions can be learned from a\nsmall sample of prior instances and these predictions are robust to errors that\nsmoothly degrade as the underlying problem instance changes.",
          "link": "http://arxiv.org/abs/2011.11743",
          "publishedOn": "2021-07-05T01:55:00.416Z",
          "wordCount": 622,
          "title": "Learnable and Instance-Robust Predictions for Online Matching, Flows and Load Balancing. (arXiv:2011.11743v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.04579",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Chabaud_U/0/1/0/all/0/1\">Ulysse Chabaud</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Markham_D/0/1/0/all/0/1\">Damian Markham</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Sohbi_A/0/1/0/all/0/1\">Adel Sohbi</a>",
          "description": "We study supervised learning algorithms in which a quantum device is used to\nperform a computational subroutine - either for prediction via probability\nestimation, or to compute a kernel via estimation of quantum states overlap. We\ndesign implementations of these quantum subroutines using Boson Sampling\narchitectures in linear optics, supplemented by adaptive measurements. We then\nchallenge these quantum algorithms by deriving classical simulation algorithms\nfor the tasks of output probability estimation and overlap estimation. We\nobtain different classical simulability regimes for these two computational\ntasks in terms of the number of adaptive measurements and input photons. In\nboth cases, our results set explicit limits to the range of parameters for\nwhich a quantum advantage can be envisaged with adaptive linear optics compared\nto classical machine learning algorithms: we show that the number of input\nphotons and the number of adaptive measurements cannot be simultaneously small\ncompared to the number of modes. Interestingly, our analysis leaves open the\npossibility of a near-term quantum advantage with a single adaptive\nmeasurement.",
          "link": "http://arxiv.org/abs/2102.04579",
          "publishedOn": "2021-07-05T01:55:00.393Z",
          "wordCount": 624,
          "title": "Quantum machine learning with adaptive linear optics. (arXiv:2102.04579v2 [quant-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.05389",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fisman_D/0/1/0/all/0/1\">Dana Fisman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frenkel_H/0/1/0/all/0/1\">Hadar Frenkel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zilles_S/0/1/0/all/0/1\">Sandra Zilles</a>",
          "description": "We revisit the complexity of procedures on SFAs (such as intersection,\nemptiness, etc.) and analyze them according to the measures we find suitable\nfor symbolic automata: the number of states, the maximal number of transitions\nexiting a state, and the size of the most complex transition predicate. We pay\nattention to the special forms of SFAs: {normalized SFAs} and {neat SFAs}, as\nwell as to SFAs over a {monotonic} effective Boolean algebra.",
          "link": "http://arxiv.org/abs/2011.05389",
          "publishedOn": "2021-07-05T01:55:00.384Z",
          "wordCount": 544,
          "title": "On the Complexity of Symbolic Finite-State Automata. (arXiv:2011.05389v3 [cs.FL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.03046",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hongbin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1\">Jinyuan Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_N/0/1/0/all/0/1\">Neil Zhenqiang Gong</a>",
          "description": "3D point cloud classification has many safety-critical applications such as\nautonomous driving and robotic grasping. However, several studies showed that\nit is vulnerable to adversarial attacks. In particular, an attacker can make a\nclassifier predict an incorrect label for a 3D point cloud via carefully\nmodifying, adding, and/or deleting a small number of its points. Randomized\nsmoothing is state-of-the-art technique to build certifiably robust 2D image\nclassifiers. However, when applied to 3D point cloud classification, randomized\nsmoothing can only certify robustness against adversarially modified points.\n\nIn this work, we propose PointGuard, the first defense that has provable\nrobustness guarantees against adversarially modified, added, and/or deleted\npoints. Specifically, given a 3D point cloud and an arbitrary point cloud\nclassifier, our PointGuard first creates multiple subsampled point clouds, each\nof which contains a random subset of the points in the original point cloud;\nthen our PointGuard predicts the label of the original point cloud as the\nmajority vote among the labels of the subsampled point clouds predicted by the\npoint cloud classifier. Our first major theoretical contribution is that we\nshow PointGuard provably predicts the same label for a 3D point cloud when the\nnumber of adversarially modified, added, and/or deleted points is bounded. Our\nsecond major theoretical contribution is that we prove the tightness of our\nderived bound when no assumptions on the point cloud classifier are made.\nMoreover, we design an efficient algorithm to compute our certified robustness\nguarantees. We also empirically evaluate PointGuard on ModelNet40 and ScanNet\nbenchmark datasets.",
          "link": "http://arxiv.org/abs/2103.03046",
          "publishedOn": "2021-07-05T01:55:00.377Z",
          "wordCount": 732,
          "title": "PointGuard: Provably Robust 3D Point Cloud Classification. (arXiv:2103.03046v2 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.01683",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Azzimonti_L/0/1/0/all/0/1\">Laura Azzimonti</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Corani_G/0/1/0/all/0/1\">Giorgio Corani</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Scutari_M/0/1/0/all/0/1\">Marco Scutari</a>",
          "description": "Score functions for learning the structure of Bayesian networks in the\nliterature assume that data are a homogeneous set of observations; whereas it\nis often the case that they comprise different related, but not homogeneous,\ndata sets collected in different ways. In this paper we propose a new Bayesian\nDirichlet score, which we call Bayesian Hierarchical Dirichlet (BHD). The\nproposed score is based on a hierarchical model that pools information across\ndata sets to learn a single encompassing network structure, while taking into\naccount the differences in their probabilistic structures. We derive a\nclosed-form expression for BHD using a variational approximation of the\nmarginal likelihood and we study its performance using simulated data. We find\nthat, when data comprise multiple related data sets, BHD outperforms the\nBayesian Dirichlet equivalent uniform (BDeu) score in terms of reconstruction\naccuracy as measured by the Structural Hamming distance, and that it is as\naccurate as BDeu when data are homogeneous. Moreover, the estimated networks\nare sparser and therefore more interpretable than those obtained with BDeu,\nthanks to a lower number of false positive arcs.",
          "link": "http://arxiv.org/abs/2008.01683",
          "publishedOn": "2021-07-05T01:55:00.359Z",
          "wordCount": 646,
          "title": "Structure Learning from Related Data Sets with a Hierarchical Bayesian Score. (arXiv:2008.01683v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.05138",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Emmenegger_N/0/1/0/all/0/1\">Nicolas Emmenegger</a>, <a href=\"http://arxiv.org/find/math/1/au:+Kyng_R/0/1/0/all/0/1\">Rasmus Kyng</a>, <a href=\"http://arxiv.org/find/math/1/au:+Zehmakan_A/0/1/0/all/0/1\">Ahad N. Zehmakan</a>",
          "description": "We prove lower bounds for higher-order methods in smooth non-convex\nfinite-sum optimization. Our contribution is threefold: We first show that a\ndeterministic algorithm cannot profit from the finite-sum structure of the\nobjective, and that simulating a pth-order regularized method on the whole\nfunction by constructing exact gradient information is optimal up to constant\nfactors. We further show lower bounds for randomized algorithms and compare\nthem with the best known upper bounds. To address some gaps between the bounds,\nwe propose a new second-order smoothness assumption that can be seen as an\nanalogue of the first-order mean-squared smoothness assumption. We prove that\nit is sufficient to ensure state-of-the-art convergence guarantees, while\nallowing for a sharper lower bound.",
          "link": "http://arxiv.org/abs/2103.05138",
          "publishedOn": "2021-07-05T01:55:00.332Z",
          "wordCount": 596,
          "title": "On the Oracle Complexity of Higher-Order Smooth Non-Convex Finite-Sum Optimization. (arXiv:2103.05138v2 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.13567",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nouri_M/0/1/0/all/0/1\">Mahbod Nouri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moradi_F/0/1/0/all/0/1\">Faraz Moradi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghaemi_H/0/1/0/all/0/1\">Hafez Ghaemi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nasrabadi_A/0/1/0/all/0/1\">Ali Motie Nasrabadi</a>",
          "description": "A conventional subject-dependent (SD) brain-computer interface (BCI) requires\na complete data-gathering, training, and calibration phase for each user before\nit can be used. In recent years, a number of subject-independent (SI) BCIs have\nbeen developed. However, there are many problems preventing them from being\nused in real-world BCI applications. A weaker performance compared to the\nsubject-dependent (SD) approach, and a relatively large model requiring high\ncomputational power are the most important ones. Therefore, a potential\nreal-world BCI would greatly benefit from a compact low-power\nsubject-independent BCI framework, ready to be used immediately after the user\nputs it on. To move towards this goal, we propose a novel subject-independent\nBCI framework named CCSPNet (Convolutional Common Spatial Pattern Network)\ntrained on the motor imagery (MI) paradigm of a large-scale\nelectroencephalography (EEG) signals database consisting of 21600 trials for 54\nsubjects performing two-class hand-movement MI tasks. The proposed framework\napplies a wavelet kernel convolutional neural network (WKCNN) and a temporal\nconvolutional neural network (TCNN) in order to represent and extract the\ndiverse spectral features of EEG signals. The outputs of the convolutional\nlayers go through a common spatial pattern (CSP) algorithm for spatial feature\nextraction. The number of CSP features is reduced by a dense neural network,\nand the final class label is determined by a linear discriminative analysis\n(LDA) classifier. The CCSPNet framework evaluation results show that it is\npossible to have a low-power compact BCI that achieves both SD and SI\nperformance comparable to complex and computationally expensive.",
          "link": "http://arxiv.org/abs/2012.13567",
          "publishedOn": "2021-07-05T01:55:00.326Z",
          "wordCount": 750,
          "title": "Towards Real-World BCI: CCSPNet, A Compact Subject-Independent Motor Imagery Framework. (arXiv:2012.13567v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.12391",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Anantrasirichai_N/0/1/0/all/0/1\">Nantheera Anantrasirichai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bull_D/0/1/0/all/0/1\">David Bull</a>",
          "description": "This paper reviews the current state of the art in Artificial Intelligence\n(AI) technologies and applications in the context of the creative industries. A\nbrief background of AI, and specifically Machine Learning (ML) algorithms, is\nprovided including Convolutional Neural Network (CNNs), Generative Adversarial\nNetworks (GANs), Recurrent Neural Networks (RNNs) and Deep Reinforcement\nLearning (DRL). We categorise creative applications into five groups related to\nhow AI technologies are used: i) content creation, ii) information analysis,\niii) content enhancement and post production workflows, iv) information\nextraction and enhancement, and v) data compression. We critically examine the\nsuccesses and limitations of this rapidly advancing technology in each of these\nareas. We further differentiate between the use of AI as a creative tool and\nits potential as a creator in its own right. We foresee that, in the near\nfuture, machine learning-based AI will be adopted widely as a tool or\ncollaborative assistant for creativity. In contrast, we observe that the\nsuccesses of machine learning in domains with fewer constraints, where AI is\nthe `creator', remain modest. The potential of AI (or its developers) to win\nawards for its original creations in competition with human creatives is also\nlimited, based on contemporary technologies. We therefore conclude that, in the\ncontext of creative industries, maximum benefit from AI will be derived where\nits focus is human centric -- where it is designed to augment, rather than\nreplace, human creativity.",
          "link": "http://arxiv.org/abs/2007.12391",
          "publishedOn": "2021-07-05T01:55:00.319Z",
          "wordCount": 746,
          "title": "Artificial Intelligence in the Creative Industries: A Review. (arXiv:2007.12391v6 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.06130",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_F/0/1/0/all/0/1\">Fei Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahn_S/0/1/0/all/0/1\">Sungjin Ahn</a>",
          "description": "A crucial ability of human intelligence is to build up models of individual\n3D objects from partial scene observations. Recent works achieve object-centric\ngeneration but without the ability to infer the representation, or achieve 3D\nscene representation learning but without object-centric compositionality.\nTherefore, learning to represent and render 3D scenes with object-centric\ncompositionality remains elusive. In this paper, we propose a probabilistic\ngenerative model for learning to build modular and compositional 3D object\nmodels from partial observations of a multi-object scene. The proposed model\ncan (i) infer the 3D object representations by learning to search and group\nobject areas and also (ii) render from an arbitrary viewpoint not only\nindividual objects but also the full scene by compositing the objects. The\nentire learning process is unsupervised and end-to-end. In experiments, in\naddition to generation quality, we also demonstrate that the learned\nrepresentation permits object-wise manipulation and novel scene generation, and\ngeneralizes to various settings. Results can be found on our project website:\nhttps://sites.google.com/view/roots3d",
          "link": "http://arxiv.org/abs/2006.06130",
          "publishedOn": "2021-07-05T01:55:00.310Z",
          "wordCount": 644,
          "title": "ROOTS: Object-Centric Representation and Rendering of 3D Scenes. (arXiv:2006.06130v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.04521",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fel_T/0/1/0/all/0/1\">Thomas Fel</a> (ANITI), <a href=\"http://arxiv.org/find/cs/1/au:+Vigouroux_D/0/1/0/all/0/1\">David Vigouroux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cadene_R/0/1/0/all/0/1\">R&#xe9;mi Cad&#xe8;ne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Serre_T/0/1/0/all/0/1\">Thomas Serre</a> (ANITI)",
          "description": "A plethora of methods have been proposed to explain howdeep neural networks\nreach a decision but comparativelylittle effort has been made to ensure that\nthe explanationsproduced by these methods are objectively relevant.\nWhiledesirable properties for a good explanation are easy to come,objective\nmeasures have been harder to derive. Here, we pro-pose two new measures to\nevaluate explanations borrowedfrom the field of algorithmic stability: relative\nconsistencyReCo and mean generalizability MeGe. We conduct severalexperiments\non multiple image datasets and network archi-tectures to demonstrate the\nbenefits of the proposed measuresover representative methods. We show that\npopular fidelitymeasures are not sufficient to guarantee good\nexplanations.Finally, we show empirically that 1-Lipschitz networks pro-vide\ngeneral and consistent explanations, regardless of theexplanation method used,\nmaking them a relevant directionfor explainability.",
          "link": "http://arxiv.org/abs/2009.04521",
          "publishedOn": "2021-07-05T01:55:00.293Z",
          "wordCount": 606,
          "title": "How good is your explanation? Algorithmic stability measures to assess the qualityof explanations for deep neural networks. (arXiv:2009.04521v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.08028",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Bae_J/0/1/0/all/0/1\">Joseph Bae</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Kapse_S/0/1/0/all/0/1\">Saarthak Kapse</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Singh_G/0/1/0/all/0/1\">Gagandeep Singh</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Gattu_R/0/1/0/all/0/1\">Rishabh Gattu</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Ali_S/0/1/0/all/0/1\">Syed Ali</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Shah_N/0/1/0/all/0/1\">Neal Shah</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Marshall_C/0/1/0/all/0/1\">Colin Marshall</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Pierce_J/0/1/0/all/0/1\">Jonathan Pierce</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Phatak_T/0/1/0/all/0/1\">Tej Phatak</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Gupta_A/0/1/0/all/0/1\">Amit Gupta</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Green_J/0/1/0/all/0/1\">Jeremy Green</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Madan_N/0/1/0/all/0/1\">Nikhil Madan</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Prasanna_P/0/1/0/all/0/1\">Prateek Prasanna</a>",
          "description": "We predict mechanical ventilation requirement and mortality using\ncomputational modeling of chest radiographs (CXRs) for coronavirus disease 2019\n(COVID-19) patients. This two-center, retrospective study analyzed 530\ndeidentified CXRs from 515 COVID-19 patients treated at Stony Brook University\nHospital and Newark Beth Israel Medical Center between March and August 2020.\nDL and machine learning classifiers to predict mechanical ventilation\nrequirement and mortality were trained and evaluated using patient CXRs. A\nnovel radiomic embedding framework was also explored for outcome prediction.\nAll results are compared against radiologist grading of CXRs (zone-wise expert\nseverity scores). Radiomic and DL classification models had mAUCs of\n0.78+/-0.02 and 0.81+/-0.04, compared with expert scores mAUCs of 0.75+/-0.02\nand 0.79+/-0.05 for mechanical ventilation requirement and mortality\nprediction, respectively. Combined classifiers using both radiomics and expert\nseverity scores resulted in mAUCs of 0.79+/-0.04 and 0.83+/-0.04 for each\nprediction task, demonstrating improvement over either artificial intelligence\nor radiologist interpretation alone. Our results also suggest instances where\ninclusion of radiomic features in DL improves model predictions, something that\nmight be explored in other pathologies. The models proposed in this study and\nthe prognostic information they provide might aid physician decision making and\nresource allocation during the COVID-19 pandemic.",
          "link": "http://arxiv.org/abs/2007.08028",
          "publishedOn": "2021-07-05T01:55:00.265Z",
          "wordCount": 757,
          "title": "Predicting Clinical Outcomes in COVID-19 using Radiomics and Deep Learning on Chest Radiographs: A Multi-Institutional Study. (arXiv:2007.08028v2 [q-bio.QM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.15951",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Iwana_B/0/1/0/all/0/1\">Brian Kenji Iwana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uchida_S/0/1/0/all/0/1\">Seiichi Uchida</a>",
          "description": "In recent times, deep artificial neural networks have achieved many successes\nin pattern recognition. Part of this success can be attributed to the reliance\non big data to increase generalization. However, in the field of time series\nrecognition, many datasets are often very small. One method of addressing this\nproblem is through the use of data augmentation. In this paper, we survey data\naugmentation techniques for time series and their application to time series\nclassification with neural networks. We propose a taxonomy and outline the four\nfamilies in time series data augmentation, including transformation-based\nmethods, pattern mixing, generative models, and decomposition methods.\nFurthermore, we empirically evaluate 12 time series data augmentation methods\non 128 time series classification datasets with six different types of neural\nnetworks. Through the results, we are able to analyze the characteristics,\nadvantages and disadvantages, and recommendations of each data augmentation\nmethod. This survey aims to help in the selection of time series data\naugmentation for neural network applications.",
          "link": "http://arxiv.org/abs/2007.15951",
          "publishedOn": "2021-07-05T01:55:00.255Z",
          "wordCount": 651,
          "title": "An Empirical Survey of Data Augmentation for Time Series Classification with Neural Networks. (arXiv:2007.15951v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.02898",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Ghalamkari_K/0/1/0/all/0/1\">Kazu Ghalamkari</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Sugiyama_M/0/1/0/all/0/1\">Mahito Sugiyama</a>",
          "description": "We present an efficient low-rank approximation algorithm for non-negative\ntensors. The algorithm is derived from our two findings: First, we show that\nrank-1 approximation for tensors can be viewed as a mean-field approximation by\ntreating each tensor as a probability distribution. Second, we theoretically\nprovide a sufficient condition for distribution parameters to reduce Tucker\nranks of tensors and, interestingly, this sufficient condition can be achieved\nby iterative application of the mean-field approximation. Since the mean-field\napproximation is always given as a closed formula, our findings lead to a fast\nlow-rank approximation algorithm without using a gradient method. We\nempirically demonstrate that our algorithm is faster than the existing\nnon-negative Tucker rank reduction methods with achieving competitive or better\napproximation of given tensors.",
          "link": "http://arxiv.org/abs/2103.02898",
          "publishedOn": "2021-07-05T01:55:00.248Z",
          "wordCount": 581,
          "title": "Fast Tucker Rank Reduction for Non-Negative Tensors Using Mean-Field Approximation. (arXiv:2103.02898v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1908.02725",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Sebbouh_O/0/1/0/all/0/1\">Othmane Sebbouh</a>, <a href=\"http://arxiv.org/find/math/1/au:+Gazagnadou_N/0/1/0/all/0/1\">Nidham Gazagnadou</a>, <a href=\"http://arxiv.org/find/math/1/au:+Jelassi_S/0/1/0/all/0/1\">Samy Jelassi</a>, <a href=\"http://arxiv.org/find/math/1/au:+Bach_F/0/1/0/all/0/1\">Francis Bach</a>, <a href=\"http://arxiv.org/find/math/1/au:+Gower_R/0/1/0/all/0/1\">Robert M. Gower</a>",
          "description": "Among the very first variance reduced stochastic methods for solving the\nempirical risk minimization problem was the SVRG method (Johnson & Zhang 2013).\nSVRG is an inner-outer loop based method, where in the outer loop a reference\nfull gradient is evaluated, after which $m \\in \\mathbb{N}$ steps of an inner\nloop are executed where the reference gradient is used to build a variance\nreduced estimate of the current gradient. The simplicity of the SVRG method and\nits analysis have led to multiple extensions and variants for even non-convex\noptimization. We provide a more general analysis of SVRG than had been\npreviously done by using arbitrary sampling, which allows us to analyse\nvirtually all forms of mini-batching through a single theorem. Furthermore, our\nanalysis is focused on more practical variants of SVRG including a new variant\nof the loopless SVRG (Hofman et al 2015, Kovalev et al 2019, Kulunchakov and\nMairal 2019) and a variant of k-SVRG (Raj and Stich 2018) where $m=n$ and where\n$n$ is the number of data points. Since our setup and analysis reflect what is\ndone in practice, we are able to set the parameters such as the mini-batch size\nand step size using our theory in such a way that produces a more efficient\nalgorithm in practice, as we show in extensive numerical experiments.",
          "link": "http://arxiv.org/abs/1908.02725",
          "publishedOn": "2021-07-05T01:55:00.242Z",
          "wordCount": 692,
          "title": "Towards closing the gap between the theory and practice of SVRG. (arXiv:1908.02725v2 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.04351",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alfarra_M/0/1/0/all/0/1\">Motasem Alfarra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bibi_A/0/1/0/all/0/1\">Adel Bibi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1\">Philip H. S. Torr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1\">Bernard Ghanem</a>",
          "description": "Randomized smoothing is a recent technique that achieves state-of-art\nperformance in training certifiably robust deep neural networks. While the\nsmoothing family of distributions is often connected to the choice of the norm\nused for certification, the parameters of these distributions are always set as\nglobal hyper parameters independent of the input data on which a network is\ncertified. In this work, we revisit Gaussian randomized smoothing and show that\nthe variance of the Gaussian distribution can be optimized at each input so as\nto maximize the certification radius for the construction of the smoothed\nclassifier. This new approach is generic, parameter-free, and easy to\nimplement. In fact, we show that our data dependent framework can be seamlessly\nincorporated into 3 randomized smoothing approaches, leading to consistent\nimproved certified accuracy. When this framework is used in the training\nroutine of these approaches followed by a data dependent certification, we\nachieve 9\\% and 6\\% improvement over the certified accuracy of the strongest\nbaseline for a radius of 0.5 on CIFAR10 and ImageNet.",
          "link": "http://arxiv.org/abs/2012.04351",
          "publishedOn": "2021-07-05T01:55:00.224Z",
          "wordCount": 633,
          "title": "Data Dependent Randomized Smoothing. (arXiv:2012.04351v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01105",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Bronskill_J/0/1/0/all/0/1\">John Bronskill</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Massiceti_D/0/1/0/all/0/1\">Daniela Massiceti</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Patacchiola_M/0/1/0/all/0/1\">Massimiliano Patacchiola</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Hofmann_K/0/1/0/all/0/1\">Katja Hofmann</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Nowozin_S/0/1/0/all/0/1\">Sebastian Nowozin</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Turner_R/0/1/0/all/0/1\">Richard E. Turner</a>",
          "description": "Meta learning approaches to few-shot classification are computationally\nefficient at test time requiring just a few optimization steps or single\nforward pass to learn a new task, but they remain highly memory-intensive to\ntrain. This limitation arises because a task's entire support set, which can\ncontain up to 1000 images, must be processed before an optimization step can be\ntaken. Harnessing the performance gains offered by large images thus requires\neither parallelizing the meta-learner across multiple GPUs, which may not be\navailable, or trade-offs between task and image size when memory constraints\napply. We improve on both options by proposing LITE, a general and memory\nefficient episodic training scheme that enables meta-training on large tasks\ncomposed of large images on a single GPU. We achieve this by observing that the\ngradients for a task can be decomposed into a sum of gradients over the task's\ntraining images. This enables us to perform a forward pass on a task's entire\ntraining set but realize significant memory savings by back-propagating only a\nrandom subset of these images which we show is an unbiased approximation of the\nfull gradient. We use LITE to train meta-learners and demonstrate new\nstate-of-the-art accuracy on the real-world ORBIT benchmark and 3 of the 4\nparts of the challenging VTAB+MD benchmark relative to leading meta-learners.\nLITE also enables meta-learners to be competitive with transfer learning\napproaches but at a fraction of the test-time computational cost, thus serving\nas a counterpoint to the recent narrative that transfer learning is all you\nneed for few-shot classification.",
          "link": "http://arxiv.org/abs/2107.01105",
          "publishedOn": "2021-07-05T01:55:00.209Z",
          "wordCount": 691,
          "title": "Memory Efficient Meta-Learning with Large Images. (arXiv:2107.01105v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2006.03774",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tann_W/0/1/0/all/0/1\">Wesley Joon-Wie Tann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_E/0/1/0/all/0/1\">Ee-Chien Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hooi_B/0/1/0/all/0/1\">Bryan Hooi</a>",
          "description": "We introduce the controllable graph generation problem, formulated as\ncontrolling graph attributes during the generative process to produce desired\ngraphs with understandable structures. Using a transparent and straightforward\nMarkov model to guide this generative process, practitioners can shape and\nunderstand the generated graphs. We propose ${\\rm S{\\small HADOW}C{\\small\nAST}}$, a generative model capable of controlling graph generation while\nretaining the original graph's intrinsic properties. The proposed model is\nbased on a conditional generative adversarial network. Given an observed graph\nand some user-specified Markov model parameters, ${\\rm S{\\small HADOW}C{\\small\nAST}}$ controls the conditions to generate desired graphs. Comprehensive\nexperiments on three real-world network datasets demonstrate our model's\ncompetitive performance in the graph generation task. Furthermore, we show its\neffective controllability by directing ${\\rm S{\\small HADOW}C{\\small AST}}$ to\ngenerate hypothetical scenarios with different graph structures.",
          "link": "http://arxiv.org/abs/2006.03774",
          "publishedOn": "2021-07-05T01:55:00.202Z",
          "wordCount": 610,
          "title": "SHADOWCAST: Controllable Graph Generation. (arXiv:2006.03774v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2004.01025",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gunasekar_S/0/1/0/all/0/1\">Suriya Gunasekar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woodworth_B/0/1/0/all/0/1\">Blake Woodworth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srebro_N/0/1/0/all/0/1\">Nathan Srebro</a>",
          "description": "We present a primal only derivation of Mirror Descent as a \"partial\"\ndiscretization of gradient flow on a Riemannian manifold where the metric\ntensor is the Hessian of the Mirror Descent potential. We contrast this\ndiscretization to Natural Gradient Descent, which is obtained by a \"full\"\nforward Euler discretization. This view helps shed light on the relationship\nbetween the methods and allows generalizing Mirror Descent to general\nRiemannian geometries, even when the metric tensor is {\\em not} a Hessian, and\nthus there is no \"dual.\"",
          "link": "http://arxiv.org/abs/2004.01025",
          "publishedOn": "2021-07-05T01:55:00.195Z",
          "wordCount": 567,
          "title": "Mirrorless Mirror Descent: A Natural Derivation of Mirror Descent. (arXiv:2004.01025v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.06952",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zweig_A/0/1/0/all/0/1\">Aaron Zweig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bruna_J/0/1/0/all/0/1\">Joan Bruna</a>",
          "description": "Symmetric functions, which take as input an unordered, fixed-size set, are\nknown to be universally representable by neural networks that enforce\npermutation invariance. These architectures only give guarantees for fixed\ninput sizes, yet in many practical applications, including point clouds and\nparticle physics, a relevant notion of generalization should include varying\nthe input size. In this work we treat symmetric functions (of any size) as\nfunctions over probability measures, and study the learning and representation\nof neural networks defined on measures. By focusing on shallow architectures,\nwe establish approximation and generalization bounds under different choices of\nregularization (such as RKHS and variation norms), that capture a hierarchy of\nfunctional spaces with increasing degree of non-linear learning. The resulting\nmodels can be learned efficiently and enjoy generalization guarantees that\nextend across input sizes, as we verify empirically.",
          "link": "http://arxiv.org/abs/2008.06952",
          "publishedOn": "2021-07-05T01:55:00.161Z",
          "wordCount": 612,
          "title": "A Functional Perspective on Learning Symmetric Functions with Neural Networks. (arXiv:2008.06952v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1912.02892",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Peterson_J/0/1/0/all/0/1\">J. Luc Peterson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bay_B/0/1/0/all/0/1\">Ben Bay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koning_J/0/1/0/all/0/1\">Joe Koning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Robinson_P/0/1/0/all/0/1\">Peter Robinson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Semler_J/0/1/0/all/0/1\">Jessica Semler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+White_J/0/1/0/all/0/1\">Jeremy White</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anirudh_R/0/1/0/all/0/1\">Rushil Anirudh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Athey_K/0/1/0/all/0/1\">Kevin Athey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bremer_P/0/1/0/all/0/1\">Peer-Timo Bremer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Natale_F/0/1/0/all/0/1\">Francesco Di Natale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fox_D/0/1/0/all/0/1\">David Fox</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaffney_J/0/1/0/all/0/1\">Jim A. Gaffney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jacobs_S/0/1/0/all/0/1\">Sam A. Jacobs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kailkhura_B/0/1/0/all/0/1\">Bhavya Kailkhura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kustowski_B/0/1/0/all/0/1\">Bogdan Kustowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Langer_S/0/1/0/all/0/1\">Steven Langer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spears_B/0/1/0/all/0/1\">Brian Spears</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thiagarajan_J/0/1/0/all/0/1\">Jayaraman Thiagarajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Essen_B/0/1/0/all/0/1\">Brian Van Essen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeom_J/0/1/0/all/0/1\">Jae-Seung Yeom</a>",
          "description": "With the growing complexity of computational and experimental facilities,\nmany scientific researchers are turning to machine learning (ML) techniques to\nanalyze large scale ensemble data. With complexities such as multi-component\nworkflows, heterogeneous machine architectures, parallel file systems, and\nbatch scheduling, care must be taken to facilitate this analysis in a high\nperformance computing (HPC) environment. In this paper, we present Merlin, a\nworkflow framework to enable large ML-friendly ensembles of scientific HPC\nsimulations. By augmenting traditional HPC with distributed compute\ntechnologies, Merlin aims to lower the barrier for scientific subject matter\nexperts to incorporate ML into their analysis. In addition to its design, we\ndescribe some example applications that Merlin has enabled on leadership-class\nHPC resources, such as the ML-augmented optimization of nuclear fusion\nexperiments and the calibration of infectious disease models to study the\nprogression of and possible mitigation strategies for COVID-19.",
          "link": "http://arxiv.org/abs/1912.02892",
          "publishedOn": "2021-07-05T01:55:00.147Z",
          "wordCount": 717,
          "title": "Enabling Machine Learning-Ready HPC Ensembles with Merlin. (arXiv:1912.02892v2 [cs.DC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01199",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bajic_M/0/1/0/all/0/1\">Milena Bajic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pour_S/0/1/0/all/0/1\">Shahrzad M. Pour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skar_A/0/1/0/all/0/1\">Asmus Skar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pettinari_M/0/1/0/all/0/1\">Matteo Pettinari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levenberg_E/0/1/0/all/0/1\">Eyal Levenberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alstrom_T/0/1/0/all/0/1\">Tommy Sonne Alstr&#xf8;m</a>",
          "description": "Road roughness is a very important road condition for the infrastructure, as\nthe roughness affects both the safety and ride comfort of passengers. The roads\ndeteriorate over time which means the road roughness must be continuously\nmonitored in order to have an accurate understand of the condition of the road\ninfrastructure. In this paper, we propose a machine learning pipeline for road\nroughness prediction using the vertical acceleration of the car and the car\nspeed. We compared well-known supervised machine learning models such as linear\nregression, naive Bayes, k-nearest neighbor, random forest, support vector\nmachine, and the multi-layer perceptron neural network. The models are trained\non an optimally selected set of features computed in the temporal and\nstatistical domain. The results demonstrate that machine learning methods can\naccurately predict road roughness, using the recordings of the cost\napproachable in-vehicle sensors installed in conventional passenger cars. Our\nfindings demonstrate that the technology is well suited to meet future pavement\ncondition monitoring, by enabling continuous monitoring of a wide road network.",
          "link": "http://arxiv.org/abs/2107.01199",
          "publishedOn": "2021-07-05T01:55:00.140Z",
          "wordCount": 606,
          "title": "Road Roughness Estimation Using Machine Learning. (arXiv:2107.01199v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01202",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ansari_M/0/1/0/all/0/1\">Mohd Zeeshan Ansari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beg_M/0/1/0/all/0/1\">M M Sufyan Beg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_T/0/1/0/all/0/1\">Tanvir Ahmad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1\">Mohd Jazib Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wasim_G/0/1/0/all/0/1\">Ghazali Wasim</a>",
          "description": "Language identification of social media text has been an interesting problem\nof study in recent years. Social media messages are predominantly in code mixed\nin non-English speaking states. Prior knowledge by pre-training contextual\nembeddings have shown state of the art results for a range of downstream tasks.\nRecently, models such as BERT have shown that using a large amount of unlabeled\ndata, the pretrained language models are even more beneficial for learning\ncommon language representations. Extensive experiments exploiting transfer\nlearning and fine-tuning BERT models to identify language on Twitter are\npresented in this paper. The work utilizes a data collection of\nHindi-English-Urdu codemixed text for language pre-training and Hindi-English\ncodemixed for subsequent word-level language classification. The results show\nthat the representations pre-trained over codemixed data produce better results\nby their monolingual counterpart.",
          "link": "http://arxiv.org/abs/2107.01202",
          "publishedOn": "2021-07-05T01:55:00.126Z",
          "wordCount": 573,
          "title": "Language Identification of Hindi-English tweets using code-mixed BERT. (arXiv:2107.01202v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/1912.02254",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhan_H/0/1/0/all/0/1\">Huixin Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Wei-Ming Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yongcan Cao</a>",
          "description": "Besides accuracy, the model size of convolutional neural networks (CNN)\nmodels is another important factor considering limited hardware resources in\npractical applications. For example, employing deep neural networks on mobile\nsystems requires the design of accurate yet fast CNN for low latency in\nclassification and object detection. To fulfill the need, we aim at obtaining\nCNN models with both high testing accuracy and small size to address resource\nconstraints in many embedded devices. In particular, this paper focuses on\nproposing a generic reinforcement learning-based model compression approach in\na two-stage compression pipeline: pruning and quantization. The first stage of\ncompression, i.e., pruning, is achieved via exploiting deep reinforcement\nlearning (DRL) to co-learn the accuracy and the FLOPs updated after layer-wise\nchannel pruning and element-wise variational pruning via information dropout.\nThe second stage, i.e., quantization, is achieved via a similar DRL approach\nbut focuses on obtaining the optimal bits representation for individual layers.\nWe further conduct experimental results on CIFAR-10 and ImageNet datasets. For\nthe CIFAR-10 dataset, the proposed method can reduce the size of VGGNet by 9x\nfrom 20.04MB to 2.2MB with a slight accuracy increase. For the ImageNet\ndataset, the proposed method can reduce the size of VGG-16 by 33x from 138MB to\n4.14MB with no accuracy loss.",
          "link": "http://arxiv.org/abs/1912.02254",
          "publishedOn": "2021-07-05T01:55:00.113Z",
          "wordCount": 674,
          "title": "Deep Model Compression Via Two-Stage Deep Reinforcement Learning. (arXiv:1912.02254v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01126",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Melkas_L/0/1/0/all/0/1\">Laila Melkas</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Savvides_R/0/1/0/all/0/1\">Rafael Savvides</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Chandramouli_S/0/1/0/all/0/1\">Suyog Chandramouli</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Makela_J/0/1/0/all/0/1\">Jarmo M&#xe4;kel&#xe4;</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Nieminen_T/0/1/0/all/0/1\">Tuomo Nieminen</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Mammarella_I/0/1/0/all/0/1\">Ivan Mammarella</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Puolamaki_K/0/1/0/all/0/1\">Kai Puolam&#xe4;ki</a>",
          "description": "Causal structure discovery (CSD) models are making inroads into several\ndomains, including Earth system sciences. Their widespread adaptation is\nhowever hampered by the fact that the resulting models often do not take into\naccount the domain knowledge of the experts and that it is often necessary to\nmodify the resulting models iteratively. We present a workflow that is required\nto take this knowledge into account and to apply CSD algorithms in Earth system\nsciences. At the same time, we describe open research questions that still need\nto be addressed. We present a way to interactively modify the outputs of the\nCSD algorithms and argue that the user interaction can be modelled as a greedy\nfinding of the local maximum-a-posteriori solution of the likelihood function,\nwhich is composed of the likelihood of the causal model and the prior\ndistribution representing the knowledge of the expert user. We use a real-world\ndata set for examples constructed in collaboration with our co-authors, who are\nthe domain area experts. We show that finding maximally usable causal models in\nthe Earth system sciences or other similar domains is a difficult task which\ncontains many interesting open research questions. We argue that taking the\ndomain knowledge into account has a substantial effect on the final causal\nmodels discovered.",
          "link": "http://arxiv.org/abs/2107.01126",
          "publishedOn": "2021-07-05T01:55:00.095Z",
          "wordCount": 681,
          "title": "Interactive Causal Structure Discovery in Earth System Sciences. (arXiv:2107.01126v1 [physics.data-an])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01131",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Guo_Q/0/1/0/all/0/1\">Qing Guo</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Chen_J/0/1/0/all/0/1\">Junya Chen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wang_D/0/1/0/all/0/1\">Dong Wang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Yang_Y/0/1/0/all/0/1\">Yuewei Yang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Deng_X/0/1/0/all/0/1\">Xinwei Deng</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Carin_L/0/1/0/all/0/1\">Lawrence Carin</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Li_F/0/1/0/all/0/1\">Fan Li</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Tao_C/0/1/0/all/0/1\">Chenyang Tao</a>",
          "description": "Successful applications of InfoNCE and its variants have popularized the use\nof contrastive variational mutual information (MI) estimators in machine\nlearning. While featuring superior stability, these estimators crucially depend\non costly large-batch training, and they sacrifice bound tightness for variance\nreduction. To overcome these limitations, we revisit the mathematics of popular\nvariational MI bounds from the lens of unnormalized statistical modeling and\nconvex optimization. Our investigation not only yields a new unified\ntheoretical framework encompassing popular variational MI bounds but also leads\nto a novel, simple, and powerful contrastive MI estimator named as FLO.\nTheoretically, we show that the FLO estimator is tight, and it provably\nconverges under stochastic gradient descent. Empirically, our FLO estimator\novercomes the limitations of its predecessors and learns more efficiently. The\nutility of FLO is verified using an extensive set of benchmarks, which also\nreveals the trade-offs in practical MI estimation.",
          "link": "http://arxiv.org/abs/2107.01131",
          "publishedOn": "2021-07-05T01:55:00.088Z",
          "wordCount": 597,
          "title": "Tight Mutual Information Estimation With Contrastive Fenchel-Legendre Optimization. (arXiv:2107.01131v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01185",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Podder_P/0/1/0/all/0/1\">Prajoy Podder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bharati_S/0/1/0/all/0/1\">Subrato Bharati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mondal_M/0/1/0/all/0/1\">M. Rubaiyat Hossain Mondal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paul_P/0/1/0/all/0/1\">Pinto Kumar Paul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kose_U/0/1/0/all/0/1\">Utku Kose</a>",
          "description": "Cybersecurity is a very emerging field that protects systems, networks, and\ndata from digital attacks. With the increase in the scale of the Internet and\nthe evolution of cyber attacks, developing novel cybersecurity tools has become\nimportant, particularly for Internet of things (IoT) networks. This paper\nprovides a systematic review of the application of deep learning (DL)\napproaches for cybersecurity. This paper provides a short description of DL\nmethods which is used in cybersecurity, including deep belief networks,\ngenerative adversarial networks, recurrent neural networks, and others. Next,\nwe illustrate the differences between shallow learning and DL. Moreover, a\ndiscussion is provided on the currently prevailing cyber-attacks in IoT and\nother networks, and the effectiveness of DL methods to manage these attacks.\nBesides, this paper describes studies that highlight the DL technique,\ncybersecurity applications, and the source of datasets. Next, a discussion is\nprovided on the feasibility of DL systems for malware detection and\nclassification, intrusion detection, and other frequent cyber-attacks,\nincluding identifying file type, spam, and network traffic. Our review\nindicates that high classification accuracy of 99.72% is obtained by restricted\nBoltzmann machine (RBM) when applied to a custom dataset, while long short-term\nmemory (LSTM) achieves an accuracy of 99.80% for KDD Cup 99 dataset. Finally,\nthis article discusses the importance of cybersecurity for reliable and\npracticable IoT-driven healthcare systems.",
          "link": "http://arxiv.org/abs/2107.01185",
          "publishedOn": "2021-07-05T01:55:00.082Z",
          "wordCount": 683,
          "title": "Artificial Neural Network for Cybersecurity: A Comprehensive Review. (arXiv:2107.01185v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01192",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liping Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tiku_S/0/1/0/all/0/1\">Saideep Tiku</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pasricha_S/0/1/0/all/0/1\">Sudeep Pasricha</a>",
          "description": "GPS technology has revolutionized the way we localize and navigate outdoors.\nHowever, the poor reception of GPS signals in buildings makes it unsuitable for\nindoor localization. WiFi fingerprinting-based indoor localization is one of\nthe most promising ways to meet this demand. Unfortunately, most work in the\ndomain fails to resolve challenges associated with deployability on\nresource-limited embedded devices. In this work, we propose a compression-aware\nand high-accuracy deep learning framework called CHISEL that outperforms the\nbest-known works in the area while maintaining localization robustness on\nembedded devices.",
          "link": "http://arxiv.org/abs/2107.01192",
          "publishedOn": "2021-07-05T01:55:00.074Z",
          "wordCount": 519,
          "title": "CHISEL: Compression-Aware High-Accuracy Embedded Indoor Localization with Deep Learning. (arXiv:2107.01192v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01201",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Rikhye_R/0/1/0/all/0/1\">Rajeev Rikhye</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Q/0/1/0/all/0/1\">Quan Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liang_Q/0/1/0/all/0/1\">Qiao Liang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+He_Y/0/1/0/all/0/1\">Yanzhang He</a>, <a href=\"http://arxiv.org/find/eess/1/au:+McGraw_I/0/1/0/all/0/1\">Ian McGraw</a>",
          "description": "In this paper, we propose a solution to allow speaker conditioned speech\nmodels, such as VoiceFilter-Lite, to support an arbitrary number of enrolled\nusers in a single pass. This is achieved by using an attention mechanism on\nmultiple speaker embeddings to compute a single attentive embedding, which is\nthen used as a side input to the model. We implemented multi-user\nVoiceFilter-Lite and evaluated it for three tasks: (1) a streaming automatic\nspeech recognition (ASR) task; (2) a text-independent speaker verification\ntask; and (3) a personalized keyphrase detection task, where ASR has to detect\nkeyphrases from multiple enrolled users in a noisy environment. Our experiments\nshow that, with up to four enrolled users, multi-user VoiceFilter-Lite is able\nto significantly reduce speech recognition and speaker verification errors when\nthere is overlapping speech, without affecting performance under other acoustic\nconditions. This attentive speaker embedding approach can also be easily\napplied to other speaker-conditioned models such as personal VAD and\npersonalized ASR.",
          "link": "http://arxiv.org/abs/2107.01201",
          "publishedOn": "2021-07-05T01:55:00.068Z",
          "wordCount": 601,
          "title": "Multi-user VoiceFilter-Lite via Attentive Speaker Embedding. (arXiv:2107.01201v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00727",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Shanu Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kurmi_V/0/1/0/all/0/1\">Vinod Kumar Kurmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_P/0/1/0/all/0/1\">Praphul Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Namboodiri_V/0/1/0/all/0/1\">Vinay P Namboodiri</a>",
          "description": "Understanding unsupervised domain adaptation has been an important task that\nhas been well explored. However, the wide variety of methods have not analyzed\nthe role of a classifier's performance in detail. In this paper, we thoroughly\nexamine the role of a classifier in terms of matching source and target\ndistributions. We specifically investigate the classifier ability by matching\na) the distribution of features, b) probabilistic uncertainty for samples and\nc) certainty activation mappings. Our analysis suggests that using these three\ndistributions does result in a consistently improved performance on all the\ndatasets. Our work thus extends present knowledge on the role of the various\ndistributions obtained from the classifier towards solving unsupervised domain\nadaptation.",
          "link": "http://arxiv.org/abs/2107.00727",
          "publishedOn": "2021-07-05T01:54:59.465Z",
          "wordCount": 552,
          "title": "Mitigating Uncertainty of Classifier for Unsupervised Domain Adaptation. (arXiv:2107.00727v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1809.04091",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sepehry_B/0/1/0/all/0/1\">Behrooz Sepehry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iranmanesh_E/0/1/0/all/0/1\">Ehsan Iranmanesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Friedlander_M/0/1/0/all/0/1\">Michael P. Friedlander</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ronagh_P/0/1/0/all/0/1\">Pooya Ronagh</a>",
          "description": "We introduce two quantum algorithms for solving structured prediction\nproblems. We first show that a stochastic gradient descent that uses the\nquantum minimum finding algorithm and takes its probabilistic failure into\naccount solves the structured prediction problem with a runtime that scales\nwith the square root of the size of the label space, and in $\\widetilde\nO\\left(1/\\epsilon\\right)$ with respect to the precision, $\\epsilon$, of the\nsolution. Motivated by robust inference techniques in machine learning, we then\nintroduce another quantum algorithm that solves a smooth approximation of the\nstructured prediction problem with a similar quantum speedup in the size of the\nlabel space and a similar scaling in the precision parameter. In doing so, we\nanalyze a variant of stochastic gradient descent for convex optimization in the\npresence of an additive error in the calculation of the gradients, and show\nthat its convergence rate does not deteriorate if the additive errors are of\nthe order $O(\\sqrt\\epsilon)$. This algorithm uses quantum Gibbs sampling at\ntemperature $\\Omega (\\epsilon)$ as a subroutine. Based on these theoretical\nobservations, we propose a method for using quantum Gibbs samplers to combine\nfeedforward neural networks with probabilistic graphical models for quantum\nmachine learning. Our numerical results using Monte Carlo simulations on an\nimage tagging task demonstrate the benefit of the approach.",
          "link": "http://arxiv.org/abs/1809.04091",
          "publishedOn": "2021-07-05T01:54:59.459Z",
          "wordCount": 716,
          "title": "Quantum Algorithms for Structured Prediction. (arXiv:1809.04091v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00996",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alfarra_M/0/1/0/all/0/1\">Motasem Alfarra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bibi_A/0/1/0/all/0/1\">Adel Bibi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khan_N/0/1/0/all/0/1\">Naeemullah Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1\">Philip H. S. Torr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1\">Bernard Ghanem</a>",
          "description": "Deep neural networks are vulnerable to input deformations in the form of\nvector fields of pixel displacements and to other parameterized geometric\ndeformations e.g. translations, rotations, etc. Current input deformation\ncertification methods either (i) do not scale to deep networks on large input\ndatasets, or (ii) can only certify a specific class of deformations, e.g. only\nrotations. We reformulate certification in randomized smoothing setting for\nboth general vector field and parameterized deformations and propose\nDeformRS-VF and DeformRS-Par, respectively. Our new formulation scales to large\nnetworks on large input datasets. For instance, DeformRS-Par certifies rich\ndeformations, covering translations, rotations, scaling, affine deformations,\nand other visually aligned deformations such as ones parameterized by\nDiscrete-Cosine-Transform basis. Extensive experiments on MNIST, CIFAR10 and\nImageNet show that DeformRS-Par outperforms existing state-of-the-art in\ncertified accuracy, e.g. improved certified accuracy of 6% against perturbed\nrotations in the set [-10,10] degrees on ImageNet.",
          "link": "http://arxiv.org/abs/2107.00996",
          "publishedOn": "2021-07-05T01:54:59.453Z",
          "wordCount": 591,
          "title": "DeformRS: Certifying Input Deformations with Randomized Smoothing. (arXiv:2107.00996v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00961",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dun_C/0/1/0/all/0/1\">Chen Dun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolfe_C/0/1/0/all/0/1\">Cameron R. Wolfe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jermaine_C/0/1/0/all/0/1\">Christopher M. Jermaine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kyrillidis_A/0/1/0/all/0/1\">Anastasios Kyrillidis</a>",
          "description": "We propose {\\rm \\texttt{ResIST}}, a novel distributed training protocol for\nResidual Networks (ResNets). {\\rm \\texttt{ResIST}} randomly decomposes a global\nResNet into several shallow sub-ResNets that are trained independently in a\ndistributed manner for several local iterations, before having their updates\nsynchronized and aggregated into the global model. In the next round, new\nsub-ResNets are randomly generated and the process repeats. By construction,\nper iteration, {\\rm \\texttt{ResIST}} communicates only a small portion of\nnetwork parameters to each machine and never uses the full model during\ntraining. Thus, {\\rm \\texttt{ResIST}} reduces the communication, memory, and\ntime requirements of ResNet training to only a fraction of the requirements of\nprevious methods. In comparison to common protocols like data-parallel training\nand data-parallel training with local SGD, {\\rm \\texttt{ResIST}} yields a\ndecrease in wall-clock training time, while being competitive with respect to\nmodel performance.",
          "link": "http://arxiv.org/abs/2107.00961",
          "publishedOn": "2021-07-05T01:54:59.446Z",
          "wordCount": 593,
          "title": "ResIST: Layer-Wise Decomposition of ResNets for Distributed Training. (arXiv:2107.00961v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00946",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lingbo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yuying Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guanbin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Ziyi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_L/0/1/0/all/0/1\">Lei Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_M/0/1/0/all/0/1\">Mingzhi Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Liang Lin</a>",
          "description": "Metro origin-destination prediction is a crucial yet challenging task for\nintelligent transportation management, which aims to accurately forecast two\nspecific types of cross-station ridership, i.e., Origin-Destination (OD) one\nand Destination-Origin (DO) one. However, complete OD matrices of previous time\nintervals can not be obtained immediately in online metro systems, and\nconventional methods only used limited information to forecast the future OD\nand DO ridership separately.In this work, we proposed a novel neural network\nmodule termed Heterogeneous Information Aggregation Machine (HIAM), which fully\nexploits heterogeneous information of historical data (e.g., incomplete OD\nmatrices, unfinished order vectors, and DO matrices) to jointly learn the\nevolutionary patterns of OD and DO ridership. Specifically, an OD modeling\nbranch estimates the potential destinations of unfinished orders explicitly to\ncomplement the information of incomplete OD matrices, while a DO modeling\nbranch takes DO matrices as input to capture the spatial-temporal distribution\nof DO ridership. Moreover, a Dual Information Transformer is introduced to\npropagate the mutual information among OD features and DO features for modeling\nthe OD-DO causality and correlation. Based on the proposed HIAM, we develop a\nunified Seq2Seq network to forecast the future OD and DO ridership\nsimultaneously. Extensive experiments conducted on two large-scale benchmarks\ndemonstrate the effectiveness of our method for online metro origin-destination\nprediction.",
          "link": "http://arxiv.org/abs/2107.00946",
          "publishedOn": "2021-07-05T01:54:59.440Z",
          "wordCount": 651,
          "title": "Online Metro Origin-Destination Prediction via Heterogeneous Information Aggregation. (arXiv:2107.00946v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01079",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hammernik_K/0/1/0/all/0/1\">Kerstin Hammernik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_C/0/1/0/all/0/1\">Cheng Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_C/0/1/0/all/0/1\">Chen Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_W/0/1/0/all/0/1\">Wenjia Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rueckert_D/0/1/0/all/0/1\">Daniel Rueckert</a>",
          "description": "Deep learning-based segmentation methods are vulnerable to unforeseen data\ndistribution shifts during deployment, e.g. change of image appearances or\ncontrasts caused by different scanners, unexpected imaging artifacts etc. In\nthis paper, we present a cooperative framework for training image segmentation\nmodels and a latent space augmentation method for generating hard examples.\nBoth contributions improve model generalization and robustness with limited\ndata. The cooperative training framework consists of a fast-thinking network\n(FTN) and a slow-thinking network (STN). The FTN learns decoupled image\nfeatures and shape features for image reconstruction and segmentation tasks.\nThe STN learns shape priors for segmentation correction and refinement. The two\nnetworks are trained in a cooperative manner. The latent space augmentation\ngenerates challenging examples for training by masking the decoupled latent\nspace in both channel-wise and spatial-wise manners. We performed extensive\nexperiments on public cardiac imaging datasets. Using only 10 subjects from a\nsingle site for training, we demonstrated improved cross-site segmentation\nperformance and increased robustness against various unforeseen imaging\nartifacts compared to strong baseline methods. Particularly, cooperative\ntraining with latent space data augmentation yields 15% improvement in terms of\naverage Dice score when compared to a standard training method.",
          "link": "http://arxiv.org/abs/2107.01079",
          "publishedOn": "2021-07-05T01:54:59.424Z",
          "wordCount": 657,
          "title": "Cooperative Training and Latent Space Data Augmentation for Robust Medical Image Segmentation. (arXiv:2107.01079v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01057",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Trauble_F/0/1/0/all/0/1\">Frederik Tr&#xe4;uble</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kugelgen_J/0/1/0/all/0/1\">Julius von K&#xfc;gelgen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kleindessner_M/0/1/0/all/0/1\">Matth&#xe4;us Kleindessner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Locatello_F/0/1/0/all/0/1\">Francesco Locatello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1\">Bernhard Sch&#xf6;lkopf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gehler_P/0/1/0/all/0/1\">Peter Gehler</a>",
          "description": "When machine learning systems meet real world applications, accuracy is only\none of several requirements. In this paper, we assay a complementary\nperspective originating from the increasing availability of pre-trained and\nregularly improving state-of-the-art models. While new improved models develop\nat a fast pace, downstream tasks vary more slowly or stay constant. Assume that\nwe have a large unlabelled data set for which we want to maintain accurate\npredictions. Whenever a new and presumably better ML models becomes available,\nwe encounter two problems: (i) given a limited budget, which data points should\nbe re-evaluated using the new model?; and (ii) if the new predictions differ\nfrom the current ones, should we update? Problem (i) is about compute cost,\nwhich matters for very large data sets and models. Problem (ii) is about\nmaintaining consistency of the predictions, which can be highly relevant for\ndownstream applications; our demand is to avoid negative flips, i.e., changing\ncorrect to incorrect predictions. In this paper, we formalize the Prediction\nUpdate Problem and present an efficient probabilistic approach as answer to the\nabove questions. In extensive experiments on standard classification benchmark\ndata sets, we show that our method outperforms alternative strategies along key\nmetrics for backward-compatible prediction updates.",
          "link": "http://arxiv.org/abs/2107.01057",
          "publishedOn": "2021-07-05T01:54:59.417Z",
          "wordCount": 637,
          "title": "Backward-Compatible Prediction Updates: A Probabilistic Approach. (arXiv:2107.01057v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01173",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guanghui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1\">Ming Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lijun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1\">Tianbao Yang</a>",
          "description": "In this paper, we study stochastic optimization of areas under\nprecision-recall curves (AUPRC), which is widely used for combating imbalanced\nclassification tasks. Although a few methods have been proposed for maximizing\nAUPRC, stochastic optimization of AUPRC with convergence guarantee remains an\nundeveloped territory. A recent work [42] has proposed a promising approach\ntowards AUPRC based on maximizing a surrogate loss for the average precision,\nand proved an $O(1/\\epsilon^5)$ complexity for finding an $\\epsilon$-stationary\nsolution of the non-convex objective. In this paper, we further improve the\nstochastic optimization of AURPC by (i) developing novel stochastic momentum\nmethods with a better iteration complexity of $O(1/\\epsilon^4)$ for finding an\n$\\epsilon$-stationary solution; and (ii) designing a novel family of stochastic\nadaptive methods with the same iteration complexity of $O(1/\\epsilon^4)$, which\nenjoy faster convergence in practice. To this end, we propose two innovative\ntechniques that are critical for improving the convergence: (i) the biased\nestimators for tracking individual ranking scores are updated in a randomized\ncoordinate-wise manner; and (ii) a momentum update is used on top of the\nstochastic gradient estimator for tracking the gradient of the objective.\nExtensive experiments on various data sets demonstrate the effectiveness of the\nproposed algorithms. Of independent interest, the proposed stochastic momentum\nand adaptive algorithms are also applicable to a class of two-level stochastic\ndependent compositional optimization problems.",
          "link": "http://arxiv.org/abs/2107.01173",
          "publishedOn": "2021-07-05T01:54:59.410Z",
          "wordCount": 655,
          "title": "Momentum Accelerates the Convergence of Stochastic AUPRC Maximization. (arXiv:2107.01173v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01091",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pavlichenko_N/0/1/0/all/0/1\">Nikita Pavlichenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stelmakh_I/0/1/0/all/0/1\">Ivan Stelmakh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ustalov_D/0/1/0/all/0/1\">Dmitry Ustalov</a>",
          "description": "Domain-specific data is the crux of the successful transfer of machine\nlearning systems from benchmarks to real life. Crowdsourcing has become one of\nthe standard tools for cheap and time-efficient data collection for simple\nproblems such as image classification: thanks in large part to advances in\nresearch on aggregation methods. However, the applicability of crowdsourcing to\nmore complex tasks (e.g., speech recognition) remains limited due to the lack\nof principled aggregation methods for these modalities. The main obstacle\ntowards designing advanced aggregation methods is the absence of training data,\nand in this work, we focus on bridging this gap in speech recognition. For\nthis, we collect and release CrowdSpeech -- the first publicly available\nlarge-scale dataset of crowdsourced audio transcriptions. Evaluation of\nexisting aggregation methods on our data shows room for improvement, suggesting\nthat our work may entail the design of better algorithms. At a higher level, we\nalso contribute to the more general challenge of collecting high-quality\ndatasets using crowdsourcing: we develop a principled pipeline for constructing\ndatasets of crowdsourced audio transcriptions in any novel domain. We show its\napplicability on an under-resourced language by constructing VoxDIY -- a\ncounterpart of CrowdSpeech for the Russian language. We also release the code\nthat allows a full replication of our data collection pipeline and share\nvarious insights on best practices of data collection via crowdsourcing.",
          "link": "http://arxiv.org/abs/2107.01091",
          "publishedOn": "2021-07-05T01:54:59.403Z",
          "wordCount": 668,
          "title": "Vox Populi, Vox DIY: Benchmark Dataset for Crowdsourced Audio Transcription. (arXiv:2107.01091v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01081",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Badias_A/0/1/0/all/0/1\">Alberto Badias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_A/0/1/0/all/0/1\">Ashis Banerjee</a>",
          "description": "We present a new framework to measure the intrinsic properties of (deep)\nneural networks. While we focus on convolutional networks, our framework can be\nextrapolated to any network architecture. In particular, we evaluate two\nnetwork properties, namely, capacity (related to expressivity) and compression,\nboth of which depend only on the network structure and are independent of the\ntraining and test data. To this end, we propose two metrics: the first one,\ncalled layer complexity, captures the architectural complexity of any network\nlayer; and, the second one, called layer intrinsic power, encodes how data is\ncompressed along the network. The metrics are based on the concept of layer\nalgebra, which is also introduced in this paper. This concept is based on the\nidea that the global properties depend on the network topology, and the leaf\nnodes of any neural network can be approximated using local transfer functions,\nthereby, allowing a simple computation of the global metrics. We also compare\nthe properties of the state-of-the art architectures using our metrics and use\nthe properties to analyze the classification accuracy on benchmark datasets.",
          "link": "http://arxiv.org/abs/2107.01081",
          "publishedOn": "2021-07-05T01:54:59.372Z",
          "wordCount": 617,
          "title": "Neural Network Layer Algebra: A Framework to Measure Capacity and Compression in Deep Learning. (arXiv:2107.01081v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01189",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jerrick Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Inkawhich_N/0/1/0/all/0/1\">Nathan Inkawhich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nina_O/0/1/0/all/0/1\">Oliver Nina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Timofte_R/0/1/0/all/0/1\">Radu Timofte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Sahil Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_B/0/1/0/all/0/1\">Bob Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_Y/0/1/0/all/0/1\">Yuru Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_W/0/1/0/all/0/1\">Wei Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Songzheng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yuxuan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jiaqi Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1\">Xueli Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1\">Mengru Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Gongzhe Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1\">Xueli Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1\">Huanqia Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_C/0/1/0/all/0/1\">Chengxue Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cummings_S/0/1/0/all/0/1\">Sol Cummings</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miron_C/0/1/0/all/0/1\">Casian Miron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pasarica_A/0/1/0/all/0/1\">Alexandru Pasarica</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Cheng-Yen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_H/0/1/0/all/0/1\">Hung-Min Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1\">Jiarui Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_J/0/1/0/all/0/1\">Jie Mei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeh_C/0/1/0/all/0/1\">Chia-Ying Yeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1\">Jenq-Neng Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xin_M/0/1/0/all/0/1\">Michael Xin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shangguan_Z/0/1/0/all/0/1\">Zhongkai Shangguan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zihe Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yifei_X/0/1/0/all/0/1\">Xu Yifei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Lehan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kele Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_M/0/1/0/all/0/1\">Min Feng</a>",
          "description": "In this paper, we introduce the first Challenge on Multi-modal Aerial View\nObject Classification (MAVOC) in conjunction with the NTIRE 2021 workshop at\nCVPR. This challenge is composed of two different tracks using EO andSAR\nimagery. Both EO and SAR sensors possess different advantages and drawbacks.\nThe purpose of this competition is to analyze how to use both sets of sensory\ninformation in complementary ways. We discuss the top methods submitted for\nthis competition and evaluate their results on our blind test set. Our\nchallenge results show significant improvement of more than 15% accuracy from\nour current baselines for each track of the competition",
          "link": "http://arxiv.org/abs/2107.01189",
          "publishedOn": "2021-07-05T01:54:59.347Z",
          "wordCount": 632,
          "title": "NTIRE 2021 Multi-modal Aerial View Object Classification Challenge. (arXiv:2107.01189v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2104.13020",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Pena_J/0/1/0/all/0/1\">Jose M. Pe&#xf1;a</a>",
          "description": "We present a method for assessing the sensitivity of the true causal effect\nto unmeasured confounding. The method requires the analyst to specify two\nintuitive parameters. Otherwise, the method is assumption-free. The method\nreturns an interval that contains the true causal effect. Moreover, the bounds\nof the interval are sharp, i.e. attainable. We show experimentally that our\nbounds can be sharper than those obtained by the method of Ding and VanderWeele\n(2016). Finally, we extend our method to bound the natural direct and indirect\neffects when there are measured mediators and unmeasured exposure-outcome\nconfounding.",
          "link": "http://arxiv.org/abs/2104.13020",
          "publishedOn": "2021-07-05T01:54:59.325Z",
          "wordCount": 543,
          "title": "Simple yet Sharp Sensitivity Analysis for Unmeasured Confounding. (arXiv:2104.13020v2 [stat.ME] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12182",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jalal_A/0/1/0/all/0/1\">Ajil Jalal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karmalkar_S/0/1/0/all/0/1\">Sushrut Karmalkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoffmann_J/0/1/0/all/0/1\">Jessica Hoffmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dimakis_A/0/1/0/all/0/1\">Alexandros G. Dimakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Price_E/0/1/0/all/0/1\">Eric Price</a>",
          "description": "This work tackles the issue of fairness in the context of generative\nprocedures, such as image super-resolution, which entail different definitions\nfrom the standard classification setting. Moreover, while traditional group\nfairness definitions are typically defined with respect to specified protected\ngroups -- camouflaging the fact that these groupings are artificial and carry\nhistorical and political motivations -- we emphasize that there are no ground\ntruth identities. For instance, should South and East Asians be viewed as a\nsingle group or separate groups? Should we consider one race as a whole or\nfurther split by gender? Choosing which groups are valid and who belongs in\nthem is an impossible dilemma and being \"fair\" with respect to Asians may\nrequire being \"unfair\" with respect to South Asians. This motivates the\nintroduction of definitions that allow algorithms to be \\emph{oblivious} to the\nrelevant groupings.\n\nWe define several intuitive notions of group fairness and study their\nincompatibilities and trade-offs. We show that the natural extension of\ndemographic parity is strongly dependent on the grouping, and \\emph{impossible}\nto achieve obliviously. On the other hand, the conceptually new definition we\nintroduce, Conditional Proportional Representation, can be achieved obliviously\nthrough Posterior Sampling. Our experiments validate our theoretical results\nand achieve fair image reconstruction using state-of-the-art generative models.",
          "link": "http://arxiv.org/abs/2106.12182",
          "publishedOn": "2021-07-05T01:54:59.318Z",
          "wordCount": 685,
          "title": "Fairness for Image Generation with Uncertain Sensitive Attributes. (arXiv:2106.12182v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01152",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Chen_J/0/1/0/all/0/1\">Junya Chen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Gan_Z/0/1/0/all/0/1\">Zhe Gan</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Li_X/0/1/0/all/0/1\">Xuan Li</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Guo_Q/0/1/0/all/0/1\">Qing Guo</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Chen_L/0/1/0/all/0/1\">Liqun Chen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Gao_S/0/1/0/all/0/1\">Shuyang Gao</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Chung_T/0/1/0/all/0/1\">Tagyoung Chung</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Xu_Y/0/1/0/all/0/1\">Yi Xu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zeng_B/0/1/0/all/0/1\">Belinda Zeng</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lu_W/0/1/0/all/0/1\">Wenlian Lu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Li_F/0/1/0/all/0/1\">Fan Li</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Carin_L/0/1/0/all/0/1\">Lawrence Carin</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Tao_C/0/1/0/all/0/1\">Chenyang Tao</a>",
          "description": "InfoNCE-based contrastive representation learners, such as SimCLR, have been\ntremendously successful in recent years. However, these contrastive schemes are\nnotoriously resource demanding, as their effectiveness breaks down with\nsmall-batch training (i.e., the log-K curse, whereas K is the batch-size). In\nthis work, we reveal mathematically why contrastive learners fail in the\nsmall-batch-size regime, and present a novel simple, non-trivial contrastive\nobjective named FlatNCE, which fixes this issue. Unlike InfoNCE, our FlatNCE no\nlonger explicitly appeals to a discriminative classification goal for\ncontrastive learning. Theoretically, we show FlatNCE is the mathematical dual\nformulation of InfoNCE, thus bridging the classical literature on energy\nmodeling; and empirically, we demonstrate that, with minimal modification of\ncode, FlatNCE enables immediate performance boost independent of the\nsubject-matter engineering efforts. The significance of this work is furthered\nby the powerful generalization of contrastive learning techniques, and the\nintroduction of new tools to monitor and diagnose contrastive training. We\nsubstantiate our claims with empirical evidence on CIFAR10, ImageNet, and other\ndatasets, where FlatNCE consistently outperforms InfoNCE.",
          "link": "http://arxiv.org/abs/2107.01152",
          "publishedOn": "2021-07-05T01:54:59.311Z",
          "wordCount": 644,
          "title": "Simpler, Faster, Stronger: Breaking The log-K Curse On Contrastive Learners With FlatNCE. (arXiv:2107.01152v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01188",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schuetz_M/0/1/0/all/0/1\">Martin J. A. Schuetz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brubaker_J/0/1/0/all/0/1\">J. Kyle Brubaker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katzgraber_H/0/1/0/all/0/1\">Helmut G. Katzgraber</a>",
          "description": "We demonstrate how graph neural networks can be used to solve combinatorial\noptimization problems. Our approach is broadly applicable to canonical NP-hard\nproblems in the form of quadratic unconstrained binary optimization problems,\nsuch as maximum cut, minimum vertex cover, maximum independent set, as well as\nIsing spin glasses and higher-order generalizations thereof in the form of\npolynomial unconstrained binary optimization problems. We apply a relaxation\nstrategy to the problem Hamiltonian to generate a differentiable loss function\nwith which we train the graph neural network and apply a simple projection to\ninteger variables once the unsupervised training process has completed. We\nshowcase our approach with numerical results for the canonical maximum cut and\nmaximum independent set problems. We find that the graph neural network\noptimizer performs on par or outperforms existing solvers, with the ability to\nscale beyond the state of the art to problems with millions of variables.",
          "link": "http://arxiv.org/abs/2107.01188",
          "publishedOn": "2021-07-05T01:54:59.305Z",
          "wordCount": 616,
          "title": "Combinatorial Optimization with Physics-Inspired Graph Neural Networks. (arXiv:2107.01188v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01184",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cody_T/0/1/0/all/0/1\">Tyler Cody</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adams_S/0/1/0/all/0/1\">Stephen Adams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beling_P/0/1/0/all/0/1\">Peter A. Beling</a>",
          "description": "Classical machine learning approaches are sensitive to non-stationarity.\nTransfer learning can address non-stationarity by sharing knowledge from one\nsystem to another, however, in areas like machine prognostics and defense, data\nis fundamentally limited. Therefore, transfer learning algorithms have little,\nif any, examples from which to learn. Herein, we suggest that these constraints\non algorithmic learning can be addressed by systems engineering. We formally\ndefine transfer distance in general terms and demonstrate its use in\nempirically quantifying the transferability of models. We consider the use of\ntransfer distance in the design of machine rebuild procedures to allow for\ntransferable prognostic models. We also consider the use of transfer distance\nin predicting operational performance in computer vision. Practitioners can use\nthe presented methodology to design and operate systems with consideration for\nthe learning theoretic challenges faced by component learning systems.",
          "link": "http://arxiv.org/abs/2107.01184",
          "publishedOn": "2021-07-05T01:54:59.298Z",
          "wordCount": 576,
          "title": "Empirically Measuring Transfer Distance for System Design and Operation. (arXiv:2107.01184v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.06926",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_T/0/1/0/all/0/1\">Tengyang Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_C/0/1/0/all/0/1\">Ching-An Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_N/0/1/0/all/0/1\">Nan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mineiro_P/0/1/0/all/0/1\">Paul Mineiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_A/0/1/0/all/0/1\">Alekh Agarwal</a>",
          "description": "The use of pessimism, when reasoning about datasets lacking exhaustive\nexploration has recently gained prominence in offline reinforcement learning.\nDespite the robustness it adds to the algorithm, overly pessimistic reasoning\ncan be equally damaging in precluding the discovery of good policies, which is\nan issue for the popular bonus-based pessimism. In this paper, we introduce the\nnotion of Bellman-consistent pessimism for general function approximation:\ninstead of calculating a point-wise lower bound for the value function, we\nimplement pessimism at the initial state over the set of functions consistent\nwith the Bellman equations. Our theoretical guarantees only require Bellman\nclosedness as standard in the exploratory setting, in which case bonus-based\npessimism fails to provide guarantees. Even in the special case of linear MDPs\nwhere stronger function-approximation assumptions hold, our result improves\nupon a recent bonus-based approach by $\\mathcal{O}(d)$ in its sample complexity\nwhen the action space is finite. Remarkably, our algorithms automatically adapt\nto the best bias-variance tradeoff in the hindsight, whereas most prior\napproaches require tuning extra hyperparameters a priori.",
          "link": "http://arxiv.org/abs/2106.06926",
          "publishedOn": "2021-07-05T01:54:59.261Z",
          "wordCount": 637,
          "title": "Bellman-consistent Pessimism for Offline Reinforcement Learning. (arXiv:2106.06926v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.04692",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Haike Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1\">Tengyu Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_S/0/1/0/all/0/1\">Simon S. Du</a>",
          "description": "This paper presents a new model-free algorithm for episodic finite-horizon\nMarkov Decision Processes (MDP), Adaptive Multi-step Bootstrap (AMB), which\nenjoys a stronger gap-dependent regret bound. The first innovation is to\nestimate the optimal $Q$-function by combining an optimistic bootstrap with an\nadaptive multi-step Monte Carlo rollout. The second innovation is to select the\naction with the largest confidence interval length among admissible actions\nthat are not dominated by any other actions. We show when each state has a\nunique optimal action, AMB achieves a gap-dependent regret bound that only\nscales with the sum of the inverse of the sub-optimality gaps. In contrast,\nSimchowitz and Jamieson (2019) showed all upper-confidence-bound (UCB)\nalgorithms suffer an additional $\\Omega\\left(\\frac{S}{\\Delta_{min}}\\right)$\nregret due to over-exploration where $\\Delta_{min}$ is the minimum\nsub-optimality gap and $S$ is the number of states. We further show that for\ngeneral MDPs, AMB suffers an additional $\\frac{|Z_{mul}|}{\\Delta_{min}}$\nregret, where $Z_{mul}$ is the set of state-action pairs $(s,a)$'s satisfying\n$a$ is a non-unique optimal action for $s$. We complement our upper bound with\na lower bound showing the dependency on $\\frac{|Z_{mul}|}{\\Delta_{min}}$ is\nunavoidable for any consistent algorithm. This lower bound also implies a\nseparation between reinforcement learning and contextual bandits.",
          "link": "http://arxiv.org/abs/2102.04692",
          "publishedOn": "2021-07-05T01:54:59.250Z",
          "wordCount": 670,
          "title": "Fine-Grained Gap-Dependent Bounds for Tabular MDPs via Adaptive Multi-Step Bootstrap. (arXiv:2102.04692v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.03466",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kanoh_R/0/1/0/all/0/1\">Ryuichi Kanoh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sugiyama_M/0/1/0/all/0/1\">Mahito Sugiyama</a>",
          "description": "A multiplicative constant scaling factor is often applied to the model output\nto adjust the dynamics of neural network parameters. This has been used as one\nof the key interventions in an empirical study of lazy and active behavior.\nHowever, we show that the combination of such scaling and a commonly used\nadaptive learning rate optimizer strongly affects the training behavior of the\nneural network. This is problematic as it can cause \\emph{unintended behavior}\nof neural networks, resulting in the misinterpretation of experimental results.\nSpecifically, for some scaling settings, the effect of the adaptive learning\nrate disappears or is strongly influenced by the scaling factor. To avoid the\nunintended effect, we present a modification of an optimization algorithm and\ndemonstrate remarkable differences between adaptive learning rate optimization\nand simple gradient descent, especially with a small ($<1.0$) scaling factor.",
          "link": "http://arxiv.org/abs/2103.03466",
          "publishedOn": "2021-07-05T01:54:59.244Z",
          "wordCount": 607,
          "title": "Unintended Effects on Adaptive Learning Rate for Training Neural Network with Output Scale Change. (arXiv:2103.03466v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13103",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cunnington_D/0/1/0/all/0/1\">Daniel Cunnington</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Law_M/0/1/0/all/0/1\">Mark Law</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Russo_A/0/1/0/all/0/1\">Alessandra Russo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lobo_J/0/1/0/all/0/1\">Jorge Lobo</a>",
          "description": "Inductive Logic Programming (ILP) aims to learn generalised, interpretable\nhypotheses in a data-efficient manner. However, current ILP systems require\ntraining examples to be specified in a structured logical form. To address this\nproblem, this paper proposes a neural-symbolic learning framework, called\nFeed-Forward Neural-Symbolic Learner (FF-NSL), that integrates state-of-the-art\nILP systems, based on the Answer Set semantics, with Neural Networks (NNs), in\norder to learn interpretable hypotheses from labelled unstructured data. To\ndemonstrate the generality and robustness of FF-NSL, we use two datasets\nsubject to distributional shifts, for which pre-trained NNs may give incorrect\npredictions with high confidence. Experimental results show that FF-NSL\noutperforms tree-based and neural-based approaches by learning more accurate\nand interpretable hypotheses with fewer examples.",
          "link": "http://arxiv.org/abs/2106.13103",
          "publishedOn": "2021-07-05T01:54:59.237Z",
          "wordCount": 572,
          "title": "FF-NSL: Feed-Forward Neural-Symbolic Learner. (arXiv:2106.13103v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.04763",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Azizi_M/0/1/0/all/0/1\">Mohammad Javad Azizi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kveton_B/0/1/0/all/0/1\">Branislav Kveton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghavamzadeh_M/0/1/0/all/0/1\">Mohammad Ghavamzadeh</a>",
          "description": "We study the problem of best-arm identification (BAI) in contextual bandits\nin the fixed-budget setting. We propose a general successive elimination\nalgorithm that proceeds in stages and eliminates a fixed fraction of suboptimal\narms in each stage. This design takes advantage of the strengths of static and\nadaptive allocations. We analyze the algorithm in linear models and obtain a\nbetter error bound than prior work. We also apply it to generalized linear\nmodels (GLMs) and bound its error. This is the first BAI algorithm for GLMs in\nthe fixed-budget setting. Our extensive numerical experiments show that our\nalgorithm outperforms the state of art.",
          "link": "http://arxiv.org/abs/2106.04763",
          "publishedOn": "2021-07-05T01:54:59.220Z",
          "wordCount": 582,
          "title": "Fixed-Budget Best-Arm Identification in Contextual Bandits: A Static-Adaptive Algorithm. (arXiv:2106.04763v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.03361",
          "author": "<a href=\"http://arxiv.org/find/astro-ph/1/au:+Zotov_M/0/1/0/all/0/1\">Mikhail Zotov</a>",
          "description": "We employ neural networks for classification of data of the TUS fluorescence\ntelescope, the world's first orbital detector of ultra-high energy cosmic rays.\nWe focus on two particular types of signals in the TUS data: track-like flashes\nproduced by cosmic ray hits of the photodetector and flashes that originated\nfrom distant lightnings. We demonstrate that even simple neural networks\ncombined with certain conventional methods of data analysis can be highly\neffective in tasks of classification of data of fluorescence telescopes.",
          "link": "http://arxiv.org/abs/2106.03361",
          "publishedOn": "2021-07-05T01:54:59.214Z",
          "wordCount": 561,
          "title": "Application of neural networks to classification of data of the TUS orbital telescope. (arXiv:2106.03361v2 [astro-ph.IM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01034",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Dumas_J/0/1/0/all/0/1\">Jonathan Dumas</a>",
          "description": "The Intergovernmental Panel on Climate Change proposes different mitigation\nstrategies to achieve the net emissions reductions that would be required to\nfollow a pathway that limits global warming to 1.5{\\deg}C with no or limited\novershoot. The transition towards a carbon-free society goes through an\ninevitable increase of the share of renewable generation in the energy mix and\na drastic decrease in terms of the total consumption of fossil fuels.\nTherefore, this thesis studies the integration of renewables in power systems\nby investigating forecasting and decision-making tools. Indeed, in contrast to\nconventional power plants, renewable energy is subject to uncertainty. Most of\nthe generation technologies based on renewable sources are non-dispatchable,\nand their production is stochastic and hard to predict in advance. A high share\nof renewables is a great challenge for power systems that have been designed\nand sized for dispatchable units. In this context, probabilistic forecasts,\nwhich aim at modeling the distribution of all possible future realizations,\nhave become an important tool to equip decision-makers, hopefully leading to\nbetter decisions in energy applications. This thesis focus on two main research\nquestions: (1) How to produce reliable probabilistic forecasts of renewable\ngeneration, consumption, and electricity prices? (2) How to make decisions with\nuncertainty using probabilistic forecasts? The thesis perimeter is the energy\nmanagement of \"small\" systems such as microgrids at a residential scale on a\nday-ahead basis. It is divided into two main parts to propose directions to\naddress both research questions (1) a forecasting part; (2) a planning and\ncontrol part.",
          "link": "http://arxiv.org/abs/2107.01034",
          "publishedOn": "2021-07-05T01:54:59.206Z",
          "wordCount": 724,
          "title": "Weather-based forecasting of energy generation, consumption and price for electrical microgrids management. (arXiv:2107.01034v1 [eess.SY])"
        },
        {
          "id": "http://arxiv.org/abs/2103.12828",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Chen_T/0/1/0/all/0/1\">Tianlong Chen</a>, <a href=\"http://arxiv.org/find/math/1/au:+Chen_X/0/1/0/all/0/1\">Xiaohan Chen</a>, <a href=\"http://arxiv.org/find/math/1/au:+Chen_W/0/1/0/all/0/1\">Wuyang Chen</a>, <a href=\"http://arxiv.org/find/math/1/au:+Heaton_H/0/1/0/all/0/1\">Howard Heaton</a>, <a href=\"http://arxiv.org/find/math/1/au:+Liu_J/0/1/0/all/0/1\">Jialin Liu</a>, <a href=\"http://arxiv.org/find/math/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Yin_W/0/1/0/all/0/1\">Wotao Yin</a>",
          "description": "Learning to optimize (L2O) is an emerging approach that leverages machine\nlearning to develop optimization methods, aiming at reducing the laborious\niterations of hand engineering. It automates the design of an optimization\nmethod based on its performance on a set of training problems. This data-driven\nprocedure generates methods that can efficiently solve problems similar to\nthose in the training. In sharp contrast, the typical and traditional designs\nof optimization methods are theory-driven, so they obtain performance\nguarantees over the classes of problems specified by the theory. The difference\nmakes L2O suitable for repeatedly solving a certain type of optimization\nproblems over a specific distribution of data, while it typically fails on\nout-of-distribution problems. The practicality of L2O depends on the type of\ntarget optimization, the chosen architecture of the method to learn, and the\ntraining procedure. This new paradigm has motivated a community of researchers\nto explore L2O and report their findings.\n\nThis article is poised to be the first comprehensive survey and benchmark of\nL2O for continuous optimization. We set up taxonomies, categorize existing\nworks and research directions, present insights, and identify open challenges.\nWe also benchmarked many existing L2O approaches on a few but representative\noptimization problems. For reproducible research and fair benchmarking\npurposes, we released our software implementation and data in the package\nOpen-L2O at https://github.com/VITA-Group/Open-L2O.",
          "link": "http://arxiv.org/abs/2103.12828",
          "publishedOn": "2021-07-05T01:54:59.199Z",
          "wordCount": 685,
          "title": "Learning to Optimize: A Primer and A Benchmark. (arXiv:2103.12828v2 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.05842",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Dwivedi_R/0/1/0/all/0/1\">Raaz Dwivedi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Mackey_L/0/1/0/all/0/1\">Lester Mackey</a>",
          "description": "We introduce kernel thinning, a new procedure for compressing a distribution\n$\\mathbb{P}$ more effectively than i.i.d. sampling or standard thinning. Given\na suitable reproducing kernel $\\mathbf{k}$ and $\\mathcal{O}(n^2)$ time, kernel\nthinning compresses an $n$-point approximation to $\\mathbb{P}$ into a\n$\\sqrt{n}$-point approximation with comparable worst-case integration error in\nthe associated reproducing kernel Hilbert space. With high probability, the\nmaximum discrepancy in integration error is\n$\\mathcal{O}_d(n^{-\\frac{1}{2}}\\sqrt{\\log n})$ for compactly supported\n$\\mathbb{P}$ and $\\mathcal{O}_d(n^{-\\frac{1}{2}} \\sqrt{(\\log n)^{d+1}\\log\\log\nn})$ for sub-exponential $\\mathbb{P}$ on $\\mathbb{R}^d$. In contrast, an\nequal-sized i.i.d. sample from $\\mathbb{P}$ suffers $\\Omega(n^{-\\frac14})$\nintegration error. Our sub-exponential guarantees resemble the classical\nquasi-Monte Carlo error rates for uniform $\\mathbb{P}$ on $[0,1]^d$ but apply\nto general distributions on $\\mathbb{R}^d$ and a wide range of common kernels.\nWe use our results to derive explicit non-asymptotic maximum mean discrepancy\nbounds for Gaussian, Mat\\'ern, and B-spline kernels and present two vignettes\nillustrating the practical benefits of kernel thinning over i.i.d. sampling and\nstandard Markov chain Monte Carlo thinning.",
          "link": "http://arxiv.org/abs/2105.05842",
          "publishedOn": "2021-07-05T01:54:59.191Z",
          "wordCount": 630,
          "title": "Kernel Thinning. (arXiv:2105.05842v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00881",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shingi_G/0/1/0/all/0/1\">Geet Shingi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saglani_H/0/1/0/all/0/1\">Harsh Saglani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_P/0/1/0/all/0/1\">Preeti Jain</a>",
          "description": "Cyberattacks are a major issues and it causes organizations great financial,\nand reputation harm. However, due to various factors, the current network\nintrusion detection systems (NIDS) seem to be insufficent. Predominant NIDS\nidentifies Cyberattacks through a handcrafted dataset of rules. Although the\nrecent applications of machine learning and deep learning have alleviated the\nenormous effort in NIDS, the security of network data has always been a prime\nconcern. However, to encounter the security problem and enable sharing among\norganizations, Federated Learning (FL) scheme is employed. Although the current\nFL systems have been successful, a network's data distribution does not always\nfit into a single global model as in FL. Thus, in such cases, having a single\nglobal model in FL is no feasible. In this paper, we propose a\nSegmented-Federated Learning (Segmented-FL) learning scheme for a more\nefficient NIDS. The Segmented-FL approach employs periodic local model\nevaluation based on which the segmentation occurs. We aim to bring similar\nnetwork environments to the same group. Further, the Segmented-FL system is\ncoupled with a weighted aggregation of local model parameters based on the\nnumber of data samples a worker possesses to further augment the performance.\nThe improved performance by our system as compared to the FL and centralized\nsystems on standard dataset further validates our system and makes a strong\ncase for extending our technique across various tasks. The solution finds its\napplication in organizations that want to collaboratively learn on diverse\nnetwork environments and protect the privacy of individual datasets.",
          "link": "http://arxiv.org/abs/2107.00881",
          "publishedOn": "2021-07-05T01:54:59.184Z",
          "wordCount": 712,
          "title": "Segmented Federated Learning for Adaptive Intrusion Detection System. (arXiv:2107.00881v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2104.00290",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gowda_T/0/1/0/all/0/1\">Thamme Gowda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mattmann_C/0/1/0/all/0/1\">Chris A Mattmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+May_J/0/1/0/all/0/1\">Jonathan May</a>",
          "description": "While there are more than 7000 languages in the world, most translation\nresearch efforts have targeted a few high-resource languages. Commercial\ntranslation systems support only one hundred languages or fewer, and do not\nmake these models available for transfer to low resource languages. In this\nwork, we present useful tools for machine translation research: MTData,\nNLCodec, and RTG. We demonstrate their usefulness by creating a multilingual\nneural machine translation model capable of translating from 500 source\nlanguages to English. We make this multilingual model readily downloadable and\nusable as a service, or as a parent model for transfer-learning to even\nlower-resource languages.",
          "link": "http://arxiv.org/abs/2104.00290",
          "publishedOn": "2021-07-05T01:54:59.164Z",
          "wordCount": 578,
          "title": "Many-to-English Machine Translation Tools, Data, and Pretrained Models. (arXiv:2104.00290v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.08955",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dovrat_S/0/1/0/all/0/1\">Shaked Dovrat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nachmani_E/0/1/0/all/0/1\">Eliya Nachmani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolf_L/0/1/0/all/0/1\">Lior Wolf</a>",
          "description": "Single channel speech separation has experienced great progress in the last\nfew years. However, training neural speech separation for a large number of\nspeakers (e.g., more than 10 speakers) is out of reach for the current methods,\nwhich rely on the Permutation Invariant Loss (PIT). In this work, we present a\npermutation invariant training that employs the Hungarian algorithm in order to\ntrain with an $O(C^3)$ time complexity, where $C$ is the number of speakers, in\ncomparison to $O(C!)$ of PIT based methods. Furthermore, we present a modified\narchitecture that can handle the increased number of speakers. Our approach\nseparates up to $20$ speakers and improves the previous results for large $C$\nby a wide margin.",
          "link": "http://arxiv.org/abs/2104.08955",
          "publishedOn": "2021-07-05T01:54:59.156Z",
          "wordCount": 599,
          "title": "Many-Speakers Single Channel Speech Separation with Optimal Permutation Training. (arXiv:2104.08955v3 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.10500",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yingjie Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1\">Xucheng Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yanru Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fanxing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Ce Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lingqiao Liu</a>",
          "description": "Weakly-supervised anomaly detection aims at learning an anomaly detector from\na limited amount of labeled data and abundant unlabeled data. Recent works\nbuild deep neural networks for anomaly detection by discriminatively mapping\nthe normal samples and abnormal samples to different regions in the feature\nspace or fitting different distributions. However, due to the limited number of\nannotated anomaly samples, directly training networks with the discriminative\nloss may not be sufficient. To overcome this issue, this paper proposes a novel\nstrategy to transform the input data into a more meaningful representation that\ncould be used for anomaly detection. Specifically, we leverage an autoencoder\nto encode the input data and utilize three factors, hidden representation,\nreconstruction residual vector, and reconstruction error, as the new\nrepresentation for the input data. This representation amounts to encode a test\nsample with its projection on the training data manifold, its direction to its\nprojection and its distance to its projection. In addition to this encoding, we\nalso propose a novel network architecture to seamlessly incorporate those three\nfactors. From our extensive experiments, the benefits of the proposed strategy\nare clearly demonstrated by its superior performance over the competitive\nmethods.",
          "link": "http://arxiv.org/abs/2105.10500",
          "publishedOn": "2021-07-05T01:54:59.149Z",
          "wordCount": 701,
          "title": "Feature Encoding with AutoEncoders for Weakly-supervised Anomaly Detection. (arXiv:2105.10500v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11083",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ramos_J/0/1/0/all/0/1\">Joao A. Candido Ramos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blonde_L/0/1/0/all/0/1\">Lionel Blond&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Armand_S/0/1/0/all/0/1\">St&#xe9;phane Armand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalousis_A/0/1/0/all/0/1\">Alexandros Kalousis</a>",
          "description": "In this work, we want to learn to model the dynamics of similar yet distinct\ngroups of interacting objects. These groups follow some common physical laws\nthat exhibit specificities that are captured through some vectorial\ndescription. We develop a model that allows us to do conditional generation\nfrom any such group given its vectorial description. Unlike previous work on\nlearning dynamical systems that can only do trajectory completion and require a\npart of the trajectory dynamics to be provided as input in generation time, we\ndo generation using only the conditioning vector with no access to generation\ntime's trajectories. We evaluate our model in the setting of modeling human\ngait and, in particular pathological human gait.",
          "link": "http://arxiv.org/abs/2106.11083",
          "publishedOn": "2021-07-05T01:54:59.142Z",
          "wordCount": 576,
          "title": "Conditional Neural Relational Inference for Interacting Systems. (arXiv:2106.11083v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.11713",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Nanni_L/0/1/0/all/0/1\">Loris Nanni</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Lumini_A/0/1/0/all/0/1\">Alessandra Lumini</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Brahnam_S/0/1/0/all/0/1\">Sheryl Brahnam</a>",
          "description": "Motivation: Automatic Anatomical Therapeutic Chemical (ATC) classification is\na critical and highly competitive area of research in bioinformatics because of\nits potential for expediting drug develop-ment and research. Predicting an\nunknown compound's therapeutic and chemical characteristics ac-cording to how\nthese characteristics affect multiple organs/systems makes automatic ATC\nclassifica-tion a challenging multi-label problem. Results: In this work, we\npropose combining multiple multi-label classifiers trained on distinct sets of\nfeatures, including sets extracted from a Bidirectional Long Short-Term Memory\nNetwork (BiLSTM). Experiments demonstrate the power of this approach, which is\nshown to outperform the best methods reported in the literature, including the\nstate-of-the-art developed by the fast.ai research group. Availability: All\nsource code developed for this study is available at\nhttps://github.com/LorisNanni. Contact: loris.nanni@unipd.it",
          "link": "http://arxiv.org/abs/2101.11713",
          "publishedOn": "2021-07-05T01:54:59.135Z",
          "wordCount": 571,
          "title": "Neural networks for Anatomical Therapeutic Chemical (ATC). (arXiv:2101.11713v2 [q-bio.QM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.00349",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Castellani_A/0/1/0/all/0/1\">Andrea Castellani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmitt_S/0/1/0/all/0/1\">Sebastian Schmitt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hammer_B/0/1/0/all/0/1\">Barbara Hammer</a>",
          "description": "In complex industrial settings, it is common practice to monitor the\noperation of machines in order to detect undesired states, adjust maintenance\nschedules, optimize system performance or collect usage statistics of\nindividual machines. In this work, we focus on estimating the power output of a\nCombined Heat and Power (CHP) machine of a medium-sized company facility by\nanalyzing the total facility power consumption. We formulate the problem as a\ntime-series classification problem where the class label represents the CHP\npower output. As the facility is fully instrumented and sensor measurements\nfrom the CHP are available, we generate the training labels in an automated\nfashion from the CHP sensor readings. However, sensor failures result in\nmislabeled training data samples which are hard to detect and remove from the\ndataset. Therefore, we propose a novel multi-task deep learning approach that\njointly trains a classifier and an autoencoder with a shared embedding\nrepresentation. The proposed approach targets to gradually correct the\nmislabelled data samples during training in a self-supervised fashion, without\nany prior assumption on the amount of label noise. We benchmark our approach on\nseveral time-series classification datasets and find it to be comparable and\nsometimes better than state-of-the-art methods. On the real-world use-case of\npredicting the CHP power output, we thoroughly evaluate the architectural\ndesign choices and show that the final architecture considerably increases the\nrobustness of the learning process and consistently beats other recent\nstate-of-the-art algorithms in the presence of unstructured as well as\nstructured label noise.",
          "link": "http://arxiv.org/abs/2105.00349",
          "publishedOn": "2021-07-05T01:54:59.116Z",
          "wordCount": 742,
          "title": "Estimating the electrical power output of industrial devices with end-to-end time-series classification in the presence of label noise. (arXiv:2105.00349v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.06006",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ding_S/0/1/0/all/0/1\">Shaojin Ding</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jia_Y/0/1/0/all/0/1\">Ye Jia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hu_K/0/1/0/all/0/1\">Ke Hu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Q/0/1/0/all/0/1\">Quan Wang</a>",
          "description": "In this paper, we propose Textual Echo Cancellation (TEC) - a framework for\ncancelling the text-to-speech (TTS) playback echo from overlapping speech\nrecordings. Such a system can largely improve speech recognition performance\nand user experience for intelligent devices such as smart speakers, as the user\ncan talk to the device while the device is still playing the TTS signal\nresponding to the previous query. We implement this system by using a novel\nsequence-to-sequence model with multi-source attention that takes both the\nmicrophone mixture signal and source text of the TTS playback as inputs, and\npredicts the enhanced audio. Experiments show that the textual information of\nthe TTS playback is critical to enhancement performance. Besides, the text\nsequence is much smaller in size compared with the raw acoustic signal of the\nTTS playback, and can be immediately transmitted to the device or ASR server\neven before the playback is synthesized. Therefore, our proposed approach\neffectively reduces Internet communication and latency compared with\nalternative approaches such as acoustic echo cancellation (AEC).",
          "link": "http://arxiv.org/abs/2008.06006",
          "publishedOn": "2021-07-05T01:54:59.108Z",
          "wordCount": 632,
          "title": "Textual Echo Cancellation. (arXiv:2008.06006v3 [eess.AS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01106",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Sun_Y/0/1/0/all/0/1\">Yifan Sun</a>, <a href=\"http://arxiv.org/find/math/1/au:+Bach_F/0/1/0/all/0/1\">Francis Bach</a>",
          "description": "The conditional gradient method (CGM) is widely used in large-scale sparse\nconvex optimization, having a low per iteration computational cost for\nstructured sparse regularizers and a greedy approach to collecting nonzeros. We\nexplore the sparsity acquiring properties of a general penalized CGM (P-CGM)\nfor convex regularizers and a reweighted penalized CGM (RP-CGM) for nonconvex\nregularizers, replacing the usual convex constraints with gauge-inspired\npenalties. This generalization does not increase the per-iteration complexity\nnoticeably. Without assuming bounded iterates or using line search, we show\n$O(1/t)$ convergence of the gap of each subproblem, which measures distance to\na stationary point. We couple this with a screening rule which is safe in the\nconvex case, converging to the true support at a rate $O(1/(\\delta^2))$ where\n$\\delta \\geq 0$ measures how close the problem is to degeneracy. In the\nnonconvex case the screening rule converges to the true support in a finite\nnumber of iterations, but is not necessarily safe in the intermediate iterates.\nIn our experiments, we verify the consistency of the method and adjust the\naggressiveness of the screening rule by tuning the concavity of the\nregularizer.",
          "link": "http://arxiv.org/abs/2107.01106",
          "publishedOn": "2021-07-05T01:54:59.096Z",
          "wordCount": 617,
          "title": "Screening for a Reweighted Penalized Conditional Gradient Method. (arXiv:2107.01106v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00967",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mi_H/0/1/0/all/0/1\">Haitao Mi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Z/0/1/0/all/0/1\">Zujie Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yafang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yi Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1\">Jing Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Melo_G/0/1/0/all/0/1\">Gerard de Melo</a>",
          "description": "Human language understanding operates at multiple levels of granularity\n(e.g., words, phrases, and sentences) with increasing levels of abstraction\nthat can be hierarchically combined. However, existing deep models with stacked\nlayers do not explicitly model any sort of hierarchical process. This paper\nproposes a recursive Transformer model based on differentiable CKY style binary\ntrees to emulate the composition process. We extend the bidirectional language\nmodel pre-training objective to this architecture, attempting to predict each\nword given its left and right abstraction nodes. To scale up our approach, we\nalso introduce an efficient pruned tree induction algorithm to enable encoding\nin just a linear number of composition steps. Experimental results on language\nmodeling and unsupervised parsing show the effectiveness of our approach.",
          "link": "http://arxiv.org/abs/2107.00967",
          "publishedOn": "2021-07-05T01:54:59.089Z",
          "wordCount": 582,
          "title": "R2D2: Recursive Transformer based on Differentiable Tree for Interpretable Hierarchical Language Modeling. (arXiv:2107.00967v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01103",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Majumdar_S/0/1/0/all/0/1\">Subhabrata Majumdar</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Chatterjee_S/0/1/0/all/0/1\">Snigdhansu Chatterjee</a>",
          "description": "High-dimensional data, where the dimension of the feature space is much\nlarger than sample size, arise in a number of statistical applications. In this\ncontext, we construct the generalized multivariate sign transformation, defined\nas a vector divided by its norm. For different choices of the norm function,\nthe resulting transformed vector adapts to certain geometrical features of the\ndata distribution. Building up on this idea, we obtain one-sample and\ntwo-sample testing procedures for mean vectors of high-dimensional data using\nthese generalized sign vectors. These tests are based on U-statistics using\nkernel inner products, do not require prohibitive assumptions, and are amenable\nto a fast randomization-based implementation. Through experiments in a number\nof data settings, we show that tests using generalized signs display higher\npower than existing tests, while maintaining nominal type-I error rates.\nFinally, we provide example applications on the MNIST and Minnesota Twin\nStudies genomic data.",
          "link": "http://arxiv.org/abs/2107.01103",
          "publishedOn": "2021-07-05T01:54:59.079Z",
          "wordCount": 583,
          "title": "Generalized Multivariate Signs for Nonparametric Hypothesis Testing in High Dimensions. (arXiv:2107.01103v1 [stat.ME])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01163",
          "author": "<a href=\"http://arxiv.org/find/cond-mat/1/au:+Baldassi_C/0/1/0/all/0/1\">Carlo Baldassi</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Lauditi_C/0/1/0/all/0/1\">Clarissa Lauditi</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Malatesta_E/0/1/0/all/0/1\">Enrico M. Malatesta</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Perugini_G/0/1/0/all/0/1\">Gabriele Perugini</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Zecchina_R/0/1/0/all/0/1\">Riccardo Zecchina</a>",
          "description": "The success of deep learning has revealed the application potential of neural\nnetworks across the sciences and opened up fundamental theoretical problems. In\nparticular, the fact that learning algorithms based on simple variants of\ngradient methods are able to find near-optimal minima of highly nonconvex loss\nfunctions is an unexpected feature of neural networks which needs to be\nunderstood in depth. Such algorithms are able to fit the data almost perfectly,\neven in the presence of noise, and yet they have excellent predictive\ncapabilities. Several empirical results have shown a reproducible correlation\nbetween the so-called flatness of the minima achieved by the algorithms and the\ngeneralization performance. At the same time, statistical physics results have\nshown that in nonconvex networks a multitude of narrow minima may coexist with\na much smaller number of wide flat minima, which generalize well. Here we show\nthat wide flat minima arise from the coalescence of minima that correspond to\nhigh-margin classifications. Despite being exponentially rare compared to\nzero-margin solutions, high-margin minima tend to concentrate in particular\nregions. These minima are in turn surrounded by other solutions of smaller and\nsmaller margin, leading to dense regions of solutions over long distances. Our\nanalysis also provides an alternative analytical method for estimating when\nflat minima appear and when algorithms begin to find solutions, as the number\nof model parameters varies.",
          "link": "http://arxiv.org/abs/2107.01163",
          "publishedOn": "2021-07-05T01:54:59.061Z",
          "wordCount": 687,
          "title": "Unveiling the structure of wide flat minima in neural networks. (arXiv:2107.01163v1 [cond-mat.dis-nn])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00927",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Marz_L/0/1/0/all/0/1\">Luisa M&#xe4;rz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schweter_S/0/1/0/all/0/1\">Stefan Schweter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poerner_N/0/1/0/all/0/1\">Nina Poerner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_B/0/1/0/all/0/1\">Benjamin Roth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1\">Hinrich Sch&#xfc;tze</a>",
          "description": "We propose new methods for in-domain and cross-domain Named Entity\nRecognition (NER) on historical data for Dutch and French. For the cross-domain\ncase, we address domain shift by integrating unsupervised in-domain data via\ncontextualized string embeddings; and OCR errors by injecting synthetic OCR\nerrors into the source domain and address data centric domain adaptation. We\npropose a general approach to imitate OCR errors in arbitrary input data. Our\ncross-domain as well as our in-domain results outperform several strong\nbaselines and establish state-of-the-art results. We publish preprocessed\nversions of the French and Dutch Europeana NER corpora.",
          "link": "http://arxiv.org/abs/2107.00927",
          "publishedOn": "2021-07-05T01:54:59.047Z",
          "wordCount": 543,
          "title": "Data Centric Domain Adaptation for Historical Text with OCR Errors. (arXiv:2107.00927v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.13922",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Rosca_M/0/1/0/all/0/1\">Mihaela Rosca</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wu_Y/0/1/0/all/0/1\">Yan Wu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Dherin_B/0/1/0/all/0/1\">Benoit Dherin</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Barrett_D/0/1/0/all/0/1\">David G. T. Barrett</a>",
          "description": "Gradient-based methods for two-player games produce rich dynamics that can\nsolve challenging problems, yet can be difficult to stabilize and understand.\nPart of this complexity originates from the discrete update steps given by\nsimultaneous or alternating gradient descent, which causes each player to drift\naway from the continuous gradient flow -- a phenomenon we call discretization\ndrift. Using backward error analysis, we derive modified continuous dynamical\nsystems that closely follow the discrete dynamics. These modified dynamics\nprovide an insight into the notorious challenges associated with zero-sum\ngames, including Generative Adversarial Networks. In particular, we identify\ndistinct components of the discretization drift that can alter performance and\nin some cases destabilize the game. Finally, quantifying discretization drift\nallows us to identify regularizers that explicitly cancel harmful forms of\ndrift or strengthen beneficial forms of drift, and thus improve performance of\nGAN training.",
          "link": "http://arxiv.org/abs/2105.13922",
          "publishedOn": "2021-07-05T01:54:59.041Z",
          "wordCount": 587,
          "title": "Discretization Drift in Two-Player Games. (arXiv:2105.13922v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01017",
          "author": "<a href=\"http://arxiv.org/find/q-fin/1/au:+Menezes_A/0/1/0/all/0/1\">Angelo Garangau Menezes</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Mastelini_S/0/1/0/all/0/1\">Saulo Martiello Mastelini</a>",
          "description": "Forecasting financial time series is considered to be a difficult task due to\nthe chaotic feature of the series. Statistical approaches have shown solid\nresults in some specific problems such as predicting market direction and\nsingle-price of stocks; however, with the recent advances in deep learning and\nbig data techniques, new promising options have arises to tackle financial time\nseries forecasting. Moreover, recent literature has shown that employing a\ncombination of statistics and machine learning may improve accuracy in the\nforecasts in comparison to single solutions. Taking into consideration the\nmentioned aspects, in this work, we proposed the MegazordNet, a framework that\nexplores statistical features within a financial series combined with a\nstructured deep learning model for time series forecasting. We evaluated our\napproach predicting the closing price of stocks in the S&P 500 using different\nmetrics, and we were able to beat single statistical and machine learning\nmethods.",
          "link": "http://arxiv.org/abs/2107.01017",
          "publishedOn": "2021-07-05T01:54:59.002Z",
          "wordCount": 598,
          "title": "MegazordNet: combining statistical and machine learning standpoints for time series forecasting. (arXiv:2107.01017v1 [q-fin.ST])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00968",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Suwannaphong_T/0/1/0/all/0/1\">Thanaphon Suwannaphong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chavana_S/0/1/0/all/0/1\">Sawaphob Chavana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tongsom_S/0/1/0/all/0/1\">Sahapol Tongsom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Palasuwan_D/0/1/0/all/0/1\">Duangdao Palasuwan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chalidabhongse_T/0/1/0/all/0/1\">Thanarat H. Chalidabhongse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anantrasirichai_N/0/1/0/all/0/1\">Nantheera Anantrasirichai</a>",
          "description": "Intestinal parasitic infection leads to several morbidities to humans\nworldwide, especially in tropical countries. The traditional diagnosis usually\nrelies on manual analysis from microscopic images which is prone to human error\ndue to morphological similarity of different parasitic eggs and abundance of\nimpurities in a sample. Many studies have developed automatic systems for\nparasite egg detection to reduce human workload. However, they work with high\nquality microscopes, which unfortunately remain unaffordable in some rural\nareas. Our work thus exploits a benefit of a low-cost USB microscope. This\ninstrument however provides poor quality of images due to limitation of\nmagnification (10x), causing difficulty in parasite detection and species\nclassification. In this paper, we propose a CNN-based technique using transfer\nlearning strategy to enhance the efficiency of automatic parasite\nclassification in poor-quality microscopic images. The patch-based technique\nwith sliding window is employed to search for location of the eggs. Two\nnetworks, AlexNet and ResNet50, are examined with a trade-off between\narchitecture size and classification performance. The results show that our\nproposed framework outperforms the state-of-the-art object recognition methods.\nOur system combined with final decision from an expert may improve the real\nfaecal examination with low-cost microscopes.",
          "link": "http://arxiv.org/abs/2107.00968",
          "publishedOn": "2021-07-05T01:54:58.985Z",
          "wordCount": 654,
          "title": "Parasitic Egg Detection and Classification in Low-cost Microscopic Images using Transfer Learning. (arXiv:2107.00968v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00948",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhiyuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1\">Haoyi Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jie Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Sijia Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boukhechba_M/0/1/0/all/0/1\">Mehdi Boukhechba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barnes_L/0/1/0/all/0/1\">Laura E. Barnes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Daqing Zhang</a>",
          "description": "Mobile Sensing Apps have been widely used as a practical approach to collect\nbehavioral and health-related information from individuals and provide timely\nintervention to promote health and well-beings, such as mental health and\nchronic cares. As the objectives of mobile sensing could be either \\emph{(a)\npersonalized medicine for individuals} or \\emph{(b) public health for\npopulations}, in this work we review the design of these mobile sensing apps,\nand propose to categorize the design of these apps/systems in two paradigms --\n\\emph{(i) Personal Sensing} and \\emph{(ii) Crowd Sensing} paradigms. While both\nsensing paradigms might incorporate with common ubiquitous sensing\ntechnologies, such as wearable sensors, mobility monitoring, mobile data\noffloading, and/or cloud-based data analytics to collect and process sensing\ndata from individuals, we present a novel taxonomy system with two major\ncomponents that can specify and classify apps/systems from aspects of the\nlife-cycle of mHealth Sensing: \\emph{(1) Sensing Task Creation \\&\nParticipation}, \\emph{(2) Health Surveillance \\& Data Collection}, and\n\\emph{(3) Data Analysis \\& Knowledge Discovery}. With respect to different\ngoals of the two paradigms, this work systematically reviews this field, and\nsummarizes the design of typical apps/systems in the view of the configurations\nand interactions between these two components. In addition to summarization,\nthe proposed taxonomy system also helps figure out the potential directions of\nmobile sensing for health from both personalized medicines and population\nhealth perspectives.",
          "link": "http://arxiv.org/abs/2107.00948",
          "publishedOn": "2021-07-05T01:54:58.978Z",
          "wordCount": 685,
          "title": "From Personalized Medicine to Population Health: A Survey of mHealth Sensing Techniques. (arXiv:2107.00948v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01032",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Shezan_S/0/1/0/all/0/1\">SK. A. Shezan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rawdah_S/0/1/0/all/0/1\">S. Rawdah</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ali_S/0/1/0/all/0/1\">Shafin Ali</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rahman_Z/0/1/0/all/0/1\">Ziaur Rahman</a>",
          "description": "The energy demand is growing daily at an accelerated pace due to the\ninternationalization and development of civilization. Yet proper economic\nutilization of additional energy generated by the Islanded Hybrid Microgrid\nSystem (IHMS) that was not consumed by the load is a major global challenge. To\nresolve the above-stated summons, this research focuses on a multi-optimal\ncombination of IHMS for the Penang Hill Resort located on Penang Island,\nMalaysia, with effective use of redundant energy. To avail this excess energy\nefficiently, an electrical heater along with a storage tank has been designed\nconcerning diversion load having proper energy management. Furthermore, the\nsystem design has adopted the HOMER Pro software for profitable and practical\nanalysis. Alongside, MATLAB Simulink had stabilized the whole system by\nrepresenting the values of 2068 and 19,072 kW that have been determined as the\napproximated peak and average load per day for the resort. Moreover, the\noptimized IHMS is comprehended of Photovoltaic (PV) cells, Diesel Generator,\nWind Turbine, Battery, and Converter. Adjacent to this, the optimized system\nensued in having a Net Present Cost (NPC) of $21.66 million, Renewable Fraction\n(RF) of 27.8%, Cost of Energy (COE) of $0.165/kWh, CO2 of 1,735,836 kg/year,\nand excess energy of 517.29MWh per annum. Since the diesel generator lead\nsystem was included in the scheme, a COE of $0.217/kWh, CO2 of 5,124,879\nkg/year, and NPC of $23.25 million were attained. The amount of excess energy\nis effectively utilized with an electrical heater as a diversion load.",
          "link": "http://arxiv.org/abs/2107.01032",
          "publishedOn": "2021-07-05T01:54:58.971Z",
          "wordCount": 736,
          "title": "Design and implementation of an islanded hybrid microgrid system for a large resort center for Penang Island with the proper application of excess energy. (arXiv:2107.01032v1 [eess.SY])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00964",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zafeiropoulos_C/0/1/0/all/0/1\">Charalampos Zafeiropoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tzortzis_I/0/1/0/all/0/1\">Ioannis N. Tzortzis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rallis_I/0/1/0/all/0/1\">Ioannis Rallis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Protopapadakis_E/0/1/0/all/0/1\">Eftychios Protopapadakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doulamis_N/0/1/0/all/0/1\">Nikolaos Doulamis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doulamis_A/0/1/0/all/0/1\">Anastasios Doulamis</a>",
          "description": "In this paper, we scrutinize the effectiveness of various clustering\ntechniques, investigating their applicability in Cultural Heritage monitoring\napplications. In the context of this paper, we detect the level of\ndecomposition and corrosion on the walls of Saint Nicholas fort in Rhodes\nutilizing hyperspectral images. A total of 6 different clustering approaches\nhave been evaluated over a set of 14 different orthorectified hyperspectral\nimages. Experimental setup in this study involves K-means, Spectral, Meanshift,\nDBSCAN, Birch and Optics algorithms. For each of these techniques we evaluate\nits performance by the use of performance metrics such as Calinski-Harabasz,\nDavies-Bouldin indexes and Silhouette value. In this approach, we evaluate the\noutcomes of the clustering methods by comparing them with a set of annotated\nimages which denotes the ground truth regarding the decomposition and/or\ncorrosion area of the original images. The results depict that a few clustering\ntechniques applied on the given dataset succeeded decent accuracy, precision,\nrecall and f1 scores. Eventually, it was observed that the deterioration was\ndetected quite accurately.",
          "link": "http://arxiv.org/abs/2107.00964",
          "publishedOn": "2021-07-05T01:54:58.965Z",
          "wordCount": 616,
          "title": "Evaluating the Usefulness of Unsupervised monitoring in Cultural Heritage Monuments. (arXiv:2107.00964v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01076",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tsakalidis_A/0/1/0/all/0/1\">Adam Tsakalidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Basile_P/0/1/0/all/0/1\">Pierpaolo Basile</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bazzi_M/0/1/0/all/0/1\">Marya Bazzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cucuringu_M/0/1/0/all/0/1\">Mihai Cucuringu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McGillivray_B/0/1/0/all/0/1\">Barbara McGillivray</a>",
          "description": "Lexical semantic change (detecting shifts in the meaning and usage of words)\nis an important task for social and cultural studies as well as for Natural\nLanguage Processing applications. Diachronic word embeddings (time-sensitive\nvector representations of words that preserve their meaning) have become the\nstandard resource for this task. However, given the significant computational\nresources needed for their generation, very few resources exist that make\ndiachronic word embeddings available to the scientific community.\n\nIn this paper we present DUKweb, a set of large-scale resources designed for\nthe diachronic analysis of contemporary English. DUKweb was created from the\nJISC UK Web Domain Dataset (1996-2013), a very large archive which collects\nresources from the Internet Archive that were hosted on domains ending in\n`.uk'. DUKweb consists of a series word co-occurrence matrices and two types of\nword embeddings for each year in the JISC UK Web Domain dataset. We show the\nreuse potential of DUKweb and its quality standards via a case study on word\nmeaning change detection.",
          "link": "http://arxiv.org/abs/2107.01076",
          "publishedOn": "2021-07-05T01:54:58.957Z",
          "wordCount": 617,
          "title": "DUKweb: Diachronic word representations from the UK Web Archive corpus. (arXiv:2107.01076v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01002",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Karmanov_I/0/1/0/all/0/1\">Ilia Karmanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zanjani_F/0/1/0/all/0/1\">Farhad G. Zanjani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Merlin_S/0/1/0/all/0/1\">Simone Merlin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kadampot_I/0/1/0/all/0/1\">Ishaque Kadampot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dijkman_D/0/1/0/all/0/1\">Daniel Dijkman</a>",
          "description": "We introduce WiCluster, a new machine learning (ML) approach for passive\nindoor positioning using radio frequency (RF) channel state information (CSI).\nWiCluster can predict both a zone-level position and a precise 2D or 3D\nposition, without using any precise position labels during training. Prior\nCSI-based indoor positioning work has relied on non-parametric approaches using\ndigital signal-processing (DSP) and, more recently, parametric approaches\n(e.g., fully supervised ML methods). However these do not handle the complexity\nof real-world environments well and do not meet requirements for large-scale\ncommercial deployments: the accuracy of DSP-based method deteriorates\nsignificantly in non-line-of-sight conditions, while supervised ML methods need\nlarge amounts of hard-to-acquire centimeter accuracy position labels. In\ncontrast, WiCluster is both precise and requires weaker label-information that\ncan be easily collected. Our first contribution is a novel dimensionality\nreduction method for charting. It combines a triplet-loss with a multi-scale\nclustering-loss to map the high-dimensional CSI representation to a 2D/3D\nlatent space. Our second contribution is two weakly supervised losses that map\nthis latent space into a Cartesian map, resulting in meter-accuracy position\nresults. These losses only require simple to acquire priors: a sketch of the\nfloorplan, approximate location of access-point locations and a few CSI packets\nthat are labeled with the corresponding zone in the floorplan. Thirdly, we\nreport results and a robustness study for 2D positioning in a single-floor\noffice building and 3D positioning in a two-floor home to show the robustness\nof our method.",
          "link": "http://arxiv.org/abs/2107.01002",
          "publishedOn": "2021-07-05T01:54:58.940Z",
          "wordCount": 696,
          "title": "WiCluster: Passive Indoor 2D/3D Positioning using WiFi without Precise Labels. (arXiv:2107.01002v1 [cs.NI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01025",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jitani_A/0/1/0/all/0/1\">Anirudha Jitani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahajan_A/0/1/0/all/0/1\">Aditya Mahajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zhongwen Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abou_zeid_H/0/1/0/all/0/1\">Hatem Abou-zeid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fapi_E/0/1/0/all/0/1\">Emmanuel T. Fapi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Purmehdi_H/0/1/0/all/0/1\">Hakimeh Purmehdi</a>",
          "description": "Mobile Edge Computing (MEC) refers to the concept of placing computational\ncapability and applications at the edge of the network, providing benefits such\nas reduced latency in handling client requests, reduced network congestion, and\nimproved performance of applications. The performance and reliability of MEC\nare degraded significantly when one or several edge servers in the cluster are\noverloaded. Especially when a server crashes due to the overload, it causes\nservice failures in MEC. In this work, an adaptive admission control policy to\nprevent edge node from getting overloaded is presented. This approach is based\non a recently-proposed low complexity RL (Reinforcement Learning) algorithm\ncalled SALMUT (Structure-Aware Learning for Multiple Thresholds), which\nexploits the structure of the optimal admission control policy in multi-class\nqueues for an average-cost setting. We extend the framework to work for node\noverload-protection problem in a discounted-cost setting. The proposed solution\nis validated using several scenarios mimicking real-world deployments in two\ndifferent settings - computer simulations and a docker testbed. Our empirical\nevaluations show that the total discounted cost incurred by SALMUT is similar\nto state-of-the-art deep RL algorithms such as PPO (Proximal Policy\nOptimization) and A2C (Advantage Actor Critic) but requires an order of\nmagnitude less time to train, outputs easily interpretable policy, and can be\ndeployed in an online manner.",
          "link": "http://arxiv.org/abs/2107.01025",
          "publishedOn": "2021-07-05T01:54:58.932Z",
          "wordCount": 671,
          "title": "Structure-aware reinforcement learning for node-overload protection in mobile edge computing. (arXiv:2107.01025v1 [cs.NI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01001",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_P/0/1/0/all/0/1\">Peng Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quek_T/0/1/0/all/0/1\">Tony Q. S. Quek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jingxuan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1\">Chaoqun You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1\">Xianbin Cao</a>",
          "description": "This paper investigates the problem of providing ultra-reliable and\nenergy-efficient virtual reality (VR) experiences for wireless mobile users. To\nensure reliable ultra-high-definition (UHD) video frame delivery to mobile\nusers and enhance their immersive visual experiences, a coordinated multipoint\n(CoMP) transmission technique and millimeter wave (mmWave) communications are\nexploited. Owing to user movement and time-varying wireless channels, the\nwireless VR experience enhancement problem is formulated as a\nsequence-dependent and mixed-integer problem with a goal of maximizing users'\nfeeling of presence (FoP) in the virtual world, subject to power consumption\nconstraints on access points (APs) and users' head-mounted displays (HMDs). The\nproblem, however, is hard to be directly solved due to the lack of users'\naccurate tracking information and the sequence-dependent and mixed-integer\ncharacteristics. To overcome this challenge, we develop a parallel echo state\nnetwork (ESN) learning method to predict users' tracking information by\ntraining fresh and historical tracking samples separately collected by APs.\nWith the learnt results, we propose a deep reinforcement learning (DRL) based\noptimization algorithm to solve the formulated problem. In this algorithm, we\nimplement deep neural networks (DNNs) as a scalable solution to produce integer\ndecision variables and solving a continuous power control problem to criticize\nthe integer decision variables. Finally, the performance of the proposed\nalgorithm is compared with various benchmark algorithms, and the impact of\ndifferent design parameters is also discussed. Simulation results demonstrate\nthat the proposed algorithm is more 4.14% energy-efficient than the benchmark\nalgorithms.",
          "link": "http://arxiv.org/abs/2107.01001",
          "publishedOn": "2021-07-05T01:54:58.924Z",
          "wordCount": 696,
          "title": "Feeling of Presence Maximization: mmWave-Enabled Virtual Reality Meets Deep Reinforcement Learning. (arXiv:2107.01001v1 [cs.NI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00998",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1\">Albert Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiu_C/0/1/0/all/0/1\">Chun Yin Chiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hains_G/0/1/0/all/0/1\">Ga&#xe9;tan Hains</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Humphrey_J/0/1/0/all/0/1\">Jack Humphrey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fuhrmann_H/0/1/0/all/0/1\">Hans Fuhrmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khmelevsky_Y/0/1/0/all/0/1\">Youry Khmelevsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mazur_C/0/1/0/all/0/1\">Chris Mazur</a>",
          "description": "Gamers Private Network (GPN) is a client/server technology that guarantees a\nconnection for online video games that is more reliable and lower latency than\na standard internet connection. Users of the GPN technology benefit from a\nstable and high-quality gaming experience for online games, which are hosted\nand played across the world. After transforming a massive volume of raw\nnetworking data collected by WTFast, we have structured the cleaned data into a\nspecial-purpose data warehouse and completed the extensive analysis using\nmachine learning and neural nets technologies, and business intelligence tools.\nThese analyses demonstrate the ability to predict and quantify changes in the\nnetwork and demonstrate the benefits gained from the use of a GPN for users\nwhen connected to an online game session.",
          "link": "http://arxiv.org/abs/2107.00998",
          "publishedOn": "2021-07-05T01:54:58.916Z",
          "wordCount": 595,
          "title": "Gamers Private Network Performance Forecasting. From Raw Data to the Data Warehouse with Machine Learning and Neural Nets. (arXiv:2107.00998v1 [cs.NI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00992",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jian Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zimin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Monperrus_M/0/1/0/all/0/1\">Martin Monperrus</a>",
          "description": "Semantic code search is about finding semantically relevant code snippets for\na given natural language query. In the state-of-the-art approaches, the\nsemantic similarity between code and query is quantified as the distance of\ntheir representation in the shared vector space. In this paper, to improve the\nvector space, we introduce tree-serialization methods on a simplified form of\nAST and build the multimodal representation for the code data. We conduct\nextensive experiments using a single corpus that is large-scale and\nmulti-language: CodeSearchNet. Our results show that both our tree-serialized\nrepresentations and multimodal learning model improve the performance of neural\ncode search. Last, we define two intuitive quantification metrics oriented to\nthe completeness of semantic and syntactic information of the code data.",
          "link": "http://arxiv.org/abs/2107.00992",
          "publishedOn": "2021-07-05T01:54:58.901Z",
          "wordCount": 556,
          "title": "Multimodal Representation for Neural Code Search. (arXiv:2107.00992v1 [cs.SE])"
        },
        {
          "id": "http://arxiv.org/abs/2104.06387",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1\">Pengfei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jinlan Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yang Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_W/0/1/0/all/0/1\">Weizhe Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shuaicheng Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1\">Junqi Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yixin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Z/0/1/0/all/0/1\">Zihuiwen Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_Z/0/1/0/all/0/1\">Zi-Yi Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>",
          "description": "With the rapid development of NLP research, leaderboards have emerged as one\ntool to track the performance of various systems on various NLP tasks. They are\neffective in this goal to some extent, but generally present a rather\nsimplistic one-dimensional view of the submitted systems, communicated only\nthrough holistic accuracy numbers. In this paper, we present a new\nconceptualization and implementation of NLP evaluation: the ExplainaBoard,\nwhich in addition to inheriting the functionality of the standard leaderboard,\nalso allows researchers to (i) diagnose strengths and weaknesses of a single\nsystem (e.g.~what is the best-performing system bad at?) (ii) interpret\nrelationships between multiple systems. (e.g.~where does system A outperform\nsystem B? What if we combine systems A, B, and C?) and (iii) examine prediction\nresults closely (e.g.~what are common errors made by multiple systems, or in\nwhat contexts do particular errors occur?). So far, ExplainaBoard covers more\nthan 400 systems, 50 datasets, 40 languages, and 12 tasks. ExplainaBoard keeps\nupdated and is recently upgraded by supporting (1) multilingual multi-task\nbenchmark, (2) meta-evaluation, and (3) more complicated task: machine\ntranslation, which reviewers also suggested.} We not only released an online\nplatform on the website \\url{this http URL} but also make\nour evaluation tool an API with MIT Licence at Github\n\\url{https://github.com/neulab/explainaBoard} and PyPi\n\\url{https://pypi.org/project/interpret-eval/} that allows users to\nconveniently assess their models offline. We additionally release all output\nfiles from systems that we have run or collected to motivate \"output-driven\"\nresearch in the future.",
          "link": "http://arxiv.org/abs/2104.06387",
          "publishedOn": "2021-07-05T01:54:58.894Z",
          "wordCount": 724,
          "title": "ExplainaBoard: An Explainable Leaderboard for NLP. (arXiv:2104.06387v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1912.09379",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gepperth_A/0/1/0/all/0/1\">Alexander Gepperth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfulb_B/0/1/0/all/0/1\">Benedikt Pf&#xfc;lb</a>",
          "description": "We present an approach for efficiently training Gaussian Mixture Model (GMM)\nby Stochastic Gradient Descent (SGD) with non-stationary, high-dimensional\nstreaming data. Our training scheme does not require data-driven parameter\ninitialization (e.g., k-means) and can thus be trained based on a random\ninitialization. Furthermore, the approach allows mini-batch sizes as low as 1,\nwhich are typical for streaming-data settings. Major problems in such settings\nare undesirable local optima during early training phases and numerical\ninstabilities due to high data dimensionalities. We introduce an adaptive\nannealing procedure to address the first problem, whereas numerical\ninstabilities are eliminated by using an exponential-free approximation to the\nstandard GMM log-likelihood. Experiments on a variety of visual and non-visual\nbenchmarks show that our SGD approach can be trained completely without, for\ninstance, k-means based centroid initialization. It also compares favorably to\nan online variant of Expectation-Maximization (EM) - stochastic EM (sEM), which\nit outperforms by a large margin for very high-dimensional data.",
          "link": "http://arxiv.org/abs/1912.09379",
          "publishedOn": "2021-07-05T01:54:58.854Z",
          "wordCount": 635,
          "title": "Gradient-based training of Gaussian Mixture Models for High-Dimensional Streaming Data. (arXiv:1912.09379v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00758",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+dEon_G/0/1/0/all/0/1\">Greg d&#x27;Eon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+dEon_J/0/1/0/all/0/1\">Jason d&#x27;Eon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wright_J/0/1/0/all/0/1\">James R. Wright</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leyton_Brown_K/0/1/0/all/0/1\">Kevin Leyton-Brown</a>",
          "description": "Supervised learning models often make systematic errors on rare subsets of\nthe data. However, such systematic errors can be difficult to identify, as\nmodel performance can only be broken down across sensitive groups when these\ngroups are known and explicitly labelled. This paper introduces a method for\ndiscovering systematic errors, which we call the spotlight. The key idea is\nthat similar inputs tend to have similar representations in the final hidden\nlayer of a neural network. We leverage this structure by \"shining a spotlight\"\non this representation space to find contiguous regions where the model\nperforms poorly. We show that the spotlight surfaces semantically meaningful\nareas of weakness in a wide variety of model architectures, including image\nclassifiers, language models, and recommender systems.",
          "link": "http://arxiv.org/abs/2107.00758",
          "publishedOn": "2021-07-05T01:54:58.836Z",
          "wordCount": 569,
          "title": "The Spotlight: A General Method for Discovering Systematic Errors in Deep Learning Models. (arXiv:2107.00758v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00844",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Younsik Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_D/0/1/0/all/0/1\">Dongjin Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huh_S/0/1/0/all/0/1\">Soonsang Huh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1\">Dongjoon Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeong_S/0/1/0/all/0/1\">Sunbeom Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwon_J/0/1/0/all/0/1\">Junyoung Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Minsoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Donghan Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ryu_H/0/1/0/all/0/1\">Hanyoung Ryu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_J/0/1/0/all/0/1\">Jongkeun Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kyung_W/0/1/0/all/0/1\">Wonshik Kyung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sohn_B/0/1/0/all/0/1\">Byungmin Sohn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Suyoung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hyun_J/0/1/0/all/0/1\">Jounghoon Hyun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yeonghoon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Yeongkwan Kimand Changyoung Kim</a>",
          "description": "In spectroscopic experiments, data acquisition in multi-dimensional phase\nspace may require long acquisition time, owing to the large phase space volume\nto be covered. In such case, the limited time available for data acquisition\ncan be a serious constraint for experiments in which multidimensional spectral\ndata are acquired. Here, taking angle-resolved photoemission spectroscopy\n(ARPES) as an example, we demonstrate a denoising method that utilizes deep\nlearning as an intelligent way to overcome the constraint. With readily\navailable ARPES data and random generation of training data set, we\nsuccessfully trained the denoising neural network without overfitting. The\ndenoising neural network can remove the noise in the data while preserving its\nintrinsic information. We show that the denoising neural network allows us to\nperform similar level of second-derivative and line shape analysis on data\ntaken with two orders of magnitude less acquisition time. The importance of our\nmethod lies in its applicability to any multidimensional spectral data that are\nsusceptible to statistical noise.",
          "link": "http://arxiv.org/abs/2107.00844",
          "publishedOn": "2021-07-05T01:54:58.830Z",
          "wordCount": 645,
          "title": "Deep learning-based statistical noise reduction for multidimensional spectral data. (arXiv:2107.00844v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01130",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zabihzadeh_D/0/1/0/all/0/1\">Davood Zabihzadeh</a>",
          "description": "Deep Metric Learning (DML) learns a non-linear semantic embedding from input\ndata that brings similar pairs together while keeps dissimilar data away from\neach other. To this end, many different methods are proposed in the last decade\nwith promising results in various applications. The success of a DML algorithm\ngreatly depends on its loss function. However, no loss function is perfect, and\nit deals only with some aspects of an optimal similarity embedding. Besides,\nthe generalizability of the DML on unseen categories during the test stage is\nan important matter that is not considered by existing loss functions. To\naddress these challenges, we propose novel approaches to combine different\nlosses built on top of a shared deep feature extractor. The proposed ensemble\nof losses enforces the deep model to extract features that are consistent with\nall losses. Since the selected losses are diverse and each emphasizes different\naspects of an optimal semantic embedding, our effective combining methods yield\na considerable improvement over any individual loss and generalize well on\nunseen categories. Here, there is no limitation in choosing loss functions, and\nour methods can work with any set of existing ones. Besides, they can optimize\neach loss function as well as its weight in an end-to-end paradigm with no need\nto adjust any hyper-parameter. We evaluate our methods on some popular datasets\nfrom the machine vision domain in conventional Zero-Shot-Learning (ZSL)\nsettings. The results are very encouraging and show that our methods outperform\nall baseline losses by a large margin in all datasets.",
          "link": "http://arxiv.org/abs/2107.01130",
          "publishedOn": "2021-07-05T01:54:58.815Z",
          "wordCount": 713,
          "title": "Ensemble of Loss Functions to Improve Generalizability of Deep Metric Learning methods. (arXiv:2107.01130v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.11272",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhiqin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>",
          "description": "We introduce Neural Marching Cubes (NMC), a data-driven approach for\nextracting a triangle mesh from a discretized implicit field. Classical MC is\ndefined by coarse tessellation templates isolated to individual cubes. While\nmore refined tessellations have been proposed, they all make heuristic\nassumptions, such as trilinearity, when determining the vertex positions and\nlocal mesh topologies in each cube. In principle, none of these approaches can\nreconstruct geometric features that reveal coherence or dependencies between\nnearby cubes (e.g., a sharp edge), as such information is unaccounted for,\nresulting in poor estimates of the true underlying implicit field. To tackle\nthese challenges, we re-cast MC from a deep learning perspective, by designing\ntessellation templates more apt at preserving geometric features, and learning\nthe vertex positions and mesh topologies from training meshes, to account for\ncontextual information from nearby cubes. We develop a compact per-cube\nparameterization to represent the output triangle mesh, while being compatible\nwith neural processing, so that a simple 3D convolutional network can be\nemployed for the training. We show that all topological cases in each cube that\nare applicable to our design can be easily derived using our representation,\nand the resulting tessellations can also be obtained naturally and efficiently\nby following a few design guidelines. In addition, our network learns local\nfeatures with limited receptive fields, hence it generalizes well to new shapes\nand new datasets. We evaluate our neural MC approach by quantitative and\nqualitative comparisons to all well-known MC variants. In particular, we\ndemonstrate the ability of our network to recover sharp features such as edges\nand corners, a long-standing issue of MC and its variants. Our network also\nreconstructs local mesh topologies more accurately than previous approaches.",
          "link": "http://arxiv.org/abs/2106.11272",
          "publishedOn": "2021-07-05T01:54:58.780Z",
          "wordCount": 738,
          "title": "Neural Marching Cubes. (arXiv:2106.11272v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.00122",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiulong Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Hui Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1\">Yang Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1\">Shihao Ji</a>",
          "description": "Joint Energy-based Model (JEM) of Grathwohl et al. shows that a standard\nsoftmax classifier can be reinterpreted as an energy-based model (EBM) for the\njoint distribution p(x,y); the resulting model can be optimized to improve\ncalibration, robustness, and out-of-distribution detection, while generating\nsamples rivaling the quality of recent GAN-based approaches. However, the\nsoftmax classifier that JEM exploits is inherently discriminative and its\nlatent feature space is not well formulated as probabilistic distributions,\nwhich may hinder its potential for image generation and incur training\ninstability. We hypothesize that generative classifiers, such as Linear\nDiscriminant Analysis (LDA), might be more suitable for image generation since\ngenerative classifiers model the data generation process explicitly. This paper\ntherefore investigates an LDA classifier for image classification and\ngeneration. In particular, the Max-Mahalanobis Classifier (MMC), a special case\nof LDA, fits our goal very well. We show that our Generative MMC (GMMC) can be\ntrained discriminatively, generatively, or jointly for image classification and\ngeneration. Extensive experiments on multiple datasets show that GMMC achieves\nstate-of-the-art discriminative and generative performances, while\noutperforming JEM in calibration, adversarial robustness, and\nout-of-distribution detection by a significant margin. Our source code is\navailable at https://github.com/sndnyang/GMMC.",
          "link": "http://arxiv.org/abs/2101.00122",
          "publishedOn": "2021-07-05T01:54:58.774Z",
          "wordCount": 692,
          "title": "Generative Max-Mahalanobis Classifiers for Image Classification, Generation and More. (arXiv:2101.00122v4 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12672",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1\">Yi Tay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_V/0/1/0/all/0/1\">Vinh Q. Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruder_S/0/1/0/all/0/1\">Sebastian Ruder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_J/0/1/0/all/0/1\">Jai Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_H/0/1/0/all/0/1\">Hyung Won Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bahri_D/0/1/0/all/0/1\">Dara Bahri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1\">Zhen Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baumgartner_S/0/1/0/all/0/1\">Simon Baumgartner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1\">Cong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metzler_D/0/1/0/all/0/1\">Donald Metzler</a>",
          "description": "State-of-the-art models in natural language processing rely on separate rigid\nsubword tokenization algorithms, which limit their generalization ability and\nadaptation to new settings. In this paper, we propose a new model inductive\nbias that learns a subword tokenization end-to-end as part of the model. To\nthis end, we introduce a soft gradient-based subword tokenization module (GBST)\nthat automatically learns latent subword representations from characters in a\ndata-driven fashion. Concretely, GBST enumerates candidate subword blocks and\nlearns to score them in a position-wise fashion using a block scoring network.\nWe additionally introduce Charformer, a deep Transformer model that integrates\nGBST and operates on the byte level. Via extensive experiments on English GLUE,\nmultilingual, and noisy text datasets, we show that Charformer outperforms a\nseries of competitive byte-level baselines while generally performing on par\nand sometimes outperforming subword-based models. Additionally, Charformer is\nfast, improving the speed of both vanilla byte-level and subword-level\nTransformers by 28%-100% while maintaining competitive quality. We believe this\nwork paves the way for highly performant token-free models that are trained\ncompletely end-to-end.",
          "link": "http://arxiv.org/abs/2106.12672",
          "publishedOn": "2021-07-05T01:54:58.767Z",
          "wordCount": 665,
          "title": "Charformer: Fast Character Transformers via Gradient-based Subword Tokenization. (arXiv:2106.12672v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00956",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kovac_G/0/1/0/all/0/1\">Grgur Kova&#x10d;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Portelas_R/0/1/0/all/0/1\">R&#xe9;my Portelas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hofmann_K/0/1/0/all/0/1\">Katja Hofmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oudeyer_P/0/1/0/all/0/1\">Pierre-Yves Oudeyer</a>",
          "description": "Building embodied autonomous agents capable of participating in social\ninteractions with humans is one of the main challenges in AI. Within the Deep\nReinforcement Learning (DRL) field, this objective motivated multiple works on\nembodied language use. However, current approaches focus on language as a\ncommunication tool in very simplified and non-diverse social situations: the\n\"naturalness\" of language is reduced to the concept of high vocabulary size and\nvariability. In this paper, we argue that aiming towards human-level AI\nrequires a broader set of key social skills: 1) language use in complex and\nvariable social contexts; 2) beyond language, complex embodied communication in\nmultimodal settings within constantly evolving social worlds. We explain how\nconcepts from cognitive sciences could help AI to draw a roadmap towards\nhuman-like intelligence, with a focus on its social dimensions. As a first\nstep, we propose to expand current research to a broader set of core social\nskills. To do this, we present SocialAI, a benchmark to assess the acquisition\nof social skills of DRL agents using multiple grid-world environments featuring\nother (scripted) social agents. We then study the limits of a recent SOTA DRL\napproach when tested on SocialAI and discuss important next steps towards\nproficient social agents. Videos and code are available at\nhttps://sites.google.com/view/socialai.",
          "link": "http://arxiv.org/abs/2107.00956",
          "publishedOn": "2021-07-05T01:54:58.752Z",
          "wordCount": 663,
          "title": "SocialAI: Benchmarking Socio-Cognitive Abilities in Deep Reinforcement Learning Agents. (arXiv:2107.00956v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00783",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yunhan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Linan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1\">Quanyan Zhu</a>",
          "description": "The rapid growth in the number of devices and their connectivity has enlarged\nthe attack surface and weakened cyber systems. As attackers become increasingly\nsophisticated and resourceful, mere reliance on traditional cyber protection,\nsuch as intrusion detection, firewalls, and encryption, is insufficient to\nsecure cyber systems. Cyber resilience provides a new security paradigm that\ncomplements inadequate protection with resilience mechanisms. A Cyber-Resilient\nMechanism (CRM) adapts to the known or zero-day threats and uncertainties in\nreal-time and strategically responds to them to maintain the critical functions\nof the cyber systems. Feedback architectures play a pivotal role in enabling\nthe online sensing, reasoning, and actuation of the CRM. Reinforcement Learning\n(RL) is an important class of algorithms that epitomize the feedback\narchitectures for cyber resiliency, allowing the CRM to provide dynamic and\nsequential responses to attacks with limited prior knowledge of the attacker.\nIn this work, we review the literature on RL for cyber resiliency and discuss\nthe cyber-resilient defenses against three major types of vulnerabilities,\ni.e., posture-related, information-related, and human-related vulnerabilities.\nWe introduce moving target defense, defensive cyber deception, and assistive\nhuman security technologies as three application domains of CRMs to elaborate\non their designs. The RL technique also has vulnerabilities itself. We explain\nthe major vulnerabilities of RL and present several attack models in which the\nattacks target the rewards, the measurements, and the actuators. We show that\nthe attacker can trick the RL agent into learning a nefarious policy with\nminimum attacking effort, which shows serious security concerns for RL-enabled\nsystems. Finally, we discuss the future challenges of RL for cyber security and\nresiliency and emerging applications of RL-based CRMs.",
          "link": "http://arxiv.org/abs/2107.00783",
          "publishedOn": "2021-07-05T01:54:58.746Z",
          "wordCount": 708,
          "title": "Reinforcement Learning for Feedback-Enabled Cyber Resilience. (arXiv:2107.00783v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00957",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Salem_T/0/1/0/all/0/1\">Tareq Si Salem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neglia_G/0/1/0/all/0/1\">Giovanni Neglia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carra_D/0/1/0/all/0/1\">Damiano Carra</a>",
          "description": "Similarity search is a key operation in multimedia retrieval systems and\nrecommender systems, and it will play an important role also for future machine\nlearning and augmented reality applications. When these systems need to serve\nlarge objects with tight delay constraints, edge servers close to the end-user\ncan operate as similarity caches to speed up the retrieval. In this paper we\npresent A\\c{C}AI, a new similarity caching policy which improves on the state\nof the art by using (i) an (approximate) index for the whole catalog to decide\nwhich objects to serve locally and which to retrieve from the remote server,\nand (ii) a mirror ascent algorithm to update the set of local objects with\nstrong guarantees even when the request process does not exhibit any\nstatistical regularity.",
          "link": "http://arxiv.org/abs/2107.00957",
          "publishedOn": "2021-07-05T01:54:58.739Z",
          "wordCount": 566,
          "title": "A\\c{C}AI: Ascent Similarity Caching with Approximate Indexes. (arXiv:2107.00957v1 [cs.NI])"
        },
        {
          "id": "http://arxiv.org/abs/2104.05592",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Naumann_P/0/1/0/all/0/1\">Philip Naumann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ntoutsi_E/0/1/0/all/0/1\">Eirini Ntoutsi</a>",
          "description": "Counterfactuals have become a popular technique nowadays for interacting with\nblack-box machine learning models and understanding how to change a particular\ninstance to obtain a desired outcome from the model. However, most existing\napproaches assume instant materialization of these changes, ignoring that they\nmay require effort and a specific order of application. Recently, methods have\nbeen proposed that also consider the order in which actions are applied,\nleading to the so-called sequential counterfactual generation problem.\n\nIn this work, we propose a model-agnostic method for sequential\ncounterfactual generation. We formulate the task as a multi-objective\noptimization problem and present a genetic algorithm approach to find optimal\nsequences of actions leading to the counterfactuals. Our cost model considers\nnot only the direct effect of an action, but also its consequences.\nExperimental results show that compared to state-of-the-art, our approach\ngenerates less costly solutions, is more efficient and provides the user with a\ndiverse set of solutions to choose from.",
          "link": "http://arxiv.org/abs/2104.05592",
          "publishedOn": "2021-07-05T01:54:58.734Z",
          "wordCount": 620,
          "title": "Consequence-aware Sequential Counterfactual Generation. (arXiv:2104.05592v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1912.05686",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chang_D/0/1/0/all/0/1\">Daniel T. Chang</a>",
          "description": "Deep learning models are full of hyperparameters, which are set manually\nbefore the learning process can start. To find the best configuration for these\nhyperparameters in such a high dimensional space, with time-consuming and\nexpensive model training / validation, is not a trivial challenge. Bayesian\noptimization is a powerful tool for the joint optimization of hyperparameters,\nefficiently trading off exploration and exploitation of the hyperparameter\nspace. In this paper, we discuss Bayesian hyperparameter optimization,\nincluding hyperparameter optimization, Bayesian optimization, and Gaussian\nprocesses. We also review BoTorch, GPyTorch and Ax, the new open-source\nframeworks that we use for Bayesian optimization, Gaussian process inference\nand adaptive experimentation, respectively. For experimentation, we apply\nBayesian hyperparameter optimization, for optimizing group weights, to weighted\ngroup pooling, which couples unsupervised tiered graph autoencoders learning\nand supervised graph prediction learning for molecular graphs. We find that Ax,\nBoTorch and GPyTorch together provide a simple-to-use but powerful framework\nfor Bayesian hyperparameter optimization, using Ax's high-level API that\nconstructs and runs a full optimization loop and returns the best\nhyperparameter configuration.",
          "link": "http://arxiv.org/abs/1912.05686",
          "publishedOn": "2021-07-05T01:54:58.726Z",
          "wordCount": 629,
          "title": "Bayesian Hyperparameter Optimization with BoTorch, GPyTorch and Ax. (arXiv:1912.05686v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.07948",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Ziheng Wang</a>",
          "description": "The last few years have seen gigantic leaps in algorithms and systems to\nsupport efficient deep learning inference. Pruning and quantization algorithms\ncan now consistently compress neural networks by an order of magnitude. For a\ncompressed neural network, a multitude of inference frameworks have been\ndesigned to maximize the performance of the target hardware. While we find\nmature support for quantized neural networks in production frameworks such as\nOpenVINO and MNN, support for pruned sparse neural networks is still lacking.\nTo tackle this challenge, we present SparseDNN, a sparse deep learning\ninference engine targeting CPUs. We present both kernel-level optimizations\nwith a sparse code generator to accelerate sparse operators and novel\nnetwork-level optimizations catering to sparse networks. We show that our\nsparse code generator can achieve significant speedups over state-of-the-art\nsparse and dense libraries. On end-to-end benchmarks such as Huggingface\npruneBERT, SparseDNN achieves up to 5x throughput improvement over dense\ninference with state-of-the-art OpenVINO.",
          "link": "http://arxiv.org/abs/2101.07948",
          "publishedOn": "2021-07-05T01:54:58.721Z",
          "wordCount": 612,
          "title": "SparseDNN: Fast Sparse Deep Learning Inference on CPUs. (arXiv:2101.07948v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00848",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Ke_N/0/1/0/all/0/1\">Nan Rosemary Ke</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Didolkar_A/0/1/0/all/0/1\">Aniket Didolkar</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Mittal_S/0/1/0/all/0/1\">Sarthak Mittal</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Goyal_A/0/1/0/all/0/1\">Anirudh Goyal</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lajoie_G/0/1/0/all/0/1\">Guillaume Lajoie</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Bauer_S/0/1/0/all/0/1\">Stefan Bauer</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Rezende_D/0/1/0/all/0/1\">Danilo Rezende</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Bengio_Y/0/1/0/all/0/1\">Yoshua Bengio</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Mozer_M/0/1/0/all/0/1\">Michael Mozer</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Pal_C/0/1/0/all/0/1\">Christopher Pal</a>",
          "description": "Inducing causal relationships from observations is a classic problem in\nmachine learning. Most work in causality starts from the premise that the\ncausal variables themselves are observed. However, for AI agents such as robots\ntrying to make sense of their environment, the only observables are low-level\nvariables like pixels in images. To generalize well, an agent must induce\nhigh-level variables, particularly those which are causal or are affected by\ncausal variables. A central goal for AI and causality is thus the joint\ndiscovery of abstract representations and causal structure. However, we note\nthat existing environments for studying causal induction are poorly suited for\nthis objective because they have complicated task-specific causal graphs which\nare impossible to manipulate parametrically (e.g., number of nodes, sparsity,\ncausal chain length, etc.). In this work, our goal is to facilitate research in\nlearning representations of high-level variables as well as causal structures\namong them. In order to systematically probe the ability of methods to identify\nthese variables and structures, we design a suite of benchmarking RL\nenvironments. We evaluate various representation learning algorithms from the\nliterature and find that explicitly incorporating structure and modularity in\nmodels can help causal induction in model-based reinforcement learning.",
          "link": "http://arxiv.org/abs/2107.00848",
          "publishedOn": "2021-07-05T01:54:58.704Z",
          "wordCount": 653,
          "title": "Systematic Evaluation of Causal Discovery in Visual Model Based Reinforcement Learning. (arXiv:2107.00848v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2101.03164",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Batzner_S/0/1/0/all/0/1\">Simon Batzner</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Musaelian_A/0/1/0/all/0/1\">Albert Musaelian</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Sun_L/0/1/0/all/0/1\">Lixin Sun</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Geiger_M/0/1/0/all/0/1\">Mario Geiger</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Mailoa_J/0/1/0/all/0/1\">Jonathan P. Mailoa</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Kornbluth_M/0/1/0/all/0/1\">Mordechai Kornbluth</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Molinari_N/0/1/0/all/0/1\">Nicola Molinari</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Smidt_T/0/1/0/all/0/1\">Tess E. Smidt</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Kozinsky_B/0/1/0/all/0/1\">Boris Kozinsky</a>",
          "description": "This work presents Neural Equivariant Interatomic Potentials (NequIP), a\nSE(3)-equivariant neural network approach for learning interatomic potentials\nfrom ab-initio calculations for molecular dynamics simulations. While most\ncontemporary symmetry-aware models use invariant convolutions and only act on\nscalars, NequIP employs SE(3)-equivariant convolutions for interactions of\ngeometric tensors, resulting in a more information-rich and faithful\nrepresentation of atomic environments. The method achieves state-of-the-art\naccuracy on a challenging set of diverse molecules and materials while\nexhibiting remarkable data efficiency. NequIP outperforms existing models with\nup to three orders of magnitude fewer training data, challenging the widely\nheld belief that deep neural networks require massive training sets. The high\ndata efficiency of the method allows for the construction of accurate\npotentials using high-order quantum chemical level of theory as reference and\nenables high-fidelity molecular dynamics simulations over long time scales.",
          "link": "http://arxiv.org/abs/2101.03164",
          "publishedOn": "2021-07-05T01:54:58.698Z",
          "wordCount": 614,
          "title": "SE(3)-Equivariant Graph Neural Networks for Data-Efficient and Accurate Interatomic Potentials. (arXiv:2101.03164v2 [physics.comp-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.01196",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cody_T/0/1/0/all/0/1\">Tyler Cody</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beling_P/0/1/0/all/0/1\">Peter A. Beling</a>",
          "description": "Existing frameworks for transfer learning are incomplete from a systems\ntheoretic perspective. They place emphasis on notions of domain and task, and\nneglect notions of structure and behavior. In doing so, they limit the extent\nto which formalism can be carried through into the elaboration of their\nframeworks. Herein, we use Mesarovician systems theory to define transfer\nlearning as a relation on sets and subsequently characterize the general nature\nof transfer learning as a mathematical construct. We interpret existing\nframeworks in terms of ours and go beyond existing frameworks to define notions\nof transferability, transfer roughness, and transfer distance. Importantly,\ndespite its formalism, our framework avoids the detailed mathematics of\nlearning theory or machine learning solution methods without excluding their\nconsideration. As such, we provide a formal, general systems framework for\nmodeling transfer learning that offers a rigorous foundation for system design\nand analysis.",
          "link": "http://arxiv.org/abs/2107.01196",
          "publishedOn": "2021-07-05T01:54:58.691Z",
          "wordCount": 580,
          "title": "A Systems Theory of Transfer Learning. (arXiv:2107.01196v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00877",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Amakasu_T/0/1/0/all/0/1\">Takashi Amakasu</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Chauvet_N/0/1/0/all/0/1\">Nicolas Chauvet</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Bachelier_G/0/1/0/all/0/1\">Guillaume Bachelier</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Huant_S/0/1/0/all/0/1\">Serge Huant</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Horisaki_R/0/1/0/all/0/1\">Ryoichi Horisaki</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Naruse_M/0/1/0/all/0/1\">Makoto Naruse</a>",
          "description": "In recent cross-disciplinary studies involving both optics and computing,\nsingle-photon-based decision-making has been demonstrated by utilizing the\nwave-particle duality of light to solve multi-armed bandit problems.\nFurthermore, entangled-photon-based decision-making has managed to solve a\ncompetitive multi-armed bandit problem in such a way that conflicts of\ndecisions among players are avoided while ensuring equality. However, as these\nstudies are based on the polarization of light, the number of available choices\nis limited to two, corresponding to two orthogonal polarization states. Here we\npropose a scalable principle to solve competitive decision-making situations by\nusing the orbital angular momentum as the tunable degree of freedom of photons,\nwhich theoretically allows an unlimited number of arms. Moreover, by extending\nthe Hong-Ou-Mandel effect to more than two states, we theoretically establish\nan experimental configuration able to generate entangled photon states with\norbital angular momentum and conditions that provide conflict-free selections\nat every turn. We numerically examine total rewards regarding three-armed\nbandit problems, for which the proposed strategy accomplishes almost the\ntheoretical maximum, which is greater than a conventional mixed strategy\nintending to realize Nash equilibrium. This is thanks to the entanglement\nproperty that achieves no-conflict selections, even in the exploring phase to\nfind the best arms.",
          "link": "http://arxiv.org/abs/2107.00877",
          "publishedOn": "2021-07-05T01:54:58.675Z",
          "wordCount": 651,
          "title": "Conflict-free collective stochastic decision making by orbital angular momentum entangled photons. (arXiv:2107.00877v1 [quant-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00871",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Takabatake_K/0/1/0/all/0/1\">Kazuya Takabatake</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akaho_S/0/1/0/all/0/1\">Shotaro Akaho</a>",
          "description": "Dependency networks (Heckerman et al., 2000) are potential probabilistic\ngraphical models for systems comprising a large number of variables. Like\nBayesian networks, the structure of a dependency network is represented by a\ndirected graph, and each node has a conditional probability table. Learning and\ninference are realized locally on individual nodes; therefore, computation\nremains tractable even with a large number of variables. However, the\ndependency network's learned distribution is the stationary distribution of a\nMarkov chain called pseudo-Gibbs sampling and has no closed-form expressions.\nThis technical disadvantage has impeded the development of dependency networks.\nIn this paper, we consider a certain manifold for each node. Then, we can\ninterpret pseudo-Gibbs sampling as iterative m-projections onto these\nmanifolds. This interpretation provides a theoretical bound for the location\nwhere the stationary distribution of pseudo-Gibbs sampling exists in\ndistribution space. Furthermore, this interpretation involves structure and\nparameter learning algorithms as optimization problems. In addition, we compare\ndependency and Bayesian networks experimentally. The results demonstrate that\nthe dependency network and the Bayesian network have roughly the same\nperformance in terms of the accuracy of their learned distributions. The\nresults also show that the dependency network can learn much faster than the\nBayesian network.",
          "link": "http://arxiv.org/abs/2107.00871",
          "publishedOn": "2021-07-05T01:54:58.667Z",
          "wordCount": 632,
          "title": "Reconsidering Dependency Networks from an Information Geometry Perspective. (arXiv:2107.00871v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.01154",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wei_W/0/1/0/all/0/1\">Wenqi Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Ling Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yanzhao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_G/0/1/0/all/0/1\">Gong Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyengar_A/0/1/0/all/0/1\">Arun Iyengar</a>",
          "description": "Federated learning(FL) is an emerging distributed learning paradigm with\ndefault client privacy because clients can keep sensitive data on their devices\nand only share local training parameter updates with the federated server.\nHowever, recent studies reveal that gradient leakages in FL may compromise the\nprivacy of client training data. This paper presents a gradient leakage\nresilient approach to privacy-preserving federated learning with per training\nexample-based client differential privacy, coined as Fed-CDP. It makes three\noriginal contributions. First, we identify three types of client gradient\nleakage threats in federated learning even with encrypted client-server\ncommunications. We articulate when and why the conventional server coordinated\ndifferential privacy approach, coined as Fed-SDP, is insufficient to protect\nthe privacy of the training data. Second, we introduce Fed-CDP, the per\nexample-based client differential privacy algorithm, and provide a formal\nanalysis of Fed-CDP with the $(\\epsilon, \\delta)$ differential privacy\nguarantee, and a formal comparison between Fed-CDP and Fed-SDP in terms of\nprivacy accounting. Third, we formally analyze the privacy-utility trade-off\nfor providing differential privacy guarantee by Fed-CDP and present a dynamic\ndecay noise-injection policy to further improve the accuracy and resiliency of\nFed-CDP. We evaluate and compare Fed-CDP and Fed-CDP(decay) with Fed-SDP in\nterms of differential privacy guarantee and gradient leakage resilience over\nfive benchmark datasets. The results show that the Fed-CDP approach outperforms\nconventional Fed-SDP in terms of resilience to client gradient leakages while\noffering competitive accuracy performance in federated learning.",
          "link": "http://arxiv.org/abs/2107.01154",
          "publishedOn": "2021-07-05T01:54:58.660Z",
          "wordCount": 664,
          "title": "Gradient-Leakage Resilient Federated Learning. (arXiv:2107.01154v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00896",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mao_T/0/1/0/all/0/1\">Tong Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1\">Zhongjie Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Ding-Xuan Zhou</a>",
          "description": "We consider a family of deep neural networks consisting of two groups of\nconvolutional layers, a downsampling operator, and a fully connected layer. The\nnetwork structure depends on two structural parameters which determine the\nnumbers of convolutional layers and the width of the fully connected layer. We\nestablish an approximation theory with explicit approximation rates when the\napproximated function takes a composite form $f\\circ Q$ with a feature\npolynomial $Q$ and a univariate function $f$. In particular, we prove that such\na network can outperform fully connected shallow networks in approximating\nradial functions with $Q(x) =|x|^2$, when the dimension $d$ of data from\n$\\mathbb{R}^d$ is large. This gives the first rigorous proof for the\nsuperiority of deep convolutional neural networks in approximating functions\nwith special structures. Then we carry out generalization analysis for\nempirical risk minimization with such a deep network in a regression framework\nwith the regression function of the form $f\\circ Q$. Our network structure\nwhich does not use any composite information or the functions $Q$ and $f$ can\nautomatically extract features and make use of the composite nature of the\nregression function via tuning the structural parameters. Our analysis provides\nan error bound which decreases with the network depth to a minimum and then\nincreases, verifying theoretically a trade-off phenomenon observed for network\ndepths in many practical applications.",
          "link": "http://arxiv.org/abs/2107.00896",
          "publishedOn": "2021-07-05T01:54:58.639Z",
          "wordCount": 653,
          "title": "Theory of Deep Convolutional Neural Networks III: Approximating Radial Functions. (arXiv:2107.00896v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00797",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">John Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1\">Qihan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kyrillidis_A/0/1/0/all/0/1\">Anastasios Kyrillidis</a>",
          "description": "The double descent curve is one of the most intriguing properties of deep\nneural networks. It contrasts the classical bias-variance curve with the\nbehavior of modern neural networks, occurring where the number of samples nears\nthe number of parameters. In this work, we explore the connection between the\ndouble descent phenomena and the number of samples in the deep neural network\nsetting. In particular, we propose a construction which augments the existing\ndataset by artificially increasing the number of samples. This construction\nempirically mitigates the double descent curve in this setting. We reproduce\nexisting work on deep double descent, and observe a smooth descent into the\noverparameterized region for our construction. This occurs both with respect to\nthe model size, and with respect to the number epochs.",
          "link": "http://arxiv.org/abs/2107.00797",
          "publishedOn": "2021-07-05T01:54:58.620Z",
          "wordCount": 553,
          "title": "Mitigating deep double descent by concatenating inputs. (arXiv:2107.00797v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00798",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Makarychev_K/0/1/0/all/0/1\">Konstantin Makarychev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_L/0/1/0/all/0/1\">Liren Shan</a>",
          "description": "We consider the problem of explainable $k$-medians and $k$-means introduced\nby Dasgupta, Frost, Moshkovitz, and Rashtchian~(ICML 2020). In this problem,\nour goal is to find a \\emph{threshold decision tree} that partitions data into\n$k$ clusters and minimizes the $k$-medians or $k$-means objective. The obtained\nclustering is easy to interpret because every decision node of a threshold tree\nsplits data based on a single feature into two groups. We propose a new\nalgorithm for this problem which is $\\tilde O(\\log k)$ competitive with\n$k$-medians with $\\ell_1$ norm and $\\tilde O(k)$ competitive with $k$-means.\nThis is an improvement over the previous guarantees of $O(k)$ and $O(k^2)$ by\nDasgupta et al (2020). We also provide a new algorithm which is $O(\\log^{3/2}\nk)$ competitive for $k$-medians with $\\ell_2$ norm. Our first algorithm is\nnear-optimal: Dasgupta et al (2020) showed a lower bound of $\\Omega(\\log k)$\nfor $k$-medians; in this work, we prove a lower bound of $\\tilde\\Omega(k)$ for\n$k$-means. We also provide a lower bound of $\\Omega(\\log k)$ for $k$-medians\nwith $\\ell_2$ norm.",
          "link": "http://arxiv.org/abs/2107.00798",
          "publishedOn": "2021-07-05T01:54:58.600Z",
          "wordCount": 608,
          "title": "Near-optimal Algorithms for Explainable k-Medians and k-Means. (arXiv:2107.00798v1 [cs.DS])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00656",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Benato_L/0/1/0/all/0/1\">Lisa Benato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buhmann_E/0/1/0/all/0/1\">Erik Buhmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erdmann_M/0/1/0/all/0/1\">Martin Erdmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fackeldey_P/0/1/0/all/0/1\">Peter Fackeldey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glombitza_J/0/1/0/all/0/1\">Jonas Glombitza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hartmann_N/0/1/0/all/0/1\">Nikolai Hartmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kasieczka_G/0/1/0/all/0/1\">Gregor Kasieczka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Korcari_W/0/1/0/all/0/1\">William Korcari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuhr_T/0/1/0/all/0/1\">Thomas Kuhr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steinheimer_J/0/1/0/all/0/1\">Jan Steinheimer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stocker_H/0/1/0/all/0/1\">Horst St&#xf6;cker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plehn_T/0/1/0/all/0/1\">Tilman Plehn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kai Zhou</a>",
          "description": "We introduce a collection of datasets from fundamental physics research --\nincluding particle physics, astroparticle physics, and hadron- and nuclear\nphysics -- for supervised machine learning studies. These datasets, containing\nhadronic top quarks, cosmic-ray induced air showers, phase transitions in\nhadronic matter, and generator-level histories, are made public to simplify\nfuture work on cross-disciplinary machine learning and transfer learning in\nfundamental physics. Based on these data, we present a simple yet flexible\ngraph-based neural network architecture that can easily be applied to a wide\nrange of supervised learning tasks in these domains. We show that our approach\nreaches performance close to state-of-the-art dedicated methods on all\ndatasets. To simplify adaptation for various problems, we provide\neasy-to-follow instructions on how graph-based representations of data\nstructures, relevant for fundamental physics, can be constructed and provide\ncode implementations for several of them. Implementations are also provided for\nour proposed method and all reference algorithms.",
          "link": "http://arxiv.org/abs/2107.00656",
          "publishedOn": "2021-07-05T01:54:58.587Z",
          "wordCount": 645,
          "title": "Shared Data and Algorithms for Deep Learning in Fundamental Physics. (arXiv:2107.00656v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00813",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Qiu_C/0/1/0/all/0/1\">Changxin Qiu</a>, <a href=\"http://arxiv.org/find/math/1/au:+Yan_J/0/1/0/all/0/1\">Jue Yan</a>",
          "description": "Motivated by finite volume scheme, a cell-average based neural network method\nis proposed. The method is based on the integral or weak formulation of partial\ndifferential equations. A simple feed forward network is forced to learn the\nsolution average evolution between two neighboring time steps. Offline\nsupervised training is carried out to obtain the optimal network parameter set,\nwhich uniquely identifies one finite volume like neural network method. Once\nwell trained, the network method is implemented as a finite volume scheme, thus\nis mesh dependent. Different to traditional numerical methods, our method can\nbe relieved from the explicit scheme CFL restriction and can adapt to any time\nstep size for solution evolution. For Heat equation, first order of convergence\nis observed and the errors are related to the spatial mesh size but are\nobserved independent of the mesh size in time. The cell-average based neural\nnetwork method can sharply evolve contact discontinuity with almost zero\nnumerical diffusion introduced. Shock and rarefaction waves are well captured\nfor nonlinear hyperbolic conservation laws.",
          "link": "http://arxiv.org/abs/2107.00813",
          "publishedOn": "2021-07-05T01:54:58.580Z",
          "wordCount": 608,
          "title": "Cell-average based neural network method for hyperbolic and parabolic partial differential equations. (arXiv:2107.00813v1 [math.NA])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00717",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kothawade_S/0/1/0/all/0/1\">Suraj Kothawade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beck_N/0/1/0/all/0/1\">Nathan Beck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Killamsetty_K/0/1/0/all/0/1\">Krishnateja Killamsetty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyer_R/0/1/0/all/0/1\">Rishabh Iyer</a>",
          "description": "Active learning has proven to be useful for minimizing labeling costs by\nselecting the most informative samples. However, existing active learning\nmethods do not work well in realistic scenarios such as imbalance or rare\nclasses, out-of-distribution data in the unlabeled set, and redundancy. In this\nwork, we propose SIMILAR (Submodular Information Measures based actIve\nLeARning), a unified active learning framework using recently proposed\nsubmodular information measures (SIM) as acquisition functions. We argue that\nSIMILAR not only works in standard active learning, but also easily extends to\nthe realistic settings considered above and acts as a one-stop solution for\nactive learning that is scalable to large real-world datasets. Empirically, we\nshow that SIMILAR significantly outperforms existing active learning algorithms\nby as much as ~5% - 18% in the case of rare classes and ~5% - 10% in the case\nof out-of-distribution data on several image classification tasks like\nCIFAR-10, MNIST, and ImageNet.",
          "link": "http://arxiv.org/abs/2107.00717",
          "publishedOn": "2021-07-05T01:54:58.559Z",
          "wordCount": 592,
          "title": "SIMILAR: Submodular Information Measures Based Active Learning In Realistic Scenarios. (arXiv:2107.00717v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00821",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Banna_V/0/1/0/all/0/1\">Vishnu Banna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chinnakotla_A/0/1/0/all/0/1\">Akhil Chinnakotla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1\">Zhengxin Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vegesana_A/0/1/0/all/0/1\">Ani Vegesana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vivek_N/0/1/0/all/0/1\">Naveen Vivek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krishnappa_K/0/1/0/all/0/1\">Kruthi Krishnappa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1\">Wenxin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yung-Hsiang Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thiruvathukal_G/0/1/0/all/0/1\">George K. Thiruvathukal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Davis_J/0/1/0/all/0/1\">James C. Davis</a>",
          "description": "Machine learning techniques are becoming a fundamental tool for scientific\nand engineering progress. These techniques are applied in contexts as diverse\nas astronomy and spam filtering. However, correctly applying these techniques\nrequires careful engineering. Much attention has been paid to the technical\npotential; relatively little attention has been paid to the software\nengineering process required to bring research-based machine learning\ntechniques into practical utility. Technology companies have supported the\nengineering community through machine learning frameworks such as TensorFLow\nand PyTorch, but the details of how to engineer complex machine learning models\nin these frameworks have remained hidden.\n\nTo promote best practices within the engineering community, academic\ninstitutions and Google have partnered to launch a Special Interest Group on\nMachine Learning Models (SIGMODELS) whose goal is to develop exemplary\nimplementations of prominent machine learning models in community locations\nsuch as the TensorFlow Model Garden (TFMG). The purpose of this report is to\ndefine a process for reproducing a state-of-the-art machine learning model at a\nlevel of quality suitable for inclusion in the TFMG. We define the engineering\nprocess and elaborate on each step, from paper analysis to model release. We\nreport on our experiences implementing the YOLO model family with a team of 26\nstudent researchers, share the tools we developed, and describe the lessons we\nlearned along the way.",
          "link": "http://arxiv.org/abs/2107.00821",
          "publishedOn": "2021-07-05T01:54:58.546Z",
          "wordCount": 688,
          "title": "An Experience Report on Machine Learning Reproducibility: Guidance for Practitioners and TensorFlow Model Garden Contributors. (arXiv:2107.00821v1 [cs.SE])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00685",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dou_Z/0/1/0/all/0/1\">Zehao Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhuoran Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhaoran Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_S/0/1/0/all/0/1\">Simon S.Du</a>",
          "description": "As one of the most popular methods in the field of reinforcement learning,\nQ-learning has received increasing attention. Recently, there have been more\ntheoretical works on the regret bound of algorithms that belong to the\nQ-learning class in different settings. In this paper, we analyze the\ncumulative regret when conducting Nash Q-learning algorithm on 2-player\nturn-based stochastic Markov games (2-TBSG), and propose the very first gap\ndependent logarithmic upper bounds in the episodic tabular setting. This bound\nmatches the theoretical lower bound only up to a logarithmic term. Furthermore,\nwe extend the conclusion to the discounted game setting with infinite horizon\nand propose a similar gap dependent logarithmic regret bound. Also, under the\nlinear MDP assumption, we obtain another logarithmic regret for 2-TBSG, in both\ncentralized and independent settings.",
          "link": "http://arxiv.org/abs/2107.00685",
          "publishedOn": "2021-07-05T01:54:58.513Z",
          "wordCount": 564,
          "title": "Gap-Dependent Bounds for Two-Player Markov Games. (arXiv:2107.00685v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00680",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lihong Li</a>",
          "description": "The rich body of Bandit literature not only offers a diverse toolbox of\nalgorithms, but also makes it hard for a practitioner to find the right\nsolution to solve the problem at hand. Typical textbooks on Bandits focus on\ndesigning and analyzing algorithms, and surveys on applications often present a\nlist of individual applications. While these are valuable resources, there\nexists a gap in mapping applications to appropriate Bandit algorithms. In this\npaper, we aim to reduce this gap with a structured map of Bandits to help\npractitioners navigate to find relevant and practical Bandit algorithms.\nInstead of providing a comprehensive overview, we focus on a small number of\nkey decision points related to reward, action, and features, which often affect\nhow Bandit algorithms are chosen in practice.",
          "link": "http://arxiv.org/abs/2107.00680",
          "publishedOn": "2021-07-05T01:54:58.507Z",
          "wordCount": 560,
          "title": "A Map of Bandits for E-commerce. (arXiv:2107.00680v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00860",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hayeon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hyung_E/0/1/0/all/0/1\">Eunyoung Hyung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1\">Sung Ju Hwang</a>",
          "description": "Despite the success of recent Neural Architecture Search (NAS) methods on\nvarious tasks which have shown to output networks that largely outperform\nhuman-designed networks, conventional NAS methods have mostly tackled the\noptimization of searching for the network architecture for a single task\n(dataset), which does not generalize well across multiple tasks (datasets).\nMoreover, since such task-specific methods search for a neural architecture\nfrom scratch for every given task, they incur a large computational cost, which\nis problematic when the time and monetary budget are limited. In this paper, we\npropose an efficient NAS framework that is trained once on a database\nconsisting of datasets and pretrained networks and can rapidly search for a\nneural architecture for a novel dataset. The proposed MetaD2A (Meta\nDataset-to-Architecture) model can stochastically generate graphs\n(architectures) from a given set (dataset) via a cross-modal latent space\nlearned with amortized meta-learning. Moreover, we also propose a\nmeta-performance predictor to estimate and select the best architecture without\ndirect training on target datasets. The experimental results demonstrate that\nour model meta-learned on subsets of ImageNet-1K and architectures from\nNAS-Bench 201 search space successfully generalizes to multiple unseen datasets\nincluding CIFAR-10 and CIFAR-100, with an average search time of 33 GPU\nseconds. Even under MobileNetV3 search space, MetaD2A is 5.5K times faster than\nNSGANetV2, a transferable NAS method, with comparable performance. We believe\nthat the MetaD2A proposes a new research direction for rapid NAS as well as\nways to utilize the knowledge from rich databases of datasets and architectures\naccumulated over the past years. Code is available at\nhttps://github.com/HayeonLee/MetaD2A.",
          "link": "http://arxiv.org/abs/2107.00860",
          "publishedOn": "2021-07-05T01:54:58.500Z",
          "wordCount": 705,
          "title": "Rapid Neural Architecture Search by Learning to Generate Graphs from Datasets. (arXiv:2107.00860v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00839",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Delarue_F/0/1/0/all/0/1\">Fran&#xe7;ois Delarue</a>, <a href=\"http://arxiv.org/find/math/1/au:+Vasileiadis_A/0/1/0/all/0/1\">Athanasios Vasileiadis</a>",
          "description": "The goal of this paper is to demonstrate that common noise may serve as an\nexploration noise for learning the solution of a mean field game. This concept\nis here exemplified through a toy linear-quadratic model, for which a suitable\nform of common noise has already been proven to restore existence and\nuniqueness. We here go one step further and prove that the same form of common\nnoise may force the convergence of the learning algorithm called `fictitious\nplay', and this without any further potential or monotone structure. Several\nnumerical examples are provided in order to support our theoretical analysis.",
          "link": "http://arxiv.org/abs/2107.00839",
          "publishedOn": "2021-07-05T01:54:58.491Z",
          "wordCount": 539,
          "title": "Exploration noise for learning linear-quadratic mean field games. (arXiv:2107.00839v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00719",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Kao_P/0/1/0/all/0/1\">Po-Yu Kao</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Kao_S/0/1/0/all/0/1\">Shu-Min Kao</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Huang_N/0/1/0/all/0/1\">Nan-Lan Huang</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Lin_Y/0/1/0/all/0/1\">Yen-Chu Lin</a>",
          "description": "Drug-target interaction (DTI) prediction plays a crucial role in drug\ndiscovery, and deep learning approaches have achieved state-of-the-art\nperformance in this field. We introduce an ensemble of deep learning models\n(EnsembleDLM) for robust DTI prediction. EnsembleDLM only uses the sequence\ninformation of chemical compounds and proteins, and it aggregates the\npredictions from multiple deep neural networks. This approach reduces the\nchance of overfitting, yields an unbiased prediction, and achieves\nstate-of-the-art performance in Davis and KIBA datasets. EnsembleDLM also\nreaches state-of-the-art performance in cross-domain applications and decent\ncross-domain performance (Pearson correlation coefficient and concordance index\n> 0.8) with transfer learning using approximately twice the amount of test data\nin the new domain.",
          "link": "http://arxiv.org/abs/2107.00719",
          "publishedOn": "2021-07-05T01:54:58.483Z",
          "wordCount": 560,
          "title": "Toward Robust Drug-Target Interaction Prediction via Ensemble Modeling and Transfer Learning. (arXiv:2107.00719v1 [q-bio.BM])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00940",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Maddu_S/0/1/0/all/0/1\">Suryanarayana Maddu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sturm_D/0/1/0/all/0/1\">Dominik Sturm</a>, <a href=\"http://arxiv.org/find/cs/1/au:+M%7Fuller_C/0/1/0/all/0/1\">Christian L. M&#xfc;ller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sbalzarini_I/0/1/0/all/0/1\">Ivo F. Sbalzarini</a>",
          "description": "We characterize and remedy a failure mode that may arise from multi-scale\ndynamics with scale imbalances during training of deep neural networks, such as\nPhysics Informed Neural Networks (PINNs). PINNs are popular machine-learning\ntemplates that allow for seamless integration of physical equation models with\ndata. Their training amounts to solving an optimization problem over a weighted\nsum of data-fidelity and equation-fidelity objectives. Conflicts between\nobjectives can arise from scale imbalances, heteroscedasticity in the data,\nstiffness of the physical equation, or from catastrophic interference during\nsequential training. We explain the training pathology arising from this and\npropose a simple yet effective inverse-Dirichlet weighting strategy to\nalleviate the issue. We compare with Sobolev training of neural networks,\nproviding the baseline of analytically $\\boldsymbol{\\epsilon}$-optimal\ntraining. We demonstrate the effectiveness of inverse-Dirichlet weighting in\nvarious applications, including a multi-scale model of active turbulence, where\nwe show orders of magnitude improvement in accuracy and convergence over\nconventional PINN training. For inverse modeling using sequential training, we\nfind that inverse-Dirichlet weighting protects a PINN against catastrophic\nforgetting.",
          "link": "http://arxiv.org/abs/2107.00940",
          "publishedOn": "2021-07-05T01:54:58.464Z",
          "wordCount": 623,
          "title": "Inverse-Dirichlet Weighting Enables Reliable Training of Physics Informed Neural Networks. (arXiv:2107.00940v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00941",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jagtap_R/0/1/0/all/0/1\">Raj Jagtap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Abhinav Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goel_R/0/1/0/all/0/1\">Rahul Goel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_S/0/1/0/all/0/1\">Shakshi Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_R/0/1/0/all/0/1\">Rajesh Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+George_C/0/1/0/all/0/1\">Clint P. George</a>",
          "description": "Millions of people use platforms such as YouTube, Facebook, Twitter, and\nother mass media. Due to the accessibility of these platforms, they are often\nused to establish a narrative, conduct propaganda, and disseminate\nmisinformation. This work proposes an approach that uses state-of-the-art NLP\ntechniques to extract features from video captions (subtitles). To evaluate our\napproach, we utilize a publicly accessible and labeled dataset for classifying\nvideos as misinformation or not. The motivation behind exploring video captions\nstems from our analysis of videos metadata. Attributes such as the number of\nviews, likes, dislikes, and comments are ineffective as videos are hard to\ndifferentiate using this information. Using caption dataset, the proposed\nmodels can classify videos among three classes (Misinformation, Debunking\nMisinformation, and Neutral) with 0.85 to 0.90 F1-score. To emphasize the\nrelevance of the misinformation class, we re-formulate our classification\nproblem as a two-class classification - Misinformation vs. others (Debunking\nMisinformation and Neutral). In our experiments, the proposed models can\nclassify videos with 0.92 to 0.95 F1-score and 0.78 to 0.90 AUC ROC.",
          "link": "http://arxiv.org/abs/2107.00941",
          "publishedOn": "2021-07-05T01:54:58.452Z",
          "wordCount": 610,
          "title": "Misinformation Detection on YouTube Using Video Captions. (arXiv:2107.00941v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00653",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yu Shi</a>",
          "description": "The Transformer model is widely used in natural language processing for\nsentence representation. However, the previous Transformer-based models focus\non function words that have limited meaning in most cases and could merely\nextract high-level semantic abstraction features. In this paper, two approaches\nare introduced to improve the performance of Transformers. We calculated the\nattention score by multiplying the part-of-speech weight vector with the\ncorrelation coefficient, which helps extract the words with more practical\nmeaning. The weight vector is obtained by the input text sequence based on the\nimportance of the part-of-speech. Furthermore, we fuse the features of each\nlayer to make the sentence representation results more comprehensive and\naccurate. In experiments, we demonstrate the effectiveness of our model\nTransformer-F on three standard text classification datasets. Experimental\nresults show that our proposed model significantly boosts the performance of\ntext classification as compared to the baseline model. Specifically, we obtain\na 5.28% relative improvement over the vanilla Transformer on the simple tasks.",
          "link": "http://arxiv.org/abs/2107.00653",
          "publishedOn": "2021-07-05T01:54:58.445Z",
          "wordCount": 597,
          "title": "Transformer-F: A Transformer network with effective methods for learning universal sentence representation. (arXiv:2107.00653v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00819",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Blanc_G/0/1/0/all/0/1\">Guy Blanc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lange_J/0/1/0/all/0/1\">Jane Lange</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_M/0/1/0/all/0/1\">Mingda Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_L/0/1/0/all/0/1\">Li-Yang Tan</a>",
          "description": "Greedy decision tree learning heuristics are mainstays of machine learning\npractice, but theoretical justification for their empirical success remains\nelusive. In fact, it has long been known that there are simple target functions\nfor which they fail badly (Kearns and Mansour, STOC 1996).\n\nRecent work of Brutzkus, Daniely, and Malach (COLT 2020) considered the\nsmoothed analysis model as a possible avenue towards resolving this disconnect.\nWithin the smoothed setting and for targets $f$ that are $k$-juntas, they\nshowed that these heuristics successfully learn $f$ with depth-$k$ decision\ntree hypotheses. They conjectured that the same guarantee holds more generally\nfor targets that are depth-$k$ decision trees.\n\nWe provide a counterexample to this conjecture: we construct targets that are\ndepth-$k$ decision trees and show that even in the smoothed setting, these\nheuristics build trees of depth $2^{\\Omega(k)}$ before achieving high accuracy.\nWe also show that the guarantees of Brutzkus et al. cannot extend to the\nagnostic setting: there are targets that are very close to $k$-juntas, for\nwhich these heuristics build trees of depth $2^{\\Omega(k)}$ before achieving\nhigh accuracy.",
          "link": "http://arxiv.org/abs/2107.00819",
          "publishedOn": "2021-07-05T01:54:58.439Z",
          "wordCount": 628,
          "title": "Decision tree heuristics can fail, even in the smoothed setting. (arXiv:2107.00819v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00793",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xia_K/0/1/0/all/0/1\">Kevin Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kai-Zhan Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1\">Yoshua Bengio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bareinboim_E/0/1/0/all/0/1\">Elias Bareinboim</a>",
          "description": "One of the central elements of any causal inference is an object called\nstructural causal model (SCM), which represents a collection of mechanisms and\nexogenous sources of random variation of the system under investigation (Pearl,\n2000). An important property of many kinds of neural networks is universal\napproximability: the ability to approximate any function to arbitrary\nprecision. Given this property, one may be tempted to surmise that a collection\nof neural nets is capable of learning any SCM by training on data generated by\nthat SCM. In this paper, we show this is not the case by disentangling the\nnotions of expressivity and learnability. Specifically, we show that the causal\nhierarchy theorem (Thm. 1, Bareinboim et al., 2020), which describes the limits\nof what can be learned from data, still holds for neural models. For instance,\nan arbitrarily complex and expressive neural net is unable to predict the\neffects of interventions given observational data alone. Given this result, we\nintroduce a special type of SCM called a neural causal model (NCM), and\nformalize a new type of inductive bias to encode structural constraints\nnecessary for performing causal inferences. Building on this new class of\nmodels, we focus on solving two canonical tasks found in the literature known\nas causal identification and estimation. Leveraging the neural toolbox, we\ndevelop an algorithm that is both sufficient and necessary to determine whether\na causal effect can be learned from data (i.e., causal identifiability); it\nthen estimates the effect whenever identifiability holds (causal estimation).\nSimulations corroborate the proposed approach.",
          "link": "http://arxiv.org/abs/2107.00793",
          "publishedOn": "2021-07-05T01:54:58.431Z",
          "wordCount": 710,
          "title": "The Causal Neural Connection: Expressiveness, Learnability, and Inference. (arXiv:2107.00793v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00778",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hong-You Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_W/0/1/0/all/0/1\">Wei-Lun Chao</a>",
          "description": "Federated learning is promising for its ability to collaboratively train\nmodels with multiple clients without accessing their data, but vulnerable when\nclients' data distributions diverge from each other. This divergence further\nleads to a dilemma: \"Should we prioritize the learned model's generic\nperformance (for future use at the server) or its personalized performance (for\neach client)?\" These two, seemingly competing goals have divided the community\nto focus on one or the other, yet in this paper we show that it is possible to\napproach both at the same time. Concretely, we propose a novel federated\nlearning framework that explicitly decouples a model's dual duties with two\nprediction tasks. On the one hand, we introduce a family of losses that are\nrobust to non-identical class distributions, enabling clients to train a\ngeneric predictor with a consistent objective across them. On the other hand,\nwe formulate the personalized predictor as a lightweight adaptive module that\nis learned to minimize each client's empirical risk on top of the generic\npredictor. With this two-loss, two-predictor framework which we name Federated\nRobust Decoupling Fed-RoD, the learned model can simultaneously achieve\nstate-of-the-art generic and personalized performance, essentially bridging the\ntwo tasks.",
          "link": "http://arxiv.org/abs/2107.00778",
          "publishedOn": "2021-07-05T01:54:58.415Z",
          "wordCount": 622,
          "title": "On Bridging Generic and Personalized Federated Learning. (arXiv:2107.00778v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00753",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Joshi_N/0/1/0/all/0/1\">Nitish Joshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">He He</a>",
          "description": "While pretrained language models achieve excellent performance on natural\nlanguage understanding benchmarks, they tend to rely on spurious correlations\nand generalize poorly to out-of-distribution (OOD) data. Recent work has\nexplored using counterfactually-augmented data (CAD) -- data generated by\nminimally perturbing examples to flip the ground-truth label -- to identify\nrobust features that are invariant under distribution shift. However, empirical\nresults using CAD for OOD generalization have been mixed. To explain this\ndiscrepancy, we draw insights from a linear Gaussian model and demonstrate the\npitfalls of CAD. Specifically, we show that (a) while CAD is effective at\nidentifying robust features, it may prevent the model from learning unperturbed\nrobust features, and (b) CAD may exacerbate existing spurious correlations in\nthe data. Our results show that the lack of perturbation diversity in current\nCAD datasets limits its effectiveness on OOD generalization, calling for\ninnovative crowdsourcing procedures to elicit diverse perturbation of examples.",
          "link": "http://arxiv.org/abs/2107.00753",
          "publishedOn": "2021-07-05T01:54:58.409Z",
          "wordCount": 584,
          "title": "An Investigation of the (In)effectiveness of Counterfactually Augmented Data. (arXiv:2107.00753v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00866",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yunzhuang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yuan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eberhard_A/0/1/0/all/0/1\">Andrew Eberhard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaodong Li</a>",
          "description": "This paper proposes a novel primal heuristic for Mixed Integer Programs, by\nemploying machine learning techniques. Mixed Integer Programming is a general\ntechnique for formulating combinatorial optimization problems. Inside a solver,\nprimal heuristics play a critical role in finding good feasible solutions that\nenable one to tighten the duality gap from the outset of the Branch-and-Bound\nalgorithm (B&B), greatly improving its performance by pruning the B&B tree\naggressively. In this paper, we investigate whether effective primal heuristics\ncan be automatically learned via machine learning. We propose a new method to\nrepresent an optimization problem as a graph, and train a Graph Convolutional\nNetwork on solved problem instances with known optimal solutions. This in turn\ncan predict the values of decision variables in the optimal solution for an\nunseen problem instance of a similar type. The prediction of variable solutions\nis then leveraged by a novel configuration of the B&B method, Probabilistic\nBranching with guided Depth-first Search (PB-DFS) approach, aiming to find\n(near-)optimal solutions quickly. The experimental results show that this new\nheuristic can find better primal solutions at a much earlier stage of the\nsolving process, compared to other state-of-the-art primal heuristics.",
          "link": "http://arxiv.org/abs/2107.00866",
          "publishedOn": "2021-07-05T01:54:58.402Z",
          "wordCount": 633,
          "title": "Learning Primal Heuristics for Mixed Integer Programs. (arXiv:2107.00866v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00921",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Han_T/0/1/0/all/0/1\">Tao Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Hantao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Ziang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_W/0/1/0/all/0/1\">Wei Han</a>",
          "description": "Neural network based speech recognition systems suffer from performance\ndegradation due to accented speech, especially unfamiliar accents. In this\npaper, we study the supervised contrastive learning framework for accented\nspeech recognition. To build different views (similar \"positive\" data samples)\nfor contrastive learning, three data augmentation techniques including noise\ninjection, spectrogram augmentation and TTS-same-sentence generation are\nfurther investigated. From the experiments on the Common Voice dataset, we have\nshown that contrastive learning helps to build data-augmentation invariant and\npronunciation invariant representations, which significantly outperforms\ntraditional joint training methods in both zero-shot and full-shot settings.\nExperiments show that contrastive learning can improve accuracy by 3.66%\n(zero-shot) and 3.78% (full-shot) on average, comparing to the joint training\nmethod.",
          "link": "http://arxiv.org/abs/2107.00921",
          "publishedOn": "2021-07-05T01:54:58.391Z",
          "wordCount": 564,
          "title": "Supervised Contrastive Learning for Accented Speech Recognition. (arXiv:2107.00921v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00838",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Karim_N/0/1/0/all/0/1\">Nazmul Karim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaeemzadeh_A/0/1/0/all/0/1\">Alireza Zaeemzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahnavard_N/0/1/0/all/0/1\">Nazanin Rahnavard</a>",
          "description": "A reinforcement-learning-based non-uniform compressed sensing (NCS) framework\nfor time-varying signals is introduced. The proposed scheme, referred to as\nRL-NCS, aims to boost the performance of signal recovery through an optimal and\nadaptive distribution of sensing energy among two groups of coefficients of the\nsignal, referred to as the region of interest (ROI) coefficients and non-ROI\ncoefficients. The coefficients in ROI usually have greater importance and need\nto be reconstructed with higher accuracy compared to non-ROI coefficients. In\norder to accomplish this task, the ROI is predicted at each time step using two\nspecific approaches. One of these approaches incorporates a long short-term\nmemory (LSTM) network for the prediction. The other approach employs the\nprevious ROI information for predicting the next step ROI. Using the\nexploration-exploitation technique, a Q-network learns to choose the best\napproach for designing the measurement matrix. Furthermore, a joint loss\nfunction is introduced for the efficient training of the Q-network as well as\nthe LSTM network. The result indicates a significant performance gain for our\nproposed method, even for rapidly varying signals and a reduced number of\nmeasurements.",
          "link": "http://arxiv.org/abs/2107.00838",
          "publishedOn": "2021-07-05T01:54:58.385Z",
          "wordCount": 625,
          "title": "RL-NCS: Reinforcement learning based data-driven approach for nonuniform compressed sensing. (arXiv:2107.00838v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00710",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cote_Allard_U/0/1/0/all/0/1\">Ulysse C&#xf4;t&#xe9;-Allard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jakobsen_P/0/1/0/all/0/1\">Petter Jakobsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stautland_A/0/1/0/all/0/1\">Andrea Stautland</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nordgreen_T/0/1/0/all/0/1\">Tine Nordgreen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fasmer_O/0/1/0/all/0/1\">Ole Bernt Fasmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oedegaard_K/0/1/0/all/0/1\">Ketil Joachim Oedegaard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torresen_J/0/1/0/all/0/1\">Jim Torresen</a>",
          "description": "Manic episodes of bipolar disorder can lead to uncritical behaviour and\ndelusional psychosis, often with destructive consequences for those affected\nand their surroundings. Early detection and intervention of a manic episode are\ncrucial to prevent escalation, hospital admission and premature death. However,\npeople with bipolar disorder may not recognize that they are experiencing a\nmanic episode and symptoms such as euphoria and increased productivity can also\ndeter affected individuals from seeking help. This work proposes to perform\nuser-independent, automatic mood-state detection based on actigraphy and\nelectrodermal activity acquired from a wrist-worn device during mania and after\nrecovery (euthymia). This paper proposes a new deep learning-based ensemble\nmethod leveraging long (20h) and short (5 minutes) time-intervals to\ndiscriminate between the mood-states. When tested on 47 bipolar patients, the\nproposed classification scheme achieves an average accuracy of 91.59% in\neuthymic/manic mood-state recognition.",
          "link": "http://arxiv.org/abs/2107.00710",
          "publishedOn": "2021-07-05T01:54:58.367Z",
          "wordCount": 609,
          "title": "Long-Short Ensemble Network for Bipolar Manic-Euthymic State Recognition Based on Wrist-worn Sensors. (arXiv:2107.00710v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00801",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Kumagai_A/0/1/0/all/0/1\">Atsutoshi Kumagai</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Iwata_T/0/1/0/all/0/1\">Tomoharu Iwata</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Fujiwara_Y/0/1/0/all/0/1\">Yasuhiro Fujiwara</a>",
          "description": "The ratio of two probability densities, called a density-ratio, is a vital\nquantity in machine learning. In particular, a relative density-ratio, which is\na bounded extension of the density-ratio, has received much attention due to\nits stability and has been used in various applications such as outlier\ndetection and dataset comparison. Existing methods for (relative) density-ratio\nestimation (DRE) require many instances from both densities. However,\nsufficient instances are often unavailable in practice. In this paper, we\npropose a meta-learning method for relative DRE, which estimates the relative\ndensity-ratio from a few instances by using knowledge in related datasets.\nSpecifically, given two datasets that consist of a few instances, our model\nextracts the datasets' information by using neural networks and uses it to\nobtain instance embeddings appropriate for the relative DRE. We model the\nrelative density-ratio by a linear model on the embedded space, whose global\noptimum solution can be obtained as a closed-form solution. The closed-form\nsolution enables fast and effective adaptation to a few instances, and its\ndifferentiability enables us to train our model such that the expected test\nerror for relative DRE can be explicitly minimized after adapting to a few\ninstances. We empirically demonstrate the effectiveness of the proposed method\nby using three problems: relative DRE, dataset comparison, and outlier\ndetection.",
          "link": "http://arxiv.org/abs/2107.00801",
          "publishedOn": "2021-07-05T01:54:58.361Z",
          "wordCount": 643,
          "title": "Meta-Learning for Relative Density-Ratio Estimation. (arXiv:2107.00801v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00745",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Masrani_V/0/1/0/all/0/1\">Vaden Masrani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brekelmans_R/0/1/0/all/0/1\">Rob Brekelmans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1\">Thang Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nielsen_F/0/1/0/all/0/1\">Frank Nielsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galstyan_A/0/1/0/all/0/1\">Aram Galstyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steeg_G/0/1/0/all/0/1\">Greg Ver Steeg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wood_F/0/1/0/all/0/1\">Frank Wood</a>",
          "description": "Many common machine learning methods involve the geometric annealing path, a\nsequence of intermediate densities between two distributions of interest\nconstructed using the geometric average. While alternatives such as the\nmoment-averaging path have demonstrated performance gains in some settings,\ntheir practical applicability remains limited by exponential family endpoint\nassumptions and a lack of closed form energy function. In this work, we\nintroduce $q$-paths, a family of paths which is derived from a generalized\nnotion of the mean, includes the geometric and arithmetic mixtures as special\ncases, and admits a simple closed form involving the deformed logarithm\nfunction from nonextensive thermodynamics. Following previous analysis of the\ngeometric path, we interpret our $q$-paths as corresponding to a\n$q$-exponential family of distributions, and provide a variational\nrepresentation of intermediate densities as minimizing a mixture of\n$\\alpha$-divergences to the endpoints. We show that small deviations away from\nthe geometric path yield empirical gains for Bayesian inference using\nSequential Monte Carlo and generative model evaluation using Annealed\nImportance Sampling.",
          "link": "http://arxiv.org/abs/2107.00745",
          "publishedOn": "2021-07-05T01:54:58.355Z",
          "wordCount": 619,
          "title": "q-Paths: Generalizing the Geometric Annealing Path using Power Means. (arXiv:2107.00745v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00816",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kumagai_A/0/1/0/all/0/1\">Atsutoshi Kumagai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iwata_T/0/1/0/all/0/1\">Tomoharu Iwata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fujiwara_Y/0/1/0/all/0/1\">Yasuhiro Fujiwara</a>",
          "description": "We propose a few-shot learning method for unsupervised feature selection,\nwhich is a task to select a subset of relevant features in unlabeled data.\nExisting methods usually require many instances for feature selection. However,\nsufficient instances are often unavailable in practice. The proposed method can\nselect a subset of relevant features in a target task given a few unlabeled\ntarget instances by training with unlabeled instances in multiple source tasks.\nOur model consists of a feature selector and decoder. The feature selector\noutputs a subset of relevant features taking a few unlabeled instances as input\nsuch that the decoder can reconstruct the original features of unseen instances\nfrom the selected ones. The feature selector uses the Concrete random variables\nto select features via gradient descent. To encode task-specific properties\nfrom a few unlabeled instances to the model, the Concrete random variables and\ndecoder are modeled using permutation-invariant neural networks that take a few\nunlabeled instances as input. Our model is trained by minimizing the expected\ntest reconstruction error given a few unlabeled instances that is calculated\nwith datasets in source tasks. We experimentally demonstrate that the proposed\nmethod outperforms existing feature selection methods.",
          "link": "http://arxiv.org/abs/2107.00816",
          "publishedOn": "2021-07-05T01:54:58.349Z",
          "wordCount": 625,
          "title": "Few-shot Learning for Unsupervised Feature Selection. (arXiv:2107.00816v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00693",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Arefeen_A/0/1/0/all/0/1\">Asiful Arefeen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Akbari_A/0/1/0/all/0/1\">Ali Akbari</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Mirzadeh_S/0/1/0/all/0/1\">Seyed Iman Mirzadeh</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jafari_R/0/1/0/all/0/1\">Roozbeh Jafari</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shirazi_B/0/1/0/all/0/1\">Behrooz A. Shirazi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ghasemzadeh_H/0/1/0/all/0/1\">Hassan Ghasemzadeh</a>",
          "description": "Inter-beat interval (IBI) measurement enables estimation of heart-rate\nvariability (HRV) which, in turns, can provide early indication of potential\ncardiovascular diseases. However, extracting IBIs from noisy signals is\nchallenging since the morphology of the signal is distorted in the presence of\nthe noise. Electrocardiogram (ECG) of a person in heavy motion is highly\ncorrupted with noise, known as motion-artifact, and IBI extracted from it is\ninaccurate. As a part of remote health monitoring and wearable system\ndevelopment, denoising ECG signals and estimating IBIs correctly from them have\nbecome an emerging topic among signal-processing researchers. Apart from\nconventional methods, deep-learning techniques have been successfully used in\nsignal denoising recently, and diagnosis process has become easier, leading to\naccuracy levels that were previously unachievable. We propose a deep-learning\napproach leveraging tiramisu autoencoder model to suppress motion-artifact\nnoise and make the R-peaks of the ECG signal prominent even in the presence of\nhigh-intensity motion. After denoising, IBIs are estimated more accurately\nexpediting diagnosis tasks. Results illustrate that our method enables IBI\nestimation from noisy ECG signals with SNR up to -30dB with average root mean\nsquare error (RMSE) of 13 milliseconds for estimated IBIs. At this noise level,\nour error percentage remains below 8% and outperforms other state of the art\ntechniques.",
          "link": "http://arxiv.org/abs/2107.00693",
          "publishedOn": "2021-07-05T01:54:58.342Z",
          "wordCount": 665,
          "title": "Inter-Beat Interval Estimation with Tiramisu Model: A Novel Approach with Reduced Error. (arXiv:2107.00693v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00833",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Curmei_M/0/1/0/all/0/1\">Mihaela Curmei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dean_S/0/1/0/all/0/1\">Sarah Dean</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Recht_B/0/1/0/all/0/1\">Benjamin Recht</a>",
          "description": "In this work, we consider how preference models in interactive recommendation\nsystems determine the availability of content and users' opportunities for\ndiscovery. We propose an evaluation procedure based on stochastic reachability\nto quantify the maximum probability of recommending a target piece of content\nto an user for a set of allowable strategic modifications. This framework\nallows us to compute an upper bound on the likelihood of recommendation with\nminimal assumptions about user behavior. Stochastic reachability can be used to\ndetect biases in the availability of content and diagnose limitations in the\nopportunities for discovery granted to users. We show that this metric can be\ncomputed efficiently as a convex program for a variety of practical settings,\nand further argue that reachability is not inherently at odds with accuracy. We\ndemonstrate evaluations of recommendation algorithms trained on large datasets\nof explicit and implicit ratings. Our results illustrate how preference models,\nselection rules, and user interventions impact reachability and how these\neffects can be distributed unevenly.",
          "link": "http://arxiv.org/abs/2107.00833",
          "publishedOn": "2021-07-05T01:54:58.325Z",
          "wordCount": 610,
          "title": "Quantifying Availability and Discovery in Recommender Systems via Stochastic Reachability. (arXiv:2107.00833v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00722",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mohtasib_A/0/1/0/all/0/1\">Abdalkarim Mohtasib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+E%2E_A/0/1/0/all/0/1\">Amir Ghalamzan E.</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bellotto_N/0/1/0/all/0/1\">Nicola Bellotto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cuayahuitl_H/0/1/0/all/0/1\">Heriberto Cuay&#xe1;huitl</a>",
          "description": "Robots learning a new manipulation task from a small amount of demonstrations\nare increasingly demanded in different workspaces. A classifier model assessing\nthe quality of actions can predict the successful completion of a task, which\ncan be used by intelligent agents for action-selection. This paper presents a\nnovel classifier that learns to classify task completion only from a few\ndemonstrations. We carry out a comprehensive comparison of different neural\nclassifiers, e.g. fully connected-based, fully convolutional-based,\nsequence2sequence-based, and domain adaptation-based classification. We also\npresent a new dataset including five robot manipulation tasks, which is\npublicly available. We compared the performances of our novel classifier and\nthe existing models using our dataset and the MIME dataset. The results suggest\ndomain adaptation and timing-based features improve success prediction. Our\nnovel model, i.e. fully convolutional neural network with domain adaptation and\ntiming features, achieves an average classification accuracy of 97.3\\% and\n95.5\\% across tasks in both datasets whereas state-of-the-art classifiers\nwithout domain adaptation and timing-features only achieve 82.4\\% and 90.3\\%,\nrespectively.",
          "link": "http://arxiv.org/abs/2107.00722",
          "publishedOn": "2021-07-05T01:54:58.319Z",
          "wordCount": 613,
          "title": "Neural Task Success Classifiers for Robotic Manipulation from Few Real Demonstrations. (arXiv:2107.00722v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00703",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kanervisto_A/0/1/0/all/0/1\">Anssi Kanervisto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scheller_C/0/1/0/all/0/1\">Christian Scheller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schraner_Y/0/1/0/all/0/1\">Yanick Schraner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hautamaki_V/0/1/0/all/0/1\">Ville Hautam&#xe4;ki</a>",
          "description": "Reinforcement learning (RL) research focuses on general solutions that can be\napplied across different domains. This results in methods that RL practitioners\ncan use in almost any domain. However, recent studies often lack the\nengineering steps (\"tricks\") which may be needed to effectively use RL, such as\nreward shaping, curriculum learning, and splitting a large task into smaller\nchunks. Such tricks are common, if not necessary, to achieve state-of-the-art\nresults and win RL competitions. To ease the engineering efforts, we distill\ndescriptions of tricks from state-of-the-art results and study how well these\ntricks can improve a standard deep Q-learning agent. The long-term goal of this\nwork is to enable combining proven RL methods with domain-specific tricks by\nproviding a unified software framework and accompanying insights in multiple\ndomains.",
          "link": "http://arxiv.org/abs/2107.00703",
          "publishedOn": "2021-07-05T01:54:58.308Z",
          "wordCount": 577,
          "title": "Distilling Reinforcement Learning Tricks for Video Games. (arXiv:2107.00703v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00761",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Costa_E/0/1/0/all/0/1\">Elia Costa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silvestri_F/0/1/0/all/0/1\">Francesco Silvestri</a>",
          "description": "A free-floating bike-sharing system (FFBSS) is a dockless rental system where\nan individual can borrow a bike and returns it everywhere, within the service\narea. To improve the rental service, available bikes should be distributed over\nthe entire service area: a customer leaving from any position is then more\nlikely to find a near bike and then to use the service. Moreover, spreading\nbikes among the entire service area increases urban spatial equity since the\nbenefits of FFBSS are not a prerogative of just a few zones. For guaranteeing\nsuch distribution, the FFBSS operator can use vans to manually relocate bikes,\nbut it incurs high economic and environmental costs. We propose a novel\napproach that exploits the existing bike flows generated by customers to\ndistribute bikes. More specifically, by envisioning the problem as an Influence\nMaximization problem, we show that it is possible to position batches of bikes\non a small number of zones, and then the daily use of FFBSS will efficiently\nspread these bikes on a large area. We show that detecting these areas is\nNP-complete, but there exists a simple and efficient $1-1/e$ approximation\nalgorithm; our approach is then evaluated on a dataset of rides from the\nfree-floating bike-sharing system of the city of Padova.",
          "link": "http://arxiv.org/abs/2107.00761",
          "publishedOn": "2021-07-05T01:54:58.297Z",
          "wordCount": 654,
          "title": "On the Bike Spreading Problem. (arXiv:2107.00761v1 [cs.DS])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00774",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Esfandiari_H/0/1/0/all/0/1\">Hossein Esfandiari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mirrokni_V/0/1/0/all/0/1\">Vahab Mirrokni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narayanan_S/0/1/0/all/0/1\">Shyam Narayanan</a>",
          "description": "Recently, due to an increasing interest for transparency in artificial\nintelligence, several methods of explainable machine learning have been\ndeveloped with the simultaneous goal of accuracy and interpretability by\nhumans. In this paper, we study a recent framework of explainable clustering\nfirst suggested by Dasgupta et al.~\\cite{dasgupta2020explainable}.\nSpecifically, we focus on the $k$-means and $k$-medians problems and provide\nnearly tight upper and lower bounds.\n\nFirst, we provide an $O(\\log k \\log \\log k)$-approximation algorithm for\nexplainable $k$-medians, improving on the best known algorithm of\n$O(k)$~\\cite{dasgupta2020explainable} and nearly matching the known\n$\\Omega(\\log k)$ lower bound~\\cite{dasgupta2020explainable}. In addition, in\nlow-dimensional spaces $d \\ll \\log k$, we show that our algorithm also provides\nan $O(d \\log^2 d)$-approximate solution for explainable $k$-medians. This\nimproves over the best known bound of $O(d \\log k)$ for low\ndimensions~\\cite{laber2021explainable}, and is a constant for constant\ndimensional spaces. To complement this, we show a nearly matching $\\Omega(d)$\nlower bound. Next, we study the $k$-means problem in this context and provide\nan $O(k \\log k)$-approximation algorithm for explainable $k$-means, improving\nover the $O(k^2)$ bound of Dasgupta et al. and the $O(d k \\log k)$ bound of\n\\cite{laber2021explainable}. To complement this we provide an almost tight\n$\\Omega(k)$ lower bound, improving over the $\\Omega(\\log k)$ lower bound of\nDasgupta et al. All our algorithms run in near linear time in the number of\npoints and the dimension.",
          "link": "http://arxiv.org/abs/2107.00774",
          "publishedOn": "2021-07-05T01:54:58.290Z",
          "wordCount": 662,
          "title": "Almost Tight Approximation Algorithms for Explainable Clustering. (arXiv:2107.00774v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00931",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Altuner_A/0/1/0/all/0/1\">Anil Berk Altuner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kilimci_Z/0/1/0/all/0/1\">Zeynep Hilal Kilimci</a>",
          "description": "Stock market prediction has been an important topic for investors,\nresearchers, and analysts. Because it is affected by too many factors, stock\nmarket prediction is a difficult task to handle. In this study, we propose a\nnovel method that is based on deep reinforcement learning methodologies for the\ndirection prediction of stocks using sentiments of community and knowledge\ngraph. For this purpose, we firstly construct a social knowledge graph of users\nby analyzing relations between connections. After that, time series analysis of\nrelated stock and sentiment analysis is blended with deep reinforcement\nmethodology. Turkish version of Bidirectional Encoder Representations from\nTransformers (BerTurk) is employed to analyze the sentiments of the users while\ndeep Q-learning methodology is used for the deep reinforcement learning side of\nthe proposed model to construct the deep Q network. In order to demonstrate the\neffectiveness of the proposed model, Garanti Bank (GARAN), Akbank (AKBNK),\nT\\\"urkiye \\.I\\c{s} Bankas{\\i} (ISCTR) stocks in Istanbul Stock Exchange are\nused as a case study. Experiment results show that the proposed novel model\nachieves remarkable results for stock market prediction task.",
          "link": "http://arxiv.org/abs/2107.00931",
          "publishedOn": "2021-07-05T01:54:58.273Z",
          "wordCount": 642,
          "title": "A Novel Deep Reinforcement Learning Based Stock Direction Prediction using Knowledge Graph and Community Aware Sentiments. (arXiv:2107.00931v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00730",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_A/0/1/0/all/0/1\">Anubhab Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Honore_A/0/1/0/all/0/1\">Antoine Honor&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Dong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henter_G/0/1/0/all/0/1\">Gustav Eje Henter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chatterjee_S/0/1/0/all/0/1\">Saikat Chatterjee</a>",
          "description": "In pursuit of explainability, we develop generative models for sequential\ndata. The proposed models provide state-of-the-art classification results and\nrobust performance for speech phone classification. We combine modern neural\nnetworks (normalizing flows) and traditional generative models (hidden Markov\nmodels - HMMs). Normalizing flow-based mixture models (NMMs) are used to model\nthe conditional probability distribution given the hidden state in the HMMs.\nModel parameters are learned through judicious combinations of time-tested\nBayesian learning methods and contemporary neural network learning methods. We\nmainly combine expectation-maximization (EM) and mini-batch gradient descent.\nThe proposed generative models can compute likelihood of a data and hence\ndirectly suitable for maximum-likelihood (ML) classification approach. Due to\nstructural flexibility of HMMs, we can use different normalizing flow models.\nThis leads to different types of HMMs providing diversity in data modeling\ncapacity. The diversity provides an opportunity for easy decision fusion from\ndifferent models. For a standard speech phone classification setup involving 39\nphones (classes) and the TIMIT dataset, we show that the use of standard\nfeatures called mel-frequency-cepstral-coeffcients (MFCCs), the proposed\ngenerative models, and the decision fusion together can achieve $86.6\\%$\naccuracy by generative training only. This result is close to state-of-the-art\nresults, for examples, $86.2\\%$ accuracy of PyTorch-Kaldi toolkit [1], and\n$85.1\\%$ accuracy using light gated recurrent units [2]. We do not use any\ndiscriminative learning approach and related sophisticated features in this\narticle.",
          "link": "http://arxiv.org/abs/2107.00730",
          "publishedOn": "2021-07-05T01:54:58.262Z",
          "wordCount": 690,
          "title": "Normalizing Flow based Hidden Markov Models for Classification of Speech Phones with Explainability. (arXiv:2107.00730v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00734",
          "author": "<a href=\"http://arxiv.org/find/hep-lat/1/au:+Hackett_D/0/1/0/all/0/1\">Daniel C. Hackett</a>, <a href=\"http://arxiv.org/find/hep-lat/1/au:+Hsieh_C/0/1/0/all/0/1\">Chung-Chun Hsieh</a>, <a href=\"http://arxiv.org/find/hep-lat/1/au:+Albergo_M/0/1/0/all/0/1\">Michael S. Albergo</a>, <a href=\"http://arxiv.org/find/hep-lat/1/au:+Boyda_D/0/1/0/all/0/1\">Denis Boyda</a>, <a href=\"http://arxiv.org/find/hep-lat/1/au:+Chen_J/0/1/0/all/0/1\">Jiunn-Wei Chen</a>, <a href=\"http://arxiv.org/find/hep-lat/1/au:+Chen_K/0/1/0/all/0/1\">Kai-Feng Chen</a>, <a href=\"http://arxiv.org/find/hep-lat/1/au:+Cranmer_K/0/1/0/all/0/1\">Kyle Cranmer</a>, <a href=\"http://arxiv.org/find/hep-lat/1/au:+Kanwar_G/0/1/0/all/0/1\">Gurtej Kanwar</a>, <a href=\"http://arxiv.org/find/hep-lat/1/au:+Shanahan_P/0/1/0/all/0/1\">Phiala E. Shanahan</a>",
          "description": "Recent results have demonstrated that samplers constructed with flow-based\ngenerative models are a promising new approach for configuration generation in\nlattice field theory. In this paper, we present a set of methods to construct\nflow models for targets with multiple separated modes (i.e. theories with\nmultiple vacua). We demonstrate the application of these methods to modeling\ntwo-dimensional real scalar field theory in its symmetry-broken phase. In this\ncontext we investigate the performance of different flow-based sampling\nalgorithms, including a composite sampling algorithm where flow-based proposals\nare occasionally augmented by applying updates using traditional algorithms\nlike HMC.",
          "link": "http://arxiv.org/abs/2107.00734",
          "publishedOn": "2021-07-05T01:54:58.256Z",
          "wordCount": 563,
          "title": "Flow-based sampling for multimodal distributions in lattice field theory. (arXiv:2107.00734v1 [hep-lat])"
        },
        {
          "id": "http://arxiv.org/abs/2106.06654",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Evtimov_I/0/1/0/all/0/1\">Ivan Evtimov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Covert_I/0/1/0/all/0/1\">Ian Covert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kusupati_A/0/1/0/all/0/1\">Aditya Kusupati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kohno_T/0/1/0/all/0/1\">Tadayoshi Kohno</a>",
          "description": "When data is publicly released for human consumption, it is unclear how to\nprevent its unauthorized usage for machine learning purposes. Successful model\ntraining may be preventable with carefully designed dataset modifications, and\nwe present a proof-of-concept approach for the image classification setting. We\npropose methods based on the notion of adversarial shortcuts, which encourage\nmodels to rely on non-robust signals rather than semantic features, and our\nexperiments demonstrate that these measures successfully prevent deep learning\nmodels from achieving high accuracy on real, unmodified data examples.",
          "link": "http://arxiv.org/abs/2106.06654",
          "publishedOn": "2021-07-02T01:58:03.349Z",
          "wordCount": 544,
          "title": "Disrupting Model Training with Adversarial Shortcuts. (arXiv:2106.06654v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12900",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Shuyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1\">Xuelong Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_B/0/1/0/all/0/1\">Bin Xiao</a>",
          "description": "Meta-learning model can quickly adapt to new tasks using few-shot labeled\ndata. However, despite achieving good generalization on few-shot classification\ntasks, it is still challenging to improve the adversarial robustness of the\nmeta-learning model in few-shot learning. Although adversarial training (AT)\nmethods such as Adversarial Query (AQ) can improve the adversarially robust\nperformance of meta-learning models, AT is still computationally expensive\ntraining. On the other hand, meta-learning models trained with AT will drop\nsignificant accuracy on the original clean images. This paper proposed a\nmeta-learning method on the adversarially robust neural network called\nLong-term Cross Adversarial Training (LCAT). LCAT will update meta-learning\nmodel parameters cross along the natural and adversarial sample distribution\ndirection with long-term to improve both adversarial and clean few-shot\nclassification accuracy. Due to cross-adversarial training, LCAT only needs\nhalf of the adversarial training epoch than AQ, resulting in a low adversarial\ntraining computation. Experiment results show that LCAT achieves superior\nperformance both on the clean and adversarial few-shot classification accuracy\nthan SOTA adversarial training methods for meta-learning models.",
          "link": "http://arxiv.org/abs/2106.12900",
          "publishedOn": "2021-07-02T01:58:03.343Z",
          "wordCount": 668,
          "title": "Long-term Cross Adversarial Training: A Robust Meta-learning Method for Few-shot Classification Tasks. (arXiv:2106.12900v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00157",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhiming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xiaofei Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haoliang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhengzi Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>",
          "description": "Deep learning-based techniques have been widely applied to the program\nanalysis tasks, in fields such as type inference, fault localization, and code\nsummarization. Hitherto deep learning-based software engineering systems rely\nthoroughly on supervised learning approaches, which require laborious manual\neffort to collect and label a prohibitively large amount of data. However, most\nTuring-complete imperative languages share similar control- and data-flow\nstructures, which make it possible to transfer knowledge learned from one\nlanguage to another. In this paper, we propose cross-lingual adaptation of\nprogram analysis, which allows us to leverage prior knowledge learned from the\nlabeled dataset of one language and transfer it to the others. Specifically, we\nimplemented a cross-lingual adaptation framework, PLATO, to transfer a deep\nlearning-based type inference procedure across weakly typed languages, e.g.,\nPython to JavaScript and vice versa. PLATO incorporates a novel joint graph\nkernelized attention based on abstract syntax tree and control flow graph, and\napplies anchor word augmentation across different languages. Besides, by\nleveraging data from strongly typed languages, PLATO improves the perplexity of\nthe backbone cross-programming-language model and the performance of downstream\ncross-lingual transfer for type inference. Experimental results illustrate that\nour framework significantly improves the transferability over the baseline\nmethod by a large margin.",
          "link": "http://arxiv.org/abs/2107.00157",
          "publishedOn": "2021-07-02T01:58:03.336Z",
          "wordCount": 634,
          "title": "Cross-Lingual Adaptation for Type Inference. (arXiv:2107.00157v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2106.12570",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Joy_T/0/1/0/all/0/1\">Tom Joy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1\">Yuge Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1\">Philip H.S. Torr</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rainforth_T/0/1/0/all/0/1\">Tom Rainforth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmon_S/0/1/0/all/0/1\">Sebastian M. Schmon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Siddharth_N/0/1/0/all/0/1\">N. Siddharth</a>",
          "description": "Multimodal VAEs seek to model the joint distribution over heterogeneous data\n(e.g.\\ vision, language), whilst also capturing a shared representation across\nsuch modalities. Prior work has typically combined information from the\nmodalities by reconciling idiosyncratic representations directly in the\nrecognition model through explicit products, mixtures, or other such\nfactorisations. Here we introduce a novel alternative, the MEME, that avoids\nsuch explicit combinations by repurposing semi-supervised VAEs to combine\ninformation between modalities implicitly through mutual supervision. This\nformulation naturally allows learning from partially-observed data where some\nmodalities can be entirely missing -- something that most existing approaches\neither cannot handle, or do so to a limited extent. We demonstrate that MEME\noutperforms baselines on standard metrics across both partial and complete\nobservation schemes on the MNIST-SVHN (image-image) and CUB (image-text)\ndatasets. We also contrast the quality of the representations learnt by mutual\nsupervision against standard approaches and observe interesting trends in its\nability to capture relatedness between data.",
          "link": "http://arxiv.org/abs/2106.12570",
          "publishedOn": "2021-07-02T01:58:03.328Z",
          "wordCount": 613,
          "title": "Learning Multimodal VAEs through Mutual Supervision. (arXiv:2106.12570v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13061",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jiaqing Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ying_R/0/1/0/all/0/1\">Rex Ying</a>",
          "description": "Structural features are important features in graph datasets. However,\nalthough there are some correlation analysis of features based on covariance,\nthere is no relevant research on exploring structural feature correlation on\ngraphs with graph neural network based models. In this paper, we introduce\ngraph feature to feature (Fea2Fea) prediction pipelines in a low dimensional\nspace to explore some preliminary results on structural feature correlation,\nwhich is based on graph neural network. The results show that there exists high\ncorrelation between some of the structural features. A redundant feature\ncombination with initial node features, which is filtered by graph neural\nnetwork has improved its classification accuracy in some graph datasets. We\ncompare the difference between concatenation methods on connecting embeddings\nbetween features and show that the simplest is the best. We generalize on the\nsynthetic geometric graphs and certify the results on prediction difficulty\nbetween two structural features.",
          "link": "http://arxiv.org/abs/2106.13061",
          "publishedOn": "2021-07-02T01:58:03.322Z",
          "wordCount": 617,
          "title": "Fea2Fea: Exploring Structural Feature Correlations via Graph Neural Networks. (arXiv:2106.13061v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11562",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cha_S/0/1/0/all/0/1\">Sungmin Cha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1\">Beomyoung Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoo_Y/0/1/0/all/0/1\">Youngjoon Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moon_T/0/1/0/all/0/1\">Taesup Moon</a>",
          "description": "We consider a class-incremental semantic segmentation (CISS) problem. While\nsome recently proposed algorithms utilized variants of knowledge distillation\n(KD) technique to tackle the problem, they only partially addressed the key\nadditional challenges in CISS that causes the catastrophic forgetting; i.e.,\nthe semantic drift of the background class and multi-label prediction issue. To\nbetter address these challenges, we propose a new method, dubbed as SSUL-M\n(Semantic Segmentation with Unknown Label with Memory), by carefully combining\nseveral techniques tailored for semantic segmentation. More specifically, we\nmake three main contributions; (1) modeling unknown class within the background\nclass to help learning future classes (help plasticity), (2) freezing backbone\nnetwork and past classifiers with binary cross-entropy loss and pseudo-labeling\nto overcome catastrophic forgetting (help stability), and (3) utilizing tiny\nexemplar memory for the first time in CISS to improve both plasticity and\nstability. As a result, we show our method achieves significantly better\nperformance than the recent state-of-the-art baselines on the standard\nbenchmark datasets. Furthermore, we justify our contributions with thorough and\nextensive ablation analyses and discuss different natures of the CISS problem\ncompared to the standard class-incremental learning for classification.",
          "link": "http://arxiv.org/abs/2106.11562",
          "publishedOn": "2021-07-02T01:58:03.314Z",
          "wordCount": 648,
          "title": "SSUL: Semantic Segmentation with Unknown Label for Exemplar-based Class-Incremental Learning. (arXiv:2106.11562v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.06605",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Halvani_O/0/1/0/all/0/1\">Oren Halvani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Graner_L/0/1/0/all/0/1\">Lukas Graner</a>",
          "description": "Authorship verification (AV) is a fundamental research task in digital text\nforensics, which addresses the problem of whether two texts were written by the\nsame person. In recent years, a variety of AV methods have been proposed that\nfocus on this problem and can be divided into two categories: The first\ncategory refers to such methods that are based on explicitly defined features,\nwhere one has full control over which features are considered and what they\nactually represent. The second category, on the other hand, relates to such AV\nmethods that are based on implicitly defined features, where no control\nmechanism is involved, so that any character sequence in a text can serve as a\npotential feature. However, AV methods belonging to the second category bear\nthe risk that the topic of the texts may bias their classification predictions,\nwhich in turn may lead to misleading conclusions regarding their results. To\ntackle this problem, we propose a preprocessing technique called POSNoise,\nwhich effectively masks topic-related content in a given text. In this way, AV\nmethods are forced to focus on such text units that are more related to the\nwriting style. Our empirical evaluation based on six AV methods (falling into\nthe second category) and seven corpora shows that POSNoise leads to better\nresults compared to a well-known topic masking approach in 34 out of 42 cases,\nwith an increase in accuracy of up to 10%.",
          "link": "http://arxiv.org/abs/2005.06605",
          "publishedOn": "2021-07-02T01:58:02.899Z",
          "wordCount": 724,
          "title": "POSNoise: An Effective Countermeasure Against Topic Biases in Authorship Analysis. (arXiv:2005.06605v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.01209",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hudson_D/0/1/0/all/0/1\">Drew A. Hudson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zitnick_C/0/1/0/all/0/1\">C. Lawrence Zitnick</a>",
          "description": "We introduce the GANformer, a novel and efficient type of transformer, and\nexplore it for the task of visual generative modeling. The network employs a\nbipartite structure that enables long-range interactions across the image,\nwhile maintaining computation of linear efficiency, that can readily scale to\nhigh-resolution synthesis. It iteratively propagates information from a set of\nlatent variables to the evolving visual features and vice versa, to support the\nrefinement of each in light of the other and encourage the emergence of\ncompositional representations of objects and scenes. In contrast to the classic\ntransformer architecture, it utilizes multiplicative integration that allows\nflexible region-based modulation, and can thus be seen as a generalization of\nthe successful StyleGAN network. We demonstrate the model's strength and\nrobustness through a careful evaluation over a range of datasets, from\nsimulated multi-object environments to rich real-world indoor and outdoor\nscenes, showing it achieves state-of-the-art results in terms of image quality\nand diversity, while enjoying fast learning and better data-efficiency. Further\nqualitative and quantitative experiments offer us an insight into the model's\ninner workings, revealing improved interpretability and stronger\ndisentanglement, and illustrating the benefits and efficacy of our approach. An\nimplementation of the model is available at\nhttps://github.com/dorarad/gansformer.",
          "link": "http://arxiv.org/abs/2103.01209",
          "publishedOn": "2021-07-02T01:58:02.883Z",
          "wordCount": 686,
          "title": "Generative Adversarial Transformers. (arXiv:2103.01209v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.07388",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zaiem_S/0/1/0/all/0/1\">Salah Zaiem</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Parcollet_T/0/1/0/all/0/1\">Titouan Parcollet</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Essid_S/0/1/0/all/0/1\">Slim Essid</a>",
          "description": "Through solving pretext tasks, self-supervised learning (SSL) leverages\nunlabeled data to extract useful latent representations replacing traditional\ninput features in the downstream task. A common pretext task consists in\npretraining a SSL model on pseudo-labels derived from the original signal. This\ntechnique is particularly relevant for speech data where various meaningful\nsignal processing features may serve as pseudo-labels. However, the process of\nselecting pseudo-labels, for speech or other types of data, remains mostly\nunexplored and currently relies on observing the results on the final\ndownstream task. Nevertheless, this methodology is not sustainable at scale due\nto substantial computational (hence carbon) costs. Thus, this paper introduces\na practical and theoretical framework to select relevant pseudo-labels with\nrespect to a given downstream task. More precisely, we propose a functional\nestimator of the pseudo-label utility grounded in the conditional independence\ntheory, which does not require any training. The experiments conducted on\nspeaker recognition and automatic speech recognition validate our estimator,\nshowing a significant correlation between the performance observed on the\ndownstream task and the utility estimates obtained with our approach,\nfacilitating the prospection of relevant pseudo-labels for self-supervised\nspeech representation learning.",
          "link": "http://arxiv.org/abs/2104.07388",
          "publishedOn": "2021-07-02T01:58:02.854Z",
          "wordCount": 665,
          "title": "Conditional independence for pretext task selection in Self-supervised speech representation learning. (arXiv:2104.07388v2 [eess.AS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.00062",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kansizoglou_I/0/1/0/all/0/1\">Ioannis Kansizoglou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bampis_L/0/1/0/all/0/1\">Loukas Bampis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gasteratos_A/0/1/0/all/0/1\">Antonios Gasteratos</a>",
          "description": "One of the most prominent attributes of Neural Networks (NNs) constitutes\ntheir capability of learning to extract robust and descriptive features from\nhigh dimensional data, like images. Hence, such an ability renders their\nexploitation as feature extractors particularly frequent in an abundant of\nmodern reasoning systems. Their application scope mainly includes complex\ncascade tasks, like multi-modal recognition and deep Reinforcement Learning\n(RL). However, NNs induce implicit biases that are difficult to avoid or to\ndeal with and are not met in traditional image descriptors. Moreover, the lack\nof knowledge for describing the intra-layer properties -- and thus their\ngeneral behavior -- restricts the further applicability of the extracted\nfeatures. With the paper at hand, a novel way of visualizing and understanding\nthe vector space before the NNs' output layer is presented, aiming to enlighten\nthe deep feature vectors' properties under classification tasks. Main attention\nis paid to the nature of overfitting in the feature space and its adverse\neffect on further exploitation. We present the findings that can be derived\nfrom our model's formulation, and we evaluate them on realistic recognition\nscenarios, proving its prominence by improving the obtained results.",
          "link": "http://arxiv.org/abs/2007.00062",
          "publishedOn": "2021-07-02T01:58:02.845Z",
          "wordCount": 671,
          "title": "Deep Feature Space: A Geometrical Perspective. (arXiv:2007.00062v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.12247",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nag_S/0/1/0/all/0/1\">Sayan Nag</a>",
          "description": "Self-supervised learning and pre-training strategies have developed over the\nlast few years especially for Convolutional Neural Networks (CNNs). Recently\napplication of such methods can also be noticed for Graph Neural Networks\n(GNNs) . In this paper, we have used a graph based self-supervised learning\nstrategy with different loss functions (Barlow Twins[Zbontar et al., 2021],\nHSIC[Tsai et al., 2021], VICReg[Bardes et al., 2021]) which have shown\npromising results when applied with CNNs previously. We have also proposed a\nhybrid loss function combining the advantages of VICReg and HSIC and called it\nas VICRegHSIC. The performance of these aforementioned methods have been\ncompared when applied to different datasets such as MUTAG, PROTEINS and\nIMDB-Binary. Moreover, the impact of different batch sizes, projector\ndimensions and data augmentation strategies have also been explored",
          "link": "http://arxiv.org/abs/2105.12247",
          "publishedOn": "2021-07-02T01:58:02.832Z",
          "wordCount": 626,
          "title": "Graph Self Supervised Learning: the BT, the HSIC, and the VICReg. (arXiv:2105.12247v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.00695",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khuat_T/0/1/0/all/0/1\">Thanh Tung Khuat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gabrys_B/0/1/0/all/0/1\">Bogdan Gabrys</a>",
          "description": "This paper proposes a simple yet powerful ensemble classifier, called Random\nHyperboxes, constructed from individual hyperbox-based classifiers trained on\nthe random subsets of sample and feature spaces of the training set. We also\nshow a generalization error bound of the proposed classifier based on the\nstrength of the individual hyperbox-based classifiers as well as the\ncorrelation among them. The effectiveness of the proposed classifier is\nanalyzed using a carefully selected illustrative example and compared\nempirically with other popular single and ensemble classifiers via 20 datasets\nusing statistical testing methods. The experimental results confirmed that our\nproposed method outperformed other fuzzy min-max neural networks, popular\nlearning algorithms, and is competitive with other ensemble methods. Finally,\nwe identify the existing issues related to the generalization error bounds of\nthe real datasets and inform the potential research directions.",
          "link": "http://arxiv.org/abs/2006.00695",
          "publishedOn": "2021-07-02T01:58:02.817Z",
          "wordCount": 608,
          "title": "Random Hyperboxes. (arXiv:2006.00695v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.04822",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Almulla_H/0/1/0/all/0/1\">Hussein Almulla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gay_G/0/1/0/all/0/1\">Gregory Gay</a>",
          "description": "Search-based test generation is guided by feedback from one or more fitness\nfunctions -- scoring functions that judge solution optimality. Choosing\ninformative fitness functions is crucial to meeting the goals of a tester.\nUnfortunately, many goals - such as forcing the class-under-test to throw\nexceptions, increasing test suite diversity, and attaining Strong Mutation\nCoverage - do not have effective fitness function formulations. We propose that\nmeeting such goals requires treating fitness function identification as a\nsecondary optimization step. An adaptive algorithm that can vary the selection\nof fitness functions could adjust its selection throughout the generation\nprocess to maximize goal attainment, based on the current population of test\nsuites. To test this hypothesis, we have implemented two reinforcement learning\nalgorithms in the EvoSuite unit test generation framework, and used these\nalgorithms to dynamically set the fitness functions used during generation for\nthe three goals identified above.\n\nWe have evaluated our framework, EvoSuiteFIT, on a set of Java case examples.\nEvoSuiteFIT techniques attain significant improvements for two of the three\ngoals, and show limited improvements on the third when the number of\ngenerations of evolution is fixed. Additionally, for two of the three goals,\nEvoSuiteFIT detects faults missed by the other techniques. The ability to\nadjust fitness functions allows strategic choices that efficiently produce more\neffective test suites, and examining these choices offers insight into how to\nattain our testing goals. We find that adaptive fitness function selection is a\npowerful technique to apply when an effective fitness function does not already\nexist for achieving a testing goal.",
          "link": "http://arxiv.org/abs/2102.04822",
          "publishedOn": "2021-07-02T01:58:02.797Z",
          "wordCount": 731,
          "title": "Learning How to Search: Generating Effective Test Cases Through Adaptive Fitness Function Selection. (arXiv:2102.04822v2 [cs.SE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.08581",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dutta_U/0/1/0/all/0/1\">Ujjal Kr Dutta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Repakula_S/0/1/0/all/0/1\">Sandeep Repakula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parmar_M/0/1/0/all/0/1\">Maulik Parmar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravi_A/0/1/0/all/0/1\">Abhinav Ravi</a>",
          "description": "In this paper, we utilize deep visual Representation Learning to address an\nimportant problem in fashion e-commerce: color variants identification, i.e.,\nidentifying fashion products that match exactly in their design (or style), but\nonly to differ in their color. At first we attempt to tackle the problem by\nobtaining manual annotations (depicting whether two products are color\nvariants), and train a supervised triplet loss based neural network model to\nlearn representations of fashion products. However, for large scale real-world\nindustrial datasets such as addressed in our paper, it is infeasible to obtain\nannotations for the entire dataset, while capturing all the difficult corner\ncases. Interestingly, we observed that color variants are essentially\nmanifestations of color jitter based augmentations. Thus, we instead explore\nSelf-Supervised Learning (SSL) to solve this problem. We observed that existing\nstate-of-the-art SSL methods perform poor, for our problem. To address this, we\npropose a novel SSL based color variants model that simultaneously focuses on\ndifferent parts of an apparel. Quantitative and qualitative evaluation shows\nthat our method outperforms existing SSL methods, and at times, the supervised\nmodel.",
          "link": "http://arxiv.org/abs/2104.08581",
          "publishedOn": "2021-07-02T01:58:02.777Z",
          "wordCount": 663,
          "title": "Color Variants Identification in Fashion e-commerce via Contrastive Self-Supervised Representation Learning. (arXiv:2104.08581v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.08194",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liang_H/0/1/0/all/0/1\">Haoyu Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_Z/0/1/0/all/0/1\">Zhihao Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1\">Yuyuan Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hang Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zihao He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_S/0/1/0/all/0/1\">Shu-Tao Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bo Zhang</a>",
          "description": "Convolutional neural networks (CNNs) have been successfully used in a range\nof tasks. However, CNNs are often viewed as \"black-box\" and lack of\ninterpretability. One main reason is due to the filter-class entanglement -- an\nintricate many-to-many correspondence between filters and classes. Most\nexisting works attempt post-hoc interpretation on a pre-trained model, while\nneglecting to reduce the entanglement underlying the model. In contrast, we\nfocus on alleviating filter-class entanglement during training. Inspired by\ncellular differentiation, we propose a novel strategy to train interpretable\nCNNs by encouraging class-specific filters, among which each filter responds to\nonly one (or few) class. Concretely, we design a learnable sparse\nClass-Specific Gate (CSG) structure to assign each filter with one (or few)\nclass in a flexible way. The gate allows a filter's activation to pass only\nwhen the input samples come from the specific class. Extensive experiments\ndemonstrate the fabulous performance of our method in generating a sparse and\nhighly class-related representation of the input, which leads to stronger\ninterpretability. Moreover, comparing with the standard training strategy, our\nmodel displays benefits in applications like object localization and\nadversarial sample detection. Code link: https://github.com/hyliang96/CSGCNN.",
          "link": "http://arxiv.org/abs/2007.08194",
          "publishedOn": "2021-07-02T01:58:02.760Z",
          "wordCount": 693,
          "title": "Training Interpretable Convolutional Neural Networks by Differentiating Class-specific Filters. (arXiv:2007.08194v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11595",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mary_P/0/1/0/all/0/1\">Philippe Mary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koivunen_V/0/1/0/all/0/1\">Visa Koivunen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moy_C/0/1/0/all/0/1\">Christophe Moy</a>",
          "description": "In this chapter, we will give comprehensive examples of applying RL in\noptimizing the physical layer of wireless communications by defining different\nclass of problems and the possible solutions to handle them. In Section 9.2, we\npresent all the basic theory needed to address a RL problem, i.e. Markov\ndecision process (MDP), Partially observable Markov decision process (POMDP),\nbut also two very important and widely used algorithms for RL, i.e. the\nQ-learning and SARSA algorithms. We also introduce the deep reinforcement\nlearning (DRL) paradigm and the section ends with an introduction to the\nmulti-armed bandits (MAB) framework. Section 9.3 focuses on some toy examples\nto illustrate how the basic concepts of RL are employed in communication\nsystems. We present applications extracted from literature with simplified\nsystem models using similar notation as in Section 9.2 of this Chapter. In\nSection 9.3, we also focus on modeling RL problems, i.e. how action and state\nspaces and rewards are chosen. The Chapter is concluded in Section 9.4 with a\nprospective thought on RL trends and it ends with a review of a broader state\nof the art in Section 9.5.",
          "link": "http://arxiv.org/abs/2106.11595",
          "publishedOn": "2021-07-02T01:58:02.753Z",
          "wordCount": 654,
          "title": "Reinforcement Learning for Physical Layer Communications. (arXiv:2106.11595v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.07621",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Borghini_E/0/1/0/all/0/1\">Eugenio Borghini</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Fernandez_X/0/1/0/all/0/1\">Ximena Fern&#xe1;ndez</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Groisman_P/0/1/0/all/0/1\">Pablo Groisman</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Mindlin_G/0/1/0/all/0/1\">Gabriel Mindlin</a>",
          "description": "We address the problem of estimating intrinsic distances in a manifold from a\nfinite sample. We prove that the metric space defined by the sample endowed\nwith a computable metric known as sample Fermat distance converges a.s. in the\nsense of Gromov-Hausdorff. The limiting object is the manifold itself endowed\nwith the population Fermat distance, an intrinsic metric that accounts for both\nthe geometry of the manifold and the density that produces the sample. This\nresult is applied to obtain intrinsic persistence diagrams, which are less\nsensitive to the particular embedding of the manifold in the Euclidean space.\nWe show that this approach is robust to outliers and deduce a method for\npattern recognition in signals, with applications in real data.",
          "link": "http://arxiv.org/abs/2012.07621",
          "publishedOn": "2021-07-02T01:58:02.745Z",
          "wordCount": 606,
          "title": "Intrinsic persistent homology via density-based metric learning. (arXiv:2012.07621v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.09458",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Whitney_W/0/1/0/all/0/1\">William F. Whitney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bloesch_M/0/1/0/all/0/1\">Michael Bloesch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Springenberg_J/0/1/0/all/0/1\">Jost Tobias Springenberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdolmaleki_A/0/1/0/all/0/1\">Abbas Abdolmaleki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1\">Kyunghyun Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riedmiller_M/0/1/0/all/0/1\">Martin Riedmiller</a>",
          "description": "Despite the close connection between exploration and sample efficiency, most\nstate of the art reinforcement learning algorithms include no considerations\nfor exploration beyond maximizing the entropy of the policy. In this work we\naddress this seeming missed opportunity. We observe that the most common\nformulation of directed exploration in deep RL, known as bonus-based\nexploration (BBE), suffers from bias and slow coverage in the few-sample\nregime. This causes BBE to be actively detrimental to policy learning in many\ncontrol tasks. We show that by decoupling the task policy from the exploration\npolicy, directed exploration can be highly effective for sample-efficient\ncontinuous control. Our method, Decoupled Exploration and Exploitation Policies\n(DEEP), can be combined with any off-policy RL algorithm without modification.\nWhen used in conjunction with soft actor-critic, DEEP incurs no performance\npenalty in densely-rewarding environments. On sparse environments, DEEP gives a\nseveral-fold improvement in data efficiency due to better exploration.",
          "link": "http://arxiv.org/abs/2101.09458",
          "publishedOn": "2021-07-02T01:58:02.738Z",
          "wordCount": 621,
          "title": "Decoupled Exploration and Exploitation Policies for Sample-Efficient Reinforcement Learning. (arXiv:2101.09458v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.05037",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Apicella_A/0/1/0/all/0/1\">Andrea Apicella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Isgro_F/0/1/0/all/0/1\">Francesco Isgr&#xf2;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prevete_R/0/1/0/all/0/1\">Roberto Prevete</a>",
          "description": "Nowadays, it is growing interest to make Machine Learning (ML) systems more\nunderstandable and trusting to general users. Thus, generating explanations for\nML system behaviours that are understandable to human beings is a central\nscientific and technological issue addressed by the rapidly growing research\narea of eXplainable Artificial Intelligence (XAI). Recently, it is becoming\nmore and more evident that new directions to create better explanations should\ntake into account what a good explanation is to a human user, and consequently,\ndevelop XAI solutions able to provide user-centred explanations. This paper\nsuggests taking advantage of developing an XAI general approach that allows\nproducing explanations for an ML system behaviour in terms of different and\nuser-selected input features, i.e., explanations composed of input properties\nthat the human user can select according to his background knowledge and goals.\nTo this end, we propose an XAI general approach which is able: 1) to construct\nexplanations in terms of input features that represent more salient and\nunderstandable input properties for a user, which we call here Middle-Level\ninput Features (MLFs), 2) to be applied to different types of MLFs. We\nexperimentally tested our approach on two different datasets and using three\ndifferent types of MLFs. The results seem encouraging.",
          "link": "http://arxiv.org/abs/2106.05037",
          "publishedOn": "2021-07-02T01:58:02.718Z",
          "wordCount": 659,
          "title": "A general approach for Explanations in terms of Middle Level Features. (arXiv:2106.05037v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.13853",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Andersson_C/0/1/0/all/0/1\">Carl R. Andersson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wahlstrom_N/0/1/0/all/0/1\">Niklas Wahlstr&#xf6;m</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schon_T/0/1/0/all/0/1\">Thomas B. Sch&#xf6;n</a>",
          "description": "We propose a model for hierarchical structured data as an extension to the\nstochastic temporal convolutional network. The proposed model combines an\nautoregressive model with a hierarchical variational autoencoder and\ndownsampling to achieve superior computational complexity. We evaluate the\nproposed model on two different types of sequential data: speech and\nhandwritten text. The results are promising with the proposed model achieving\nstate-of-the-art performance.",
          "link": "http://arxiv.org/abs/2104.13853",
          "publishedOn": "2021-07-02T01:58:02.712Z",
          "wordCount": 536,
          "title": "Learning deep autoregressive models for hierarchical data. (arXiv:2104.13853v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00606",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mazzia_V/0/1/0/all/0/1\">Vittorio Mazzia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Angarano_S/0/1/0/all/0/1\">Simone Angarano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salvetti_F/0/1/0/all/0/1\">Francesco Salvetti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Angelini_F/0/1/0/all/0/1\">Federico Angelini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiaberge_M/0/1/0/all/0/1\">Marcello Chiaberge</a>",
          "description": "Deep neural networks based purely on attention have been successful across\nseveral domains, relying on minimal architectural priors from the designer. In\nHuman Action Recognition (HAR), attention mechanisms have been primarily\nadopted on top of standard convolutional or recurrent layers, improving the\noverall generalization capability. In this work, we introduce Action\nTransformer (AcT), a simple, fully self-attentional architecture that\nconsistently outperforms more elaborated networks that mix convolutional,\nrecurrent, and attentive layers. In order to limit computational and energy\nrequests, building on previous human action recognition research, the proposed\napproach exploits 2D pose representations over small temporal windows,\nproviding a low latency solution for accurate and effective real-time\nperformance. Moreover, we open-source MPOSE2021, a new large-scale dataset, as\nan attempt to build a formal training and evaluation benchmark for real-time\nshort-time human action recognition. Extensive experimentation on MPOSE2021\nwith our proposed methodology and several previous architectural solutions\nproves the effectiveness of the AcT model and poses the base for future work on\nHAR.",
          "link": "http://arxiv.org/abs/2107.00606",
          "publishedOn": "2021-07-02T01:58:02.704Z",
          "wordCount": 607,
          "title": "Action Transformer: A Self-Attention Model for Short-Time Human Action Recognition. (arXiv:2107.00606v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10978",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Durrschnabel_D/0/1/0/all/0/1\">Dominik D&#xfc;rrschnabel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koyda_M/0/1/0/all/0/1\">Maren Koyda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stumme_G/0/1/0/all/0/1\">Gerd Stumme</a>",
          "description": "Formal Concept Analysis (FCA) allows to analyze binary data by deriving\nconcepts and ordering them in lattices. One of the main goals of FCA is to\nenable humans to comprehend the information that is encapsulated in the data;\nhowever, the large size of concept lattices is a limiting factor for the\nfeasibility of understanding the underlying structural properties. The size of\nsuch a lattice depends on the number of subcontexts in the corresponding formal\ncontext that are isomorphic to a contranominal scale of high dimension. In this\nwork, we propose the algorithm ContraFinder that enables the computation of all\ncontranominal scales of a given formal context. Leveraging this algorithm, we\nintroduce delta-adjusting, a novel approach in order to decrease the number of\ncontranominal scales in a formal context by the selection of an appropriate\nattribute subset. We demonstrate that delta-adjusting a context reduces the\nsize of the hereby emerging sub-semilattice and that the implication set is\nrestricted to meaningful implications. This is evaluated with respect to its\nassociated knowledge by means of a classification task. Hence, our proposed\ntechnique strongly improves understandability while preserving important\nconceptual structures.",
          "link": "http://arxiv.org/abs/2106.10978",
          "publishedOn": "2021-07-02T01:58:02.697Z",
          "wordCount": 652,
          "title": "Attribute Selection using Contranominal Scales. (arXiv:2106.10978v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.03988",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Koloski_B/0/1/0/all/0/1\">Boshko Koloski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perdih_T/0/1/0/all/0/1\">Timen Stepi&#x161;nik Perdih</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pollak_S/0/1/0/all/0/1\">Senja Pollak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skrlj_B/0/1/0/all/0/1\">Bla&#x17e; &#x160;krlj</a>",
          "description": "Identification of Fake News plays a prominent role in the ongoing pandemic,\nimpacting multiple aspects of day-to-day life. In this work we present a\nsolution to the shared task titled COVID19 Fake News Detection in English,\nscoring the 50th place amongst 168 submissions. The solution was within 1.5% of\nthe best performing solution. The proposed solution employs a heterogeneous\nrepresentation ensemble, adapted for the classification task via an additional\nneural classification head comprised of multiple hidden layers. The paper\nconsists of detailed ablation studies further displaying the proposed method's\nbehavior and possible implications. The solution is freely available.\n\\url{https://gitlab.com/boshko.koloski/covid19-fake-news}",
          "link": "http://arxiv.org/abs/2101.03988",
          "publishedOn": "2021-07-02T01:58:02.691Z",
          "wordCount": 619,
          "title": "Identification of COVID-19 related Fake News via Neural Stacking. (arXiv:2101.03988v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.03479",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jian Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Siyang Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harding_S/0/1/0/all/0/1\">Seth Austin Harding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Haibin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_S/0/1/0/all/0/1\">Shih-wei Liao</a>",
          "description": "Many complex multi-robot systems such as robot swarms control and autonomous\nvehicle coordination can be modeled as Multi-Agent Reinforcement Learning\n(MARL) tasks. QMIX, a widely popular MARL algorithm, has been used as a\nbaseline for the benchmark environments, e.g., Starcraft Multi-Agent Challenge\n(SMAC), Difficulty-Enhanced Predator-Prey (DEPP). Recent variants of QMIX\ntarget relaxing the monotonicity constraint of QMIX, allowing for performance\nimprovement in SMAC. In this paper, we investigate the code-level optimizations\nof these variants and the monotonicity constraint. (1) We find that such\nimprovements of the variants are significantly affected by various code-level\noptimizations. (2) The experiment results show that QMIX with normalized\noptimizations outperforms other works in SMAC; (3) beyond the common wisdom\nfrom these works, the monotonicity constraint can improve sample efficiency in\nSMAC and DEPP. We also discuss why monotonicity constraints work well in purely\ncooperative tasks with a theoretical analysis. We open-source the code at\n\\url{https://github.com/hijkzzz/pymarl2}.",
          "link": "http://arxiv.org/abs/2102.03479",
          "publishedOn": "2021-07-02T01:58:02.672Z",
          "wordCount": 722,
          "title": "Rethinking the Implementation Tricks and Monotonicity Constraint in Cooperative Multi-Agent Reinforcement Learning. (arXiv:2102.03479v12 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.01061",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mishra_K/0/1/0/all/0/1\">Kumar Vijay Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_M/0/1/0/all/0/1\">M. Ashok Kumar</a>",
          "description": "We examine the role of information geometry in the context of classical\nCram\\'er-Rao (CR) type inequalities. In particular, we focus on Eguchi's theory\nof obtaining dualistic geometric structures from a divergence function and then\napplying Amari-Nagoaka's theory to obtain a CR type inequality. The classical\ndeterministic CR inequality is derived from Kullback-Leibler (KL)-divergence.\nWe show that this framework could be generalized to other CR type inequalities\nthrough four examples: $\\alpha$-version of CR inequality, generalized CR\ninequality, Bayesian CR inequality, and Bayesian $\\alpha$-CR inequality. These\nare obtained from, respectively, $I_\\alpha$-divergence (or relative\n$\\alpha$-entropy), generalized Csisz\\'ar divergence, Bayesian KL divergence,\nand Bayesian $I_\\alpha$-divergence.",
          "link": "http://arxiv.org/abs/2104.01061",
          "publishedOn": "2021-07-02T01:58:02.664Z",
          "wordCount": 591,
          "title": "Information Geometry and Classical Cram\\'{e}r-Rao Type Inequalities. (arXiv:2104.01061v2 [cs.IT] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.11918",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Ren_Z/0/1/0/all/0/1\">Zhimei Ren</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhengyuan Zhou</a>",
          "description": "We study the problem of dynamic batch learning in high-dimensional sparse\nlinear contextual bandits, where a decision maker can only adapt decisions at a\nbatch level. In particular, the decision maker, only observing rewards at the\nend of each batch, dynamically decides how many individuals to include in the\nnext batch (at the current batch's end) and what personalized action-selection\nscheme to adopt within the batch. Such batch constraints are ubiquitous in a\nvariety of practical contexts, including personalized product offerings in\nmarketing and medical treatment selection in clinical trials. We characterize\nthe fundamental learning limit in this problem via a novel lower bound analysis\nand provide a simple, exploration-free algorithm that uses the LASSO estimator,\nwhich achieves the minimax optimal performance characterized by the lower bound\n(up to log factors). To our best knowledge, our work provides the first inroad\ninto a rigorous understanding of dynamic batch learning with high-dimensional\ncovariates. We also demonstrate the efficacy of our algorithm on both synthetic\ndata and the Warfarin medical dosing data. The empirical results show that with\nthree batches (hence only two opportunities to adapt), our algorithm already\nperforms comparably (in terms of statistical performance) to the\nstate-of-the-art fully online high-dimensional linear contextual bandits\nalgorithm. As an added bonus, since our algorithm operates in batches, it is\norders of magnitudes faster than fully online learning algorithms. As such, our\nalgorithm provides a desirable candidate for practical data-driven personalized\ndecision making problems, where limited adaptivity is often a hard constraint.",
          "link": "http://arxiv.org/abs/2008.11918",
          "publishedOn": "2021-07-02T01:58:02.654Z",
          "wordCount": 717,
          "title": "Dynamic Batch Learning in High-Dimensional Sparse Linear Contextual Bandits. (arXiv:2008.11918v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.08222",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alistarh_D/0/1/0/all/0/1\">Dan Alistarh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Korhonen_J/0/1/0/all/0/1\">Janne H. Korhonen</a>",
          "description": "We consider a standard distributed optimisation setting where $N$ machines,\neach holding a $d$-dimensional function $f_i$, aim to jointly minimise the sum\nof the functions $\\sum_{i = 1}^N f_i (x)$. This problem arises naturally in\nlarge-scale distributed optimisation, where a standard solution is to apply\nvariants of (stochastic) gradient descent. We focus on the communication\ncomplexity of this problem: our main result provides the first fully\nunconditional bounds on total number of bits which need to be sent and received\nby the $N$ machines to solve this problem under point-to-point communication,\nwithin a given error-tolerance. Specifically, we show that $\\Omega( Nd \\log d /\nN\\varepsilon)$ total bits need to be communicated between the machines to find\nan additive $\\epsilon$-approximation to the minimum of $\\sum_{i = 1}^N f_i\n(x)$. The result holds for both deterministic and randomised algorithms, and,\nimportantly, requires no assumptions on the algorithm structure. The lower\nbound is tight under certain restrictions on parameter values, and is matched\nwithin constant factors for quadratic objectives by a new variant of quantised\ngradient descent, which we describe and analyse. Our results bring over tools\nfrom communication complexity to distributed optimisation, which has potential\nfor further applications.",
          "link": "http://arxiv.org/abs/2010.08222",
          "publishedOn": "2021-07-02T01:58:02.647Z",
          "wordCount": 665,
          "title": "Towards Tight Communication Lower Bounds for Distributed Optimisation. (arXiv:2010.08222v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.07627",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xinchi Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parcollet_T/0/1/0/all/0/1\">Titouan Parcollet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandez_Marques_J/0/1/0/all/0/1\">Javier Fernandez-Marques</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gusmao_P/0/1/0/all/0/1\">Pedro Porto Buarque de Gusmao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beutel_D/0/1/0/all/0/1\">Daniel J. Beutel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Topal_T/0/1/0/all/0/1\">Taner Topal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathur_A/0/1/0/all/0/1\">Akhil Mathur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lane_N/0/1/0/all/0/1\">Nicholas D. Lane</a>",
          "description": "Despite impressive results, deep learning-based technologies also raise\nsevere privacy and environmental concerns induced by the training procedure\noften conducted in datacenters. In response, alternatives to centralized\ntraining such as Federated Learning (FL) have emerged. Perhaps unexpectedly,\nFL, in particular, is starting to be deployed at a global scale by companies\nthat must adhere to new legal demands and policies originating from governments\nand civil society for privacy protection. However, the potential environmental\nimpact related to FL remains unclear and unexplored. This paper offers the\nfirst-ever systematic study of the carbon footprint of FL. First, we propose a\nrigorous model to quantify the carbon footprint, hence facilitating the\ninvestigation of the relationship between FL design and carbon emissions. Then,\nwe compare the carbon footprint of FL to traditional centralized learning. Our\nfindings show that FL, despite being slower to converge in some cases, may\nresult in a comparatively greener impact than a centralized equivalent setup.\nWe performed extensive experiments across different types of datasets,\nsettings, and various deep learning models with FL. Finally, we highlight and\nconnect the reported results to the future challenges and trends in FL to\nreduce its environmental impact, including algorithms efficiency, hardware\ncapabilities, and stronger industry transparency.",
          "link": "http://arxiv.org/abs/2102.07627",
          "publishedOn": "2021-07-02T01:58:02.640Z",
          "wordCount": 707,
          "title": "A first look into the carbon footprint of federated learning. (arXiv:2102.07627v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.12876",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Priyanshu_A/0/1/0/all/0/1\">Aman Priyanshu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sinha_M/0/1/0/all/0/1\">Mudit Sinha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehta_S/0/1/0/all/0/1\">Shreyans Mehta</a>",
          "description": "Social media platforms such as Twitter, Facebook etc can be utilised as an\nimportant source of information during disaster events. This information can be\nused for disaster response and crisis management if processed accurately and\nquickly. However, the data present in such situations is ever-changing, and\nusing considerable resources during such a crisis is not feasible. Therefore,\nwe have to develop a low resource and continually learning system that\nincorporates text classification models which are robust against noisy and\nunordered data. We utilised Distributed learning which enabled us to learn on\nresource-constrained devices, then to alleviate catastrophic forgetting in our\ntarget neural networks we utilized regularization. We then applied federated\naveraging for distributed learning and to aggregate the central model for\ncontinual learning.",
          "link": "http://arxiv.org/abs/2104.12876",
          "publishedOn": "2021-07-02T01:58:02.620Z",
          "wordCount": 604,
          "title": "Continual Distributed Learning for Crisis Management. (arXiv:2104.12876v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.09376",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Chuxiong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_H/0/1/0/all/0/1\">Hongming Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jie Hu</a>",
          "description": "It is hard to directly implement Graph Neural Networks (GNNs) on large scaled\ngraphs. Besides of existed neighbor sampling techniques, scalable methods\ndecoupling graph convolutions and other learnable transformations into\npreprocessing and post classifier allow normal minibatch training. By replacing\nredundant concatenation operation with attention mechanism in SIGN, we propose\nScalable and Adaptive Graph Neural Networks (SAGN). SAGN can adaptively gather\nneighborhood information among different hops. To further improve scalable\nmodels on semi-supervised learning tasks, we propose Self-Label-Enhance (SLE)\nframework combining self-training approach and label propagation in depth. We\nadd base model with a scalable node label module. Then we iteratively train\nmodels and enhance train set in several stages. To generate input of node label\nmodule, we directly apply label propagation based on one-hot encoded label\nvectors without inner random masking. We find out that empirically the label\nleakage has been effectively alleviated after graph convolutions. The hard\npseudo labels in enhanced train set participate in label propagation with true\nlabels. Experiments on both inductive and transductive datasets demonstrate\nthat, compared with other sampling-based and sampling-free methods, SAGN\nachieves better or comparable results and SLE can further improve performance.",
          "link": "http://arxiv.org/abs/2104.09376",
          "publishedOn": "2021-07-02T01:58:02.614Z",
          "wordCount": 666,
          "title": "Scalable and Adaptive Graph Neural Networks with Self-Label-Enhanced training. (arXiv:2104.09376v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00465",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Nellikkath_R/0/1/0/all/0/1\">Rahul Nellikkath</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chatzivasileiadis_S/0/1/0/all/0/1\">Spyros Chatzivasileiadis</a>",
          "description": "Physics-informed neural networks exploit the existing models of the\nunderlying physical systems to generate higher accuracy results with fewer\ndata. Such approaches can help drastically reduce the computation time and\ngenerate a good estimate of computationally intensive processes in power\nsystems, such as dynamic security assessment or optimal power flow. Combined\nwith the extraction of worst-case guarantees for the neural network\nperformance, such neural networks can be applied in safety-critical\napplications in power systems and build a high level of trust among power\nsystem operators. This paper takes the first step and applies, for the first\ntime to our knowledge, Physics-Informed Neural Networks with Worst-Case\nGuarantees for the DC Optimal Power Flow problem. We look for guarantees\nrelated to (i) maximum constraint violations, (ii) maximum distance between\npredicted and optimal decision variables, and (iii) maximum sub-optimality in\nthe entire input domain. In a range of PGLib-OPF networks, we demonstrate how\nphysics-informed neural networks can be supplied with worst-case guarantees and\nhow they can lead to reduced worst-case violations compared with conventional\nneural networks.",
          "link": "http://arxiv.org/abs/2107.00465",
          "publishedOn": "2021-07-02T01:58:02.607Z",
          "wordCount": 632,
          "title": "Physics-Informed Neural Networks for Minimising Worst-Case Violations in DC Optimal Power Flow. (arXiv:2107.00465v1 [eess.SY])"
        },
        {
          "id": "http://arxiv.org/abs/2011.14196",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Hosseini_S/0/1/0/all/0/1\">Seyed Mohsen Hosseini</a>",
          "description": "A novel method for feature fusion in convolutional neural networks is\nproposed in this paper. Different feature fusion techniques are suggested to\nfacilitate the flow of information and improve the training of deep neural\nnetworks. Some of these techniques as well as the proposed network can be\nconsidered a type of Directed Acyclic Graph (DAG) Network, where a layer can\nreceive inputs from other layers and have outputs to other layers. In the\nproposed general framework of Lattice Fusion Network (LFNet), feature maps of\neach convolutional layer are passed to other layers based on a lattice graph\nstructure, where nodes are convolutional layers. To evaluate the performance of\nthe proposed architecture, different designs based on the general framework of\nLFNet are implemented for the task of image denoising. This task is used as an\nexample where training deep convolutional networks is needed. Results are\ncompared with state of the art methods. The proposed network is able to achieve\nbetter results with far fewer learnable parameters, which shows the\neffectiveness of LFNets for training of deep neural networks.",
          "link": "http://arxiv.org/abs/2011.14196",
          "publishedOn": "2021-07-02T01:58:02.601Z",
          "wordCount": 643,
          "title": "Lattice Fusion Networks for Image Denoising. (arXiv:2011.14196v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.10424",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wolfe_C/0/1/0/all/0/1\">Cameron R. Wolfe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jingkang Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_A/0/1/0/all/0/1\">Arindam Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dun_C/0/1/0/all/0/1\">Chen Dun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bayer_A/0/1/0/all/0/1\">Artun Bayer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Segarra_S/0/1/0/all/0/1\">Santiago Segarra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kyrillidis_A/0/1/0/all/0/1\">Anastasios Kyrillidis</a>",
          "description": "The graph convolutional network (GCN) is a go-to solution for machine\nlearning on graphs, but its training is notoriously difficult to scale both in\nterms of graph size and the number of model parameters. Although some work has\nexplored training on large-scale graphs (e.g., GraphSAGE, ClusterGCN, etc.), we\npioneer efficient training of large-scale GCN models (i.e., ultra-wide,\noverparameterized models) with the proposal of a novel, distributed training\nframework. Our proposed training methodology, called GIST, disjointly\npartitions the parameters of a GCN model into several, smaller sub-GCNs that\nare trained independently and in parallel. In addition to being compatible with\nany GCN architecture, GIST improves model performance, scales to training on\narbitrarily large graphs, significantly decreases wall-clock training time, and\nenables the training of markedly overparameterized GCN models. Remarkably, with\nGIST, we train an astonishgly-wide 32,768-dimensional GraphSAGE model, which\nexceeds the capacity of a single GPU by a factor of 8X, to SOTA performance on\nthe Amazon2M dataset.",
          "link": "http://arxiv.org/abs/2102.10424",
          "publishedOn": "2021-07-02T01:58:02.595Z",
          "wordCount": 662,
          "title": "GIST: Distributed Training for Large-Scale Graph Convolutional Networks. (arXiv:2102.10424v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00652",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xiaoyi Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1\">Jianmin Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dongdong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weiming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_N/0/1/0/all/0/1\">Nenghai Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lu Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_B/0/1/0/all/0/1\">Baining Guo</a>",
          "description": "We present CSWin Transformer, an efficient and effective Transformer-based\nbackbone for general-purpose vision tasks. A challenging issue in Transformer\ndesign is that global self-attention is very expensive to compute whereas local\nself-attention often limits the field of interactions of each token. To address\nthis issue, we develop the Cross-Shaped Window self-attention mechanism for\ncomputing self-attention in the horizontal and vertical stripes in parallel\nthat form a cross-shaped window, with each stripe obtained by splitting the\ninput feature into stripes of equal width. We provide a detailed mathematical\nanalysis of the effect of the stripe width and vary the stripe width for\ndifferent layers of the Transformer network which achieves strong modeling\ncapability while limiting the computation cost. We also introduce\nLocally-enhanced Positional Encoding (LePE), which handles the local positional\ninformation better than existing encoding schemes. LePE naturally supports\narbitrary input resolutions, and is thus especially effective and friendly for\ndownstream tasks. Incorporated with these designs and a hierarchical structure,\nCSWin Transformer demonstrates competitive performance on common vision tasks.\nSpecifically, it achieves 85.4% Top-1 accuracy on ImageNet-1K without any extra\ntraining data or label, 53.9 box AP and 46.4 mask AP on the COCO detection\ntask, and 51.7 mIOU on the ADE20K semantic segmentation task, surpassing\nprevious state-of-the-art Swin Transformer backbone by +1.2, +2.0, +1.4, and\n+2.0 respectively under the similar FLOPs setting. By further pretraining on\nthe larger dataset ImageNet-21K, we achieve 87.5% Top-1 accuracy on ImageNet-1K\nand state-of-the-art segmentation performance on ADE20K with 55.2 mIoU. The\ncode and models will be available at\nhttps://github.com/microsoft/CSWin-Transformer.",
          "link": "http://arxiv.org/abs/2107.00652",
          "publishedOn": "2021-07-02T01:58:02.576Z",
          "wordCount": 721,
          "title": "CSWin Transformer: A General Vision Transformer Backbone with Cross-Shaped Windows. (arXiv:2107.00652v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00648",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Braman_N/0/1/0/all/0/1\">Nathaniel Braman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gordon_J/0/1/0/all/0/1\">Jacob W. H. Gordon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goossens_E/0/1/0/all/0/1\">Emery T. Goossens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Willis_C/0/1/0/all/0/1\">Caleb Willis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stumpe_M/0/1/0/all/0/1\">Martin C. Stumpe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venkataraman_J/0/1/0/all/0/1\">Jagadish Venkataraman</a>",
          "description": "Clinical decision-making in oncology involves multimodal data such as\nradiology scans, molecular profiling, histopathology slides, and clinical\nfactors. Despite the importance of these modalities individually, no deep\nlearning framework to date has combined them all to predict patient prognosis.\nHere, we predict the overall survival (OS) of glioma patients from diverse\nmultimodal data with a Deep Orthogonal Fusion (DOF) model. The model learns to\ncombine information from multiparametric MRI exams, biopsy-based modalities\n(such as H&E slide images and/or DNA sequencing), and clinical variables into a\ncomprehensive multimodal risk score. Prognostic embeddings from each modality\nare learned and combined via attention-gated tensor fusion. To maximize the\ninformation gleaned from each modality, we introduce a multimodal\northogonalization (MMO) loss term that increases model performance by\nincentivizing constituent embeddings to be more complementary. DOF predicts OS\nin glioma patients with a median C-index of 0.788 +/- 0.067, significantly\noutperforming (p=0.023) the best performing unimodal model with a median\nC-index of 0.718 +/- 0.064. The prognostic model significantly stratifies\nglioma patients by OS within clinical subsets, adding further granularity to\nprognostic clinical grading and molecular subtyping.",
          "link": "http://arxiv.org/abs/2107.00648",
          "publishedOn": "2021-07-02T01:58:02.558Z",
          "wordCount": 659,
          "title": "Deep Orthogonal Fusion: Multimodal Prognostic Biomarker Discovery Integrating Radiology, Pathology, Genomic, and Clinical Data. (arXiv:2107.00648v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00593",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bynum_L/0/1/0/all/0/1\">Lucius E.J. Bynum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loftus_J/0/1/0/all/0/1\">Joshua R. Loftus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stoyanovich_J/0/1/0/all/0/1\">Julia Stoyanovich</a>",
          "description": "A significant body of research in the data sciences considers unfair\ndiscrimination against social categories such as race or gender that could\noccur or be amplified as a result of algorithmic decisions. Simultaneously,\nreal-world disparities continue to exist, even before algorithmic decisions are\nmade. In this work, we draw on insights from the social sciences and humanistic\nstudies brought into the realm of causal modeling and constrained optimization,\nand develop a novel algorithmic framework for tackling pre-existing real-world\ndisparities. The purpose of our framework, which we call the \"impact\nremediation framework,\" is to measure real-world disparities and discover the\noptimal intervention policies that could help improve equity or access to\nopportunity for those who are underserved with respect to an outcome of\ninterest. We develop a disaggregated approach to tackling pre-existing\ndisparities that relaxes the typical set of assumptions required for the use of\nsocial categories in structural causal models. Our approach flexibly\nincorporates counterfactuals and is compatible with various ontological\nassumptions about the nature of social categories. We demonstrate impact\nremediation with a real-world case study and compare our disaggregated approach\nto an existing state-of-the-art approach, comparing its structure and resulting\npolicy recommendations. In contrast to most work on optimal policy learning, we\nexplore disparity reduction itself as an objective, explicitly focusing the\npower of algorithms on reducing inequality.",
          "link": "http://arxiv.org/abs/2107.00593",
          "publishedOn": "2021-07-02T01:58:02.552Z",
          "wordCount": 664,
          "title": "Impact Remediation: Optimal Interventions to Reduce Inequality. (arXiv:2107.00593v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2008.03534",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Gautier_R/0/1/0/all/0/1\">Raphael Gautier</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Pandita_P/0/1/0/all/0/1\">Piyush Pandita</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ghosh_S/0/1/0/all/0/1\">Sayan Ghosh</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Mavris_D/0/1/0/all/0/1\">Dimitri Mavris</a>",
          "description": "Modern day engineering problems are ubiquitously characterized by\nsophisticated computer codes that map parameters or inputs to an underlying\nphysical process. In other situations, experimental setups are used to model\nthe physical process in a laboratory, ensuring high precision while being\ncostly in materials and logistics. In both scenarios, only limited amount of\ndata can be generated by querying the expensive information source at a finite\nnumber of inputs or designs. This problem is compounded further in the presence\nof a high-dimensional input space. State-of-the-art parameter space dimension\nreduction methods, such as active subspace, aim to identify a subspace of the\noriginal input space that is sufficient to explain the output response. These\nmethods are restricted by their reliance on gradient evaluations or copious\ndata, making them inadequate to expensive problems without direct access to\ngradients. The proposed methodology is gradient-free and fully Bayesian, as it\nquantifies uncertainty in both the low-dimensional subspace and the surrogate\nmodel parameters. This enables a full quantification of epistemic uncertainty\nand robustness to limited data availability. It is validated on multiple\ndatasets from engineering and science and compared to two other\nstate-of-the-art methods based on four aspects: a) recovery of the active\nsubspace, b) deterministic prediction accuracy, c) probabilistic prediction\naccuracy, and d) training time. The comparison shows that the proposed method\nimproves the active subspace recovery and predictive accuracy, in both the\ndeterministic and probabilistic sense, when only few model observations are\navailable for training, at the cost of increased training time.",
          "link": "http://arxiv.org/abs/2008.03534",
          "publishedOn": "2021-07-02T01:58:02.543Z",
          "wordCount": 706,
          "title": "A Fully Bayesian Gradient-Free Supervised Dimension Reduction Method using Gaussian Processes. (arXiv:2008.03534v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00488",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiongjie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_H/0/1/0/all/0/1\">Hao Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yunpeng Li</a>",
          "description": "Differentiable particle filters provide a flexible mechanism to adaptively\ntrain dynamic and measurement models by learning from observed data. However,\nmost existing differentiable particle filters are within the bootstrap particle\nfiltering framework and fail to incorporate the information from latest\nobservations to construct better proposals. In this paper, we utilize\nconditional normalizing flows to construct proposal distributions for\ndifferentiable particle filters, enriching the distribution families that the\nproposal distributions can represent. In addition, normalizing flows are\nincorporated in the construction of the dynamic model, resulting in a more\nexpressive dynamic model. We demonstrate the performance of the proposed\nconditional normalizing flow-based differentiable particle filters in a visual\ntracking task.",
          "link": "http://arxiv.org/abs/2107.00488",
          "publishedOn": "2021-07-02T01:58:02.535Z",
          "wordCount": 544,
          "title": "Differentiable Particle Filters through Conditional Normalizing Flow. (arXiv:2107.00488v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00472",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Zhou_B/0/1/0/all/0/1\">Baojian Zhou</a>, <a href=\"http://arxiv.org/find/math/1/au:+Sun_Y/0/1/0/all/0/1\">Yifan Sun</a>",
          "description": "In this paper, we propose approximate Frank-Wolfe (FW) algorithms to solve\nconvex optimization problems over graph-structured support sets where the\n\\textit{linear minimization oracle} (LMO) cannot be efficiently obtained in\ngeneral. We first demonstrate that two popular approximation assumptions\n(\\textit{additive} and \\textit{multiplicative gap errors)}, are not valid for\nour problem, in that no cheap gap-approximate LMO oracle exists in general.\nInstead, a new \\textit{approximate dual maximization oracle} (DMO) is proposed,\nwhich approximates the inner product rather than the gap. When the objective is\n$L$-smooth, we prove that the standard FW method using a $\\delta$-approximate\nDMO converges as $\\mathcal{O}(L / \\delta t + (1-\\delta)(\\delta^{-1} +\n\\delta^{-2}))$ in general, and as $\\mathcal{O}(L/(\\delta^2(t+2)))$ over a\n$\\delta$-relaxation of the constraint set. Additionally, when the objective is\n$\\mu$-strongly convex and the solution is unique, a variant of FW converges to\n$\\mathcal{O}(L^2\\log(t)/(\\mu \\delta^6 t^2))$ with the same per-iteration\ncomplexity. Our empirical results suggest that even these improved bounds are\npessimistic, with significant improvement in recovering real-world images with\ngraph-structured sparsity.",
          "link": "http://arxiv.org/abs/2107.00472",
          "publishedOn": "2021-07-02T01:58:02.517Z",
          "wordCount": 598,
          "title": "Approximate Frank-Wolfe Algorithms over Graph-structured Support Sets. (arXiv:2107.00472v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00429",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1\">Yu-Wei Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Natali_L/0/1/0/all/0/1\">Laura Natali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jamialahmadi_O/0/1/0/all/0/1\">Oveis Jamialahmadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romeo_S/0/1/0/all/0/1\">Stefano Romeo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pereira_J/0/1/0/all/0/1\">Joana B. Pereira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Volpe_G/0/1/0/all/0/1\">Giovanni Volpe</a>",
          "description": "Neural network training and validation rely on the availability of large\nhigh-quality datasets. However, in many cases only incomplete datasets are\navailable, particularly in health care applications, where each patient\ntypically undergoes different clinical procedures or can drop out of a study.\nSince the data to train the neural networks need to be complete, most studies\ndiscard the incomplete datapoints, which reduces the size of the training data,\nor impute the missing features, which can lead to artefacts. Alas, both\napproaches are inadequate when a large portion of the data is missing. Here, we\nintroduce GapNet, an alternative deep-learning training approach that can use\nhighly incomplete datasets. First, the dataset is split into subsets of samples\ncontaining all values for a certain cluster of features. Then, these subsets\nare used to train individual neural networks. Finally, this ensemble of neural\nnetworks is combined into a single neural network whose training is fine-tuned\nusing all complete datapoints. Using two highly incomplete real-world medical\ndatasets, we show that GapNet improves the identification of patients with\nunderlying Alzheimer's disease pathology and of patients at risk of\nhospitalization due to Covid-19. By distilling the information available in\nincomplete datasets without having to reduce their size or to impute missing\nvalues, GapNet will permit to extract valuable information from a wide range of\ndatasets, benefiting diverse fields from medicine to engineering.",
          "link": "http://arxiv.org/abs/2107.00429",
          "publishedOn": "2021-07-02T01:58:02.509Z",
          "wordCount": 721,
          "title": "Neural Network Training with Highly Incomplete Datasets. (arXiv:2107.00429v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00637",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dittadi_A/0/1/0/all/0/1\">Andrea Dittadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papa_S/0/1/0/all/0/1\">Samuele Papa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vita_M/0/1/0/all/0/1\">Michele De Vita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1\">Bernhard Sch&#xf6;lkopf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Winther_O/0/1/0/all/0/1\">Ole Winther</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Locatello_F/0/1/0/all/0/1\">Francesco Locatello</a>",
          "description": "The idea behind object-centric representation learning is that natural scenes\ncan better be modeled as compositions of objects and their relations as opposed\nto distributed representations. This inductive bias can be injected into neural\nnetworks to potentially improve systematic generalization and learning\nefficiency of downstream tasks in scenes with multiple objects. In this paper,\nwe train state-of-the-art unsupervised models on five common multi-object\ndatasets and evaluate segmentation accuracy and downstream object property\nprediction. In addition, we study systematic generalization and robustness by\ninvestigating the settings where either single objects are out-of-distribution\n-- e.g., having unseen colors, textures, and shapes -- or global properties of\nthe scene are altered -- e.g., by occlusions, cropping, or increasing the\nnumber of objects. From our experimental study, we find object-centric\nrepresentations to be generally useful for downstream tasks and robust to\nshifts in the data distribution, especially if shifts affect single objects.",
          "link": "http://arxiv.org/abs/2107.00637",
          "publishedOn": "2021-07-02T01:58:02.501Z",
          "wordCount": 593,
          "title": "Generalization and Robustness Implications in Object-Centric Learning. (arXiv:2107.00637v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00352",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Wang_Y/0/1/0/all/0/1\">Yifei Wang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wang_Y/0/1/0/all/0/1\">Yisen Wang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Yang_J/0/1/0/all/0/1\">Jiansheng Yang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lin_Z/0/1/0/all/0/1\">Zhouchen Lin</a>",
          "description": "Recently, sampling methods have been successfully applied to enhance the\nsample quality of Generative Adversarial Networks (GANs). However, in practice,\nthey typically have poor sample efficiency because of the independent proposal\nsampling from the generator. In this work, we propose REP-GAN, a novel sampling\nmethod that allows general dependent proposals by REParameterizing the Markov\nchains into the latent space of the generator. Theoretically, we show that our\nreparameterized proposal admits a closed-form Metropolis-Hastings acceptance\nratio. Empirically, extensive experiments on synthetic and real datasets\ndemonstrate that our REP-GAN largely improves the sample efficiency and obtains\nbetter sample quality simultaneously.",
          "link": "http://arxiv.org/abs/2107.00352",
          "publishedOn": "2021-07-02T01:58:02.494Z",
          "wordCount": 532,
          "title": "Reparameterized Sampling for Generative Adversarial Networks. (arXiv:2107.00352v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00421",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1\">Yiyang Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jiang_W/0/1/0/all/0/1\">Wei Jiang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Charalambous_T/0/1/0/all/0/1\">Themistoklis Charalambous</a>",
          "description": "The repetitive tracking task for time-varying systems (TVSs) with\nnon-repetitive time-varying parameters, which is also called non-repetitive\nTVSs, is realized in this paper using iterative learning control (ILC). A\nmachine learning (ML) based nominal model update mechanism, which utilizes the\nlinear regression technique to update the nominal model at each ILC trial only\nusing the current trial information, is proposed for non-repetitive TVSs in\norder to enhance the ILC performance. Given that the ML mechanism forces the\nmodel uncertainties to remain within the ILC robust tolerance, an ILC update\nlaw is proposed to deal with non-repetitive TVSs. How to tune parameters inside\nML and ILC algorithms to achieve the desired aggregate performance is also\nprovided. The robustness and reliability of the proposed method are verified by\nsimulations. Comparison with current state-of-the-art demonstrates its superior\ncontrol performance in terms of controlling precision. This paper broadens ILC\napplications from time-invariant systems to non-repetitive TVSs, adopts ML\nregression technique to estimate non-repetitive time-varying parameters between\ntwo ILC trials and proposes a detailed parameter tuning mechanism to achieve\ndesired performance, which are the main contributions.",
          "link": "http://arxiv.org/abs/2107.00421",
          "publishedOn": "2021-07-02T01:58:02.483Z",
          "wordCount": 624,
          "title": "Machine learning based iterative learning control for non-repetitive time-varying systems. (arXiv:2107.00421v1 [eess.SY])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00481",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lei_W/0/1/0/all/0/1\">Wanlu Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1\">Yu Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_M/0/1/0/all/0/1\">Ming Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Skoglund_M/0/1/0/all/0/1\">Mikael Skoglund</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1\">Zhu Han</a>",
          "description": "Edge computing provides a promising paradigm to support the implementation of\nIndustrial Internet of Things (IIoT) by offloading tasks to nearby edge nodes.\nMeanwhile, the increasing network size makes it impractical for centralized\ndata processing due to limited bandwidth, and consequently a decentralized\nlearning scheme is preferable. Reinforcement learning (RL) has been widely\ninvestigated and shown to be a promising solution for decision-making and\noptimal control processes. For RL in a decentralized setup, edge nodes (agents)\nconnected through a communication network aim to work collaboratively to find a\npolicy to optimize the global reward as the sum of local rewards. However,\ncommunication costs, scalability and adaptation in complex environments with\nheterogeneous agents may significantly limit the performance of decentralized\nRL. Alternating direction method of multipliers (ADMM) has a structure that\nallows for decentralized implementation, and has shown faster convergence than\ngradient descent based methods. Therefore, we propose an adaptive stochastic\nincremental ADMM (asI-ADMM) algorithm and apply the asI-ADMM to decentralized\nRL with edge-computing-empowered IIoT networks. We provide convergence\nproperties for proposed algorithms by designing a Lyapunov function and prove\nthat the asI-ADMM has $O(\\frac{1}{k}) +O(\\frac{1}{M})$ convergence rate where\n$k$ and $ M$ are the number of iterations and batch samples, respectively.\nThen, we test our algorithm with two supervised learning problems. For\nperformance evaluation, we simulate two applications in decentralized RL\nsettings with homogeneous and heterogeneous agents. The experiment results show\nthat our proposed algorithms outperform the state of the art in terms of\ncommunication costs and scalability, and can well adapt to complex IoT\nenvironments.",
          "link": "http://arxiv.org/abs/2107.00481",
          "publishedOn": "2021-07-02T01:58:02.466Z",
          "wordCount": 701,
          "title": "Adaptive Stochastic ADMM for Decentralized Reinforcement Learning in Edge Industrial IoT. (arXiv:2107.00481v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00630",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kingma_D/0/1/0/all/0/1\">Diederik P. Kingma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salimans_T/0/1/0/all/0/1\">Tim Salimans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poole_B/0/1/0/all/0/1\">Ben Poole</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ho_J/0/1/0/all/0/1\">Jonathan Ho</a>",
          "description": "Diffusion-based generative models have demonstrated a capacity for\nperceptually impressive synthesis, but can they also be great likelihood-based\nmodels? We answer this in the affirmative, and introduce a family of\ndiffusion-based generative models that obtain state-of-the-art likelihoods on\nstandard image density estimation benchmarks. Unlike other diffusion-based\nmodels, our method allows for efficient optimization of the noise schedule\njointly with the rest of the model. We show that the variational lower bound\n(VLB) simplifies to a remarkably short expression in terms of the\nsignal-to-noise ratio of the diffused data, thereby improving our theoretical\nunderstanding of this model class. Using this insight, we prove an equivalence\nbetween several models proposed in the literature. In addition, we show that\nthe continuous-time VLB is invariant to the noise schedule, except for the\nsignal-to-noise ratio at its endpoints. This enables us to learn a noise\nschedule that minimizes the variance of the resulting VLB estimator, leading to\nfaster optimization. Combining these advances with architectural improvements,\nwe obtain state-of-the-art likelihoods on image density estimation benchmarks,\noutperforming autoregressive models that have dominated these benchmarks for\nmany years, with often significantly faster optimization. In addition, we show\nhow to turn the model into a bits-back compression scheme, and demonstrate\nlossless compression rates close to the theoretical optimum.",
          "link": "http://arxiv.org/abs/2107.00630",
          "publishedOn": "2021-07-02T01:58:02.457Z",
          "wordCount": 636,
          "title": "Variational Diffusion Models. (arXiv:2107.00630v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00595",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ji_Z/0/1/0/all/0/1\">Ziwei Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srebro_N/0/1/0/all/0/1\">Nathan Srebro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Telgarsky_M/0/1/0/all/0/1\">Matus Telgarsky</a>",
          "description": "We present and analyze a momentum-based gradient method for training linear\nclassifiers with an exponentially-tailed loss (e.g., the exponential or\nlogistic loss), which maximizes the classification margin on separable data at\na rate of $\\widetilde{\\mathcal{O}}(1/t^2)$. This contrasts with a rate of\n$\\mathcal{O}(1/\\log(t))$ for standard gradient descent, and $\\mathcal{O}(1/t)$\nfor normalized gradient descent. This momentum-based method is derived via the\nconvex dual of the maximum-margin problem, and specifically by applying\nNesterov acceleration to this dual, which manages to result in a simple and\nintuitive method in the primal. This dual view can also be used to derive a\nstochastic variant, which performs adaptive non-uniform sampling via the dual\nvariables.",
          "link": "http://arxiv.org/abs/2107.00595",
          "publishedOn": "2021-07-02T01:58:02.451Z",
          "wordCount": 546,
          "title": "Fast Margin Maximization via Dual Acceleration. (arXiv:2107.00595v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00471",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Thambawita_V/0/1/0/all/0/1\">Vajira Thambawita</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Salehi_P/0/1/0/all/0/1\">Pegah Salehi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sheshkal_S/0/1/0/all/0/1\">Sajad Amouei Sheshkal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hicks_S/0/1/0/all/0/1\">Steven A. Hicks</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hammer_H/0/1/0/all/0/1\">Hugo L.Hammer</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Parasa_S/0/1/0/all/0/1\">Sravanthi Parasa</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lange_T/0/1/0/all/0/1\">Thomas de Lange</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Halvorsen_P/0/1/0/all/0/1\">P&#xe5;l Halvorsen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Riegler_M/0/1/0/all/0/1\">Michael A. Riegler</a>",
          "description": "Processing medical data to find abnormalities is a time-consuming and costly\ntask, requiring tremendous efforts from medical experts. Therefore, Ai has\nbecome a popular tool for the automatic processing of medical data, acting as a\nsupportive tool for doctors. AI tools highly depend on data for training the\nmodels. However, there are several constraints to access to large amounts of\nmedical data to train machine learning algorithms in the medical domain, e.g.,\ndue to privacy concerns and the costly, time-consuming medical data annotation\nprocess. To address this, in this paper we present a novel synthetic data\ngeneration pipeline called SinGAN-Seg to produce synthetic medical data with\nthe corresponding annotated ground truth masks. We show that these synthetic\ndata generation pipelines can be used as an alternative to bypass privacy\nconcerns and as an alternative way to produce artificial segmentation datasets\nwith corresponding ground truth masks to avoid the tedious medical data\nannotation process. As a proof of concept, we used an open polyp segmentation\ndataset. By training UNet++ using both the real polyp segmentation dataset and\nthe corresponding synthetic dataset generated from the SinGAN-Seg pipeline, we\nshow that the synthetic data can achieve a very close performance to the real\ndata when the real segmentation datasets are large enough. In addition, we show\nthat synthetic data generated from the SinGAN-Seg pipeline improving the\nperformance of segmentation algorithms when the training dataset is very small.\nSince our SinGAN-Seg pipeline is applicable for any medical dataset, this\npipeline can be used with any other segmentation datasets.",
          "link": "http://arxiv.org/abs/2107.00471",
          "publishedOn": "2021-07-02T01:58:02.440Z",
          "wordCount": 718,
          "title": "SinGAN-Seg: Synthetic Training Data Generation for Medical Image Segmentation. (arXiv:2107.00471v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00464",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Li_C/0/1/0/all/0/1\">Chris Junchi Li</a>, <a href=\"http://arxiv.org/find/math/1/au:+Yu_Y/0/1/0/all/0/1\">Yaodong Yu</a>, <a href=\"http://arxiv.org/find/math/1/au:+Loizou_N/0/1/0/all/0/1\">Nicolas Loizou</a>, <a href=\"http://arxiv.org/find/math/1/au:+Gidel_G/0/1/0/all/0/1\">Gauthier Gidel</a>, <a href=\"http://arxiv.org/find/math/1/au:+Ma_Y/0/1/0/all/0/1\">Yi Ma</a>, <a href=\"http://arxiv.org/find/math/1/au:+Roux_N/0/1/0/all/0/1\">Nicolas Le Roux</a>, <a href=\"http://arxiv.org/find/math/1/au:+Jordan_M/0/1/0/all/0/1\">Michael I. Jordan</a>",
          "description": "We study the stochastic bilinear minimax optimization problem, presenting an\nanalysis of the Stochastic ExtraGradient (SEG) method with constant step size,\nand presenting variations of the method that yield favorable convergence. We\nfirst note that the last iterate of the basic SEG method only contracts to a\nfixed neighborhood of the Nash equilibrium, independent of the step size. This\ncontrasts sharply with the standard setting of minimization where standard\nstochastic algorithms converge to a neighborhood that vanishes in proportion to\nthe square-root (constant) step size. Under the same setting, however, we prove\nthat when augmented with iteration averaging, SEG provably converges to the\nNash equilibrium, and such a rate is provably accelerated by incorporating a\nscheduled restarting procedure. In the interpolation setting, we achieve an\noptimal convergence rate up to tight constants. We present numerical\nexperiments that validate our theoretical findings and demonstrate the\neffectiveness of the SEG method when equipped with iteration averaging and\nrestarting.",
          "link": "http://arxiv.org/abs/2107.00464",
          "publishedOn": "2021-07-02T01:58:02.428Z",
          "wordCount": 623,
          "title": "On the Convergence of Stochastic Extragradient for Bilinear Games with Restarted Iteration Averaging. (arXiv:2107.00464v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00284",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1\">Kai Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yuyang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Gang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_B/0/1/0/all/0/1\">Bei Peng</a>",
          "description": "Cooperative problems under continuous control have always been the focus of\nmulti-agent reinforcement learning. Existing algorithms suffer from the problem\nof uneven learning degree with the increase of the number of agents. In this\npaper, a new structure for a multi-agent actor critic is proposed, and the\nself-attention mechanism is applied in the critic network and the value\ndecomposition method used to solve the uneven problem. The proposed algorithm\nmakes full use of the samples in the replay memory buffer to learn the behavior\nof a class of agents. First, a new update method is proposed for policy\nnetworks that promotes learning efficiency. Second, the utilization of samples\nis improved, at the same time reflecting the ability of perspective-taking\namong groups. Finally, the \"deceptive signal\" in training is eliminated and the\nlearning degree among agents is more uniform than in the existing methods.\nMultiple experiments were conducted in two typical scenarios of a multi-agent\nparticle environment. Experimental results show that the proposed algorithm can\nperform better than the state-of-the-art ones, and that it exhibits higher\nlearning efficiency with an increasing number of agents.",
          "link": "http://arxiv.org/abs/2107.00284",
          "publishedOn": "2021-07-02T01:58:02.408Z",
          "wordCount": 622,
          "title": "SA-MATD3:Self-attention-based multi-agent continuous control method in cooperative environments. (arXiv:2107.00284v1 [cs.MA])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08126",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Arabskyy_Y/0/1/0/all/0/1\">Yuriy Arabskyy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Agarwal_A/0/1/0/all/0/1\">Aashish Agarwal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dey_S/0/1/0/all/0/1\">Subhadeep Dey</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Koller_O/0/1/0/all/0/1\">Oscar Koller</a>",
          "description": "This paper describes the winning approach in the Shared Task 3 at SwissText\n2021 on Swiss German Speech to Standard German Text, a public competition on\ndialect recognition and translation. Swiss German refers to the multitude of\nAlemannic dialects spoken in the German-speaking parts of Switzerland. Swiss\nGerman differs significantly from standard German in pronunciation, word\ninventory and grammar. It is mostly incomprehensible to native German speakers.\nMoreover, it lacks a standardized written script. To solve the challenging\ntask, we propose a hybrid automatic speech recognition system with a lexicon\nthat incorporates translations, a 1st pass language model that deals with Swiss\nGerman particularities, a transfer-learned acoustic model and a strong neural\nlanguage model for 2nd pass rescoring. Our submission reaches 46.04% BLEU on a\nblind conversational test set and outperforms the second best competitor by a\n12% relative margin.",
          "link": "http://arxiv.org/abs/2106.08126",
          "publishedOn": "2021-07-02T01:58:02.401Z",
          "wordCount": 633,
          "title": "Dialectal Speech Recognition and Translation of Swiss German Speech to Standard German Text: Microsoft's Submission to SwissText 2021. (arXiv:2106.08126v2 [eess.AS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.01433",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Parada_Mayorga_A/0/1/0/all/0/1\">Alejandro Parada-Mayorga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ribeiro_A/0/1/0/all/0/1\">Alejandro Ribeiro</a>",
          "description": "We study algebraic neural networks (AlgNNs) with commutative algebras which\nunify diverse architectures such as Euclidean convolutional neural networks,\ngraph neural networks, and group neural networks under the umbrella of\nalgebraic signal processing. An AlgNN is a stacked layered information\nprocessing structure where each layer is conformed by an algebra, a vector\nspace and a homomorphism between the algebra and the space of endomorphisms of\nthe vector space. Signals are modeled as elements of the vector space and are\nprocessed by convolutional filters that are defined as the images of the\nelements of the algebra under the action of the homomorphism. We analyze\nstability of algebraic filters and AlgNNs to deformations of the homomorphism\nand derive conditions on filters that lead to Lipschitz stable operators. We\nconclude that stable algebraic filters have frequency responses -- defined as\neigenvalue domain representations -- whose derivative is inversely proportional\nto the frequency -- defined as eigenvalue magnitudes. It follows that for a\ngiven level of discriminability, AlgNNs are more stable than algebraic filters,\nthereby explaining their better empirical performance. This same phenomenon has\nbeen proven for Euclidean convolutional neural networks and graph neural\nnetworks. Our analysis shows that this is a deep algebraic property shared by a\nnumber of architectures.",
          "link": "http://arxiv.org/abs/2009.01433",
          "publishedOn": "2021-07-02T01:58:02.392Z",
          "wordCount": 688,
          "title": "Algebraic Neural Networks: Stability to Deformations. (arXiv:2009.01433v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00401",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Viale_A/0/1/0/all/0/1\">Alberto Viale</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marchisio_A/0/1/0/all/0/1\">Alberto Marchisio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martina_M/0/1/0/all/0/1\">Maurizio Martina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masera_G/0/1/0/all/0/1\">Guido Masera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shafique_M/0/1/0/all/0/1\">Muhammad Shafique</a>",
          "description": "Autonomous Driving (AD) related features provide new forms of mobility that\nare also beneficial for other kind of intelligent and autonomous systems like\nrobots, smart transportation, and smart industries. For these applications, the\ndecisions need to be made fast and in real-time. Moreover, in the quest for\nelectric mobility, this task must follow low power policy, without affecting\nmuch the autonomy of the mean of transport or the robot. These two challenges\ncan be tackled using the emerging Spiking Neural Networks (SNNs). When deployed\non a specialized neuromorphic hardware, SNNs can achieve high performance with\nlow latency and low power consumption. In this paper, we use an SNN connected\nto an event-based camera for facing one of the key problems for AD, i.e., the\nclassification between cars and other objects. To consume less power than\ntraditional frame-based cameras, we use a Dynamic Vision Sensor (DVS). The\nexperiments are made following an offline supervised learning rule, followed by\nmapping the learnt SNN model on the Intel Loihi Neuromorphic Research Chip. Our\nbest experiment achieves an accuracy on offline implementation of 86%, that\ndrops to 83% when it is ported onto the Loihi Chip. The Neuromorphic Hardware\nimplementation has maximum 0.72 ms of latency for every sample, and consumes\nonly 310 mW. To the best of our knowledge, this work is the first\nimplementation of an event-based car classifier on a Neuromorphic Chip.",
          "link": "http://arxiv.org/abs/2107.00401",
          "publishedOn": "2021-07-02T01:58:02.385Z",
          "wordCount": 694,
          "title": "CarSNN: An Efficient Spiking Neural Network for Event-Based Autonomous Cars on the Loihi Neuromorphic Research Processor. (arXiv:2107.00401v1 [cs.NE])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00643",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mayee Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goel_K/0/1/0/all/0/1\">Karan Goel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sohoni_N/0/1/0/all/0/1\">Nimit Sohoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poms_F/0/1/0/all/0/1\">Fait Poms</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fatahalian_K/0/1/0/all/0/1\">Kayvon Fatahalian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Re_C/0/1/0/all/0/1\">Christopher R&#xe9;</a>",
          "description": "Machine learning models are often deployed in different settings than they\nwere trained and validated on, posing a challenge to practitioners who wish to\npredict how well the deployed model will perform on a target distribution. If\nan unlabeled sample from the target distribution is available, along with a\nlabeled sample from a possibly different source distribution, standard\napproaches such as importance weighting can be applied to estimate performance\non the target. However, importance weighting struggles when the source and\ntarget distributions have non-overlapping support or are high-dimensional.\nTaking inspiration from fields such as epidemiology and polling, we develop\nMandoline, a new evaluation framework that mitigates these issues. Our key\ninsight is that practitioners may have prior knowledge about the ways in which\nthe distribution shifts, which we can use to better guide the importance\nweighting procedure. Specifically, users write simple \"slicing functions\" -\nnoisy, potentially correlated binary functions intended to capture possible\naxes of distribution shift - to compute reweighted performance estimates. We\nfurther describe a density ratio estimation framework for the slices and show\nhow its estimation error scales with slice quality and dataset size. Empirical\nvalidation on NLP and vision tasks shows that \\name can estimate performance on\nthe target distribution up to $3\\times$ more accurately compared to standard\nbaselines.",
          "link": "http://arxiv.org/abs/2107.00643",
          "publishedOn": "2021-07-02T01:58:02.366Z",
          "wordCount": 654,
          "title": "Mandoline: Model Evaluation under Distribution Shift. (arXiv:2107.00643v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00379",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Tseran_H/0/1/0/all/0/1\">Hanna Tseran</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Montufar_G/0/1/0/all/0/1\">Guido Mont&#xfa;far</a>",
          "description": "Learning with neural networks relies on the complexity of the representable\nfunctions, but more importantly, the particular assignment of typical\nparameters to functions of different complexity. Taking the number of\nactivation regions as a complexity measure, recent works have shown that the\npractical complexity of deep ReLU networks is often far from the theoretical\nmaximum. In this work we show that this phenomenon also occurs in networks with\nmaxout (multi-argument) activation functions and when considering the decision\nboundaries in classification tasks. We also show that the parameter space has a\nmultitude of full-dimensional regions with widely different complexity, and\nobtain nontrivial lower bounds on the expected complexity. Finally, we\ninvestigate different parameter initialization procedures and show that they\ncan increase the speed of convergence in training.",
          "link": "http://arxiv.org/abs/2107.00379",
          "publishedOn": "2021-07-02T01:58:02.358Z",
          "wordCount": 559,
          "title": "On the Expected Complexity of Maxout Networks. (arXiv:2107.00379v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00571",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gillot_P/0/1/0/all/0/1\">Pierre Gillot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parviainen_P/0/1/0/all/0/1\">Pekka Parviainen</a>",
          "description": "Bayesian networks represent relations between variables using a directed\nacyclic graph (DAG). Learning the DAG is an NP-hard problem and exact learning\nalgorithms are feasible only for small sets of variables. We propose two\nscalable heuristics for learning DAGs in the linear structural equation case.\nOur methods learn the DAG by alternating between unconstrained gradient\ndescent-based step to optimize an objective function and solving a maximum\nacyclic subgraph problem to enforce acyclicity. Thanks to this decoupling, our\nmethods scale up beyond thousands of variables.",
          "link": "http://arxiv.org/abs/2107.00571",
          "publishedOn": "2021-07-02T01:58:02.351Z",
          "wordCount": 518,
          "title": "Learning Large DAGs by Combining Continuous Optimization and Feedback Arc Set Heuristics. (arXiv:2107.00571v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00364",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Littwin_E/0/1/0/all/0/1\">Etai Littwin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saremi_O/0/1/0/all/0/1\">Omid Saremi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_S/0/1/0/all/0/1\">Shuangfei Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thilak_V/0/1/0/all/0/1\">Vimal Thilak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goh_H/0/1/0/all/0/1\">Hanlin Goh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Susskind_J/0/1/0/all/0/1\">Joshua M. Susskind</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1\">Greg Yang</a>",
          "description": "We analyze the learning dynamics of infinitely wide neural networks with a\nfinite sized bottle-neck. Unlike the neural tangent kernel limit, a bottleneck\nin an otherwise infinite width network al-lows data dependent feature learning\nin its bottle-neck representation. We empirically show that a single bottleneck\nin infinite networks dramatically accelerates training when compared to purely\nin-finite networks, with an improved overall performance. We discuss the\nacceleration phenomena by drawing similarities to infinitely wide deep linear\nmodels, where the acceleration effect of a bottleneck can be understood\ntheoretically.",
          "link": "http://arxiv.org/abs/2107.00364",
          "publishedOn": "2021-07-02T01:58:02.344Z",
          "wordCount": 529,
          "title": "Implicit Acceleration and Feature Learning inInfinitely Wide Neural Networks with Bottlenecks. (arXiv:2107.00364v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00623",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fonseca_E/0/1/0/all/0/1\">Eduardo Fonseca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferraro_A/0/1/0/all/0/1\">Andres Ferraro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Serra_X/0/1/0/all/0/1\">Xavier Serra</a>",
          "description": "Recent studies have put into question the commonly assumed shift invariance\nproperty of convolutional networks, showing that small shifts in the input can\naffect the output predictions substantially. In this paper, we ask whether lack\nof shift invariance is a problem in sound event classification, and whether\nthere are benefits in addressing it. Specifically, we evaluate two pooling\nmethods to improve shift invariance in CNNs, based on low-pass filtering and\nadaptive sampling of incoming feature maps. These methods are implemented via\nsmall architectural modifications inserted into the pooling layers of CNNs. We\nevaluate the effect of these architectural changes on the FSD50K dataset using\nmodels of different capacity and in presence of strong regularization. We show\nthat these modifications consistently improve sound event classification in all\ncases considered, without adding any (or adding very few) trainable parameters,\nwhich makes them an appealing alternative to conventional pooling layers. The\noutcome is a new state-of-the-art mAP of 0.541 on the FSD50K classification\nbenchmark.",
          "link": "http://arxiv.org/abs/2107.00623",
          "publishedOn": "2021-07-02T01:58:02.337Z",
          "wordCount": 606,
          "title": "Improving Sound Event Classification by Increasing Shift Invariance in Convolutional Neural Networks. (arXiv:2107.00623v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00323",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pezeshkpour_P/0/1/0/all/0/1\">Pouya Pezeshkpour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Sarthak Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sameer Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wallace_B/0/1/0/all/0/1\">Byron C. Wallace</a>",
          "description": "Training the large deep neural networks that dominate NLP requires large\ndatasets. Many of these are collected automatically or via crowdsourcing, and\nmay exhibit systematic biases or annotation artifacts. By the latter, we mean\ncorrelations between inputs and outputs that are spurious, insofar as they do\nnot represent a generally held causal relationship between features and\nclasses; models that exploit such correlations may appear to perform a given\ntask well, but fail on out of sample data. In this paper we propose methods to\nfacilitate identification of training data artifacts, using new hybrid\napproaches that combine saliency maps (which highlight important input\nfeatures) with instance attribution methods (which retrieve training samples\ninfluential to a given prediction). We show that this proposed training-feature\nattribution approach can be used to uncover artifacts in training data, and use\nit to identify previously unreported artifacts in a few standard NLP datasets.\nWe execute a small user study to evaluate whether these methods are useful to\nNLP researchers in practice, with promising results. We make code for all\nmethods and experiments in this paper available.",
          "link": "http://arxiv.org/abs/2107.00323",
          "publishedOn": "2021-07-02T01:58:02.302Z",
          "wordCount": 617,
          "title": "Combining Feature and Instance Attribution to Detect Artifacts. (arXiv:2107.00323v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2003.08089",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Whang_J/0/1/0/all/0/1\">Jay Whang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_Q/0/1/0/all/0/1\">Qi Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dimakis_A/0/1/0/all/0/1\">Alexandros G. Dimakis</a>",
          "description": "We study image inverse problems with a normalizing flow prior. Our\nformulation views the solution as the maximum a posteriori estimate of the\nimage conditioned on the measurements. This formulation allows us to use noise\nmodels with arbitrary dependencies as well as non-linear forward operators. We\nempirically validate the efficacy of our method on various inverse problems,\nincluding compressed sensing with quantized measurements and denoising with\nhighly structured noise patterns. We also present initial theoretical recovery\nguarantees for solving inverse problems with a flow prior.",
          "link": "http://arxiv.org/abs/2003.08089",
          "publishedOn": "2021-07-02T01:58:02.282Z",
          "wordCount": 563,
          "title": "Solving Inverse Problems with a Flow-based Noise Model. (arXiv:2003.08089v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00645",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rao_Y/0/1/0/all/0/1\">Yongming Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1\">Wenliang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zheng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jiwen Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>",
          "description": "Recent advances in self-attention and pure multi-layer perceptrons (MLP)\nmodels for vision have shown great potential in achieving promising performance\nwith fewer inductive biases. These models are generally based on learning\ninteraction among spatial locations from raw data. The complexity of\nself-attention and MLP grows quadratically as the image size increases, which\nmakes these models hard to scale up when high-resolution features are required.\nIn this paper, we present the Global Filter Network (GFNet), a conceptually\nsimple yet computationally efficient architecture, that learns long-term\nspatial dependencies in the frequency domain with log-linear complexity. Our\narchitecture replaces the self-attention layer in vision transformers with\nthree key operations: a 2D discrete Fourier transform, an element-wise\nmultiplication between frequency-domain features and learnable global filters,\nand a 2D inverse Fourier transform. We exhibit favorable accuracy/complexity\ntrade-offs of our models on both ImageNet and downstream tasks. Our results\ndemonstrate that GFNet can be a very competitive alternative to\ntransformer-style models and CNNs in efficiency, generalization ability and\nrobustness. Code is available at https://github.com/raoyongming/GFNet",
          "link": "http://arxiv.org/abs/2107.00645",
          "publishedOn": "2021-07-02T01:58:02.276Z",
          "wordCount": 616,
          "title": "Global Filter Networks for Image Classification. (arXiv:2107.00645v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.15722",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liang_C/0/1/0/all/0/1\">Chengdong Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Menglong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiao-Lei Zhang</a>",
          "description": "Self-attention (SA), which encodes vector sequences according to their\npairwise similarity, is widely used in speech recognition due to its strong\ncontext modeling ability. However, when applied to long sequence data, its\naccuracy is reduced. This is caused by the fact that its weighted average\noperator may lead to the dispersion of the attention distribution, which\nresults in the relationship between adjacent signals ignored. To address this\nissue, in this paper, we introduce relative-position-awareness self-attention\n(RPSA). It not only maintains the global-range dependency modeling ability of\nself-attention, but also improves the localness modeling ability. Because the\nlocal window length of the original RPSA is fixed and sensitive to different\ntest data, here we propose Gaussian-based self-attention (GSA) whose window\nlength is learnable and adaptive to the test data automatically. We further\ngeneralize GSA to a new residual Gaussian self-attention (resGSA) for the\nperformance improvement. We apply RPSA, GSA, and resGSA to Transformer-based\nspeech recognition respectively. Experimental results on the AISHELL-1 Mandarin\nspeech recognition corpus demonstrate the effectiveness of the proposed\nmethods. For example, the resGSA-Transformer achieves a character error rate\n(CER) of 5.86% on the test set, which is relative 7.8% lower than that of the\nSA-Transformer. Although the performance of the proposed resGSA-Transformer is\nonly slightly better than that of the RPSA-Transformer, it does not have to\ntune the window length manually.",
          "link": "http://arxiv.org/abs/2103.15722",
          "publishedOn": "2021-07-02T01:58:02.269Z",
          "wordCount": 705,
          "title": "Transformer-based end-to-end speech recognition with residual Gaussian-based self-attention. (arXiv:2103.15722v3 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00110",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Asai_M/0/1/0/all/0/1\">Masataro Asai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kajino_H/0/1/0/all/0/1\">Hiroshi Kajino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fukunaga_A/0/1/0/all/0/1\">Alex Fukunaga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muise_C/0/1/0/all/0/1\">Christian Muise</a>",
          "description": "Current domain-independent, classical planners require symbolic models of the\nproblem domain and instance as input, resulting in a knowledge acquisition\nbottleneck. Meanwhile, although deep learning has achieved significant success\nin many fields, the knowledge is encoded in a subsymbolic representation which\nis incompatible with symbolic systems such as planners. We propose Latplan, an\nunsupervised architecture combining deep learning and classical planning. Given\nonly an unlabeled set of image pairs showing a subset of transitions allowed in\nthe environment (training inputs), Latplan learns a complete propositional PDDL\naction model of the environment. Later, when a pair of images representing the\ninitial and the goal states (planning inputs) is given, Latplan finds a plan to\nthe goal state in a symbolic latent space and returns a visualized plan\nexecution. We evaluate Latplan using image-based versions of 6 planning\ndomains: 8-puzzle, 15-Puzzle, Blocksworld, Sokoban and Two variations of\nLightsOut.",
          "link": "http://arxiv.org/abs/2107.00110",
          "publishedOn": "2021-07-02T01:58:02.250Z",
          "wordCount": 586,
          "title": "Classical Planning in Deep Latent Space. (arXiv:2107.00110v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00400",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Nguyen_D/0/1/0/all/0/1\">Dat Thanh Nguyen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Quach_M/0/1/0/all/0/1\">Maurice Quach</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Valenzise_G/0/1/0/all/0/1\">Giuseppe Valenzise</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Duhamel_P/0/1/0/all/0/1\">Pierre Duhamel</a>",
          "description": "This paper proposes a lossless point cloud (PC) geometry compression method\nthat uses neural networks to estimate the probability distribution of voxel\noccupancy. First, to take into account the PC sparsity, our method adaptively\npartitions a point cloud into multiple voxel block sizes. This partitioning is\nsignalled via an octree. Second, we employ a deep auto-regressive generative\nmodel to estimate the occupancy probability of each voxel given the previously\nencoded ones. We then employ the estimated probabilities to code efficiently a\nblock using a context-based arithmetic coder. Our context has variable size and\ncan expand beyond the current block to learn more accurate probabilities. We\nalso consider using data augmentation techniques to increase the generalization\ncapability of the learned probability models, in particular in the presence of\nnoise and lower-density point clouds. Experimental evaluation, performed on a\nvariety of point clouds from four different datasets and with diverse\ncharacteristics, demonstrates that our method reduces significantly (by up to\n30%) the rate for lossless coding compared to the state-of-the-art MPEG codec.",
          "link": "http://arxiv.org/abs/2107.00400",
          "publishedOn": "2021-07-02T01:58:02.242Z",
          "wordCount": 652,
          "title": "Lossless Coding of Point Cloud Geometry using a Deep Generative Model. (arXiv:2107.00400v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00451",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Koot_R/0/1/0/all/0/1\">Raivo Koot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Haiping Lu</a>",
          "description": "Efficient video action recognition remains a challenging problem. One large\nmodel after another takes the place of the state-of-the-art on the Kinetics\ndataset, but real-world efficiency evaluations are often lacking. In this work,\nwe fill this gap and investigate the use of transformers for efficient action\nrecognition. We propose a novel, lightweight action recognition architecture,\nVideoLightFormer. In a factorized fashion, we carefully extend the 2D\nconvolutional Temporal Segment Network with transformers, while maintaining\nspatial and temporal video structure throughout the entire model. Existing\nmethods often resort to one of the two extremes, where they either apply huge\ntransformers to video features, or minimal transformers on highly pooled video\nfeatures. Our method differs from them by keeping the transformer models small,\nbut leveraging full spatiotemporal feature structure. We evaluate\nVideoLightFormer in a high-efficiency setting on the temporally-demanding\nEPIC-KITCHENS-100 and Something-Something-V2 (SSV2) datasets and find that it\nachieves a better mix of efficiency and accuracy than existing state-of-the-art\nmodels, apart from the Temporal Shift Module on SSV2.",
          "link": "http://arxiv.org/abs/2107.00451",
          "publishedOn": "2021-07-02T01:58:02.235Z",
          "wordCount": 597,
          "title": "VideoLightFormer: Lightweight Action Recognition using Transformers. (arXiv:2107.00451v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00306",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1\">Rui Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_M/0/1/0/all/0/1\">Meng Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_L/0/1/0/all/0/1\">Lei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yali Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_F/0/1/0/all/0/1\">Feng Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiu Li</a>",
          "description": "Solving multi-goal reinforcement learning (RL) problems with sparse rewards\nis generally challenging. Existing approaches have utilized goal relabeling on\ncollected experiences to alleviate issues raised from sparse rewards. However,\nthese methods are still limited in efficiency and cannot make full use of\nexperiences. In this paper, we propose Model-based Hindsight Experience Replay\n(MHER), which exploits experiences more efficiently by leveraging environmental\ndynamics to generate virtual achieved goals. Replacing original goals with\nvirtual goals generated from interaction with a trained dynamics model leads to\na novel relabeling method, \\emph{model-based relabeling} (MBR). Based on MBR,\nMHER performs both reinforcement learning and supervised learning for efficient\npolicy improvement. Theoretically, we also prove the supervised part in MHER,\ni.e., goal-conditioned supervised learning with MBR data, optimizes a lower\nbound on the multi-goal RL objective. Experimental results in several\npoint-based tasks and simulated robotics environments show that MHER achieves\nsignificantly higher sample efficiency than previous state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2107.00306",
          "publishedOn": "2021-07-02T01:58:02.229Z",
          "wordCount": 592,
          "title": "MHER: Model-based Hindsight Experience Replay. (arXiv:2107.00306v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00462",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wurster_S/0/1/0/all/0/1\">Skylar W. Wurster</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shen_H/0/1/0/all/0/1\">Han-Wei Shen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Guo_H/0/1/0/all/0/1\">Hanqi Guo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Peterka_T/0/1/0/all/0/1\">Thomas Peterka</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Raj_M/0/1/0/all/0/1\">Mukund Raj</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_J/0/1/0/all/0/1\">Jiayi Xu</a>",
          "description": "We present an approach for hierarchical super resolution (SR) using neural\nnetworks on an octree data representation. We train a hierarchy of neural\nnetworks, each capable of 2x upscaling in each spatial dimension between two\nlevels of detail, and use these networks in tandem to facilitate large scale\nfactor super resolution, scaling with the number of trained networks. We\nutilize these networks in a hierarchical super resolution algorithm that\nupscales multiresolution data to a uniform high resolution without introducing\nseam artifacts on octree node boundaries. We evaluate application of this\nalgorithm in a data reduction framework by dynamically downscaling input data\nto an octree-based data structure to represent the multiresolution data before\ncompressing for additional storage reduction. We demonstrate that our approach\navoids seam artifacts common to multiresolution data formats, and show how\nneural network super resolution assisted data reduction can preserve global\nfeatures better than compressors alone at the same compression ratios.",
          "link": "http://arxiv.org/abs/2107.00462",
          "publishedOn": "2021-07-02T01:58:02.213Z",
          "wordCount": 609,
          "title": "Deep Hierarchical Super-Resolution for Scientific Data Reduction and Visualization. (arXiv:2107.00462v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2009.12064",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kitada_S/0/1/0/all/0/1\">Shunsuke Kitada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyatomi_H/0/1/0/all/0/1\">Hitoshi Iyatomi</a>",
          "description": "Although attention mechanisms have been applied to a variety of deep learning\nmodels and have been shown to improve the prediction performance, it has been\nreported to be vulnerable to perturbations to the mechanism. To overcome the\nvulnerability to perturbations in the mechanism, we are inspired by adversarial\ntraining (AT), which is a powerful regularization technique for enhancing the\nrobustness of the models. In this paper, we propose a general training\ntechnique for natural language processing tasks, including AT for attention\n(Attention AT) and more interpretable AT for attention (Attention iAT). The\nproposed techniques improved the prediction performance and the model\ninterpretability by exploiting the mechanisms with AT. In particular, Attention\niAT boosts those advantages by introducing adversarial perturbation, which\nenhances the difference in the attention of the sentences. Evaluation\nexperiments with ten open datasets revealed that AT for attention mechanisms,\nespecially Attention iAT, demonstrated (1) the best performance in nine out of\nten tasks and (2) more interpretable attention (i.e., the resulting attention\ncorrelated more strongly with gradient-based word importance) for all tasks.\nAdditionally, the proposed techniques are (3) much less dependent on\nperturbation size in AT. Our code is available at\nhttps://github.com/shunk031/attention-meets-perturbation",
          "link": "http://arxiv.org/abs/2009.12064",
          "publishedOn": "2021-07-02T01:58:02.206Z",
          "wordCount": 683,
          "title": "Attention Meets Perturbations: Robust and Interpretable Attention with Adversarial Training. (arXiv:2009.12064v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00315",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Varshney_N/0/1/0/all/0/1\">Neeraj Varshney</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1\">Swaroop Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1\">Chitta Baral</a>",
          "description": "Standard NLP tasks do not incorporate several common real-world scenarios\nsuch as seeking clarifications about the question, taking advantage of clues,\nabstaining in order to avoid incorrect answers, etc. This difference in task\nformulation hinders the adoption of NLP systems in real-world settings. In this\nwork, we take a step towards bridging this gap and present a multi-stage task\nthat simulates a typical human-human questioner-responder interaction such as\nan interview. Specifically, the system is provided with question\nsimplifications, knowledge statements, examples, etc. at various stages to\nimprove its prediction when it is not sufficiently confident. We instantiate\nthe proposed task in Natural Language Inference setting where a system is\nevaluated on both in-domain and out-of-domain (OOD) inputs. We conduct\ncomprehensive experiments and find that the multi-stage formulation of our task\nleads to OOD generalization performance improvement up to 2.29% in Stage 1,\n1.91% in Stage 2, 54.88% in Stage 3, and 72.02% in Stage 4 over the standard\nunguided prediction. However, our task leaves a significant challenge for NLP\nresearchers to further improve OOD performance at each stage.",
          "link": "http://arxiv.org/abs/2107.00315",
          "publishedOn": "2021-07-02T01:58:02.197Z",
          "wordCount": 626,
          "title": "Interviewer-Candidate Role Play: Towards Developing Real-World NLP Systems. (arXiv:2107.00315v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00309",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Haibin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_P/0/1/0/all/0/1\">Po-chun Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Ji Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shanshan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shen Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_J/0/1/0/all/0/1\">Jian Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhiyong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_H/0/1/0/all/0/1\">Helen Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>",
          "description": "Automatic speaker verification (ASV), one of the most important technology\nfor biometric identification, has been widely adopted in security-critic\napplications, including transaction authentication and access control. However,\nprevious works have shown ASV is seriously vulnerable to recently emerged\nadversarial attacks, yet effective countermeasures against them are limited. In\nthis paper, we adopt neural vocoders to spot adversarial samples for ASV. We\nuse neural vocoder to re-synthesize audio and find that the difference between\nthe ASV scores for the original and re-synthesized audio is a good indicator to\ndistinguish genuine and adversarial samples. As the very beginning work in this\ndirection of detecting adversarial samples for ASV, there is no reliable\nbaseline for comparison. So we first implement Griffin-Lim for detection and\nset it as our baseline. The proposed method accomplishes effective detection\nperformance and outperforms all the baselines in all the settings. We also show\nthe neural vocoder adopted in the detection framework is dataset independent.\nOur codes will be made open-source for future works to do comparison.",
          "link": "http://arxiv.org/abs/2107.00309",
          "publishedOn": "2021-07-02T01:58:02.190Z",
          "wordCount": 623,
          "title": "Spotting adversarial samples for speaker verification by neural vocoders. (arXiv:2107.00309v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2003.00937",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yi-Rui Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wu-Jun Li</a>",
          "description": "Distributed learning has become a hot research topic, due to its wide\napplication in cluster-based large-scale learning, federated learning, edge\ncomputing and so on. Most distributed learning methods assume no error and\nattack on the workers. However, many unexpected cases, such as communication\nerror and even malicious attack, may happen in real applications. Hence,\nByzantine learning (BL), which refers to distributed learning with attack or\nerror, has recently attracted much attention. Most existing BL methods are\nsynchronous, which will result in slow convergence when there exist\nheterogeneous workers. Furthermore, in some applications like federated\nlearning and edge computing, synchronization cannot even be performed most of\nthe time due to the online workers (clients or edge servers). Hence,\nasynchronous BL (ABL) is more general and practical than synchronous BL (SBL).\nTo the best of our knowledge, there exist only two ABL methods. One of them\ncannot resist malicious attack. The other needs to store some training\ninstances on the server, which has the privacy leak problem. In this paper, we\npropose a novel method, called buffered asynchronous stochastic gradient\ndescent (BASGD), for BL. BASGD is an asynchronous method. Furthermore, BASGD\nhas no need to store any training instances on the server, and hence can\npreserve privacy in ABL. BASGD is theoretically proved to have the ability of\nresisting against error and malicious attack. Moreover, BASGD has a similar\ntheoretical convergence rate to that of vanilla asynchronous SGD (ASGD), with\nan extra constant variance. Empirical results show that BASGD can significantly\noutperform vanilla ASGD and other ABL baselines, when there exists error or\nattack on workers.",
          "link": "http://arxiv.org/abs/2003.00937",
          "publishedOn": "2021-07-02T01:58:02.170Z",
          "wordCount": 728,
          "title": "BASGD: Buffered Asynchronous SGD for Byzantine Learning. (arXiv:2003.00937v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00385",
          "author": "<a href=\"http://arxiv.org/find/astro-ph/1/au:+Maslej_Kresnakova_V/0/1/0/all/0/1\">Viera Maslej-Kre&#x161;&#x148;&#xe1;kov&#xe1;</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Bouchefry_K/0/1/0/all/0/1\">Khadija El Bouchefry</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Butka_P/0/1/0/all/0/1\">Peter Butka</a>",
          "description": "Machine learning techniques have been increasingly used in astronomical\napplications and have proven to successfully classify objects in image data\nwith high accuracy. The current work uses archival data from the Faint Images\nof the Radio Sky at Twenty Centimeters (FIRST) to classify radio galaxies into\nfour classes: Fanaroff-Riley Class I (FRI), Fanaroff-Riley Class II (FRII),\nBent-Tailed (BENT), and Compact (COMPT). The model presented in this work is\nbased on Convolutional Neural Networks (CNNs). The proposed architecture\ncomprises three parallel blocks of convolutional layers combined and processed\nfor final classification by two feed-forward layers. Our model classified\nselected classes of radio galaxy sources on an independent testing subset with\nan average of 96\\% for precision, recall, and F1 score. The best selected\naugmentation techniques were rotations, horizontal or vertical flips, and\nincrease of brightness. Shifts, zoom and decrease of brightness worsened the\nperformance of the model. The current results show that model developed in this\nwork is able to identify different morphological classes of radio galaxies with\na high efficiency and performance",
          "link": "http://arxiv.org/abs/2107.00385",
          "publishedOn": "2021-07-02T01:58:02.152Z",
          "wordCount": 663,
          "title": "Morphological classification of compact and extended radio galaxies using convolutional neural networks and data augmentation techniques. (arXiv:2107.00385v1 [astro-ph.GA])"
        },
        {
          "id": "http://arxiv.org/abs/2106.01708",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_B/0/1/0/all/0/1\">Buliao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yunhui Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Usman_M/0/1/0/all/0/1\">Muhammad Usman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huanhuan Chen</a>",
          "description": "Incomplete instances with various missing attributes in many real-world\napplications have brought challenges to the classification tasks. Missing\nvalues imputation methods are often employed to replace the missing values with\nsubstitute values. However, this process often separates the imputation and\nclassification, which may lead to inferior performance since label information\nare often ignored during imputation. Moreover, traditional methods may rely on\nimproper assumptions to initialize the missing values, whereas the\nunreliability of such initialization might lead to inferior performance. To\naddress these problems, a novel semi-supervised conditional normalizing flow\n(SSCFlow) is proposed in this paper. SSCFlow explicitly utilizes the label\ninformation to facilitate the imputation and classification simultaneously by\nestimating the conditional distribution of incomplete instances with a novel\nsemi-supervised normalizing flow. Moreover, SSCFlow treats the initialized\nmissing values as corrupted initial imputation and iteratively reconstructs\ntheir latent representations with an overcomplete denoising autoencoder to\napproximate their true conditional distribution. Experiments on real-world\ndatasets demonstrate the robustness and effectiveness of the proposed\nalgorithm.",
          "link": "http://arxiv.org/abs/2106.01708",
          "publishedOn": "2021-07-02T01:58:02.123Z",
          "wordCount": 609,
          "title": "Semi-supervised Learning with Missing Values Imputation. (arXiv:2106.01708v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00371",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Gao_S/0/1/0/all/0/1\">Sheng Gao</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ma_Z/0/1/0/all/0/1\">Zongming Ma</a>",
          "description": "Generalized correlation analysis (GCA) is concerned with uncovering linear\nrelationships across multiple datasets. It generalizes canonical correlation\nanalysis that is designed for two datasets. We study sparse GCA when there are\npotentially multiple generalized correlation tuples in data and the loading\nmatrix has a small number of nonzero rows. It includes sparse CCA and sparse\nPCA of correlation matrices as special cases. We first formulate sparse GCA as\ngeneralized eigenvalue problems at both population and sample levels via a\ncareful choice of normalization constraints. Based on a Lagrangian form of the\nsample optimization problem, we propose a thresholded gradient descent\nalgorithm for estimating GCA loading vectors and matrices in high dimensions.\nWe derive tight estimation error bounds for estimators generated by the\nalgorithm with proper initialization. We also demonstrate the prowess of the\nalgorithm on a number of synthetic datasets.",
          "link": "http://arxiv.org/abs/2107.00371",
          "publishedOn": "2021-07-02T01:58:02.115Z",
          "wordCount": 565,
          "title": "Sparse GCA and Thresholded Gradient Descent. (arXiv:2107.00371v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/1905.02515",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Puolamaki_K/0/1/0/all/0/1\">Kai Puolam&#xe4;ki</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Oikarinen_E/0/1/0/all/0/1\">Emilia Oikarinen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Henelius_A/0/1/0/all/0/1\">Andreas Henelius</a>",
          "description": "Efficient explorative data analysis systems must take into account both what\na user knows and wants to know. This paper proposes a principled framework for\ninteractive visual exploration of relations in data, through views most\ninformative given the user's current knowledge and objectives. The user can\ninput pre-existing knowledge of relations in the data and also formulate\nspecific exploration interests, which are then taken into account in the\nexploration. The idea is to steer the exploration process towards the interests\nof the user, instead of showing uninteresting or already known relations. The\nuser's knowledge is modelled by a distribution over data sets parametrised by\nsubsets of rows and columns of data, called tile constraints. We provide a\ncomputationally efficient implementation of this concept based on constrained\nrandomisation. Furthermore, we describe a novel dimensionality reduction method\nfor finding the views most informative to the user, which at the limit of no\nbackground knowledge and with generic objectives reduces to PCA. We show that\nthe method is suitable for interactive use and is robust to noise, outperforms\nstandard projection pursuit visualisation methods, and gives understandable and\nuseful results in analysis of real-world data. We provide an open-source\nimplementation of the framework.",
          "link": "http://arxiv.org/abs/1905.02515",
          "publishedOn": "2021-07-02T01:58:02.108Z",
          "wordCount": 676,
          "title": "Guided Visual Exploration of Relations in Data Sets. (arXiv:1905.02515v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00469",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Amir_I/0/1/0/all/0/1\">Idan Amir</a>, <a href=\"http://arxiv.org/find/math/1/au:+Carmon_Y/0/1/0/all/0/1\">Yair Carmon</a>, <a href=\"http://arxiv.org/find/math/1/au:+Koren_T/0/1/0/all/0/1\">Tomer Koren</a>, <a href=\"http://arxiv.org/find/math/1/au:+Livni_R/0/1/0/all/0/1\">Roi Livni</a>",
          "description": "We study the generalization performance of $\\text{full-batch}$ optimization\nalgorithms for stochastic convex optimization: these are first-order methods\nthat only access the exact gradient of the empirical risk (rather than\ngradients with respect to individual data points), that include a wide range of\nalgorithms such as gradient descent, mirror descent, and their regularized\nand/or accelerated variants. We provide a new separation result showing that,\nwhile algorithms such as stochastic gradient descent can generalize and\noptimize the population risk to within $\\epsilon$ after $O(1/\\epsilon^2)$\niterations, full-batch methods either need at least $\\Omega(1/\\epsilon^4)$\niterations or exhibit a dimension-dependent sample complexity.",
          "link": "http://arxiv.org/abs/2107.00469",
          "publishedOn": "2021-07-02T01:58:02.088Z",
          "wordCount": 534,
          "title": "Never Go Full Batch (in Stochastic Convex Optimization). (arXiv:2107.00469v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/1911.02728",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Liu_M/0/1/0/all/0/1\">Meimei Liu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhengwu Zhang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Dunson_D/0/1/0/all/0/1\">David B. Dunson</a>",
          "description": "There has been huge interest in studying human brain connectomes inferred\nfrom different imaging modalities and exploring their relationship with human\ntraits, such as cognition. Brain connectomes are usually represented as\nnetworks, with nodes corresponding to different regions of interest (ROIs) and\nedges to connection strengths between ROIs. Due to the high-dimensionality and\nnon-Euclidean nature of networks, it is challenging to depict their population\ndistribution and relate them to human traits. Current approaches focus on\nsummarizing the network using either pre-specified topological features or\nprincipal components analysis (PCA). In this paper, building on recent advances\nin deep learning, we develop a nonlinear latent factor model to characterize\nthe population distribution of brain graphs and infer the relationships between\nbrain structural connectomes and human traits. We refer to our method as Graph\nAuTo-Encoding (GATE). We applied GATE to two large-scale brain imaging\ndatasets, the Adolescent Brain Cognitive Development (ABCD) study and the Human\nConnectome Project (HCP) for adults, to understand the structural brain\nconnectome and its relationship with cognition. Numerical results demonstrate\nhuge advantages of GATE over competitors in terms of prediction accuracy,\nstatistical inference and computing efficiency. We found that structural\nconnectomes have a stronger association with a wide range of human cognitive\ntraits than was apparent using previous approaches.",
          "link": "http://arxiv.org/abs/1911.02728",
          "publishedOn": "2021-07-02T01:58:02.081Z",
          "wordCount": 679,
          "title": "Auto-encoding brain networks with applications to analyzing large-scale brain imaging datasets. (arXiv:1911.02728v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.00050",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feder_M/0/1/0/all/0/1\">Meir Feder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Polyanskiy_Y/0/1/0/all/0/1\">Yury Polyanskiy</a>",
          "description": "We consider the question of sequential prediction under the log-loss in terms\nof cumulative regret. Namely, given a hypothesis class of distributions,\nlearner sequentially predicts the (distribution of the) next letter in sequence\nand its performance is compared to the baseline of the best constant predictor\nfrom the hypothesis class. The well-specified case corresponds to an additional\nassumption that the data-generating distribution belongs to the hypothesis\nclass as well. Here we present results in the more general misspecified case.\nDue to special properties of the log-loss, the same problem arises in the\ncontext of competitive-optimality in density estimation, and model selection.\nFor the $d$-dimensional Gaussian location hypothesis class, we show that\ncumulative regrets in the well-specified and misspecified cases asymptotically\ncoincide. In other words, we provide an $o(1)$ characterization of the\ndistribution-free (or PAC) regret in this case -- the first such result as far\nas we know. We recall that the worst-case (or individual-sequence) regret in\nthis case is larger by an additive constant ${d\\over 2} + o(1)$. Surprisingly,\nneither the traditional Bayesian estimators, nor the Shtarkov's normalized\nmaximum likelihood achieve the PAC regret and our estimator requires special\n\"robustification\" against heavy-tailed data. In addition, we show two general\nresults for misspecified regret: the existence and uniqueness of the optimal\nestimator, and the bound sandwiching the misspecified regret between\nwell-specified regrets with (asymptotically) close hypotheses classes.",
          "link": "http://arxiv.org/abs/2102.00050",
          "publishedOn": "2021-07-02T01:58:02.043Z",
          "wordCount": 688,
          "title": "Sequential prediction under log-loss and misspecification. (arXiv:2102.00050v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00366",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Damirchi_H/0/1/0/all/0/1\">Hamed Damirchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khorrambakht_R/0/1/0/all/0/1\">Rooholla Khorrambakht</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taghirad_H/0/1/0/all/0/1\">Hamid D. Taghirad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moshiri_B/0/1/0/all/0/1\">Behzad Moshiri</a>",
          "description": "The incremental poses computed through odometry can be integrated over time\nto calculate the pose of a device with respect to an initial location. The\nresulting global pose may be used to formulate a second, consistency based,\nloss term in a deep odometry setting. In such cases where multiple losses are\nimposed on a network, the uncertainty over each output can be derived to weigh\nthe different loss terms in a maximum likelihood setting. However, when\nimposing a constraint on the integrated transformation, due to how only\nodometry is estimated at each iteration of the algorithm, there is no\ninformation about the uncertainty associated with the global pose to weigh the\nglobal loss term. In this paper, we associate uncertainties with the output\nposes of a deep odometry network and propagate the uncertainties through each\niteration. Our goal is to use the estimated covariance matrix at each\nincremental step to weigh the loss at the corresponding step while weighting\nthe global loss term using the compounded uncertainty. This formulation\nprovides an adaptive method to weigh the incremental and integrated loss terms\nagainst each other, noting the increase in uncertainty as new estimates arrive.\nWe provide quantitative and qualitative analysis of pose estimates and show\nthat our method surpasses the accuracy of the state-of-the-art Visual Odometry\napproaches. Then, uncertainty estimates are evaluated and comparisons against\nfixed baselines are provided. Finally, the uncertainty values are used in a\nrealistic example to show the effectiveness of uncertainty quantification for\nlocalization.",
          "link": "http://arxiv.org/abs/2107.00366",
          "publishedOn": "2021-07-02T01:58:02.036Z",
          "wordCount": 695,
          "title": "A Consistency-Based Loss for Deep Odometry Through Uncertainty Propagation. (arXiv:2107.00366v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00363",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Dewolf_N/0/1/0/all/0/1\">Nicolas Dewolf</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Baets_B/0/1/0/all/0/1\">Bernard De Baets</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Waegeman_W/0/1/0/all/0/1\">Willem Waegeman</a>",
          "description": "Over the last few decades, various methods have been proposed for estimating\nprediction intervals in regression settings, including Bayesian methods,\nensemble methods, direct interval estimation methods and conformal prediction\nmethods. An important issue is the calibration of these methods: the generated\nprediction intervals should have a predefined coverage level, without being\noverly conservative. In this work, we review the above four classes of methods\nfrom a conceptual and experimental point of view. Results on benchmark data\nsets from various domains highlight large fluctuations in performance from one\ndata set to another. These observations can be attributed to the violation of\ncertain assumptions that are inherent to some classes of methods. We illustrate\nhow conformal prediction can be used as a general calibration procedure for\nmethods that deliver poor results without a calibration step.",
          "link": "http://arxiv.org/abs/2107.00363",
          "publishedOn": "2021-07-02T01:58:02.029Z",
          "wordCount": 566,
          "title": "Well-calibrated prediction intervals for regression problems. (arXiv:2107.00363v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00090",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Frankel_A/0/1/0/all/0/1\">Ari Frankel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Safta_C/0/1/0/all/0/1\">Cosmin Safta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alleman_C/0/1/0/all/0/1\">Coleman Alleman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jones_R/0/1/0/all/0/1\">Reese Jones</a>",
          "description": "Predicting the evolution of a representative sample of a material with\nmicrostructure is a fundamental problem in homogenization. In this work we\npropose a graph convolutional neural network that utilizes the discretized\nrepresentation of the initial microstructure directly, without segmentation or\nclustering. Compared to feature-based and pixel-based convolutional neural\nnetwork models, the proposed method has a number of advantages: (a) it is deep\nin that it does not require featurization but can benefit from it, (b) it has a\nsimple implementation with standard convolutional filters and layers, (c) it\nworks natively on unstructured and structured grid data without interpolation\n(unlike pixel-based convolutional neural networks), and (d) it preserves\nrotational invariance like other graph-based convolutional neural networks. We\ndemonstrate the performance of the proposed network and compare it to\ntraditional pixel-based convolution neural network models and feature-based\ngraph convolutional neural networks on three large datasets.",
          "link": "http://arxiv.org/abs/2107.00090",
          "publishedOn": "2021-07-02T01:58:02.019Z",
          "wordCount": 586,
          "title": "Mesh-based graph convolutional neural network models of processes with complex initial states. (arXiv:2107.00090v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00516",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Choudhury_M/0/1/0/all/0/1\">Muntabir Hasan Choudhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jayanetti_H/0/1/0/all/0/1\">Himarsha R. Jayanetti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jian Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ingram_W/0/1/0/all/0/1\">William A. Ingram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fox_E/0/1/0/all/0/1\">Edward A. Fox</a>",
          "description": "Electronic Theses and Dissertations (ETDs) contain domain knowledge that can\nbe used for many digital library tasks, such as analyzing citation networks and\npredicting research trends. Automatic metadata extraction is important to build\nscalable digital library search engines. Most existing methods are designed for\nborn-digital documents, so they often fail to extract metadata from scanned\ndocuments such as for ETDs. Traditional sequence tagging methods mainly rely on\ntext-based features. In this paper, we propose a conditional random field (CRF)\nmodel that combines text-based and visual features. To verify the robustness of\nour model, we extended an existing corpus and created a new ground truth corpus\nconsisting of 500 ETD cover pages with human validated metadata. Our\nexperiments show that CRF with visual features outperformed both a heuristic\nand a CRF model with only text-based features. The proposed model achieved\n81.3%-96% F1 measure on seven metadata fields. The data and source code are\npublicly available on Google Drive (https://tinyurl.com/y8kxzwrp) and a GitHub\nrepository (https://github.com/lamps-lab/ETDMiner/tree/master/etd_crf),\nrespectively.",
          "link": "http://arxiv.org/abs/2107.00516",
          "publishedOn": "2021-07-02T01:58:01.876Z",
          "wordCount": 630,
          "title": "Automatic Metadata Extraction Incorporating Visual Features from Scanned Electronic Theses and Dissertations. (arXiv:2107.00516v1 [cs.DL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07989",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gronlund_A/0/1/0/all/0/1\">Allan Gr&#xf8;nlund</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hogsgaard_M/0/1/0/all/0/1\">Mikael H&#xf8;gsgaard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamma_L/0/1/0/all/0/1\">Lior Kamma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Larsen_K/0/1/0/all/0/1\">Kasper Green Larsen</a>",
          "description": "Explaining the surprising generalization performance of deep neural networks\nis an active and important line of research in theoretical machine learning.\nInfluential work by Arora et al. (ICML'18) showed that, noise stability\nproperties of deep nets occurring in practice can be used to provably compress\nmodel representations. They then argued that the small representations of\ncompressed networks imply good generalization performance albeit only of the\ncompressed nets. Extending their compression framework to yield generalization\nbounds for the original uncompressed networks remains elusive.\n\nOur main contribution is the establishment of a compression-based framework\nfor proving generalization bounds. The framework is simple and powerful enough\nto extend the generalization bounds by Arora et al. to also hold for the\noriginal network. To demonstrate the flexibility of the framework, we also show\nthat it allows us to give simple proofs of the strongest known generalization\nbounds for other popular machine learning models, namely Support Vector\nMachines and Boosting.",
          "link": "http://arxiv.org/abs/2106.07989",
          "publishedOn": "2021-07-02T01:58:01.867Z",
          "wordCount": 619,
          "title": "Compression Implies Generalization. (arXiv:2106.07989v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00223",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Uchizawa_K/0/1/0/all/0/1\">Kei Uchizawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abe_H/0/1/0/all/0/1\">Haruki Abe</a>",
          "description": "We study computational hardness of feature and conjunction search through the\nlens of circuit complexity. Let $x = (x_1, ... , x_n)$ (resp., $y = (y_1, ... ,\ny_n)$) be Boolean variables each of which takes the value one if and only if a\nneuron at place $i$ detects a feature (resp., another feature). We then simply\nformulate the feature and conjunction search as Boolean functions ${\\rm\nFTR}_n(x) = \\bigvee_{i=1}^n x_i$ and ${\\rm CONJ}_n(x, y) = \\bigvee_{i=1}^n x_i\n\\wedge y_i$, respectively. We employ a threshold circuit or a discretized\ncircuit (such as a sigmoid circuit or a ReLU circuit with discretization) as\nour models of neural networks, and consider the following four computational\nresources: [i] the number of neurons (size), [ii] the number of levels (depth),\n[iii] the number of active neurons outputting non-zero values (energy), and\n[iv] synaptic weight resolution (weight).\n\nWe first prove that any threshold circuit $C$ of size $s$, depth $d$, energy\n$e$ and weight $w$ satisfies $\\log rk(M_C) \\le ed (\\log s + \\log w + \\log n)$,\nwhere $rk(M_C)$ is the rank of the communication matrix $M_C$ of a\n$2n$-variable Boolean function that $C$ computes. Since ${\\rm CONJ}_n$ has rank\n$2^n$, we have $n \\le ed (\\log s + \\log w + \\log n)$. Thus, an exponential\nlower bound on the size of even sublinear-depth threshold circuits exists if\nthe energy and weight are sufficiently small. Since ${\\rm FTR}_n$ is computable\nindependently of $n$, our result suggests that computational capacity for the\nfeature and conjunction search are different. We also show that the inequality\nis tight up to a constant factor if $ed = o(n/ \\log n)$. We next show that a\nsimilar inequality holds for any discretized circuit. Thus, if we regard the\nnumber of gates outputting non-zero values as a measure for sparse activity,\nour results suggest that larger depth helps neural networks to acquire sparse\nactivity.",
          "link": "http://arxiv.org/abs/2107.00223",
          "publishedOn": "2021-07-02T01:58:01.839Z",
          "wordCount": 752,
          "title": "Circuit Complexity of Visual Search. (arXiv:2107.00223v1 [cs.CC])"
        },
        {
          "id": "http://arxiv.org/abs/2102.02642",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Christoffersen_B/0/1/0/all/0/1\">Benjamin Christoffersen</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Clements_M/0/1/0/all/0/1\">Mark Clements</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Humphreys_K/0/1/0/all/0/1\">Keith Humphreys</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Kjellstrom_H/0/1/0/all/0/1\">Hedvig Kjellstr&#xf6;m</a>",
          "description": "Missing values with mixed data types is a common problem in a large number of\nmachine learning applications such as processing of surveys and in different\nmedical applications. Recently, Gaussian copula models have been suggested as a\nmeans of performing imputation of missing values using a probabilistic\nframework. While the present Gaussian copula models have shown to yield state\nof the art performance, they have two limitations: they are based on an\napproximation that is fast but may be imprecise and they do not support\nunordered multinomial variables. We address the first limitation using direct\nand arbitrarily precise approximations both for model estimation and imputation\nby using randomized quasi-Monte Carlo procedures. The method we provide has\nlower errors for the estimated model parameters and the imputed values,\ncompared to previously proposed methods. We also extend the previous Gaussian\ncopula models to include unordered multinomial variables in addition to the\npresent support of ordinal, binary, and continuous variables.",
          "link": "http://arxiv.org/abs/2102.02642",
          "publishedOn": "2021-07-02T01:58:01.832Z",
          "wordCount": 632,
          "title": "Asymptotically Exact and Fast Gaussian Copula Models for Imputation of Mixed Data Types. (arXiv:2102.02642v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2002.04788",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_H/0/1/0/all/0/1\">Hsiang Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Diaz_M/0/1/0/all/0/1\">Mario Diaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Calmon_F/0/1/0/all/0/1\">Flavio P. Calmon</a>",
          "description": "Disparate treatment occurs when a machine learning model yields different\ndecisions for individuals based on a sensitive attribute (e.g., age, sex). In\ndomains where prediction accuracy is paramount, it could potentially be\nacceptable to fit a model which exhibits disparate treatment. To evaluate the\neffect of disparate treatment, we compare the performance of split classifiers\n(i.e., classifiers trained and deployed separately on each group) with\ngroup-blind classifiers (i.e., classifiers which do not use a sensitive\nattribute). We introduce the benefit-of-splitting for quantifying the\nperformance improvement by splitting classifiers. Computing the\nbenefit-of-splitting directly from its definition could be intractable since it\ninvolves solving optimization problems over an infinite-dimensional functional\nspace. Under different performance measures, we (i) prove an equivalent\nexpression for the benefit-of-splitting which can be efficiently computed by\nsolving small-scale convex programs; (ii) provide sharp upper and lower bounds\nfor the benefit-of-splitting which reveal precise conditions where a\ngroup-blind classifier will always suffer from a non-trivial performance gap\nfrom the split classifiers. In the finite sample regime, splitting is not\nnecessarily beneficial and we provide data-dependent bounds to understand this\neffect. Finally, we validate our theoretical results through numerical\nexperiments on both synthetic and real-world datasets.",
          "link": "http://arxiv.org/abs/2002.04788",
          "publishedOn": "2021-07-02T01:58:01.822Z",
          "wordCount": 693,
          "title": "To Split or Not to Split: The Impact of Disparate Treatment in Classification. (arXiv:2002.04788v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00436",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Larsen_E/0/1/0/all/0/1\">Erik Larsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noever_D/0/1/0/all/0/1\">David Noever</a>, <a href=\"http://arxiv.org/find/cs/1/au:+MacVittie_K/0/1/0/all/0/1\">Korey MacVittie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lilly_J/0/1/0/all/0/1\">John Lilly</a>",
          "description": "Twenty-three machine learning algorithms were trained then scored to\nestablish baseline comparison metrics and to select an image classification\nalgorithm worthy of embedding into mission-critical satellite imaging systems.\nThe Overhead-MNIST dataset is a collection of satellite images similar in style\nto the ubiquitous MNIST hand-written digits found in the machine learning\nliterature. The CatBoost classifier, Light Gradient Boosting Machine, and\nExtreme Gradient Boosting models produced the highest accuracies, Areas Under\nthe Curve (AUC), and F1 scores in a PyCaret general comparison. Separate\nevaluations showed that a deep convolutional architecture was the most\npromising. We present results for the overall best performing algorithm as a\nbaseline for edge deployability and future performance improvement: a\nconvolutional neural network (CNN) scoring 0.965 categorical accuracy on unseen\ntest data.",
          "link": "http://arxiv.org/abs/2107.00436",
          "publishedOn": "2021-07-02T01:58:01.804Z",
          "wordCount": 570,
          "title": "Overhead-MNIST: Machine Learning Baselines for Image Classification. (arXiv:2107.00436v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00641",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jianwei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chunyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1\">Pengchuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1\">Xiyang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_B/0/1/0/all/0/1\">Bin Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1\">Lu Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>",
          "description": "Recently, Vision Transformer and its variants have shown great promise on\nvarious computer vision tasks. The ability of capturing short- and long-range\nvisual dependencies through self-attention is arguably the main source for the\nsuccess. But it also brings challenges due to quadratic computational overhead,\nespecially for the high-resolution vision tasks (e.g., object detection). In\nthis paper, we present focal self-attention, a new mechanism that incorporates\nboth fine-grained local and coarse-grained global interactions. Using this new\nmechanism, each token attends the closest surrounding tokens at fine\ngranularity but the tokens far away at coarse granularity, and thus can capture\nboth short- and long-range visual dependencies efficiently and effectively.\nWith focal self-attention, we propose a new variant of Vision Transformer\nmodels, called Focal Transformer, which achieves superior performance over the\nstate-of-the-art vision Transformers on a range of public image classification\nand object detection benchmarks. In particular, our Focal Transformer models\nwith a moderate size of 51.1M and a larger size of 89.8M achieve 83.5 and 83.8\nTop-1 accuracy, respectively, on ImageNet classification at 224x224 resolution.\nUsing Focal Transformers as the backbones, we obtain consistent and substantial\nimprovements over the current state-of-the-art Swin Transformers for 6\ndifferent object detection methods trained with standard 1x and 3x schedules.\nOur largest Focal Transformer yields 58.7/58.9 box mAPs and 50.9/51.3 mask mAPs\non COCO mini-val/test-dev, and 55.4 mIoU on ADE20K for semantic segmentation,\ncreating new SoTA on three of the most challenging computer vision tasks.",
          "link": "http://arxiv.org/abs/2107.00641",
          "publishedOn": "2021-07-02T01:58:01.787Z",
          "wordCount": 689,
          "title": "Focal Self-attention for Local-Global Interactions in Vision Transformers. (arXiv:2107.00641v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00296",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Niu_Y/0/1/0/all/0/1\">Yuhao Niu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gu_L/0/1/0/all/0/1\">Lin Gu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_Y/0/1/0/all/0/1\">Yitian Zhao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lu_F/0/1/0/all/0/1\">Feng Lu</a>",
          "description": "Though deep learning has shown successful performance in classifying the\nlabel and severity stage of certain diseases, most of them give few\nexplanations on how to make predictions. Inspired by Koch's Postulates, the\nfoundation in evidence-based medicine (EBM) to identify the pathogen, we\npropose to exploit the interpretability of deep learning application in medical\ndiagnosis. By determining and isolating the neuron activation patterns on which\ndiabetic retinopathy (DR) detector relies to make decisions, we demonstrate the\ndirect relation between the isolated neuron activation and lesions for a\npathological explanation. To be specific, we first define novel pathological\ndescriptors using activated neurons of the DR detector to encode both spatial\nand appearance information of lesions. Then, to visualize the symptom encoded\nin the descriptor, we propose Patho-GAN, a new network to synthesize medically\nplausible retinal images. By manipulating these descriptors, we could even\narbitrarily control the position, quantity, and categories of generated\nlesions. We also show that our synthesized images carry the symptoms directly\nrelated to diabetic retinopathy diagnosis. Our generated images are both\nqualitatively and quantitatively superior to the ones by previous methods.\nBesides, compared to existing methods that take hours to generate an image, our\nsecond level speed endows the potential to be an effective solution for data\naugmentation.",
          "link": "http://arxiv.org/abs/2107.00296",
          "publishedOn": "2021-07-02T01:58:01.770Z",
          "wordCount": 667,
          "title": "Explainable Diabetic Retinopathy Detection and Retinal Image Generation. (arXiv:2107.00296v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.03039",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ban_Y/0/1/0/all/0/1\">Yikun Ban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jingrui He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cook_C/0/1/0/all/0/1\">Curtiss B. Cook</a>",
          "description": "Contextual multi-armed bandit has shown to be an effective tool in\nrecommender systems. In this paper, we study a novel problem of multi-facet\nbandits involving a group of bandits, each characterizing the users' needs from\none unique aspect. In each round, for the given user, we need to select one arm\nfrom each bandit, such that the combination of all arms maximizes the final\nreward. This problem can find immediate applications in E-commerce, healthcare,\netc. To address this problem, we propose a novel algorithm, named MuFasa, which\nutilizes an assembled neural network to jointly learn the underlying reward\nfunctions of multiple bandits. It estimates an Upper Confidence Bound (UCB)\nlinked with the expected reward to balance between exploitation and\nexploration. Under mild assumptions, we provide the regret analysis of MuFasa.\nIt can achieve the near-optimal $\\widetilde{ \\mathcal{O}}((K+1)\\sqrt{T})$\nregret bound where $K$ is the number of bandits and $T$ is the number of played\nrounds. Furthermore, we conduct extensive experiments to show that MuFasa\noutperforms strong baselines on real-world data sets.",
          "link": "http://arxiv.org/abs/2106.03039",
          "publishedOn": "2021-07-02T01:58:01.752Z",
          "wordCount": 626,
          "title": "Multi-facet Contextual Bandits: A Neural Network Perspective. (arXiv:2106.03039v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00197",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Han-Jia Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ming_L/0/1/0/all/0/1\">Lu Ming</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_D/0/1/0/all/0/1\">De-Chuan Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_W/0/1/0/all/0/1\">Wei-Lun Chao</a>",
          "description": "Few-shot learning (FSL) aims to train a strong classifier using limited\nlabeled examples. Many existing works take the meta-learning approach, sampling\nfew-shot tasks in turn and optimizing the few-shot learner's performance on\nclassifying the query examples. In this paper, we point out two potential\nweaknesses of this approach. First, the sampled query examples may not provide\nsufficient supervision for the few-shot learner. Second, the effectiveness of\nmeta-learning diminishes sharply with increasing shots (i.e., the number of\ntraining examples per class). To resolve these issues, we propose a novel\nobjective to directly train the few-shot learner to perform like a strong\nclassifier. Concretely, we associate each sampled few-shot task with a strong\nclassifier, which is learned with ample labeled examples. The strong classifier\nhas a better generalization ability and we use it to supervise the few-shot\nlearner. We present an efficient way to construct the strong classifier, making\nour proposed objective an easily plug-and-play term to existing meta-learning\nbased FSL methods. We validate our approach in combinations with many\nrepresentative meta-learning methods. On several benchmark datasets including\nminiImageNet and tiredImageNet, our approach leads to a notable improvement\nacross a variety of tasks. More importantly, with our approach, meta-learning\nbased FSL methods can consistently outperform non-meta-learning based ones,\neven in a many-shot setting, greatly strengthening their applicability.",
          "link": "http://arxiv.org/abs/2107.00197",
          "publishedOn": "2021-07-02T01:58:01.724Z",
          "wordCount": 654,
          "title": "Few-Shot Learning with a Strong Teacher. (arXiv:2107.00197v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2011.08091",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Moreo_A/0/1/0/all/0/1\">Alejandro Moreo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sebastiani_F/0/1/0/all/0/1\">Fabrizio Sebastiani</a>",
          "description": "Sentiment quantification is the task of estimating the relative frequency (or\n\"prevalence\") of sentiment-related classes (such as Positive, Neutral,\nNegative) in a sample of unlabelled texts; this is especially important when\nthese texts are tweets, since most sentiment classification endeavours carried\nout on Twitter data actually have quantification (and not the classification of\nindividual tweets) as their ultimate goal. It is well-known that solving\nquantification via \"classify and count\" (i.e., by classifying all unlabelled\nitems via a standard classifier and counting the items that have been assigned\nto a given class) is suboptimal in terms of accuracy, and that more accurate\nquantification methods exist. In 2016, Gao and Sebastiani carried out a\nsystematic comparison of quantification methods on the task of tweet sentiment\nquantification. In hindsight, we observe that the experimental protocol\nfollowed in that work is flawed, and that its results are thus unreliable. We\nnow re-evaluate those quantification methods on the very same datasets, this\ntime following a now consolidated and much more robust experimental protocol,\nthat involves 5775 as many experiments as run in the original study. Our\nexperimentation yields results dramatically different from those obtained by\nGao and Sebastiani, and thus provide a different, much more solid understanding\nof the relative strengths and weaknesses of different sentiment quantification\nmethods.",
          "link": "http://arxiv.org/abs/2011.08091",
          "publishedOn": "2021-07-02T01:58:01.632Z",
          "wordCount": 676,
          "title": "Tweet Sentiment Quantification: An Experimental Re-Evaluation. (arXiv:2011.08091v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1909.03773",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cai_X/0/1/0/all/0/1\">Xin-Qiang Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yao-Xiang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1\">Yuan Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhi-Hua Zhou</a>",
          "description": "One of the key issues for imitation learning lies in making policy learned\nfrom limited samples to generalize well in the whole state-action space. This\nproblem is much more severe in high-dimensional state environments, such as\ngame playing with raw pixel inputs. Under this situation, even state-of-the-art\nadversary-based imitation learning algorithms fail. Through empirical studies,\nwe find that the main cause lies in the failure of training a powerful\ndiscriminator to generate meaningful rewards in high-dimensional environments.\nAlthough it seems that dimensionality reduction can help, a straightforward\napplication of off-the-shelf methods cannot achieve good performance. In this\nwork, we show in theory that the balance between dimensionality reduction and\ndiscriminative training is essential for effective learning. To achieve this\ntarget, we propose HashReward, which utilizes the idea of supervised hashing to\nrealize such an ideal balance. Experimental results show that HashReward could\noutperform state-of-the-art methods for a large gap under the challenging\nhigh-dimensional environments.",
          "link": "http://arxiv.org/abs/1909.03773",
          "publishedOn": "2021-07-02T01:58:01.625Z",
          "wordCount": 628,
          "title": "Imitation Learning from Pixel-Level Demonstrations by HashReward. (arXiv:1909.03773v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.01089",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qingru Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wipf_D/0/1/0/all/0/1\">David Wipf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_Q/0/1/0/all/0/1\">Quan Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1\">Le Song</a>",
          "description": "Graph neural networks (GNN) have recently emerged as a vehicle for applying\ndeep network architectures to graph and relational data. However, given the\nincreasing size of industrial datasets, in many practical situations the\nmessage passing computations required for sharing information across GNN layers\nare no longer scalable. Although various sampling methods have been introduced\nto approximate full-graph training within a tractable budget, there remain\nunresolved complications such as high variances and limited theoretical\nguarantees. To address these issues, we build upon existing work and treat GNN\nneighbor sampling as a multi-armed bandit problem but with a newly-designed\nreward function that introduces some degree of bias designed to reduce variance\nand avoid unstable, possibly-unbounded pay outs. And unlike prior bandit-GNN\nuse cases, the resulting policy leads to near-optimal regret while accounting\nfor the GNN training dynamics introduced by SGD. From a practical standpoint,\nthis translates into lower variance estimates and competitive or superior test\naccuracy across several benchmarks.",
          "link": "http://arxiv.org/abs/2103.01089",
          "publishedOn": "2021-07-02T01:58:01.617Z",
          "wordCount": 623,
          "title": "A Biased Graph Neural Network Sampler with Near-Optimal Regret. (arXiv:2103.01089v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00339",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Grace Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_L/0/1/0/all/0/1\">Linghan Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Youngwoon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_J/0/1/0/all/0/1\">Joseph J. Lim</a>",
          "description": "The ability to transfer a policy from one environment to another is a\npromising avenue for efficient robot learning in realistic settings where task\nsupervision is not available. This can allow us to take advantage of\nenvironments well suited for training, such as simulators or laboratories, to\nlearn a policy for a real robot in a home or office. To succeed, such policy\ntransfer must overcome both the visual domain gap (e.g. different illumination\nor background) and the dynamics domain gap (e.g. different robot calibration or\nmodelling error) between source and target environments. However, prior policy\ntransfer approaches either cannot handle a large domain gap or can only address\none type of domain gap at a time. In this paper, we propose a novel policy\ntransfer method with iterative \"environment grounding\", IDAPT, that alternates\nbetween (1) directly minimizing both visual and dynamics domain gaps by\ngrounding the source environment in the target environment domains, and (2)\ntraining a policy on the grounded source environment. This iterative training\nprogressively aligns the domains between the two environments and adapts the\npolicy to the target environment. Once trained, the policy can be directly\nexecuted on the target environment. The empirical results on locomotion and\nrobotic manipulation tasks demonstrate that our approach can effectively\ntransfer a policy across visual and dynamics domain gaps with minimal\nsupervision and interaction with the target environment. Videos and code are\navailable at https://clvrai.com/idapt .",
          "link": "http://arxiv.org/abs/2107.00339",
          "publishedOn": "2021-07-02T01:58:01.611Z",
          "wordCount": 688,
          "title": "Policy Transfer across Visual and Dynamics Domain Gaps via Iterative Grounding. (arXiv:2107.00339v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2012.02409",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Chatterji_N/0/1/0/all/0/1\">Niladri S. Chatterji</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Long_P/0/1/0/all/0/1\">Philip M. Long</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Bartlett_P/0/1/0/all/0/1\">Peter L. Bartlett</a>",
          "description": "We study the training of finite-width two-layer smoothed ReLU networks for\nbinary classification using the logistic loss. We show that gradient descent\ndrives the training loss to zero if the initial loss is small enough. When the\ndata satisfies certain cluster and separation conditions and the network is\nwide enough, we show that one step of gradient descent reduces the loss\nsufficiently that the first result applies.",
          "link": "http://arxiv.org/abs/2012.02409",
          "publishedOn": "2021-07-02T01:58:01.593Z",
          "wordCount": 549,
          "title": "When does gradient descent with logistic loss find interpolating two-layer networks?. (arXiv:2012.02409v4 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.02255",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Flennerhag_S/0/1/0/all/0/1\">Sebastian Flennerhag</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jane X. Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sprechmann_P/0/1/0/all/0/1\">Pablo Sprechmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Visin_F/0/1/0/all/0/1\">Francesco Visin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galashov_A/0/1/0/all/0/1\">Alexandre Galashov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kapturowski_S/0/1/0/all/0/1\">Steven Kapturowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borsa_D/0/1/0/all/0/1\">Diana L. Borsa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heess_N/0/1/0/all/0/1\">Nicolas Heess</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barreto_A/0/1/0/all/0/1\">Andre Barreto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pascanu_R/0/1/0/all/0/1\">Razvan Pascanu</a>",
          "description": "An effective approach to exploration in reinforcement learning is to rely on\nan agent's uncertainty over the optimal policy, which can yield near-optimal\nexploration strategies in tabular settings. However, in non-tabular settings\nthat involve function approximators, obtaining accurate uncertainty estimates\nis almost as challenging a problem. In this paper, we highlight that value\nestimates are easily biased and temporally inconsistent. In light of this, we\npropose a novel method for estimating uncertainty over the value function that\nrelies on inducing a distribution over temporal difference errors. This\nexploration signal controls for state-action transitions so as to isolate\nuncertainty in value that is due to uncertainty over the agent's parameters.\nBecause our measure of uncertainty conditions on state-action transitions, we\ncannot act on this measure directly. Instead, we incorporate it as an intrinsic\nreward and treat exploration as a separate learning problem, induced by the\nagent's temporal difference uncertainties. We introduce a distinct exploration\npolicy that learns to collect data with high estimated uncertainty, which gives\nrise to a curriculum that smoothly changes throughout learning and vanishes in\nthe limit of perfect value estimates. We evaluate our method on hard\nexploration tasks, including Deep Sea and Atari 2600 environments and find that\nour proposed form of exploration facilitates both diverse and deep exploration.",
          "link": "http://arxiv.org/abs/2010.02255",
          "publishedOn": "2021-07-02T01:58:01.586Z",
          "wordCount": 703,
          "title": "Temporal Difference Uncertainties as a Signal for Exploration. (arXiv:2010.02255v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2003.13314",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wenbo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leshem_A/0/1/0/all/0/1\">Amir Leshem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niyato_D/0/1/0/all/0/1\">Dusit Niyato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1\">Zhu Han</a>",
          "description": "We study a decentralized channel allocation problem in an ad-hoc Internet of\nThings network underlaying on the spectrum licensed to a primary cellular\nnetwork. In the considered network, the impoverished channel sensing/probing\ncapability and computational resource on the IoT devices make them difficult to\nacquire the detailed Channel State Information (CSI) for the shared multiple\nchannels. In practice, the unknown patterns of the primary users' transmission\nactivities and the time-varying CSI (e.g., due to small-scale fading or device\nmobility) also cause stochastic changes in the channel quality. Decentralized\nIoT links are thus expected to learn channel conditions online based on partial\nobservations, while acquiring no information about the channels that they are\nnot operating on. They also have to reach an efficient, collision-free solution\nof channel allocation with limited coordination. Our study maps this problem\ninto a contextual multi-player, multi-armed bandit game, and proposes a purely\ndecentralized, three-stage policy learning algorithm through trial-and-error.\nTheoretical analyses shows that the proposed scheme guarantees the IoT links to\njointly converge to the social optimal channel allocation with a sub-linear\n(i.e., polylogarithmic) regret with respect to the operational time.\nSimulations demonstrate that it strikes a good balance between efficiency and\nnetwork scalability when compared with the other state-of-the-art decentralized\nbandit algorithms.",
          "link": "http://arxiv.org/abs/2003.13314",
          "publishedOn": "2021-07-02T01:58:01.579Z",
          "wordCount": 714,
          "title": "Decentralized Learning for Channel Allocation in IoT Networks over Unlicensed Bandwidth as a Contextual Multi-player Multi-armed Bandit Game. (arXiv:2003.13314v3 [cs.MA] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.04998",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Chatterji_N/0/1/0/all/0/1\">Niladri S. Chatterji</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Long_P/0/1/0/all/0/1\">Philip M. Long</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Bartlett_P/0/1/0/all/0/1\">Peter L. Bartlett</a>",
          "description": "We establish conditions under which gradient descent applied to fixed-width\ndeep networks drives the logistic loss to zero, and prove bounds on the rate of\nconvergence. Our analysis applies for smoothed approximations to the ReLU, such\nas Swish and the Huberized ReLU, proposed in previous applied work. We provide\ntwo sufficient conditions for convergence. The first is simply a bound on the\nloss at initialization. The second is a data separation condition used in prior\nanalyses.",
          "link": "http://arxiv.org/abs/2102.04998",
          "publishedOn": "2021-07-02T01:58:01.572Z",
          "wordCount": 553,
          "title": "When does gradient descent with logistic loss interpolate using deep networks with smoothed ReLU activations?. (arXiv:2102.04998v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.01054",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gutierrez_R/0/1/0/all/0/1\">Ricardo Luna Gutierrez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leonetti_M/0/1/0/all/0/1\">Matteo Leonetti</a>",
          "description": "In Meta-Reinforcement Learning (meta-RL) an agent is trained on a set of\ntasks to prepare for and learn faster in new, unseen, but related tasks. The\ntraining tasks are usually hand-crafted to be representative of the expected\ndistribution of test tasks and hence all used in training. We show that given a\nset of training tasks, learning can be both faster and more effective (leading\nto better performance in the test tasks), if the training tasks are\nappropriately selected. We propose a task selection algorithm,\nInformation-Theoretic Task Selection (ITTS), based on information theory, which\noptimizes the set of tasks used for training in meta-RL, irrespectively of how\nthey are generated. The algorithm establishes which training tasks are both\nsufficiently relevant for the test tasks, and different enough from one\nanother. We reproduce different meta-RL experiments from the literature and\nshow that ITTS improves the final performance in all of them.",
          "link": "http://arxiv.org/abs/2011.01054",
          "publishedOn": "2021-07-02T01:58:01.565Z",
          "wordCount": 607,
          "title": "Information-theoretic Task Selection for Meta-Reinforcement Learning. (arXiv:2011.01054v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.08812",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiongjie Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yongxin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yunpeng Li</a>",
          "description": "While theoretically appealing, the application of the Wasserstein distance to\nlarge-scale machine learning problems has been hampered by its prohibitive\ncomputational cost. The sliced Wasserstein distance and its variants improve\nthe computational efficiency through the random projection, yet they suffer\nfrom low accuracy if the number of projections is not sufficiently large,\nbecause the majority of projections result in trivially small values. In this\nwork, we propose a new family of distance metrics, called augmented sliced\nWasserstein distances (ASWDs), constructed by first mapping samples to\nhigher-dimensional hypersurfaces parameterized by neural networks. It is\nderived from a key observation that (random) linear projections of samples\nresiding on these hypersurfaces would translate to much more flexible nonlinear\nprojections in the original sample space, so they can capture complex\nstructures of the data distribution. We show that the hypersurfaces can be\noptimized by gradient ascent efficiently. We provide the condition under which\nthe ASWD is a valid metric and show that this can be obtained by an injective\nneural network architecture. Numerical results demonstrate that the ASWD\nsignificantly outperforms other Wasserstein variants for both synthetic and\nreal-world problems.",
          "link": "http://arxiv.org/abs/2006.08812",
          "publishedOn": "2021-07-02T01:58:01.548Z",
          "wordCount": 660,
          "title": "Augmented Sliced Wasserstein Distances. (arXiv:2006.08812v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.13010",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jian Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_Y/0/1/0/all/0/1\">Yuling Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yunfei Yang</a>",
          "description": "This paper studies how well generative adversarial networks (GANs) learn\nprobability distributions from finite samples. Our main results establish the\nconvergence rates of GANs under a collection of integral probability metrics\ndefined through H\\\"older classes, including the Wasserstein distance as a\nspecial case. We also show that GANs are able to adaptively learn data\ndistributions with low-dimensional structures or have H\\\"older densities, when\nthe network architectures are chosen properly. In particular, for distributions\nconcentrated around a low-dimensional set, we show that the learning rates of\nGANs do not depend on the high ambient dimension, but on the lower intrinsic\ndimension. Our analysis is based on a new oracle inequality decomposing the\nestimation error into the generator and discriminator approximation error and\nthe statistical error, which may be of independent interest.",
          "link": "http://arxiv.org/abs/2105.13010",
          "publishedOn": "2021-07-02T01:58:01.541Z",
          "wordCount": 628,
          "title": "An error analysis of generative adversarial networks for learning distributions. (arXiv:2105.13010v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00520",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Puli_A/0/1/0/all/0/1\">Aahlad Puli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Lily H. Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oermann_E/0/1/0/all/0/1\">Eric K. Oermann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ranganath_R/0/1/0/all/0/1\">Rajesh Ranganath</a>",
          "description": "Deep predictive models often make use of spurious correlations between the\nlabel and the covariates that differ between training and test distributions.\nIn many classification tasks, spurious correlations are induced by a changing\nrelationship between the label and some nuisance variables correlated with the\ncovariates. For example, in classifying animals in natural images, the\nbackground, which is the nuisance, can predict the type of animal, but this\nnuisance label relationship does not always hold. This nuisance-label\nrelationship does not always hold. We formalize a family of distributions that\nonly differ in the nuisance-label relationship and and introduce a distribution\nwhere this relationship is broken called the nuisance-randomized distribution.\nWe introduce a set of predictive models built from the nuisance-randomized\ndistribution with representations, that when conditioned on, do not correlate\nthe label and the nuisance. For models in this set, we lower bound the\nperformance for any member of the family with the mutual information between\nthe representation and the label under the nuisance-randomized distribution. To\nbuild predictive models that maximize the performance lower bound, we develop\nNuisance-Randomized Distillation (NURD). We evaluate NURD on a synthetic\nexample, colored-MNIST, and classifying chest X-rays. When using non-lung\npatches as the nuisance in classifying chest X-rays, NURD produces models that\npredict pneumonia under strong spurious correlations.",
          "link": "http://arxiv.org/abs/2107.00520",
          "publishedOn": "2021-07-02T01:58:01.534Z",
          "wordCount": 652,
          "title": "Predictive Modeling in the Presence of Nuisance-Induced Spurious Correlations. (arXiv:2107.00520v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00561",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Manohar_Alers_N/0/1/0/all/0/1\">Nelson Manohar-Alers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_R/0/1/0/all/0/1\">Ryan Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sahib Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jiguo Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prakash_A/0/1/0/all/0/1\">Atul Prakash</a>",
          "description": "We present DeClaW, a system for detecting, classifying, and warning of\nadversarial inputs presented to a classification neural network. In contrast to\ncurrent state-of-the-art methods that, given an input, detect whether an input\nis clean or adversarial, we aim to also identify the types of adversarial\nattack (e.g., PGD, Carlini-Wagner or clean). To achieve this, we extract\nstatistical profiles, which we term as anomaly feature vectors, from a set of\nlatent features. Preliminary findings suggest that AFVs can help distinguish\namong several types of adversarial attacks (e.g., PGD versus Carlini-Wagner)\nwith close to 93% accuracy on the CIFAR-10 dataset. The results open the door\nto using AFV-based methods for exploring not only adversarial attack detection\nbut also classification of the attack type and then design of attack-specific\nmitigation strategies.",
          "link": "http://arxiv.org/abs/2107.00561",
          "publishedOn": "2021-07-02T01:58:01.527Z",
          "wordCount": 593,
          "title": "Using Anomaly Feature Vectors for Detecting, Classifying and Warning of Outlier Adversarial Examples. (arXiv:2107.00561v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00391",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Lopez_Ramos_L/0/1/0/all/0/1\">Luis Miguel Lopez-Ramos</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Roy_K/0/1/0/all/0/1\">Kevin Roy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Beferull_Lozano_B/0/1/0/all/0/1\">Baltasar Beferull-Lozano</a>",
          "description": "A method for nonlinear topology identification is proposed, based on the\nassumption that a collection of time series are generated in two steps: i) a\nvector autoregressive process in a latent space, and ii) a nonlinear,\ncomponent-wise, monotonically increasing observation mapping. The latter\nmappings are assumed invertible, and are modelled as shallow neural networks,\nso that their inverse can be numerically evaluated, and their parameters can be\nlearned using a technique inspired in deep learning. Due to the function\ninversion, the back-propagation step is not straightforward, and this paper\nexplains the steps needed to calculate the gradients applying implicit\ndifferentiation. Whereas the model explainability is the same as that for\nlinear VAR processes, preliminary numerical tests show that the prediction\nerror becomes smaller.",
          "link": "http://arxiv.org/abs/2107.00391",
          "publishedOn": "2021-07-02T01:58:01.521Z",
          "wordCount": 587,
          "title": "Explainable nonlinear modelling of multiple time series with invertible neural networks. (arXiv:2107.00391v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/1905.11797",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cesa_Bianchi_N/0/1/0/all/0/1\">Nicol&#xf2; Cesa-Bianchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cesari_T/0/1/0/all/0/1\">Tommaso R. Cesari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mansour_Y/0/1/0/all/0/1\">Yishay Mansour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perchet_V/0/1/0/all/0/1\">Vianney Perchet</a>",
          "description": "We introduce a novel theoretical framework for Return On Investment (ROI)\nmaximization in repeated decision-making. Our setting is motivated by the use\ncase of companies that regularly receive proposals for technological\ninnovations and want to quickly decide whether they are worth implementing. We\ndesign an algorithm for learning ROI-maximizing decision-making policies over a\nsequence of innovation proposals. Our algorithm provably converges to an\noptimal policy in class $\\Pi$ at a rate of order\n$\\min\\big\\{1/(N\\Delta^2),N^{-1/3}\\}$, where $N$ is the number of innovations\nand $\\Delta$ is the suboptimality gap in $\\Pi$. A significant hurdle of our\nformulation, which sets it aside from other online learning problems such as\nbandits, is that running a policy does not provide an unbiased estimate of its\nperformance.",
          "link": "http://arxiv.org/abs/1905.11797",
          "publishedOn": "2021-07-02T01:58:01.503Z",
          "wordCount": 619,
          "title": "A New Theoretical Framework for Fast and Accurate Online Decision-Making. (arXiv:1905.11797v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.00102",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Malenica_I/0/1/0/all/0/1\">Ivana Malenica</a>, <a href=\"http://arxiv.org/find/math/1/au:+Bibaut_A/0/1/0/all/0/1\">Aurelien Bibaut</a>, <a href=\"http://arxiv.org/find/math/1/au:+Laan_M/0/1/0/all/0/1\">Mark J. van der Laan</a>",
          "description": "The current work is motivated by the need for robust statistical methods for\nprecision medicine; as such, we address the need for statistical methods that\nprovide actionable inference for a single unit at any point in time. We aim to\nlearn an optimal, unknown choice of the controlled components of the design in\norder to optimize the expected outcome; with that, we adapt the randomization\nmechanism for future time-point experiments based on the data collected on the\nindividual over time. Our results demonstrate that one can learn the optimal\nrule based on a single sample, and thereby adjust the design at any point t\nwith valid inference for the mean target parameter. This work provides several\ncontributions to the field of statistical precision medicine. First, we define\na general class of averages of conditional causal parameters defined by the\ncurrent context for the single unit time-series data. We define a nonparametric\nmodel for the probability distribution of the time-series under few\nassumptions, and aim to fully utilize the sequential randomization in the\nestimation procedure via the double robust structure of the efficient influence\ncurve of the proposed target parameter. We present multiple\nexploration-exploitation strategies for assigning treatment, and methods for\nestimating the optimal rule. Lastly, we present the study of the data-adaptive\ninference on the mean under the optimal treatment rule, where the target\nparameter adapts over time in response to the observed context of the\nindividual. Our target parameter is pathwise differentiable with an efficient\ninfluence function that is doubly robust - which makes it easier to estimate\nthan previously proposed variations. We characterize the limit distribution of\nour estimator under a Donsker condition expressed in terms of a notion of\nbracketing entropy adapted to martingale settings.",
          "link": "http://arxiv.org/abs/2102.00102",
          "publishedOn": "2021-07-02T01:58:01.495Z",
          "wordCount": 754,
          "title": "Adaptive Sequential Design for a Single Time-Series. (arXiv:2102.00102v2 [math.ST] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00534",
          "author": "<a href=\"http://arxiv.org/find/q-fin/1/au:+Shi_Z/0/1/0/all/0/1\">Zijian Shi</a>, <a href=\"http://arxiv.org/find/q-fin/1/au:+Cartlidge_J/0/1/0/all/0/1\">John Cartlidge</a>",
          "description": "The limit order book (LOB) depicts the fine-grained demand and supply\nrelationship for financial assets and is widely used in market microstructure\nstudies. Nevertheless, the availability and high cost of LOB data restrict its\nwider application. The LOB recreation model (LOBRM) was recently proposed to\nbridge this gap by synthesizing the LOB from trades and quotes (TAQ) data.\nHowever, in the original LOBRM study, there were two limitations: (1)\nexperiments were conducted on a relatively small dataset containing only one\nday of LOB data; and (2) the training and testing were performed in a\nnon-chronological fashion, which essentially re-frames the task as\ninterpolation and potentially introduces lookahead bias. In this study, we\nextend the research on LOBRM and further validate its use in real-world\napplication scenarios. We first advance the workflow of LOBRM by (1) adding a\ntime-weighted z-score standardization for the LOB and (2) substituting the\nordinary differential equation kernel with an exponential decay kernel to lower\ncomputation complexity. Experiments are conducted on the extended LOBSTER\ndataset in a chronological fashion, as it would be used in a real-world\napplication. We find that (1) LOBRM with decay kernel is superior to\ntraditional non-linear models, and module ensembling is effective; (2)\nprediction accuracy is negatively related to the volatility of order volumes\nresting in the LOB; (3) the proposed sparse encoding method for TAQ exhibits\ngood generalization ability and can facilitate manifold tasks; and (4) the\ninfluence of stochastic drift on prediction accuracy can be alleviated by\nincreasing historical samples.",
          "link": "http://arxiv.org/abs/2107.00534",
          "publishedOn": "2021-07-02T01:58:01.489Z",
          "wordCount": 718,
          "title": "The Limit Order Book Recreation Model (LOBRM): An Extended Analysis. (arXiv:2107.00534v1 [q-fin.TR])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00096",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Notin_P/0/1/0/all/0/1\">Pascal Notin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hernandez_Lobato_J/0/1/0/all/0/1\">Jos&#xe9; Miguel Hern&#xe1;ndez-Lobato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gal_Y/0/1/0/all/0/1\">Yarin Gal</a>",
          "description": "Optimization in the latent space of variational autoencoders is a promising\napproach to generate high-dimensional discrete objects that maximize an\nexpensive black-box property (e.g., drug-likeness in molecular generation,\nfunction approximation with arithmetic expressions). However, existing methods\nlack robustness as they may decide to explore areas of the latent space for\nwhich no data was available during training and where the decoder can be\nunreliable, leading to the generation of unrealistic or invalid objects. We\npropose to leverage the epistemic uncertainty of the decoder to guide the\noptimization process. This is not trivial though, as a naive estimation of\nuncertainty in the high-dimensional and structured settings we consider would\nresult in high estimator variance. To solve this problem, we introduce an\nimportance sampling-based estimator that provides more robust estimates of\nepistemic uncertainty. Our uncertainty-guided optimization approach does not\nrequire modifications of the model architecture nor the training process. It\nproduces samples with a better trade-off between black-box objective and\nvalidity of the generated samples, sometimes improving both simultaneously. We\nillustrate these advantages across several experimental settings in digit\ngeneration, arithmetic expression approximation and molecule generation for\ndrug design.",
          "link": "http://arxiv.org/abs/2107.00096",
          "publishedOn": "2021-07-02T01:58:01.482Z",
          "wordCount": 624,
          "title": "Improving black-box optimization in VAE latent space using decoder uncertainty. (arXiv:2107.00096v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2006.09365",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Karimireddy_S/0/1/0/all/0/1\">Sai Praneeth Karimireddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Lie He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaggi_M/0/1/0/all/0/1\">Martin Jaggi</a>",
          "description": "In Byzantine robust distributed or federated learning, a central server wants\nto train a machine learning model over data distributed across multiple\nworkers. However, a fraction of these workers may deviate from the prescribed\nalgorithm and send arbitrary messages. While this problem has received\nsignificant attention recently, most current defenses assume that the workers\nhave identical data. For realistic cases when the data across workers are\nheterogeneous (non-iid), we design new attacks which circumvent current\ndefenses, leading to significant loss of performance. We then propose a simple\nresampling scheme that adapts existing robust algorithms to heterogeneous\ndatasets at a negligible computational cost. We also theoretically and\nexperimentally validate our approach, showing that combining resampling with\nexisting robust algorithms is effective against challenging attacks. Our work\nis the first to establish guaranteed convergence for the non-iid Byzantine\nrobust problem under realistic assumptions.",
          "link": "http://arxiv.org/abs/2006.09365",
          "publishedOn": "2021-07-02T01:58:01.475Z",
          "wordCount": 632,
          "title": "Byzantine-Robust Learning on Heterogeneous Datasets via Resampling. (arXiv:2006.09365v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.00642",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hwang_W/0/1/0/all/0/1\">Wonseok Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yim_J/0/1/0/all/0/1\">Jinyeong Yim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Seunghyun Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Sohee Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_M/0/1/0/all/0/1\">Minjoon Seo</a>",
          "description": "Information Extraction (IE) for semi-structured document images is often\napproached as a sequence tagging problem by classifying each recognized input\ntoken into one of the IOB (Inside, Outside, and Beginning) categories. However,\nsuch problem setup has two inherent limitations that (1) it cannot easily\nhandle complex spatial relationships and (2) it is not suitable for highly\nstructured information, which are nevertheless frequently observed in\nreal-world document images. To tackle these issues, we first formulate the IE\ntask as spatial dependency parsing problem that focuses on the relationship\namong text tokens in the documents. Under this setup, we then propose SPADE\n(SPAtial DEpendency parser) that models highly complex spatial relationships\nand an arbitrary number of information layers in the documents in an end-to-end\nmanner. We evaluate it on various kinds of documents such as receipts, name\ncards, forms, and invoices, and show that it achieves a similar or better\nperformance compared to strong baselines including BERT-based IOB taggger.",
          "link": "http://arxiv.org/abs/2005.00642",
          "publishedOn": "2021-07-02T01:58:01.452Z",
          "wordCount": 640,
          "title": "Spatial Dependency Parsing for Semi-Structured Document Information Extraction. (arXiv:2005.00642v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00055",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dong_R/0/1/0/all/0/1\">Roy Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ratliff_L/0/1/0/all/0/1\">Lillian J. Ratliff</a>",
          "description": "As data-driven methods are deployed in real-world settings, the processes\nthat generate the observed data will often react to the decisions of the\nlearner. For example, a data source may have some incentive for the algorithm\nto provide a particular label (e.g. approve a bank loan), and manipulate their\nfeatures accordingly. Work in strategic classification and decision-dependent\ndistributions seeks to characterize the closed-loop behavior of deploying\nlearning algorithms by explicitly considering the effect of the classifier on\nthe underlying data distribution. More recently, works in performative\nprediction seek to classify the closed-loop behavior by considering general\nproperties of the mapping from classifier to data distribution, rather than an\nexplicit form. Building on this notion, we analyze repeated risk minimization\nas the perturbed trajectories of the gradient flows of performative risk\nminimization. We consider the case where there may be multiple local minimizers\nof performative risk, motivated by real world situations where the initial\nconditions may have significant impact on the long-term behavior of the system.\nAs a motivating example, we consider a company whose current employee\ndemographics affect the applicant pool they interview: the initial demographics\nof the company can affect the long-term hiring policies of the company. We\nprovide sufficient conditions to characterize the region of attraction for the\nvarious equilibria in this settings. Additionally, we introduce the notion of\nperformative alignment, which provides a geometric condition on the convergence\nof repeated risk minimization to performative risk minimizers.",
          "link": "http://arxiv.org/abs/2107.00055",
          "publishedOn": "2021-07-02T01:58:01.439Z",
          "wordCount": 684,
          "title": "Which Echo Chamber? Regions of Attraction in Learning with Decision-Dependent Distributions. (arXiv:2107.00055v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2012.15566",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Warrington_A/0/1/0/all/0/1\">Andrew Warrington</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lavington_J/0/1/0/all/0/1\">J. Wilder Lavington</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scibior_A/0/1/0/all/0/1\">Adam &#x15a;cibior</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_M/0/1/0/all/0/1\">Mark Schmidt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wood_F/0/1/0/all/0/1\">Frank Wood</a>",
          "description": "Policies for partially observed Markov decision processes can be efficiently\nlearned by imitating policies for the corresponding fully observed Markov\ndecision processes. Unfortunately, existing approaches for this kind of\nimitation learning have a serious flaw: the expert does not know what the\ntrainee cannot see, and so may encourage actions that are sub-optimal, even\nunsafe, under partial information. We derive an objective to instead train the\nexpert to maximize the expected reward of the imitating agent policy, and use\nit to construct an efficient algorithm, adaptive asymmetric DAgger (A2D), that\njointly trains the expert and the agent. We show that A2D produces an expert\npolicy that the agent can safely imitate, in turn outperforming policies\nlearned by imitating a fixed expert.",
          "link": "http://arxiv.org/abs/2012.15566",
          "publishedOn": "2021-07-02T01:58:01.422Z",
          "wordCount": 593,
          "title": "Robust Asymmetric Learning in POMDPs. (arXiv:2012.15566v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00204",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1\">Wenjun Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yi Liu</a>",
          "description": "In membership/subscriber acquisition and retention, we sometimes need to\nrecommend marketing content for multiple pages in sequence. Different from\ngeneral sequential decision making process, the use cases have a simpler flow\nwhere customers per seeing recommended content on each page can only return\nfeedback as moving forward in the process or dropping from it until a\ntermination state. We refer to this type of problems as sequential decision\nmaking in linear--flow. We propose to formulate the problem as an MDP with\nBandits where Bandits are employed to model the transition probability matrix.\nAt recommendation time, we use Thompson sampling (TS) to sample the transition\nprobabilities and allocate the best series of actions with analytical solution\nthrough exact dynamic programming. The way that we formulate the problem allows\nus to leverage TS's efficiency in balancing exploration and exploitation and\nBandit's convenience in modeling actions' incompatibility. In the simulation\nstudy, we observe the proposed MDP with Bandits algorithm outperforms\nQ-learning with $\\epsilon$-greedy and decreasing $\\epsilon$, independent\nBandits, and interaction Bandits. We also find the proposed algorithm's\nperformance is the most robust to changes in the across-page interdependence\nstrength.",
          "link": "http://arxiv.org/abs/2107.00204",
          "publishedOn": "2021-07-02T01:58:01.413Z",
          "wordCount": 638,
          "title": "Markov Decision Process modeled with Bandits for Sequential Decision Making in Linear-flow. (arXiv:2107.00204v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.04315",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Beik_Mohammadi_H/0/1/0/all/0/1\">Hadi Beik-Mohammadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hauberg_S/0/1/0/all/0/1\">S&#xf8;ren Hauberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arvanitidis_G/0/1/0/all/0/1\">Georgios Arvanitidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neumann_G/0/1/0/all/0/1\">Gerhard Neumann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rozo_L/0/1/0/all/0/1\">Leonel Rozo</a>",
          "description": "For robots to work alongside humans and perform in unstructured environments,\nthey must learn new motion skills and adapt them to unseen situations on the\nfly. This demands learning models that capture relevant motion patterns, while\noffering enough flexibility to adapt the encoded skills to new requirements,\nsuch as dynamic obstacle avoidance. We introduce a Riemannian manifold\nperspective on this problem, and propose to learn a Riemannian manifold from\nhuman demonstrations on which geodesics are natural motion skills. We realize\nthis with a variational autoencoder (VAE) over the space of position and\norientations of the robot end-effector. Geodesic motion skills let a robot plan\nmovements from and to arbitrary points on the data manifold. They also provide\na straightforward method to avoid obstacles by redefining the ambient metric in\nan online fashion. Moreover, geodesics naturally exploit the manifold resulting\nfrom multiple--mode tasks to design motions that were not explicitly\ndemonstrated previously. We test our learning framework using a 7-DoF robotic\nmanipulator, where the robot satisfactorily learns and reproduces realistic\nskills featuring elaborated motion patterns, avoids previously unseen\nobstacles, and generates novel movements in multiple-mode settings.",
          "link": "http://arxiv.org/abs/2106.04315",
          "publishedOn": "2021-07-02T01:58:01.396Z",
          "wordCount": 638,
          "title": "Learning Riemannian Manifolds for Geodesic Motion Skills. (arXiv:2106.04315v2 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00052",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Loizou_N/0/1/0/all/0/1\">Nicolas Loizou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berard_H/0/1/0/all/0/1\">Hugo Berard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gidel_G/0/1/0/all/0/1\">Gauthier Gidel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitliagkas_I/0/1/0/all/0/1\">Ioannis Mitliagkas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lacoste_Julien_S/0/1/0/all/0/1\">Simon Lacoste-Julien</a>",
          "description": "Two of the most prominent algorithms for solving unconstrained smooth games\nare the classical stochastic gradient descent-ascent (SGDA) and the recently\nintroduced stochastic consensus optimization (SCO) (Mescheder et al., 2017).\nSGDA is known to converge to a stationary point for specific classes of games,\nbut current convergence analyses require a bounded variance assumption. SCO is\nused successfully for solving large-scale adversarial problems, but its\nconvergence guarantees are limited to its deterministic variant. In this work,\nwe introduce the expected co-coercivity condition, explain its benefits, and\nprovide the first last-iterate convergence guarantees of SGDA and SCO under\nthis condition for solving a class of stochastic variational inequality\nproblems that are potentially non-monotone. We prove linear convergence of both\nmethods to a neighborhood of the solution when they use constant step-size, and\nwe propose insightful stepsize-switching rules to guarantee convergence to the\nexact solution. In addition, our convergence guarantees hold under the\narbitrary sampling paradigm, and as such, we give insights into the complexity\nof minibatching.",
          "link": "http://arxiv.org/abs/2107.00052",
          "publishedOn": "2021-07-02T01:58:01.389Z",
          "wordCount": 633,
          "title": "Stochastic Gradient Descent-Ascent and Consensus Optimization for Smooth Games: Convergence Analysis under Expected Co-coercivity. (arXiv:2107.00052v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00360",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Schaaf_N/0/1/0/all/0/1\">Nina Schaaf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitri_O/0/1/0/all/0/1\">Omar de Mitri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hang Beom Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Windberger_A/0/1/0/all/0/1\">Alexander Windberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huber_M/0/1/0/all/0/1\">Marco F. Huber</a>",
          "description": "Convolutional Neural Networks (CNN) have become de fact state-of-the-art for\nthe main computer vision tasks. However, due to the complex underlying\nstructure their decisions are hard to understand which limits their use in some\ncontext of the industrial world. A common and hard to detect challenge in\nmachine learning (ML) tasks is data bias. In this work, we present a systematic\napproach to uncover data bias by means of attribution maps. For this purpose,\nfirst an artificial dataset with a known bias is created and used to train\nintentionally biased CNNs. The networks' decisions are then inspected using\nattribution maps. Finally, meaningful metrics are used to measure the\nattribution maps' representativeness with respect to the known bias. The\nproposed study shows that some attribution map techniques highlight the\npresence of bias in the data better than others and metrics can support the\nidentification of bias.",
          "link": "http://arxiv.org/abs/2107.00360",
          "publishedOn": "2021-07-02T01:58:01.380Z",
          "wordCount": 606,
          "title": "Towards Measuring Bias in Image Classification. (arXiv:2107.00360v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09330",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nissani_D/0/1/0/all/0/1\">Daniel N. Nissani</a> (Nissensohn)",
          "description": "Generative neural networks are able to mimic intricate probability\ndistributions such as those of handwritten text, natural images, etc. Since\ntheir inception several models were proposed. The most successful of these were\nbased on adversarial (GAN), auto-encoding (VAE) and maximum mean discrepancy\n(MMD) relatively complex architectures and schemes. Surprisingly, a very simple\narchitecture (a single feed-forward neural network) in conjunction with an\nobvious optimization goal (Kullback_Leibler divergence) was apparently\noverlooked. This paper demonstrates that such a model (denoted SGN for its\nsimplicity) is able to generate samples visually and quantitatively competitive\nas compared with the fore-mentioned state of the art methods.",
          "link": "http://arxiv.org/abs/2106.09330",
          "publishedOn": "2021-07-02T01:58:01.362Z",
          "wordCount": 540,
          "title": "A Simple Generative Network. (arXiv:2106.09330v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.13807",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shengyi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ontanon_S/0/1/0/all/0/1\">Santiago Onta&#xf1;&#xf3;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bamford_C/0/1/0/all/0/1\">Chris Bamford</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grela_L/0/1/0/all/0/1\">Lukasz Grela</a>",
          "description": "In recent years, researchers have achieved great success in applying Deep\nReinforcement Learning (DRL) algorithms to Real-time Strategy (RTS) games,\ncreating strong autonomous agents that could defeat professional players in\nStarCraft~II. However, existing approaches to tackle full games have high\ncomputational costs, usually requiring the use of thousands of GPUs and CPUs\nfor weeks. This paper has two main contributions to address this issue: 1) We\nintroduce Gym-$\\mu$RTS (pronounced \"gym-micro-RTS\") as a fast-to-run RL\nenvironment for full-game RTS research and 2) we present a collection of\ntechniques to scale DRL to play full-game $\\mu$RTS as well as ablation studies\nto demonstrate their empirical importance. Our best-trained bot can defeat\nevery $\\mu$RTS bot we tested from the past $\\mu$RTS competitions when working\nin a single-map setting, resulting in a state-of-the-art DRL agent while only\ntaking about 60 hours of training using a single machine (one GPU, three vCPU,\n16GB RAM).",
          "link": "http://arxiv.org/abs/2105.13807",
          "publishedOn": "2021-07-02T01:58:01.334Z",
          "wordCount": 625,
          "title": "Gym-$\\mu$RTS: Toward Affordable Full Game Real-time Strategy Games Research with Deep Reinforcement Learning. (arXiv:2105.13807v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00124",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ganesan_A/0/1/0/all/0/1\">Ashwinkumar Ganesan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferraro_F/0/1/0/all/0/1\">Francis Ferraro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oates_T/0/1/0/all/0/1\">Tim Oates</a>",
          "description": "We propose a Bi-Directional Manifold Alignment (BDMA) that learns a\nnon-linear mapping between two manifolds by explicitly training it to be\nbijective. We demonstrate BDMA by training a model for a pair of languages\nrather than individual, directed source and target combinations, reducing the\nnumber of models by 50%. We show that models trained with BDMA in the \"forward\"\n(source to target) direction can successfully map words in the \"reverse\"\n(target to source) direction, yielding equivalent (or better) performance to\nstandard unidirectional translation models where the source and target language\nis flipped. We also show how BDMA reduces the overall size of the model.",
          "link": "http://arxiv.org/abs/2107.00124",
          "publishedOn": "2021-07-02T01:58:01.326Z",
          "wordCount": 555,
          "title": "Learning a Reversible Embedding Mapping using Bi-Directional Manifold Alignment. (arXiv:2107.00124v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2004.00601",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Garrido_Merchan_E/0/1/0/all/0/1\">Eduardo C. Garrido-Merch&#xe1;n</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Hernandez_Lobato_D/0/1/0/all/0/1\">Daniel Hern&#xe1;ndez-Lobato</a>",
          "description": "Real-world problems often involve the optimization of several objectives\nunder multiple constraints. An example is the hyper-parameter tuning problem of\nmachine learning algorithms. In particular, the minimization of the estimation\nof the generalization error of a deep neural network and at the same time the\nminimization of its prediction time. We may also consider as a constraint that\nthe deep neural network must be implemented in a chip with an area below some\nsize. Here, both the objectives and the constraint are black boxes, i.e.,\nfunctions whose analytical expressions are unknown and are expensive to\nevaluate. Bayesian optimization (BO) methodologies have given state-of-the-art\nresults for the optimization of black-boxes. Nevertheless, most BO methods are\nsequential and evaluate the objectives and the constraints at just one input\nlocation, iteratively. Sometimes, however, we may have resources to evaluate\nseveral configurations in parallel. Notwithstanding, no parallel BO method has\nbeen proposed to deal with the optimization of multiple objectives under\nseveral constraints. If the expensive evaluations can be carried out in\nparallel (as when a cluster of computers is available), sequential evaluations\nresult in a waste of resources. This article introduces PPESMOC, Parallel\nPredictive Entropy Search for Multi-objective Bayesian Optimization with\nConstraints, an information-based batch method for the simultaneous\noptimization of multiple expensive-to-evaluate black-box functions under the\npresence of several constraints. Iteratively, PPESMOC selects a batch of input\nlocations at which to evaluate the black-boxes so as to maximally reduce the\nentropy of the Pareto set of the optimization problem. We present empirical\nevidence in the form of synthetic, benchmark and real-world experiments that\nillustrate the effectiveness of PPESMOC.",
          "link": "http://arxiv.org/abs/2004.00601",
          "publishedOn": "2021-07-02T01:58:01.319Z",
          "wordCount": 725,
          "title": "Parallel Predictive Entropy Search for Multi-objective Bayesian Optimization with Constraints. (arXiv:2004.00601v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00230",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Binghui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xin_S/0/1/0/all/0/1\">Shiji Xin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qizhe Zhang</a>",
          "description": "The neural network with $1$-Lipschitz property based on $\\ell_\\infty$-dist\nneuron has a theoretical guarantee in certified $\\ell_\\infty$ robustness.\nHowever, due to the inherent difficulties in the training of the network, the\ncertified accuracy of previous work is limited. In this paper, we propose two\napproaches to deal with these difficuties. Aiming at the characteristics of the\ntraining process based on $\\ell_\\infty$-norm neural network, we introduce the\nEMA method to improve the training process. Considering the randomness of the\ntraining algorithm, we propose an ensemble method based on trained base models\nthat have the $1$-Lipschitz property and gain significant improvement in the\nsmall parameter network. Moreover, we give the theoretical analysis of the\nensemble method based on the $1$-Lipschitz property on the certified\nrobustness, which ensures the effectiveness and stability of the algorithm. Our\ncode is available at\nhttps://github.com/Theia-4869/EMA-and-Ensemble-Lip-Networks.",
          "link": "http://arxiv.org/abs/2107.00230",
          "publishedOn": "2021-07-02T01:58:01.311Z",
          "wordCount": 572,
          "title": "Boosting Certified $\\ell_\\infty$ Robustness with EMA Method and Ensemble Model. (arXiv:2107.00230v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00092",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_G/0/1/0/all/0/1\">Gaurab Bhattacharya</a>",
          "description": "In recent times, the trend in very large scale integration (VLSI) industry is\nmulti-dimensional, for example, reduction of energy consumption, occupancy of\nless space, precise result, less power dissipation, faster response. To meet\nthese needs, the hardware architecture should be reliable and robust to these\nproblems. Recently, neural network and deep learning has been started to impact\nthe present research paradigm significantly which consists of parameters in the\norder of millions, nonlinear function for activation, convolutional operation\nfor feature extraction, regression for classification, generative adversarial\nnetworks. These operations involve huge calculation and memory overhead.\nPresently available DSP processors are incapable of performing these operations\nand they mostly face the problems, for example, memory overhead, performance\ndrop and compromised accuracy. Moreover, if a huge silicon area is powered to\naccelerate the operation using parallel computation, the ICs will be having\nsignificant chance of burning out due to the considerable generation of heat.\nHence, novel dark silicon constraint is developed to reduce the heat\ndissipation without sacrificing the accuracy. Similarly, different algorithms\nhave been adapted to design a DSP processor compatible for fast performance in\nneural network, activation function, convolutional neural network and\ngenerative adversarial network. In this review, we illustrate the recent\ndevelopments in hardware for accelerating the efficient implementation of deep\nlearning networks with enhanced performance. The techniques investigated in\nthis review are expected to direct future research challenges of hardware\noptimization for high-performance computations.",
          "link": "http://arxiv.org/abs/2107.00092",
          "publishedOn": "2021-07-02T01:58:01.287Z",
          "wordCount": 671,
          "title": "From DNNs to GANs: Review of efficient hardware architectures for deep learning. (arXiv:2107.00092v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00272",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ahmedt_Aristizabal_D/0/1/0/all/0/1\">David Ahmedt-Aristizabal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Armin_M/0/1/0/all/0/1\">Mohammad Ali Armin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denman_S/0/1/0/all/0/1\">Simon Denman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fookes_C/0/1/0/all/0/1\">Clinton Fookes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petersson_L/0/1/0/all/0/1\">Lars Petersson</a>",
          "description": "With the remarkable success of representation learning for prediction\nproblems, we have witnessed a rapid expansion of the use of machine learning\nand deep learning for the analysis of digital pathology and biopsy image\npatches. However, traditional learning over patch-wise features using\nconvolutional neural networks limits the model when attempting to capture\nglobal contextual information. The phenotypical and topological distribution of\nconstituent histological entities play a critical role in tissue diagnosis. As\nsuch, graph data representations and deep learning have attracted significant\nattention for encoding tissue representations, and capturing intra- and inter-\nentity level interactions. In this review, we provide a conceptual grounding of\ngraph-based deep learning and discuss its current success for tumor\nlocalization and classification, tumor invasion and staging, image retrieval,\nand survival prediction. We provide an overview of these methods in a\nsystematic manner organized by the graph representation of the input image\nincluding whole slide images and tissue microarrays. We also outline the\nlimitations of existing techniques, and suggest potential future advances in\nthis domain.",
          "link": "http://arxiv.org/abs/2107.00272",
          "publishedOn": "2021-07-02T01:58:01.185Z",
          "wordCount": 617,
          "title": "A Survey on Graph-Based Deep Learning for Computational Histopathology. (arXiv:2107.00272v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00101",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xinyun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1\">Dawn Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yuandong Tian</a>",
          "description": "Program synthesis from input-output examples has been a long-standing\nchallenge, and recent works have demonstrated some success in designing deep\nneural networks for program synthesis. However, existing efforts in\ninput-output neural program synthesis have been focusing on domain-specific\nlanguages, thus the applicability of previous approaches to synthesize code in\nfull-fledged popular programming languages, such as C, remains a question. The\nmain challenges lie in two folds. On the one hand, the program search space\ngrows exponentially when the syntax and semantics of the programming language\nbecome more complex, which poses higher requirements on the synthesis\nalgorithm. On the other hand, increasing the complexity of the programming\nlanguage also imposes more difficulties on data collection, since building a\nlarge-scale training set for input-output program synthesis require random\nprogram generators to sample programs and input-output examples. In this work,\nwe take the first step to synthesize C programs from input-output examples. In\nparticular, we propose LaSynth, which learns the latent representation to\napproximate the execution of partially generated programs, even if their\nsemantics are not well-defined. We demonstrate the possibility of synthesizing\nelementary C code from input-output examples, and leveraging learned execution\nsignificantly improves the prediction performance over existing approaches.\nMeanwhile, compared to the randomly generated ground-truth programs, LaSynth\nsynthesizes more concise programs that resemble human-written code. We show\nthat training on these synthesized programs further improves the prediction\nperformance for both Karel and C program synthesis, indicating the promise of\nleveraging the learned program synthesizer to improve the dataset quality for\ninput-output program synthesis.",
          "link": "http://arxiv.org/abs/2107.00101",
          "publishedOn": "2021-07-02T01:58:01.152Z",
          "wordCount": 691,
          "title": "Latent Execution for Neural Program Synthesis Beyond Domain-Specific Languages. (arXiv:2107.00101v1 [cs.PL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00228",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fischer_M/0/1/0/all/0/1\">Marc Fischer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baader_M/0/1/0/all/0/1\">Maximilian Baader</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vechev_M/0/1/0/all/0/1\">Martin Vechev</a>",
          "description": "We present a new certification method for image and point cloud segmentation\nbased on randomized smoothing. The method leverages a novel scalable algorithm\nfor prediction and certification that correctly accounts for multiple testing,\nnecessary for ensuring statistical guarantees. The key to our approach is\nreliance on established multiple-testing correction mechanisms as well as the\nability to abstain from classifying single pixels or points while still\nrobustly segmenting the overall input. Our experimental evaluation on synthetic\ndata and challenging datasets, such as Pascal Context, Cityscapes, and\nShapeNet, shows that our algorithm can achieve, for the first time, competitive\naccuracy and certification guarantees on real-world segmentation tasks. We\nprovide an implementation at https://github.com/eth-sri/segmentation-smoothing.",
          "link": "http://arxiv.org/abs/2107.00228",
          "publishedOn": "2021-07-02T01:58:01.137Z",
          "wordCount": 545,
          "title": "Scalable Certified Segmentation via Randomized Smoothing. (arXiv:2107.00228v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00070",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1\">Shihao Ji</a>",
          "description": "Training deep neural networks with an $L_0$ regularization is one of the\nprominent approaches for network pruning or sparsification. The method prunes\nthe network during training by encouraging weights to become exactly zero.\nHowever, recent work of Gale et al. reveals that although this method yields\nhigh compression rates on smaller datasets, it performs inconsistently on\nlarge-scale learning tasks, such as ResNet50 on ImageNet. We analyze this\nphenomenon through the lens of variational inference and find that it is likely\ndue to the independent modeling of binary gates, the mean-field approximation,\nwhich is known in Bayesian statistics for its poor performance due to the crude\napproximation. To mitigate this deficiency, we propose a dependency modeling of\nbinary gates, which can be modeled effectively as a multi-layer perceptron\n(MLP). We term our algorithm Dep-$L_0$ as it prunes networks via a\ndependency-enabled $L_0$ regularization. Extensive experiments on CIFAR10,\nCIFAR100 and ImageNet with VGG16, ResNet50, ResNet56 show that our Dep-$L_0$\noutperforms the original $L_0$-HC algorithm of Louizos et al. by a significant\nmargin, especially on ImageNet. Compared with the state-of-the-arts network\nsparsification algorithms, our dependency modeling makes the $L_0$-based\nsparsification once again very competitive on large-scale learning tasks. Our\nsource code is available at https://github.com/leo-yangli/dep-l0.",
          "link": "http://arxiv.org/abs/2107.00070",
          "publishedOn": "2021-07-02T01:58:01.131Z",
          "wordCount": 646,
          "title": "Dep-$L_0$: Improving $L_0$-based Network Sparsification via Dependency Modeling. (arXiv:2107.00070v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00195",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Li_W/0/1/0/all/0/1\">Wei-Ming Li</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Ran_S/0/1/0/all/0/1\">Shi-Ju Ran</a>",
          "description": "In quantum and quantum-inspired machine learning, the very first step is to\nembed the data in quantum space known as Hilbert space. Developing quantum\nkernel function (QKF), which defines the distances among the samples in the\nHilbert space, belongs to the fundamental topics for machine learning. In this\nwork, we propose the rescaled logarithmic fidelity (RLF) and a non-parametric\nactive learning in the quantum space, which we name as RLF-NAL. The rescaling\ntakes advantage of the non-linearity of the kernel to tune the mutual distances\nof samples in the Hilbert space, and meanwhile avoids the exponentially-small\nfidelities between quantum many-qubit states. We compare RLF-NAL with several\nwell-known non-parametric algorithms including naive Bayes classifiers,\n$k$-nearest neighbors, and spectral clustering. Our method exhibits excellent\naccuracy particularly for the unsupervised case with no labeled samples and the\nfew-shot cases with small numbers of labeled samples. With the visualizations\nby t-SNE, our results imply that the machine learning in the Hilbert space\ncomplies with the principles of maximal coding rate reduction, where the\nlow-dimensional data exhibit within-class compressibility, between-class\ndiscrimination, and overall diversity. Our proposals can be applied to other\nquantum and quantum-inspired machine learning, including the methods using the\nparametric models such as tensor networks, quantum circuits, and quantum neural\nnetworks.",
          "link": "http://arxiv.org/abs/2107.00195",
          "publishedOn": "2021-07-02T01:58:01.110Z",
          "wordCount": 654,
          "title": "Non-parametric Active Learning and Rate Reduction in Many-body Hilbert Space with Rescaled Logarithmic Fidelity. (arXiv:2107.00195v1 [quant-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00243",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wenger_J/0/1/0/all/0/1\">Jonathan Wenger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pleiss_G/0/1/0/all/0/1\">Geoff Pleiss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hennig_P/0/1/0/all/0/1\">Philipp Hennig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cunningham_J/0/1/0/all/0/1\">John P. Cunningham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gardner_J/0/1/0/all/0/1\">Jacob R. Gardner</a>",
          "description": "Gaussian processes remain popular as a flexible and expressive model class,\nbut the computational cost of kernel hyperparameter optimization stands as a\nmajor limiting factor to their scaling and broader adoption. Recent work has\nmade great strides combining stochastic estimation with iterative numerical\ntechniques, essentially boiling down GP inference to the cost of (many)\nmatrix-vector multiplies. Preconditioning -- a highly effective step for any\niterative method involving matrix-vector multiplication -- can be used to\naccelerate convergence and thus reduce bias in hyperparameter optimization.\nHere, we prove that preconditioning has an additional benefit that has been\npreviously unexplored. It not only reduces the bias of the $\\log$-marginal\nlikelihood estimator and its derivatives, but it also simultaneously can reduce\nvariance at essentially negligible cost. We leverage this result to derive\nsample-efficient algorithms for GP hyperparameter optimization requiring as few\nas $\\mathcal{O}(\\log(\\varepsilon^{-1}))$ instead of\n$\\mathcal{O}(\\varepsilon^{-2})$ samples to achieve error $\\varepsilon$. Our\ntheoretical results enable provably efficient and scalable optimization of\nkernel hyperparameters, which we validate empirically on a set of large-scale\nbenchmark problems. There, variance reduction via preconditioning results in an\norder of magnitude speedup in hyperparameter optimization of exact GPs.",
          "link": "http://arxiv.org/abs/2107.00243",
          "publishedOn": "2021-07-02T01:58:01.103Z",
          "wordCount": 632,
          "title": "Reducing the Variance of Gaussian Process Hyperparameter Optimization with Preconditioning. (arXiv:2107.00243v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.08775",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Kim_J/0/1/0/all/0/1\">Junhyung Lyle Kim</a>, <a href=\"http://arxiv.org/find/math/1/au:+Benitez_J/0/1/0/all/0/1\">Jose Antonio Lara Benitez</a>, <a href=\"http://arxiv.org/find/math/1/au:+Toghani_M/0/1/0/all/0/1\">Mohammad Taha Toghani</a>, <a href=\"http://arxiv.org/find/math/1/au:+Wolfe_C/0/1/0/all/0/1\">Cameron Wolfe</a>, <a href=\"http://arxiv.org/find/math/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhiwei Zhang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Kyrillidis_A/0/1/0/all/0/1\">Anastasios Kyrillidis</a>",
          "description": "We present a novel, practical, and provable approach for solving diagonally\nconstrained semi-definite programming (SDP) problems at scale using accelerated\nnon-convex programming. Our algorithm non-trivially combines acceleration\nmotions from convex optimization with coordinate power iteration and matrix\nfactorization techniques. The algorithm is extremely simple to implement, and\nadds only a single extra hyperparameter -- momentum. We prove that our method\nadmits local linear convergence in the neighborhood of the optimum and always\nconverges to a first-order critical point. Experimentally, we showcase the\nmerits of our method on three major application domains: MaxCut, MaxSAT, and\nMIMO signal detection. In all cases, our methodology provides significant\nspeedups over non-convex and convex SDP solvers -- 5X faster than\nstate-of-the-art non-convex solvers, and 9 to 10^3 X faster than convex SDP\nsolvers -- with comparable or improved solution quality.",
          "link": "http://arxiv.org/abs/2106.08775",
          "publishedOn": "2021-07-02T01:58:01.094Z",
          "wordCount": 607,
          "title": "Momentum-inspired Low-Rank Coordinate Descent for Diagonally Constrained SDPs. (arXiv:2106.08775v1 [math.OC] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00219",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Brian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_M/0/1/0/all/0/1\">Miaolan Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Udell_M/0/1/0/all/0/1\">Madeleine Udell</a>",
          "description": "Tree ensembles distribute feature importance evenly amongst groups of\ncorrelated features. The average feature ranking of the correlated group is\nsuppressed, which reduces interpretability and complicates feature selection.\nIn this paper we present ControlBurn, a feature selection algorithm that uses a\nweighted LASSO-based feature selection method to prune unnecessary features\nfrom tree ensembles, just as low-intensity fire reduces overgrown vegetation.\nLike the linear LASSO, ControlBurn assigns all the feature importance of a\ncorrelated group of features to a single feature. Moreover, the algorithm is\nefficient and only requires a single training iteration to run, unlike\niterative wrapper-based feature selection methods. We show that ControlBurn\nperforms substantially better than feature selection methods with comparable\ncomputational costs on datasets with correlated features.",
          "link": "http://arxiv.org/abs/2107.00219",
          "publishedOn": "2021-07-02T01:58:01.087Z",
          "wordCount": 552,
          "title": "ControlBurn: Feature Selection by Sparse Forests. (arXiv:2107.00219v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00181",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhen Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1\">Xu Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_J/0/1/0/all/0/1\">Jun Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tongliang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_X/0/1/0/all/0/1\">Xinmei Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Houqiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_B/0/1/0/all/0/1\">Bing Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jianqiang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_X/0/1/0/all/0/1\">Xian-Sheng Hua</a>",
          "description": "Knowledge Distillation (KD) is a popular technique to transfer knowledge from\na teacher model or ensemble to a student model. Its success is generally\nattributed to the privileged information on similarities/consistency between\nthe class distributions or intermediate feature representations of the teacher\nmodel and the student model. However, directly pushing the student model to\nmimic the probabilities/features of the teacher model to a large extent limits\nthe student model in learning undiscovered knowledge/features. In this paper,\nwe propose a novel inheritance and exploration knowledge distillation framework\n(IE-KD), in which a student model is split into two parts - inheritance and\nexploration. The inheritance part is learned with a similarity loss to transfer\nthe existing learned knowledge from the teacher model to the student model,\nwhile the exploration part is encouraged to learn representations different\nfrom the inherited ones with a dis-similarity loss. Our IE-KD framework is\ngeneric and can be easily combined with existing distillation or mutual\nlearning methods for training deep neural networks. Extensive experiments\ndemonstrate that these two parts can jointly push the student model to learn\nmore diversified and effective representations, and our IE-KD can be a general\ntechnique to improve the student network to achieve SOTA performance.\nFurthermore, by applying our IE-KD to the training of two networks, the\nperformance of both can be improved w.r.t. deep mutual learning. The code and\nmodels of IE-KD will be make publicly available at\nhttps://github.com/yellowtownhz/IE-KD.",
          "link": "http://arxiv.org/abs/2107.00181",
          "publishedOn": "2021-07-02T01:58:01.080Z",
          "wordCount": 693,
          "title": "Revisiting Knowledge Distillation: An Inheritance and Exploration Framework. (arXiv:2107.00181v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00068",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zixiu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yiwen Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_H/0/1/0/all/0/1\">Hu Ding</a>",
          "description": "In this big data era, we often confront large-scale data in many machine\nlearning tasks. A common approach for dealing with large-scale data is to build\na small summary, {\\em e.g.,} coreset, that can efficiently represent the\noriginal input. However, real-world datasets usually contain outliers and most\nexisting coreset construction methods are not resilient against outliers (in\nparticular, the outliers can be located arbitrarily in the space by an\nadversarial attacker). In this paper, we propose a novel robust coreset method\nfor the {\\em continuous-and-bounded learning} problem (with outliers) which\nincludes a broad range of popular optimization objectives in machine learning,\nlike logistic regression and $ k $-means clustering. Moreover, our robust\ncoreset can be efficiently maintained in fully-dynamic environment. To the best\nof our knowledge, this is the first robust and fully-dynamic coreset\nconstruction method for these optimization problems. We also conduct the\nexperiments to evaluate the effectiveness of our robust coreset in practice.",
          "link": "http://arxiv.org/abs/2107.00068",
          "publishedOn": "2021-07-02T01:58:01.053Z",
          "wordCount": 595,
          "title": "Robust Coreset for Continuous-and-Bounded Learning (with Outliers). (arXiv:2107.00068v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00359",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Beik_Mohammadi_H/0/1/0/all/0/1\">Hadi Beik-Mohammadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kerzel_M/0/1/0/all/0/1\">Matthias Kerzel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pleintinger_B/0/1/0/all/0/1\">Benedikt Pleintinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hulin_T/0/1/0/all/0/1\">Thomas Hulin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reisich_P/0/1/0/all/0/1\">Philipp Reisich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmidt_A/0/1/0/all/0/1\">Annika Schmidt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pereira_A/0/1/0/all/0/1\">Aaron Pereira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wermter_S/0/1/0/all/0/1\">Stefan Wermter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lii_N/0/1/0/all/0/1\">Neal Y. Lii</a>",
          "description": "Telerobotic systems must adapt to new environmental conditions and deal with\nhigh uncertainty caused by long-time delays. As one of the best alternatives to\nhuman-level intelligence, Reinforcement Learning (RL) may offer a solution to\ncope with these issues. This paper proposes to integrate RL with the Model\nMediated Teleoperation (MMT) concept. The teleoperator interacts with a\nsimulated virtual environment, which provides instant feedback. Whereas\nfeedback from the real environment is delayed, feedback from the model is\ninstantaneous, leading to high transparency. The MMT is realized in combination\nwith an intelligent system with two layers. The first layer utilizes Dynamic\nMovement Primitives (DMP) which accounts for certain changes in the avatar\nenvironment. And, the second layer addresses the problems caused by uncertainty\nin the model using RL methods. Augmented reality was also provided to fuse the\navatar device and virtual environment models for the teleoperator. Implemented\non DLR's Exodex Adam hand-arm haptic exoskeleton, the results show RL methods\nare able to find different solutions when changes are applied to the object\nposition after the demonstration. The results also show DMPs to be effective at\nadapting to new conditions where there is no uncertainty involved.",
          "link": "http://arxiv.org/abs/2107.00359",
          "publishedOn": "2021-07-02T01:58:01.046Z",
          "wordCount": 652,
          "title": "Model Mediated Teleoperation with a Hand-Arm Exoskeleton in Long Time Delays Using Reinforcement Learning. (arXiv:2107.00359v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00283",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Thambawita_V/0/1/0/all/0/1\">Vajira Thambawita</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hicks_S/0/1/0/all/0/1\">Steven A. Hicks</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Halvorsen_P/0/1/0/all/0/1\">P&#xe5;l Halvorsen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Riegler_M/0/1/0/all/0/1\">Michael A. Riegler</a>",
          "description": "Detection of colon polyps has become a trending topic in the intersecting\nfields of machine learning and gastrointestinal endoscopy. The focus has mainly\nbeen on per-frame classification. More recently, polyp segmentation has gained\nattention in the medical community. Segmentation has the advantage of being\nmore accurate than per-frame classification or object detection as it can show\nthe affected area in greater detail. For our contribution to the EndoCV 2021\nsegmentation challenge, we propose two separate approaches. First, a\nsegmentation model named TriUNet composed of three separate UNet models.\nSecond, we combine TriUNet with an ensemble of well-known segmentation models,\nnamely UNet++, FPN, DeepLabv3, and DeepLabv3+, into a model called\nDivergentNets to produce more generalizable medical image segmentation masks.\nIn addition, we propose a modified Dice loss that calculates loss only for a\nsingle class when performing multiclass segmentation, forcing the model to\nfocus on what is most important. Overall, the proposed methods achieved the\nbest average scores for each respective round in the challenge, with TriUNet\nbeing the winning model in Round I and DivergentNets being the winning model in\nRound II of the segmentation generalization challenge at EndoCV 2021. The\nimplementation of our approach is made publicly available on GitHub.",
          "link": "http://arxiv.org/abs/2107.00283",
          "publishedOn": "2021-07-02T01:58:01.038Z",
          "wordCount": 691,
          "title": "DivergentNets: Medical Image Segmentation by Network Ensemble. (arXiv:2107.00283v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00507",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1\">Han-Chih Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jianwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Ching-Seh Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stamp_M/0/1/0/all/0/1\">Mark Stamp</a>",
          "description": "Keystroke dynamics can be used to analyze the way that users type by\nmeasuring various aspects of keyboard input. Previous work has demonstrated the\nfeasibility of user authentication and identification utilizing keystroke\ndynamics. In this research, we consider a wide variety of machine learning and\ndeep learning techniques based on fixed-text keystroke-derived features, we\noptimize the resulting models, and we compare our results to those obtained in\nrelated research. We find that models based on extreme gradient boosting\n(XGBoost) and multi-layer perceptrons (MLP)perform well in our experiments. Our\nbest models outperform previous comparable research.",
          "link": "http://arxiv.org/abs/2107.00507",
          "publishedOn": "2021-07-02T01:58:01.030Z",
          "wordCount": 525,
          "title": "Machine Learning and Deep Learning for Fixed-Text Keystroke Dynamics. (arXiv:2107.00507v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00541",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chane_Sane_E/0/1/0/all/0/1\">Elliot Chane-Sane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmid_C/0/1/0/all/0/1\">Cordelia Schmid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laptev_I/0/1/0/all/0/1\">Ivan Laptev</a>",
          "description": "Goal-conditioned reinforcement learning endows an agent with a large variety\nof skills, but it often struggles to solve tasks that require more temporally\nextended reasoning. In this work, we propose to incorporate imagined subgoals\ninto policy learning to facilitate learning of complex tasks. Imagined subgoals\nare predicted by a separate high-level policy, which is trained simultaneously\nwith the policy and its critic. This high-level policy predicts intermediate\nstates halfway to the goal using the value function as a reachability metric.\nWe don't require the policy to reach these subgoals explicitly. Instead, we use\nthem to define a prior policy, and incorporate this prior into a KL-constrained\npolicy iteration scheme to speed up and regularize learning. Imagined subgoals\nare used during policy learning, but not during test time, where we only apply\nthe learned policy. We evaluate our approach on complex robotic navigation and\nmanipulation tasks and show that it outperforms existing methods by a large\nmargin.",
          "link": "http://arxiv.org/abs/2107.00541",
          "publishedOn": "2021-07-02T01:58:01.011Z",
          "wordCount": 593,
          "title": "Goal-Conditioned Reinforcement Learning with Imagined Subgoals. (arXiv:2107.00541v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00247",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hosseini_A/0/1/0/all/0/1\">Alireza Mousavi Hosseini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abouei_A/0/1/0/all/0/1\">Amir Mohammad Abouei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rohban_M/0/1/0/all/0/1\">Mohammad Hossein Rohban</a>",
          "description": "Adversarial training tends to result in models that are less accurate on\nnatural (unperturbed) examples compared to standard models. This can be\nattributed to either an algorithmic shortcoming or a fundamental property of\nthe training data distribution, which admits different solutions for optimal\nstandard and adversarial classifiers. In this work, we focus on the latter case\nunder a binary Gaussian mixture classification problem. Unlike earlier work, we\naim to derive the natural accuracy gap between the optimal Bayes and\nadversarial classifiers, and study the effect of different distributional\nparameters, namely separation between class centroids, class proportions, and\nthe covariance matrix, on the derived gap. We show that under certain\nconditions, the natural error of the optimal adversarial classifier, as well as\nthe gap, are locally minimized when classes are balanced, contradicting the\nperformance of the Bayes classifier where perfect balance induces the worst\naccuracy. Moreover, we show that with an $\\ell_\\infty$ bounded perturbation and\nan adversarial budget of $\\epsilon$, this gap is $\\Theta(\\epsilon^2)$ for the\nworst-case parameters, which for suitably small $\\epsilon$ indicates the\ntheoretical possibility of achieving robust classifiers with near-perfect\naccuracy, which is rarely reflected in practical algorithms.",
          "link": "http://arxiv.org/abs/2107.00247",
          "publishedOn": "2021-07-02T01:58:01.004Z",
          "wordCount": 640,
          "title": "The Interplay between Distribution Parameters and the Accuracy-Robustness Tradeoff in Classification. (arXiv:2107.00247v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00206",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1\">Shuai Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zhenfeng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhizhe Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Zhenyu Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yao Zhao</a>",
          "description": "Benefiting from the powerful expressive capability of graphs, graph-based\napproaches have achieved impressive performance in various biomedical\napplications. Most existing methods tend to define the adjacency matrix among\nsamples manually based on meta-features, and then obtain the node embeddings\nfor downstream tasks by Graph Representation Learning (GRL). However, it is not\neasy for these approaches to generalize to unseen samples. Meanwhile, the\ncomplex correlation between modalities is also ignored. As a result, these\nfactors inevitably yield the inadequacy of providing valid information about\nthe patient's condition for a reliable diagnosis. In this paper, we propose an\nend-to-end Multimodal Graph Learning framework (MMGL) for disease prediction.\nTo effectively exploit the rich information across multi-modality associated\nwith diseases, amodal-attentional multi-modal fusion is proposed to integrate\nthe features of each modality by leveraging the correlation and complementarity\nbetween the modalities. Furthermore, instead of defining the adjacency matrix\nmanually as existing methods, the latent graph structure can be captured\nthrough a novel way of adaptive graph learning. It could be jointly optimized\nwith the prediction model, thus revealing the intrinsic connections among\nsamples. Unlike the previous transductive methods, our model is also applicable\nto the scenario of inductive learning for those unseen data. An extensive group\nof experiments on two disease prediction problems is then carefully designed\nand presented, demonstrating that MMGL obtains more favorable performances. In\naddition, we also visualize and analyze the learned graph structure to provide\nmore reliable decision support for doctors in real medical applications and\ninspiration for disease research.",
          "link": "http://arxiv.org/abs/2107.00206",
          "publishedOn": "2021-07-02T01:58:00.993Z",
          "wordCount": 693,
          "title": "Multi-modal Graph Learning for Disease Prediction. (arXiv:2107.00206v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2102.12871",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Han Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jiahui Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiaozhe Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenguo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwok_J/0/1/0/all/0/1\">James T. Kwok</a>",
          "description": "Transformer-based models are popularly used in natural language processing\n(NLP). Its core component, self-attention, has aroused widespread interest. To\nunderstand the self-attention mechanism, a direct method is to visualize the\nattention map of a pre-trained model. Based on the patterns observed, a series\nof efficient Transformers with different sparse attention masks have been\nproposed. From a theoretical perspective, universal approximability of\nTransformer-based models is also recently proved. However, the above\nunderstanding and analysis of self-attention is based on a pre-trained model.\nTo rethink the importance analysis in self-attention, we study the significance\nof different positions in attention matrix during pre-training. A surprising\nresult is that diagonal elements in the attention map are the least important\ncompared with other attention positions. We provide a proof showing that these\ndiagonal elements can indeed be removed without deteriorating model\nperformance. Furthermore, we propose a Differentiable Attention Mask (DAM)\nalgorithm, which further guides the design of the SparseBERT. Extensive\nexperiments verify our interesting findings and illustrate the effect of the\nproposed algorithm.",
          "link": "http://arxiv.org/abs/2102.12871",
          "publishedOn": "2021-07-02T01:58:00.986Z",
          "wordCount": 648,
          "title": "SparseBERT: Rethinking the Importance Analysis in Self-attention. (arXiv:2102.12871v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2004.10178",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_P/0/1/0/all/0/1\">Pushpendu Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neufeld_A/0/1/0/all/0/1\">Ariel Neufeld</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sahoo_J/0/1/0/all/0/1\">Jajati Keshari Sahoo</a>",
          "description": "We employ both random forests and LSTM networks (more precisely CuDNNLSTM) as\ntraining methodologies to analyze their effectiveness in forecasting\nout-of-sample directional movements of constituent stocks of the S&P 500 from\nJanuary 1993 till December 2018 for intraday trading. We introduce a\nmulti-feature setting consisting not only of the returns with respect to the\nclosing prices, but also with respect to the opening prices and intraday\nreturns. As trading strategy, we use Krauss et al. (2017) and Fischer & Krauss\n(2018) as benchmark. On each trading day, we buy the 10 stocks with the highest\nprobability and sell short the 10 stocks with the lowest probability to\noutperform the market in terms of intraday returns -- all with equal monetary\nweight. Our empirical results show that the multi-feature setting provides a\ndaily return, prior to transaction costs, of 0.64% using LSTM networks, and\n0.54% using random forests. Hence we outperform the single-feature setting in\nFischer & Krauss (2018) and Krauss et al. (2017) consisting only of the daily\nreturns with respect to the closing prices, having corresponding daily returns\nof 0.41% and of 0.39% with respect to LSTM and random forests, respectively.",
          "link": "http://arxiv.org/abs/2004.10178",
          "publishedOn": "2021-07-02T01:58:00.979Z",
          "wordCount": 672,
          "title": "Forecasting directional movements of stock prices for intraday trading using LSTM and random forests. (arXiv:2004.10178v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00003",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shu_J/0/1/0/all/0/1\">Juan Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xi_B/0/1/0/all/0/1\">Bowei Xi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamhoua_C/0/1/0/all/0/1\">Charles Kamhoua</a>",
          "description": "Deep neural network (DNN) is a popular model implemented in many systems to\nhandle complex tasks such as image classification, object recognition, natural\nlanguage processing etc. Consequently DNN structural vulnerabilities become\npart of the security vulnerabilities in those systems. In this paper we study\nthe root cause of DNN adversarial examples. We examine the DNN response surface\nto understand its classification boundary. Our study reveals the structural\nproblem of DNN classification boundary that leads to the adversarial examples.\nExisting attack algorithms can generate from a handful to a few hundred\nadversarial examples given one clean image. We show there are infinitely many\nadversarial images given one clean sample, all within a small neighborhood of\nthe clean sample. We then define DNN uncertainty regions and show\ntransferability of adversarial examples is not universal. We also argue that\ngeneralization error, the large sample theoretical guarantee established for\nDNN, cannot adequately capture the phenomenon of adversarial examples. We need\nnew theory to measure DNN robustness.",
          "link": "http://arxiv.org/abs/2107.00003",
          "publishedOn": "2021-07-02T01:58:00.955Z",
          "wordCount": 597,
          "title": "Understanding Adversarial Examples Through Deep Neural Network's Response Surface and Uncertainty Regions. (arXiv:2107.00003v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00425",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Morales_Hernandez_A/0/1/0/all/0/1\">Alejandro Morales-Hern&#xe1;ndez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Napoles_G/0/1/0/all/0/1\">Gonzalo N&#xe1;poles</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jastrzebska_A/0/1/0/all/0/1\">Agnieszka Jastrzebska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salgueiro_Y/0/1/0/all/0/1\">Yamisleydi Salgueiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vanhoof_K/0/1/0/all/0/1\">Koen Vanhoof</a>",
          "description": "Forecasting windmill time series is often the basis of other processes such\nas anomaly detection, health monitoring, or maintenance scheduling. The amount\nof data generated on windmill farms makes online learning the most viable\nstrategy to follow. Such settings require retraining the model each time a new\nbatch of data is available. However, update the model with the new information\nis often very expensive to perform using traditional Recurrent Neural Networks\n(RNNs). In this paper, we use Long Short-term Cognitive Networks (LSTCNs) to\nforecast windmill time series in online settings. These recently introduced\nneural systems consist of chained Short-term Cognitive Network blocks, each\nprocessing a temporal data chunk. The learning algorithm of these blocks is\nbased on a very fast, deterministic learning rule that makes LSTCNs suitable\nfor online learning tasks. The numerical simulations using a case study with\nfour windmills showed that our approach reported the lowest forecasting errors\nwith respect to a simple RNN, a Long Short-term Memory, a Gated Recurrent Unit,\nand a Hidden Markov Model. What is perhaps more important is that the LSTCN\napproach is significantly faster than these state-of-the-art models.",
          "link": "http://arxiv.org/abs/2107.00425",
          "publishedOn": "2021-07-02T01:58:00.942Z",
          "wordCount": 627,
          "title": "Online learning of windmill time series using Long Short-term Cognitive Networks. (arXiv:2107.00425v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1907.09693",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qinbin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_Z/0/1/0/all/0/1\">Zeyi Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zhaomin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1\">Sixu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1\">Naibo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_B/0/1/0/all/0/1\">Bingsheng He</a>",
          "description": "Federated learning has been a hot research topic in enabling the\ncollaborative training of machine learning models among different organizations\nunder the privacy restrictions. As researchers try to support more machine\nlearning models with different privacy-preserving approaches, there is a\nrequirement in developing systems and infrastructures to ease the development\nof various federated learning algorithms. Similar to deep learning systems such\nas PyTorch and TensorFlow that boost the development of deep learning,\nfederated learning systems (FLSs) are equivalently important, and face\nchallenges from various aspects such as effectiveness, efficiency, and privacy.\nIn this survey, we conduct a comprehensive review on federated learning\nsystems. To achieve smooth flow and guide future research, we introduce the\ndefinition of federated learning systems and analyze the system components.\nMoreover, we provide a thorough categorization for federated learning systems\naccording to six different aspects, including data distribution, machine\nlearning model, privacy mechanism, communication architecture, scale of\nfederation and motivation of federation. The categorization can help the design\nof federated learning systems as shown in our case studies. By systematically\nsummarizing the existing federated learning systems, we present the design\nfactors, case studies, and future research opportunities.",
          "link": "http://arxiv.org/abs/1907.09693",
          "publishedOn": "2021-07-02T01:58:00.934Z",
          "wordCount": 720,
          "title": "A Survey on Federated Learning Systems: Vision, Hype and Reality for Data Privacy and Protection. (arXiv:1907.09693v6 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1901.11331",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kobayashi_M/0/1/0/all/0/1\">Masahiro Kobayashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_K/0/1/0/all/0/1\">Kazuho Watanabe</a>",
          "description": "DP-means clustering was obtained as an extension of $K$-means clustering.\nWhile it is implemented with a simple and efficient algorithm, it can estimate\nthe number of clusters simultaneously. However, DP-means is specifically\ndesigned for the average distortion measure. Therefore, it is vulnerable to\noutliers in data, and can cause large maximum distortion in clusters. In this\nwork, we extend the objective function of the DP-means to $f$-separable\ndistortion measures and propose a unified learning algorithm to overcome the\nabove problems by selecting the function $f$. Further, the influence function\nof the estimated cluster center is analyzed to evaluate the robustness against\noutliers. We demonstrate the performance of the generalized method by numerical\nexperiments using real datasets.",
          "link": "http://arxiv.org/abs/1901.11331",
          "publishedOn": "2021-07-02T01:58:00.925Z",
          "wordCount": 586,
          "title": "Generalized Dirichlet-process-means for $f$-separable distortion measures. (arXiv:1901.11331v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00591",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seunghyun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_Y/0/1/0/all/0/1\">Younggyo Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kimin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1\">Pieter Abbeel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1\">Jinwoo Shin</a>",
          "description": "Recent advance in deep offline reinforcement learning (RL) has made it\npossible to train strong robotic agents from offline datasets. However,\ndepending on the quality of the trained agents and the application being\nconsidered, it is often desirable to fine-tune such agents via further online\ninteractions. In this paper, we observe that state-action distribution shift\nmay lead to severe bootstrap error during fine-tuning, which destroys the good\ninitial policy obtained via offline RL. To address this issue, we first propose\na balanced replay scheme that prioritizes samples encountered online while also\nencouraging the use of near-on-policy samples from the offline dataset.\nFurthermore, we leverage multiple Q-functions trained pessimistically offline,\nthereby preventing overoptimism concerning unfamiliar actions at novel states\nduring the initial training phase. We show that the proposed method improves\nsample-efficiency and final performance of the fine-tuned robotic agents on\nvarious locomotion and manipulation tasks. Our code is available at:\nhttps://github.com/shlee94/Off2OnRL.",
          "link": "http://arxiv.org/abs/2107.00591",
          "publishedOn": "2021-07-02T01:58:00.917Z",
          "wordCount": 589,
          "title": "Offline-to-Online Reinforcement Learning via Balanced Replay and Pessimistic Q-Ensemble. (arXiv:2107.00591v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/1910.11390",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chang_D/0/1/0/all/0/1\">Daniel T. Chang</a>",
          "description": "Tiered graph autoencoders provide the architecture and mechanisms for\nlearning tiered latent representations and latent spaces for molecular graphs\nthat explicitly represent and utilize groups (e.g., functional groups). This\nenables the utilization and exploration of tiered molecular latent spaces,\neither individually - the node (atom) tier, the group tier, or the graph\n(molecule) tier - or jointly, as well as navigation across the tiers. In this\npaper, we discuss the use of tiered graph autoencoders together with graph\nprediction for molecular graphs. We show features of molecular graphs used, and\ngroups in molecular graphs identified for some sample molecules. We briefly\nreview graph prediction and the QM9 dataset for background information, and\ndiscuss the use of tiered graph embeddings for graph prediction, particularly\nweighted group pooling. We find that functional groups and ring groups\neffectively capture and represent the chemical essence of molecular graphs\n(structures). Further, tiered graph autoencoders and graph prediction together\nprovide effective, efficient and interpretable deep learning for molecular\ngraphs, with the former providing unsupervised, transferable learning and the\nlatter providing supervised, task-optimized learning.",
          "link": "http://arxiv.org/abs/1910.11390",
          "publishedOn": "2021-07-02T01:58:00.898Z",
          "wordCount": 652,
          "title": "Deep Learning for Molecular Graphs with Tiered Graph Autoencoders and Graph Prediction. (arXiv:1910.11390v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1910.04952",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">John Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wolfe_C/0/1/0/all/0/1\">Cameron Wolfe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kyrillidis_A/0/1/0/all/0/1\">Anastasios Kyrillidis</a>",
          "description": "Momentum is a widely used technique for gradient-based optimizers in deep\nlearning. In this paper, we propose a decaying momentum (\\textsc{Demon}) rule.\nWe conduct the first large-scale empirical analysis of momentum decay methods\nfor modern neural network optimization, in addition to the most popular\nlearning rate decay schedules. Across 28 relevant combinations of models,\nepochs, datasets, and optimizers, \\textsc{Demon} achieves the highest number of\nTop-1 and Top-3 finishes at 39\\% and 85\\% respectively, almost doubling the\nsecond-placed learning rate cosine schedule at 17\\% and 60\\%, respectively.\n\\textsc{Demon} also outperforms other widely used schedulers including, but not\nlimited to, the learning rate step schedule, linear schedule, OneCycle\nschedule, and exponential schedule. Compared with the widely used learning rate\nstep schedule, \\textsc{Demon} is observed to be less sensitive to parameter\ntuning, which is critical to training neural networks in practice. Results are\ndemonstrated across a variety of settings and architectures, including image\nclassification, generative models, and language models. \\textsc{Demon} is easy\nto implement, requires no additional tuning, and incurs almost no extra\ncomputational overhead compared to the vanilla counterparts. Code is readily\navailable.",
          "link": "http://arxiv.org/abs/1910.04952",
          "publishedOn": "2021-07-02T01:58:00.891Z",
          "wordCount": 677,
          "title": "Demon: Improved Neural Network Training with Momentum Decay. (arXiv:1910.04952v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00644",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hansen_N/0/1/0/all/0/1\">Nicklas Hansen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hao Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaolong Wang</a>",
          "description": "While agents trained by Reinforcement Learning (RL) can solve increasingly\nchallenging tasks directly from visual observations, generalizing learned\nskills to novel environments remains very challenging. Extensive use of data\naugmentation is a promising technique for improving generalization in RL, but\nit is often found to decrease sample efficiency and can even lead to\ndivergence. In this paper, we investigate causes of instability when using data\naugmentation in common off-policy RL algorithms. We identify two problems, both\nrooted in high-variance Q-targets. Based on our findings, we propose a simple\nyet effective technique for stabilizing this class of algorithms under\naugmentation. We perform extensive empirical evaluation of image-based RL using\nboth ConvNets and Vision Transformers (ViT) on a family of benchmarks based on\nDeepMind Control Suite, as well as in robotic manipulation tasks. Our method\ngreatly improves stability and sample efficiency of ConvNets under\naugmentation, and achieves generalization results competitive with\nstate-of-the-art methods for image-based RL. We further show that our method\nscales to RL with ViT-based architectures, and that data augmentation may be\nespecially important in this setting.",
          "link": "http://arxiv.org/abs/2107.00644",
          "publishedOn": "2021-07-02T01:58:00.872Z",
          "wordCount": 630,
          "title": "Stabilizing Deep Q-Learning with ConvNets and Vision Transformers under Data Augmentation. (arXiv:2107.00644v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1910.03201",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yognjin Lee</a>",
          "description": "Deep neural networks have relieved a great deal of burden on human experts in\nrelation to feature engineering. However, comparable efforts are instead\nrequired to determine effective architectures. In addition, as the sizes of\nnetworks have grown overly large, a considerable amount of resources is also\ninvested in reducing the sizes. The sparsification of an over-complete model\naddresses these problems as it removes redundant components and connections. In\nthis study, we propose a fully differentiable sparsification method for deep\nneural networks which allows parameters to be zero during training via\nstochastic gradient descent. Thus, the proposed method can learn the sparsified\nstructure and weights of a network in an end-to-end manner. The method is\ndirectly applicable to various modern deep neural networks and imposes minimum\nmodification to existing models. To the best of our knowledge, this is the\nfirst fully [sub-]differentiable sparsification method that zeroes out\nparameters. It provides a foundation for future structure learning and model\ncompression methods.",
          "link": "http://arxiv.org/abs/1910.03201",
          "publishedOn": "2021-07-02T01:58:00.856Z",
          "wordCount": 635,
          "title": "Differentiable Sparsification for Deep Neural Networks. (arXiv:1910.03201v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00594",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zaiem_S/0/1/0/all/0/1\">Salah Zaiem</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Parcollet_T/0/1/0/all/0/1\">Titouan Parcollet</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Essid_S/0/1/0/all/0/1\">Slim Essid</a>",
          "description": "Through solving pretext tasks, self-supervised learning leverages unlabeled\ndata to extract useful latent representations replacing traditional input\nfeatures in the downstream task. In various application domains, including\ncomputer vision, natural language processing and audio/speech signal\nprocessing, a wide range of features where engineered through decades of\nresearch efforts. As it turns out, learning to predict such features has proven\nto be a particularly relevant pretext task leading to building useful\nself-supervised representations that prove to be effective for downstream\ntasks. However, methods and common practices for combining such pretext tasks,\nwhere each task targets a different group of features for better performance on\nthe downstream task have not been explored and understood properly. In fact,\nthe process relies almost exclusively on a computationally heavy experimental\nprocedure, which becomes intractable with the increase of the number of pretext\ntasks. This paper introduces a method to select a group of pretext tasks among\na set of candidates. The method we propose estimates properly calibrated\nweights for the partial losses corresponding to the considered pretext tasks\nduring the self-supervised training process. The experiments conducted on\nspeaker recognition and automatic speech recognition validate our approach, as\nthe groups selected and weighted with our method perform better than classic\nbaselines, thus facilitating the selection and combination of relevant\npseudo-labels for self-supervised representation learning.",
          "link": "http://arxiv.org/abs/2107.00594",
          "publishedOn": "2021-07-02T01:58:00.849Z",
          "wordCount": 668,
          "title": "Pretext Tasks selection for multitask self-supervised speech representation learning. (arXiv:2107.00594v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00415",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Marchisio_A/0/1/0/all/0/1\">Alberto Marchisio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pira_G/0/1/0/all/0/1\">Giacomo Pira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martina_M/0/1/0/all/0/1\">Maurizio Martina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Masera_G/0/1/0/all/0/1\">Guido Masera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shafique_M/0/1/0/all/0/1\">Muhammad Shafique</a>",
          "description": "Spiking Neural Networks (SNNs), despite being energy-efficient when\nimplemented on neuromorphic hardware and coupled with event-based Dynamic\nVision Sensors (DVS), are vulnerable to security threats, such as adversarial\nattacks, i.e., small perturbations added to the input for inducing a\nmisclassification. Toward this, we propose DVS-Attacks, a set of stealthy yet\nefficient adversarial attack methodologies targeted to perturb the event\nsequences that compose the input of the SNNs. First, we show that noise filters\nfor DVS can be used as defense mechanisms against adversarial attacks.\nAfterwards, we implement several attacks and test them in the presence of two\ntypes of noise filters for DVS cameras. The experimental results show that the\nfilters can only partially defend the SNNs against our proposed DVS-Attacks.\nUsing the best settings for the noise filters, our proposed Mask Filter-Aware\nDash Attack reduces the accuracy by more than 20% on the DVS-Gesture dataset\nand by more than 65% on the MNIST dataset, compared to the original clean\nframes. The source code of all the proposed DVS-Attacks and noise filters is\nreleased at https://github.com/albertomarchisio/DVS-Attacks.",
          "link": "http://arxiv.org/abs/2107.00415",
          "publishedOn": "2021-07-02T01:58:00.829Z",
          "wordCount": 633,
          "title": "DVS-Attacks: Adversarial Attacks on Dynamic Vision Sensors for Spiking Neural Networks. (arXiv:2107.00415v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00501",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Keller_M/0/1/0/all/0/1\">Marcel Keller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_K/0/1/0/all/0/1\">Ke Sun</a>",
          "description": "We have implemented training of neural networks in secure multi-party\ncomputation (MPC) using quantization commonly used in the said setting. To the\nbest of our knowledge, we are the first to present an MNIST classifier purely\ntrained in MPC that comes within 0.2 percent of the accuracy of the same\nconvolutional neural network trained via plaintext computation. More\nconcretely, we have trained a network with two convolution and two dense layers\nto 99.2% accuracy in 25 epochs. This took 3.5 hours in our MPC implementation\n(under one hour for 99% accuracy).",
          "link": "http://arxiv.org/abs/2107.00501",
          "publishedOn": "2021-07-02T01:58:00.823Z",
          "wordCount": 515,
          "title": "Secure Quantized Training for Deep Learning. (arXiv:2107.00501v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1912.05081",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Ziwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravela_S/0/1/0/all/0/1\">Sai Ravela</a>",
          "description": "The use of artificial neural networks as models of chaotic dynamics has been\nrapidly expanding. Still, a theoretical understanding of how neural networks\nlearn chaos is lacking. Here, we employ a geometric perspective to show that\nneural networks can efficiently model chaotic dynamics by becoming structurally\nchaotic themselves. We first confirm neural network's efficiency in emulating\nchaos by showing that a parsimonious neural network trained only on few data\npoints can reconstruct strange attractors, extrapolate outside training data\nboundaries, and accurately predict local divergence rates. We then posit that\nthe trained network's map comprises sequential geometric stretching, rotation,\nand compression operations. These geometric operations indicate topological\nmixing and chaos, explaining why neural networks are naturally suitable to\nemulate chaotic dynamics.",
          "link": "http://arxiv.org/abs/1912.05081",
          "publishedOn": "2021-07-02T01:58:00.814Z",
          "wordCount": 620,
          "title": "Neural Networks as Geometric Chaotic Maps. (arXiv:1912.05081v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2107.00079",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Muralidhar_N/0/1/0/all/0/1\">Nikhil Muralidhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muthiah_S/0/1/0/all/0/1\">Sathappah Muthiah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Butler_P/0/1/0/all/0/1\">Patrick Butler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_M/0/1/0/all/0/1\">Manish Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yu Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burne_K/0/1/0/all/0/1\">Katy Burne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Weipeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jones_D/0/1/0/all/0/1\">David Jones</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arunachalam_P/0/1/0/all/0/1\">Prakash Arunachalam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCormick_H/0/1/0/all/0/1\">Hays &#x27;Skip&#x27; McCormick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramakrishnan_N/0/1/0/all/0/1\">Naren Ramakrishnan</a>",
          "description": "We describe lessons learned from developing and deploying machine learning\nmodels at scale across the enterprise in a range of financial analytics\napplications. These lessons are presented in the form of antipatterns. Just as\ndesign patterns codify best software engineering practices, antipatterns\nprovide a vocabulary to describe defective practices and methodologies. Here we\ncatalog and document numerous antipatterns in financial ML operations (MLOps).\nSome antipatterns are due to technical errors, while others are due to not\nhaving sufficient knowledge of the surrounding context in which ML results are\nused. By providing a common vocabulary to discuss these situations, our intent\nis that antipatterns will support better documentation of issues, rapid\ncommunication between stakeholders, and faster resolution of problems. In\naddition to cataloging antipatterns, we describe solutions, best practices, and\nfuture directions toward MLOps maturity.",
          "link": "http://arxiv.org/abs/2107.00079",
          "publishedOn": "2021-07-02T01:58:00.734Z",
          "wordCount": 574,
          "title": "Using AntiPatterns to avoid MLOps Mistakes. (arXiv:2107.00079v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00191",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_W/0/1/0/all/0/1\">Wonju Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Byun_S/0/1/0/all/0/1\">Seok-Yong Byun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jooeun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_M/0/1/0/all/0/1\">Minje Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chechil_K/0/1/0/all/0/1\">Kirill Chechil</a>",
          "description": "While many real-world data streams imply that they change frequently in a\nnonstationary way, most of deep learning methods optimize neural networks on\ntraining data, and this leads to severe performance degradation when dataset\nshift happens. However, it is less possible to annotate or inspect newly\nstreamed data by humans, and thus it is desired to measure model drift at\ninference time in an unsupervised manner. In this paper, we propose a novel\nmethod of model drift estimation by exploiting statistics of batch\nnormalization layer on unlabeled test data. To remedy possible sampling error\nof streamed input data, we adopt low-rank approximation to each\nrepresentational layer. We show the effectiveness of our method not only on\ndataset shift detection but also on model selection when there are multiple\ncandidate models among model zoo or training trajectories in an unsupervised\nway. We further demonstrate the consistency of our method by comparing model\ndrift scores between different network architectures.",
          "link": "http://arxiv.org/abs/2107.00191",
          "publishedOn": "2021-07-02T01:58:00.727Z",
          "wordCount": 616,
          "title": "Unsupervised Model Drift Estimation with Batch Normalization Statistics for Dataset Shift Detection and Model Selection. (arXiv:2107.00191v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00001",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Portisch_J/0/1/0/all/0/1\">Jan Portisch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hladik_M/0/1/0/all/0/1\">Michael Hladik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Paulheim_H/0/1/0/all/0/1\">Heiko Paulheim</a>",
          "description": "The use of external background knowledge can be beneficial for the task of\nmatching schemas or ontologies automatically. In this paper, we exploit six\ngeneral-purpose knowledge graphs as sources of background knowledge for the\nmatching task. The background sources are evaluated by applying three different\nexploitation strategies. We find that explicit strategies still outperform\nlatent ones and that the choice of the strategy has a greater impact on the\nfinal alignment than the actual background dataset on which the strategy is\napplied. While we could not identify a universally superior resource, BabelNet\nachieved consistently good results. Our best matcher configuration with\nBabelNet performs very competitively when compared to other matching systems\neven though no dataset-specific optimizations were made.",
          "link": "http://arxiv.org/abs/2107.00001",
          "publishedOn": "2021-07-02T01:58:00.709Z",
          "wordCount": 563,
          "title": "Background Knowledge in Schema Matching: Strategy vs. Data. (arXiv:2107.00001v1 [cs.DB])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00088",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Hooten_S/0/1/0/all/0/1\">Sean Hooten</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Vaerenbergh_T/0/1/0/all/0/1\">Thomas Van Vaerenbergh</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Beausoleil_R/0/1/0/all/0/1\">Raymond G. Beausoleil</a>",
          "description": "We present a proof-of-concept technique for the inverse design of\nelectromagnetic devices motivated by the policy gradient method in\nreinforcement learning, named PHORCED (PHotonic Optimization using REINFORCE\nCriteria for Enhanced Design). This technique uses a probabilistic generative\nneural network interfaced with an electromagnetic solver to assist in the\ndesign of photonic devices, such as grating couplers. We show that PHORCED\nobtains better performing grating coupler designs than local gradient-based\ninverse design via the adjoint method, while potentially providing faster\nconvergence over competing state-of-the-art generative methods. Furthermore, we\nimplement transfer learning with PHORCED, demonstrating that a neural network\ntrained to optimize 8$^\\circ$ grating couplers can then be re-trained on\ngrating couplers with alternate scattering angles while requiring >$10\\times$\nfewer simulations than control cases.",
          "link": "http://arxiv.org/abs/2107.00088",
          "publishedOn": "2021-07-02T01:58:00.702Z",
          "wordCount": 574,
          "title": "Inverse Design of Grating Couplers Using the Policy Gradient Method from Reinforcement Learning. (arXiv:2107.00088v1 [physics.comp-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00231",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bochen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yuxuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_Z/0/1/0/all/0/1\">Zhiyao Duan</a>",
          "description": "Separating a song into vocal and accompaniment components is an active\nresearch topic, and recent years witnessed an increased performance from\nsupervised training using deep learning techniques. We propose to apply the\nvisual information corresponding to the singers' vocal activities to further\nimprove the quality of the separated vocal signals. The video frontend model\ntakes the input of mouth movement and fuses it into the feature embeddings of\nan audio-based separation framework. To facilitate the network to learn\naudiovisual correlation of singing activities, we add extra vocal signals\nirrelevant to the mouth movement to the audio mixture during training. We\ncreate two audiovisual singing performance datasets for training and\nevaluation, respectively, one curated from audition recordings on the Internet,\nand the other recorded in house. The proposed method outperforms audio-based\nmethods in terms of separation quality on most test recordings. This advantage\nis especially pronounced when there are backing vocals in the accompaniment,\nwhich poses a great challenge for audio-only methods.",
          "link": "http://arxiv.org/abs/2107.00231",
          "publishedOn": "2021-07-02T01:58:00.695Z",
          "wordCount": 589,
          "title": "Audiovisual Singing Voice Separation. (arXiv:2107.00231v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00116",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Memarian_F/0/1/0/all/0/1\">Farzan Memarian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hashemi_A/0/1/0/all/0/1\">Abolfazl Hashemi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niekum_S/0/1/0/all/0/1\">Scott Niekum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Topcu_U/0/1/0/all/0/1\">Ufuk Topcu</a>",
          "description": "We explore methodologies to improve the robustness of generative adversarial\nimitation learning (GAIL) algorithms to observation noise. Towards this\nobjective, we study the effect of local Lipschitzness of the discriminator and\nthe generator on the robustness of policies learned by GAIL. In many robotics\napplications, the learned policies by GAIL typically suffer from a degraded\nperformance at test time since the observations from the environment might be\ncorrupted by noise. Hence, robustifying the learned policies against the\nobservation noise is of critical importance. To this end, we propose a\nregularization method to induce local Lipschitzness in the generator and the\ndiscriminator of adversarial imitation learning methods. We show that the\nmodified objective leads to learning significantly more robust policies.\nMoreover, we demonstrate -- both theoretically and experimentally -- that\ntraining a locally Lipschitz discriminator leads to a locally Lipschitz\ngenerator, thereby improving the robustness of the resultant policy. We perform\nextensive experiments on simulated robot locomotion environments from the\nMuJoCo suite that demonstrate the proposed method learns policies that\nsignificantly outperform the state-of-the-art generative adversarial imitation\nlearning algorithm when applied to test scenarios with noise-corrupted\nobservations.",
          "link": "http://arxiv.org/abs/2107.00116",
          "publishedOn": "2021-07-02T01:58:00.688Z",
          "wordCount": 616,
          "title": "Robust Generative Adversarial Imitation Learning via Local Lipschitzness. (arXiv:2107.00116v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00179",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Cai_T/0/1/0/all/0/1\">T. Tony Cai</a>, <a href=\"http://arxiv.org/find/math/1/au:+Wei_H/0/1/0/all/0/1\">Hongji Wei</a>",
          "description": "Distributed minimax estimation and distributed adaptive estimation under\ncommunication constraints for Gaussian sequence model and white noise model are\nstudied. The minimax rate of convergence for distributed estimation over a\ngiven Besov class, which serves as a benchmark for the cost of adaptation, is\nestablished. We then quantify the exact communication cost for adaptation and\nconstruct an optimally adaptive procedure for distributed estimation over a\nrange of Besov classes. The results demonstrate significant differences between\nnonparametric function estimation in the distributed setting and the\nconventional centralized setting. For global estimation, adaptation in general\ncannot be achieved for free in the distributed setting. The new technical tools\nto obtain the exact characterization for the cost of adaptation can be of\nindependent interest.",
          "link": "http://arxiv.org/abs/2107.00179",
          "publishedOn": "2021-07-02T01:58:00.673Z",
          "wordCount": 574,
          "title": "Distributed Nonparametric Function Estimation: Optimal Rate of Convergence and Cost of Adaptation. (arXiv:2107.00179v1 [math.ST])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00166",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiaolong Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_G/0/1/0/all/0/1\">Geng Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1\">Xuan Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tianlong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xuxi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiaohan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1\">Ning Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_M/0/1/0/all/0/1\">Minghai Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Sijia Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yanzhi Wang</a>",
          "description": "There have been long-standing controversies and inconsistencies over the\nexperiment setup and criteria for identifying the \"winning ticket\" in\nliterature. To reconcile such, we revisit the definition of lottery ticket\nhypothesis, with comprehensive and more rigorous conditions. Under our new\ndefinition, we show concrete evidence to clarify whether the winning ticket\nexists across the major DNN architectures and/or applications. Through\nextensive experiments, we perform quantitative analysis on the correlations\nbetween winning tickets and various experimental factors, and empirically study\nthe patterns of our observations. We find that the key training\nhyperparameters, such as learning rate and training epochs, as well as the\narchitecture characteristics such as capacities and residual connections, are\nall highly correlated with whether and when the winning tickets can be\nidentified. Based on our analysis, we summarize a guideline for parameter\nsettings in regards of specific architecture characteristics, which we hope to\ncatalyze the research progress on the topic of lottery ticket hypothesis.",
          "link": "http://arxiv.org/abs/2107.00166",
          "publishedOn": "2021-07-02T01:58:00.648Z",
          "wordCount": 620,
          "title": "Sanity Checks for Lottery Tickets: Does Your Winning Ticket Really Win the Jackpot?. (arXiv:2107.00166v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00100",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mishra_P/0/1/0/all/0/1\">Prateek Mishra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mani_K/0/1/0/all/0/1\">Kumar Divya Mani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johri_P/0/1/0/all/0/1\">Prashant Johri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arya_D/0/1/0/all/0/1\">Dikhsa Arya</a>",
          "description": "Processed data are insightful, and crude data are obtuse. A serious threat to\ndata reliability is missing values. Such data leads to inaccurate analysis and\nwrong predictions. We propose an efficient technique to impute the missing\nvalue in the dataset based on correlation called FCMI (Feature Correlation\nbased Missing Data Imputation). We have considered the correlation of the\nattributes of the dataset, and that is our central idea. Our proposed algorithm\npicks the highly correlated attributes of the dataset and uses these attributes\nto build a regression model whose parameters are optimized such that the\ncorrelation of the dataset is maintained. Experiments conducted on both\nclassification and regression datasets show that the proposed imputation\ntechnique outperforms existing imputation algorithms.",
          "link": "http://arxiv.org/abs/2107.00100",
          "publishedOn": "2021-07-02T01:58:00.637Z",
          "wordCount": 550,
          "title": "FCMI: Feature Correlation based Missing Data Imputation. (arXiv:2107.00100v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00080",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Radford_B/0/1/0/all/0/1\">Benjamin J. Radford</a>",
          "description": "Text data are an important source of detailed information about social and\npolitical events. Automated systems parse large volumes of text data to infer\nor extract structured information that describes actors, actions, dates, times,\nand locations. One of these sub-tasks is geocoding: predicting the geographic\ncoordinates associated with events or locations described by a given text. We\npresent an end-to-end probabilistic model for geocoding text data.\nAdditionally, we collect a novel data set for evaluating the performance of\ngeocoding systems. We compare the model-based solution, called ELECTRo-map, to\nthe current state-of-the-art open source system for geocoding texts for event\ndata. Finally, we discuss the benefits of end-to-end model-based geocoding,\nincluding principled uncertainty estimation and the ability of these models to\nleverage contextual information.",
          "link": "http://arxiv.org/abs/2107.00080",
          "publishedOn": "2021-07-02T01:58:00.630Z",
          "wordCount": 565,
          "title": "Regressing Location on Text for Probabilistic Geocoding. (arXiv:2107.00080v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00233",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yoon_T/0/1/0/all/0/1\">Tehrim Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_S/0/1/0/all/0/1\">Sumin Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1\">Sung Ju Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_E/0/1/0/all/0/1\">Eunho Yang</a>",
          "description": "Federated learning (FL) allows edge devices to collectively learn a model\nwithout directly sharing data within each device, thus preserving privacy and\neliminating the need to store data globally. While there are promising results\nunder the assumption of independent and identically distributed (iid) local\ndata, current state-of-the-art algorithms suffer from performance degradation\nas the heterogeneity of local data across clients increases. To resolve this\nissue, we propose a simple framework, Mean Augmented Federated Learning (MAFL),\nwhere clients send and receive averaged local data, subject to the privacy\nrequirements of target applications. Under our framework, we propose a new\naugmentation algorithm, named FedMix, which is inspired by a phenomenal yet\nsimple data augmentation method, Mixup, but does not require local raw data to\nbe directly shared among devices. Our method shows greatly improved performance\nin the standard benchmark datasets of FL, under highly non-iid federated\nsettings, compared to conventional algorithms.",
          "link": "http://arxiv.org/abs/2107.00233",
          "publishedOn": "2021-07-02T01:58:00.613Z",
          "wordCount": 604,
          "title": "FedMix: Approximation of Mixup under Mean Augmented Federated Learning. (arXiv:2107.00233v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00051",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pan_W/0/1/0/all/0/1\">Wanning Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1\">Lichao Sun</a>",
          "description": "Knowledge distillation has caught a lot of attention in Federated Learning\n(FL) recently. It has the advantage for FL to train on heterogeneous clients\nwhich have different data size and data structure. However, data samples across\nall devices are usually not independent and identically distributed\n(non-i.i.d), posing additional challenges to the convergence and speed of\nfederated learning. As FL randomly asks the clients to join the training\nprocess and each client only learns from local non-i.i.d data, which makes\nlearning processing even slower. In order to solve this problem, an intuitive\nidea is using the global model to guide local training. In this paper, we\npropose a novel global knowledge distillation method, named FedGKD, which\nlearns the knowledge from past global models to tackle down the local bias\ntraining problem. By learning from global knowledge and consistent with current\nlocal models, FedGKD learns a global knowledge model in FL. To demonstrate the\neffectiveness of the proposed method, we conduct extensive experiments on\nvarious CV datasets (CIFAR-10/100) and settings (non-i.i.d data). The\nevaluation results show that FedGKD outperforms previous state-of-the-art\nmethods.",
          "link": "http://arxiv.org/abs/2107.00051",
          "publishedOn": "2021-07-02T01:58:00.598Z",
          "wordCount": 613,
          "title": "Global Knowledge Distillation in Federated Learning. (arXiv:2107.00051v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00254",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Niu_S/0/1/0/all/0/1\">Shuaicheng Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jiaxiang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_G/0/1/0/all/0/1\">Guanghui Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yifan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yong Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1\">Peilin Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Peng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1\">Mingkui Tan</a>",
          "description": "In real-world applications, data often come in a growing manner, where the\ndata volume and the number of classes may increase dynamically. This will bring\na critical challenge for learning: given the increasing data volume or the\nnumber of classes, one has to instantaneously adjust the neural model capacity\nto obtain promising performance. Existing methods either ignore the growing\nnature of data or seek to independently search an optimal architecture for a\ngiven dataset, and thus are incapable of promptly adjusting the architectures\nfor the changed data. To address this, we present a neural architecture\nadaptation method, namely Adaptation eXpert (AdaXpert), to efficiently adjust\nprevious architectures on the growing data. Specifically, we introduce an\narchitecture adjuster to generate a suitable architecture for each data\nsnapshot, based on the previous architecture and the different extent between\ncurrent and previous data distributions. Furthermore, we propose an adaptation\ncondition to determine the necessity of adjustment, thereby avoiding\nunnecessary and time-consuming adjustments. Extensive experiments on two growth\nscenarios (increasing data volume and number of classes) demonstrate the\neffectiveness of the proposed method.",
          "link": "http://arxiv.org/abs/2107.00254",
          "publishedOn": "2021-07-02T01:58:00.589Z",
          "wordCount": 626,
          "title": "AdaXpert: Adapting Neural Architecture for Growing Data. (arXiv:2107.00254v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2107.00002",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Honggui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galayko_D/0/1/0/all/0/1\">Dimitri Galayko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trocan_M/0/1/0/all/0/1\">Maria Trocan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sawan_M/0/1/0/all/0/1\">Mohamad Sawan</a>",
          "description": "Autoencoders are composed of coding and decoding units, hence they hold the\ninherent potential of high-performance data compression and signal compressed\nsensing. The main disadvantages of current autoencoders comprise the following\nseveral aspects: the research objective is not data reconstruction but feature\nrepresentation; the performance evaluation of data recovery is neglected; it is\nhard to achieve lossless data reconstruction by pure autoencoders, even by pure\ndeep learning. This paper aims for image reconstruction of autoencoders,\nemploys cascade decoders-based autoencoders, perfects the performance of image\nreconstruction, approaches gradually lossless image recovery, and provides\nsolid theory and application basis for autoencoders-based image compression and\ncompressed sensing. The proposed serial decoders-based autoencoders include the\narchitectures of multi-level decoders and the related optimization algorithms.\nThe cascade decoders consist of general decoders, residual decoders,\nadversarial decoders and their combinations. It is evaluated by the\nexperimental results that the proposed autoencoders outperform the classical\nautoencoders in the performance of image reconstruction.",
          "link": "http://arxiv.org/abs/2107.00002",
          "publishedOn": "2021-07-02T01:58:00.574Z",
          "wordCount": 579,
          "title": "Cascade Decoders-Based Autoencoders for Image Reconstruction. (arXiv:2107.00002v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2103.05247",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lu_K/0/1/0/all/0/1\">Kevin Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grover_A/0/1/0/all/0/1\">Aditya Grover</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1\">Pieter Abbeel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mordatch_I/0/1/0/all/0/1\">Igor Mordatch</a>",
          "description": "We investigate the capability of a transformer pretrained on natural language\nto generalize to other modalities with minimal finetuning -- in particular,\nwithout finetuning of the self-attention and feedforward layers of the residual\nblocks. We consider such a model, which we call a Frozen Pretrained Transformer\n(FPT), and study finetuning it on a variety of sequence classification tasks\nspanning numerical computation, vision, and protein fold prediction. In\ncontrast to prior works which investigate finetuning on the same modality as\nthe pretraining dataset, we show that pretraining on natural language can\nimprove performance and compute efficiency on non-language downstream tasks.\nAdditionally, we perform an analysis of the architecture, comparing the\nperformance of a random initialized transformer to a random LSTM. Combining the\ntwo insights, we find language-pretrained transformers can obtain strong\nperformance on a variety of non-language tasks.",
          "link": "http://arxiv.org/abs/2103.05247",
          "publishedOn": "2021-07-01T01:59:34.732Z",
          "wordCount": 600,
          "title": "Pretrained Transformers as Universal Computation Engines. (arXiv:2103.05247v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.06315",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Wen_L/0/1/0/all/0/1\">Linjie Wen</a>, <a href=\"http://arxiv.org/find/math/1/au:+Li_J/0/1/0/all/0/1\">Jinglai Li</a>",
          "description": "We propose a linear-mapping based variational Ensemble Kalman filter for\nsequential Bayesian filtering problems with generic observation models.\nSpecifically, the proposed method is formulated as to construct a linear\nmapping from the prior ensemble to the posterior one, and the linear mapping is\ncomputed via a variational Bayesian formulation, i.e., by minimizing the\nKullback-Leibler divergence between the transformed distribution by the linear\nmapping and the actual posterior. A gradient descent scheme is proposed to\nsolve the resulting optimization problem. With numerical examples we\ndemonstrate that the method has competitive performance against existing\nmethods.",
          "link": "http://arxiv.org/abs/2103.06315",
          "publishedOn": "2021-07-01T01:59:34.726Z",
          "wordCount": 550,
          "title": "Linear-Mapping based Variational Ensemble Kalman Filter. (arXiv:2103.06315v3 [math.NA] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.00543",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1\">Saurabh Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venkataraman_S/0/1/0/all/0/1\">Shivaram Venkataraman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papailiopoulos_D/0/1/0/all/0/1\">Dimitris Papailiopoulos</a>",
          "description": "A rich body of prior work has highlighted the existence of communication\nbottlenecks in synchronous data-parallel training. To alleviate these\nbottlenecks, a long line of recent work proposes gradient and model compression\nmethods. In this work, we evaluate the efficacy of gradient compression methods\nand compare their scalability with optimized implementations of synchronous\ndata-parallel SGD across more than 200 different setups. Surprisingly, we\nobserve that only in 6 cases out of more than 200, gradient compression methods\nprovide speedup over optimized synchronous data-parallel training in the\ntypical data-center setting. We conduct an extensive investigation to identify\nthe root causes of this phenomenon, and offer a performance model that can be\nused to identify the benefits of gradient compression for a variety of system\nsetups. Based on our analysis, we propose a list of desirable properties that\ngradient compression methods should satisfy, in order for them to provide a\nmeaningful end-to-end speedup.",
          "link": "http://arxiv.org/abs/2103.00543",
          "publishedOn": "2021-07-01T01:59:34.720Z",
          "wordCount": 634,
          "title": "On the Utility of Gradient Compression in Distributed Training Systems. (arXiv:2103.00543v3 [cs.DC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.07932",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Henry_R/0/1/0/all/0/1\">Robin Henry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ernst_D/0/1/0/all/0/1\">Damien Ernst</a>",
          "description": "Active network management (ANM) of electricity distribution networks include\nmany complex stochastic sequential optimization problems. These problems need\nto be solved for integrating renewable energies and distributed storage into\nfuture electrical grids. In this work, we introduce Gym-ANM, a framework for\ndesigning reinforcement learning (RL) environments that model ANM tasks in\nelectricity distribution networks. These environments provide new playgrounds\nfor RL research in the management of electricity networks that do not require\nan extensive knowledge of the underlying dynamics of such systems. Along with\nthis work, we are releasing an implementation of an introductory\ntoy-environment, ANM6-Easy, designed to emphasize common challenges in ANM. We\nalso show that state-of-the-art RL algorithms can already achieve good\nperformance on ANM6-Easy when compared against a model predictive control (MPC)\napproach. Finally, we provide guidelines to create new Gym-ANM environments\ndiffering in terms of (a) the distribution network topology and parameters, (b)\nthe observation space, (c) the modelling of the stochastic processes present in\nthe system, and (d) a set of hyperparameters influencing the reward signal.\nGym-ANM can be downloaded at https://github.com/robinhenry/gym-anm.",
          "link": "http://arxiv.org/abs/2103.07932",
          "publishedOn": "2021-07-01T01:59:34.714Z",
          "wordCount": 669,
          "title": "Gym-ANM: Reinforcement Learning Environments for Active Network Management Tasks in Electricity Distribution Systems. (arXiv:2103.07932v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.16213",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Merrill_W/0/1/0/all/0/1\">William Merrill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1\">Yoav Goldberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwartz_R/0/1/0/all/0/1\">Roy Schwartz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>",
          "description": "Transformers have become a standard architecture for many NLP problems. This\nhas motivated theoretically analyzing their capabilities as models of language,\nin order to understand what makes them successful, and what their potential\nweaknesses might be. Recent work has shown that transformers with hard\nattention are quite limited in capacity, and in fact can be simulated by\nconstant-depth circuits. However, hard attention is a restrictive assumption,\nwhich may complicate the relevance of these results for practical transformers.\nIn this work, we analyze the circuit complexity of transformers with saturated\nattention: a generalization of hard attention that more closely captures the\nattention patterns learnable in practical transformers. We show that saturated\ntransformers transcend the limitations of hard-attention transformers. With\nsome minor assumptions, we prove that the number of bits needed to represent a\nsaturated transformer memory vector is $O(\\log n)$, which implies saturated\ntransformers can be simulated by log-depth circuits. Thus, the jump from hard\nto saturated attention can be understood as increasing the transformer's\neffective circuit depth by a factor of $O(\\log n)$.",
          "link": "http://arxiv.org/abs/2106.16213",
          "publishedOn": "2021-07-01T01:59:34.708Z",
          "wordCount": 622,
          "title": "On the Power of Saturated Transformers: A View from Circuit Complexity. (arXiv:2106.16213v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2103.12923",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zanette_A/0/1/0/all/0/1\">Andrea Zanette</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_C/0/1/0/all/0/1\">Ching-An Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_A/0/1/0/all/0/1\">Alekh Agarwal</a>",
          "description": "Policy optimization methods are popular reinforcement learning algorithms,\nbecause their incremental and on-policy nature makes them more stable than the\nvalue-based counterparts. However, the same properties also make them slow to\nconverge and sample inefficient, as the on-policy requirement precludes data\nreuse and the incremental updates couple large iteration complexity into the\nsample complexity. These characteristics have been observed in experiments as\nwell as in theory in the recent work of~\\citet{agarwal2020pc}, which provides a\npolicy optimization method PCPG that can robustly find near optimal polices for\napproximately linear Markov decision processes but suffers from an extremely\npoor sample complexity compared with value-based techniques.\n\nIn this paper, we propose a new algorithm, COPOE, that overcomes the sample\ncomplexity issue of PCPG while retaining its robustness to model\nmisspecification. Compared with PCPG, COPOE makes several important algorithmic\nenhancements, such as enabling data reuse, and uses more refined analysis\ntechniques, which we expect to be more broadly applicable to designing new\nreinforcement learning algorithms. The result is an improvement in sample\ncomplexity from $\\widetilde{O}(1/\\epsilon^{11})$ for PCPG to\n$\\widetilde{O}(1/\\epsilon^3)$ for PCPG, nearly bridging the gap with\nvalue-based techniques.",
          "link": "http://arxiv.org/abs/2103.12923",
          "publishedOn": "2021-07-01T01:59:34.692Z",
          "wordCount": 650,
          "title": "Cautiously Optimistic Policy Optimization and Exploration with Linear Function Approximation. (arXiv:2103.12923v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.16147",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gamlath_B/0/1/0/all/0/1\">Buddhima Gamlath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1\">Xinrui Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Polak_A/0/1/0/all/0/1\">Adam Polak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Svensson_O/0/1/0/all/0/1\">Ola Svensson</a>",
          "description": "We study the problem of explainable clustering in the setting first\nformalized by Moshkovitz, Dasgupta, Rashtchian, and Frost (ICML 2020). A\n$k$-clustering is said to be explainable if it is given by a decision tree\nwhere each internal node splits data points with a threshold cut in a single\ndimension (feature), and each of the $k$ leaves corresponds to a cluster. We\ngive an algorithm that outputs an explainable clustering that loses at most a\nfactor of $O(\\log^2 k)$ compared to an optimal (not necessarily explainable)\nclustering for the $k$-medians objective, and a factor of $O(k \\log^2 k)$ for\nthe $k$-means objective. This improves over the previous best upper bounds of\n$O(k)$ and $O(k^2)$, respectively, and nearly matches the previous $\\Omega(\\log\nk)$ lower bound for $k$-medians and our new $\\Omega(k)$ lower bound for\n$k$-means. The algorithm is remarkably simple. In particular, given an initial\nnot necessarily explainable clustering in $\\mathbb{R}^d$, it is oblivious to\nthe data points and runs in time $O(dk \\log^2 k)$, independent of the number of\ndata points $n$. Our upper and lower bounds also generalize to objectives given\nby higher $\\ell_p$-norms.",
          "link": "http://arxiv.org/abs/2106.16147",
          "publishedOn": "2021-07-01T01:59:34.686Z",
          "wordCount": 621,
          "title": "Nearly-Tight and Oblivious Algorithms for Explainable Clustering. (arXiv:2106.16147v1 [cs.DS])"
        },
        {
          "id": "http://arxiv.org/abs/2011.03813",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yiyuan Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_P/0/1/0/all/0/1\">Panpan Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_D/0/1/0/all/0/1\">David Hsu</a>",
          "description": "The partially observable Markov decision process (POMDP) is a principled\ngeneral framework for robot decision making under uncertainty, but POMDP\nplanning suffers from high computational complexity, when long-term planning is\nrequired. While temporally-extended macro-actions help to cut down the\neffective planning horizon and significantly improve computational efficiency,\nhow do we acquire good macro-actions? This paper proposes Macro-Action\nGenerator-Critic (MAGIC), which performs offline learning of macro-actions\noptimized for online POMDP planning. Specifically, MAGIC learns a macro-action\ngenerator end-to-end, using an online planner's performance as the feedback.\nDuring online planning, the generator generates on the fly situation-aware\nmacro-actions conditioned on the robot's belief and the environment context. We\nevaluated MAGIC on several long-horizon planning tasks both in simulation and\non a real robot. The experimental results show that the learned macro-actions\noffer significant benefits in online planning performance, compared with\nprimitive actions and handcrafted macro-actions.",
          "link": "http://arxiv.org/abs/2011.03813",
          "publishedOn": "2021-07-01T01:59:34.680Z",
          "wordCount": 622,
          "title": "MAGIC: Learning Macro-Actions for Online POMDP Planning. (arXiv:2011.03813v3 [cs.RO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.01117",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Amir_I/0/1/0/all/0/1\">Idan Amir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koren_T/0/1/0/all/0/1\">Tomer Koren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Livni_R/0/1/0/all/0/1\">Roi Livni</a>",
          "description": "We give a new separation result between the generalization performance of\nstochastic gradient descent (SGD) and of full-batch gradient descent (GD) in\nthe fundamental stochastic convex optimization model. While for SGD it is\nwell-known that $O(1/\\epsilon^2)$ iterations suffice for obtaining a solution\nwith $\\epsilon$ excess expected risk, we show that with the same number of\nsteps GD may overfit and emit a solution with $\\Omega(1)$ generalization error.\nMoreover, we show that in fact $\\Omega(1/\\epsilon^4)$ iterations are necessary\nfor GD to match the generalization performance of SGD, which is also tight due\nto recent work by Bassily et al. (2020). We further discuss how regularizing\nthe empirical risk minimized by GD essentially does not change the above\nresult, and revisit the concepts of stability, implicit bias and the role of\nthe learning algorithm in generalization.",
          "link": "http://arxiv.org/abs/2102.01117",
          "publishedOn": "2021-07-01T01:59:34.658Z",
          "wordCount": 604,
          "title": "SGD Generalizes Better Than GD (And Regularization Doesn't Help). (arXiv:2102.01117v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.01085",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dhaka_A/0/1/0/all/0/1\">Akash Kumar Dhaka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Catalina_A/0/1/0/all/0/1\">Alejandro Catalina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Welandawe_M/0/1/0/all/0/1\">Manushi Welandawe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andersen_M/0/1/0/all/0/1\">Michael Riis Andersen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huggins_J/0/1/0/all/0/1\">Jonathan Huggins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vehtari_A/0/1/0/all/0/1\">Aki Vehtari</a>",
          "description": "Current black-box variational inference (BBVI) methods require the user to\nmake numerous design choices -- such as the selection of variational objective\nand approximating family -- yet there is little principled guidance on how to\ndo so. We develop a conceptual framework and set of experimental tools to\nunderstand the effects of these choices, which we leverage to propose best\npractices for maximizing posterior approximation accuracy. Our approach is\nbased on studying the pre-asymptotic tail behavior of the density ratios\nbetween the joint distribution and the variational approximation, then\nexploiting insights and tools from the importance sampling literature. Our\nframework and supporting experiments help to distinguish between the behavior\nof BBVI methods for approximating low-dimensional versus\nmoderate-to-high-dimensional posteriors. In the latter case, we show that\nmass-covering variational objectives are difficult to optimize and do not\nimprove accuracy, but flexible variational families can improve accuracy and\nthe effectiveness of importance sampling -- at the cost of additional\noptimization challenges. Therefore, for moderate-to-high-dimensional posteriors\nwe recommend using the (mode-seeking) exclusive KL divergence since it is the\neasiest to optimize, and improving the variational family or using model\nparameter transformations to make the posterior and optimal variational\napproximation more similar. On the other hand, in low-dimensional settings, we\nshow that heavy-tailed variational families and mass-covering divergences are\neffective and can increase the chances that the approximation can be improved\nby importance sampling.",
          "link": "http://arxiv.org/abs/2103.01085",
          "publishedOn": "2021-07-01T01:59:34.652Z",
          "wordCount": 704,
          "title": "Challenges and Opportunities in High-dimensional Variational Inference. (arXiv:2103.01085v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.02888",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Hanlin Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_S/0/1/0/all/0/1\">Shaoduo Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awan_A/0/1/0/all/0/1\">Ammar Ahmad Awan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajbhandari_S/0/1/0/all/0/1\">Samyam Rajbhandari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Conglong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lian_X/0/1/0/all/0/1\">Xiangru Lian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Ji Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Ce Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yuxiong He</a>",
          "description": "Scalable training of large models (like BERT and GPT-3) requires careful\noptimization rooted in model design, architecture, and system capabilities.\nFrom a system standpoint, communication has become a major bottleneck,\nespecially on commodity systems with standard TCP interconnects that offer\nlimited network bandwidth. Communication compression is an important technique\nto reduce training time on such systems. One of the most effective methods is\nerror-compensated compression, which offers robust convergence speed even under\n1-bit compression. However, state-of-the-art error compensation techniques only\nwork with basic optimizers like SGD and momentum SGD, which are linearly\ndependent on the gradients. They do not work with non-linear gradient-based\noptimizers like Adam, which offer state-of-the-art convergence efficiency and\naccuracy for models like BERT. In this paper, we propose 1-bit Adam that\nreduces the communication volume by up to $5\\times$, offers much better\nscalability, and provides the same convergence speed as uncompressed Adam. Our\nkey finding is that Adam's variance (non-linear term) becomes stable (after a\nwarmup phase) and can be used as a fixed precondition for the rest of the\ntraining (compression phase). Experiments on up to 256 GPUs show that 1-bit\nAdam enables up to $3.3\\times$ higher throughput for BERT-Large pre-training\nand up to $2.9\\times$ higher throughput for SQuAD fine-tuning. In addition, we\nprovide theoretical analysis for our proposed work.",
          "link": "http://arxiv.org/abs/2102.02888",
          "publishedOn": "2021-07-01T01:59:34.646Z",
          "wordCount": 706,
          "title": "1-bit Adam: Communication Efficient Large-Scale Training with Adam's Convergence Speed. (arXiv:2102.02888v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.16233",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Napoles_G/0/1/0/all/0/1\">Gonzalo N&#xe1;poles</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grau_I/0/1/0/all/0/1\">Isel Grau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jastrzebska_A/0/1/0/all/0/1\">Agnieszka Jastrzebska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salgueiro_Y/0/1/0/all/0/1\">Yamisleydi Salgueiro</a>",
          "description": "In this paper, we present a recurrent neural system named Long Short-term\nCognitive Networks (LSTCNs) as a generalisation of the Short-term Cognitive\nNetwork (STCN) model. Such a generalisation is motivated by the difficulty of\nforecasting very long time series in an efficient, greener fashion. The LSTCN\nmodel can be defined as a collection of STCN blocks, each processing a specific\ntime patch of the (multivariate) time series being modelled. In this neural\nensemble, each block passes information to the subsequent one in the form of a\nweight matrix referred to as the prior knowledge matrix. As a second\ncontribution, we propose a deterministic learning algorithm to compute the\nlearnable weights while preserving the prior knowledge resulting from previous\nlearning processes. As a third contribution, we introduce a feature influence\nscore as a proxy to explain the forecasting process in multivariate time\nseries. The simulations using three case studies show that our neural system\nreports small forecasting errors while being up to thousands of times faster\nthan state-of-the-art recurrent models.",
          "link": "http://arxiv.org/abs/2106.16233",
          "publishedOn": "2021-07-01T01:59:34.640Z",
          "wordCount": 594,
          "title": "Long Short-term Cognitive Networks. (arXiv:2106.16233v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2012.08177",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Goutay_M/0/1/0/all/0/1\">Mathieu Goutay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aoudia_F/0/1/0/all/0/1\">Fay&#xe7;al Ait Aoudia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoydis_J/0/1/0/all/0/1\">Jakob Hoydis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gorce_J/0/1/0/all/0/1\">Jean-Marie Gorce</a>",
          "description": "Machine learning (ML) starts to be widely used to enhance the performance of\nmulti-user multiple-input multiple-output (MU-MIMO) receivers. However, it is\nstill unclear if such methods are truly competitive with respect to\nconventional methods in realistic scenarios and under practical constraints. In\naddition to enabling accurate signal reconstruction on realistic channel\nmodels, MU-MIMO receive algorithms must allow for easy adaptation to a varying\nnumber of users without the need for retraining. In contrast to existing work,\nwe propose an ML-enhanced MU-MIMO receiver that builds on top of a conventional\nlinear minimum mean squared error (LMMSE) architecture. It preserves the\ninterpretability and scalability of the LMMSE receiver, while improving its\naccuracy in two ways. First, convolutional neural networks (CNNs) are used to\ncompute an approximation of the second-order statistics of the channel\nestimation error which are required for accurate equalization. Second, a\nCNN-based demapper jointly processes a large number of orthogonal\nfrequency-division multiplexing (OFDM) symbols and subcarriers, which allows it\nto compute better log likelihood ratios (LLRs) by compensating for channel\naging. The resulting architecture can be used in the up- and downlink and is\ntrained in an end-to-end manner, removing the need for hard-to-get perfect\nchannel state information (CSI) during the training phase. Simulation results\ndemonstrate consistent performance improvements over the baseline which are\nespecially pronounced in high mobility scenarios.",
          "link": "http://arxiv.org/abs/2012.08177",
          "publishedOn": "2021-07-01T01:59:34.634Z",
          "wordCount": 699,
          "title": "Machine Learning for MU-MIMO Receive Processing in OFDM Systems. (arXiv:2012.08177v2 [cs.IT] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.16049",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mylonas_C/0/1/0/all/0/1\">Charilaos Mylonas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdallah_I/0/1/0/all/0/1\">Imad Abdallah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chatzi_E/0/1/0/all/0/1\">Eleni Chatzi</a>",
          "description": "Graph Networks (GNs) enable the fusion of prior knowledge and relational\nreasoning with flexible function approximations. In this work, a general\nGN-based model is proposed which takes full advantage of the relational\nmodeling capabilities of GNs and extends these to probabilistic modeling with\nVariational Bayes (VB). To that end, we combine complementary pre-existing\napproaches on VB for graph data and propose an approach that relies on\ngraph-structured latent and conditioning variables. It is demonstrated that\nNeural Processes can also be viewed through the lens of the proposed model. We\nshow applications on the problem of structured probability density modeling for\nsimulated and real wind farm monitoring data, as well as on the meta-learning\nof simulated Gaussian Process data. We release the source code, along with the\nsimulated datasets.",
          "link": "http://arxiv.org/abs/2106.16049",
          "publishedOn": "2021-07-01T01:59:34.628Z",
          "wordCount": 590,
          "title": "Relational VAE: A Continuous Latent Variable Model for Graph Structured Data. (arXiv:2106.16049v1 [cs.CE])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15775",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ohnishi_M/0/1/0/all/0/1\">Motoya Ohnishi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ishikawa_I/0/1/0/all/0/1\">Isao Ishikawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lowrey_K/0/1/0/all/0/1\">Kendall Lowrey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ikeda_M/0/1/0/all/0/1\">Masahiro Ikeda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kakade_S/0/1/0/all/0/1\">Sham Kakade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kawahara_Y/0/1/0/all/0/1\">Yoshinobu Kawahara</a>",
          "description": "Most modern reinforcement learning algorithms optimize a cumulative\nsingle-step cost along a trajectory. The optimized motions are often\n'unnatural', representing, for example, behaviors with sudden accelerations\nthat waste energy and lack predictability. In this work, we present a novel\nparadigm of controlling nonlinear systems via the minimization of the Koopman\nspectrum cost: a cost over the Koopman operator of the controlled dynamics.\nThis induces a broader class of dynamical behaviors that evolve over stable\nmanifolds such as nonlinear oscillators, closed loops, and smooth movements. We\ndemonstrate that some dynamics realizations that are not possible with a\ncumulative cost are feasible in this paradigm. Moreover, we present a provably\nefficient online learning algorithm for our problem that enjoys a sub-linear\nregret bound under some structural assumptions.",
          "link": "http://arxiv.org/abs/2106.15775",
          "publishedOn": "2021-07-01T01:59:34.622Z",
          "wordCount": 571,
          "title": "Koopman Spectrum Nonlinear Regulator and Provably Efficient Online Learning. (arXiv:2106.15775v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2102.08788",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Unal_A/0/1/0/all/0/1\">Ali Burak &#xdc;nal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfeifer_N/0/1/0/all/0/1\">Nico Pfeifer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akgun_M/0/1/0/all/0/1\">Mete Akg&#xfc;n</a>",
          "description": "Computing an AUC as a performance measure to compare the quality of different\nmachine learning models is one of the final steps of many research projects.\nMany of these methods are trained on privacy-sensitive data and there are\nseveral different approaches like $\\epsilon$-differential privacy, federated\nmachine learning and methods based on cryptographic approaches if the datasets\ncannot be shared or evaluated jointly at one place. In this setting, it can\nalso be a problem to compute the global performance measure like an AUC, since\nthe labels might also contain privacy-sensitive information. There have been\napproaches based on $\\epsilon$-differential privacy to deal with this problem,\nbut to the best of our knowledge, no exact privacy preserving solution has been\nintroduced. In this paper, we propose an MPC-based framework, called \\fw{},\nwith private merging of sorted lists and novel methods for comparing two\nsecret-shared values, selecting between two secret-shared values, converting\nthe modulus, and performing division to compute the exact AUC as one could\nobtain on the pooled original test samples. With \\fw{} computation of the exact\narea under precision-recall curve and receiver operating characteristic curve\nis even possible when ties between prediction confidence values exist. To show\nthe applicability of \\fw{}, we use it to evaluate a model trained to predict\nacute myeloid leukemia therapy response and we also assess its scalability via\nexperiments on synthetic data. The experiments show that we efficiently compute\nexactly the same AUC with both evaluation metrics in a privacy preserving\nmanner as one can obtain on the pooled test samples in the plaintext domain.\nOur solution provides security against semi-honest corruption of at most one of\nthe servers performing the secure computation.",
          "link": "http://arxiv.org/abs/2102.08788",
          "publishedOn": "2021-07-01T01:59:34.605Z",
          "wordCount": 753,
          "title": "ppAURORA: Privacy Preserving Area Under Receiver Operating Characteristic and Precision-Recall Curves with Secure 3-Party Computation. (arXiv:2102.08788v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.07850",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Corenflos_A/0/1/0/all/0/1\">Adrien Corenflos</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Thornton_J/0/1/0/all/0/1\">James Thornton</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Deligiannidis_G/0/1/0/all/0/1\">George Deligiannidis</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Doucet_A/0/1/0/all/0/1\">Arnaud Doucet</a>",
          "description": "Particle Filtering (PF) methods are an established class of procedures for\nperforming inference in non-linear state-space models. Resampling is a key\ningredient of PF, necessary to obtain low variance likelihood and states\nestimates. However, traditional resampling methods result in PF-based loss\nfunctions being non-differentiable with respect to model and PF parameters. In\na variational inference context, resampling also yields high variance gradient\nestimates of the PF-based evidence lower bound. By leveraging optimal transport\nideas, we introduce a principled differentiable particle filter and provide\nconvergence results. We demonstrate this novel method on a variety of\napplications.",
          "link": "http://arxiv.org/abs/2102.07850",
          "publishedOn": "2021-07-01T01:59:34.588Z",
          "wordCount": 571,
          "title": "Differentiable Particle Filtering via Entropy-Regularized Optimal Transport. (arXiv:2102.07850v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.03647",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gama_R/0/1/0/all/0/1\">Ricardo Gama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fernandes_H/0/1/0/all/0/1\">Hugo L. Fernandes</a>",
          "description": "The Orienteering Problem with Time Windows (OPTW) is a combinatorial\noptimization problem where the goal is to maximize the total score collected\nfrom different visited locations. The application of neural network models to\ncombinatorial optimization has recently shown promising results in dealing with\nsimilar problems, like the Travelling Salesman Problem. A neural network allows\nlearning solutions using reinforcement learning or supervised learning,\ndepending on the available data. After the learning stage, it can be\ngeneralized and quickly fine-tuned to further improve performance and\npersonalization. The advantages are evident since, for real-world applications,\nsolution quality, personalization, and execution times are all important\nfactors that should be taken into account.\n\nThis study explores the use of Pointer Network models trained using\nreinforcement learning to solve the OPTW problem. We propose a modified\narchitecture that leverages Pointer Networks to better address problems related\nwith dynamic time-dependent constraints. Among its various applications, the\nOPTW can be used to model the Tourist Trip Design Problem (TTDP). We train the\nPointer Network with the TTDP problem in mind, by sampling variables that can\nchange across tourists visiting a particular instance-region: starting\nposition, starting time, available time, and the scores given to each point of\ninterest. Once a model-region is trained, it can infer a solution for a\nparticular tourist using beam search. We based the assessment of our approach\non several existing benchmark OPTW instances. We show that it generalizes\nacross different tourists that visit each region and that it generally\noutperforms the most commonly used heuristic, while computing the solution in\nrealistic times.",
          "link": "http://arxiv.org/abs/2011.03647",
          "publishedOn": "2021-07-01T01:59:34.575Z",
          "wordCount": 718,
          "title": "A Reinforcement Learning Approach to the Orienteering Problem with Time Windows. (arXiv:2011.03647v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.00451",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Puchkin_N/0/1/0/all/0/1\">Nikita Puchkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhivotovskiy_N/0/1/0/all/0/1\">Nikita Zhivotovskiy</a>",
          "description": "We show that in pool-based active classification without assumptions on the\nunderlying distribution, if the learner is given the power to abstain from some\npredictions by paying the price marginally smaller than the average loss $1/2$\nof a random guess, exponential savings in the number of label requests are\npossible whenever they are possible in the corresponding realizable problem. We\nextend this result to provide a necessary and sufficient condition for\nexponential savings in pool-based active classification under the model\nmisspecification.",
          "link": "http://arxiv.org/abs/2102.00451",
          "publishedOn": "2021-07-01T01:59:34.558Z",
          "wordCount": 556,
          "title": "Exponential Savings in Agnostic Active Learning through Abstention. (arXiv:2102.00451v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.16239",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Piotrowski_T/0/1/0/all/0/1\">Tomasz Piotrowski</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Cavalcante_R/0/1/0/all/0/1\">Renato L. G. Cavalcante</a>",
          "description": "We derive conditions for the existence of fixed points of neural networks, an\nimportant research objective to understand their behavior in modern\napplications involving autoencoders and loop unrolling techniques, among\nothers. In particular, we focus on networks with nonnegative inputs and\nnonnegative network parameters, as often considered in the literature. We show\nthat such networks can be recognized as monotonic and (weakly) scalable\nfunctions within the framework of nonlinear Perron-Frobenius theory. This fact\nenables us to derive conditions for the existence of a nonempty fixed point set\nof the neural networks, and these conditions are weaker than those obtained\nrecently using arguments in convex analysis, which are typically based on the\nassumption of nonexpansivity of the activation functions. Furthermore, we prove\nthat the shape of the fixed point set of monotonic and weakly scalable neural\nnetworks is often an interval, which degenerates to a point for the case of\nscalable networks. The chief results of this paper are verified in numerical\nsimulations, where we consider an autoencoder-type network that first\ncompresses angular power spectra in massive MIMO systems, and, second,\nreconstruct the input spectra from the compressed signal.",
          "link": "http://arxiv.org/abs/2106.16239",
          "publishedOn": "2021-07-01T01:59:34.553Z",
          "wordCount": 624,
          "title": "Fixed points of monotonic and (weakly) scalable neural networks. (arXiv:2106.16239v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16046",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Liyue Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Leye Wang</a>",
          "description": "In the big data and AI era, context is widely exploited as extra information\nwhich makes it easier to learn a more complex pattern in machine learning\nsystems. However, most of the existing related studies seldom take context into\naccount. The difficulty lies in the unknown generalization ability of both\ncontext and its modeling techniques across different scenarios. To fill the\nabove gaps, we conduct a large-scale analytical and empirical study on the\nspatiotemporal crowd prediction (STCFP) problem that is a widely-studied and\nhot research topic. We mainly make three efforts:(i) we develop new taxonomy\nabout both context features and context modeling techniques based on extensive\ninvestigations in prevailing STCFP research; (ii) we conduct extensive\nexperiments on seven datasets with hundreds of millions of records to\nquantitatively evaluate the generalization ability of both distinct context\nfeatures and context modeling techniques; (iii) we summarize some guidelines\nfor researchers to conveniently utilize context in diverse applications.",
          "link": "http://arxiv.org/abs/2106.16046",
          "publishedOn": "2021-07-01T01:59:34.547Z",
          "wordCount": 588,
          "title": "Exploring Context Modeling Techniques on the Spatiotemporal Crowd Flow Prediction. (arXiv:2106.16046v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15980",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Jerfel_G/0/1/0/all/0/1\">Ghassen Jerfel</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wang_S/0/1/0/all/0/1\">Serena Wang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Fannjiang_C/0/1/0/all/0/1\">Clara Fannjiang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Heller_K/0/1/0/all/0/1\">Katherine A. Heller</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ma_Y/0/1/0/all/0/1\">Yian Ma</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Jordan_M/0/1/0/all/0/1\">Michael I. Jordan</a>",
          "description": "Variational Inference (VI) is a popular alternative to asymptotically exact\nsampling in Bayesian inference. Its main workhorse is optimization over a\nreverse Kullback-Leibler divergence (RKL), which typically underestimates the\ntail of the posterior leading to miscalibration and potential degeneracy.\nImportance sampling (IS), on the other hand, is often used to fine-tune and\nde-bias the estimates of approximate Bayesian inference procedures. The quality\nof IS crucially depends on the choice of the proposal distribution. Ideally,\nthe proposal distribution has heavier tails than the target, which is rarely\nachievable by minimizing the RKL. We thus propose a novel combination of\noptimization and sampling techniques for approximate Bayesian inference by\nconstructing an IS proposal distribution through the minimization of a forward\nKL (FKL) divergence. This approach guarantees asymptotic consistency and a fast\nconvergence towards both the optimal IS estimator and the optimal variational\napproximation. We empirically demonstrate on real data that our method is\ncompetitive with variational boosting and MCMC.",
          "link": "http://arxiv.org/abs/2106.15980",
          "publishedOn": "2021-07-01T01:59:34.541Z",
          "wordCount": 616,
          "title": "Variational Refinement for Importance Sampling Using the Forward Kullback-Leibler Divergence. (arXiv:2106.15980v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16091",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Leeb_F/0/1/0/all/0/1\">Felix Leeb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bauer_S/0/1/0/all/0/1\">Stefan Bauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1\">Bernhard Sch&#xf6;lkopf</a>",
          "description": "The encoders and decoders of autoencoders effectively project the input onto\nlearned manifolds in the latent space and data space respectively. We propose a\nframework, called latent responses, for probing the learned data manifold using\ninterventions in the latent space. Using this framework, we investigate \"holes\"\nin the representation to quantitatively ascertain to what extent the latent\nspace of a trained VAE is consistent with the chosen prior. Furthermore, we use\nthe identified structure to improve interpolation between latent vectors. We\nevaluate how our analyses improve the quality of the generated samples using\nthe VAE on a variety of benchmark datasets.",
          "link": "http://arxiv.org/abs/2106.16091",
          "publishedOn": "2021-07-01T01:59:34.491Z",
          "wordCount": 541,
          "title": "Interventional Assays for the Latent Space of Autoencoders. (arXiv:2106.16091v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16060",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sansone_E/0/1/0/all/0/1\">Emanuele Sansone</a>",
          "description": "This work considers the problem of learning structured representations from\nraw images using self-supervised learning. We propose a principled framework\nbased on a mutual information objective, which integrates self-supervised and\nstructure learning. Furthermore, we devise a post-hoc procedure to interpret\nthe meaning of the learnt representations. Preliminary experiments on CIFAR-10\nshow that the proposed framework achieves higher generalization performance in\ndownstream classification tasks and provides more interpretable representations\ncompared to the ones learnt through traditional self-supervised learning.",
          "link": "http://arxiv.org/abs/2106.16060",
          "publishedOn": "2021-07-01T01:59:34.477Z",
          "wordCount": 503,
          "title": "Leveraging Hidden Structure in Self-Supervised Learning. (arXiv:2106.16060v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2011.05944",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Kirschner_J/0/1/0/all/0/1\">Johannes Kirschner</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lattimore_T/0/1/0/all/0/1\">Tor Lattimore</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Vernade_C/0/1/0/all/0/1\">Claire Vernade</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Szepesvari_C/0/1/0/all/0/1\">Csaba Szepesv&#xe1;ri</a>",
          "description": "We introduce a simple and efficient algorithm for stochastic linear bandits\nwith finitely many actions that is asymptotically optimal and (nearly)\nworst-case optimal in finite time. The approach is based on the frequentist\ninformation-directed sampling (IDS) framework, with a surrogate for the\ninformation gain that is informed by the optimization problem that defines the\nasymptotic lower bound. Our analysis sheds light on how IDS balances the\ntrade-off between regret and information and uncovers a surprising connection\nbetween the recently proposed primal-dual methods and the IDS algorithm. We\ndemonstrate empirically that IDS is competitive with UCB in finite-time, and\ncan be significantly better in the asymptotic regime.",
          "link": "http://arxiv.org/abs/2011.05944",
          "publishedOn": "2021-07-01T01:59:34.464Z",
          "wordCount": 574,
          "title": "Asymptotically Optimal Information-Directed Sampling. (arXiv:2011.05944v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.16087",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Kokalj_Filipovic_S/0/1/0/all/0/1\">Silvija Kokalj-Filipovic</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Toliver_P/0/1/0/all/0/1\">Paul Toliver</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Johnson_W/0/1/0/all/0/1\">William Johnson</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Miller_R/0/1/0/all/0/1\">Rob Miller</a>",
          "description": "Current radio frequency (RF) sensors at the Edge lack the computational\nresources to support practical, in-situ training for intelligent spectrum\nmonitoring, and sensor data classification in general. We propose a solution\nvia Deep Delay Loop Reservoir Computing (DLR), a processing architecture that\nsupports general machine learning algorithms on compact mobile devices by\nleveraging delay-loop reservoir computing in combination with innovative\nelectrooptical hardware. With both digital and photonic realizations of our\ndesign of the loops, DLR delivers reductions in form factor, hardware\ncomplexity and latency, compared to the State-of-the-Art (SoA). The main impact\nof the reservoir is to project the input data into a higher dimensional space\nof reservoir state vectors in order to linearly separate the input classes.\nOnce the classes are well separated, traditionally complex, power-hungry\nclassification models are no longer needed for the learning process. Yet, even\nwith simple classifiers based on Ridge regression (RR), the complexity grows at\nleast quadratically with the input size. Hence, the hardware reduction required\nfor training on compact devices is in contradiction with the large dimension of\nstate vectors. DLR employs a RR-based classifier to exceed the SoA accuracy,\nwhile further reducing power consumption by leveraging the architecture of\nparallel (split) loops. We present DLR architectures composed of multiple\nsmaller loops whose state vectors are linearly combined to create a lower\ndimensional input into Ridge regression. We demonstrate the advantages of using\nDLR for two distinct applications: RF Specific Emitter Identification (SEI) for\nIoT authentication, and wireless protocol recognition for IoT situational\nawareness.",
          "link": "http://arxiv.org/abs/2106.16087",
          "publishedOn": "2021-07-01T01:59:34.417Z",
          "wordCount": 710,
          "title": "Reservoir Based Edge Training on RF Data To Deliver Intelligent and Efficient IoT Spectrum Sensors. (arXiv:2106.16087v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16188",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yifei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_V/0/1/0/all/0/1\">Vincent Gao</a>",
          "description": "E-commerce stores collect customer feedback to let sellers learn about\ncustomer concerns and enhance customer order experience. Because customer\nfeedback often contains redundant information, a concise summary of the\nfeedback can be generated to help sellers better understand the issues causing\ncustomer dissatisfaction. Previous state-of-the-art abstractive text\nsummarization models make two major types of factual errors when producing\nsummaries from customer feedback, which are wrong entity detection (WED) and\nincorrect product-defect description (IPD). In this work, we introduce a set of\nmethods to enhance the factual consistency of abstractive summarization on\ncustomer feedback. We augment the training data with artificially corrupted\nsummaries, and use them as counterparts of the target summaries. We add a\ncontrastive loss term into the training objective so that the model learns to\navoid certain factual errors. Evaluation results show that a large portion of\nWED and IPD errors are alleviated for BART and T5. Furthermore, our approaches\ndo not depend on the structure of the summarization model and thus are\ngeneralizable to any abstractive summarization systems.",
          "link": "http://arxiv.org/abs/2106.16188",
          "publishedOn": "2021-07-01T01:59:34.392Z",
          "wordCount": 606,
          "title": "Improving Factual Consistency of Abstractive Summarization on Customer Feedback. (arXiv:2106.16188v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2001.10133",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1\">Ping Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_Z/0/1/0/all/0/1\">Zhi Tian</a>",
          "description": "This paper studies the decentralized optimization and learning problem where\nmultiple interconnected agents aim to learn an optimal decision function\ndefined over a reproducing kernel Hilbert space by jointly minimizing a global\nobjective function, with access to their own locally observed dataset. As a\nnon-parametric approach, kernel learning faces a major challenge in distributed\nimplementation: the decision variables of local objective functions are\ndata-dependent and thus cannot be optimized under the decentralized consensus\nframework without any raw data exchange among agents. To circumvent this major\nchallenge, we leverage the random feature (RF) approximation approach to enable\nconsensus on the function modeled in the RF space by data-independent\nparameters across different agents. We then design an iterative algorithm,\ntermed DKLA, for fast-convergent implementation via ADMM. Based on DKLA, we\nfurther develop a communication-censored kernel learning (COKE) algorithm that\nreduces the communication load of DKLA by preventing an agent from transmitting\nat every iteration unless its local updates are deemed informative. Theoretical\nresults in terms of linear convergence guarantee and generalization performance\nanalysis of DKLA and COKE are provided. Comprehensive tests on both synthetic\nand real datasets are conducted to verify the communication efficiency and\nlearning effectiveness of COKE.",
          "link": "http://arxiv.org/abs/2001.10133",
          "publishedOn": "2021-07-01T01:59:34.375Z",
          "wordCount": 655,
          "title": "COKE: Communication-Censored Decentralized Kernel Learning. (arXiv:2001.10133v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.16079",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Pihlajasalo_J/0/1/0/all/0/1\">Jaakko Pihlajasalo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Korpi_D/0/1/0/all/0/1\">Dani Korpi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Honkala_M/0/1/0/all/0/1\">Mikko Honkala</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huttunen_J/0/1/0/all/0/1\">Janne M.J. Huttunen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Riihonen_T/0/1/0/all/0/1\">Taneli Riihonen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Talvitie_J/0/1/0/all/0/1\">Jukka Talvitie</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Brihuega_A/0/1/0/all/0/1\">Alberto Brihuega</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Uusitalo_M/0/1/0/all/0/1\">Mikko A. Uusitalo</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Valkama_M/0/1/0/all/0/1\">Mikko Valkama</a>",
          "description": "In this paper, we propose a machine learning (ML) based physical layer\nreceiver solution for demodulating OFDM signals that are subject to a high\nlevel of nonlinear distortion. Specifically, a novel deep learning based\nconvolutional neural network receiver is devised, containing layers in both\ntime- and frequency domains, allowing to demodulate and decode the transmitted\nbits reliably despite the high error vector magnitude (EVM) in the transmit\nsignal. Extensive set of numerical results is provided, in the context of 5G NR\nuplink incorporating also measured terminal power amplifier characteristics.\nThe obtained results show that the proposed receiver system is able to clearly\noutperform classical linear receivers as well as existing ML receiver\napproaches, especially when the EVM is high in comparison with modulation\norder. The proposed ML receiver can thus facilitate pushing the terminal power\namplifier (PA) systems deeper into saturation, and thereon improve the terminal\npower-efficiency, radiated power and network coverage.",
          "link": "http://arxiv.org/abs/2106.16079",
          "publishedOn": "2021-07-01T01:59:34.369Z",
          "wordCount": 617,
          "title": "HybridDeepRx: Deep Learning Receiver for High-EVM Signals. (arXiv:2106.16079v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16225",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sidak Pal Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bachmann_G/0/1/0/all/0/1\">Gregor Bachmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hofmann_T/0/1/0/all/0/1\">Thomas Hofmann</a>",
          "description": "The Hessian of a neural network captures parameter interactions through\nsecond-order derivatives of the loss. It is a fundamental object of study,\nclosely tied to various problems in deep learning, including model design,\noptimization, and generalization. Most prior work has been empirical, typically\nfocusing on low-rank approximations and heuristics that are blind to the\nnetwork structure. In contrast, we develop theoretical tools to analyze the\nrange of the Hessian map, providing us with a precise understanding of its rank\ndeficiency as well as the structural reasons behind it. This yields exact\nformulas and tight upper bounds for the Hessian rank of deep linear networks,\nallowing for an elegant interpretation in terms of rank deficiency. Moreover,\nwe demonstrate that our bounds remain faithful as an estimate of the numerical\nHessian rank, for a larger class of models such as rectified and hyperbolic\ntangent networks. Further, we also investigate the implications of model\narchitecture (e.g.~width, depth, bias) on the rank deficiency. Overall, our\nwork provides novel insights into the source and extent of redundancy in\noverparameterized networks.",
          "link": "http://arxiv.org/abs/2106.16225",
          "publishedOn": "2021-07-01T01:59:34.364Z",
          "wordCount": 627,
          "title": "Analytic Insights into Structure and Rank of Neural Network Hessian Maps. (arXiv:2106.16225v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16200",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Franzese_G/0/1/0/all/0/1\">Giulio Franzese</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milios_D/0/1/0/all/0/1\">Dimitrios Milios</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Filippone_M/0/1/0/all/0/1\">Maurizio Filippone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Michiardi_P/0/1/0/all/0/1\">Pietro Michiardi</a>",
          "description": "In this work, we revisit the theoretical properties of Hamiltonian stochastic\ndifferential equations (SDEs) for Bayesian posterior sampling, and we study the\ntwo types of errors that arise from numerical SDE simulation: the\ndiscretization error and the error due to noisy gradient estimates in the\ncontext of data subsampling. We consider overlooked results describing the\nergodic convergence rates of numerical integration schemes, and we produce a\nnovel analysis for the effect of mini-batches through the lens of differential\noperator splitting. In our analysis, the stochastic component of the proposed\nHamiltonian SDE is decoupled from the gradient noise, for which we make no\nnormality assumptions. This allows us to derive interesting connections among\ndifferent sampling schemes, including the original Hamiltonian Monte Carlo\n(HMC) algorithm, and explain their performance. We show that for a careful\nselection of numerical integrators, both errors vanish at a rate\n$\\mathcal{O}(\\eta^2)$, where $\\eta$ is the integrator step size. Our\ntheoretical results are supported by an empirical study on a variety of\nregression and classification tasks for Bayesian neural networks.",
          "link": "http://arxiv.org/abs/2106.16200",
          "publishedOn": "2021-07-01T01:59:34.358Z",
          "wordCount": 603,
          "title": "A Unified View of Stochastic Hamiltonian Sampling. (arXiv:2106.16200v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15883",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Parker_Holder_J/0/1/0/all/0/1\">Jack Parker-Holder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_V/0/1/0/all/0/1\">Vu Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Desai_S/0/1/0/all/0/1\">Shaan Desai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roberts_S/0/1/0/all/0/1\">Stephen Roberts</a>",
          "description": "Despite a series of recent successes in reinforcement learning (RL), many RL\nalgorithms remain sensitive to hyperparameters. As such, there has recently\nbeen interest in the field of AutoRL, which seeks to automate design decisions\nto create more general algorithms. Recent work suggests that population based\napproaches may be effective AutoRL algorithms, by learning hyperparameter\nschedules on the fly. In particular, the PB2 algorithm is able to achieve\nstrong performance in RL tasks by formulating online hyperparameter\noptimization as time varying GP-bandit problem, while also providing\ntheoretical guarantees. However, PB2 is only designed to work for continuous\nhyperparameters, which severely limits its utility in practice. In this paper\nwe introduce a new (provably) efficient hierarchical approach for optimizing\nboth continuous and categorical variables, using a new time-varying bandit\nalgorithm specifically designed for the population based training regime. We\nevaluate our approach on the challenging Procgen benchmark, where we show that\nexplicitly modelling dependence between data augmentation and other\nhyperparameters improves generalization.",
          "link": "http://arxiv.org/abs/2106.15883",
          "publishedOn": "2021-07-01T01:59:34.352Z",
          "wordCount": 599,
          "title": "Tuning Mixed Input Hyperparameters on the Fly for Efficient Population Based AutoRL. (arXiv:2106.15883v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16006",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tabani_H/0/1/0/all/0/1\">Hamid Tabani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balasubramaniam_A/0/1/0/all/0/1\">Ajay Balasubramaniam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marzban_S/0/1/0/all/0/1\">Shabbir Marzban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arani_E/0/1/0/all/0/1\">Elahe Arani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zonooz_B/0/1/0/all/0/1\">Bahram Zonooz</a>",
          "description": "Transformers provide promising accuracy and have become popular and used in\nvarious domains such as natural language processing and computer vision.\nHowever, due to their massive number of model parameters, memory and\ncomputation requirements, they are not suitable for resource-constrained\nlow-power devices. Even with high-performance and specialized devices, the\nmemory bandwidth can become a performance-limiting bottleneck. In this paper,\nwe present a performance analysis of state-of-the-art vision transformers on\nseveral devices. We propose to reduce the overall memory footprint and memory\ntransfers by clustering the model parameters. We show that by using only 64\nclusters to represent model parameters, it is possible to reduce the data\ntransfer from the main memory by more than 4x, achieve up to 22% speedup and\n39% energy savings on mobile devices with less than 0.1% accuracy loss.",
          "link": "http://arxiv.org/abs/2106.16006",
          "publishedOn": "2021-07-01T01:59:34.333Z",
          "wordCount": 589,
          "title": "Improving the Efficiency of Transformers for Resource-Constrained Devices. (arXiv:2106.16006v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16042",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Z/0/1/0/all/0/1\">Zhongyuan Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_D/0/1/0/all/0/1\">Dong Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuan Zhang</a>",
          "description": "We introduce a unified framework, formulated as general latent space models,\nto study complex higher-order network interactions among multiple entities. Our\nframework covers several popular models in recent network analysis literature,\nincluding mixture multi-layer latent space model and hypergraph latent space\nmodel. We formulate the relationship between the latent positions and the\nobserved data via a generalized multilinear kernel as the link function. While\nour model enjoys decent generality, its maximum likelihood parameter estimation\nis also convenient via a generalized tensor decomposition procedure.We propose\na novel algorithm using projected gradient descent on Grassmannians. We also\ndevelop original theoretical guarantees for our algorithm. First, we show its\nlinear convergence under mild conditions. Second, we establish finite-sample\nstatistical error rates of latent position estimation, determined by the signal\nstrength, degrees of freedom and the smoothness of link function, for both\ngeneral and specific latent space models. We demonstrate the effectiveness of\nour method on synthetic data. We also showcase the merit of our method on two\nreal-world datasets that are conventionally described by different specific\nmodels in producing meaningful and interpretable parameter estimations and\naccurate link prediction. We demonstrate the effectiveness of our method on\nsynthetic data. We also showcase the merit of our method on two real-world\ndatasets that are conventionally described by different specific models in\nproducing meaningful and interpretable parameter estimations and accurate link\nprediction.",
          "link": "http://arxiv.org/abs/2106.16042",
          "publishedOn": "2021-07-01T01:59:34.324Z",
          "wordCount": 667,
          "title": "Latent Space Model for Higher-order Networks and Generalized Tensor Decomposition. (arXiv:2106.16042v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16037",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ceran_E/0/1/0/all/0/1\">Elif Tugce Ceran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gunduz_D/0/1/0/all/0/1\">Deniz Gunduz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gyorgy_A/0/1/0/all/0/1\">Andras Gyorgy</a>",
          "description": "The time average expected age of information (AoI) is studied for status\nupdates sent over an error-prone channel from an energy-harvesting transmitter\nwith a finite-capacity battery. Energy cost of sensing new status updates is\ntaken into account as well as the transmission energy cost better capturing\npractical systems. The optimal scheduling policy is first studied under the\nhybrid automatic repeat request (HARQ) protocol when the channel and energy\nharvesting statistics are known, and the existence of a threshold-based optimal\npolicy is shown. For the case of unknown environments, average-cost\nreinforcement-learning algorithms are proposed that learn the system parameters\nand the status update policy in real-time. The effectiveness of the proposed\nmethods is demonstrated through numerical results.",
          "link": "http://arxiv.org/abs/2106.16037",
          "publishedOn": "2021-07-01T01:59:34.307Z",
          "wordCount": 577,
          "title": "Learning to Minimize Age of Information over an Unreliable Channel with Energy Harvesting. (arXiv:2106.16037v1 [cs.IT])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16048",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Mou_Z/0/1/0/all/0/1\">Zhiyu Mou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gao_F/0/1/0/all/0/1\">Feifei Gao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1\">Jun Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wu_Q/0/1/0/all/0/1\">Qihui Wu</a>",
          "description": "In this paper, we study the self-healing problem of unmanned aerial vehicle\n(UAV) swarm network (USNET) that is required to quickly rebuild the\ncommunication connectivity under unpredictable external disruptions (UEDs).\nFirstly, to cope with the one-off UEDs, we propose a graph convolutional neural\nnetwork (GCN) and find the recovery topology of the USNET in an on-line manner.\nSecondly, to cope with general UEDs, we develop a GCN based trajectory planning\nalgorithm that can make UAVs rebuild the communication connectivity during the\nself-healing process. We also design a meta learning scheme to facilitate the\non-line executions of the GCN. Numerical results show that the proposed\nalgorithms can rebuild the communication connectivity of the USNET more quickly\nthan the existing algorithms under both one-off UEDs and general UEDs. The\nsimulation results also show that the meta learning scheme can not only enhance\nthe performance of the GCN but also reduce the time complexity of the on-line\nexecutions.",
          "link": "http://arxiv.org/abs/2106.16048",
          "publishedOn": "2021-07-01T01:59:34.290Z",
          "wordCount": 595,
          "title": "Resilient UAV Swarm Communications with Graph Convolutional Neural Network. (arXiv:2106.16048v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2105.06241",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Heckerman_D/0/1/0/all/0/1\">David Heckerman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geiger_D/0/1/0/all/0/1\">Dan Geiger</a>",
          "description": "We develop simple methods for constructing likelihoods and parameter priors\nfor learning about the parameters and structure of a Bayesian network. In\nparticular, we introduce several assumptions that permit the construction of\nlikelihoods and parameter priors for a large number of Bayesian-network\nstructures from a small set of assessments. The most notable assumption is that\nof likelihood equivalence, which says that data can not help to discriminate\nnetwork structures that encode the same assertions of conditional independence.\nWe describe the constructions that follow from these assumptions, and also\npresent a method for directly computing the marginal likelihood of a random\nsample with no missing observations. Also, we show how these assumptions lead\nto a general framework for characterizing parameter priors of multivariate\ndistributions.",
          "link": "http://arxiv.org/abs/2105.06241",
          "publishedOn": "2021-07-01T01:59:34.181Z",
          "wordCount": 592,
          "title": "Likelihoods and Parameter Priors for Bayesian Networks. (arXiv:2105.06241v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.00510",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ghosal_D/0/1/0/all/0/1\">Deepanway Ghosal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hong_P/0/1/0/all/0/1\">Pengfei Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_S/0/1/0/all/0/1\">Siqi Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Majumder_N/0/1/0/all/0/1\">Navonil Majumder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihalcea_R/0/1/0/all/0/1\">Rada Mihalcea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poria_S/0/1/0/all/0/1\">Soujanya Poria</a>",
          "description": "Commonsense inference to understand and explain human language is a\nfundamental research problem in natural language processing. Explaining human\nconversations poses a great challenge as it requires contextual understanding,\nplanning, inference, and several aspects of reasoning including causal,\ntemporal, and commonsense reasoning. In this work, we introduce CIDER -- a\nmanually curated dataset that contains dyadic dialogue explanations in the form\nof implicit and explicit knowledge triplets inferred using contextual\ncommonsense inference. Extracting such rich explanations from conversations can\nbe conducive to improving several downstream applications. The annotated\ntriplets are categorized by the type of commonsense knowledge present (e.g.,\ncausal, conditional, temporal). We set up three different tasks conditioned on\nthe annotated dataset: Dialogue-level Natural Language Inference, Span\nExtraction, and Multi-choice Span Selection. Baseline results obtained with\ntransformer-based models reveal that the tasks are difficult, paving the way\nfor promising future research. The dataset and the baseline implementations are\npublicly available at https://cider-task.github.io/cider/.",
          "link": "http://arxiv.org/abs/2106.00510",
          "publishedOn": "2021-07-01T01:59:34.166Z",
          "wordCount": 618,
          "title": "CIDER: Commonsense Inference for Dialogue Explanation and Reasoning. (arXiv:2106.00510v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15921",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Thin_A/0/1/0/all/0/1\">Achille Thin</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Kotelevskii_N/0/1/0/all/0/1\">Nikita Kotelevskii</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Doucet_A/0/1/0/all/0/1\">Arnaud Doucet</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Durmus_A/0/1/0/all/0/1\">Alain Durmus</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Moulines_E/0/1/0/all/0/1\">Eric Moulines</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Panov_M/0/1/0/all/0/1\">Maxim Panov</a>",
          "description": "Variational auto-encoders (VAE) are popular deep latent variable models which\nare trained by maximizing an Evidence Lower Bound (ELBO). To obtain tighter\nELBO and hence better variational approximations, it has been proposed to use\nimportance sampling to get a lower variance estimate of the evidence. However,\nimportance sampling is known to perform poorly in high dimensions. While it has\nbeen suggested many times in the literature to use more sophisticated\nalgorithms such as Annealed Importance Sampling (AIS) and its Sequential\nImportance Sampling (SIS) extensions, the potential benefits brought by these\nadvanced techniques have never been realized for VAE: the AIS estimate cannot\nbe easily differentiated, while SIS requires the specification of carefully\nchosen backward Markov kernels. In this paper, we address both issues and\ndemonstrate the performance of the resulting Monte Carlo VAEs on a variety of\napplications.",
          "link": "http://arxiv.org/abs/2106.15921",
          "publishedOn": "2021-07-01T01:59:34.160Z",
          "wordCount": 567,
          "title": "Monte Carlo Variational Auto-Encoders. (arXiv:2106.15921v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2105.13813",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pitchforth_D/0/1/0/all/0/1\">Daniel J Pitchforth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rogers_T/0/1/0/all/0/1\">Timothy J Rogers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tygesen_U/0/1/0/all/0/1\">Ulf T Tygesen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cross_E/0/1/0/all/0/1\">Elizabeth J Cross</a>",
          "description": "The quantification of wave loading on offshore structures and components is a\ncrucial element in the assessment of their useful remaining life. In many\napplications the well-known Morison's equation is employed to estimate the\nforcing from waves with assumed particle velocities and accelerations. This\npaper develops a grey-box modelling approach to improve the predictions of the\nforce on structural members. A grey-box model intends to exploit the enhanced\npredictive capabilities of data-based modelling whilst retaining physical\ninsight into the behaviour of the system; in the context of the work carried\nout here, this can be considered as physics-informed machine learning. There\nare a number of possible approaches to establish a grey-box model. This paper\ndemonstrates two means of combining physics (white box) and data-based (black\nbox) components; one where the model is a simple summation of the two\ncomponents, the second where the white-box prediction is fed into the black box\nas an additional input. Here Morison's equation is used as the physics-based\ncomponent in combination with a data-based Gaussian process NARX - a dynamic\nvariant of the more well-known Gaussian process regression. Two key challenges\nwith employing the GP-NARX formulation that are addressed here are the\nselection of appropriate lag terms and the proper treatment of uncertainty\npropagation within the dynamic GP. The best performing grey-box model, the\nresidual modelling GP-NARX, was able to achieve a 29.13\\% and 5.48\\% relative\nreduction in NMSE over Morison's Equation and a black-box GP-NARX respectively,\nalongside significant benefits in extrapolative capabilities of the model, in\ncircumstances of low dataset coverage.",
          "link": "http://arxiv.org/abs/2105.13813",
          "publishedOn": "2021-07-01T01:59:34.129Z",
          "wordCount": 732,
          "title": "Grey-box models for wave loading prediction. (arXiv:2105.13813v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.05739",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Domingo_Enrich_C/0/1/0/all/0/1\">Carles Domingo-Enrich</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Mroueh_Y/0/1/0/all/0/1\">Youssef Mroueh</a>",
          "description": "Several works in implicit and explicit generative modeling empirically\nobserved that feature-learning discriminators outperform fixed-kernel\ndiscriminators in terms of the sample quality of the models. We provide\nseparation results between probability metrics with fixed-kernel and\nfeature-learning discriminators using the function classes $\\mathcal{F}_2$ and\n$\\mathcal{F}_1$ respectively, which were developed to study overparametrized\ntwo-layer neural networks. In particular, we construct pairs of distributions\nover hyper-spheres that can not be discriminated by fixed kernel\n$(\\mathcal{F}_2)$ integral probability metric (IPM) and Stein discrepancy (SD)\nin high dimensions, but that can be discriminated by their feature learning\n($\\mathcal{F}_1$) counterparts. To further study the separation we provide\nlinks between the $\\mathcal{F}_1$ and $\\mathcal{F}_2$ IPMs with sliced\nWasserstein distances. Our work suggests that fixed-kernel discriminators\nperform worse than their feature learning counterparts because their\ncorresponding metrics are weaker.",
          "link": "http://arxiv.org/abs/2106.05739",
          "publishedOn": "2021-07-01T01:59:34.100Z",
          "wordCount": 601,
          "title": "Separation Results between Fixed-Kernel and Feature-Learning Probability Metrics. (arXiv:2106.05739v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.04140",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1\">Byeonggeun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Simyung Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jinkyu Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sung_D/0/1/0/all/0/1\">Dooyong Sung</a>",
          "description": "Keyword spotting is an important research field because it plays a key role\nin device wake-up and user interaction on smart devices. However, it is\nchallenging to minimize errors while operating efficiently in devices with\nlimited resources such as mobile phones. We present a broadcasted residual\nlearning method to achieve high accuracy with small model size and\ncomputational load. Our method configures most of the residual functions as 1D\ntemporal convolution while still allows 2D convolution together using a\nbroadcasted-residual connection that expands temporal output to\nfrequency-temporal dimension. This residual mapping enables the network to\neffectively represent useful audio features with much less computation than\nconventional convolutional neural networks. We also propose a novel network\narchitecture, Broadcasting-residual network (BC-ResNet), based on broadcasted\nresidual learning and describe how to scale up the model according to the\ntarget device's resources. BC-ResNets achieve state-of-the-art 98.0% and 98.7%\ntop-1 accuracy on Google speech command datasets v1 and v2, respectively, and\nconsistently outperform previous approaches, using fewer computations and\nparameters.",
          "link": "http://arxiv.org/abs/2106.04140",
          "publishedOn": "2021-07-01T01:59:34.094Z",
          "wordCount": 624,
          "title": "Broadcasted Residual Learning for Efficient Keyword Spotting. (arXiv:2106.04140v2 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.16118",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kollar_T/0/1/0/all/0/1\">Thomas Kollar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laskey_M/0/1/0/all/0/1\">Michael Laskey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stone_K/0/1/0/all/0/1\">Kevin Stone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thananjeyan_B/0/1/0/all/0/1\">Brijen Thananjeyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tjersland_M/0/1/0/all/0/1\">Mark Tjersland</a>",
          "description": "Robot manipulation of unknown objects in unstructured environments is a\nchallenging problem due to the variety of shapes, materials, arrangements and\nlighting conditions. Even with large-scale real-world data collection, robust\nperception and manipulation of transparent and reflective objects across\nvarious lighting conditions remain challenging. To address these challenges we\npropose an approach to performing sim-to-real transfer of robotic perception.\nThe underlying model, SimNet, is trained as a single multi-headed neural\nnetwork using simulated stereo data as input and simulated object segmentation\nmasks, 3D oriented bounding boxes (OBBs), object keypoints, and disparity as\noutput. A key component of SimNet is the incorporation of a learned stereo\nsub-network that predicts disparity. SimNet is evaluated on 2D car detection,\nunknown object detection, and deformable object keypoint detection and\nsignificantly outperforms a baseline that uses a structured light RGB-D sensor.\nBy inferring grasp positions using the OBB and keypoint predictions, SimNet can\nbe used to perform end-to-end manipulation of unknown objects in both easy and\nhard scenarios using our fleet of Toyota HSR robots in four home environments.\nIn unknown object grasping experiments, the predictions from the baseline RGB-D\nnetwork and SimNet enable successful grasps of most of the easy objects.\nHowever, the RGB-D baseline only grasps 35% of the hard (e.g., transparent)\nobjects, while SimNet grasps 95%, suggesting that SimNet can enable robust\nmanipulation of unknown objects, including transparent objects, in unknown\nenvironments.",
          "link": "http://arxiv.org/abs/2106.16118",
          "publishedOn": "2021-07-01T01:59:34.088Z",
          "wordCount": 683,
          "title": "SimNet: Enabling Robust Unknown Object Manipulation from Pure Synthetic Data via Stereo. (arXiv:2106.16118v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16088",
          "author": "<a href=\"http://arxiv.org/find/q-fin/1/au:+Bajpai_S/0/1/0/all/0/1\">Supriya Bajpai</a>",
          "description": "In stock trading, feature extraction and trading strategy design are the two\nimportant tasks to achieve long-term benefits using machine learning\ntechniques. Several methods have been proposed to design trading strategy by\nacquiring trading signals to maximize the rewards. In the present paper the\ntheory of deep reinforcement learning is applied for stock trading strategy and\ninvestment decisions to Indian markets. The experiments are performed\nsystematically with three classical Deep Reinforcement Learning models Deep\nQ-Network, Double Deep Q-Network and Dueling Double Deep Q-Network on ten\nIndian stock datasets. The performance of the models are evaluated and\ncomparison is made.",
          "link": "http://arxiv.org/abs/2106.16088",
          "publishedOn": "2021-07-01T01:59:34.076Z",
          "wordCount": 540,
          "title": "Application of deep reinforcement learning for Indian stock trading automation. (arXiv:2106.16088v1 [q-fin.TR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07036",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Clyde_A/0/1/0/all/0/1\">Austin Clyde</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Brettin_T/0/1/0/all/0/1\">Thomas Brettin</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Partin_A/0/1/0/all/0/1\">Alexander Partin</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Yoo_H/0/1/0/all/0/1\">Hyunseung Yoo</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Babuji_Y/0/1/0/all/0/1\">Yadu Babuji</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Blaiszik_B/0/1/0/all/0/1\">Ben Blaiszik</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Merzky_A/0/1/0/all/0/1\">Andre Merzky</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Turilli_M/0/1/0/all/0/1\">Matteo Turilli</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Jha_S/0/1/0/all/0/1\">Shantenu Jha</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Ramanathan_A/0/1/0/all/0/1\">Arvind Ramanathan</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Stevens_R/0/1/0/all/0/1\">Rick Stevens</a>",
          "description": "We propose a benchmark to study surrogate model accuracy for protein-ligand\ndocking. We share a dataset consisting of 200 million 3D complex structures and\n2D structure scores across a consistent set of 13 million \"in-stock\" molecules\nover 15 receptors, or binding sites, across the SARS-CoV-2 proteome. Our work\nshows surrogate docking models have six orders of magnitude more throughput\nthan standard docking protocols on the same supercomputer node types. We\ndemonstrate the power of high-speed surrogate models by running each target\nagainst 1 billion molecules in under a day (50k predictions per GPU seconds).\nWe showcase a workflow for docking utilizing surrogate ML models as a\npre-filter. Our workflow is ten times faster at screening a library of\ncompounds than the standard technique, with an error rate less than 0.01\\% of\ndetecting the underlying best scoring 0.1\\% of compounds. Our analysis of the\nspeedup explains that to screen more molecules under a docking paradigm,\nanother order of magnitude speedup must come from model accuracy rather than\ncomputing speed (which, if increased, will not anymore alter our throughput to\nscreen molecules). We believe this is strong evidence for the community to\nbegin focusing on improving the accuracy of surrogate models to improve the\nability to screen massive compound libraries 100x or even 1000x faster than\ncurrent techniques.",
          "link": "http://arxiv.org/abs/2106.07036",
          "publishedOn": "2021-07-01T01:59:34.051Z",
          "wordCount": 734,
          "title": "Protein-Ligand Docking Surrogate Models: A SARS-CoV-2 Benchmark for Deep Learning Accelerated Virtual Screening. (arXiv:2106.07036v2 [q-bio.BM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.03123",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ge_W/0/1/0/all/0/1\">Wanying Ge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panariello_M/0/1/0/all/0/1\">Michele Panariello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patino_J/0/1/0/all/0/1\">Jose Patino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Todisco_M/0/1/0/all/0/1\">Massimiliano Todisco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Evans_N/0/1/0/all/0/1\">Nicholas Evans</a>",
          "description": "This paper reports the first successful application of a differentiable\narchitecture search (DARTS) approach to the deepfake and spoofing detection\nproblems. An example of neural architecture search, DARTS operates upon a\ncontinuous, differentiable search space which enables both the architecture and\nparameters to be optimised via gradient descent. Solutions based on\npartially-connected DARTS use random channel masking in the search space to\nreduce GPU time and automatically learn and optimise complex neural\narchitectures composed of convolutional operations and residual blocks. Despite\nbeing learned quickly with little human effort, the resulting networks are\ncompetitive with the best performing systems reported in the literature. Some\nare also far less complex, containing 85% fewer parameters than a Res2Net\ncompetitor.",
          "link": "http://arxiv.org/abs/2104.03123",
          "publishedOn": "2021-07-01T01:59:34.038Z",
          "wordCount": 594,
          "title": "Partially-Connected Differentiable Architecture Search for Deepfake and Spoofing Detection. (arXiv:2104.03123v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.08440",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ilhan_E/0/1/0/all/0/1\">Ercument Ilhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gow_J/0/1/0/all/0/1\">Jeremy Gow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_Liebana_D/0/1/0/all/0/1\">Diego Perez-Liebana</a>",
          "description": "Deep Reinforcement Learning (RL) techniques can benefit greatly from\nleveraging prior experience, which can be either self-generated or acquired\nfrom other entities. Action advising is a framework that provides a flexible\nway to transfer such knowledge in the form of actions between teacher-student\npeers. However, due to the realistic concerns, the number of these interactions\nis limited with a budget; therefore, it is crucial to perform these in the most\nappropriate moments. There have been several promising studies recently that\naddress this problem setting especially from the student's perspective. Despite\ntheir success, they have some shortcomings when it comes to the practical\napplicability and integrity as an overall solution to the learning from advice\nchallenge. In this paper, we extend the idea of advice reusing via teacher\nimitation to construct a unified approach that addresses both advice collection\nand advice utilisation problems. We also propose a method to automatically tune\nthe relevant hyperparameters of these components on-the-fly to make it able to\nadapt to any task with minimal human intervention. The experiments we performed\nin 5 different Atari games verify that our algorithm either surpasses or\nperforms on-par with its top competitors while being far simpler to be\nemployed. Furthermore, its individual components are also found to be providing\nsignificant advantages alone.",
          "link": "http://arxiv.org/abs/2104.08440",
          "publishedOn": "2021-07-01T01:59:34.032Z",
          "wordCount": 678,
          "title": "Learning on a Budget via Teacher Imitation. (arXiv:2104.08440v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.04121",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fompeyrine_D/0/1/0/all/0/1\">D. Fompeyrine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vorm_E/0/1/0/all/0/1\">E. S. Vorm</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ricka_N/0/1/0/all/0/1\">N. Ricka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rose_F/0/1/0/all/0/1\">F. Rose</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pellegrin_G/0/1/0/all/0/1\">G. Pellegrin</a>",
          "description": "Machine Learning (ML) has recently been demonstrated to rival expert-level\nhuman accuracy in prediction and detection tasks in a variety of domains,\nincluding medicine. Despite these impressive findings, however, a key barrier\nto the full realization of ML's potential in medical prognoses is technology\nacceptance. Recent efforts to produce explainable AI (XAI) have made progress\nin improving the interpretability of some ML models, but these efforts suffer\nfrom limitations intrinsic to their design: they work best at identifying why a\nsystem fails, but do poorly at explaining when and why a model's prediction is\ncorrect. We posit that the acceptability of ML predictions in expert domains is\nlimited by two key factors: the machine's horizon of prediction that extends\nbeyond human capability, and the inability for machine predictions to\nincorporate human intuition into their models. We propose the use of a novel ML\narchitecture, Neural Ordinary Differential Equations (NODEs) to enhance human\nunderstanding and encourage acceptability. Our approach prioritizes human\ncognitive intuition at the center of the algorithm design, and offers a\ndistribution of predictions rather than single outputs. We explain how this\napproach may significantly improve human-machine collaboration in prediction\ntasks in expert domains such as medical prognoses. We propose a model and\ndemonstrate, by expanding a concrete example from the literature, how our model\nadvances the vision of future hybrid Human-AI systems.",
          "link": "http://arxiv.org/abs/2102.04121",
          "publishedOn": "2021-07-01T01:59:34.016Z",
          "wordCount": 702,
          "title": "Enhancing Human-Machine Teaming for Medical Prognosis Through Neural Ordinary Differential Equations (NODEs). (arXiv:2102.04121v2 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.05704",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hassani_A/0/1/0/all/0/1\">Ali Hassani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walton_S/0/1/0/all/0/1\">Steven Walton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_N/0/1/0/all/0/1\">Nikhil Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abuduweili_A/0/1/0/all/0/1\">Abulikemu Abuduweili</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiachen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Humphrey Shi</a>",
          "description": "With the rise of Transformers as the standard for language processing, and\ntheir advancements in computer vision, along with their unprecedented size and\namounts of training data, many have come to believe that they are not suitable\nfor small sets of data. This trend leads to great concerns, including but not\nlimited to: limited availability of data in certain scientific domains and the\nexclusion of those with limited resource from research in the field. In this\npaper, we dispel the myth that transformers are \"data hungry\" and therefore can\nonly be applied to large sets of data. We show for the first time that with the\nright size and tokenization, transformers can perform head-to-head with\nstate-of-the-art CNNs on small datasets. Our model eliminates the requirement\nfor class token and positional embeddings through a novel sequence pooling\nstrategy and the use of convolutions. We show that compared to CNNs, our\ncompact transformers have fewer parameters and MACs, while obtaining similar\naccuracies. Our method is flexible in terms of model size, and can have as\nlittle as 0.28M parameters and achieve reasonable results. It can reach an\naccuracy of 95.29 % when training from scratch on CIFAR-10, which is comparable\nwith modern CNN based approaches, and a significant improvement over previous\nTransformer based models. Our simple and compact design democratizes\ntransformers by making them accessible to those equipped with basic computing\nresources and/or dealing with important small datasets. Our method works on\nlarger datasets, such as ImageNet (80.28% accuracy with 29% parameters of ViT),\nand NLP tasks as well. Our code and pre-trained models are publicly available\nat https://github.com/SHI-Labs/Compact-Transformers.",
          "link": "http://arxiv.org/abs/2104.05704",
          "publishedOn": "2021-07-01T01:59:34.010Z",
          "wordCount": 748,
          "title": "Escaping the Big Data Paradigm with Compact Transformers. (arXiv:2104.05704v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10479",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tan_Y/0/1/0/all/0/1\">Yang Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shao-Lun Huang</a>",
          "description": "Transferability estimation is an essential problem in transfer learning to\npredict how good the performance is when transferring a source model (or source\ntask) to a target task. Recent analytical transferability metrics have been\nwidely used for source model selection and multi-task learning. A major\nchallenge is how to make transfereability estimation robust under the\ncross-domain cross-task settings. The recently proposed OTCE score solves this\nproblem by considering both domain and task differences, with the help of\ntransfer experiences on auxiliary tasks, which causes an efficiency overhead.\nIn this work, we propose a practical transferability metric called JC-NCE score\nthat dramatically improves the robustness of the task difference estimation in\nOTCE, thus removing the need for auxiliary tasks. Specifically, we build the\njoint correspondences between source and target data via solving an optimal\ntransport problem with a ground cost considering both the sample distance and\nlabel distance, and then compute the transferability score as the negative\nconditional entropy of the matched labels. Extensive validations under the\nintra-dataset and inter-dataset transfer settings demonstrate that our JC-NCE\nscore outperforms the auxiliary-task free version of OTCE for 7% and 12%,\nrespectively, and is also more robust than other existing transferability\nmetrics on average.",
          "link": "http://arxiv.org/abs/2106.10479",
          "publishedOn": "2021-07-01T01:59:34.004Z",
          "wordCount": 660,
          "title": "Practical Transferability Estimation for Image Classification Tasks. (arXiv:2106.10479v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.07743",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kratsios_A/0/1/0/all/0/1\">Anastasis Kratsios</a>",
          "description": "We introduce a general framework for approximating regular conditional\ndistributions (RCDs). Our approximations of these RCDs are implemented by a new\nclass of geometric deep learning models with inputs in $\\mathbb{R}^d$ and\noutputs in the Wasserstein-$1$ space $\\mathcal{P}_1(\\mathbb{R}^D)$. We find\nthat the models built using our framework can approximate any continuous\nfunctions from $\\mathbb{R}^d$ to $\\mathcal{P}_1(\\mathbb{R}^D)$ uniformly on\ncompacts, and quantitative rates are obtained. We identify two methods for\navoiding the \"curse of dimensionality\"; i.e.: the number of parameters\ndetermining the approximating neural network depends only polynomially on the\ninvolved dimension and the approximation error. The first solution describes\nfunctions in $C(\\mathbb{R}^d,\\mathcal{P}_1(\\mathbb{R}^D))$ which can be\nefficiently approximated on any compact subset of $\\mathbb{R}^d$. Conversely,\nthe second approach describes sets in $\\mathbb{R}^d$, on which any function in\n$C(\\mathbb{R}^d,\\mathcal{P}_1(\\mathbb{R}^D))$ can be efficiently approximated.\nOur framework is used to obtain an affirmative answer to the open conjecture of\nBishop (1994); namely: mixture density networks are universal regular\nconditional distributions. The predictive performance of the proposed models is\nevaluated against comparable learning models on various probabilistic\npredictions tasks in the context of ELMs, model uncertainty, and\nheteroscedastic regression. All the results are obtained for more general input\nand output spaces and thus apply to geometric deep learning contexts.",
          "link": "http://arxiv.org/abs/2105.07743",
          "publishedOn": "2021-07-01T01:59:33.997Z",
          "wordCount": 698,
          "title": "Universal Regular Conditional Distributions. (arXiv:2105.07743v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.12199",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Jaiswal_P/0/1/0/all/0/1\">Prateek Jaiswal</a>, <a href=\"http://arxiv.org/find/math/1/au:+Honnappa_H/0/1/0/all/0/1\">Harsha Honnappa</a>, <a href=\"http://arxiv.org/find/math/1/au:+Rao_V/0/1/0/all/0/1\">Vinayak A. Rao</a>",
          "description": "This paper considers data-driven chance-constrained stochastic optimization\nproblems in a Bayesian framework. Bayesian posteriors afford a principled\nmechanism to incorporate data and prior knowledge into stochastic optimization\nproblems. However, the computation of Bayesian posteriors is typically an\nintractable problem, and has spawned a large literature on approximate Bayesian\ncomputation. Here, in the context of chance-constrained optimization, we focus\non the question of statistical consistency (in an appropriate sense) of the\noptimal value, computed using an approximate posterior distribution. To this\nend, we rigorously prove a frequentist consistency result demonstrating the\nconvergence of the optimal value to the optimal value of a fixed, parameterized\nconstrained optimization problem. We augment this by also establishing a\nprobabilistic rate of convergence of the optimal value. We also prove the\nconvex feasibility of the approximate Bayesian stochastic optimization problem.\nFinally, we demonstrate the utility of our approach on an optimal staffing\nproblem for an M/M/c queueing model.",
          "link": "http://arxiv.org/abs/2106.12199",
          "publishedOn": "2021-07-01T01:59:33.992Z",
          "wordCount": 614,
          "title": "Bayesian Joint Chance Constrained Optimization: Approximations and Statistical Consistency. (arXiv:2106.12199v2 [math.ST] CROSS LISTED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.11396",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Huang_F/0/1/0/all/0/1\">Feihu Huang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Huang_H/0/1/0/all/0/1\">Heng Huang</a>",
          "description": "Bilevel optimization recently has attracted increased interest in machine\nlearning due to its many applications such as hyper-parameter optimization and\npolicy optimization. Although some methods recently have been proposed to solve\nthe bilevel problems, these methods do not consider using adaptive learning\nrates. To fill this gap, in the paper, we propose a class of fast and effective\nadaptive methods for solving bilevel optimization problems that the outer\nproblem is possibly nonconvex and the inner problem is strongly-convex.\nSpecifically, we propose a fast single-loop BiAdam algorithm based on the basic\nmomentum technique, which achieves a sample complexity of\n$\\tilde{O}(\\epsilon^{-4})$ for finding an $\\epsilon$-stationary point. At the\nsame time, we propose an accelerated version of BiAdam algorithm (VR-BiAdam) by\nusing variance reduced technique, which reaches the best known sample\ncomplexity of $\\tilde{O}(\\epsilon^{-3})$. To further reduce computation in\nestimating derivatives, we propose a fast single-loop stochastic approximated\nBiAdam algorithm (saBiAdam) by avoiding the Hessian inverse, which still\nachieves a sample complexity of $\\tilde{O}(\\epsilon^{-4})$ without large\nbatches. We further present an accelerated version of saBiAdam algorithm\n(VR-saBiAdam), which also reaches the best known sample complexity of\n$\\tilde{O}(\\epsilon^{-3})$. We apply the unified adaptive matrices to our\nmethods as the SUPER-ADAM \\citep{huang2021super}, which including many types of\nadaptive learning rates. Moreover, our framework can flexibly use the momentum\nand variance reduced techniques. In particular, we provide a useful convergence\nanalysis framework for both the constrained and unconstrained bilevel\noptimization. To the best of our knowledge, we first study the adaptive bilevel\noptimization methods with adaptive learning rates.",
          "link": "http://arxiv.org/abs/2106.11396",
          "publishedOn": "2021-07-01T01:59:33.986Z",
          "wordCount": 714,
          "title": "BiAdam: Fast Adaptive Bilevel Optimization Methods. (arXiv:2106.11396v2 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15755",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alchihabi_A/0/1/0/all/0/1\">Abdullah Alchihabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yuhong Guo</a>",
          "description": "Graph Neural Networks (GNNs) require a relatively large number of labeled\nnodes and a reliable/uncorrupted graph connectivity structure in order to\nobtain good performance on the semi-supervised node classification task. The\nperformance of GNNs can degrade significantly as the number of labeled nodes\ndecreases or the graph connectivity structure is corrupted by adversarial\nattacks or due to noises in data measurement /collection. Therefore, it is\nimportant to develop GNN models that are able to achieve good performance when\nthere is limited supervision knowledge -- a few labeled nodes and noisy graph\nstructures. In this paper, we propose a novel Dual GNN learning framework to\naddress this challenge task. The proposed framework has two GNN based node\nprediction modules. The primary module uses the input graph structure to induce\nregular node embeddings and predictions with a regular GNN baseline, while the\nauxiliary module constructs a new graph structure through fine-grained spectral\nclusterings and learns new node embeddings and predictions. By integrating the\ntwo modules in a dual GNN learning framework, we perform joint learning in an\nend-to-end fashion. This general framework can be applied on many GNN baseline\nmodels. The experimental results validate that the proposed dual GNN framework\ncan greatly outperform the GNN baseline methods when the labeled nodes are\nscarce and the graph connectivity structure is noisy.",
          "link": "http://arxiv.org/abs/2106.15755",
          "publishedOn": "2021-07-01T01:59:33.968Z",
          "wordCount": 650,
          "title": "Dual GNNs: Graph Neural Network Learning with Limited Supervision. (arXiv:2106.15755v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14503",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chandran_P/0/1/0/all/0/1\">Pravin Chandran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhat_R/0/1/0/all/0/1\">Raghavendra Bhat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakravarthi_A/0/1/0/all/0/1\">Avinash Chakravarthi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandar_S/0/1/0/all/0/1\">Srikanth Chandar</a>",
          "description": "Federated Learning allows training of data stored in distributed devices\nwithout the need for centralizing training data, thereby maintaining data\nprivacy. Addressing the ability to handle data heterogeneity (non-identical and\nindependent distribution or non-IID) is a key enabler for the wider deployment\nof Federated Learning. In this paper, we propose a novel Divide-and-Conquer\ntraining methodology that enables the use of the popular FedAvg aggregation\nalgorithm by overcoming the acknowledged FedAvg limitations in non-IID\nenvironments. We propose a novel use of Cosine-distance based Weight Divergence\nmetric to determine the exact point where a Deep Learning network can be\ndivided into class agnostic initial layers and class-specific deep layers for\nperforming a Divide and Conquer training. We show that the methodology achieves\ntrained model accuracy at par (and in certain cases exceeding) with numbers\nachieved by state-of-the-art Aggregation algorithms like FedProx, FedMA, etc.\nAlso, we show that this methodology leads to compute and bandwidth\noptimizations under certain documented conditions.",
          "link": "http://arxiv.org/abs/2106.14503",
          "publishedOn": "2021-07-01T01:59:33.961Z",
          "wordCount": 618,
          "title": "Weight Divergence Driven Divide-and-Conquer Approach for Optimal Federated Learning from non-IID Data. (arXiv:2106.14503v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.10587",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Conde_M/0/1/0/all/0/1\">Marcos V. Conde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turgutlu_K/0/1/0/all/0/1\">Kerem Turgutlu</a>",
          "description": "Existing computer vision research in categorization struggles with\nfine-grained attributes recognition due to the inherently high intra-class\nvariances and low inter-class variances. SOTA methods tackle this challenge by\nlocating the most informative image regions and rely on them to classify the\ncomplete image. The most recent work, Vision Transformer (ViT), shows its\nstrong performance in both traditional and fine-grained classification tasks.\nIn this work, we propose a multi-stage ViT framework for fine-grained image\nclassification tasks, which localizes the informative image regions without\nrequiring architectural changes using the inherent multi-head self-attention\nmechanism. We also introduce attention-guided augmentations for improving the\nmodel's capabilities. We demonstrate the value of our approach by experimenting\nwith four popular fine-grained benchmarks: CUB-200-2011, Stanford Cars,\nStanford Dogs, and FGVC7 Plant Pathology. We also prove our model's\ninterpretability via qualitative results.",
          "link": "http://arxiv.org/abs/2106.10587",
          "publishedOn": "2021-07-01T01:59:33.955Z",
          "wordCount": 621,
          "title": "Exploring Vision Transformers for Fine-grained Classification. (arXiv:2106.10587v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15835",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fernando_T/0/1/0/all/0/1\">Tharindu Fernando</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sridharan_S/0/1/0/all/0/1\">Sridha Sridharan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Denman_S/0/1/0/all/0/1\">Simon Denman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghaemmaghami_H/0/1/0/all/0/1\">Houman Ghaemmaghami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fookes_C/0/1/0/all/0/1\">Clinton Fookes</a>",
          "description": "This paper proposes a novel framework for lung sound event detection,\nsegmenting continuous lung sound recordings into discrete events and performing\nrecognition on each event. Exploiting the lightweight nature of Temporal\nConvolution Networks (TCNs) and their superior results compared to their\nrecurrent counterparts, we propose a lightweight, yet robust, and completely\ninterpretable framework for lung sound event detection. We propose the use of a\nmulti-branch TCN architecture and exploit a novel fusion strategy to combine\nthe resultant features from these branches. This not only allows the network to\nretain the most salient information across different temporal granularities and\ndisregards irrelevant information, but also allows our network to process\nrecordings of arbitrary length. Results: The proposed method is evaluated on\nmultiple public and in-house benchmarks of irregular and noisy recordings of\nthe respiratory auscultation process for the identification of numerous\nauscultation events including inhalation, exhalation, crackles, wheeze,\nstridor, and rhonchi. We exceed the state-of-the-art results in all\nevaluations. Furthermore, we empirically analyse the effect of the proposed\nmulti-branch TCN architecture and the feature fusion strategy and provide\nquantitative and qualitative evaluations to illustrate their efficiency.\nMoreover, we provide an end-to-end model interpretation pipeline that\ninterprets the operations of all the components of the proposed framework. Our\nanalysis of different feature fusion strategies shows that the proposed feature\nconcatenation method leads to better suppression of non-informative features,\nwhich drastically reduces the classifier overhead resulting in a robust\nlightweight network.The lightweight nature of our model allows it to be\ndeployed in end-user devices such as smartphones, and it has the ability to\ngenerate predictions in real-time.",
          "link": "http://arxiv.org/abs/2106.15835",
          "publishedOn": "2021-07-01T01:59:33.939Z",
          "wordCount": 719,
          "title": "Robust and Interpretable Temporal Convolution Network for Event Detection in Lung Sound Recordings. (arXiv:2106.15835v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2008.08903",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_N/0/1/0/all/0/1\">Nan Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1\">Hao Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_W/0/1/0/all/0/1\">Wei Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Sichen Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_K/0/1/0/all/0/1\">Kyle Kai Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prabowo_A/0/1/0/all/0/1\">Arian Prabowo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahaman_M/0/1/0/all/0/1\">Mohammad Saiedur Rahaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salim_F/0/1/0/all/0/1\">Flora D. Salim</a>",
          "description": "Generative Adversarial Networks (GANs) have shown remarkable success in\nproducing realistic-looking images in the computer vision area. Recently,\nGAN-based techniques are shown to be promising for spatio-temporal-based\napplications such as trajectory prediction, events generation and time-series\ndata imputation. While several reviews for GANs in computer vision have been\npresented, no one has considered addressing the practical applications and\nchallenges relevant to spatio-temporal data. In this paper, we have conducted a\ncomprehensive review of the recent developments of GANs for spatio-temporal\ndata. We summarise the application of popular GAN architectures for\nspatio-temporal data and the common practices for evaluating the performance of\nspatio-temporal applications with GANs. Finally, we point out future research\ndirections to benefit researchers in this area.",
          "link": "http://arxiv.org/abs/2008.08903",
          "publishedOn": "2021-07-01T01:59:33.933Z",
          "wordCount": 625,
          "title": "Generative Adversarial Networks for Spatio-temporal Data: A Survey. (arXiv:2008.08903v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.09370",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dumas_J/0/1/0/all/0/1\">Jonathan Dumas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lanaspeze_A/0/1/0/all/0/1\">Antoine Wehenkel Damien Lanaspeze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cornelusse_B/0/1/0/all/0/1\">Bertrand Corn&#xe9;lusse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sutera_A/0/1/0/all/0/1\">Antonio Sutera</a>",
          "description": "Greater direct electrification of end-use sectors with a higher share of\nrenewables is one of the pillars to power a carbon-neutral society by 2050.\nThis study uses a recent deep learning technique, the normalizing flows, to\nproduce accurate probabilistic forecasts that are crucial for decision-makers\nto face the new challenges in power systems applications. Through comprehensive\nempirical evaluations using the open data of the Global Energy Forecasting\nCompetition 2014, we demonstrate that our methodology is competitive with other\nstate-of-the-art deep learning generative models: generative adversarial\nnetworks and variational autoencoders. The models producing weather-based wind,\nsolar power, and load scenarios are properly compared both in terms of forecast\nvalue, by considering the case study of an energy retailer, and quality using\nseveral complementary metrics.",
          "link": "http://arxiv.org/abs/2106.09370",
          "publishedOn": "2021-07-01T01:59:33.927Z",
          "wordCount": 585,
          "title": "Deep generative modeling for probabilistic forecasting in power systems. (arXiv:2106.09370v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.02848",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gopi_S/0/1/0/all/0/1\">Sivakanth Gopi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yin Tat Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wutschitz_L/0/1/0/all/0/1\">Lukas Wutschitz</a>",
          "description": "We give a fast algorithm to optimally compose privacy guarantees of\ndifferentially private (DP) algorithms to arbitrary accuracy. Our method is\nbased on the notion of privacy loss random variables to quantify the privacy\nloss of DP algorithms. The running time and memory needed for our algorithm to\napproximate the privacy curve of a DP algorithm composed with itself $k$ times\nis $\\tilde{O}(\\sqrt{k})$. This improves over the best prior method by Koskela\net al. (2020) which requires $\\tilde{\\Omega}(k^{1.5})$ running time. We\ndemonstrate the utility of our algorithm by accurately computing the privacy\nloss of DP-SGD algorithm of Abadi et al. (2016) and showing that our algorithm\nspeeds up the privacy computations by a few orders of magnitude compared to\nprior work, while maintaining similar accuracy.",
          "link": "http://arxiv.org/abs/2106.02848",
          "publishedOn": "2021-07-01T01:59:33.903Z",
          "wordCount": 578,
          "title": "Numerical Composition of Differential Privacy. (arXiv:2106.02848v2 [cs.DS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.03242",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bilos_M/0/1/0/all/0/1\">Marin Bilo&#x161;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gunnemann_S/0/1/0/all/0/1\">Stephan G&#xfc;nnemann</a>",
          "description": "Modeling sets is an important problem in machine learning since this type of\ndata can be found in many domains. A promising approach defines a family of\npermutation invariant densities with continuous normalizing flows. This allows\nus to maximize the likelihood directly and sample new realizations with ease.\nIn this work, we demonstrate how calculating the trace, a crucial step in this\nmethod, raises issues that occur both during training and inference, limiting\nits practicality. We propose an alternative way of defining permutation\nequivariant transformations that give closed form trace. This leads not only to\nimprovements while training, but also to better final performance. We\ndemonstrate the benefits of our approach on point processes and general set\nmodeling.",
          "link": "http://arxiv.org/abs/2010.03242",
          "publishedOn": "2021-07-01T01:59:33.897Z",
          "wordCount": 581,
          "title": "Scalable Normalizing Flows for Permutation Invariant Densities. (arXiv:2010.03242v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1809.11165",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gardner_J/0/1/0/all/0/1\">Jacob R. Gardner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pleiss_G/0/1/0/all/0/1\">Geoff Pleiss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bindel_D/0/1/0/all/0/1\">David Bindel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weinberger_K/0/1/0/all/0/1\">Kilian Q. Weinberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilson_A/0/1/0/all/0/1\">Andrew Gordon Wilson</a>",
          "description": "Despite advances in scalable models, the inference tools used for Gaussian\nprocesses (GPs) have yet to fully capitalize on developments in computing\nhardware. We present an efficient and general approach to GP inference based on\nBlackbox Matrix-Matrix multiplication (BBMM). BBMM inference uses a modified\nbatched version of the conjugate gradients algorithm to derive all terms for\ntraining and inference in a single call. BBMM reduces the asymptotic complexity\nof exact GP inference from $O(n^3)$ to $O(n^2)$. Adapting this algorithm to\nscalable approximations and complex GP models simply requires a routine for\nefficient matrix-matrix multiplication with the kernel and its derivative. In\naddition, BBMM uses a specialized preconditioner to substantially speed up\nconvergence. In experiments we show that BBMM effectively uses GPU hardware to\ndramatically accelerate both exact GP inference and scalable approximations.\nAdditionally, we provide GPyTorch, a software platform for scalable GP\ninference via BBMM, built on PyTorch.",
          "link": "http://arxiv.org/abs/1809.11165",
          "publishedOn": "2021-07-01T01:59:33.888Z",
          "wordCount": 671,
          "title": "GPyTorch: Blackbox Matrix-Matrix Gaussian Process Inference with GPU Acceleration. (arXiv:1809.11165v6 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.10159",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Nguyen_D/0/1/0/all/0/1\">Du Nguyen</a>",
          "description": "We provide an explicit formula for the Levi-Civita connection and Riemannian\nHessian for a Riemannian manifold that is a quotient of a manifold embedded in\nan inner product space with a non-constant metric function. Together with a\nclassical formula for projection, this allows us to evaluate Riemannian\ngradient and Hessian for several families of metrics on classical manifolds,\nincluding a family of metrics on Stiefel manifolds connecting both the constant\nand canonical ambient metrics with closed-form geodesics. Using these formulas,\nwe derive Riemannian optimization frameworks on quotients of Stiefel manifolds,\nincluding flag manifolds, and a new family of complete quotient metrics on the\nmanifold of positive-semidefinite matrices of fixed rank, considered as a\nquotient of a product of Stiefel and positive-definite matrix manifold with\naffine-invariant metrics. The method is procedural, and in many instances, the\nRiemannian gradient and Hessian formulas could be derived by symbolic calculus.\nThe method extends the list of potential metrics that could be used in manifold\noptimization and machine learning.",
          "link": "http://arxiv.org/abs/2009.10159",
          "publishedOn": "2021-07-01T01:59:33.882Z",
          "wordCount": 639,
          "title": "Operator-valued formulas for Riemannian Gradient and Hessian and families of tractable metrics. (arXiv:2009.10159v2 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.16074",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Goutay_M/0/1/0/all/0/1\">Mathieu Goutay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aoudia_F/0/1/0/all/0/1\">Fay&#xe7;al Ait Aoudia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoydis_J/0/1/0/all/0/1\">Jakob Hoydis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gorce_J/0/1/0/all/0/1\">Jean-Marie Gorce</a>",
          "description": "Machine learning (ML) can be used in various ways to improve multi-user\nmultiple-input multiple-output (MU-MIMO) receive processing. Typical approaches\neither augment a single processing step, such as symbol detection, or replace\nmultiple steps jointly by a single neural network (NN). These techniques\ndemonstrate promising results but often assume perfect channel state\ninformation (CSI) or fail to satisfy the interpretability and scalability\nconstraints imposed by practical systems. In this paper, we propose a new\nstrategy which preserves the benefits of a conventional receiver, but enhances\nspecific parts with ML components. The key idea is to exploit the orthogonal\nfrequency-division multiplexing (OFDM) signal structure to improve both the\ndemapping and the computation of the channel estimation error statistics.\nEvaluation results show that the proposed ML-enhanced receiver beats practical\nbaselines on all considered scenarios, with significant gains at high speeds.",
          "link": "http://arxiv.org/abs/2106.16074",
          "publishedOn": "2021-07-01T01:59:33.856Z",
          "wordCount": 579,
          "title": "Machine Learning-enhanced Receive Processing for MU-MIMO OFDM Systems. (arXiv:2106.16074v1 [cs.IT])"
        },
        {
          "id": "http://arxiv.org/abs/2104.09866",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bhat_P/0/1/0/all/0/1\">Prashant Bhat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arani_E/0/1/0/all/0/1\">Elahe Arani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zonooz_B/0/1/0/all/0/1\">Bahram Zonooz</a>",
          "description": "Self-supervised learning solves pretext prediction tasks that do not require\nannotations to learn feature representations. For vision tasks, pretext tasks\nsuch as predicting rotation, solving jigsaw are solely created from the input\ndata. Yet, predicting this known information helps in learning representations\nuseful for downstream tasks. However, recent works have shown that wider and\ndeeper models benefit more from self-supervised learning than smaller models.\nTo address the issue of self-supervised pre-training of smaller models, we\npropose Distill-on-the-Go (DoGo), a self-supervised learning paradigm using\nsingle-stage online knowledge distillation to improve the representation\nquality of the smaller models. We employ deep mutual learning strategy in which\ntwo models collaboratively learn from each other to improve one another.\nSpecifically, each model is trained using self-supervised learning along with\ndistillation that aligns each model's softmax probabilities of similarity\nscores with that of the peer model. We conduct extensive experiments on\nmultiple benchmark datasets, learning objectives, and architectures to\ndemonstrate the potential of our proposed method. Our results show significant\nperformance gain in the presence of noisy and limited labels and generalization\nto out-of-distribution data.",
          "link": "http://arxiv.org/abs/2104.09866",
          "publishedOn": "2021-07-01T01:59:33.843Z",
          "wordCount": 663,
          "title": "Distill on the Go: Online knowledge distillation in self-supervised learning. (arXiv:2104.09866v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1905.12278",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Castera_C/0/1/0/all/0/1\">Camille Castera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bolte_J/0/1/0/all/0/1\">J&#xe9;r&#xf4;me Bolte</a> (UT1), <a href=\"http://arxiv.org/find/cs/1/au:+Fevotte_C/0/1/0/all/0/1\">C&#xe9;dric F&#xe9;votte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pauwels_E/0/1/0/all/0/1\">Edouard Pauwels</a> (UT3)",
          "description": "We introduce a new second-order inertial optimization method for machine\nlearning called INNA. It exploits the geometry of the loss function while only\nrequiring stochastic approximations of the function values and the generalized\ngradients. This makes INNA fully implementable and adapted to large-scale\noptimization problems such as the training of deep neural networks. The\nalgorithm combines both gradient-descent and Newton-like behaviors as well as\ninertia. We prove the convergence of INNA for most deep learning problems. To\ndo so, we provide a well-suited framework to analyze deep learning loss\nfunctions involving tame optimization in which we study a continuous dynamical\nsystem together with its discrete stochastic approximations. We prove sublinear\nconvergence for the continuous-time differential inclusion which underlies our\nalgorithm. Additionally, we also show how standard optimization mini-batch\nmethods applied to non-smooth non-convex problems can yield a certain type of\nspurious stationary points never discussed before. We address this issue by\nproviding a theoretical framework around the new idea of $D$-criticality; we\nthen give a simple asymptotic analysis of INNA. Our algorithm allows for using\nan aggressive learning rate of $o(1/\\log k)$. From an empirical viewpoint, we\nshow that INNA returns competitive results with respect to state of the art\n(stochastic gradient descent, ADAGRAD, ADAM) on popular deep learning benchmark\nproblems.",
          "link": "http://arxiv.org/abs/1905.12278",
          "publishedOn": "2021-07-01T01:59:33.817Z",
          "wordCount": 726,
          "title": "An Inertial Newton Algorithm for Deep Learning. (arXiv:1905.12278v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.12962",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1\">Rui Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_J/0/1/0/all/0/1\">Jiafei Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ya_J/0/1/0/all/0/1\">Jiangpeng Ya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_F/0/1/0/all/0/1\">Feng Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_D/0/1/0/all/0/1\">Dijun Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lanqing Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiu Li</a>",
          "description": "Multi-goal reinforcement learning is widely applied in planning and robot\nmanipulation. Two main challenges in multi-goal reinforcement learning are\nsparse rewards and sample inefficiency. Hindsight Experience Replay (HER) aims\nto tackle the two challenges via goal relabeling. However, HER-related works\nstill need millions of samples and a huge computation. In this paper, we\npropose Multi-step Hindsight Experience Replay (MHER), incorporating multi-step\nrelabeled returns based on $n$-step relabeling to improve sample efficiency.\nDespite the advantages of $n$-step relabeling, we theoretically and\nexperimentally prove the off-policy $n$-step bias introduced by $n$-step\nrelabeling may lead to poor performance in many environments. To address the\nabove issue, two bias-reduced MHER algorithms, MHER($\\lambda$) and Model-based\nMHER (MMHER) are presented. MHER($\\lambda$) exploits the $\\lambda$ return while\nMMHER benefits from model-based value expansions. Experimental results on\nnumerous multi-goal robotic tasks show that our solutions can successfully\nalleviate off-policy $n$-step bias and achieve significantly higher sample\nefficiency than HER and Curriculum-guided HER with little additional\ncomputation beyond HER.",
          "link": "http://arxiv.org/abs/2102.12962",
          "publishedOn": "2021-07-01T01:59:33.811Z",
          "wordCount": 643,
          "title": "Bias-reduced Multi-step Hindsight Experience Replay for Efficient Multi-goal Reinforcement Learning. (arXiv:2102.12962v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08208",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Huang_F/0/1/0/all/0/1\">Feihu Huang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Li_J/0/1/0/all/0/1\">Junyi Li</a>, <a href=\"http://arxiv.org/find/math/1/au:+Huang_H/0/1/0/all/0/1\">Heng Huang</a>",
          "description": "Adaptive gradient methods have shown excellent performance for solving many\nmachine learning problems. Although multiple adaptive methods were recently\nstudied, they mainly focus on either empirical or theoretical aspects and also\nonly work for specific problems by using specific adaptive learning rates. It\nis desired to design a universal framework for practical algorithms of adaptive\ngradients with theoretical guarantee to solve general problems. To fill this\ngap, we propose a faster and universal framework of adaptive gradients (i.e.,\nSUPER-ADAM) by introducing a universal adaptive matrix that includes most\nexisting adaptive gradient forms. Moreover, our framework can flexibly\nintegrates the momentum and variance reduced techniques. In particular, our\nnovel framework provides the convergence analysis support for adaptive gradient\nmethods under the nonconvex setting. In theoretical analysis, we prove that our\nnew algorithm can achieve the best known complexity of\n$\\tilde{O}(\\epsilon^{-3})$ for finding an $\\epsilon$-stationary point of\nnonconvex optimization, which matches the lower bound for stochastic smooth\nnonconvex optimization. In numerical experiments, we employ various deep\nlearning tasks to validate that our algorithm consistently outperforms the\nexisting adaptive algorithms.",
          "link": "http://arxiv.org/abs/2106.08208",
          "publishedOn": "2021-07-01T01:59:33.793Z",
          "wordCount": 648,
          "title": "SUPER-ADAM: Faster and Universal Framework of Adaptive Gradients. (arXiv:2106.08208v2 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.13503",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zihan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_X/0/1/0/all/0/1\">Xiangyang Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_S/0/1/0/all/0/1\">Simon S. Du</a>",
          "description": "Episodic reinforcement learning and contextual bandits are two widely studied\nsequential decision-making problems. Episodic reinforcement learning\ngeneralizes contextual bandits and is often perceived to be more difficult due\nto long planning horizon and unknown state-dependent transitions. The current\npaper shows that the long planning horizon and the unknown state-dependent\ntransitions (at most) pose little additional difficulty on sample complexity.\n\nWe consider the episodic reinforcement learning with $S$ states, $A$ actions,\nplanning horizon $H$, total reward bounded by $1$, and the agent plays for $K$\nepisodes. We propose a new algorithm, \\textbf{M}onotonic \\textbf{V}alue\n\\textbf{P}ropagation (MVP), which relies on a new Bernstein-type bonus.\nCompared to existing bonus constructions, the new bonus is tighter since it is\nbased on a well-designed monotonic value function. In particular, the\n\\emph{constants} in the bonus should be subtly setting to ensure optimism and\nmonotonicity.\n\nWe show MVP enjoys an $O\\left(\\left(\\sqrt{SAK} + S^2A\\right) \\poly\\log\n\\left(SAHK\\right)\\right)$ regret, approaching the\n$\\Omega\\left(\\sqrt{SAK}\\right)$ lower bound of \\emph{contextual bandits} up to\nlogarithmic terms. Notably, this result 1) \\emph{exponentially} improves the\nstate-of-the-art polynomial-time algorithms by Dann et al. [2019] and Zanette\net al. [2019] in terms of the dependency on $H$, and 2) \\emph{exponentially}\nimproves the running time in [Wang et al. 2020] and significantly improves the\ndependency on $S$, $A$ and $K$ in sample complexity.",
          "link": "http://arxiv.org/abs/2009.13503",
          "publishedOn": "2021-07-01T01:59:33.785Z",
          "wordCount": 690,
          "title": "Is Reinforcement Learning More Difficult Than Bandits? A Near-optimal Algorithm Escaping the Curse of Horizon. (arXiv:2009.13503v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07954",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haoxue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1\">Liexin Cheng</a>",
          "description": "Loan risk for small businesses has long been a complex problem worthy of\nexploring. Predicting the loan risk can benefit entrepreneurship by developing\nmore jobs for the society. CatBoost (Categorical Boosting) is a powerful\nmachine learning algorithm suitable for dataset with many categorical variables\nlike the dataset for forecasting loan risk. In this paper, we identify the\nimportant risk factors that contribute to loan status classification problem.\nThen we compare the performance between boosting-type algorithms(especially\nCatBoost) with other traditional yet popular ones. The dataset we adopt in the\nresearch comes from the U.S. Small Business Administration (SBA) and holds a\nvery large sample size (899,164 observations and 27 features). In order to make\nthe best use of the important features in the dataset, we propose a technique\nnamed \"synthetic generation\" to develop more combined features based on\narithmetic operation, which ends up improving the accuracy and AUC of the\noriginal CatBoost model. We obtain a high accuracy of 95.84% and well-performed\nAUC of 98.80% compared with the existent literature of related research.",
          "link": "http://arxiv.org/abs/2106.07954",
          "publishedOn": "2021-07-01T01:59:33.778Z",
          "wordCount": 646,
          "title": "CatBoost model with synthetic features in application to loan risk assessment of small businesses. (arXiv:2106.07954v3 [cs.CE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2010.04012",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Siyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Haitao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zang_Z/0/1/0/all/0/1\">Zelin Zang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Lirong Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_J/0/1/0/all/0/1\">Jun Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Stan Z. Li</a>",
          "description": "Dimension reduction (DR) aims to learn low-dimensional representations of\nhigh-dimensional data with the preservation of essential information. In the\ncontext of manifold learning, we define that the representation after\ninformation-lossless DR preserves the topological and geometric properties of\ndata manifolds formally, and propose a novel two-stage DR method, called\ninvertible manifold learning (inv-ML) to bridge the gap between theoretical\ninformation-lossless and practical DR. The first stage includes a homeomorphic\nsparse coordinate transformation to learn low-dimensional representations\nwithout destroying topology and a local isometry constraint to preserve local\ngeometry. In the second stage, a linear compression is implemented for the\ntrade-off between the target dimension and the incurred information loss in\nexcessive DR scenarios. Experiments are conducted on seven datasets with a\nneural network implementation of inv-ML, called i-ML-Enc. Empirically, i-ML-Enc\nachieves invertible DR in comparison with typical existing methods as well as\nreveals the characteristics of the learned manifolds. Through latent space\ninterpolation on real-world datasets, we find that the reliability of tangent\nspace approximated by the local neighborhood is the key to the success of\nmanifold-based DR algorithms.",
          "link": "http://arxiv.org/abs/2010.04012",
          "publishedOn": "2021-07-01T01:59:33.772Z",
          "wordCount": 652,
          "title": "Invertible Manifold Learning for Dimension Reduction. (arXiv:2010.04012v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15791",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiashuo Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1\">Zheyan Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_P/0/1/0/all/0/1\">Peng Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Linjun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuang_K/0/1/0/all/0/1\">Kun Kuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>",
          "description": "Machine learning algorithms with empirical risk minimization are vulnerable\nunder distributional shifts due to the greedy adoption of all the correlations\nfound in training data. There is an emerging literature on tackling this\nproblem by minimizing the worst-case risk over an uncertainty set. However,\nexisting methods mostly construct ambiguity sets by treating all variables\nequally regardless of the stability of their correlations with the target,\nresulting in the overwhelmingly-large uncertainty set and low confidence of the\nlearner. In this paper, we propose a novel Stable Adversarial Learning (SAL)\nalgorithm that leverages heterogeneous data sources to construct a more\npractical uncertainty set and conduct differentiated robustness optimization,\nwhere covariates are differentiated according to the stability of their\ncorrelations with the target. We theoretically show that our method is\ntractable for stochastic gradient-based optimization and provide the\nperformance guarantees for our method. Empirical studies on both simulation and\nreal datasets validate the effectiveness of our method in terms of uniformly\ngood performance across unknown distributional shifts.",
          "link": "http://arxiv.org/abs/2106.15791",
          "publishedOn": "2021-07-01T01:59:33.756Z",
          "wordCount": 604,
          "title": "Distributionally Robust Learning with Stable Adversarial Training. (arXiv:2106.15791v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2104.05025",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Caccia_L/0/1/0/all/0/1\">Lucas Caccia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aljundi_R/0/1/0/all/0/1\">Rahaf Aljundi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Asadi_N/0/1/0/all/0/1\">Nader Asadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tuytelaars_T/0/1/0/all/0/1\">Tinne Tuytelaars</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pineau_J/0/1/0/all/0/1\">Joelle Pineau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belilovsky_E/0/1/0/all/0/1\">Eugene Belilovsky</a>",
          "description": "In the online continual learning paradigm, agents must learn from a changing\ndistribution while respecting memory and compute constraints. Previous work in\nthis setting often tries to reduce catastrophic forgetting by limiting changes\nin the space of model parameters. In this work we instead focus on the change\nin representations of observed data that arises when previously unobserved\nclasses appear in the incoming data stream, and new classes must be\ndistinguished from previous ones. Starting from a popular approach, experience\nreplay, we consider metric learning based loss functions which, when adjusted\nto appropriately select negative samples, can effectively nudge the learned\nrepresentations to be more robust to new future classes. We show that this\nselection of negatives is in fact critical for reducing representation drift of\npreviously observed data. Motivated by this we further introduce a simple\nadjustment to the standard cross entropy loss used in prior experience replay\nthat achieves similar effect. Our approach directly improves the performance of\nexperience replay for this setting, obtaining state-of-the-art results on\nseveral existing benchmarks in online continual learning, while remaining\nefficient in both memory and compute. We release an implementation of our\nexperiments at https://github.com/naderAsadi/AML",
          "link": "http://arxiv.org/abs/2104.05025",
          "publishedOn": "2021-07-01T01:59:33.750Z",
          "wordCount": 657,
          "title": "Reducing Representation Drift in Online Continual Learning. (arXiv:2104.05025v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15933",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Jacot_A/0/1/0/all/0/1\">Arthur Jacot</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ged_F/0/1/0/all/0/1\">Fran&#xe7;ois Ged</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Gabriel_F/0/1/0/all/0/1\">Franck Gabriel</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Simsek_B/0/1/0/all/0/1\">Berfin &#x15e;im&#x15f;ek</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Hongler_C/0/1/0/all/0/1\">Cl&#xe9;ment Hongler</a>",
          "description": "For deep linear networks (DLN), various hyperparameters alter the dynamics of\ntraining dramatically. We investigate how the rank of the linear map found by\ngradient descent is affected by (1) the initialization norm and (2) the\naddition of $L_{2}$ regularization on the parameters. For (1), we study two\nregimes: (1a) the linear/lazy regime, for large norm initialization; (1b) a\n\\textquotedbl saddle-to-saddle\\textquotedbl{} regime for small initialization\nnorm. In the (1a) setting, the dynamics of a DLN of any depth is similar to\nthat of a standard linear model, without any low-rank bias. In the (1b)\nsetting, we conjecture that throughout training, gradient descent approaches a\nsequence of saddles, each corresponding to linear maps of increasing rank,\nuntil reaching a minimal rank global minimum. We support this conjecture with a\npartial proof and some numerical experiments. For (2), we show that adding a\n$L_{2}$ regularization on the parameters corresponds to the addition to the\ncost of a $L_{p}$-Schatten (quasi)norm on the linear map with $p=\\frac{2}{L}$\n(for a depth-$L$ network), leading to a stronger low-rank bias as $L$ grows.\nThe effect of $L_{2}$ regularization on the loss surface depends on the depth:\nfor shallow networks, all critical points are either strict saddles or global\nminima, whereas for deep networks, some local minima appear. We numerically\nobserve that these local minima can generalize better than global ones in some\nsettings.",
          "link": "http://arxiv.org/abs/2106.15933",
          "publishedOn": "2021-07-01T01:59:33.744Z",
          "wordCount": 672,
          "title": "Deep Linear Networks Dynamics: Low-Rank Biases Induced by Initialization Scale and L2 Regularization. (arXiv:2106.15933v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/1301.6697",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Geiger_D/0/1/0/all/0/1\">Dan Geiger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heckerman_D/0/1/0/all/0/1\">David Heckerman</a>",
          "description": "We show that the only parameter prior for complete Gaussian DAG models that\nsatisfies global parameter independence, complete model equivalence, and some\nweak regularity assumptions, is the normal-Wishart distribution. Our analysis\nis based on the following new characterization of the Wishart distribution: let\nW be an n x n, n >= 3, positive-definite symmetric matrix of random variables\nand f(W) be a pdf of W. Then, f(W) is a Wishart distribution if and only if\nW_{11}-W_{12}W_{22}^{-1}W_{12}' is independent of {W_{12}, W_{22}} for every\nblock partitioning W_{11}, W_{12}, W_{12}', W_{22} of W. Similar\ncharacterizations of the normal and normal-Wishart distributions are provided\nas well. We also show how to construct a prior for every DAG model over X from\nthe prior of a single regression model.",
          "link": "http://arxiv.org/abs/1301.6697",
          "publishedOn": "2021-07-01T01:59:33.737Z",
          "wordCount": 643,
          "title": "Parameter Priors for Directed Acyclic Graphical Models and the Characterization of Several Probability Distributions. (arXiv:1301.6697v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.11866",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zekun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_Z/0/1/0/all/0/1\">Zeyu Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaoyu Zhang</a>",
          "description": "Factorization machine (FM) is a prevalent approach to modeling pairwise\n(second-order) feature interactions when dealing with high-dimensional sparse\ndata. However, on the one hand, FM fails to capture higher-order feature\ninteractions suffering from combinatorial expansion, on the other hand, taking\ninto account interaction between every pair of features may introduce noise and\ndegrade prediction accuracy. To solve the problems, we propose a novel approach\nGraph Factorization Machine (GraphFM) by naturally representing features in the\ngraph structure. In particular, a novel mechanism is designed to select the\nbeneficial feature interactions and formulate them as edges between features.\nThen our proposed model which integrates the interaction function of FM into\nthe feature aggregation strategy of Graph Neural Network (GNN), can model\narbitrary-order feature interactions on the graph-structured features by\nstacking layers. Experimental results on several real-world datasets has\ndemonstrated the rationality and effectiveness of our proposed approach.",
          "link": "http://arxiv.org/abs/2105.11866",
          "publishedOn": "2021-07-01T01:59:33.731Z",
          "wordCount": 615,
          "title": "GraphFM: Graph Factorization Machines for Feature Interaction Modeling. (arXiv:2105.11866v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15991",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zernetsch_S/0/1/0/all/0/1\">Stefan Zernetsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trupp_O/0/1/0/all/0/1\">Oliver Trupp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kress_V/0/1/0/all/0/1\">Viktor Kress</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doll_K/0/1/0/all/0/1\">Konrad Doll</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sick_B/0/1/0/all/0/1\">Bernhard Sick</a>",
          "description": "This article presents a novel approach to incorporate visual cues from\nvideo-data from a wide-angle stereo camera system mounted at an urban\nintersection into the forecast of cyclist trajectories. We extract features\nfrom image and optical flow (OF) sequences using 3D convolutional neural\nnetworks (3D-ConvNet) and combine them with features extracted from the\ncyclist's past trajectory to forecast future cyclist positions. By the use of\nadditional information, we are able to improve positional accuracy by about 7.5\n% for our test dataset and by up to 22 % for specific motion types compared to\na method solely based on past trajectories. Furthermore, we compare the use of\nimage sequences to the use of OF sequences as additional information, showing\nthat OF alone leads to significant improvements in positional accuracy. By\ntraining and testing our methods using a real-world dataset recorded at a\nheavily frequented public intersection and evaluating the methods' runtimes, we\ndemonstrate the applicability in real traffic scenarios. Our code and parts of\nour dataset are made publicly available.",
          "link": "http://arxiv.org/abs/2106.15991",
          "publishedOn": "2021-07-01T01:59:33.725Z",
          "wordCount": 613,
          "title": "Cyclist Trajectory Forecasts by Incorporation of Multi-View Video Information. (arXiv:2106.15991v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16174",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Chen_P/0/1/0/all/0/1\">Pingjun Chen</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Aminu_M/0/1/0/all/0/1\">Muhammad Aminu</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Hussein_S/0/1/0/all/0/1\">Siba El Hussein</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Khoury_J/0/1/0/all/0/1\">Joseph Khoury</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Wu_J/0/1/0/all/0/1\">Jia Wu</a>",
          "description": "The cells and their spatial patterns in the tumor microenvironment (TME) play\na key role in tumor evolution, and yet remains an understudied topic in\ncomputational pathology. This study, to the best of our knowledge, is among the\nfirst to hybrid local and global graph methods to profile orchestration and\ninteraction of cellular components. To address the challenge in hematolymphoid\ncancers where the cell classes in TME are unclear, we first implemented cell\nlevel unsupervised learning and identified two new cell subtypes. Local cell\ngraphs or supercells were built for each image by considering the individual\ncell's geospatial location and classes. Then, we applied supercell level\nclustering and identified two new cell communities. In the end, we built global\ngraphs to abstract spatial interaction patterns and extract features for\ndisease diagnosis. We evaluate the proposed algorithm on H\\&E slides of 60\nhematolymphoid neoplasm patients and further compared it with three cell level\ngraph-based algorithms, including the global cell graph, cluster cell graph,\nand FLocK. The proposed algorithm achieves a mean diagnosis accuracy of 0.703\nwith the repeated 5-fold cross-validation scheme. In conclusion, our algorithm\nshows superior performance over the existing methods and can be potentially\napplied to other cancer types.",
          "link": "http://arxiv.org/abs/2106.16174",
          "publishedOn": "2021-07-01T01:59:33.700Z",
          "wordCount": 661,
          "title": "Hierarchical Phenotyping and Graph Modeling of Spatial Architecture in Lymphoid Neoplasms. (arXiv:2106.16174v1 [q-bio.QM])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15941",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1\">Yehui Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1\">Kai Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_A/0/1/0/all/0/1\">An Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1\">Yiping Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1\">Chao Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yunhe Wang</a>",
          "description": "Transformer models have achieved great progress on computer vision tasks\nrecently. The rapid development of vision transformers is mainly contributed by\ntheir high representation ability for extracting informative features from\ninput images. However, the mainstream transformer models are designed with deep\narchitectures, and the feature diversity will be continuously reduced as the\ndepth increases, i.e., feature collapse. In this paper, we theoretically\nanalyze the feature collapse phenomenon and study the relationship between\nshortcuts and feature diversity in these transformer models. Then, we present\nan augmented shortcut scheme, which inserts additional paths with learnable\nparameters in parallel on the original shortcuts. To save the computational\ncosts, we further explore an efficient approach that uses the block-circulant\nprojection to implement augmented shortcuts. Extensive experiments conducted on\nbenchmark datasets demonstrate the effectiveness of the proposed method, which\nbrings about 1% accuracy increase of the state-of-the-art visual transformers\nwithout obviously increasing their parameters and FLOPs.",
          "link": "http://arxiv.org/abs/2106.15941",
          "publishedOn": "2021-07-01T01:59:33.694Z",
          "wordCount": 591,
          "title": "Augmented Shortcuts for Vision Transformers. (arXiv:2106.15941v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15649",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Abbas_A/0/1/0/all/0/1\">Ammar Abbas</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bollepalli_B/0/1/0/all/0/1\">Bajibabu Bollepalli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Moinet_A/0/1/0/all/0/1\">Alexis Moinet</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Joly_A/0/1/0/all/0/1\">Arnaud Joly</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Karanasou_P/0/1/0/all/0/1\">Penny Karanasou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Makarov_P/0/1/0/all/0/1\">Peter Makarov</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Slangens_S/0/1/0/all/0/1\">Simon Slangens</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Karlapati_S/0/1/0/all/0/1\">Sri Karlapati</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Drugman_T/0/1/0/all/0/1\">Thomas Drugman</a>",
          "description": "We propose a novel Multi-Scale Spectrogram (MSS) modelling approach to\nsynthesise speech with an improved coarse and fine-grained prosody. We present\na generic multi-scale spectrogram prediction mechanism where the system first\npredicts coarser scale mel-spectrograms that capture the suprasegmental\ninformation in speech, and later uses these coarser scale mel-spectrograms to\npredict finer scale mel-spectrograms capturing fine-grained prosody.\n\nWe present details for two specific versions of MSS called Word-level MSS and\nSentence-level MSS where the scales in our system are motivated by the\nlinguistic units. The Word-level MSS models word, phoneme, and frame-level\nspectrograms while Sentence-level MSS models sentence-level spectrogram in\naddition.\n\nSubjective evaluations show that Word-level MSS performs statistically\nsignificantly better compared to the baseline on two voices.",
          "link": "http://arxiv.org/abs/2106.15649",
          "publishedOn": "2021-07-01T01:59:33.688Z",
          "wordCount": 580,
          "title": "Multi-Scale Spectrogram Modelling for Neural Text-to-Speech. (arXiv:2106.15649v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15905",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Meng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_E/0/1/0/all/0/1\">Ermin Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berry_R/0/1/0/all/0/1\">Randall Berry</a>",
          "description": "Federated learning enables machine learning algorithms to be trained over a\nnetwork of multiple decentralized edge devices without requiring the exchange\nof local datasets. Successfully deploying federated learning requires ensuring\nthat agents (e.g., mobile devices) faithfully execute the intended algorithm,\nwhich has been largely overlooked in the literature. In this study, we first\nuse risk bounds to analyze how the key feature of federated learning,\nunbalanced and non-i.i.d. data, affects agents' incentives to voluntarily\nparticipate and obediently follow traditional federated learning algorithms.\n\nTo be more specific, our analysis reveals that agents with less typical data\ndistributions and relatively more samples are more likely to opt out of or\ntamper with federated learning algorithms. To this end, we formulate the first\nfaithful implementation problem of federated learning and design two faithful\nfederated learning mechanisms which satisfy economic properties, scalability,\nand privacy. Further, the time complexity of computing all agents' payments in\nthe number of agents is $\\mathcal{O}(1)$. First, we design a Faithful Federated\nLearning (FFL) mechanism which approximates the Vickrey-Clarke-Groves (VCG)\npayments via an incremental computation. We show that it achieves (probably\napproximate) optimality, faithful implementation, voluntary participation, and\nsome other economic properties (such as budget balance). Second, by\npartitioning agents into several subsets, we present a scalable VCG mechanism\napproximation. We further design a scalable and Differentially Private FFL\n(DP-FFL) mechanism, the first differentially private faithful mechanism, that\nmaintains the economic properties. Our mechanism enables one to make three-way\nperformance tradeoffs among privacy, the iterations needed, and payment\naccuracy loss.",
          "link": "http://arxiv.org/abs/2106.15905",
          "publishedOn": "2021-07-01T01:59:33.670Z",
          "wordCount": 701,
          "title": "Faithful Edge Federated Learning: Scalability and Privacy. (arXiv:2106.15905v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15962",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Haoyue Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1\">Tao Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jintao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tie-Yan Liu</a>",
          "description": "We study whether and how can we model a joint distribution $p(x,z)$ using two\nconditional models $p(x|z)$ and $q(z|x)$ that form a cycle. This is motivated\nby the observation that deep generative models, in addition to a likelihood\nmodel $p(x|z)$, often also use an inference model $q(z|x)$ for data\nrepresentation, but they rely on a usually uninformative prior distribution\n$p(z)$ to define a joint distribution, which may render problems like posterior\ncollapse and manifold mismatch. To explore the possibility to model a joint\ndistribution using only $p(x|z)$ and $q(z|x)$, we study their compatibility and\ndeterminacy, corresponding to the existence and uniqueness of a joint\ndistribution whose conditional distributions coincide with them. We develop a\ngeneral theory for novel and operable equivalence criteria for compatibility,\nand sufficient conditions for determinacy. Based on the theory, we propose the\nCyGen framework for cyclic-conditional generative modeling, including methods\nto enforce compatibility and use the determined distribution to fit and\ngenerate data. With the prior constraint removed, CyGen better fits data and\ncaptures more representative features, supported by experiments showing better\ngeneration and downstream classification performance.",
          "link": "http://arxiv.org/abs/2106.15962",
          "publishedOn": "2021-07-01T01:59:33.664Z",
          "wordCount": 620,
          "title": "On the Generative Utility of Cyclic Conditionals. (arXiv:2106.15962v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10013",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vendramini_M/0/1/0/all/0/1\">Marcos Vendramini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliveira_H/0/1/0/all/0/1\">Hugo Oliveira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Machado_A/0/1/0/all/0/1\">Alexei Machado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Santos_J/0/1/0/all/0/1\">Jefersson A. dos Santos</a>",
          "description": "Image classification methods are usually trained to perform predictions\ntaking into account a predefined group of known classes. Real-world problems,\nhowever, may not allow for a full knowledge of the input and label spaces,\nmaking failures in recognition a hazard to deep visual learning. Open set\nrecognition methods are characterized by the ability to correctly identify\ninputs of known and unknown classes. In this context, we propose GeMOS: simple\nand plug-and-play open set recognition modules that can be attached to\npretrained Deep Neural Networks for visual recognition. The GeMOS framework\npairs pre-trained Convolutional Neural Networks with generative models for open\nset recognition to extract open set scores for each sample, allowing for\nfailure recognition in object recognition tasks. We conduct a thorough\nevaluation of the proposed method in comparison with state-of-the-art open set\nalgorithms, finding that GeMOS either outperforms or is statistically\nindistinguishable from more complex and costly models.",
          "link": "http://arxiv.org/abs/2105.10013",
          "publishedOn": "2021-07-01T01:59:33.658Z",
          "wordCount": 626,
          "title": "Opening Deep Neural Networks with Generative Models. (arXiv:2105.10013v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.07636",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Saharia_C/0/1/0/all/0/1\">Chitwan Saharia</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ho_J/0/1/0/all/0/1\">Jonathan Ho</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chan_W/0/1/0/all/0/1\">William Chan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Salimans_T/0/1/0/all/0/1\">Tim Salimans</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fleet_D/0/1/0/all/0/1\">David J. Fleet</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Norouzi_M/0/1/0/all/0/1\">Mohammad Norouzi</a>",
          "description": "We present SR3, an approach to image Super-Resolution via Repeated\nRefinement. SR3 adapts denoising diffusion probabilistic models to conditional\nimage generation and performs super-resolution through a stochastic denoising\nprocess. Inference starts with pure Gaussian noise and iteratively refines the\nnoisy output using a U-Net model trained on denoising at various noise levels.\nSR3 exhibits strong performance on super-resolution tasks at different\nmagnification factors, on faces and natural images. We conduct human evaluation\non a standard 8X face super-resolution task on CelebA-HQ, comparing with SOTA\nGAN methods. SR3 achieves a fool rate close to 50%, suggesting photo-realistic\noutputs, while GANs do not exceed a fool rate of 34%. We further show the\neffectiveness of SR3 in cascaded image generation, where generative models are\nchained with super-resolution models, yielding a competitive FID score of 11.3\non ImageNet.",
          "link": "http://arxiv.org/abs/2104.07636",
          "publishedOn": "2021-07-01T01:59:33.652Z",
          "wordCount": 600,
          "title": "Image Super-Resolution via Iterative Refinement. (arXiv:2104.07636v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.03248",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Geiger_D/0/1/0/all/0/1\">Dan Geiger</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Heckerman_D/0/1/0/all/0/1\">David Heckerman</a>",
          "description": "We develop simple methods for constructing parameter priors for model choice\namong Directed Acyclic Graphical (DAG) models. In particular, we introduce\nseveral assumptions that permit the construction of parameter priors for a\nlarge number of DAG models from a small set of assessments. We then present a\nmethod for directly computing the marginal likelihood of every DAG model given\na random sample with no missing observations. We apply this methodology to\nGaussian DAG models which consist of a recursive set of linear regression\nmodels. We show that the only parameter prior for complete Gaussian DAG models\nthat satisfies our assumptions is the normal-Wishart distribution. Our analysis\nis based on the following new characterization of the Wishart distribution: let\n$W$ be an $n \\times n$, $n \\ge 3$, positive-definite symmetric matrix of random\nvariables and $f(W)$ be a pdf of $W$. Then, f$(W)$ is a Wishart distribution if\nand only if $W_{11} - W_{12} W_{22}^{-1} W'_{12}$ is independent of\n$\\{W_{12},W_{22}\\}$ for every block partitioning $W_{11},W_{12}, W'_{12},\nW_{22}$ of $W$. Similar characterizations of the normal and normal-Wishart\ndistributions are provided as well.",
          "link": "http://arxiv.org/abs/2105.03248",
          "publishedOn": "2021-07-01T01:59:33.646Z",
          "wordCount": 675,
          "title": "Parameter Priors for Directed Acyclic Graphical Models and the Characterization of Several Probability Distributions. (arXiv:2105.03248v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.16057",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tihon_S/0/1/0/all/0/1\">Simon Tihon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Javaid_M/0/1/0/all/0/1\">Muhammad Usama Javaid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fourure_D/0/1/0/all/0/1\">Damien Fourure</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Posocco_N/0/1/0/all/0/1\">Nicolas Posocco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peel_T/0/1/0/all/0/1\">Thomas Peel</a>",
          "description": "Missing data is a recurrent and challenging problem, especially when using\nmachine learning algorithms for real-world applications. For this reason,\nmissing data imputation has become an active research area, in which recent\ndeep learning approaches have achieved state-of-the-art results. We propose\nDAEMA (Denoising Autoencoder with Mask Attention), an algorithm based on a\ndenoising autoencoder architecture with an attention mechanism. While most\nimputation algorithms use incomplete inputs as they would use complete data -\nup to basic preprocessing (e.g. mean imputation) - DAEMA leverages a mask-based\nattention mechanism to focus on the observed values of its inputs. We evaluate\nDAEMA both in terms of reconstruction capabilities and downstream prediction\nand show that it achieves superior performance to state-of-the-art algorithms\non several publicly available real-world datasets under various missingness\nsettings.",
          "link": "http://arxiv.org/abs/2106.16057",
          "publishedOn": "2021-07-01T01:59:33.629Z",
          "wordCount": 578,
          "title": "DAEMA: Denoising Autoencoder with Mask Attention. (arXiv:2106.16057v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15910",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Nagahama_M/0/1/0/all/0/1\">Masatoshi Nagahama</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yamada_K/0/1/0/all/0/1\">Koki Yamada</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tanaka_Y/0/1/0/all/0/1\">Yuichi Tanaka</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chan_S/0/1/0/all/0/1\">Stanley H. Chan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Eldar_Y/0/1/0/all/0/1\">Yonina C. Eldar</a>",
          "description": "Graph signal processing is a ubiquitous task in many applications such as\nsensor, social, transportation and brain networks, point cloud processing, and\ngraph neural networks. Graph signals are often corrupted through sensing\nprocesses, and need to be restored for the above applications. In this paper,\nwe propose two graph signal restoration methods based on deep algorithm\nunrolling (DAU). First, we present a graph signal denoiser by unrolling\niterations of the alternating direction method of multiplier (ADMM). We then\npropose a general restoration method for linear degradation by unrolling\niterations of Plug-and-Play ADMM (PnP-ADMM). In the second method, the unrolled\nADMM-based denoiser is incorporated as a submodule. Therefore, our restoration\nmethod has a nested DAU structure. Thanks to DAU, parameters in the proposed\ndenoising/restoration methods are trainable in an end-to-end manner. Since the\nproposed restoration methods are based on iterations of a (convex) optimization\nalgorithm, the method is interpretable and keeps the number of parameters small\nbecause we only need to tune graph-independent regularization parameters. We\nsolve two main problems in existing graph signal restoration methods: 1)\nlimited performance of convex optimization algorithms due to fixed parameters\nwhich are often determined manually. 2) large number of parameters of graph\nneural networks that result in difficulty of training. Several experiments for\ngraph signal denoising and interpolation are performed on synthetic and\nreal-world data. The proposed methods show performance improvements to several\nexisting methods in terms of root mean squared error in both tasks.",
          "link": "http://arxiv.org/abs/2106.15910",
          "publishedOn": "2021-07-01T01:59:33.623Z",
          "wordCount": 683,
          "title": "Graph Signal Restoration Using Nested Deep Algorithm Unrolling. (arXiv:2106.15910v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2105.06456",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rogoz_A/0/1/0/all/0/1\">Ana-Cristina Rogoz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaman_M/0/1/0/all/0/1\">Mihaela Gaman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ionescu_R/0/1/0/all/0/1\">Radu Tudor Ionescu</a>",
          "description": "In this work, we introduce a corpus for satire detection in Romanian news. We\ngathered 55,608 public news articles from multiple real and satirical news\nsources, composing one of the largest corpora for satire detection regardless\nof language and the only one for the Romanian language. We provide an official\nsplit of the text samples, such that training news articles belong to different\nsources than test news articles, thus ensuring that models do not achieve high\nperformance simply due to overfitting. We conduct experiments with two\nstate-of-the-art deep neural models, resulting in a set of strong baselines for\nour novel corpus. Our results show that the machine-level accuracy for satire\ndetection in Romanian is quite low (under 73% on the test set) compared to the\nhuman-level accuracy (87%), leaving enough room for improvement in future\nresearch.",
          "link": "http://arxiv.org/abs/2105.06456",
          "publishedOn": "2021-07-01T01:59:33.617Z",
          "wordCount": 621,
          "title": "SaRoCo: Detecting Satire in a Novel Romanian Corpus of News Articles. (arXiv:2105.06456v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1801.10502",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aschenbach_M/0/1/0/all/0/1\">Martin Aschenbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kotzing_T/0/1/0/all/0/1\">Timo K&#xf6;tzing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seidel_K/0/1/0/all/0/1\">Karen Seidel</a>",
          "description": "Learning from positive and negative information, so-called \\emph{informants},\nbeing one of the models for human and machine learning introduced by\nE.~M.~Gold, is investigated. Particularly, naturally arising questions about\nthis learning setting, originating in results on learning from solely positive\ninformation, are answered. By a carefully arranged argument learners can be\nassumed to only change their hypothesis in case it is inconsistent with the\ndata (such a learning behavior is called \\emph{conservative}). The deduced main\ntheorem states the relations between the most important delayable learning\nsuccess criteria, being the ones not ruined by a delayed in time hypothesis\noutput. Additionally, our investigations concerning the non-delayable\nrequirement of consistent learning underpin the claim for \\emph{delayability}\nbeing the right structural property to gain a deeper understanding concerning\nthe nature of learning success criteria. Moreover, we obtain an anomalous\n\\emph{hierarchy} when allowing for an increasing finite number of\n\\emph{anomalies} of the hypothesized language by the learner compared with the\nlanguage to be learned. In contrast to the vacillatory hierarchy for learning\nfrom solely positive information, we observe a \\emph{duality} depending on\nwhether infinitely many \\emph{vacillations} between different (almost) correct\nhypotheses are still considered a successful learning behavior.",
          "link": "http://arxiv.org/abs/1801.10502",
          "publishedOn": "2021-07-01T01:59:33.610Z",
          "wordCount": 686,
          "title": "Learning from Informants: Relations between Learning Success Criteria. (arXiv:1801.10502v5 [cs.FL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.15207",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiefeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yixuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yingyu Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jha_S/0/1/0/all/0/1\">Somesh Jha</a>",
          "description": "Detecting out-of-distribution (OOD) inputs is critical for safely deploying\ndeep learning models in an open-world setting. However, existing OOD detection\nsolutions can be brittle in the open world, facing various types of adversarial\nOOD inputs. While methods leveraging auxiliary OOD data have emerged, our\nanalysis on illuminative examples reveals a key insight that the majority of\nauxiliary OOD examples may not meaningfully improve or even hurt the decision\nboundary of the OOD detector, which is also observed in empirical results on\nreal data. In this paper, we provide a theoretically motivated method,\nAdversarial Training with informative Outlier Mining (ATOM), which improves the\nrobustness of OOD detection. We show that, by mining informative auxiliary OOD\ndata, one can significantly improve OOD detection performance, and somewhat\nsurprisingly, generalize to unseen adversarial attacks. ATOM achieves\nstate-of-the-art performance under a broad family of classic and adversarial\nOOD evaluation tasks. For example, on the CIFAR-10 in-distribution dataset,\nATOM reduces the FPR (at TPR 95%) by up to 57.99% under adversarial OOD inputs,\nsurpassing the previous best baseline by a large margin.",
          "link": "http://arxiv.org/abs/2006.15207",
          "publishedOn": "2021-07-01T01:59:33.587Z",
          "wordCount": 669,
          "title": "ATOM: Robustifying Out-of-distribution Detection Using Outlier Mining. (arXiv:2006.15207v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15860",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qiaoben_Y/0/1/0/all/0/1\">You Qiaoben</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ying_C/0/1/0/all/0/1\">Chengyang Ying</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xinning Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1\">Hang Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Bo Zhang</a>",
          "description": "Recent works demonstrate that deep reinforcement learning (DRL) models are\nvulnerable to adversarial attacks which can decrease the victim's total reward\nby manipulating the observations. Compared with adversarial attacks in\nsupervised learning, it is much more challenging to deceive a DRL model since\nthe adversary has to infer the environmental dynamics. To address this issue,\nwe reformulate the problem of adversarial attacks in function space and\nseparate the previous gradient based attacks into several subspace. Following\nthe analysis of the function space, we design a generic two-stage framework in\nthe subspace where the adversary lures the agent to a target trajectory or a\ndeceptive policy. In the first stage, we train a deceptive policy by hacking\nthe environment, and discover a set of trajectories routing to the lowest\nreward. The adversary then misleads the victim to imitate the deceptive policy\nby perturbing the observations. Our method provides a tighter theoretical upper\nbound for the attacked agent's performance than the existing approaches.\nExtensive experiments demonstrate the superiority of our method and we achieve\nthe state-of-the-art performance on both Atari and MuJoCo environments.",
          "link": "http://arxiv.org/abs/2106.15860",
          "publishedOn": "2021-07-01T01:59:33.581Z",
          "wordCount": 617,
          "title": "Understanding Adversarial Attacks on Observations in Deep Reinforcement Learning. (arXiv:2106.15860v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1911.07192",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zou_Q/0/1/0/all/0/1\">Qin Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_L/0/1/0/all/0/1\">Ling Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Long Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Song Wang</a>",
          "description": "Hash coding has been widely used in approximate nearest neighbor search for\nlarge-scale image retrieval. Given semantic annotations such as class labels\nand pairwise similarities of the training data, hashing methods can learn and\ngenerate effective and compact binary codes. While some newly introduced images\nmay contain undefined semantic labels, which we call unseen images, zeor-shot\nhashing techniques have been studied. However, existing zeor-shot hashing\nmethods focus on the retrieval of single-label images, and cannot handle\nmulti-label images. In this paper, for the first time, a novel transductive\nzero-shot hashing method is proposed for multi-label unseen image retrieval. In\norder to predict the labels of the unseen/target data, a visual-semantic bridge\nis built via instance-concept coherence ranking on the seen/source data. Then,\npairwise similarity loss and focal quantization loss are constructed for\ntraining a hashing model using both the seen/source and unseen/target data.\nExtensive evaluations on three popular multi-label datasets demonstrate that,\nthe proposed hashing method achieves significantly better results than the\ncompeting methods.",
          "link": "http://arxiv.org/abs/1911.07192",
          "publishedOn": "2021-07-01T01:59:33.575Z",
          "wordCount": 648,
          "title": "Transductive Zero-Shot Hashing for Multilabel Image Retrieval. (arXiv:1911.07192v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15988",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Tsirtsis_S/0/1/0/all/0/1\">Stratis Tsirtsis</a>, <a href=\"http://arxiv.org/find/stat/1/au:+De_A/0/1/0/all/0/1\">Abir De</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lorch_L/0/1/0/all/0/1\">Lars Lorch</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Gomez_Rodriguez_M/0/1/0/all/0/1\">Manuel Gomez-Rodriguez</a>",
          "description": "Testing is recommended for all close contacts of confirmed COVID-19 patients.\nHowever, existing group testing methods are oblivious to the circumstances of\ncontagion provided by contact tracing. Here, we build upon a well-known\nsemi-adaptive pool testing method, Dorfman's method with imperfect tests, and\nderive a simple group testing method based on dynamic programming that is\nspecifically designed to use the information provided by contact tracing.\nExperiments using a variety of reproduction numbers and dispersion levels,\nincluding those estimated in the context of the COVID-19 pandemic, show that\nthe pools found using our method result in a significantly lower number of\ntests than those found using standard Dorfman's method, especially when the\nnumber of contacts of an infected individual is small. Moreover, our results\nshow that our method can be more beneficial when the secondary infections are\nhighly overdispersed.",
          "link": "http://arxiv.org/abs/2106.15988",
          "publishedOn": "2021-07-01T01:59:33.568Z",
          "wordCount": 614,
          "title": "Group Testing under Superspreading Dynamics. (arXiv:2106.15988v1 [stat.AP])"
        },
        {
          "id": "http://arxiv.org/abs/2101.10657",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Zaidenberg_D/0/1/0/all/0/1\">Daniela A. Zaidenberg</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Sebastianelli_A/0/1/0/all/0/1\">Alessandro Sebastianelli</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Spiller_D/0/1/0/all/0/1\">Dario Spiller</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Saux_B/0/1/0/all/0/1\">Bertrand Le Saux</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Ullo_S/0/1/0/all/0/1\">Silvia Liberata Ullo</a>",
          "description": "This concept paper aims to provide a brief outline of quantum computers,\nexplore existing methods of quantum image classification techniques, so\nfocusing on remote sensing applications, and discuss the bottlenecks of\nperforming these algorithms on currently available open source platforms.\nInitial results demonstrate feasibility. Next steps include expanding the size\nof the quantum hidden layer and increasing the variety of output image options.",
          "link": "http://arxiv.org/abs/2101.10657",
          "publishedOn": "2021-07-01T01:59:33.543Z",
          "wordCount": 542,
          "title": "Advantages and Bottlenecks of Quantum Machine Learning for Remote Sensing. (arXiv:2101.10657v3 [quant-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.04000",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Hengyuan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lerer_A/0/1/0/all/0/1\">Adam Lerer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_B/0/1/0/all/0/1\">Brandon Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1\">David Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pineda_L/0/1/0/all/0/1\">Luis Pineda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brown_N/0/1/0/all/0/1\">Noam Brown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foerster_J/0/1/0/all/0/1\">Jakob Foerster</a>",
          "description": "The standard problem setting in Dec-POMDPs is self-play, where the goal is to\nfind a set of policies that play optimally together. Policies learned through\nself-play may adopt arbitrary conventions and implicitly rely on multi-step\nreasoning based on fragile assumptions about other agents' actions and thus\nfail when paired with humans or independently trained agents at test time. To\naddress this, we present off-belief learning (OBL). At each timestep OBL agents\nfollow a policy $\\pi_1$ that is optimized assuming past actions were taken by a\ngiven, fixed policy ($\\pi_0$), but assuming that future actions will be taken\nby $\\pi_1$. When $\\pi_0$ is uniform random, OBL converges to an optimal policy\nthat does not rely on inferences based on other agents' behavior (an optimal\ngrounded policy). OBL can be iterated in a hierarchy, where the optimal policy\nfrom one level becomes the input to the next, thereby introducing multi-level\ncognitive reasoning in a controlled manner. Unlike existing approaches, which\nmay converge to any equilibrium policy, OBL converges to a unique policy,\nmaking it suitable for zero-shot coordination (ZSC). OBL can be scaled to\nhigh-dimensional settings with a fictitious transition mechanism and shows\nstrong performance in both a toy-setting and the benchmark human-AI & ZSC\nproblem Hanabi.",
          "link": "http://arxiv.org/abs/2103.04000",
          "publishedOn": "2021-07-01T01:59:33.530Z",
          "wordCount": 672,
          "title": "Off-Belief Learning. (arXiv:2103.04000v3 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.03834",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bradley_T/0/1/0/all/0/1\">Tai-Danae Bradley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vlassopoulos_Y/0/1/0/all/0/1\">Yiannis Vlassopoulos</a>",
          "description": "This work originates from the observation that today's state of the art\nstatistical language models are impressive not only for their performance, but\nalso - and quite crucially - because they are built entirely from correlations\nin unstructured text data. The latter observation prompts a fundamental\nquestion that lies at the heart of this paper: What mathematical structure\nexists in unstructured text data? We put forth enriched category theory as a\nnatural answer. We show that sequences of symbols from a finite alphabet, such\nas those found in a corpus of text, form a category enriched over\nprobabilities. We then address a second fundamental question: How can this\ninformation be stored and modeled in a way that preserves the categorical\nstructure? We answer this by constructing a functor from our enriched category\nof text to a particular enriched category of reduced density operators. The\nlatter leverages the Loewner order on positive semidefinite operators, which\ncan further be interpreted as a toy example of entailment.",
          "link": "http://arxiv.org/abs/2007.03834",
          "publishedOn": "2021-07-01T01:59:33.524Z",
          "wordCount": 650,
          "title": "Language Modeling with Reduced Densities. (arXiv:2007.03834v3 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.16245",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Han-Jia Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_W/0/1/0/all/0/1\">Wei-Lun Chao</a>",
          "description": "Model-agnostic meta-learning (MAML) is arguably the most popular\nmeta-learning algorithm nowadays, given its flexibility to incorporate various\nmodel architectures and to be applied to different problems. Nevertheless, its\nperformance on few-shot classification is far behind many recent algorithms\ndedicated to the problem. In this paper, we point out several key facets of how\nto train MAML to excel in few-shot classification. First, we find that a large\nnumber of gradient steps are needed for the inner loop update, which\ncontradicts the common usage of MAML for few-shot classification. Second, we\nfind that MAML is sensitive to the permutation of class assignments in\nmeta-testing: for a few-shot task of $N$ classes, there are exponentially many\nways to assign the learned initialization of the $N$-way classifier to the $N$\nclasses, leading to an unavoidably huge variance. Third, we investigate several\nways for permutation invariance and find that learning a shared classifier\ninitialization for all the classes performs the best. On benchmark datasets\nsuch as MiniImageNet and TieredImageNet, our approach, which we name\nUNICORN-MAML, performs on a par with or even outperforms state-of-the-art\nalgorithms, while keeping the simplicity of MAML without adding any extra\nsub-networks.",
          "link": "http://arxiv.org/abs/2106.16245",
          "publishedOn": "2021-07-01T01:59:33.501Z",
          "wordCount": 632,
          "title": "How to Train Your MAML to Excel in Few-Shot Classification. (arXiv:2106.16245v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16004",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vlaar_T/0/1/0/all/0/1\">Tiffany Vlaar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frankle_J/0/1/0/all/0/1\">Jonathan Frankle</a>",
          "description": "Studying neural network loss landscapes provides insights into the nature of\nthe underlying optimization problems. Unfortunately, loss landscapes are\nnotoriously difficult to visualize in a human-comprehensible fashion. One\ncommon way to address this problem is to plot linear slices of the landscape,\nfor example from the initial state of the network to the final state after\noptimization. On the basis of this analysis, prior work has drawn broader\nconclusions about the difficulty of the optimization problem. In this paper, we\nput inferences of this kind to the test, systematically evaluating how linear\ninterpolation and final performance vary when altering the data, choice of\ninitialization, and other optimizer and architecture design choices. Further,\nwe use linear interpolation to study the role played by individual layers and\nsubstructures of the network. We find that certain layers are more sensitive to\nthe choice of initialization and optimizer hyperparameter settings, and we\nexploit these observations to design custom optimization schemes. However, our\nresults cast doubt on the broader intuition that the presence or absence of\nbarriers when interpolating necessarily relates to the success of optimization.",
          "link": "http://arxiv.org/abs/2106.16004",
          "publishedOn": "2021-07-01T01:59:33.495Z",
          "wordCount": 622,
          "title": "What can linear interpolation of neural network loss landscapes tell us?. (arXiv:2106.16004v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16198",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Madan_S/0/1/0/all/0/1\">Spandan Madan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sasaki_T/0/1/0/all/0/1\">Tomotake Sasaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tzu-Mao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boix_X/0/1/0/all/0/1\">Xavier Boix</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfister_H/0/1/0/all/0/1\">Hanspeter Pfister</a>",
          "description": "Neural networks are susceptible to small transformations including 2D\nrotations and shifts, image crops, and even changes in object colors. This is\noften attributed to biases in the training dataset, and the lack of 2D\nshift-invariance due to not respecting the sampling theorem. In this paper, we\nchallenge this hypothesis by training and testing on unbiased datasets, and\nshowing that networks are brittle to both small 3D perspective changes and\nlighting variations which cannot be explained by dataset bias or lack of\nshift-invariance. To find these in-distribution errors, we introduce an\nevolution strategies (ES) based approach, which we call CMA-Search. Despite\ntraining with a large-scale (0.5 million images), unbiased dataset of camera\nand light variations, in over 71% cases CMA-Search can find camera parameters\nin the vicinity of a correctly classified image which lead to in-distribution\nmisclassifications with < 3.6% change in parameters. With lighting changes,\nCMA-Search finds misclassifications in 33% cases with < 11.6% change in\nparameters. Finally, we extend this method to find misclassifications in the\nvicinity of ImageNet images for both ResNet and OpenAI's CLIP model.",
          "link": "http://arxiv.org/abs/2106.16198",
          "publishedOn": "2021-07-01T01:59:33.489Z",
          "wordCount": 630,
          "title": "Small in-distribution changes in 3D perspective and lighting fool both CNNs and Transformers. (arXiv:2106.16198v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2011.14317",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_A/0/1/0/all/0/1\">Arindam Bhattacharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Varambally_S/0/1/0/all/0/1\">Sumanth Varambally</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bagchi_A/0/1/0/all/0/1\">Amitabha Bagchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bedathur_S/0/1/0/all/0/1\">Srikanta Bedathur</a>",
          "description": "We present Fast Random projection-based One-Class Classification (FROCC), an\nextremely efficient method for one-class classification. Our method is based on\na simple idea of transforming the training data by projecting it onto a set of\nrandom unit vectors that are chosen uniformly and independently from the unit\nsphere, and bounding the regions based on separation of the data. FROCC can be\nnaturally extended with kernels. We theoretically prove that FROCC generalizes\nwell in the sense that it is stable and has low bias. FROCC achieves up to 3.1\npercent points better ROC, with 1.2--67.8x speedup in training and test times\nover a range of state-of-the-art benchmarks including the SVM and the deep\nlearning based models for the OCC task.",
          "link": "http://arxiv.org/abs/2011.14317",
          "publishedOn": "2021-07-01T01:59:33.483Z",
          "wordCount": 586,
          "title": "FROCC: Fast Random projection-based One-Class Classification. (arXiv:2011.14317v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.15334",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1\">Jiahua Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cong_Y/0/1/0/all/0/1\">Yang Cong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_G/0/1/0/all/0/1\">Gan Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1\">Xu Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1\">Xiaowei Xu</a>",
          "description": "Online metric learning has been widely exploited for large-scale data\nclassification due to the low computational cost. However, amongst online\npractical scenarios where the features are evolving (e.g., some features are\nvanished and some new features are augmented), most metric learning models\ncannot be successfully applied to these scenarios, although they can tackle the\nevolving instances efficiently. To address the challenge, we develop a new\nonline Evolving Metric Learning (EML) model for incremental and decremental\nfeatures, which can handle the instance and feature evolutions simultaneously\nby incorporating with a smoothed Wasserstein metric distance. Specifically, our\nmodel contains two essential stages: a Transforming stage (T-stage) and a\nInheriting stage (I-stage). For the T-stage, we propose to extract important\ninformation from vanished features while neglecting non-informative knowledge,\nand forward it into survived features by transforming them into a low-rank\ndiscriminative metric space. It further explores the intrinsic low-rank\nstructure of heterogeneous samples to reduce the computation and memory burden\nespecially for highly-dimensional large-scale data. For the I-stage, we inherit\nthe metric performance of survived features from the T-stage and then expand to\ninclude the new augmented features. Moreover, a smoothed Wasserstein distance\nis utilized to characterize the similarity relationships among the\nheterogeneous and complex samples, since the evolving features are not strictly\naligned in the different stages. In addition to tackling the challenges in\none-shot case, we also extend our model into multishot scenario. After deriving\nan efficient optimization strategy for both T-stage and I-stage, extensive\nexperiments on several datasets verify the superior performance of our EML\nmodel.",
          "link": "http://arxiv.org/abs/2006.15334",
          "publishedOn": "2021-07-01T01:59:33.465Z",
          "wordCount": 738,
          "title": "Evolving Metric Learning for Incremental and Decremental Features. (arXiv:2006.15334v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.04259",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Even_M/0/1/0/all/0/1\">Mathieu Even</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Massoulie_L/0/1/0/all/0/1\">Laurent Massouli&#xe9;</a>",
          "description": "Dimension is an inherent bottleneck to some modern learning tasks, where\noptimization methods suffer from the size of the data. In this paper, we study\nnon-isotropic distributions of data and develop tools that aim at reducing\nthese dimensional costs by a dependency on an effective dimension rather than\nthe ambient one. Based on non-asymptotic estimates of the metric entropy of\nellipsoids -- that prove to generalize to infinite dimensions -- and on a\nchaining argument, our uniform concentration bounds involve an effective\ndimension instead of the global dimension, improving over existing results. We\nshow the importance of taking advantage of non-isotropic properties in learning\nproblems with the following applications: i) we improve state-of-the-art\nresults in statistical preconditioning for communication-efficient distributed\noptimization, ii) we introduce a non-isotropic randomized smoothing for\nnon-smooth optimization. Both applications cover a class of functions that\nencompasses empirical risk minization (ERM) for linear models.",
          "link": "http://arxiv.org/abs/2102.04259",
          "publishedOn": "2021-07-01T01:59:33.418Z",
          "wordCount": 619,
          "title": "Concentration of Non-Isotropic Random Tensors with Applications to Learning and Empirical Risk Minimization. (arXiv:2102.04259v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.09396",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Borji_A/0/1/0/all/0/1\">Ali Borji</a>",
          "description": "This work is an update of a previous paper on the same topic published a few\nyears ago. With the dramatic progress in generative modeling, a suite of new\nquantitative and qualitative techniques to evaluate models has emerged.\nAlthough some measures such as Inception Score, Frechet Inception Distance,\nPrecision-Recall, and Perceptual Path Length are relatively more popular, GAN\nevaluation is not a settled issue and there is still room for improvement.\nHere, I describe new dimensions that are becoming important in assessing models\n(e.g. bias and fairness) and discuss the connection between GAN evaluation and\ndeepfakes. These are important areas of concern in the machine learning\ncommunity today and progress in GAN evaluation can help mitigate them.",
          "link": "http://arxiv.org/abs/2103.09396",
          "publishedOn": "2021-07-01T01:59:33.411Z",
          "wordCount": 581,
          "title": "Pros and Cons of GAN Evaluation Measures: New Developments. (arXiv:2103.09396v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15820",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Amich_A/0/1/0/all/0/1\">Abderrahmen Amich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eshete_B/0/1/0/all/0/1\">Birhanu Eshete</a>",
          "description": "Machine Learning (ML) models are susceptible to evasion attacks. Evasion\naccuracy is typically assessed using aggregate evasion rate, and it is an open\nquestion whether aggregate evasion rate enables feature-level diagnosis on the\neffect of adversarial perturbations on evasive predictions. In this paper, we\nintroduce a novel framework that harnesses explainable ML methods to guide\nhigh-fidelity assessment of ML evasion attacks. Our framework enables\nexplanation-guided correlation analysis between pre-evasion perturbations and\npost-evasion explanations. Towards systematic assessment of ML evasion attacks,\nwe propose and evaluate a novel suite of model-agnostic metrics for\nsample-level and dataset-level correlation analysis. Using malware and image\nclassifiers, we conduct comprehensive evaluations across diverse model\narchitectures and complementary feature representations. Our explanation-guided\ncorrelation analysis reveals correlation gaps between adversarial samples and\nthe corresponding perturbations performed on them. Using a case study on\nexplanation-guided evasion, we show the broader usage of our methodology for\nassessing robustness of ML models.",
          "link": "http://arxiv.org/abs/2106.15820",
          "publishedOn": "2021-07-01T01:59:33.406Z",
          "wordCount": 602,
          "title": "Explanation-Guided Diagnosis of Machine Learning Evasion Attacks. (arXiv:2106.15820v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15810",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1\">Abhay Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1\">Qian Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Sijia Linda Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhalerao_O/0/1/0/all/0/1\">Omkar Bhalerao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">Horace He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1\">Ser-Nam Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Benson_A/0/1/0/all/0/1\">Austin R. Benson</a>",
          "description": "Graphs are a common model for complex relational data such as social networks\nand protein interactions, and such data can evolve over time (e.g., new\nfriendships) and be noisy (e.g., unmeasured interactions). Link prediction aims\nto predict future edges or infer missing edges in the graph, and has diverse\napplications in recommender systems, experimental design, and complex systems.\nEven though link prediction algorithms strongly depend on the set of edges in\nthe graph, existing approaches typically do not modify the graph topology to\nimprove performance. Here, we demonstrate how simply adding a set of edges,\nwhich we call a \\emph{proposal set}, to the graph as a pre-processing step can\nimprove the performance of several link prediction algorithms. The underlying\nidea is that if the edges in the proposal set generally align with the\nstructure of the graph, link prediction algorithms are further guided towards\npredicting the right edges; in other words, adding a proposal set of edges is a\nsignal-boosting pre-processing step. We show how to use existing link\nprediction algorithms to generate effective proposal sets and evaluate this\napproach on various synthetic and empirical datasets. We find that proposal\nsets meaningfully improve the accuracy of link prediction algorithms based on\nboth neighborhood heuristics and graph neural networks. Code is available at\n\\url{https://github.com/CUAI/Edge-Proposal-Sets}.",
          "link": "http://arxiv.org/abs/2106.15810",
          "publishedOn": "2021-07-01T01:59:33.389Z",
          "wordCount": 657,
          "title": "Edge Proposal Sets for Link Prediction. (arXiv:2106.15810v1 [cs.SI])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15764",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mirsky_Y/0/1/0/all/0/1\">Yisroel Mirsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demontis_A/0/1/0/all/0/1\">Ambra Demontis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kotak_J/0/1/0/all/0/1\">Jaidip Kotak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shankar_R/0/1/0/all/0/1\">Ram Shankar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gelei_D/0/1/0/all/0/1\">Deng Gelei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1\">Liu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_W/0/1/0/all/0/1\">Wenke Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elovici_Y/0/1/0/all/0/1\">Yuval Elovici</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biggio_B/0/1/0/all/0/1\">Battista Biggio</a>",
          "description": "AI has provided us with the ability to automate tasks, extract information\nfrom vast amounts of data, and synthesize media that is nearly\nindistinguishable from the real thing. However, positive tools can also be used\nfor negative purposes. In particular, cyber adversaries can use AI (such as\nmachine learning) to enhance their attacks and expand their campaigns.\n\nAlthough offensive AI has been discussed in the past, there is a need to\nanalyze and understand the threat in the context of organizations. For example,\nhow does an AI-capable adversary impact the cyber kill chain? Does AI benefit\nthe attacker more than the defender? What are the most significant AI threats\nfacing organizations today and what will be their impact on the future?\n\nIn this survey, we explore the threat of offensive AI on organizations.\nFirst, we present the background and discuss how AI changes the adversary's\nmethods, strategies, goals, and overall attack model. Then, through a\nliterature review, we identify 33 offensive AI capabilities which adversaries\ncan use to enhance their attacks. Finally, through a user study spanning\nindustry and academia, we rank the AI threats and provide insights on the\nadversaries.",
          "link": "http://arxiv.org/abs/2106.15764",
          "publishedOn": "2021-07-01T01:59:33.382Z",
          "wordCount": 645,
          "title": "The Threat of Offensive AI to Organizations. (arXiv:2106.15764v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15842",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhizheng Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Song_W/0/1/0/all/0/1\">Wen Song</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Q/0/1/0/all/0/1\">Qiqiang Li</a>",
          "description": "Remaining useful life prediction (RUL) is one of the key technologies of\ncondition-based maintenance, which is important to maintain the reliability and\nsafety of industrial equipments. While deep learning has achieved great success\nin RUL prediction, existing methods have difficulties in processing long\nsequences and extracting information from the sensor and time step aspects. In\nthis paper, we propose Dual Aspect Self-attention based on Transformer (DAST),\na novel deep RUL prediction method. DAST consists of two encoders, which work\nin parallel to simultaneously extract features of different sensors and time\nsteps. Solely based on self-attention, the DAST encoders are more effective in\nprocessing long data sequences, and are capable of adaptively learning to focus\non more important parts of input. Moreover, the parallel feature extraction\ndesign avoids mutual influence of information from two aspects. Experimental\nresults on two real turbofan engine datasets show that our method significantly\noutperforms state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2106.15842",
          "publishedOn": "2021-07-01T01:59:33.370Z",
          "wordCount": 596,
          "title": "Dual Aspect Self-Attention based on Transformer for Remaining Useful Life Prediction. (arXiv:2106.15842v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16116",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rudi_A/0/1/0/all/0/1\">Alessandro Rudi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ciliberto_C/0/1/0/all/0/1\">Carlo Ciliberto</a>",
          "description": "Finding a good way to model probability densities is key to probabilistic\ninference. An ideal model should be able to concisely approximate any\nprobability, while being also compatible with two main operations:\nmultiplications of two models (product rule) and marginalization with respect\nto a subset of the random variables (sum rule). In this work, we show that a\nrecently proposed class of positive semi-definite (PSD) models for non-negative\nfunctions is particularly suited to this end. In particular, we characterize\nboth approximation and generalization capabilities of PSD models, showing that\nthey enjoy strong theoretical guarantees. Moreover, we show that we can perform\nefficiently both sum and product rule in closed form via matrix operations,\nenjoying the same versatility of mixture models. Our results open the way to\napplications of PSD models to density estimation, decision theory and\ninference. Preliminary empirical evaluation supports our findings.",
          "link": "http://arxiv.org/abs/2106.16116",
          "publishedOn": "2021-07-01T01:59:33.364Z",
          "wordCount": 581,
          "title": "PSD Representations for Effective Probability Models. (arXiv:2106.16116v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2010.03415",
          "author": "<a href=\"http://arxiv.org/find/nlin/1/au:+Jiahao_T/0/1/0/all/0/1\">Tom Z. Jiahao</a>, <a href=\"http://arxiv.org/find/nlin/1/au:+Hsieh_M/0/1/0/all/0/1\">M. Ani Hsieh</a>, <a href=\"http://arxiv.org/find/nlin/1/au:+Forgoston_E/0/1/0/all/0/1\">Eric Forgoston</a>",
          "description": "Extracting predictive models from nonlinear systems is a central task in\nscientific machine learning. One key problem is the reconciliation between\nmodern data-driven approaches and first principles. Despite rapid advances in\nmachine learning techniques, embedding domain knowledge into data-driven models\nremains a challenge. In this work, we present a universal learning framework\nfor extracting predictive models from nonlinear systems based on observations.\nOur framework can readily incorporate first principle knowledge because it\nnaturally models nonlinear systems as continuous-time systems. This both\nimproves the extracted models' extrapolation power and reduces the amount of\ndata needed for training. In addition, our framework has the advantages of\nrobustness to observational noise and applicability to irregularly sampled\ndata. We demonstrate the effectiveness of our scheme by learning predictive\nmodels for a wide variety of systems including a stiff Van der Pol oscillator,\nthe Lorenz system, and the Kuramoto-Sivashinsky equation. For the Lorenz\nsystem, different types of domain knowledge are incorporated to demonstrate the\nstrength of knowledge embedding in data-driven system identification.",
          "link": "http://arxiv.org/abs/2010.03415",
          "publishedOn": "2021-07-01T01:59:33.358Z",
          "wordCount": 642,
          "title": "Knowledge-Based Learning of Nonlinear Dynamics and Chaos. (arXiv:2010.03415v3 [nlin.CD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.04008",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ha_S/0/1/0/all/0/1\">Seungwoong Ha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeong_H/0/1/0/all/0/1\">Hawoong Jeong</a>",
          "description": "Invariants and conservation laws convey critical information about the\nunderlying dynamics of a system, yet it is generally infeasible to find them\nfrom large-scale data without any prior knowledge or human insight. We propose\nConservNet to achieve this goal, a neural network that spontaneously discovers\na conserved quantity from grouped data where the members of each group share\ninvariants, similar to a general experimental setting where trajectories from\ndifferent trials are observed. As a neural network trained with a novel and\nintuitive loss function called noise-variance loss, ConservNet learns the\nhidden invariants in each group of multi-dimensional observables in a\ndata-driven, end-to-end manner. Our model successfully discovers underlying\ninvariants from the simulated systems having invariants as well as a real-world\ndouble pendulum trajectory. Since the model is robust to various noises and\ndata conditions compared to baseline, our approach is directly applicable to\nexperimental data for discovering hidden conservation laws and further, general\nrelationships between variables.",
          "link": "http://arxiv.org/abs/2102.04008",
          "publishedOn": "2021-07-01T01:59:33.342Z",
          "wordCount": 635,
          "title": "Discovering conservation laws from trajectories via machine learning. (arXiv:2102.04008v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15671",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wehenkel_A/0/1/0/all/0/1\">Antoine Wehenkel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Louppe_G/0/1/0/all/0/1\">Gilles Louppe</a>",
          "description": "Among likelihood-based approaches for deep generative modelling, variational\nautoencoders (VAEs) offer scalable amortized posterior inference and fast\nsampling. However, VAEs are also more and more outperformed by competing models\nsuch as normalizing flows (NFs), deep-energy models, or the new denoising\ndiffusion probabilistic models (DDPMs). In this preliminary work, we improve\nVAEs by demonstrating how DDPMs can be used for modelling the prior\ndistribution of the latent variables. The diffusion prior model improves upon\nGaussian priors of classical VAEs and is competitive with NF-based priors.\nFinally, we hypothesize that hierarchical VAEs could similarly benefit from the\nenhanced capacity of diffusion priors.",
          "link": "http://arxiv.org/abs/2106.15671",
          "publishedOn": "2021-07-01T01:59:33.284Z",
          "wordCount": 520,
          "title": "Diffusion Priors In Variational Autoencoders. (arXiv:2106.15671v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16187",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Saboo_K/0/1/0/all/0/1\">Krishnakant V. Saboo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choudhary_A/0/1/0/all/0/1\">Anirudh Choudhary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yurui Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Worrell_G/0/1/0/all/0/1\">Gregory A. Worrell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jones_D/0/1/0/all/0/1\">David T. Jones</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyer_R/0/1/0/all/0/1\">Ravishankar K. Iyer</a>",
          "description": "We model Alzheimer's disease (AD) progression by combining differential\nequations (DEs) and reinforcement learning (RL) with domain knowledge. DEs\nprovide relationships between some, but not all, factors relevant to AD. We\nassume that the missing relationships must satisfy general criteria about the\nworking of the brain, for e.g., maximizing cognition while minimizing the cost\nof supporting cognition. This allows us to extract the missing relationships by\nusing RL to optimize an objective (reward) function that captures the above\ncriteria. We use our model consisting of DEs (as a simulator) and the trained\nRL agent to predict individualized 10-year AD progression using baseline (year\n0) features on synthetic and real data. The model was comparable or better at\npredicting 10-year cognition trajectories than state-of-the-art learning-based\nmodels. Our interpretable model demonstrated, and provided insights into,\n\"recovery/compensatory\" processes that mitigate the effect of AD, even though\nthose processes were not explicitly encoded in the model. Our framework\ncombines DEs with RL for modelling AD progression and has broad applicability\nfor understanding other neurological disorders.",
          "link": "http://arxiv.org/abs/2106.16187",
          "publishedOn": "2021-07-01T01:59:33.202Z",
          "wordCount": 621,
          "title": "Reinforcement Learning based Disease Progression Model for Alzheimer's Disease. (arXiv:2106.16187v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16036",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Verma_P/0/1/0/all/0/1\">Prateek Verma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chafe_C/0/1/0/all/0/1\">Chris Chafe</a>",
          "description": "This paper proposes a novel way of doing audio synthesis at the waveform\nlevel using Transformer architectures. We propose a deep neural network for\ngenerating waveforms, similar to wavenet \\cite{oord2016wavenet}. This is fully\nprobabilistic, auto-regressive, and causal, i.e. each sample generated depends\nonly on the previously observed samples. Our approach outperforms a widely used\nwavenet architecture by up to 9\\% on a similar dataset for predicting the next\nstep. Using the attention mechanism, we enable the architecture to learn which\naudio samples are important for the prediction of the future sample. We show\nhow causal transformer generative models can be used for raw waveform\nsynthesis. We also show that this performance can be improved by another 2\\% by\nconditioning samples over a wider context. The flexibility of the current model\nto synthesize audio from latent representations suggests a large number of\npotential applications. The novel approach of using generative transformer\narchitectures for raw audio synthesis is, however, still far away from\ngenerating any meaningful music, without using latent codes/meta-data to aid\nthe generation process.",
          "link": "http://arxiv.org/abs/2106.16036",
          "publishedOn": "2021-07-01T01:59:33.177Z",
          "wordCount": 620,
          "title": "A Generative Model for Raw Audio Using Transformer Architectures. (arXiv:2106.16036v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16039",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Goutay_M/0/1/0/all/0/1\">Mathieu Goutay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aoudia_F/0/1/0/all/0/1\">Fay&#xe7;al Ait Aoudia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoydis_J/0/1/0/all/0/1\">Jakob Hoydis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gorce_J/0/1/0/all/0/1\">Jean-Marie Gorce</a>",
          "description": "Orthogonal frequency-division multiplexing (OFDM) is widely used in modern\nwireless networks thanks to its efficient handling of multipath environment.\nHowever, it suffers from a poor peak-to-average power ratio (PAPR) which\nrequires a large power backoff, degrading the power amplifier (PA) efficiency.\nIn this work, we propose to use a neural network (NN) at the transmitter to\nlearn a high-dimensional modulation scheme allowing to control the PAPR and\nadjacent channel leakage ratio (ACLR). On the receiver side, a NN-based\nreceiver is implemented to carry out demapping of the transmitted bits. The two\nNNs operate on top of OFDM, and are jointly optimized in and end-to-end manner\nusing a training algorithm that enforces constraints on the PAPR and ACLR.\nSimulation results show that the learned waveforms enable higher information\nrates than a tone reservation baseline, while satisfying predefined PAPR and\nACLR targets.",
          "link": "http://arxiv.org/abs/2106.16039",
          "publishedOn": "2021-07-01T01:59:33.148Z",
          "wordCount": 586,
          "title": "End-to-End Learning of OFDM Waveforms with PAPR and ACLR Constraints. (arXiv:2106.16039v1 [cs.IT])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16194",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Hojatian_H/0/1/0/all/0/1\">Hamed Hojatian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nadal_J/0/1/0/all/0/1\">Jeremy Nadal</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Frigon_J/0/1/0/all/0/1\">Jean-Francois Frigon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Leduc_Primeau_F/0/1/0/all/0/1\">Francois Leduc-Primeau</a>",
          "description": "Cell-free massive MIMO (CF-mMIMO) systems represent a promising approach to\nincrease the spectral efficiency of wireless communication systems. However,\nnear-optimal solutions require a large amount of signaling exchange between\naccess points (APs) and the network controller (NC). In addition, the use of\nhybrid beamforming in each AP reduces the number of power hungry RF chains, but\nimposes a large computational complexity to find near-optimal precoders. In\nthis letter, we propose two unsupervised deep neural networks (DNN)\narchitectures, fully and partially distributed, that can perform coordinated\nhybrid beamforming with zero or limited communication overhead between APs and\nNC, while achieving near-optimal sum-rate with a reduced computational\ncomplexity compared to conventional near-optimal solutions.",
          "link": "http://arxiv.org/abs/2106.16194",
          "publishedOn": "2021-07-01T01:59:33.112Z",
          "wordCount": 568,
          "title": "Limited-Fronthaul Cell-Free Hybrid Beamforming with Distributed Deep Neural Network. (arXiv:2106.16194v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15698",
          "author": "<a href=\"http://arxiv.org/find/econ/1/au:+Consoli_S/0/1/0/all/0/1\">Sergio Consoli</a>, <a href=\"http://arxiv.org/find/econ/1/au:+Pezzoli_L/0/1/0/all/0/1\">Luca Tiozzo Pezzoli</a>, <a href=\"http://arxiv.org/find/econ/1/au:+Tosetti_E/0/1/0/all/0/1\">Elisa Tosetti</a>",
          "description": "We show how emotions extracted from macroeconomic news can be used to explain\nand forecast future behaviour of sovereign bond yield spreads in Italy and\nSpain. We use a big, open-source, database known as Global Database of Events,\nLanguage and Tone to construct emotion indicators of bond market affective\nstates. We find that negative emotions extracted from news improve the\nforecasting power of government yield spread models during distressed periods\neven after controlling for the number of negative words present in the text. In\naddition, stronger negative emotions, such as panic, reveal useful information\nfor predicting changes in spread at the short-term horizon, while milder\nemotions, such as distress, are useful at longer time horizons. Emotions\ngenerated by the Italian political turmoil propagate to the Spanish news\naffecting this neighbourhood market.",
          "link": "http://arxiv.org/abs/2106.15698",
          "publishedOn": "2021-07-01T01:59:33.100Z",
          "wordCount": 606,
          "title": "Emotions in Macroeconomic News and their Impact on the European Bond Market. (arXiv:2106.15698v1 [econ.GN])"
        },
        {
          "id": "http://arxiv.org/abs/2009.07439",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1\">Dachao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_R/0/1/0/all/0/1\">Ruoyu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhihua Zhang</a>",
          "description": "Sparse neural networks have received increasing interests due to their small\nsize compared to dense networks. Nevertheless, most existing works on neural\nnetwork theory have focused on dense neural networks, and our understanding of\nsparse networks is very limited. In this paper, we study the loss landscape of\none-hidden-layer sparse networks. We first consider sparse networks with linear\nactivations. We show that sparse linear networks can have spurious strict\nminima, which is in sharp contrast to dense linear networks which do not even\nhave spurious minima. Second, we show that spurious valleys can exist for wide\nsparse non-linear networks. This is different from wide dense networks which do\nnot have spurious valleys under mild assumptions.",
          "link": "http://arxiv.org/abs/2009.07439",
          "publishedOn": "2021-07-01T01:59:33.085Z",
          "wordCount": 586,
          "title": "On the Landscape of One-hidden-layer Sparse Networks and Beyond. (arXiv:2009.07439v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.16020",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fourure_D/0/1/0/all/0/1\">Damien Fourure</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Javaid_M/0/1/0/all/0/1\">Muhammad Usama Javaid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Posocco_N/0/1/0/all/0/1\">Nicolas Posocco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tihon_S/0/1/0/all/0/1\">Simon Tihon</a>",
          "description": "Anomaly detection is a widely explored domain in machine learning. Many\nmodels are proposed in the literature, and compared through different metrics\nmeasured on various datasets. The most popular metrics used to compare\nperformances are F1-score, AUC and AVPR. In this paper, we show that F1-score\nand AVPR are highly sensitive to the contamination rate. One consequence is\nthat it is possible to artificially increase their values by modifying the\ntrain-test split procedure. This leads to misleading comparisons between\nalgorithms in the literature, especially when the evaluation protocol is not\nwell detailed. Moreover, we show that the F1-score and the AVPR cannot be used\nto compare performances on different datasets as they do not reflect the\nintrinsic difficulty of modeling such data. Based on these observations, we\nclaim that F1-score and AVPR should not be used as metrics for anomaly\ndetection. We recommend a generic evaluation procedure for unsupervised anomaly\ndetection, including the use of other metrics such as the AUC, which are more\nrobust to arbitrary choices in the evaluation protocol.",
          "link": "http://arxiv.org/abs/2106.16020",
          "publishedOn": "2021-07-01T01:59:33.079Z",
          "wordCount": 634,
          "title": "Anomaly Detection: How to Artificially Increase your F1-Score with a Biased Evaluation Protocol. (arXiv:2106.16020v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15927",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Lijia Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xiao-Shan Gao</a>",
          "description": "In this paper, we present a robust classification-autoencoder (CAE) which has\nstrong ability to recognize outliers and defend adversaries. The basic idea is\nto change the autoencoder from an unsupervised learning method into a\nclassifier. The CAE is a modified autoencoder, where the encoder is used to\ncompress samples with different labels into disjoint compression spaces and the\ndecoder is used to recover a sample with a given label from the corresponding\ncompression space. The encoder is used as a classifier and the decoder is used\nto decide whether the classification given by the encoder is correct by\ncomparing the input sample with the output. Since adversary samples are seeming\ninevitable for the current DNN framework, we introduce the list classification\nbased on CAE to defend adversaries, which outputs several labels and the\ncorresponding samples recovered by the CAE. The CAE is evaluated using the\nMNIST dataset in great detail. It is shown that the CAE network can recognize\nalmost all outliers and the list classification contains the correct label for\nalmost all adversaries.",
          "link": "http://arxiv.org/abs/2106.15927",
          "publishedOn": "2021-07-01T01:59:33.060Z",
          "wordCount": 604,
          "title": "A Robust Classification-autoencoder to Defend Outliers and Adversaries. (arXiv:2106.15927v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15998",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wiens_D/0/1/0/all/0/1\">Daniel Wiens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hammer_B/0/1/0/all/0/1\">Barbara Hammer</a>",
          "description": "Even though deep neural networks succeed on many different tasks including\nsemantic segmentation, they lack on robustness against adversarial examples. To\ncounteract this exploit, often adversarial training is used. However, it is\nknown that adversarial training with weak adversarial attacks (e.g. using the\nFast Gradient Method) does not improve the robustness against stronger attacks.\nRecent research shows that it is possible to increase the robustness of such\nsingle-step methods by choosing an appropriate step size during the training.\nFinding such a step size, without increasing the computational effort of\nsingle-step adversarial training, is still an open challenge. In this work we\naddress the computationally particularly demanding task of semantic\nsegmentation and propose a new step size control algorithm that increases the\nrobustness of single-step adversarial training. The proposed algorithm does not\nincrease the computational effort of single-step adversarial training\nconsiderably and also simplifies training, because it is free of\nmeta-parameter. We show that the robustness of our approach can compete with\nmulti-step adversarial training on two popular benchmarks for semantic\nsegmentation.",
          "link": "http://arxiv.org/abs/2106.15998",
          "publishedOn": "2021-07-01T01:59:33.024Z",
          "wordCount": 609,
          "title": "Single-Step Adversarial Training for Semantic Segmentation. (arXiv:2106.15998v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15792",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1\">Zhen Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jie Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1\">Anjin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Feng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1\">Guangquan Zhang</a>",
          "description": "Traditional supervised learning aims to train a classifier in the closed-set\nworld, where training and test samples share the same label space. In this\npaper, we target a more challenging and realistic setting: open-set learning\n(OSL), where there exist test samples from the classes that are unseen during\ntraining. Although researchers have designed many methods from the algorithmic\nperspectives, there are few methods that provide generalization guarantees on\ntheir ability to achieve consistent performance on different training samples\ndrawn from the same distribution. Motivated by the transfer learning and\nprobably approximate correct (PAC) theory, we make a bold attempt to study OSL\nby proving its generalization error-given training samples with size n, the\nestimation error will get close to order O_p(1/\\sqrt{n}). This is the first\nstudy to provide a generalization bound for OSL, which we do by theoretically\ninvestigating the risk of the target classifier on unknown classes. According\nto our theory, a novel algorithm, called auxiliary open-set risk (AOSR) is\nproposed to address the OSL problem. Experiments verify the efficacy of AOSR.\nThe code is available at github.com/Anjin-Liu/Openset_Learning_AOSR.",
          "link": "http://arxiv.org/abs/2106.15792",
          "publishedOn": "2021-07-01T01:59:32.980Z",
          "wordCount": 619,
          "title": "Learning Bounds for Open-Set Learning. (arXiv:2106.15792v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15850",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Waqas_A/0/1/0/all/0/1\">Asim Waqas</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Rasool_G/0/1/0/all/0/1\">Ghulam Rasool</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Farooq_H/0/1/0/all/0/1\">Hamza Farooq</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Bouaynaya_N/0/1/0/all/0/1\">Nidhal C. Bouaynaya</a> (1), ((1) Rowan University, (2) University of Minnesota)",
          "description": "Motivated by graph theory, artificial neural networks (ANNs) are\ntraditionally structured as layers of neurons (nodes), which learn useful\ninformation by the passage of data through interconnections (edges). In the\nmachine learning realm, graph structures (i.e., neurons and connections) of\nANNs have recently been explored using various graph-theoretic measures linked\nto their predictive performance. On the other hand, in network science\n(NetSci), certain graph measures including entropy and curvature are known to\nprovide insight into the robustness and fragility of real-world networks. In\nthis work, we use these graph measures to explore the robustness of various\nANNs to adversarial attacks. To this end, we (1) explore the design space of\ninter-layer and intra-layers connectivity regimes of ANNs in the graph domain\nand record their predictive performance after training under different types of\nadversarial attacks, (2) use graph representations for both inter-layer and\nintra-layers connectivity regimes to calculate various graph-theoretic\nmeasures, including curvature and entropy, and (3) analyze the relationship\nbetween these graph measures and the adversarial performance of ANNs. We show\nthat curvature and entropy, while operating in the graph domain, can quantify\nthe robustness of ANNs without having to train these ANNs. Our results suggest\nthat the real-world networks, including brain networks, financial networks, and\nsocial networks may provide important clues to the neural architecture search\nfor robust ANNs. We propose a search strategy that efficiently finds robust\nANNs amongst a set of well-performing ANNs without having a need to train all\nof these ANNs.",
          "link": "http://arxiv.org/abs/2106.15850",
          "publishedOn": "2021-07-01T01:59:32.967Z",
          "wordCount": 696,
          "title": "Exploring Robustness of Neural Networks through Graph Measures. (arXiv:2106.15850v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15831",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Andreassen_A/0/1/0/all/0/1\">Anders Andreassen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bahri_Y/0/1/0/all/0/1\">Yasaman Bahri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neyshabur_B/0/1/0/all/0/1\">Behnam Neyshabur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roelofs_R/0/1/0/all/0/1\">Rebecca Roelofs</a>",
          "description": "Although machine learning models typically experience a drop in performance\non out-of-distribution data, accuracies on in- versus out-of-distribution data\nare widely observed to follow a single linear trend when evaluated across a\ntestbed of models. Models that are more accurate on the out-of-distribution\ndata relative to this baseline exhibit \"effective robustness\" and are\nexceedingly rare. Identifying such models, and understanding their properties,\nis key to improving out-of-distribution performance. We conduct a thorough\nempirical investigation of effective robustness during fine-tuning and\nsurprisingly find that models pre-trained on larger datasets exhibit effective\nrobustness during training that vanishes at convergence. We study how\nproperties of the data influence effective robustness, and we show that it\nincreases with the larger size, more diversity, and higher example difficulty\nof the dataset. We also find that models that display effective robustness are\nable to correctly classify 10% of the examples that no other current testbed\nmodel gets correct. Finally, we discuss several strategies for scaling\neffective robustness to the high-accuracy regime to improve the\nout-of-distribution accuracy of state-of-the-art models.",
          "link": "http://arxiv.org/abs/2106.15831",
          "publishedOn": "2021-07-01T01:59:32.949Z",
          "wordCount": 617,
          "title": "The Evolution of Out-of-Distribution Robustness Throughout Fine-Tuning. (arXiv:2106.15831v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.16101",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Huang_F/0/1/0/all/0/1\">Feihu Huang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Huang_H/0/1/0/all/0/1\">Heng Huang</a>",
          "description": "In the paper, we propose a class of faster adaptive gradient descent ascent\nmethods for solving the nonconvex-strongly-concave minimax problems by using\nunified adaptive matrices used in the SUPER-ADAM \\citep{huang2021super}.\nSpecifically, we propose a fast adaptive gradient decent ascent (AdaGDA) method\nbased on the basic momentum technique, which reaches a low sample complexity of\n$O(\\kappa^4\\epsilon^{-4})$ for finding an $\\epsilon$-stationary point without\nlarge batches, which improves the existing result of adaptive minimax\noptimization method by a factor of $O(\\sqrt{\\kappa})$. Moreover, we present an\naccelerated version of AdaGDA (VR-AdaGDA) method based on the momentum-based\nvariance reduced technique, which achieves the best known sample complexity of\n$O(\\kappa^3\\epsilon^{-3})$ for finding an $\\epsilon$-stationary point without\nlarge batches. Further assume the bounded Lipschitz parameter of objective\nfunction, we prove that our VR-AdaGDA method reaches a lower sample complexity\nof $O(\\kappa^{2.5}\\epsilon^{-3})$ with the mini-batch size $O(\\kappa)$. In\nparticular, we provide an effective convergence analysis framework for our\nadaptive methods based on unified adaptive matrices, which include almost\nexisting adaptive learning rates.",
          "link": "http://arxiv.org/abs/2106.16101",
          "publishedOn": "2021-07-01T01:59:32.935Z",
          "wordCount": 610,
          "title": "AdaGDA: Faster Adaptive Gradient Descent Ascent Methods for Minimax Optimization. (arXiv:2106.16101v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2002.00874",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zaiwei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maguluri_S/0/1/0/all/0/1\">Siva Theja Maguluri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shakkottai_S/0/1/0/all/0/1\">Sanjay Shakkottai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shanmugam_K/0/1/0/all/0/1\">Karthikeyan Shanmugam</a>",
          "description": "Stochastic Approximation (SA) is a popular approach for solving fixed-point\nequations where the information is corrupted by noise. In this paper, we\nconsider an SA involving a contraction mapping with respect to an arbitrary\nnorm, and show its finite-sample error bounds while using different stepsizes.\nThe idea is to construct a smooth Lyapunov function using the generalized\nMoreau envelope, and show that the iterates of SA have negative drift with\nrespect to that Lyapunov function. Our result is applicable in Reinforcement\nLearning (RL). In particular, we use it to establish the first-known\nconvergence rate of the V-trace algorithm for off-policy TD-learning. Moreover,\nwe also use it to study TD-learning in the on-policy setting, and recover the\nexisting state-of-the-art results for $Q$-learning. Importantly, our\nconstruction results in only a logarithmic dependence of the convergence bound\non the size of the state-space.",
          "link": "http://arxiv.org/abs/2002.00874",
          "publishedOn": "2021-07-01T01:59:32.917Z",
          "wordCount": 649,
          "title": "Finite-Sample Analysis of Stochastic Approximation Using Smooth Convex Envelopes. (arXiv:2002.00874v6 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15776",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gao_R/0/1/0/all/0/1\">Ruize Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Feng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1\">Kaiwen Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_G/0/1/0/all/0/1\">Gang Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1\">Bo Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1\">James Cheng</a>",
          "description": "Instances-reweighted adversarial training (IRAT) can significantly boost the\nrobustness of trained models, where data being less/more vulnerable to the\ngiven attack are assigned smaller/larger weights during training. However, when\ntested on attacks different from the given attack simulated in training, the\nrobustness may drop significantly (e.g., even worse than no reweighting). In\nthis paper, we study this problem and propose our solution--locally reweighted\nadversarial training (LRAT). The rationale behind IRAT is that we do not need\nto pay much attention to an instance that is already safe under the attack. We\nargue that the safeness should be attack-dependent, so that for the same\ninstance, its weight can change given different attacks based on the same\nmodel. Thus, if the attack simulated in training is mis-specified, the weights\nof IRAT are misleading. To this end, LRAT pairs each instance with its\nadversarial variants and performs local reweighting inside each pair, while\nperforming no global reweighting--the rationale is to fit the instance itself\nif it is immune to the attack, but not to skip the pair, in order to passively\ndefend different attacks in future. Experiments show that LRAT works better\nthan both IRAT (i.e., global reweighting) and the standard AT (i.e., no\nreweighting) when trained with an attack and tested on different attacks.",
          "link": "http://arxiv.org/abs/2106.15776",
          "publishedOn": "2021-07-01T01:59:32.895Z",
          "wordCount": 640,
          "title": "Local Reweighting for Adversarial Training. (arXiv:2106.15776v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15739",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lobacheva_E/0/1/0/all/0/1\">Ekaterina Lobacheva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kodryan_M/0/1/0/all/0/1\">Maxim Kodryan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chirkova_N/0/1/0/all/0/1\">Nadezhda Chirkova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malinin_A/0/1/0/all/0/1\">Andrey Malinin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vetrov_D/0/1/0/all/0/1\">Dmitry Vetrov</a>",
          "description": "Despite the conventional wisdom that using batch normalization with weight\ndecay may improve neural network training, some recent works show their joint\nusage may cause instabilities at the late stages of training. Other works, in\ncontrast, show convergence to the equilibrium, i.e., the stabilization of\ntraining metrics. In this paper, we study this contradiction and show that\ninstead of converging to a stable equilibrium, the training dynamics converge\nto consistent periodic behavior. That is, the training process regularly\nexhibits instabilities which, however, do not lead to complete training\nfailure, but cause a new period of training. We rigorously investigate the\nmechanism underlying this discovered periodic behavior both from an empirical\nand theoretical point of view and show that this periodic behavior is indeed\ncaused by the interaction between batch normalization and weight decay.",
          "link": "http://arxiv.org/abs/2106.15739",
          "publishedOn": "2021-07-01T01:59:32.889Z",
          "wordCount": 589,
          "title": "On the Periodic Behavior of Neural Network Training with Batch Normalization and Weight Decay. (arXiv:2106.15739v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15853",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yingbin Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_E/0/1/0/all/0/1\">Erkun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1\">Bo Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yanhua Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiatong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1\">Yinian Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_G/0/1/0/all/0/1\">Gang Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tongliang Liu</a>",
          "description": "The memorization effect of deep neural network (DNN) plays a pivotal role in\nmany state-of-the-art label-noise learning methods. To exploit this property,\nthe early stopping trick, which stops the optimization at the early stage of\ntraining, is usually adopted. Current methods generally decide the early\nstopping point by considering a DNN as a whole. However, a DNN can be\nconsidered as a composition of a series of layers, and we find that the latter\nlayers in a DNN are much more sensitive to label noise, while their former\ncounterparts are quite robust. Therefore, selecting a stopping point for the\nwhole network may make different DNN layers antagonistically affected each\nother, thus degrading the final performance. In this paper, we propose to\nseparate a DNN into different parts and progressively train them to address\nthis problem. Instead of the early stopping, which trains a whole DNN all at\nonce, we initially train former DNN layers by optimizing the DNN with a\nrelatively large number of epochs. During training, we progressively train the\nlatter DNN layers by using a smaller number of epochs with the preceding layers\nfixed to counteract the impact of noisy labels. We term the proposed method as\nprogressive early stopping (PES). Despite its simplicity, compared with the\nearly stopping, PES can help to obtain more promising and stable results.\nFurthermore, by combining PES with existing approaches on noisy label training,\nwe achieve state-of-the-art performance on image classification benchmarks.",
          "link": "http://arxiv.org/abs/2106.15853",
          "publishedOn": "2021-07-01T01:59:32.871Z",
          "wordCount": 686,
          "title": "Understanding and Improving Early Stopping for Learning with Noisy Labels. (arXiv:2106.15853v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15793",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yao_X/0/1/0/all/0/1\">Xingxu Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1\">Sicheng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1\">Pengfei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jufeng Yang</a>",
          "description": "To reduce annotation labor associated with object detection, an increasing\nnumber of studies focus on transferring the learned knowledge from a labeled\nsource domain to another unlabeled target domain. However, existing methods\nassume that the labeled data are sampled from a single source domain, which\nignores a more generalized scenario, where labeled data are from multiple\nsource domains. For the more challenging task, we propose a unified Faster\nR-CNN based framework, termed Divide-and-Merge Spindle Network (DMSN), which\ncan simultaneously enhance domain invariance and preserve discriminative power.\nSpecifically, the framework contains multiple source subnets and a pseudo\ntarget subnet. First, we propose a hierarchical feature alignment strategy to\nconduct strong and weak alignments for low- and high-level features,\nrespectively, considering their different effects for object detection. Second,\nwe develop a novel pseudo subnet learning algorithm to approximate optimal\nparameters of pseudo target subset by weighted combination of parameters in\ndifferent source subnets. Finally, a consistency regularization for region\nproposal network is proposed to facilitate each subnet to learn more abstract\ninvariances. Extensive experiments on different adaptation scenarios\ndemonstrate the effectiveness of the proposed model.",
          "link": "http://arxiv.org/abs/2106.15793",
          "publishedOn": "2021-07-01T01:59:32.864Z",
          "wordCount": 623,
          "title": "Multi-Source Domain Adaptation for Object Detection. (arXiv:2106.15793v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15767",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xian Li</a>",
          "description": "Procedural fairness has been a public concern, which leads to controversy\nwhen making decisions with respect to protected classes, such as race, social\nstatus, and disability. Some protected classes can be inferred according to\nsome safe proxies like surname and geolocation for the race. Hence, implicitly\nutilizing the predicted protected classes based on the related proxies when\nmaking decisions is an efficient approach to circumvent this issue and seek\njust decisions. In this article, we propose a hierarchical random forest model\nfor prediction without explicitly involving protected classes. Simulation\nexperiments are conducted to show the performance of the hierarchical random\nforest model. An example is analyzed from Boston police interview records to\nillustrate the usefulness of the proposed model.",
          "link": "http://arxiv.org/abs/2106.15767",
          "publishedOn": "2021-07-01T01:59:32.805Z",
          "wordCount": 551,
          "title": "Unaware Fairness: Hierarchical Random Forest for Protected Classes. (arXiv:2106.15767v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15808",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1\">Baihan Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouneffouf_D/0/1/0/all/0/1\">Djallel Bouneffouf</a>",
          "description": "In light of the COVID-19 pandemic, it is an open challenge and critical\npractical problem to find a optimal way to dynamically prescribe the best\npolicies that balance both the governmental resources and epidemic control in\ndifferent countries and regions. To solve this multi-dimensional tradeoff of\nexploitation and exploration, we formulate this technical challenge as a\ncontextual combinatorial bandit problem that jointly optimizes a multi-criteria\nreward function. Given the historical daily cases in a region and the past\nintervention plans in place, the agent should generate useful intervention\nplans that policy makers can implement in real time to minimizing both the\nnumber of daily COVID-19 cases and the stringency of the recommended\ninterventions. We prove this concept with simulations of multiple realistic\npolicy making scenarios.",
          "link": "http://arxiv.org/abs/2106.15808",
          "publishedOn": "2021-07-01T01:59:32.799Z",
          "wordCount": 620,
          "title": "Optimal Epidemic Control as a Contextual Combinatorial Bandit with Budget. (arXiv:2106.15808v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15845",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jo_J/0/1/0/all/0/1\">Jaehyeong Jo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baek_J/0/1/0/all/0/1\">Jinheon Baek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seul Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Dongki Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_M/0/1/0/all/0/1\">Minki Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1\">Sung Ju Hwang</a>",
          "description": "Graph neural networks have recently achieved remarkable success in\nrepresenting graph-structured data, with rapid progress in both the node\nembedding and graph pooling methods. Yet, they mostly focus on capturing\ninformation from the nodes considering their connectivity, and not much work\nhas been done in representing the edges, which are essential components of a\ngraph. However, for tasks such as graph reconstruction and generation, as well\nas graph classification tasks for which the edges are important for\ndiscrimination, accurately representing edges of a given graph is crucial to\nthe success of the graph representation learning. To this end, we propose a\nnovel edge representation learning framework based on Dual Hypergraph\nTransformation (DHT), which transforms the edges of a graph into the nodes of a\nhypergraph. This dual hypergraph construction allows us to apply message\npassing techniques for node representations to edges. After obtaining edge\nrepresentations from the hypergraphs, we then cluster or drop edges to obtain\nholistic graph-level edge representations. We validate our edge representation\nlearning method with hypergraphs on diverse graph datasets for graph\nrepresentation and generation performance, on which our method largely\noutperforms existing graph representation learning methods. Moreover, our edge\nrepresentation learning and pooling method also largely outperforms\nstate-of-the-art graph pooling methods on graph classification, not only\nbecause of its accurate edge representation learning, but also due to its\nlossless compression of the nodes and removal of irrelevant edges for effective\nmessage passing.",
          "link": "http://arxiv.org/abs/2106.15845",
          "publishedOn": "2021-07-01T01:59:32.775Z",
          "wordCount": 665,
          "title": "Edge Representation Learning with Hypergraphs. (arXiv:2106.15845v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15734",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Su Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hosseinalipour_S/0/1/0/all/0/1\">Seyyedali Hosseinalipour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gorlatova_M/0/1/0/all/0/1\">Maria Gorlatova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brinton_C/0/1/0/all/0/1\">Christopher G. Brinton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiang_M/0/1/0/all/0/1\">Mung Chiang</a>",
          "description": "We consider distributed machine learning (ML) through unmanned aerial\nvehicles (UAVs) for geo-distributed device clusters. We propose five new\ntechnologies/techniques: (i) stratified UAV swarms with leader, worker, and\ncoordinator UAVs, (ii) hierarchical nested personalized federated learning\n(HN-PFL): a holistic distributed ML framework for personalized model training\nacross the worker-leader-core network hierarchy, (iii) cooperative UAV resource\npooling for distributed ML using the UAVs' local computational capabilities,\n(iv) aerial data caching and relaying for efficient data relaying to conduct\nML, and (v) concept/model drift, capturing online data variations at the\ndevices. We split the UAV-enabled model training problem as two parts. (a)\nNetwork-aware HN-PFL, where we optimize a tradeoff between energy consumption\nand ML model performance by configuring data offloading among devices-UAVs and\nUAV-UAVs, UAVs' CPU frequencies, and mini-batch sizes subject to\ncommunication/computation network heterogeneity. We tackle this optimization\nproblem via the method of posynomial condensation and propose a distributed\nalgorithm with a performance guarantee. (b) Macro-trajectory and learning\nduration design, which we formulate as a sequential decision making problem,\ntackled via deep reinforcement learning. Our simulations demonstrate the\nsuperiority of our methodology with regards to the distributed ML performance,\nthe optimization of network resources, and the swarm trajectory efficiency.",
          "link": "http://arxiv.org/abs/2106.15734",
          "publishedOn": "2021-07-01T01:59:32.744Z",
          "wordCount": 656,
          "title": "UAV-assisted Online Machine Learning over Multi-Tiered Networks: A Hierarchical Nested Personalized Federated Learning Approach. (arXiv:2106.15734v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15762",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haifeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1\">Jun Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jiawei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1\">Qing Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1\">Guohua Wu</a>",
          "description": "Graph neural networks (GNNs) have achieved great success in many graph-based\ntasks. Much work is dedicated to empowering GNNs with the adaptive locality\nability, which enables measuring the importance of neighboring nodes to the\ntarget node by a node-specific mechanism. However, the current node-specific\nmechanisms are deficient in distinguishing the importance of nodes in the\ntopology structure. We believe that the structural importance of neighboring\nnodes is closely related to their importance in aggregation. In this paper, we\nintroduce discrete graph curvature (the Ricci curvature) to quantify the\nstrength of structural connection of pairwise nodes. And we propose Curvature\nGraph Neural Network (CGNN), which effectively improves the adaptive locality\nability of GNNs by leveraging the structural property of graph curvature. To\nimprove the adaptability of curvature to various datasets, we explicitly\ntransform curvature into the weights of neighboring nodes by the necessary\nNegative Curvature Processing Module and Curvature Normalization Module. Then,\nwe conduct numerous experiments on various synthetic datasets and real-world\ndatasets. The experimental results on synthetic datasets show that CGNN\neffectively exploits the topology structure information, and the performance is\nimproved significantly. CGNN outperforms the baselines on 5 dense node\nclassification benchmark datasets. This study deepens the understanding of how\nto utilize advanced topology information and assign the importance of\nneighboring nodes from the perspective of graph curvature and encourages us to\nbridge the gap between graph theory and neural networks.",
          "link": "http://arxiv.org/abs/2106.15762",
          "publishedOn": "2021-07-01T01:59:32.711Z",
          "wordCount": 668,
          "title": "Curvature Graph Neural Network. (arXiv:2106.15762v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15666",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Miller_J/0/1/0/all/0/1\">Jacob Miller</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Roeder_G/0/1/0/all/0/1\">Geoffrey Roeder</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Bradley_T/0/1/0/all/0/1\">Tai-Danae Bradley</a>",
          "description": "We investigate a correspondence between two formalisms for discrete\nprobabilistic modeling: probabilistic graphical models (PGMs) and tensor\nnetworks (TNs), a powerful modeling framework for simulating complex quantum\nsystems. The graphical calculus of PGMs and TNs exhibits many similarities,\nwith discrete undirected graphical models (UGMs) being a special case of TNs.\nHowever, more general probabilistic TN models such as Born machines (BMs)\nemploy complex-valued hidden states to produce novel forms of correlation among\nthe probabilities. While representing a new modeling resource for capturing\nstructure in discrete probability distributions, this behavior also renders the\ndirect application of standard PGM tools impossible. We aim to bridge this gap\nby introducing a hybrid PGM-TN formalism that integrates quantum-like\ncorrelations into PGM models in a principled manner, using the\nphysically-motivated concept of decoherence. We first prove that applying\ndecoherence to the entirety of a BM model converts it into a discrete UGM, and\nconversely, that any subgraph of a discrete UGM can be represented as a\ndecohered BM. This method allows a broad family of probabilistic TN models to\nbe encoded as partially decohered BMs, a fact we leverage to combine the\nrepresentational strengths of both model families. We experimentally verify the\nperformance of such hybrid models in a sequential modeling task, and identify\npromising uses of our method within the context of existing applications of\ngraphical models.",
          "link": "http://arxiv.org/abs/2106.15666",
          "publishedOn": "2021-07-01T01:59:32.688Z",
          "wordCount": 668,
          "title": "Probabilistic Graphical Models and Tensor Networks: A Hybrid Framework. (arXiv:2106.15666v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15716",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Scott_C/0/1/0/all/0/1\">Cory Braker Scott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mjolsness_E/0/1/0/all/0/1\">Eric Mjolsness</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oyen_D/0/1/0/all/0/1\">Diane Oyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kodera_C/0/1/0/all/0/1\">Chie Kodera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouchez_D/0/1/0/all/0/1\">David Bouchez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uyttewaal_M/0/1/0/all/0/1\">Magalie Uyttewaal</a>",
          "description": "We present a method for learning \"spectrally descriptive\" edge weights for\ngraphs. We generalize a previously known distance measure on graphs (Graph\nDiffusion Distance), thereby allowing it to be tuned to minimize an arbitrary\nloss function. Because all steps involved in calculating this modified GDD are\ndifferentiable, we demonstrate that it is possible for a small neural network\nmodel to learn edge weights which minimize loss. GDD alone does not effectively\ndiscriminate between graphs constructed from shoot apical meristem images of\nwild-type vs. mutant \\emph{Arabidopsis thaliana} specimens. However, training\nedge weights and kernel parameters with contrastive loss produces a learned\ndistance metric with large margins between these graph categories. We\ndemonstrate this by showing improved performance of a simple\nk-nearest-neighbors classifier on the learned distance matrix. We also\ndemonstrate a further application of this method to biological image analysis:\nonce trained, we use our model to compute the distance between the biological\ngraphs and a set of graphs output by a cell division simulator. This allows us\nto identify simulation parameter regimes which are similar to each class of\ngraph in our original dataset.",
          "link": "http://arxiv.org/abs/2106.15716",
          "publishedOn": "2021-07-01T01:59:32.553Z",
          "wordCount": 638,
          "title": "Diff2Dist: Learning Spectrally Distinct Edge Functions, with Applications to Cell Morphology Analysis. (arXiv:2106.15716v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15662",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Qiao_M/0/1/0/all/0/1\">Mingda Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valiant_G/0/1/0/all/0/1\">Gregory Valiant</a>",
          "description": "We study the selective learning problem introduced by Qiao and Valiant\n(2019), in which the learner observes $n$ labeled data points one at a time. At\na time of its choosing, the learner selects a window length $w$ and a model\n$\\hat\\ell$ from the model class $\\mathcal{L}$, and then labels the next $w$\ndata points using $\\hat\\ell$. The excess risk incurred by the learner is\ndefined as the difference between the average loss of $\\hat\\ell$ over those $w$\ndata points and the smallest possible average loss among all models in\n$\\mathcal{L}$ over those $w$ data points.\n\nWe give an improved algorithm, termed the hybrid exponential weights\nalgorithm, that achieves an expected excess risk of $O((\\log\\log|\\mathcal{L}| +\n\\log\\log n)/\\log n)$. This result gives a doubly exponential improvement in the\ndependence on $|\\mathcal{L}|$ over the best known bound of\n$O(\\sqrt{|\\mathcal{L}|/\\log n})$. We complement the positive result with an\nalmost matching lower bound, which suggests the worst-case optimality of the\nalgorithm.\n\nWe also study a more restrictive family of learning algorithms that are\nbounded-recall in the sense that when a prediction window of length $w$ is\nchosen, the learner's decision only depends on the most recent $w$ data points.\nWe analyze an exponential weights variant of the ERM algorithm in Qiao and\nValiant (2019). This new algorithm achieves an expected excess risk of\n$O(\\sqrt{\\log |\\mathcal{L}|/\\log n})$, which is shown to be nearly optimal\namong all bounded-recall learners. Our analysis builds on a generalized version\nof the selective mean prediction problem in Drucker (2013); Qiao and Valiant\n(2019), which may be of independent interest.",
          "link": "http://arxiv.org/abs/2106.15662",
          "publishedOn": "2021-07-01T01:59:32.547Z",
          "wordCount": 699,
          "title": "Exponential Weights Algorithms for Selective Learning. (arXiv:2106.15662v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15728",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiefeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Frederick Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avci_B/0/1/0/all/0/1\">Besim Avci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yingyu Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jha_S/0/1/0/all/0/1\">Somesh Jha</a>",
          "description": "When a deep learning model is deployed in the wild, it can encounter test\ndata drawn from distributions different from the training data distribution and\nsuffer drop in performance. For safe deployment, it is essential to estimate\nthe accuracy of the pre-trained model on the test data. However, the labels for\nthe test inputs are usually not immediately available in practice, and\nobtaining them can be expensive. This observation leads to two challenging\ntasks: (1) unsupervised accuracy estimation, which aims to estimate the\naccuracy of a pre-trained classifier on a set of unlabeled test inputs; (2)\nerror detection, which aims to identify mis-classified test inputs. In this\npaper, we propose a principled and practically effective framework that\nsimultaneously addresses the two tasks. The proposed framework iteratively\nlearns an ensemble of models to identify mis-classified data points and\nperforms self-training to improve the ensemble with the identified points.\nTheoretical analysis demonstrates that our framework enjoys provable guarantees\nfor both accuracy estimation and error detection under mild conditions readily\nsatisfied by practical deep learning models. Along with the framework, we\nproposed and experimented with two instantiations and achieved state-of-the-art\nresults on 59 tasks. For example, on iWildCam, one instantiation reduces the\nestimation error for unsupervised accuracy estimation by at least 70% and\nimproves the F1 score for error detection by at least 4.7% compared to existing\nmethods.",
          "link": "http://arxiv.org/abs/2106.15728",
          "publishedOn": "2021-07-01T01:59:32.505Z",
          "wordCount": 665,
          "title": "Detecting Errors and Estimating Accuracy on Unlabeled Data with Self-training Ensembles. (arXiv:2106.15728v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15691",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1\">Annie Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Back_T/0/1/0/all/0/1\">Thomas B&#xe4;ck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kononova_A/0/1/0/all/0/1\">Anna V. Kononova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plaat_A/0/1/0/all/0/1\">Aske Plaat</a>",
          "description": "This paper surveys the field of multiagent deep reinforcement learning. The\ncombination of deep neural networks with reinforcement learning has gained\nincreased traction in recent years and is slowly shifting the focus from\nsingle-agent to multiagent environments. Dealing with multiple agents is\ninherently more complex as (a) the future rewards depend on the joint actions\nof multiple players and (b) the computational complexity of functions\nincreases. We present the most common multiagent problem representations and\ntheir main challenges, and identify five research areas that address one or\nmore of these challenges: centralised training and decentralised execution,\nopponent modelling, communication, efficient coordination, and reward shaping.\nWe find that many computational studies rely on unrealistic assumptions or are\nnot generalisable to other settings; they struggle to overcome the curse of\ndimensionality or nonstationarity. Approaches from psychology and sociology\ncapture promising relevant behaviours such as communication and coordination.\nWe suggest that, for multiagent reinforcement learning to be successful, future\nresearch addresses these challenges with an interdisciplinary approach to open\nup new possibilities for more human-oriented solutions in multiagent\nreinforcement learning.",
          "link": "http://arxiv.org/abs/2106.15691",
          "publishedOn": "2021-07-01T01:59:32.499Z",
          "wordCount": 636,
          "title": "Multiagent Deep Reinforcement Learning: Challenges and Directions Towards Human-Like Approaches. (arXiv:2106.15691v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15674",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mahmoudi_S/0/1/0/all/0/1\">Seyyed Ehsan Mahmoudi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shamsfard_M/0/1/0/all/0/1\">Mehrnoush Shamsfard</a>",
          "description": "In recent years there has been a special interest in word embeddings as a new\napproach to convert words to vectors. It has been a focal point to understand\nhow much of the semantics of the the words has been transferred into embedding\nvectors. This is important as the embedding is going to be used as the basis\nfor downstream NLP applications and it will be costly to evaluate the\napplication end-to-end in order to identify quality of the used embedding\nmodel. Generally the word embeddings are evaluated through a number of tests,\nincluding analogy test. In this paper we propose a test framework for Persian\nembedding models. Persian is a low resource language and there is no rich\nsemantic benchmark to evaluate word embedding models for this language. In this\npaper we introduce an evaluation framework including a hand crafted Persian SAT\nbased analogy dataset, a colliquial test set (specific to Persian) and a\nbenchmark to study the impact of various parameters on the semantic evaluation\ntask.",
          "link": "http://arxiv.org/abs/2106.15674",
          "publishedOn": "2021-07-01T01:59:32.482Z",
          "wordCount": 607,
          "title": "SAT Based Analogy Evaluation Framework for Persian Word Embeddings. (arXiv:2106.15674v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2011.01619",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Long_Y/0/1/0/all/0/1\">Yonghao Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jie Ying Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_B/0/1/0/all/0/1\">Bo Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Yueming Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Unberath_M/0/1/0/all/0/1\">Mathias Unberath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yun-Hui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heng_P/0/1/0/all/0/1\">Pheng Ann Heng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_Q/0/1/0/all/0/1\">Qi Dou</a>",
          "description": "Automatic surgical gesture recognition is fundamentally important to enable\nintelligent cognitive assistance in robotic surgery. With recent advancement in\nrobot-assisted minimally invasive surgery, rich information including surgical\nvideos and robotic kinematics can be recorded, which provide complementary\nknowledge for understanding surgical gestures. However, existing methods either\nsolely adopt uni-modal data or directly concatenate multi-modal\nrepresentations, which can not sufficiently exploit the informative\ncorrelations inherent in visual and kinematics data to boost gesture\nrecognition accuracies. In this regard, we propose a novel online approach of\nmulti-modal relational graph network (i.e., MRG-Net) to dynamically integrate\nvisual and kinematics information through interactive message propagation in\nthe latent feature space. In specific, we first extract embeddings from video\nand kinematics sequences with temporal convolutional networks and LSTM units.\nNext, we identify multi-relations in these multi-modal embeddings and leverage\nthem through a hierarchical relational graph learning module. The effectiveness\nof our method is demonstrated with state-of-the-art results on the public\nJIGSAWS dataset, outperforming current uni-modal and multi-modal methods on\nboth suturing and knot typing tasks. Furthermore, we validated our method on\nin-house visual-kinematics datasets collected with da Vinci Research Kit (dVRK)\nplatforms in two centers, with consistent promising performance achieved.",
          "link": "http://arxiv.org/abs/2011.01619",
          "publishedOn": "2021-06-30T02:01:04.203Z",
          "wordCount": 701,
          "title": "Relational Graph Learning on Visual and Kinematics Embeddings for Accurate Gesture Recognition in Robotic Surgery. (arXiv:2011.01619v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.11802",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhanlin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldwasser_J/0/1/0/all/0/1\">Jeremy Goldwasser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tuckman_P/0/1/0/all/0/1\">Philip Tuckman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gerstein_M/0/1/0/all/0/1\">Mark Gerstein</a>",
          "description": "With the rise of single-cell sequencing technologies, there is a growing need\nfor robust clustering algorithms to extract deeper insights from data. Here, we\nintroduce an intuitive and efficient clustering method, Forest Fire Clustering,\nfor discovering and validating cell types in single-cell sequencing analysis.\nCompared to existing methods, our clustering algorithm makes minimum prior\nassumptions about the data distribution and can provide a point-wise\nsignificance value via Monte Carlo simulations for internal validation.\nAdditionally, point-wise label entropies can highlight novel transition cell\ntypes \\emph{de novo} along developmental pseudo-time manifolds. Lastly, our\ninductive algorithm has the ability to make robust inferences in an\nonline-learning context. In this paper, we describe the method, provide a\nsummary of its performance against common clustering benchmarks, and\ndemonstrate that Forest Fire Clustering is uniquely suitable for single-cell\nsequencing analysis.",
          "link": "http://arxiv.org/abs/2103.11802",
          "publishedOn": "2021-06-30T02:01:04.189Z",
          "wordCount": 625,
          "title": "Forest Fire Clustering: Iterative Label Propagation Clustering and Monte Carlo Validation For Single-cell Sequencing Analysis. (arXiv:2103.11802v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.06115",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Barenboim_G/0/1/0/all/0/1\">Gabriela Barenboim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hirn_J/0/1/0/all/0/1\">Johannes Hirn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanz_V/0/1/0/all/0/1\">Veronica Sanz</a>",
          "description": "We explore whether Neural Networks (NNs) can {\\it discover} the presence of\nsymmetries as they learn to perform a task. For this, we train hundreds of NNs\non a {\\it decoy task} based on well-controlled Physics templates, where no\ninformation on symmetry is provided. We use the output from the last hidden\nlayer of all these NNs, projected to fewer dimensions, as the input for a\nsymmetry classification task, and show that information on symmetry had indeed\nbeen identified by the original NN without guidance. As an interdisciplinary\napplication of this procedure, we identify the presence and level of symmetry\nin artistic paintings from different styles such as those of Picasso, Pollock\nand Van Gogh.",
          "link": "http://arxiv.org/abs/2103.06115",
          "publishedOn": "2021-06-30T02:01:04.183Z",
          "wordCount": 579,
          "title": "Symmetry meets AI. (arXiv:2103.06115v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2005.05535",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Perov_I/0/1/0/all/0/1\">Ivan Perov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_D/0/1/0/all/0/1\">Daiheng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chervoniy_N/0/1/0/all/0/1\">Nikolay Chervoniy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1\">Kunlin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marangonda_S/0/1/0/all/0/1\">Sugasa Marangonda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ume_C/0/1/0/all/0/1\">Chris Um&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dpfks_M/0/1/0/all/0/1\">Mr. Dpfks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Facenheim_C/0/1/0/all/0/1\">Carl Shift Facenheim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+RP_L/0/1/0/all/0/1\">Luis RP</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1\">Jian Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Sheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_P/0/1/0/all/0/1\">Pingyu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Bo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Weiming Zhang</a>",
          "description": "Deepfake defense not only requires the research of detection but also\nrequires the efforts of generation methods. However, current deepfake methods\nsuffer the effects of obscure workflow and poor performance. To solve this\nproblem, we present DeepFaceLab, the current dominant deepfake framework for\nface-swapping. It provides the necessary tools as well as an easy-to-use way to\nconduct high-quality face-swapping. It also offers a flexible and loose\ncoupling structure for people who need to strengthen their pipeline with other\nfeatures without writing complicated boilerplate code. We detail the principles\nthat drive the implementation of DeepFaceLab and introduce its pipeline,\nthrough which every aspect of the pipeline can be modified painlessly by users\nto achieve their customization purpose. It is noteworthy that DeepFaceLab could\nachieve cinema-quality results with high fidelity. We demonstrate the advantage\nof our system by comparing our approach with other face-swapping methods.For\nmore information, please visit:https://github.com/iperov/DeepFaceLab/.",
          "link": "http://arxiv.org/abs/2005.05535",
          "publishedOn": "2021-06-30T02:01:04.171Z",
          "wordCount": 674,
          "title": "DeepFaceLab: Integrated, flexible and extensible face-swapping framework. (arXiv:2005.05535v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1904.03936",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fatras_K/0/1/0/all/0/1\">Kilian Fatras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Damodaran_B/0/1/0/all/0/1\">Bharath Bhushan Damodaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lobry_S/0/1/0/all/0/1\">Sylvain Lobry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Flamary_R/0/1/0/all/0/1\">R&#xe9;mi Flamary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tuia_D/0/1/0/all/0/1\">Devis Tuia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Courty_N/0/1/0/all/0/1\">Nicolas Courty</a>",
          "description": "Noisy labels often occur in vision datasets, especially when they are\nobtained from crowdsourcing or Web scraping. We propose a new regularization\nmethod, which enables learning robust classifiers in presence of noisy data. To\nachieve this goal, we propose a new adversarial regularization scheme based on\nthe Wasserstein distance. Using this distance allows taking into account\nspecific relations between classes by leveraging the geometric properties of\nthe labels space. Our Wasserstein Adversarial Regularization (WAR) encodes a\nselective regularization, which promotes smoothness of the classifier between\nsome classes, while preserving sufficient complexity of the decision boundary\nbetween others. We first discuss how and why adversarial regularization can be\nused in the context of label noise and then show the effectiveness of our\nmethod on five datasets corrupted with noisy labels: in both benchmarks and\nreal datasets, WAR outperforms the state-of-the-art competitors.",
          "link": "http://arxiv.org/abs/1904.03936",
          "publishedOn": "2021-06-30T02:01:04.147Z",
          "wordCount": 638,
          "title": "Wasserstein Adversarial Regularization (WAR) on label noise. (arXiv:1904.03936v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.10362",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Spooner_T/0/1/0/all/0/1\">Thomas Spooner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vadori_N/0/1/0/all/0/1\">Nelson Vadori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganesh_S/0/1/0/all/0/1\">Sumitra Ganesh</a>",
          "description": "Policy gradient methods can solve complex tasks but often fail when the\ndimensionality of the action-space or objective multiplicity grow very large.\nThis occurs, in part, because the variance on score-based gradient estimators\nscales quadratically. In this paper, we address this problem through a causal\nbaseline which exploits independence structure encoded in a novel action-target\ninfluence network. Causal policy gradients (CPGs), which follow, provide a\ncommon framework for analysing key state-of-the-art algorithms, are shown to\ngeneralise traditional policy gradients, and yield a principled way of\nincorporating prior knowledge of a problem domain's generative processes. We\nprovide an analysis of the proposed estimator and identify the conditions under\nwhich variance is reduced. The algorithmic aspects of CPGs are discussed,\nincluding optimal policy factorisation, as characterised by minimum biclique\ncoverings, and the implications for the bias-variance trade-off of incorrectly\nspecifying the network. Finally, we demonstrate the performance advantages of\nour algorithm on large-scale bandit and traffic intersection problems,\nproviding a novel contribution to the latter in the form of a spatio-causal\napproximation.",
          "link": "http://arxiv.org/abs/2102.10362",
          "publishedOn": "2021-06-30T02:01:04.141Z",
          "wordCount": 648,
          "title": "Causal Policy Gradients: Leveraging Structure for Efficient Learning in (Factored) MOMDPs. (arXiv:2102.10362v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.13195",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Brooks_E/0/1/0/all/0/1\">Ethan A. Brooks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajendran_J/0/1/0/all/0/1\">Janarthanan Rajendran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_R/0/1/0/all/0/1\">Richard L. Lewis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Satinder Singh</a>",
          "description": "Learning to flexibly follow task instructions in dynamic environments poses\ninteresting challenges for reinforcement learning agents. We focus here on the\nproblem of learning control flow that deviates from a strict step-by-step\nexecution of instructions -- that is, control flow that may skip forward over\nparts of the instructions or return backward to previously completed or skipped\nsteps. Demand for such flexible control arises in two fundamental ways:\nexplicitly when control is specified in the instructions themselves (such as\nconditional branching and looping) and implicitly when stochastic environment\ndynamics require re-completion of instructions whose effects have been\nperturbed, or opportunistic skipping of instructions whose effects are already\npresent. We formulate an attention-based architecture that meets these\nchallenges by learning, from task reward only, to flexibly attend to and\ncondition behavior on an internal encoding of the instructions. We test the\narchitecture's ability to learn both explicit and implicit control in two\nillustrative domains -- one inspired by Minecraft and the other by StarCraft --\nand show that the architecture exhibits zero-shot generalization to novel\ninstructions of length greater than those in a training set, at a performance\nlevel unmatched by two baseline recurrent architectures and one ablation\narchitecture.",
          "link": "http://arxiv.org/abs/2102.13195",
          "publishedOn": "2021-06-30T02:01:04.136Z",
          "wordCount": 662,
          "title": "Reinforcement Learning of Implicit and Explicit Control Flow in Instructions. (arXiv:2102.13195v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2003.13607",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Jin_Q/0/1/0/all/0/1\">Qiujiang Jin</a>, <a href=\"http://arxiv.org/find/math/1/au:+Mokhtari_A/0/1/0/all/0/1\">Aryan Mokhtari</a>",
          "description": "In this paper, we study and prove the non-asymptotic superlinear convergence\nrate of the Broyden class of quasi-Newton methods including\nDavidon--Fletcher--Powell (DFP) method and Broyden--Fletcher--Goldfarb--Shanno\n(BFGS) method. The asymptotic superlinear convergence rate of these\nquasi-Newton methods has been extensively studied, but their explicit finite\ntime local convergence rate is not fully investigated. In this paper, we\nprovide a finite time (non-asymptotic) convergence analysis for BFGS and DFP\nmethods under the assumptions that the objective function is strongly convex,\nits gradient is Lipschitz continuous, and its Hessian is Lipschitz continuous\nonly in the direction of the optimal solution. We show that in a local\nneighborhood of the optimal solution, the iterates generated by both DFP and\nBFGS converge to the optimal solution at a superlinear rate of $(1/k)^{k/2}$,\nwhere $k$ is the number of iterations. We also prove the same local superlinear\nconvergence rate in the case that the objective function is self-concordant.\nNumerical experiments on different objective functions confirm our explicit\nconvergence rates. Our theoretical guarantee is one of the first results that\nprovide a non-asymptotic superlinear convergence rate for DFP and BFGS\nquasi-Newton methods.",
          "link": "http://arxiv.org/abs/2003.13607",
          "publishedOn": "2021-06-30T02:01:04.131Z",
          "wordCount": 641,
          "title": "Non-asymptotic Superlinear Convergence of Standard Quasi-Newton Methods. (arXiv:2003.13607v3 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.07868",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Achituve_I/0/1/0/all/0/1\">Idan Achituve</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navon_A/0/1/0/all/0/1\">Aviv Navon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yemini_Y/0/1/0/all/0/1\">Yochai Yemini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chechik_G/0/1/0/all/0/1\">Gal Chechik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fetaya_E/0/1/0/all/0/1\">Ethan Fetaya</a>",
          "description": "Gaussian processes (GPs) are non-parametric, flexible, models that work well\nin many tasks. Combining GPs with deep learning methods via deep kernel\nlearning (DKL) is especially compelling due to the strong representational\npower induced by the network. However, inference in GPs, whether with or\nwithout DKL, can be computationally challenging on large datasets. Here, we\npropose GP-Tree, a novel method for multi-class classification with Gaussian\nprocesses and DKL. We develop a tree-based hierarchical model in which each\ninternal node of the tree fits a GP to the data using the P\\'olya Gamma\naugmentation scheme. As a result, our method scales well with both the number\nof classes and data size. We demonstrate the effectiveness of our method\nagainst other Gaussian process training baselines, and we show how our general\nGP approach achieves improved accuracy on standard incremental few-shot\nlearning benchmarks.",
          "link": "http://arxiv.org/abs/2102.07868",
          "publishedOn": "2021-06-30T02:01:04.126Z",
          "wordCount": 609,
          "title": "GP-Tree: A Gaussian Process Classifier for Few-Shot Incremental Learning. (arXiv:2102.07868v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.06695",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Potapczynski_A/0/1/0/all/0/1\">Andres Potapczynski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Luhuan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Biderman_D/0/1/0/all/0/1\">Dan Biderman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pleiss_G/0/1/0/all/0/1\">Geoff Pleiss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cunningham_J/0/1/0/all/0/1\">John P. Cunningham</a>",
          "description": "Scalable Gaussian Process methods are computationally attractive, yet\nintroduce modeling biases that require rigorous study. This paper analyzes two\ncommon techniques: early truncated conjugate gradients (CG) and random Fourier\nfeatures (RFF). We find that both methods introduce a systematic bias on the\nlearned hyperparameters: CG tends to underfit while RFF tends to overfit. We\naddress these issues using randomized truncation estimators that eliminate bias\nin exchange for increased variance. In the case of RFF, we show that the\nbias-to-variance conversion is indeed a trade-off: the additional variance\nproves detrimental to optimization. However, in the case of CG, our unbiased\nlearning procedure meaningfully outperforms its biased counterpart with minimal\nadditional computation.",
          "link": "http://arxiv.org/abs/2102.06695",
          "publishedOn": "2021-06-30T02:01:04.110Z",
          "wordCount": 584,
          "title": "Bias-Free Scalable Gaussian Processes via Randomized Truncations. (arXiv:2102.06695v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.12238",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jagadeesan_M/0/1/0/all/0/1\">Meena Jagadeesan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Razenshteyn_I/0/1/0/all/0/1\">Ilya Razenshteyn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gunasekar_S/0/1/0/all/0/1\">Suriya Gunasekar</a>",
          "description": "We study the function space characterization of the inductive bias resulting\nfrom controlling the $\\ell_2$ norm of the weights in linear convolutional\nnetworks. We view this in terms of an induced regularizer in the function space\ngiven by the minimum norm of weights required to realize a linear function. For\ntwo layer linear convolutional networks with $C$ output channels and kernel\nsize $K$, we show the following: (a) If the inputs to the network have a single\nchannel, the induced regularizer for any $K$ is a norm given by a semidefinite\nprogram (SDP) that is independent of the number of output channels $C$. (b) In\ncontrast, for networks with multi-channel inputs, multiple output channels can\nbe necessary to merely realize all matrix-valued linear functions and thus the\ninductive bias does depend on $C$. Further, for sufficiently large $C$, the\ninduced regularizer for $K=1$ and $K=D$ are the nuclear norm and the\n$\\ell_{2,1}$ group-sparse norm, respectively, of the Fourier coefficients. (c)\nComplementing our theoretical results, we show through experiments on MNIST and\nCIFAR-10 that our key findings extend to implicit biases from gradient descent\nin overparameterized networks.",
          "link": "http://arxiv.org/abs/2102.12238",
          "publishedOn": "2021-06-30T02:01:04.105Z",
          "wordCount": 662,
          "title": "Inductive Bias of Multi-Channel Linear Convolutional Networks with Bounded Weight Norm. (arXiv:2102.12238v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.05390",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kratsios_A/0/1/0/all/0/1\">Anastasis Kratsios</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papon_L/0/1/0/all/0/1\">Leonie Papon</a>",
          "description": "This paper addresses the growing need to process non-Euclidean data, by\nintroducing a geometric deep learning (GDL) framework for building universal\nfeedforward-type models compatible with differentiable manifold geometries. We\nshow that our GDL models can approximate any continuous target function\nuniformly on compacts of a controlled maximum diameter. We obtain curvature\ndependant lower-bounds on this maximum diameter and upper-bounds on the depth\nof our approximating GDL models. Conversely, we find that there is always a\ncontinuous function between any two non-degenerate compact manifolds that any\n\"locally-defined\" GDL model cannot uniformly approximate. Our last main result\nidentifies data-dependent conditions guaranteeing that the GDL model\nimplementing our approximation breaks \"the curse of dimensionality.\" We find\nthat any \"real-world\" (i.e. finite) dataset always satisfies our condition and,\nconversely, any dataset satisfies our requirement if the target function is\nsmooth. As applications, we confirm the universal approximation capabilities of\nthe following GDL models: Ganea et al. (2018)'s hyperbolic feedforward\nnetworks, the architecture implementing Krishnan et al. (2015)'s deep\nKalman-Filter, and deep softmax classifiers. We build universal\nextensions/variants of: the SPD-matrix regressor of Meyer et al. (2011), and\nFletcher et al. (2009)'s Procrustean regressor. In the Euclidean setting, our\nresults imply a quantitative version of Kidger and Lyons (2020)'s approximation\ntheorem and a data-dependent version of Yarotsky and Zhevnerchuk (2020)'s\nuncursed approximation rates.",
          "link": "http://arxiv.org/abs/2101.05390",
          "publishedOn": "2021-06-30T02:01:04.099Z",
          "wordCount": 734,
          "title": "Universal Approximation Theorems for Differentiable Geometric Deep Learning. (arXiv:2101.05390v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.11600",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kwon_J/0/1/0/all/0/1\">Jungmin Kwon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jeongseop Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_H/0/1/0/all/0/1\">Hyunseo Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_I/0/1/0/all/0/1\">In Kwon Choi</a>",
          "description": "Recently, learning algorithms motivated from sharpness of loss surface as an\neffective measure of generalization gap have shown state-of-the-art\nperformances. Nevertheless, sharpness defined in a rigid region with a fixed\nradius, has a drawback in sensitivity to parameter re-scaling which leaves the\nloss unaffected, leading to weakening of the connection between sharpness and\ngeneralization gap. In this paper, we introduce the concept of adaptive\nsharpness which is scale-invariant and propose the corresponding generalization\nbound. We suggest a novel learning method, adaptive sharpness-aware\nminimization (ASAM), utilizing the proposed generalization bound. Experimental\nresults in various benchmark datasets show that ASAM contributes to significant\nimprovement of model generalization performance.",
          "link": "http://arxiv.org/abs/2102.11600",
          "publishedOn": "2021-06-30T02:01:04.094Z",
          "wordCount": 593,
          "title": "ASAM: Adaptive Sharpness-Aware Minimization for Scale-Invariant Learning of Deep Neural Networks. (arXiv:2102.11600v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.08454",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Hussein_A/0/1/0/all/0/1\">Amir Hussein</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ali_A/0/1/0/all/0/1\">Ahmed Ali</a>",
          "description": "Recent advances in automatic speech recognition (ASR) have achieved accuracy\nlevels comparable to human transcribers, which led researchers to debate if the\nmachine has reached human performance. Previous work focused on the English\nlanguage and modular hidden Markov model-deep neural network (HMM-DNN) systems.\nIn this paper, we perform a comprehensive benchmarking for end-to-end\ntransformer ASR, modular HMM-DNN ASR, and human speech recognition (HSR) on the\nArabic language and its dialects. For the HSR, we evaluate linguist performance\nand lay-native speaker performance on a new dataset collected as a part of this\nstudy. For ASR the end-to-end work led to 12.5%, 27.5%, 33.8% WER; a new\nperformance milestone for the MGB2, MGB3, and MGB5 challenges respectively. Our\nresults suggest that human performance in the Arabic language is still\nconsiderably better than the machine with an absolute WER gap of 3.5% on\naverage.",
          "link": "http://arxiv.org/abs/2101.08454",
          "publishedOn": "2021-06-30T02:01:04.082Z",
          "wordCount": 608,
          "title": "Arabic Speech Recognition by End-to-End, Modular Systems and Human. (arXiv:2101.08454v2 [eess.AS] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.08663",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Visschers_J/0/1/0/all/0/1\">Jim C. Visschers</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Budker_D/0/1/0/all/0/1\">Dmitry Budker</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bougas_L/0/1/0/all/0/1\">Lykourgos Bougas</a>",
          "description": "In this work we demonstrate the use of neural networks for rapid extraction\nof signal parameters of discretely sampled signals. In particular, we use dense\nautoencoder networks to extract the parameters of interest from exponentially\ndecaying signals and decaying oscillations. By using a three-stage training\nmethod and careful choice of the neural network size, we are able to retrieve\nthe relevant signal parameters directly from the latent space of the\nautoencoder network at significantly improved rates compared to traditional\nalgorithmic signal-analysis approaches. We show that the achievable precision\nand accuracy of this method of analysis is similar to conventional\nalgorithm-based signal analysis methods, by demonstrating that the extracted\nsignal parameters are approaching their fundamental parameter estimation limit\nas provided by the Cram\\'er-Rao bound. Furthermore, we demonstrate that\nautoencoder networks are able to achieve signal analysis, and, hence, parameter\nextraction, at rates of 75 kHz, orders-of-magnitude faster than conventional\ntechniques with similar precision. Finally, we explore the limitations of our\napproach, demonstrating that analysis rates of $>$200 kHz are feasible with\nfurther optimization of the transfer rate between the data-acquisition system\nand data-analysis system.",
          "link": "http://arxiv.org/abs/2103.08663",
          "publishedOn": "2021-06-30T02:01:04.077Z",
          "wordCount": 658,
          "title": "Rapid parameter estimation of discrete decaying signals using autoencoder networks. (arXiv:2103.08663v2 [eess.SP] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1912.07702",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Lan_G/0/1/0/all/0/1\">Guanghui Lan</a>",
          "description": "Stochastic dual dynamic programming is a cutting plane type algorithm for\nmulti-stage stochastic optimization originated about 30 years ago. In spite of\nits popularity in practice, there does not exist any analysis on the\nconvergence rates of this method. In this paper, we first establish the number\nof iterations, i.e., iteration complexity, required by a basic dynamic cutting\nplane method for solving relatively simple multi-stage optimization problems,\nby introducing novel mathematical tools including the saturation of search\npoints. We then refine these basic tools and establish the iteration complexity\nfor both deterministic and stochastic dual dynamic programming methods for\nsolving more general multi-stage stochastic optimization problems under the\nstandard stage-wise independence assumption. Our results indicate that the\ncomplexity of these methods mildly increases with the number of stages $T$, in\nfact linearly dependent on $T$ for discounted problems. Therefore, they are\nefficient for strategic decision making which involves a large number of\nstages, but with a relatively small number of decision variables in each stage.\nWithout explicitly discretizing the state and action spaces, these methods\nmight also be pertinent to the related reinforcement learning and stochastic\ncontrol areas.",
          "link": "http://arxiv.org/abs/1912.07702",
          "publishedOn": "2021-06-30T02:01:04.061Z",
          "wordCount": 668,
          "title": "Complexity of Stochastic Dual Dynamic Programming. (arXiv:1912.07702v6 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.02316",
          "author": "<a href=\"http://arxiv.org/find/astro-ph/1/au:+Kong_Z/0/1/0/all/0/1\">Zhihui Kong</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Jiang_J/0/1/0/all/0/1\">Jonathan H. Jiang</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Zhu_Z/0/1/0/all/0/1\">Zong-Hong Zhu</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Fahy_K/0/1/0/all/0/1\">Kristen A. Fahy</a>, <a href=\"http://arxiv.org/find/astro-ph/1/au:+Burn_R/0/1/0/all/0/1\">Remo Burn</a>",
          "description": "Exoplanet detection in the past decade by efforts including NASA's Kepler and\nTESS missions has discovered many worlds that differ substantially from planets\nin our own Solar system, including more than 400 exoplanets orbiting binary or\nmulti-star systems. This not only broadens our understanding of the diversity\nof exoplanets, but also promotes our study of exoplanets in the complex binary\nand multi-star systems and provides motivation to explore their habitability.\nIn this study, we analyze orbital stability of exoplanets in non-coplanar\ncircumbinary systems using a numerical simulation method, with which a large\nnumber of circumbinary planet samples are generated in order to quantify the\neffects of various orbital parameters on orbital stability. We also train a\nmachine learning model that can quickly determine the stability of the\ncircumbinary planetary systems. Our results indicate that larger inclinations\nof the planet tend to increase the stability of its orbit, but change in the\nplanet's mass range between Earth and Jupiter has little effect on the\nstability of the system. In addition, we find that Deep Neural Networks (DNNs)\nhave higher accuracy and precision than other machine learning algorithms.",
          "link": "http://arxiv.org/abs/2101.02316",
          "publishedOn": "2021-06-30T02:01:04.056Z",
          "wordCount": 661,
          "title": "Analyzing the Stability of Non-coplanar Circumbinary Planets using Machine Learning. (arXiv:2101.02316v2 [astro-ph.EP] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15566",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Charikar_M/0/1/0/all/0/1\">Moses Charikar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_L/0/1/0/all/0/1\">Lunjia Hu</a>",
          "description": "Many clustering algorithms are guided by certain cost functions such as the\nwidely-used $k$-means cost. These algorithms divide data points into clusters\nwith often complicated boundaries, creating difficulties in explaining the\nclustering decision. In a recent work, Dasgupta, Frost, Moshkovitz, and\nRashtchian (ICML'20) introduced explainable clustering, where the cluster\nboundaries are axis-parallel hyperplanes and the clustering is obtained by\napplying a decision tree to the data. The central question here is: how much\ndoes the explainability constraint increase the value of the cost function?\n\nGiven $d$-dimensional data points, we show an efficient algorithm that finds\nan explainable clustering whose $k$-means cost is at most $k^{1 -\n2/d}\\mathrm{poly}(d\\log k)$ times the minimum cost achievable by a clustering\nwithout the explainability constraint, assuming $k,d\\ge 2$. Combining this with\nan independent work by Makarychev and Shan (ICML'21), we get an improved bound\nof $k^{1 - 2/d}\\mathrm{polylog}(k)$, which we show is optimal for every choice\nof $k,d\\ge 2$ up to a poly-logarithmic factor in $k$. For $d = 2$ in\nparticular, we show an $O(\\log k\\log\\log k)$ bound, improving exponentially\nover the previous best bound of $\\widetilde O(k)$.",
          "link": "http://arxiv.org/abs/2106.15566",
          "publishedOn": "2021-06-30T02:01:04.050Z",
          "wordCount": 624,
          "title": "Near-Optimal Explainable $k$-Means for All Dimensions. (arXiv:2106.15566v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2101.12365",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Siegel_J/0/1/0/all/0/1\">Jonathan W. Siegel</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Xu_J/0/1/0/all/0/1\">Jinchao Xu</a>",
          "description": "This article addresses the problem of approximating a function in a Hilbert\nspace by an expansion over a dictionary $\\mathbb{D}$. We introduce the notion\nof a smoothly parameterized dictionary and give upper bounds on the\napproximation rates, metric entropy and $n$-widths of the absolute convex hull,\nwhich we denote $B_1(\\mathbb{D})$, of such dictionaries. The upper bounds\ndepend upon the order of smoothness of the parameterization, and improve upon\nexisting results in many cases. The main applications of these results is to\nthe dictionaries $\\mathbb{D} = \\{\\sigma(\\omega\\cdot x + b)\\}\\subset L^2$\ncorresponding to shallow neural networks with activation function $\\sigma$, and\nto the dictionary of decaying Fourier modes corresponding to the spectral\nBarron space. This improves upon existing approximation rates for shallow\nneural networks when $\\sigma = \\text{ReLU}^k$ for $k\\geq 2$, sharpens bounds on\nthe metric entropy, and provides the first bounds on the Gelfand $n$-widths of\nthe Barron space and spectral Barron space.",
          "link": "http://arxiv.org/abs/2101.12365",
          "publishedOn": "2021-06-30T02:01:04.045Z",
          "wordCount": 652,
          "title": "Improved Approximation Properties of Dictionaries and Applications to Neural Networks. (arXiv:2101.12365v6 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.05588",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Krause_S/0/1/0/all/0/1\">Stefanie Krause</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Otto_O/0/1/0/all/0/1\">Oliver Otto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stolzenburg_F/0/1/0/all/0/1\">Frieder Stolzenburg</a>",
          "description": "Recurrent neural networks are a powerful means in diverse applications. We\nshow that, together with so-called conceptors, they also allow fast learning,\nin contrast to other deep learning methods. In addition, a relatively small\nnumber of examples suffices to train neural networks with high accuracy. We\ndemonstrate this with two applications, namely speech recognition and detecting\ncar driving maneuvers. We improve the state of the art by application-specific\npreparation techniques: For speech recognition, we use mel frequency cepstral\ncoefficients leading to a compact representation of the frequency spectra, and\ndetecting car driving maneuvers can be done without the commonly used\npolynomial interpolation, as our evaluation suggests.",
          "link": "http://arxiv.org/abs/2102.05588",
          "publishedOn": "2021-06-30T02:01:04.040Z",
          "wordCount": 628,
          "title": "Fast Classification Learning with Neural Networks and Conceptors for Speech Recognition and Car Driving Maneuvers. (arXiv:2102.05588v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.02051",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Esfandiari_Y/0/1/0/all/0/1\">Yasaman Esfandiari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_S/0/1/0/all/0/1\">Sin Yong Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zhanhong Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balu_A/0/1/0/all/0/1\">Aditya Balu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herron_E/0/1/0/all/0/1\">Ethan Herron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hegde_C/0/1/0/all/0/1\">Chinmay Hegde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarkar_S/0/1/0/all/0/1\">Soumik Sarkar</a>",
          "description": "Decentralized learning enables a group of collaborative agents to learn\nmodels using a distributed dataset without the need for a central parameter\nserver. Recently, decentralized learning algorithms have demonstrated\nstate-of-the-art results on benchmark data sets, comparable with centralized\nalgorithms. However, the key assumption to achieve competitive performance is\nthat the data is independently and identically distributed (IID) among the\nagents which, in real-life applications, is often not applicable. Inspired by\nideas from continual learning, we propose Cross-Gradient Aggregation (CGA), a\nnovel decentralized learning algorithm where (i) each agent aggregates\ncross-gradient information, i.e., derivatives of its model with respect to its\nneighbors' datasets, and (ii) updates its model using a projected gradient\nbased on quadratic programming (QP). We theoretically analyze the convergence\ncharacteristics of CGA and demonstrate its efficiency on non-IID data\ndistributions sampled from the MNIST and CIFAR-10 datasets. Our empirical\ncomparisons show superior learning performance of CGA over existing\nstate-of-the-art decentralized learning algorithms, as well as maintaining the\nimproved performance under information compression to reduce peer-to-peer\ncommunication overhead. The code is available here on GitHub.",
          "link": "http://arxiv.org/abs/2103.02051",
          "publishedOn": "2021-06-30T02:01:04.027Z",
          "wordCount": 654,
          "title": "Cross-Gradient Aggregation for Decentralized Learning from Non-IID data. (arXiv:2103.02051v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.04462",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lounici_K/0/1/0/all/0/1\">Karim Lounici</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meziani_K/0/1/0/all/0/1\">Katia Meziani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riu_B/0/1/0/all/0/1\">Benjamin Riu</a>",
          "description": "Deep Learning (DL) is considered the state-of-the-art in computer vision,\nspeech recognition and natural language processing. Until recently, it was also\nwidely accepted that DL is irrelevant for learning tasks on tabular data,\nespecially in the small sample regime where ensemble methods are acknowledged\nas the gold standard. We present a new end-to-end differentiable method to\ntrain a standard FFNN. Our method, \\textbf{Muddling labels for Regularization}\n(\\texttt{MLR}), penalizes memorization through the generation of uninformative\nlabels and the application of a differentiable close-form regularization scheme\non the last hidden layer during training. \\texttt{MLR} outperforms classical NN\nand the gold standard (GBDT, RF) for regression and classification tasks on\nseveral datasets from the UCI database and Kaggle covering a large range of\nsample sizes and feature to sample ratios. Researchers and practitioners can\nuse \\texttt{MLR} on its own as an off-the-shelf \\DL{} solution or integrate it\ninto the most advanced ML pipelines.",
          "link": "http://arxiv.org/abs/2106.04462",
          "publishedOn": "2021-06-30T02:01:04.021Z",
          "wordCount": 606,
          "title": "Muddling Label Regularization: Deep Learning for Tabular Datasets. (arXiv:2106.04462v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.08977",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Huang_A/0/1/0/all/0/1\">Audrey Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leqi_L/0/1/0/all/0/1\">Liu Leqi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lipton_Z/0/1/0/all/0/1\">Zachary C. Lipton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azizzadenesheli_K/0/1/0/all/0/1\">Kamyar Azizzadenesheli</a>",
          "description": "Even when unable to run experiments, practitioners can evaluate prospective\npolicies, using previously logged data. However, while the bandits literature\nhas adopted a diverse set of objectives, most research on off-policy evaluation\nto date focuses on the expected reward. In this paper, we introduce Lipschitz\nrisk functionals, a broad class of objectives that subsumes conditional\nvalue-at-risk (CVaR), variance, mean-variance, many distorted risks, and CPT\nrisks, among others. We propose Off-Policy Risk Assessment (OPRA), a framework\nthat first estimates a target policy's CDF and then generates plugin estimates\nfor any collection of Lipschitz risks, providing finite sample guarantees that\nhold simultaneously over the entire class. We instantiate OPRA with both\nimportance sampling and doubly robust estimators. Our primary theoretical\ncontributions are (i) the first uniform concentration inequalities for both CDF\nestimators in contextual bandits and (ii) error bounds on our Lipschitz risk\nestimates, which all converge at a rate of $O(1/\\sqrt{n})$.",
          "link": "http://arxiv.org/abs/2104.08977",
          "publishedOn": "2021-06-30T02:01:04.015Z",
          "wordCount": 611,
          "title": "Off-Policy Risk Assessment in Contextual Bandits. (arXiv:2104.08977v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15610",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gabbay_A/0/1/0/all/0/1\">Aviv Gabbay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_N/0/1/0/all/0/1\">Niv Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoshen_Y/0/1/0/all/0/1\">Yedid Hoshen</a>",
          "description": "Unsupervised disentanglement has been shown to be theoretically impossible\nwithout inductive biases on the models and the data. As an alternative\napproach, recent methods rely on limited supervision to disentangle the factors\nof variation and allow their identifiability. While annotating the true\ngenerative factors is only required for a limited number of observations, we\nargue that it is infeasible to enumerate all the factors of variation that\ndescribe a real-world image distribution. To this end, we propose a method for\ndisentangling a set of factors which are only partially labeled, as well as\nseparating the complementary set of residual factors that are never explicitly\nspecified. Our success in this challenging setting, demonstrated on synthetic\nbenchmarks, gives rise to leveraging off-the-shelf image descriptors to\npartially annotate a subset of attributes in real image domains (e.g. of human\nfaces) with minimal manual effort. Specifically, we use a recent language-image\nembedding model (CLIP) to annotate a set of attributes of interest in a\nzero-shot manner and demonstrate state-of-the-art disentangled image\nmanipulation results.",
          "link": "http://arxiv.org/abs/2106.15610",
          "publishedOn": "2021-06-30T02:01:04.009Z",
          "wordCount": 625,
          "title": "An Image is Worth More Than a Thousand Words: Towards Disentanglement in the Wild. (arXiv:2106.15610v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15546",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ravichandran_N/0/1/0/all/0/1\">Naresh Balaji Ravichandran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lansner_A/0/1/0/all/0/1\">Anders Lansner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herman_P/0/1/0/all/0/1\">Pawel Herman</a>",
          "description": "Learning internal representations from data using no or few labels is useful\nfor machine learning research, as it allows using massive amounts of unlabeled\ndata. In this work, we use the Bayesian Confidence Propagation Neural Network\n(BCPNN) model developed as a biologically plausible model of the cortex. Recent\nwork has demonstrated that these networks can learn useful internal\nrepresentations from data using local Bayesian-Hebbian learning rules. In this\nwork, we show how such representations can be leveraged in a semi-supervised\nsetting by introducing and comparing different classifiers. We also evaluate\nand compare such networks with other popular semi-supervised classifiers.",
          "link": "http://arxiv.org/abs/2106.15546",
          "publishedOn": "2021-06-30T02:01:04.003Z",
          "wordCount": 533,
          "title": "Semi-supervised learning with Bayesian Confidence Propagation Neural Network. (arXiv:2106.15546v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2103.09576",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Saem Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_D/0/1/0/all/0/1\">Donghoon Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwak_N/0/1/0/all/0/1\">Nojun Kwak</a>",
          "description": "Video frame interpolation is the task of creating an interframe between two\nadjacent frames along the time axis. So, instead of simply averaging two\nadjacent frames to create an intermediate image, this operation should maintain\nsemantic continuity with the adjacent frames. Most conventional methods use\noptical flow, and various tools such as occlusion handling and object smoothing\nare indispensable. Since the use of these various tools leads to complex\nproblems, we tried to tackle the video interframe generation problem without\nusing problematic optical flow . To enable this , we have tried to use a deep\nneural network with an invertible structure, and developed an U-Net based\nGenerative Flow which is a modified normalizing flow. In addition, we propose a\nlearning method with a new consistency loss in the latent space to maintain\nsemantic temporal consistency between frames. The resolution of the generated\nimage is guaranteed to be identical to that of the original images by using an\ninvertible network. Furthermore, as it is not a random image like the ones by\ngenerative models, our network guarantees stable outputs without flicker.\nThrough experiments, we \\sam {confirmed the feasibility of the proposed\nalgorithm and would like to suggest the U-Net based Generative Flow as a new\npossibility for baseline in video frame interpolation. This paper is meaningful\nin that it is the world's first attempt to use invertible networks instead of\noptical flows for video interpolation.",
          "link": "http://arxiv.org/abs/2103.09576",
          "publishedOn": "2021-06-30T02:01:03.991Z",
          "wordCount": 718,
          "title": "The U-Net based GLOW for Optical-Flow-free Video Interframe Generation. (arXiv:2103.09576v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.07810",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jing_B/0/1/0/all/0/1\">Baoyu Jing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_C/0/1/0/all/0/1\">Chanyoung Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_H/0/1/0/all/0/1\">Hanghang Tong</a>",
          "description": "Networks have been widely used to represent the relations between objects\nsuch as academic networks and social networks, and learning embedding for\nnetworks has thus garnered plenty of research attention. Self-supervised\nnetwork representation learning aims at extracting node embedding without\nexternal supervision. Recently, maximizing the mutual information between the\nlocal node embedding and the global summary (e.g. Deep Graph Infomax, or DGI\nfor short) has shown promising results on many downstream tasks such as node\nclassification. However, there are two major limitations of DGI. Firstly, DGI\nmerely considers the extrinsic supervision signal (i.e., the mutual information\nbetween node embedding and global summary) while ignores the intrinsic signal\n(i.e., the mutual dependence between node embedding and node attributes).\nSecondly, nodes in a real-world network are usually connected by multiple edges\nwith different relations, while DGI does not fully explore the various\nrelations among nodes. To address the above-mentioned problems, we propose a\nnovel framework, called High-order Deep Multiplex Infomax (HDMI), for learning\nnode embedding on multiplex networks in a self-supervised way. To be more\nspecific, we first design a joint supervision signal containing both extrinsic\nand intrinsic mutual information by high-order mutual information, and we\npropose a High-order Deep Infomax (HDI) to optimize the proposed supervision\nsignal. Then we propose an attention based fusion module to combine node\nembedding from different layers of the multiplex network. Finally, we evaluate\nthe proposed HDMI on various downstream tasks such as unsupervised clustering\nand supervised classification. The experimental results show that HDMI achieves\nstate-of-the-art performance on these tasks.",
          "link": "http://arxiv.org/abs/2102.07810",
          "publishedOn": "2021-06-30T02:01:03.985Z",
          "wordCount": 751,
          "title": "HDMI: High-order Deep Multiplex Infomax. (arXiv:2102.07810v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.02685",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Ganassali_L/0/1/0/all/0/1\">Luca Ganassali</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Massoulie_L/0/1/0/all/0/1\">Laurent Massouli&#xe9;</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lelarge_M/0/1/0/all/0/1\">Marc Lelarge</a>",
          "description": "Random graph alignment refers to recovering the underlying vertex\ncorrespondence between two random graphs with correlated edges. This can be\nviewed as an average-case and noisy version of the well-known graph isomorphism\nproblem. For the correlated Erd\\\"os-R\\'enyi model, we prove an impossibility\nresult for partial recovery in the sparse regime, with constant average degree\nand correlation, as well as a general bound on the maximal reachable overlap.\nOur bound is tight in the noiseless case (the graph isomorphism problem) and we\nconjecture that it is still tight with noise. Our proof technique relies on a\ncareful application of the probabilistic method to build automorphisms between\ntree components of a subcritical Erd\\\"os-R\\'enyi graph.",
          "link": "http://arxiv.org/abs/2102.02685",
          "publishedOn": "2021-06-30T02:01:03.980Z",
          "wordCount": 582,
          "title": "Impossibility of Partial Recovery in the Graph Alignment Problem. (arXiv:2102.02685v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1908.05715",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Olshevsky_V/0/1/0/all/0/1\">Vyacheslav Olshevsky</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Khotyaintsev_Y/0/1/0/all/0/1\">Yuri V. Khotyaintsev</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Lalti_A/0/1/0/all/0/1\">Ahmad Lalti</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Divin_A/0/1/0/all/0/1\">Andrey Divin</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Delzanno_G/0/1/0/all/0/1\">Gian Luca Delzanno</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Anderzen_S/0/1/0/all/0/1\">Sven Anderzen</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Herman_P/0/1/0/all/0/1\">Pawel Herman</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Chien_S/0/1/0/all/0/1\">Steven W.D. Chien</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Avanov_L/0/1/0/all/0/1\">Levon Avanov</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Dimmock_A/0/1/0/all/0/1\">Andrew P. Dimmock</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Markidis_S/0/1/0/all/0/1\">Stefano Markidis</a>",
          "description": "We investigate the properties of the ion sky maps produced by the Dual Ion\nSpectrometers (DIS) from the Fast Plasma Investigation (FPI). We have trained a\nconvolutional neural network classifier to predict four regions crossed by the\nMMS on the dayside magnetosphere: solar wind, ion foreshock, magnetosheath, and\nmagnetopause using solely DIS spectrograms. The accuracy of the classifier is\n>98%. We use the classifier to detect mixed plasma regions, in particular to\nfind the bow shock regions. A similar approach can be used to identify the\nmagnetopause crossings and reveal regions prone to magnetic reconnection. Data\nprocessing through the trained classifier is fast and efficient and thus can be\nused for classification for the whole MMS database.",
          "link": "http://arxiv.org/abs/1908.05715",
          "publishedOn": "2021-06-30T02:01:03.975Z",
          "wordCount": 612,
          "title": "Automated classification of plasma regions using 3D particle energy distributions. (arXiv:1908.05715v3 [physics.space-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.05268",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wangchunshu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jinyi Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hanlin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Chenyan Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1\">Jian Tang</a>",
          "description": "Recently generating natural language explanations has shown very promising\nresults in not only offering interpretable explanations but also providing\nadditional information and supervision for prediction. However, existing\napproaches usually require a large set of human annotated explanations for\ntraining while collecting a large set of explanations is not only time\nconsuming but also expensive. In this paper, we develop a general framework for\ninterpretable natural language understanding that requires only a small set of\nhuman annotated explanations for training. Our framework treats natural\nlanguage explanations as latent variables that model the underlying reasoning\nprocess of a neural model. We develop a variational EM framework for\noptimization where an explanation generation module and an\nexplanation-augmented prediction module are alternatively optimized and\nmutually enhance each other. Moreover, we further propose an explanation-based\nself-training method under this framework for semi-supervised learning. It\nalternates between assigning pseudo-labels to unlabeled data and generating new\nexplanations to iteratively improve each other. Experiments on two natural\nlanguage understanding tasks demonstrate that our framework can not only make\neffective predictions in both supervised and semi-supervised settings, but also\ngenerate good natural language explanation.",
          "link": "http://arxiv.org/abs/2011.05268",
          "publishedOn": "2021-06-30T02:01:03.961Z",
          "wordCount": 670,
          "title": "Towards Interpretable Natural Language Understanding with Explanations as Latent Variables. (arXiv:2011.05268v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.05893",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mhaskar_H/0/1/0/all/0/1\">H.N. Mhaskar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pereverzyev_S/0/1/0/all/0/1\">S.V. Pereverzyev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Walt_M/0/1/0/all/0/1\">M.D. van der Walt</a>",
          "description": "The problem of real time prediction of blood glucose (BG) levels based on the\nreadings from a continuous glucose monitoring (CGM) device is a problem of\ngreat importance in diabetes care, and therefore, has attracted a lot of\nresearch in recent years, especially based on machine learning. An accurate\nprediction with a 30, 60, or 90 minute prediction horizon has the potential of\nsaving millions of dollars in emergency care costs. In this paper, we treat the\nproblem as one of function approximation, where the value of the BG level at\ntime $t+h$ (where $h$ the prediction horizon) is considered to be an unknown\nfunction of $d$ readings prior to the time $t$. This unknown function may be\nsupported in particular on some unknown submanifold of the $d$-dimensional\nEuclidean space. While manifold learning is classically done in a\nsemi-supervised setting, where the entire data has to be known in advance, we\nuse recent ideas to achieve an accurate function approximation in a supervised\nsetting; i.e., construct a model for the target function. We use the\nstate-of-the-art clinically relevant PRED-EGA grid to evaluate our results, and\ndemonstrate that for a real life dataset, our method performs better than a\nstandard deep network, especially in hypoglycemic and hyperglycemic regimes.\nOne noteworthy aspect of this work is that the training data and test data may\ncome from different distributions.",
          "link": "http://arxiv.org/abs/2105.05893",
          "publishedOn": "2021-06-30T02:01:03.956Z",
          "wordCount": 709,
          "title": "A function approximation approach to the prediction of blood glucose levels. (arXiv:2105.05893v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15606",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Thakur_N/0/1/0/all/0/1\">Nirmalya Thakur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_C/0/1/0/all/0/1\">Chia Y. Han</a>",
          "description": "This work makes multiple scientific contributions to the field of Indoor\nLocalization for Ambient Assisted Living in Smart Homes. First, it presents a\nBig-Data driven methodology that studies the multimodal components of user\ninteractions and analyzes the data from Bluetooth Low Energy (BLE) beacons and\nBLE scanners to detect a user's indoor location in a specific activity-based\nzone during Activities of Daily Living. Second, it introduces a context\nindependent approach that can interpret the accelerometer and gyroscope data\nfrom diverse behavioral patterns to detect the zone-based indoor location of a\nuser in any Internet of Things (IoT)-based environment. These two approaches\nachieved performance accuracies of 81.36% and 81.13%, respectively, when tested\non a dataset. Third, it presents a methodology to detect the spatial\ncoordinates of a user's indoor position that outperforms all similar works in\nthis field, as per the associated root mean squared error - one of the\nperformance evaluation metrics in ISO/IEC18305:2016- an international standard\nfor testing Localization and Tracking Systems. Finally, it presents a\ncomprehensive comparative study that includes Random Forest, Artificial Neural\nNetwork, Decision Tree, Support Vector Machine, k-NN, Gradient Boosted Trees,\nDeep Learning, and Linear Regression, to address the challenge of identifying\nthe optimal machine learning approach for Indoor Localization.",
          "link": "http://arxiv.org/abs/2106.15606",
          "publishedOn": "2021-06-30T02:01:03.951Z",
          "wordCount": 668,
          "title": "Multimodal Approaches for Indoor Localization for Ambient Assisted Living in Smart Homes. (arXiv:2106.15606v1 [cs.HC])"
        },
        {
          "id": "http://arxiv.org/abs/2104.03546",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gatti_A/0/1/0/all/0/1\">Alice Gatti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhixiong Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smidt_T/0/1/0/all/0/1\">Tess Smidt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_E/0/1/0/all/0/1\">Esmond G. Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghysels_P/0/1/0/all/0/1\">Pieter Ghysels</a>",
          "description": "We present a novel method for graph partitioning, based on reinforcement\nlearning and graph convolutional neural networks. Our approach is to\nrecursively partition coarser representations of a given graph. The neural\nnetwork is implemented using SAGE graph convolution layers, and trained using\nan advantage actor critic (A2C) agent. We present two variants, one for finding\nan edge separator that minimizes the normalized cut or quotient cut, and one\nthat finds a small vertex separator. The vertex separators are then used to\nconstruct a nested dissection ordering to permute a sparse matrix so that its\ntriangular factorization will incur less fill-in. The partitioning quality is\ncompared with partitions obtained using METIS and SCOTCH, and the nested\ndissection ordering is evaluated in the sparse solver SuperLU. Our results show\nthat the proposed method achieves similar partitioning quality as METIS and\nSCOTCH. Furthermore, the method generalizes across different classes of graphs,\nand works well on a variety of graphs from the SuiteSparse sparse matrix\ncollection.",
          "link": "http://arxiv.org/abs/2104.03546",
          "publishedOn": "2021-06-30T02:01:03.940Z",
          "wordCount": 634,
          "title": "Graph Partitioning and Sparse Matrix Ordering using Reinforcement Learning and Graph Neural Networks. (arXiv:2104.03546v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2003.10613",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gadgil_S/0/1/0/all/0/1\">Soham Gadgil</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1\">Qingyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfefferbaum_A/0/1/0/all/0/1\">Adolf Pfefferbaum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sullivan_E/0/1/0/all/0/1\">Edith V. Sullivan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adeli_E/0/1/0/all/0/1\">Ehsan Adeli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pohl_K/0/1/0/all/0/1\">Kilian M. Pohl</a>",
          "description": "The Blood-Oxygen-Level-Dependent (BOLD) signal of resting-state fMRI\n(rs-fMRI) records the temporal dynamics of intrinsic functional networks in the\nbrain. However, existing deep learning methods applied to rs-fMRI either\nneglect the functional dependency between different brain regions in a network\nor discard the information in the temporal dynamics of brain activity. To\novercome those shortcomings, we propose to formulate functional connectivity\nnetworks within the context of spatio-temporal graphs. We train a\nspatio-temporal graph convolutional network (ST-GCN) on short sub-sequences of\nthe BOLD time series to model the non-stationary nature of functional\nconnectivity. Simultaneously, the model learns the importance of graph edges\nwithin ST-GCN to gain insight into the functional connectivities contributing\nto the prediction. In analyzing the rs-fMRI of the Human Connectome Project\n(HCP, N=1,091) and the National Consortium on Alcohol and Neurodevelopment in\nAdolescence (NCANDA, N=773), ST-GCN is significantly more accurate than common\napproaches in predicting gender and age based on BOLD signals. Furthermore, the\nbrain regions and functional connections significantly contributing to the\npredictions of our model are important markers according to the neuroscience\nliterature.",
          "link": "http://arxiv.org/abs/2003.10613",
          "publishedOn": "2021-06-30T02:01:03.925Z",
          "wordCount": 654,
          "title": "Spatio-Temporal Graph Convolution for Resting-State fMRI Analysis. (arXiv:2003.10613v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.10333",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Karimireddy_S/0/1/0/all/0/1\">Sai Praneeth Karimireddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Lie He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaggi_M/0/1/0/all/0/1\">Martin Jaggi</a>",
          "description": "Byzantine robustness has received significant attention recently given its\nimportance for distributed and federated learning. In spite of this, we\nidentify severe flaws in existing algorithms even when the data across the\nparticipants is identically distributed. First, we show realistic examples\nwhere current state of the art robust aggregation rules fail to converge even\nin the absence of any Byzantine attackers. Secondly, we prove that even if the\naggregation rules may succeed in limiting the influence of the attackers in a\nsingle round, the attackers can couple their attacks across time eventually\nleading to divergence. To address these issues, we present two surprisingly\nsimple strategies: a new robust iterative clipping procedure, and incorporating\nworker momentum to overcome time-coupled attacks. This is the first provably\nrobust method for the standard stochastic optimization setting. Our code is\nopen sourced at https://github.com/epfml/byzantine-robust-optimizer.",
          "link": "http://arxiv.org/abs/2012.10333",
          "publishedOn": "2021-06-30T02:01:03.880Z",
          "wordCount": 643,
          "title": "Learning from History for Byzantine Robust Optimization. (arXiv:2012.10333v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.16935",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Olivieri_M/0/1/0/all/0/1\">Marco Olivieri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pezzoli_M/0/1/0/all/0/1\">Mirco Pezzoli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Antonacci_F/0/1/0/all/0/1\">Fabio Antonacci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarti_A/0/1/0/all/0/1\">Augusto Sarti</a>",
          "description": "Near-field Acoustic Holography (NAH) is a well-known problem aimed at\nestimating the vibrational velocity field of a structure by means of acoustic\nmeasurements. In this paper, we propose a NAH technique based on Convolutional\nNeural Network (CNN). The devised CNN predicts the vibrational field on the\nsurface of arbitrary shaped plates (violin plates) with orthotropic material\nproperties from a limited number of measurements. In particular, the\narchitecture, named Super Resolution CNN (SRCNN), is able to estimate the\nvibrational field with a higher spatial resolution compared to the input\npressure. The pressure and velocity datasets have been generated through Finite\nElement Method simulations. We validate the proposed method by comparing the\nestimates with the synthesized ground truth and with a state-of-the-art\ntechnique. Moreover, we evaluate the robustness of the devised network against\nnoisy input data.",
          "link": "http://arxiv.org/abs/2103.16935",
          "publishedOn": "2021-06-30T02:01:03.872Z",
          "wordCount": 612,
          "title": "Near field Acoustic Holography on arbitrary shapes using Convolutional Neural Network. (arXiv:2103.16935v2 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1910.08613",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Ozbay_A/0/1/0/all/0/1\">Ali Girayhan &#xd6;zbay</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Hamzehloo_A/0/1/0/all/0/1\">Arash Hamzehloo</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Laizet_S/0/1/0/all/0/1\">Sylvain Laizet</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Tzirakis_P/0/1/0/all/0/1\">Panagiotis Tzirakis</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Rizos_G/0/1/0/all/0/1\">Georgios Rizos</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Schuller_B/0/1/0/all/0/1\">Bj&#xf6;rn Schuller</a>",
          "description": "The Poisson equation is commonly encountered in engineering, for instance in\ncomputational fluid dynamics (CFD) where it is needed to compute corrections to\nthe pressure field to ensure the incompressibility of the velocity field. In\nthe present work, we propose a novel fully convolutional neural network (CNN)\narchitecture to infer the solution of the Poisson equation on a 2D Cartesian\ngrid with different resolutions given the right hand side term, arbitrary\nboundary conditions and grid parameters. It provides unprecedented versatility\nfor a CNN approach dealing with partial differential equations. The boundary\nconditions are handled using a novel approach by decomposing the original\nPoisson problem into a homogeneous Poisson problem plus four inhomogeneous\nLaplace sub-problems. The model is trained using a novel loss function\napproximating the continuous $L^p$ norm between the prediction and the target.\nEven when predicting on grids denser than previously encountered, our model\ndemonstrates encouraging capacity to reproduce the correct solution profile.\nThe proposed model, which outperforms well-known neural network models, can be\nincluded in a CFD solver to help with solving the Poisson equation. Analytical\ntest cases indicate that our CNN architecture is capable of predicting the\ncorrect solution of a Poisson problem with mean percentage errors below 10%, an\nimprovement by comparison to the first step of conventional iterative methods.\nPredictions from our model, used as the initial guess to iterative algorithms\nlike Multigrid, can reduce the RMS error after a single iteration by more than\n90% compared to a zero initial guess.",
          "link": "http://arxiv.org/abs/1910.08613",
          "publishedOn": "2021-06-30T02:01:03.780Z",
          "wordCount": 765,
          "title": "Poisson CNN: Convolutional neural networks for the solution of the Poisson equation on a Cartesian mesh. (arXiv:1910.08613v3 [physics.comp-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.07500",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mahajan_D/0/1/0/all/0/1\">Divyat Mahajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tople_S/0/1/0/all/0/1\">Shruti Tople</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Amit Sharma</a>",
          "description": "In the domain generalization literature, a common objective is to learn\nrepresentations independent of the domain after conditioning on the class\nlabel. We show that this objective is not sufficient: there exist\ncounter-examples where a model fails to generalize to unseen domains even after\nsatisfying class-conditional domain invariance. We formalize this observation\nthrough a structural causal model and show the importance of modeling\nwithin-class variations for generalization. Specifically, classes contain\nobjects that characterize specific causal features, and domains can be\ninterpreted as interventions on these objects that change non-causal features.\nWe highlight an alternative condition: inputs across domains should have the\nsame representation if they are derived from the same object. Based on this\nobjective, we propose matching-based algorithms when base objects are observed\n(e.g., through data augmentation) and approximate the objective when objects\nare not observed (MatchDG). Our simple matching-based algorithms are\ncompetitive to prior work on out-of-domain accuracy for rotated MNIST,\nFashion-MNIST, PACS, and Chest-Xray datasets. Our method MatchDG also recovers\nground-truth object matches: on MNIST and Fashion-MNIST, top-10 matches from\nMatchDG have over 50% overlap with ground-truth matches.",
          "link": "http://arxiv.org/abs/2006.07500",
          "publishedOn": "2021-06-30T02:01:03.760Z",
          "wordCount": 665,
          "title": "Domain Generalization using Causal Matching. (arXiv:2006.07500v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.14100",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Zheng_H/0/1/0/all/0/1\">Huangjie Zheng</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zhou_M/0/1/0/all/0/1\">Mingyuan Zhou</a>",
          "description": "To measure the difference between two probability distributions, referred to\nas the source and target, respectively, we exploit both the chain rule and\nBayes' theorem to construct conditional transport (CT), which is constituted by\nboth a forward component and a backward one. The forward CT is the expected\ncost of moving a source data point to a target one, with their joint\ndistribution defined by the product of the source probability density function\n(PDF) and a source-dependent conditional distribution, which is related to the\ntarget PDF via Bayes' theorem. The backward CT is defined by reversing the\ndirection. The CT cost can be approximated by replacing the source and target\nPDFs with their discrete empirical distributions supported on mini-batches,\nmaking it amenable to implicit distributions and stochastic gradient\ndescent-based optimization. When applied to train a generative model, CT is\nshown to strike a good balance between mode-covering and mode-seeking behaviors\nand strongly resist mode collapse. On a wide variety of benchmark datasets for\ngenerative modeling, substituting the default statistical distance of an\nexisting generative adversarial network with CT is shown to consistently\nimprove the performance. PyTorch-style code is provided.",
          "link": "http://arxiv.org/abs/2012.14100",
          "publishedOn": "2021-06-30T02:01:03.755Z",
          "wordCount": 666,
          "title": "Exploiting Chain Rule and Bayes' Theorem to Compare Probability Distributions. (arXiv:2012.14100v4 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15499",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bae_S/0/1/0/all/0/1\">Sangmin Bae</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sungnyun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ko_J/0/1/0/all/0/1\">Jongwoo Ko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1\">Gihun Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noh_S/0/1/0/all/0/1\">Seungjong Noh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yun_S/0/1/0/all/0/1\">Se-Young Yun</a>",
          "description": "This paper proposes a novel contrastive learning framework, coined as\nSelf-Contrastive (SelfCon) Learning, that self-contrasts within multiple\noutputs from the different levels of a network. We confirmed that SelfCon loss\nguarantees the lower bound of mutual information (MI) between the intermediate\nand last representations. Besides, we empirically showed, via various MI\nestimators, that SelfCon loss highly correlates to the increase of MI and\nbetter classification performance. In our experiments, SelfCon surpasses\nsupervised contrastive (SupCon) learning without the need for a multi-viewed\nbatch and with the cheaper computational cost. Especially on ResNet-18, we\nachieved top-1 classification accuracy of 76.45% for the CIFAR-100 dataset,\nwhich is 2.87% and 4.36% higher than SupCon and cross-entropy loss,\nrespectively. We found that mitigating both vanishing gradient and overfitting\nissue makes our method outperform the counterparts.",
          "link": "http://arxiv.org/abs/2106.15499",
          "publishedOn": "2021-06-30T02:01:03.728Z",
          "wordCount": 555,
          "title": "Self-Contrastive Learning. (arXiv:2106.15499v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2102.00314",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vardi_G/0/1/0/all/0/1\">Gal Vardi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reichman_D/0/1/0/all/0/1\">Daniel Reichman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pitassi_T/0/1/0/all/0/1\">Toniann Pitassi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shamir_O/0/1/0/all/0/1\">Ohad Shamir</a>",
          "description": "When studying the expressive power of neural networks, a main challenge is to\nunderstand how the size and depth of the network affect its ability to\napproximate real functions. However, not all functions are interesting from a\npractical viewpoint: functions of interest usually have a polynomially-bounded\nLipschitz constant, and can be computed efficiently. We call functions that\nsatisfy these conditions \"benign\", and explore the benefits of size and depth\nfor approximation of benign functions with ReLU networks. As we show, this\nproblem is more challenging than the corresponding problem for non-benign\nfunctions. We give barriers to showing depth-lower-bounds: Proving existence of\na benign function that cannot be approximated by polynomial-size networks of\ndepth $4$ would settle longstanding open problems in computational complexity.\nIt implies that beyond depth $4$ there is a barrier to showing depth-separation\nfor benign functions, even between networks of constant depth and networks of\nnonconstant depth. We also study size-separation, namely, whether there are\nbenign functions that can be approximated with networks of size $O(s(d))$, but\nnot with networks of size $O(s'(d))$. We show a complexity-theoretic barrier to\nproving such results beyond size $O(d\\log^2(d))$, but also show an explicit\nbenign function, that can be approximated with networks of size $O(d)$ and not\nwith networks of size $o(d/\\log d)$. For approximation in $L_\\infty$ we achieve\nsuch separation already between size $O(d)$ and size $o(d)$. Moreover, we show\nsuperpolynomial size lower bounds and barriers to such lower bounds, depending\non the assumptions on the function. Our size-separation results rely on an\nanalysis of size lower bounds for Boolean functions, which is of independent\ninterest: We show linear size lower bounds for computing explicit Boolean\nfunctions with neural networks and threshold circuits.",
          "link": "http://arxiv.org/abs/2102.00314",
          "publishedOn": "2021-06-30T02:01:03.723Z",
          "wordCount": 780,
          "title": "Size and Depth Separation in Approximating Benign Functions with Neural Networks. (arXiv:2102.00314v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15360",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhuo Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1\">Yufei Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiangliang Zhang</a>",
          "description": "Despite of the pervasive existence of multi-label evasion attack, it is an\nopen yet essential problem to characterize the origin of the adversarial\nvulnerability of a multi-label learning system and assess its attackability. In\nthis study, we focus on non-targeted evasion attack against multi-label\nclassifiers. The goal of the threat is to cause miss-classification with\nrespect to as many labels as possible, with the same input perturbation. Our\nwork gains in-depth understanding about the multi-label adversarial attack by\nfirst characterizing the transferability of the attack based on the functional\nproperties of the multi-label classifier. We unveil how the transferability\nlevel of the attack determines the attackability of the classifier via\nestablishing an information-theoretic analysis of the adversarial risk.\nFurthermore, we propose a transferability-centered attackability assessment,\nnamed Soft Attackability Estimator (SAE), to evaluate the intrinsic\nvulnerability level of the targeted multi-label classifier. This estimator is\nthen integrated as a transferability-tuning regularization term into the\nmulti-label learning paradigm to achieve adversarially robust classification.\nThe experimental study on real-world data echos the theoretical analysis and\nverify the validity of the transferability-regularized multi-label learning\nmethod.",
          "link": "http://arxiv.org/abs/2106.15360",
          "publishedOn": "2021-06-30T02:01:03.717Z",
          "wordCount": 618,
          "title": "Attack Transferability Characterization for Adversarially Robust Multi-label Classification. (arXiv:2106.15360v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15432",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Du_Y/0/1/0/all/0/1\">Yuxuan Du</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Tao_D/0/1/0/all/0/1\">Dacheng Tao</a>",
          "description": "Quantum auto-encoder (QAE) is a powerful tool to relieve the curse of\ndimensionality encountered in quantum physics, celebrated by the ability to\nextract low-dimensional patterns from quantum states living in the\nhigh-dimensional space. Despite its attractive properties, little is known\nabout the practical applications of QAE with provable advantages. To address\nthese issues, here we prove that QAE can be used to efficiently calculate the\neigenvalues and prepare the corresponding eigenvectors of a high-dimensional\nquantum state with the low-rank property. With this regard, we devise three\neffective QAE-based learning protocols to solve the low-rank state fidelity\nestimation, the quantum Gibbs state preparation, and the quantum metrology\ntasks, respectively. Notably, all of these protocols are scalable and can be\nreadily executed on near-term quantum machines. Moreover, we prove that the\nerror bounds of the proposed QAE-based methods outperform those in previous\nliterature. Numerical simulations collaborate with our theoretical analysis.\nOur work opens a new avenue of utilizing QAE to tackle various quantum physics\nand quantum information processing problems in a scalable way.",
          "link": "http://arxiv.org/abs/2106.15432",
          "publishedOn": "2021-06-30T02:01:03.703Z",
          "wordCount": 601,
          "title": "On exploring practical potentials of quantum auto-encoder with advantages. (arXiv:2106.15432v1 [quant-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15481",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fujiwara_T/0/1/0/all/0/1\">Takanori Fujiwara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xinhai Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jian Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1\">Kwan-Liu Ma</a>",
          "description": "Finding the similarities and differences between two or more groups of\ndatasets is a fundamental analysis task. For high-dimensional data,\ndimensionality reduction (DR) methods are often used to find the\ncharacteristics of each group. However, existing DR methods provide limited\ncapability and flexibility for such comparative analysis as each method is\ndesigned only for a narrow analysis target, such as identifying factors that\nmost differentiate groups. In this work, we introduce an interactive DR\nframework where we integrate our new DR method, called ULCA (unified linear\ncomparative analysis), with an interactive visual interface. ULCA unifies two\nDR schemes, discriminant analysis and contrastive learning, to support various\ncomparative analysis tasks. To provide flexibility for comparative analysis, we\ndevelop an optimization algorithm that enables analysts to interactively refine\nULCA results. Additionally, we provide an interactive visualization interface\nto examine ULCA results with a rich set of analysis libraries. We evaluate ULCA\nand the optimization algorithm to show their efficiency as well as present\nmultiple case studies using real-world datasets to demonstrate the usefulness\nof our framework.",
          "link": "http://arxiv.org/abs/2106.15481",
          "publishedOn": "2021-06-30T02:01:03.698Z",
          "wordCount": 616,
          "title": "Interactive Dimensionality Reduction for Comparative Analysis. (arXiv:2106.15481v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2007.00153",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Lan_G/0/1/0/all/0/1\">Guanghui Lan</a>, <a href=\"http://arxiv.org/find/math/1/au:+Romeijn_E/0/1/0/all/0/1\">Edwin Romeijn</a>, <a href=\"http://arxiv.org/find/math/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhiqiang Zhou</a>",
          "description": "Conditional gradient methods have attracted much attention in both machine\nlearning and optimization communities recently. These simple methods can\nguarantee the generation of sparse solutions. In addition, without the\ncomputation of full gradients, they can handle huge-scale problems sometimes\neven with an exponentially increasing number of decision variables. This paper\naims to significantly expand the application areas of these methods by\npresenting new conditional gradient methods for solving convex optimization\nproblems with general affine and nonlinear constraints. More specifically, we\nfirst present a new constraint extrapolated condition gradient (CoexCG) method\nthat can achieve an ${\\cal O}(1/\\epsilon^2)$ iteration complexity for both\nsmooth and structured nonsmooth function constrained convex optimization. We\nfurther develop novel variants of CoexCG, namely constraint extrapolated and\ndual regularized conditional gradient (CoexDurCG) methods, that can achieve\nsimilar iteration complexity to CoexCG but allow adaptive selection for\nalgorithmic parameters. We illustrate the effectiveness of these methods for\nsolving an important class of radiation therapy treatment planning problems\narising from healthcare industry. To the best of our knowledge, all the\nalgorithmic schemes and their complexity results are new in the area of\nprojection-free methods.",
          "link": "http://arxiv.org/abs/2007.00153",
          "publishedOn": "2021-06-30T02:01:03.693Z",
          "wordCount": 652,
          "title": "Conditional Gradient Methods for Convex Optimization with General Affine and Nonlinear Constraints. (arXiv:2007.00153v3 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15504",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bhatia_S/0/1/0/all/0/1\">Siddharth Bhatia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yiwei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hooi_B/0/1/0/all/0/1\">Bryan Hooi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_T/0/1/0/all/0/1\">Tanmoy Chakraborty</a>",
          "description": "Finding anomalous snapshots from a graph has garnered huge attention\nrecently. Existing studies address the problem using shallow learning\nmechanisms such as subspace selection, ego-network, or community analysis.\nThese models do not take into account the multifaceted interactions between the\nstructure and attributes in the network. In this paper, we propose GraphAnoGAN,\nan anomalous snapshot ranking framework, which consists of two core components\n-- generative and discriminative models. Specifically, the generative model\nlearns to approximate the distribution of anomalous samples from the candidate\nset of graph snapshots, and the discriminative model detects whether the\nsampled snapshot is from the ground-truth or not. Experiments on 4 real-world\nnetworks show that GraphAnoGAN outperforms 6 baselines with a significant\nmargin (28.29% and 22.01% higher precision and recall, respectively compared to\nthe best baseline, averaged across all datasets).",
          "link": "http://arxiv.org/abs/2106.15504",
          "publishedOn": "2021-06-30T02:01:03.688Z",
          "wordCount": 576,
          "title": "GraphAnoGAN: Detecting Anomalous Snapshots from Attributed Graphs. (arXiv:2106.15504v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2008.10066",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sikchi_H/0/1/0/all/0/1\">Harshit Sikchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wenxuan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Held_D/0/1/0/all/0/1\">David Held</a>",
          "description": "Reinforcement learning (RL) in low-data and risk-sensitive domains requires\nperformant and flexible deployment policies that can readily incorporate\nconstraints during deployment. One such class of policies are the\nsemi-parametric H-step lookahead policies, which select actions using\ntrajectory optimization over a dynamics model for a fixed horizon with a\nterminal value function. In this work, we investigate a novel instantiation of\nH-step lookahead with a learned model and a terminal value function learned by\na model-free off-policy algorithm, named Learning Off-Policy with Online\nPlanning (LOOP). We provide a theoretical analysis of this method, suggesting a\ntradeoff between model errors and value function errors and empirically\ndemonstrate this tradeoff to be beneficial in deep reinforcement learning.\nFurthermore, we identify the \"Actor Divergence\" issue in this framework and\npropose Actor Regularized Control (ARC), a modified trajectory optimization\nprocedure. We evaluate our method on a set of robotic tasks for Offline and\nOnline RL and demonstrate improved performance. We also show the flexibility of\nLOOP to incorporate safety constraints during deployment with a set of\nnavigation environments. We demonstrate that LOOP is a desirable framework for\nrobotics applications based on its strong performance in various important RL\nsettings.",
          "link": "http://arxiv.org/abs/2008.10066",
          "publishedOn": "2021-06-30T02:01:03.684Z",
          "wordCount": 671,
          "title": "Learning Off-Policy with Online Planning. (arXiv:2008.10066v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15497",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shi_C/0/1/0/all/0/1\">Chaochen Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_Y/0/1/0/all/0/1\">Yong Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doss_R/0/1/0/all/0/1\">Robin Ram Mohan Doss</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1\">Jiangshan Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sood_K/0/1/0/all/0/1\">Keshav Sood</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Longxiang Gao</a>",
          "description": "With the development of blockchain technologies, the number of smart\ncontracts deployed on blockchain platforms is growing exponentially, which\nmakes it difficult for users to find desired services by manual screening. The\nautomatic classification of smart contracts can provide blockchain users with\nkeyword-based contract searching and helps to manage smart contracts\neffectively. Current research on smart contract classification focuses on\nNatural Language Processing (NLP) solutions which are based on contract source\ncode. However, more than 94% of smart contracts are not open-source, so the\napplication scenarios of NLP methods are very limited. Meanwhile, NLP models\nare vulnerable to adversarial attacks. This paper proposes a classification\nmodel based on features from contract bytecode instead of source code to solve\nthese problems. We also use feature selection and ensemble learning to optimize\nthe model. Our experimental studies on over 3,300 real-world Ethereum smart\ncontracts show that our model can classify smart contracts without source code\nand has better performance than baseline models. Our model also has good\nresistance to adversarial attacks compared with NLP-based models. In addition,\nour analysis reveals that account features used in many smart contract\nclassification models have little effect on classification and can be excluded.",
          "link": "http://arxiv.org/abs/2106.15497",
          "publishedOn": "2021-06-30T02:01:03.678Z",
          "wordCount": 645,
          "title": "A Bytecode-based Approach for Smart Contract Classification. (arXiv:2106.15497v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15416",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gorban_A/0/1/0/all/0/1\">Alexander N. Gorban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grechuk_B/0/1/0/all/0/1\">Bogdan Grechuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mirkes_E/0/1/0/all/0/1\">Evgeny M. Mirkes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stasenko_S/0/1/0/all/0/1\">Sergey V. Stasenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tyukin_I/0/1/0/all/0/1\">Ivan Y. Tyukin</a>",
          "description": "This work is driven by a practical question, corrections of Artificial\nIntelligence (AI) errors. Systematic re-training of a large AI system is hardly\npossible. To solve this problem, special external devices, correctors, are\ndeveloped. They should provide quick and non-iterative system fix without\nmodification of a legacy AI system. A common universal part of the AI corrector\nis a classifier that should separate undesired and erroneous behavior from\nnormal operation. Training of such classifiers is a grand challenge at the\nheart of the one- and few-shot learning methods. Effectiveness of one- and\nfew-short methods is based on either significant dimensionality reductions or\nthe blessing of dimensionality effects. Stochastic separability is a blessing\nof dimensionality phenomenon that allows one-and few-shot error correction: in\nhigh-dimensional datasets under broad assumptions each point can be separated\nfrom the rest of the set by simple and robust linear discriminant. The\nhierarchical structure of data universe is introduced where each data cluster\nhas a granular internal structure, etc. New stochastic separation theorems for\nthe data distributions with fine-grained structure are formulated and proved.\nSeparation theorems in infinite-dimensional limits are proven under assumptions\nof compact embedding of patterns into data space. New multi-correctors of AI\nsystems are presented and illustrated with examples of predicting errors and\nlearning new classes of objects by a deep convolutional neural network.",
          "link": "http://arxiv.org/abs/2106.15416",
          "publishedOn": "2021-06-30T02:01:03.673Z",
          "wordCount": 662,
          "title": "High-dimensional separability for one- and few-shot learning. (arXiv:2106.15416v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15419",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhikang T. Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ueda_M/0/1/0/all/0/1\">Masahito Ueda</a>",
          "description": "Despite the empirical success of the deep Q network (DQN) reinforcement\nlearning algorithm and its variants, DQN is still not well understood and it\ndoes not guarantee convergence. In this work, we show that DQN can diverge and\ncease to operate in realistic settings. Although there exist gradient-based\nconvergent methods, we show that they actually have inherent problems in\nlearning behaviour and elucidate why they often fail in practice. To overcome\nthese problems, we propose a convergent DQN algorithm (C-DQN) by carefully\nmodifying DQN, and we show that the algorithm is convergent and can work with\nlarge discount factors (0.9998). It learns robustly in difficult settings and\ncan learn several difficult games in the Atari 2600 benchmark where DQN fail,\nwithin a moderate computational budget. Our codes have been publicly released\nand can be used to reproduce our results.",
          "link": "http://arxiv.org/abs/2106.15419",
          "publishedOn": "2021-06-30T02:01:03.660Z",
          "wordCount": 571,
          "title": "A Convergent and Efficient Deep Q Network Algorithm. (arXiv:2106.15419v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15456",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Saachi Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radhakrishnan_A/0/1/0/all/0/1\">Adityanarayanan Radhakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uhler_C/0/1/0/all/0/1\">Caroline Uhler</a>",
          "description": "Aligned latent spaces, where meaningful semantic shifts in the input space\ncorrespond to a translation in the embedding space, play an important role in\nthe success of downstream tasks such as unsupervised clustering and data\nimputation. In this work, we prove that linear and nonlinear autoencoders\nproduce aligned latent spaces by stretching along the left singular vectors of\nthe data. We fully characterize the amount of stretching in linear autoencoders\nand provide an initialization scheme to arbitrarily stretch along the top\ndirections using these networks. We also quantify the amount of stretching in\nnonlinear autoencoders in a simplified setting. We use our theoretical results\nto align drug signatures across cell types in gene expression space and\nsemantic shifts in word embedding spaces.",
          "link": "http://arxiv.org/abs/2106.15456",
          "publishedOn": "2021-06-30T02:01:03.654Z",
          "wordCount": 556,
          "title": "A Mechanism for Producing Aligned Latent Spaces with Autoencoders. (arXiv:2106.15456v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15523",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jing Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_T/0/1/0/all/0/1\">Tong Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spathis_D/0/1/0/all/0/1\">Dimitris Spathis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bondareva_E/0/1/0/all/0/1\">Erika Bondareva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brown_C/0/1/0/all/0/1\">Chlo&#xeb; Brown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chauhan_J/0/1/0/all/0/1\">Jagmohan Chauhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dang_T/0/1/0/all/0/1\">Ting Dang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grammenos_A/0/1/0/all/0/1\">Andreas Grammenos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasthanasombat_A/0/1/0/all/0/1\">Apinan Hasthanasombat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Floto_A/0/1/0/all/0/1\">Andres Floto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cicuta_P/0/1/0/all/0/1\">Pietro Cicuta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mascolo_C/0/1/0/all/0/1\">Cecilia Mascolo</a>",
          "description": "Researchers have been battling with the question of how we can identify\nCoronavirus disease (COVID-19) cases efficiently, affordably and at scale.\nRecent work has shown how audio based approaches, which collect respiratory\naudio data (cough, breathing and voice) can be used for testing, however there\nis a lack of exploration of how biases and methodological decisions impact\nthese tools' performance in practice. In this paper, we explore the realistic\nperformance of audio-based digital testing of COVID-19. To investigate this, we\ncollected a large crowdsourced respiratory audio dataset through a mobile app,\nalongside recent COVID-19 test result and symptoms intended as a ground truth.\nWithin the collected dataset, we selected 5,240 samples from 2,478 participants\nand split them into different participant-independent sets for model\ndevelopment and validation. Among these, we controlled for potential\nconfounding factors (such as demographics and language). The unbiased model\ntakes features extracted from breathing, coughs, and voice signals as\npredictors and yields an AUC-ROC of 0.71 (95\\% CI: 0.65$-$0.77). We further\nexplore different unbalanced distributions to show how biases and participant\nsplits affect performance. Finally, we discuss how the realistic model\npresented could be integrated in clinical practice to realize continuous,\nubiquitous, sustainable and affordable testing at population scale.",
          "link": "http://arxiv.org/abs/2106.15523",
          "publishedOn": "2021-06-30T02:01:03.650Z",
          "wordCount": 706,
          "title": "Sounds of COVID-19: exploring realistic performance of audio-based digital testing. (arXiv:2106.15523v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15411",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bogatinovski_J/0/1/0/all/0/1\">Jasmin Bogatinovski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Todorovski_L/0/1/0/all/0/1\">Ljup&#x10d;o Todorovski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dzeroski_S/0/1/0/all/0/1\">Sa&#x161;o D&#x17e;eroski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kocev_D/0/1/0/all/0/1\">Dragi Kocev</a>",
          "description": "Meta learning generalizes the empirical experience with different learning\ntasks and holds promise for providing important empirical insight into the\nbehaviour of machine learning algorithms. In this paper, we present a\ncomprehensive meta-learning study of data sets and methods for multi-label\nclassification (MLC). MLC is a practically relevant machine learning task where\neach example is labelled with multiple labels simultaneously. Here, we analyze\n40 MLC data sets by using 50 meta features describing different properties of\nthe data. The main findings of this study are as follows. First, the most\nprominent meta features that describe the space of MLC data sets are the ones\nassessing different aspects of the label space. Second, the meta models show\nthat the most important meta features describe the label space, and, the meta\nfeatures describing the relationships among the labels tend to occur a bit more\noften than the meta features describing the distributions between and within\nthe individual labels. Third, the optimization of the hyperparameters can\nimprove the predictive performance, however, quite often the extent of the\nimprovements does not always justify the resource utilization.",
          "link": "http://arxiv.org/abs/2106.15411",
          "publishedOn": "2021-06-30T02:01:03.636Z",
          "wordCount": 621,
          "title": "Explaining the Performance of Multi-label Classification Methods with Data Set Properties. (arXiv:2106.15411v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15406",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_R/0/1/0/all/0/1\">Rongfei Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_C/0/1/0/all/0/1\">Chao Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xingwei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_X/0/1/0/all/0/1\">Xiaowen Chu</a>",
          "description": "Federated learning utilizes various resources provided by participants to\ncollaboratively train a global model, which potentially address the data\nprivacy issue of machine learning. In such promising paradigm, the performance\nwill be deteriorated without sufficient training data and other resources in\nthe learning process. Thus, it is quite crucial to inspire more participants to\ncontribute their valuable resources with some payments for federated learning.\nIn this paper, we present a comprehensive survey of incentive schemes for\nfederate learning. Specifically, we identify the incentive problem in federated\nlearning and then provide a taxonomy for various schemes. Subsequently, we\nsummarize the existing incentive mechanisms in terms of the main techniques,\nsuch as Stackelberg game, auction, contract theory, Shapley value,\nreinforcement learning, blockchain. By reviewing and comparing some impressive\nresults, we figure out three directions for the future study.",
          "link": "http://arxiv.org/abs/2106.15406",
          "publishedOn": "2021-06-30T02:01:03.631Z",
          "wordCount": 582,
          "title": "A Comprehensive Survey of Incentive Mechanism for Federated Learning. (arXiv:2106.15406v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15348",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Razghandi_M/0/1/0/all/0/1\">Mina Razghandi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_H/0/1/0/all/0/1\">Hao Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Erol_Kantarci_M/0/1/0/all/0/1\">Melike Erol-Kantarci</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Turgut_D/0/1/0/all/0/1\">Damla Turgut</a>",
          "description": "Appliance-level load forecasting plays a critical role in residential energy\nmanagement, besides having significant importance for ancillary services\nperformed by the utilities. In this paper, we propose to use an LSTM-based\nsequence-to-sequence (seq2seq) learning model that can capture the load\nprofiles of appliances. We use a real dataset collected fromfour residential\nbuildings and compare our proposed schemewith three other techniques, namely\nVARMA, Dilated One Dimensional Convolutional Neural Network, and an LSTM\nmodel.The results show that the proposed LSTM-based seq2seq model outperforms\nother techniques in terms of prediction error in most cases.",
          "link": "http://arxiv.org/abs/2106.15348",
          "publishedOn": "2021-06-30T02:01:03.626Z",
          "wordCount": 556,
          "title": "Short-Term Load Forecasting for Smart HomeAppliances with Sequence to Sequence Learning. (arXiv:2106.15348v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15357",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaosen Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1\">Chuanbiao Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liwei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_K/0/1/0/all/0/1\">Kun He</a>",
          "description": "In the field of adversarial robustness, there is a common practice that\nadopts the single-step adversarial training for quickly developing\nadversarially robust models. However, the single-step adversarial training is\nmost likely to cause catastrophic overfitting, as after a few training epochs\nit will be hard to generate strong adversarial examples to continuously boost\nthe adversarial robustness. In this work, we aim to avoid the catastrophic\noverfitting by introducing multi-step adversarial examples during the\nsingle-step adversarial training. Then, to balance the large training overhead\nof generating multi-step adversarial examples, we propose a Multi-stage\nOptimization based Adversarial Training (MOAT) method that periodically trains\nthe model on mixed benign examples, single-step adversarial examples, and\nmulti-step adversarial examples stage by stage. In this way, the overall\ntraining overhead is reduced significantly, meanwhile, the model could avoid\ncatastrophic overfitting. Extensive experiments on CIFAR-10 and CIFAR-100\ndatasets demonstrate that under similar amount of training overhead, the\nproposed MOAT exhibits better robustness than either single-step or multi-step\nadversarial training methods.",
          "link": "http://arxiv.org/abs/2106.15357",
          "publishedOn": "2021-06-30T02:01:03.621Z",
          "wordCount": 597,
          "title": "Multi-stage Optimization based Adversarial Training. (arXiv:2106.15357v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15434",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Shu_Y/0/1/0/all/0/1\">Yang Shu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kou_Z/0/1/0/all/0/1\">Zhi Kou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1\">Zhangjie Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianmin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_M/0/1/0/all/0/1\">Mingsheng Long</a>",
          "description": "With the development of deep networks on various large-scale datasets, a\nlarge zoo of pretrained models are available. When transferring from a model\nzoo, applying classic single-model based transfer learning methods to each\nsource model suffers from high computational burden and cannot fully utilize\nthe rich knowledge in the zoo. We propose \\emph{Zoo-Tuning} to address these\nchallenges, which learns to adaptively transfer the parameters of pretrained\nmodels to the target task. With the learnable channel alignment layer and\nadaptive aggregation layer, Zoo-Tuning \\emph{adaptively aggregates channel\naligned pretrained parameters} to derive the target model, which promotes\nknowledge transfer by simultaneously adapting multiple source models to\ndownstream tasks. The adaptive aggregation substantially reduces the\ncomputation cost at both training and inference. We further propose lite\nZoo-Tuning with the temporal ensemble of batch average gating values to reduce\nthe storage cost at the inference time. We evaluate our approach on a variety\nof tasks, including reinforcement learning, image classification, and facial\nlandmark detection. Experiment results demonstrate that the proposed adaptive\ntransfer learning approach can transfer knowledge from a zoo of models more\neffectively and efficiently.",
          "link": "http://arxiv.org/abs/2106.15434",
          "publishedOn": "2021-06-30T02:01:03.616Z",
          "wordCount": 618,
          "title": "Zoo-Tuning: Adaptive Transfer from a Zoo of Models. (arXiv:2106.15434v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15382",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1\">Tianyu Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Q/0/1/0/all/0/1\">Quanxue Gao</a>",
          "description": "Graph-based multi-view clustering has become an active topic due to the\nefficiency in characterizing both the complex structure and relationship\nbetween multimedia data. However, existing methods have the following\nshortcomings: (1) They are inefficient or even fail for graph learning in large\nscale due to the graph construction and eigen-decomposition. (2) They cannot\nwell exploit both the complementary information and spatial structure embedded\nin graphs of different views. To well exploit complementary information and\ntackle the scalability issue plaguing graph-based multi-view clustering, we\npropose an efficient multiple graph learning model via a small number of anchor\npoints and tensor Schatten p-norm minimization. Specifically, we construct a\nhidden and tractable large graph by anchor graph for each view and well exploit\ncomplementary information embedded in anchor graphs of different views by\ntensor Schatten p-norm regularizer. Finally, we develop an efficient algorithm,\nwhich scales linearly with the data size, to solve our proposed model.\nExtensive experimental results on several datasets indicate that our proposed\nmethod outperforms some state-of-the-art multi-view clustering algorithms.",
          "link": "http://arxiv.org/abs/2106.15382",
          "publishedOn": "2021-06-30T02:01:03.611Z",
          "wordCount": 603,
          "title": "Multiple Graph Learning for Scalable Multi-view Clustering. (arXiv:2106.15382v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15400",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Q/0/1/0/all/0/1\">Qiuqiang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1\">Chuanhou Gao</a>",
          "description": "Click-Through Rate prediction aims to predict the ratio of clicks to\nimpressions of a specific link. This is a challenging task since (1) there are\nusually categorical features, and the inputs will be extremely high-dimensional\nif one-hot encoding is applied, (2) not only the original features but also\ntheir interactions are important, (3) an effective prediction may rely on\ndifferent features and interactions in different time periods. To overcome\nthese difficulties, we propose a new interaction detection method, named Online\nRandom Intersection Chains. The method, which is based on the idea of frequent\nitemset mining, detects informative interactions by observing the intersections\nof randomly chosen samples. The discovered interactions enjoy high\ninterpretability as they can be comprehended as logical expressions. ORIC can\nbe updated every time new data is collected, without being retrained on\nhistorical data. What's more, the importance of the historical and latest data\ncan be controlled by a tuning parameter. A framework is designed to deal with\nthe streaming interactions, so almost all existing models for CTR prediction\ncan be applied after interaction detection. Empirical results demonstrate the\nefficiency and effectiveness of ORIC on three benchmark datasets.",
          "link": "http://arxiv.org/abs/2106.15400",
          "publishedOn": "2021-06-30T02:01:03.606Z",
          "wordCount": 625,
          "title": "Online Interaction Detection for Click-Through Rate Prediction. (arXiv:2106.15400v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15482",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Achituve_I/0/1/0/all/0/1\">Idan Achituve</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shamsian_A/0/1/0/all/0/1\">Aviv Shamsian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navon_A/0/1/0/all/0/1\">Aviv Navon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chechik_G/0/1/0/all/0/1\">Gal Chechik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fetaya_E/0/1/0/all/0/1\">Ethan Fetaya</a>",
          "description": "Federated learning aims to learn a global model that performs well on client\ndevices with limited cross-client communication. Personalized federated\nlearning (PFL) further extends this setup to handle data heterogeneity between\nclients by learning personalized models. A key challenge in this setting is to\nlearn effectively across clients even though each client has unique data that\nis often limited in size. Here we present pFedGP, a solution to PFL that is\nbased on Gaussian processes (GPs) with deep kernel learning. GPs are highly\nexpressive models that work well in the low data regime due to their Bayesian\nnature. However, applying GPs to PFL raises multiple challenges. Mainly, GPs\nperformance depends heavily on access to a good kernel function, and learning a\nkernel requires a large training set. Therefore, we propose learning a shared\nkernel function across all clients, parameterized by a neural network, with a\npersonal GP classifier for each client. We further extend pFedGP to include\ninducing points using two novel methods, the first helps to improve\ngeneralization in the low data regime and the second reduces the computational\ncost. We derive a PAC-Bayes generalization bound on novel clients and\nempirically show that it gives non-vacuous guarantees. Extensive experiments on\nstandard PFL benchmarks with CIFAR-10, CIFAR-100, and CINIC-10, and on a new\nsetup of learning under input noise show that pFedGP achieves well-calibrated\npredictions while significantly outperforming baseline methods, reaching up to\n21% in accuracy gain.",
          "link": "http://arxiv.org/abs/2106.15482",
          "publishedOn": "2021-06-30T02:01:03.602Z",
          "wordCount": 669,
          "title": "Personalized Federated Learning with Gaussian Processes. (arXiv:2106.15482v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15502",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chakrabarty_A/0/1/0/all/0/1\">Ankush Chakrabarty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wichern_G/0/1/0/all/0/1\">Gordon Wichern</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laughman_C/0/1/0/all/0/1\">Christopher Laughman</a>",
          "description": "Physics-informed dynamical system models form critical components of digital\ntwins of the built environment. These digital twins enable the design of\nenergy-efficient infrastructure, but must be properly calibrated to accurately\nreflect system behavior for downstream prediction and analysis. Dynamical\nsystem models of modern buildings are typically described by a large number of\nparameters and incur significant computational expenditure during simulations.\nTo handle large-scale calibration of digital twins without exorbitant\nsimulations, we propose ANP-BBO: a scalable and parallelizable batch-wise\nBayesian optimization (BBO) methodology that leverages attentive neural\nprocesses (ANPs).",
          "link": "http://arxiv.org/abs/2106.15502",
          "publishedOn": "2021-06-30T02:01:03.583Z",
          "wordCount": 553,
          "title": "Attentive Neural Processes and Batch Bayesian Optimization for Scalable Calibration of Physics-Informed Digital Twins. (arXiv:2106.15502v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15427",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Nadjahi_K/0/1/0/all/0/1\">Kimia Nadjahi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Durmus_A/0/1/0/all/0/1\">Alain Durmus</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Jacob_P/0/1/0/all/0/1\">Pierre E. Jacob</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Badeau_R/0/1/0/all/0/1\">Roland Badeau</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Simsekli_U/0/1/0/all/0/1\">Umut &#x15e;im&#x15f;ekli</a>",
          "description": "The Sliced-Wasserstein distance (SW) is being increasingly used in machine\nlearning applications as an alternative to the Wasserstein distance and offers\nsignificant computational and statistical benefits. Since it is defined as an\nexpectation over random projections, SW is commonly approximated by Monte\nCarlo. We adopt a new perspective to approximate SW by making use of the\nconcentration of measure phenomenon: under mild assumptions, one-dimensional\nprojections of a high-dimensional random vector are approximately Gaussian.\nBased on this observation, we develop a simple deterministic approximation for\nSW. Our method does not require sampling a number of random projections, and is\ntherefore both accurate and easy to use compared to the usual Monte Carlo\napproximation. We derive nonasymptotical guarantees for our approach, and show\nthat the approximation error goes to zero as the dimension increases, under a\nweak dependence condition on the data distribution. We validate our theoretical\nfindings on synthetic datasets, and illustrate the proposed approximation on a\ngenerative modeling problem.",
          "link": "http://arxiv.org/abs/2106.15427",
          "publishedOn": "2021-06-30T02:01:03.568Z",
          "wordCount": 602,
          "title": "Fast Approximation of the Sliced-Wasserstein Distance Using Concentration of Random Projections. (arXiv:2106.15427v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15612",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fu_X/0/1/0/all/0/1\">Xiang Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1\">Ge Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_P/0/1/0/all/0/1\">Pulkit Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaakkola_T/0/1/0/all/0/1\">Tommi Jaakkola</a>",
          "description": "Current model-based reinforcement learning methods struggle when operating\nfrom complex visual scenes due to their inability to prioritize task-relevant\nfeatures. To mitigate this problem, we propose learning Task Informed\nAbstractions (TIA) that explicitly separates reward-correlated visual features\nfrom distractors. For learning TIA, we introduce the formalism of Task Informed\nMDP (TiMDP) that is realized by training two models that learn visual features\nvia cooperative reconstruction, but one model is adversarially dissociated from\nthe reward signal. Empirical evaluation shows that TIA leads to significant\nperformance gains over state-of-the-art methods on many visual control tasks\nwhere natural and unconstrained visual distractions pose a formidable\nchallenge.",
          "link": "http://arxiv.org/abs/2106.15612",
          "publishedOn": "2021-06-30T02:01:03.556Z",
          "wordCount": 537,
          "title": "Learning Task Informed Abstraction. (arXiv:2106.15612v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15599",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Thakur_N/0/1/0/all/0/1\">Nirmalya Thakur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_C/0/1/0/all/0/1\">Chia Y. Han</a>",
          "description": "The population of elderly people has been increasing at a rapid rate over the\nlast few decades and their population is expected to further increase in the\nupcoming future. Their increasing population is associated with their\nincreasing needs due to problems like physical disabilities, cognitive issues,\nweakened memory and disorganized behavior, that elderly people face with\nincreasing age. To reduce their financial burden on the world economy and to\nenhance their quality of life, it is essential to develop technology-based\nsolutions that are adaptive, assistive and intelligent in nature. Intelligent\nAffect Aware Systems that can not only analyze but also predict the behavior of\nelderly people in the context of their day to day interactions with technology\nin an IoT-based environment, holds immense potential for serving as a long-term\nsolution for improving the user experience of elderly in smart homes. This work\ntherefore proposes the framework for an Intelligent Affect Aware environment\nfor elderly people that can not only analyze the affective components of their\ninteractions but also predict their likely user experience even before they\nstart engaging in any activity in the given smart home environment. This\nforecasting of user experience would provide scope for enhancing the same,\nthereby increasing the assistive and adaptive nature of such intelligent\nsystems. To uphold the efficacy of this proposed framework for improving the\nquality of life of elderly people in smart homes, it has been tested on three\ndatasets and the results are presented and discussed.",
          "link": "http://arxiv.org/abs/2106.15599",
          "publishedOn": "2021-06-30T02:01:03.540Z",
          "wordCount": 723,
          "title": "Framework for an Intelligent Affect Aware Smart Home Environment for Elderly People. (arXiv:2106.15599v1 [cs.HC])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15594",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Quinteiro_R/0/1/0/all/0/1\">Ricardo Quinteiro</a>, <a href=\"http://arxiv.org/find/math/1/au:+Melo_F/0/1/0/all/0/1\">Francisco S. Melo</a>, <a href=\"http://arxiv.org/find/math/1/au:+Santos_P/0/1/0/all/0/1\">Pedro A. Santos</a>",
          "description": "This paper addresses the problem of optimal control using search trees. We\nstart by considering multi-armed bandit problems with continuous action spaces\nand propose LD-HOO, a limited depth variant of the hierarchical optimistic\noptimization (HOO) algorithm. We provide a regret analysis for LD-HOO and show\nthat, asymptotically, our algorithm exhibits the same cumulative regret as the\noriginal HOO while being faster and more memory efficient. We then propose a\nMonte Carlo tree search algorithm based on LD-HOO for optimal control problems\nand illustrate the resulting approach's application in several optimal control\nproblems.",
          "link": "http://arxiv.org/abs/2106.15594",
          "publishedOn": "2021-06-30T02:01:03.535Z",
          "wordCount": 535,
          "title": "Limited depth bandit-based strategy for Monte Carlo planning in continuous action spaces. (arXiv:2106.15594v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/1909.07755",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Eberts_M/0/1/0/all/0/1\">Markus Eberts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ulges_A/0/1/0/all/0/1\">Adrian Ulges</a>",
          "description": "We introduce SpERT, an attention model for span-based joint entity and\nrelation extraction. Our key contribution is a light-weight reasoning on BERT\nembeddings, which features entity recognition and filtering, as well as\nrelation classification with a localized, marker-free context representation.\nThe model is trained using strong within-sentence negative samples, which are\nefficiently extracted in a single BERT pass. These aspects facilitate a search\nover all spans in the sentence.\n\nIn ablation studies, we demonstrate the benefits of pre-training, strong\nnegative sampling and localized context. Our model outperforms prior work by up\nto 2.6% F1 score on several datasets for joint entity and relation extraction.",
          "link": "http://arxiv.org/abs/1909.07755",
          "publishedOn": "2021-06-30T02:01:03.529Z",
          "wordCount": 618,
          "title": "Span-based Joint Entity and Relation Extraction with Transformer Pre-training. (arXiv:1909.07755v4 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15580",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Deng_R/0/1/0/all/0/1\">Ruizhi Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brubaker_M/0/1/0/all/0/1\">Marcus A. Brubaker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mori_G/0/1/0/all/0/1\">Greg Mori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lehrmann_A/0/1/0/all/0/1\">Andreas M. Lehrmann</a>",
          "description": "Partial observations of continuous time-series dynamics at arbitrary time\nstamps exist in many disciplines. Fitting this type of data using statistical\nmodels with continuous dynamics is not only promising at an intuitive level but\nalso has practical benefits, including the ability to generate continuous\ntrajectories and to perform inference on previously unseen time stamps. Despite\nexciting progress in this area, the existing models still face challenges in\nterms of their representational power and the quality of their variational\napproximations. We tackle these challenges with continuous latent process flows\n(CLPF), a principled architecture decoding continuous latent processes into\ncontinuous observable processes using a time-dependent normalizing flow driven\nby a stochastic differential equation. To optimize our model using maximum\nlikelihood, we propose a novel piecewise construction of a variational\nposterior process and derive the corresponding variational lower bound using\ntrajectory re-weighting. Our ablation studies demonstrate the effectiveness of\nour contributions in various inference tasks on irregular time grids.\nComparisons to state-of-the-art baselines show our model's favourable\nperformance on both synthetic and real-world time-series data.",
          "link": "http://arxiv.org/abs/2106.15580",
          "publishedOn": "2021-06-30T02:01:03.523Z",
          "wordCount": 601,
          "title": "Continuous Latent Process Flows. (arXiv:2106.15580v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15529",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kosasih_E/0/1/0/all/0/1\">Edward Elson Kosasih</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cabezas_J/0/1/0/all/0/1\">Joaquin Cabezas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sumba_X/0/1/0/all/0/1\">Xavier Sumba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bielak_P/0/1/0/all/0/1\">Piotr Bielak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tagowski_K/0/1/0/all/0/1\">Kamil Tagowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Idanwekhai_K/0/1/0/all/0/1\">Kelvin Idanwekhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tjandra_B/0/1/0/all/0/1\">Benedict Aaron Tjandra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jamasb_A/0/1/0/all/0/1\">Arian Rokkum Jamasb</a>",
          "description": "In order to advance large-scale graph machine learning, the Open Graph\nBenchmark Large Scale Challenge (OGB-LSC) was proposed at the KDD Cup 2021. The\nPCQM4M-LSC dataset defines a molecular HOMO-LUMO property prediction task on\nabout 3.8M graphs. In this short paper, we show our current work-in-progress\nsolution which builds an ensemble of three graph neural networks models based\non GIN, Bayesian Neural Networks and DiffPool. Our approach outperforms the\nprovided baseline by 7.6%. Moreover, using uncertainty in our ensemble's\nprediction, we can identify molecules whose HOMO-LUMO gaps are harder to\npredict (with Pearson's correlation of 0.5181). We anticipate that this will\nfacilitate active learning.",
          "link": "http://arxiv.org/abs/2106.15529",
          "publishedOn": "2021-06-30T02:01:03.518Z",
          "wordCount": 557,
          "title": "On Graph Neural Network Ensembles for Large-Scale Molecular Property Prediction. (arXiv:2106.15529v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15575",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Upadhyay_U/0/1/0/all/0/1\">Uddeshya Upadhyay</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Awate_S/0/1/0/all/0/1\">Suyash Awate</a>",
          "description": "Deep neural networks for image quality enhancement typically need large\nquantities of highly-curated training data comprising pairs of low-quality\nimages and their corresponding high-quality images. While high-quality image\nacquisition is typically expensive and time-consuming, medium-quality images\nare faster to acquire, at lower equipment costs, and available in larger\nquantities. Thus, we propose a novel generative adversarial network (GAN) that\ncan leverage training data at multiple levels of quality (e.g., high and medium\nquality) to improve performance while limiting costs of data curation. We apply\nour mixed-supervision GAN to (i) super-resolve histopathology images and (ii)\nenhance laparoscopy images by combining super-resolution and surgical smoke\nremoval. Results on large clinical and pre-clinical datasets show the benefits\nof our mixed-supervision GAN over the state of the art.",
          "link": "http://arxiv.org/abs/2106.15575",
          "publishedOn": "2021-06-30T02:01:03.503Z",
          "wordCount": 573,
          "title": "A Mixed-Supervision Multilevel GAN Framework for Image Quality Enhancement. (arXiv:2106.15575v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15516",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kwak_B/0/1/0/all/0/1\">Bumju Kwak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jo_J/0/1/0/all/0/1\">Jeonghee Jo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_B/0/1/0/all/0/1\">Byunghan Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1\">Sungroh Yoon</a>",
          "description": "Recently, graph neural networks (GNNs) have achieved remarkable performances\nfor quantum mechanical problems. However, a graph convolution can only cover a\nlocalized region, and cannot capture long-range interactions of atoms. This\nbehavior is contrary to theoretical interatomic potentials, which is a\nfundamental limitation of the spatial based GNNs. In this work, we propose a\nnovel attention-based framework for molecular property prediction tasks. We\nrepresent a molecular conformation as a discrete atomic sequence combined by\natom-atom distance attributes, named Geometry-aware Transformer (GeoT). In\nparticular, we adopt a Transformer architecture, which has been widely used for\nsequential data. Our proposed model trains sequential representations of\nmolecular graphs based on globally constructed attentions, maintaining all\nspatial arrangements of atom pairs. Our method does not suffer from cost\nintensive computations, such as angle calculations. The experimental results on\nseveral public benchmarks and visualization maps verified that keeping the\nlong-range interatomic attributes can significantly improve the model\npredictability.",
          "link": "http://arxiv.org/abs/2106.15516",
          "publishedOn": "2021-06-30T02:01:03.472Z",
          "wordCount": 588,
          "title": "Geometry-aware Transformer for molecular property prediction. (arXiv:2106.15516v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15535",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Jiaqi Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Junwei Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mei_Q/0/1/0/all/0/1\">Qiaozhu Mei</a>",
          "description": "Despite enormous successful applications of graph neural networks (GNNs)\nrecently, theoretical understandings of their generalization ability,\nespecially for node-level tasks where data are not independent and\nidentically-distributed (IID), have been sparse. The theoretical investigation\nof the generalization performance is beneficial for understanding fundamental\nissues (such as fairness) of GNN models and designing better learning methods.\nIn this paper, we present a novel PAC-Bayesian analysis for GNNs under a\nnon-IID semi-supervised learning setup. Moreover, we analyze the generalization\nperformances on different subgroups of unlabeled nodes, which allows us to\nfurther study an accuracy-(dis)parity-style (un)fairness of GNNs from a\ntheoretical perspective. Under reasonable assumptions, we demonstrate that the\ndistance between a test subgroup and the training set can be a key factor\naffecting the GNN performance on that subgroup, which calls special attention\nto the training node selection for fair learning. Experiments across multiple\nGNN models and datasets support our theoretical results.",
          "link": "http://arxiv.org/abs/2106.15535",
          "publishedOn": "2021-06-30T02:01:03.467Z",
          "wordCount": 582,
          "title": "Subgroup Generalization and Fairness of Graph Neural Networks. (arXiv:2106.15535v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15448",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Robinson_C/0/1/0/all/0/1\">Caleb Robinson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ortiz_A/0/1/0/all/0/1\">Anthony Ortiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hughey_L/0/1/0/all/0/1\">Lacey Hughey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stabach_J/0/1/0/all/0/1\">Jared A. Stabach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferres_J/0/1/0/all/0/1\">Juan M. Lavista Ferres</a>",
          "description": "Localizing and counting large ungulates -- hoofed mammals like cows and elk\n-- in very high-resolution satellite imagery is an important task for\nsupporting ecological studies. Prior work has shown that this is feasible with\ndeep learning based methods and sub-meter multi-spectral satellite imagery. We\nextend this line of work by proposing a baseline method, CowNet, that\nsimultaneously estimates the number of animals in an image (counts), as well as\npredicts their location at a pixel level (localizes). We also propose an\nmethodology for evaluating such models on counting and localization tasks\nacross large scenes that takes the uncertainty of noisy labels and the\ninformation needed by stakeholders in ecological monitoring tasks into account.\nFinally, we benchmark our baseline method with state of the art vision methods\nfor counting objects in scenes. We specifically test the temporal\ngeneralization of the resulting models over a large landscape in Point Reyes\nSeashore, CA. We find that the LC-FCN model performs the best and achieves an\naverage precision between 0.56 and 0.61 and an average recall between 0.78 and\n0.92 over three held out test scenes.",
          "link": "http://arxiv.org/abs/2106.15448",
          "publishedOn": "2021-06-30T02:01:03.458Z",
          "wordCount": 638,
          "title": "Detecting Cattle and Elk in the Wild from Space. (arXiv:2106.15448v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15563",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kivva_B/0/1/0/all/0/1\">Bohdan Kivva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajendran_G/0/1/0/all/0/1\">Goutham Rajendran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravikumar_P/0/1/0/all/0/1\">Pradeep Ravikumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aragam_B/0/1/0/all/0/1\">Bryon Aragam</a>",
          "description": "We study the problem of reconstructing a causal graphical model from data in\nthe presence of latent variables. The main problem of interest is recovering\nthe causal structure over the latent variables while allowing for general,\npotentially nonlinear dependence between the variables. In many practical\nproblems, the dependence between raw observations (e.g. pixels in an image) is\nmuch less relevant than the dependence between certain high-level, latent\nfeatures (e.g. concepts or objects), and this is the setting of interest. We\nprovide conditions under which both the latent representations and the\nunderlying latent causal model are identifiable by a reduction to a mixture\noracle. The proof is constructive, and leads to several algorithms for\nexplicitly reconstructing the full graphical model. We discuss efficient\nalgorithms and provide experiments illustrating the algorithms in practice.",
          "link": "http://arxiv.org/abs/2106.15563",
          "publishedOn": "2021-06-30T02:01:03.449Z",
          "wordCount": 571,
          "title": "Learning latent causal graphs via mixture oracles. (arXiv:2106.15563v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15397",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Nikitin_N/0/1/0/all/0/1\">Nikolay O. Nikitin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vychuzhanin_P/0/1/0/all/0/1\">Pavel Vychuzhanin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarafanov_M/0/1/0/all/0/1\">Mikhail Sarafanov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Polonskaia_I/0/1/0/all/0/1\">Iana S. Polonskaia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Revin_I/0/1/0/all/0/1\">Ilia Revin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barabanova_I/0/1/0/all/0/1\">Irina V. Barabanova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maximov_G/0/1/0/all/0/1\">Gleb Maximov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalyuzhnaya_A/0/1/0/all/0/1\">Anna V. Kalyuzhnaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boukhanovsky_A/0/1/0/all/0/1\">Alexander Boukhanovsky</a>",
          "description": "The effectiveness of the machine learning methods for real-world tasks\ndepends on the proper structure of the modeling pipeline. The proposed approach\nis aimed to automate the design of composite machine learning pipelines, which\nis equivalent to computation workflows that consist of models and data\noperations. The approach combines key ideas of both automated machine learning\nand workflow management systems. It designs the pipelines with a customizable\ngraph-based structure, analyzes the obtained results, and reproduces them. The\nevolutionary approach is used for the flexible identification of pipeline\nstructure. The additional algorithms for sensitivity analysis, atomization, and\nhyperparameter tuning are implemented to improve the effectiveness of the\napproach. Also, the software implementation on this approach is presented as an\nopen-source framework. The set of experiments is conducted for the different\ndatasets and tasks (classification, regression, time series forecasting). The\nobtained results confirm the correctness and effectiveness of the proposed\napproach in the comparison with the state-of-the-art competitors and baseline\nsolutions.",
          "link": "http://arxiv.org/abs/2106.15397",
          "publishedOn": "2021-06-30T02:01:03.435Z",
          "wordCount": 609,
          "title": "Automated Evolutionary Approach for the Design of Composite Machine Learning Pipelines. (arXiv:2106.15397v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15409",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Symeonidis_C/0/1/0/all/0/1\">C. Symeonidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nousi_P/0/1/0/all/0/1\">P. Nousi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tosidis_P/0/1/0/all/0/1\">P. Tosidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsampazis_K/0/1/0/all/0/1\">K. Tsampazis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Passalis_N/0/1/0/all/0/1\">N. Passalis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tefas_A/0/1/0/all/0/1\">A. Tefas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nikolaidis_N/0/1/0/all/0/1\">N. Nikolaidis</a>",
          "description": "The performance of supervised deep learning algorithms depends significantly\non the scale, quality and diversity of the data used for their training.\nCollecting and manually annotating large amount of data can be both\ntime-consuming and costly tasks to perform. In the case of tasks related to\nvisual human-centric perception, the collection and distribution of such data\nmay also face restrictions due to legislation regarding privacy. In addition,\nthe design and testing of complex systems, e.g., robots, which often employ\ndeep learning-based perception models, may face severe difficulties as even\nstate-of-the-art methods trained on real and large-scale datasets cannot always\nperform adequately as they have not adapted to the visual differences between\nthe virtual and the real world data. As an attempt to tackle and mitigate the\neffect of these issues, we present a method that automatically generates\nrealistic synthetic data with annotations for a) person detection, b) face\nrecognition, and c) human pose estimation. The proposed method takes as input\nreal background images and populates them with human figures in various poses.\nInstead of using hand-made 3D human models, we propose the use of models\ngenerated through deep learning methods, further reducing the dataset creation\ncosts, while maintaining a high level of realism. In addition, we provide\nopen-source and easy to use tools that implement the proposed pipeline,\nallowing for generating highly-realistic synthetic datasets for a variety of\ntasks. A benchmarking and evaluation in the corresponding tasks shows that\nsynthetic data can be effectively used as a supplement to real data.",
          "link": "http://arxiv.org/abs/2106.15409",
          "publishedOn": "2021-06-30T02:01:03.430Z",
          "wordCount": 700,
          "title": "Efficient Realistic Data Generation Framework leveraging Deep Learning-based Human Digitization. (arXiv:2106.15409v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15412",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shuhan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Fan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_C/0/1/0/all/0/1\">Changhao Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Dian Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1\">Xuan Zeng</a>",
          "description": "Bayesian optimization is a promising methodology for analog circuit\nsynthesis. However, the sequential nature of the Bayesian optimization\nframework significantly limits its ability to fully utilize real-world\ncomputational resources. In this paper, we propose an efficient parallelizable\nBayesian optimization algorithm via Multi-objective ACquisition function\nEnsemble (MACE) to further accelerate the optimization procedure. By sampling\nquery points from the Pareto front of the probability of improvement (PI),\nexpected improvement (EI) and lower confidence bound (LCB), we combine the\nbenefits of state-of-the-art acquisition functions to achieve a delicate\ntradeoff between exploration and exploitation for the unconstrained\noptimization problem. Based on this batch design, we further adjust the\nalgorithm for the constrained optimization problem. By dividing the\noptimization procedure into two stages and first focusing on finding an initial\nfeasible point, we manage to gain more information about the valid region and\ncan better avoid sampling around the infeasible area. After achieving the first\nfeasible point, we favor the feasible region by adopting a specially designed\npenalization term to the acquisition function ensemble. The experimental\nresults quantitatively demonstrate that our proposed algorithm can reduce the\noverall simulation time by up to 74 times compared to differential evolution\n(DE) for the unconstrained optimization problem when the batch size is 15. For\nthe constrained optimization problem, our proposed algorithm can speed up the\noptimization process by up to 15 times compared to the weighted expected\nimprovement based Bayesian optimization (WEIBO) approach, when the batch size\nis 15.",
          "link": "http://arxiv.org/abs/2106.15412",
          "publishedOn": "2021-06-30T02:01:03.412Z",
          "wordCount": 701,
          "title": "An Efficient Batch Constrained Bayesian Optimization Approach for Analog Circuit Synthesis via Multi-objective Acquisition Ensemble. (arXiv:2106.15412v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15367",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kao_C/0/1/0/all/0/1\">Chia-Hsiang Kao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiu_W/0/1/0/all/0/1\">Wei-Chen Chiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Pin-Yu Chen</a>",
          "description": "Model-agnostic meta-learning (MAML) is one of the most popular and\nwidely-adopted meta-learning algorithms nowadays, which achieves remarkable\nsuccess in various learning problems. Yet, with the unique design of nested\ninner-loop and outer-loop updates which respectively govern the task-specific\nand meta-model-centric learning, the underlying learning objective of MAML\nstill remains implicit and thus impedes a more straightforward understanding of\nit. In this paper, we provide a new perspective to the working mechanism of\nMAML and discover that: MAML is analogous to a meta-learner using a supervised\ncontrastive objective function, where the query features are pulled towards the\nsupport features of the same class and against those of different classes, in\nwhich such contrastiveness is experimentally verified via an analysis based on\nthe cosine similarity. Moreover, our analysis reveals that the vanilla MAML\nalgorithm has an undesirable interference term originating from the random\ninitialization and the cross-task interaction. We therefore propose a simple\nbut effective technique, zeroing trick, to alleviate such interference, where\nthe extensive experiments are then conducted on both miniImagenet and Omniglot\ndatasets to demonstrate the consistent improvement brought by our proposed\ntechnique thus well validating its effectiveness.",
          "link": "http://arxiv.org/abs/2106.15367",
          "publishedOn": "2021-06-30T02:01:03.406Z",
          "wordCount": 621,
          "title": "MAML is a Noisy Contrastive Learner. (arXiv:2106.15367v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15503",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Miani_M/0/1/0/all/0/1\">Marco Miani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parton_M/0/1/0/all/0/1\">Maurizio Parton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Romito_M/0/1/0/all/0/1\">Marco Romito</a>",
          "description": "Having access to an exploring restart distribution (the so-called wide\ncoverage assumption) is critical with policy gradient methods. This is due to\nthe fact that, while the objective function is insensitive to updates in\nunlikely states, the agent may still need improvements in those states in order\nto reach a nearly optimal payoff. For this reason, wide coverage is used in\nsome form when analyzing theoretical properties of practical policy gradient\nmethods. However, this assumption can be unfeasible in certain environments,\nfor instance when learning is online, or when restarts are possible only from a\nfixed initial state. In these cases, classical policy gradient algorithms can\nhave very poor convergence properties and sample efficiency. In this paper, we\ndevelop Curious Explorer, a novel and simple iterative state space exploration\nstrategy that can be used with any starting distribution $\\rho$. Curious\nExplorer starts from $\\rho$, then using intrinsic rewards assigned to the set\nof poorly visited states produces a sequence of policies, each one more\nexploratory than the previous one in an informed way, and finally outputs a\nrestart model $\\mu$ based on the state visitation distribution of the\nexploratory policies. Curious Explorer is provable, in the sense that we\nprovide theoretical upper bounds on how often an optimal policy visits poorly\nvisited states. These bounds can be used to prove PAC convergence and sample\nefficiency results when a PAC optimizer is plugged in Curious Explorer. This\nallows to achieve global convergence and sample efficiency results without any\ncoverage assumption for REINFORCE, and potentially for any other policy\ngradient method ensuring PAC convergence with wide coverage. Finally, we plug\n(the output of) Curious Explorer into REINFORCE and TRPO, and show empirically\nthat it can improve performance in MDPs with challenging exploration.",
          "link": "http://arxiv.org/abs/2106.15503",
          "publishedOn": "2021-06-30T02:01:03.400Z",
          "wordCount": 726,
          "title": "Curious Explorer: a provable exploration strategy in Policy Learning. (arXiv:2106.15503v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15380",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Infante_G/0/1/0/all/0/1\">Guillermo Infante</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jonsso_A/0/1/0/all/0/1\">Anders Jonsso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gomez_V/0/1/0/all/0/1\">Vicen&#xe7; G&#xf3;mez</a>",
          "description": "In this work we present a novel approach to hierarchical reinforcement\nlearning for linearly-solvable Markov decision processes. Our approach assumes\nthat the state space is partitioned, and the subtasks consist in moving between\nthe partitions. We represent value functions on several levels of abstraction,\nand use the compositionality of subtasks to estimate the optimal values of the\nstates in each partition. The policy is implicitly defined on these optimal\nvalue estimates, rather than being decomposed among the subtasks. As a\nconsequence, our approach can learn the globally optimal policy, and does not\nsuffer from the non-stationarity of high-level decisions. If several partitions\nhave equivalent dynamics, the subtasks of those partitions can be shared. If\nthe set of boundary states is smaller than the entire state space, our approach\ncan have significantly smaller sample complexity than that of a flat learner,\nand we validate this empirically in several experiments.",
          "link": "http://arxiv.org/abs/2106.15380",
          "publishedOn": "2021-06-30T02:01:03.384Z",
          "wordCount": 583,
          "title": "Globally Optimal Hierarchical Reinforcement Learning for Linearly-Solvable Markov Decision Processes. (arXiv:2106.15380v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15325",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hafiz_A/0/1/0/all/0/1\">Abdul Mueed Hafiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhat_R/0/1/0/all/0/1\">Rouf Ul Alam Bhat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parah_S/0/1/0/all/0/1\">Shabir Ahmad Parah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassaballah_M/0/1/0/all/0/1\">M. Hassaballah</a>",
          "description": "3D model generation from single 2D RGB images is a challenging and actively\nresearched computer vision task. Various techniques using conventional network\narchitectures have been proposed for the same. However, the body of research\nwork is limited and there are various issues like using inefficient 3D\nrepresentation formats, weak 3D model generation backbones, inability to\ngenerate dense point clouds, dependence of post-processing for generation of\ndense point clouds, and dependence on silhouettes in RGB images. In this paper,\na novel 2D RGB image to point cloud conversion technique is proposed, which\nimproves the state of art in the field due to its efficient, robust and simple\nmodel by using the concept of parallelization in network architecture. It not\nonly uses the efficient and rich 3D representation of point clouds, but also\nuses a novel and robust point cloud generation backbone in order to address the\nprevalent issues. This involves using a single-encoder multiple-decoder deep\nnetwork architecture wherein each decoder generates certain fixed viewpoints.\nThis is followed by fusing all the viewpoints to generate a dense point cloud.\nVarious experiments are conducted on the technique and its performance is\ncompared with those of other state of the art techniques and impressive gains\nin performance are demonstrated. Code is available at\nhttps://github.com/mueedhafiz1982/",
          "link": "http://arxiv.org/abs/2106.15325",
          "publishedOn": "2021-06-30T02:01:03.377Z",
          "wordCount": 672,
          "title": "SE-MD: A Single-encoder multiple-decoder deep network for point cloud generation from 2D images. (arXiv:2106.15325v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15356",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Liwei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyer_A/0/1/0/all/0/1\">Akshay Iyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yerramilli_S/0/1/0/all/0/1\">Suraj Yerramilli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Apley_D/0/1/0/all/0/1\">Daniel Apley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_P/0/1/0/all/0/1\">Ping Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wei Chen</a>",
          "description": "Scientific and engineering problems often require the use of artificial\nintelligence to aid understanding and the search for promising designs. While\nGaussian processes (GP) stand out as easy-to-use and interpretable learners,\nthey have difficulties in accommodating big datasets, categorical inputs, and\nmultiple responses, which has become a common challenge for a growing number of\ndata-driven design applications. In this paper, we propose a GP model that\nutilizes latent variables and functions obtained through variational inference\nto address the aforementioned challenges simultaneously. The method is built\nupon the latent variable Gaussian process (LVGP) model where categorical\nfactors are mapped into a continuous latent space to enable GP modeling of\nmixed-variable datasets. By extending variational inference to LVGP models, the\nlarge training dataset is replaced by a small set of inducing points to address\nthe scalability issue. Output response vectors are represented by a linear\ncombination of independent latent functions, forming a flexible kernel\nstructure to handle multiple responses that might have distinct behaviors.\nComparative studies demonstrate that the proposed method scales well for large\ndatasets with over 10^4 data points, while outperforming state-of-the-art\nmachine learning methods without requiring much hyperparameter tuning. In\naddition, an interpretable latent space is obtained to draw insights into the\neffect of categorical factors, such as those associated with building blocks of\narchitectures and element choices in metamaterial and materials design. Our\napproach is demonstrated for machine learning of ternary oxide materials and\ntopology optimization of a multiscale compliant mechanism with aperiodic\nmicrostructures and multiple materials.",
          "link": "http://arxiv.org/abs/2106.15356",
          "publishedOn": "2021-06-30T02:01:03.372Z",
          "wordCount": 703,
          "title": "Scalable Gaussian Processes for Data-Driven Design using Big Data with Categorical Factors. (arXiv:2106.15356v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15343",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Maniar_T/0/1/0/all/0/1\">Tabish Maniar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akkinepally_A/0/1/0/all/0/1\">Alekhya Akkinepally</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Anantha Sharma</a>",
          "description": "The use of machine learning algorithms to model user behavior and drive\nbusiness decisions has become increasingly commonplace, specifically providing\nintelligent recommendations to automated decision making. This has led to an\nincrease in the use of customers personal data to analyze customer behavior and\npredict their interests in a companys products. Increased use of this customer\npersonal data can lead to better models but also to the potential of customer\ndata being leaked, reverse engineered, and mishandled. In this paper, we assess\ndifferential privacy as a solution to address these privacy problems by\nbuilding privacy protections into the data engineering and model training\nstages of predictive model development. Our interest is a pragmatic\nimplementation in an operational environment, which necessitates a general\npurpose differentially private modeling framework, and we evaluate one such\ntool from LeapYear as applied to the Credit Risk modeling domain. Credit Risk\nModel is a major modeling methodology in banking and finance where user data is\nanalyzed to determine the total Expected Loss to the bank. We examine the\napplication of differential privacy on the credit risk model and evaluate the\nperformance of a Differentially Private Model with a Non Differentially Private\nModel. Credit Risk Model is a major modeling methodology in banking and finance\nwhere users data is analyzed to determine the total Expected Loss to the bank.\nIn this paper, we explore the application of differential privacy on the credit\nrisk model and evaluate the performance of a Non Differentially Private Model\nwith Differentially Private Model.",
          "link": "http://arxiv.org/abs/2106.15343",
          "publishedOn": "2021-06-30T02:01:03.366Z",
          "wordCount": 697,
          "title": "Differential Privacy for Credit Risk Model. (arXiv:2106.15343v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15305",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zehni_M/0/1/0/all/0/1\">Mona Zehni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Shaona Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sridhar_K/0/1/0/all/0/1\">Krishna Sridhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raman_S/0/1/0/all/0/1\">Sethu Raman</a>",
          "description": "Inverse rendering is the problem of decomposing an image into its intrinsic\ncomponents, i.e. albedo, normal and lighting. To solve this ill-posed problem\nfrom single image, state-of-the-art methods in shape from shading mostly resort\nto supervised training on all the components on either synthetic or real\ndatasets. Here, we propose a new self-supervised training paradigm that 1)\nreduces the need for full supervision on the decomposition task and 2) takes\ninto account the relighting task. We introduce new self-supervised loss terms\nthat leverage the consistencies between multi-lit images (images of the same\nscene under different illuminations). Our approach is applicable to multi-lit\ndatasets. We apply our training approach in two settings: 1) train on a mixture\nof synthetic and real data, 2) train on real datasets with limited supervision.\nWe show-case the effectiveness of our training paradigm on both intrinsic\ndecomposition and relighting and demonstrate how the model struggles in both\ntasks without the self-supervised loss terms in limited supervision settings.\nWe provide results of comprehensive experiments on SfSNet, CelebA and Photoface\ndatasets and verify the performance of our approach on images in the wild.",
          "link": "http://arxiv.org/abs/2106.15305",
          "publishedOn": "2021-06-30T02:01:03.361Z",
          "wordCount": 625,
          "title": "Joint Learning of Portrait Intrinsic Decomposition and Relighting. (arXiv:2106.15305v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15339",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xinyun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maniatis_P/0/1/0/all/0/1\">Petros Maniatis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1\">Rishabh Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sutton_C/0/1/0/all/0/1\">Charles Sutton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_H/0/1/0/all/0/1\">Hanjun Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1\">Max Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1\">Denny Zhou</a>",
          "description": "Spreadsheet formula prediction has been an important program synthesis\nproblem with many real-world applications. Previous works typically utilize\ninput-output examples as the specification for spreadsheet formula synthesis,\nwhere each input-output pair simulates a separate row in the spreadsheet.\nHowever, this formulation does not fully capture the rich context in real-world\nspreadsheets. First, spreadsheet data entries are organized as tables, thus\nrows and columns are not necessarily independent from each other. In addition,\nmany spreadsheet tables include headers, which provide high-level descriptions\nof the cell data. However, previous synthesis approaches do not consider\nheaders as part of the specification. In this work, we present the first\napproach for synthesizing spreadsheet formulas from tabular context, which\nincludes both headers and semi-structured tabular data. In particular, we\npropose SpreadsheetCoder, a BERT-based model architecture to represent the\ntabular context in both row-based and column-based formats. We train our model\non a large dataset of spreadsheets, and demonstrate that SpreadsheetCoder\nachieves top-1 prediction accuracy of 42.51%, which is a considerable\nimprovement over baselines that do not employ rich tabular context. Compared to\nthe rule-based system, SpreadsheetCoder assists 82% more users in composing\nformulas on Google Sheets.",
          "link": "http://arxiv.org/abs/2106.15339",
          "publishedOn": "2021-06-30T02:01:03.346Z",
          "wordCount": 636,
          "title": "SpreadsheetCoder: Formula Prediction from Semi-structured Context. (arXiv:2106.15339v1 [cs.SE])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15365",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Graafland_C/0/1/0/all/0/1\">Catharina Elisabeth Graafland</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Gutierrez_J/0/1/0/all/0/1\">Jos&#xe9; Manuel Guti&#xe9;rrez</a>",
          "description": "Gene expression datasets consist of thousand of genes with relatively small\nsamplesizes (i.e. are large-$p$-small-$n$). Moreover, dependencies of various\norders co-exist in the datasets. In the Undirected probabilistic Graphical\nModel (UGM) framework the Glasso algorithm has been proposed to deal with high\ndimensional micro-array datasets forcing sparsity. Also, modifications of the\ndefault Glasso algorithm are developed to overcome the problem of complex\ninteraction structure. In this work we advocate the use of a simple score-based\nHill Climbing algorithm (HC) that learns Gaussian Bayesian Networks (BNs)\nleaning on Directed Acyclic Graphs (DAGs). We compare HC with Glasso and its\nmodifications in the UGM framework on their capability to reconstruct GRNs from\nmicro-array data belonging to the Escherichia Coli genome. We benefit from the\nanalytical properties of the Joint Probability Density (JPD) function on which\nboth directed and undirected PGMs build to convert DAGs to UGMs.\n\nWe conclude that dependencies in complex data are learned best by the HC\nalgorithm, presenting them most accurately and efficiently, simultaneously\nmodelling strong local and weaker but significant global connections coexisting\nin the gene expression dataset. The HC algorithm adapts intrinsically to the\ncomplex dependency structure of the dataset, without forcing a specific\nstructure in advance. On the contrary, Glasso and modifications model\nunnecessary dependencies at the expense of the probabilistic information in the\nnetwork and of a structural bias in the JPD function that can only be relieved\nincluding many parameters.",
          "link": "http://arxiv.org/abs/2106.15365",
          "publishedOn": "2021-06-30T02:01:03.340Z",
          "wordCount": 696,
          "title": "Learning complex dependency structure of gene regulatory networks from high dimensional micro-array data with Gaussian Bayesian networks. (arXiv:2106.15365v1 [q-bio.MN])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15373",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Demir_C/0/1/0/all/0/1\">Caglar Demir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ngomo_A/0/1/0/all/0/1\">Axel-Cyrille Ngonga Ngomo</a>",
          "description": "Approaches based on refinement operators have been successfully applied to\nclass expression learning on RDF knowledge graphs. These approaches often need\nto explore a large number of concepts to find adequate hypotheses. This need\narguably stems from current approaches relying on myopic heuristic functions to\nguide their search through an infinite concept space. In turn, deep\nreinforcement learning provides effective means to address myopia by estimating\nhow much discounted cumulated future reward states promise. In this work, we\nleverage deep reinforcement learning to accelerate the learning of concepts in\n$\\mathcal{ALC}$ by proposing DRILL -- a novel class expression learning\napproach that uses a convolutional deep Q-learning model to steer its search.\nBy virtue of its architecture, DRILL is able to compute the expected discounted\ncumulated future reward of more than $10^3$ class expressions in a second on\nstandard hardware. We evaluate DRILL on four benchmark datasets against\nstate-of-the-art approaches. Our results suggest that DRILL converges to goal\nstates at least 2.7$\\times$ faster than state-of-the-art models on all\nbenchmark datasets. We provide an open-source implementation of our approach,\nincluding training and evaluation scripts as well as pre-trained models.",
          "link": "http://arxiv.org/abs/2106.15373",
          "publishedOn": "2021-06-30T02:01:03.334Z",
          "wordCount": 626,
          "title": "DRILL-- Deep Reinforcement Learning for Refinement Operators in $\\mathcal{ALC}$. (arXiv:2106.15373v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15349",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chatterjee_S/0/1/0/all/0/1\">Sayan Chatterjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hanawal_M/0/1/0/all/0/1\">Manjesh K. Hanawal</a>",
          "description": "Critical role of Internet of Things (IoT) in various domains like smart city,\nhealthcare, supply chain and transportation has made them the target of\nmalicious attacks. Past works in this area focused on centralized Intrusion\nDetection System (IDS), assuming the existence of a central entity to perform\ndata analysis and identify threats. However, such IDS may not always be\nfeasible, mainly due to spread of data across multiple sources and gathering at\ncentral node can be costly. Also, the earlier works primarily focused on\nimproving True Positive Rate (TPR) and ignored the False Positive Rate (FPR),\nwhich is also essential to avoid unnecessary downtime of the systems. In this\npaper, we first present an architecture for IDS based on hybrid ensemble model,\nnamed PHEC, which gives improved performance compared to state-of-the-art\narchitectures. We then adapt this model to a federated learning framework that\nperforms local training and aggregates only the model parameters. Next, we\npropose Noise-Tolerant PHEC in centralized and federated settings to address\nthe label-noise problem. The proposed idea uses classifiers using weighted\nconvex surrogate loss functions. Natural robustness of KNN classifier towards\nnoisy data is also used in the proposed architecture. Experimental results on\nfour benchmark datasets drawn from various security attacks show that our model\nachieves high TPR while keeping FPR low on noisy and clean data. Further, they\nalso demonstrate that the hybrid ensemble models achieve performance in\nfederated settings close to that of the centralized settings.",
          "link": "http://arxiv.org/abs/2106.15349",
          "publishedOn": "2021-06-30T02:01:03.329Z",
          "wordCount": 691,
          "title": "Federated Learning for Intrusion Detection in IoT Security: A Hybrid Ensemble Approach. (arXiv:2106.15349v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15345",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yuanqi Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quan_Q/0/1/0/all/0/1\">Quan Quan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_H/0/1/0/all/0/1\">Hu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">S. Kevin Zhou</a>",
          "description": "Pseudo-normality synthesis, which computationally generates a pseudo-normal\nimage from an abnormal one (e.g., with lesions), is critical in many\nperspectives, from lesion detection, data augmentation to clinical surgery\nsuggestion. However, it is challenging to generate high-quality pseudo-normal\nimages in the absence of the lesion information. Thus, expensive lesion\nsegmentation data have been introduced to provide lesion information for the\ngenerative models and improve the quality of the synthetic images. In this\npaper, we aim to alleviate the need of a large amount of lesion segmentation\ndata when generating pseudo-normal images. We propose a Semi-supervised Medical\nImage generative LEarning network (SMILE) which not only utilizes limited\nmedical images with segmentation masks, but also leverages massive medical\nimages without segmentation masks to generate realistic pseudo-normal images.\nExtensive experiments show that our model outperforms the best state-of-the-art\nmodel by up to 6% for data augmentation task and 3% in generating high-quality\nimages. Moreover, the proposed semi-supervised learning achieves comparable\nmedical image synthesis quality with supervised learning model, using only 50\nof segmentation data.",
          "link": "http://arxiv.org/abs/2106.15345",
          "publishedOn": "2021-06-30T02:01:03.324Z",
          "wordCount": 623,
          "title": "Where is the disease? Semi-supervised pseudo-normality synthesis from an abnormal image. (arXiv:2106.15345v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15341",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vasata_D/0/1/0/all/0/1\">Daniel Va&#x161;ata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Halama_T/0/1/0/all/0/1\">Tom&#xe1;&#x161; Halama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Friedjungova_M/0/1/0/all/0/1\">Magda Friedjungov&#xe1;</a>",
          "description": "Image inpainting is one of the important tasks in computer vision which\nfocuses on the reconstruction of missing regions in an image. The aim of this\npaper is to introduce an image inpainting model based on Wasserstein Generative\nAdversarial Imputation Network. The generator network of the model uses\nbuilding blocks of convolutional layers with different dilation rates, together\nwith skip connections that help the model reproduce fine details of the output.\nThis combination yields a universal imputation model that is able to handle\nvarious scenarios of missingness with sufficient quality. To show this\nexperimentally, the model is simultaneously trained to deal with three\nscenarios given by missing pixels at random, missing various smaller square\nregions, and one missing square placed in the center of the image. It turns out\nthat our model achieves high-quality inpainting results on all scenarios.\nPerformance is evaluated using peak signal-to-noise ratio and structural\nsimilarity index on two real-world benchmark datasets, CelebA faces and Paris\nStreetView. The results of our model are compared to biharmonic imputation and\nto some of the other state-of-the-art image inpainting methods.",
          "link": "http://arxiv.org/abs/2106.15341",
          "publishedOn": "2021-06-30T02:01:03.309Z",
          "wordCount": 634,
          "title": "Image Inpainting Using Wasserstein Generative Adversarial Imputation Network. (arXiv:2106.15341v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15335",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luo_T/0/1/0/all/0/1\">Tao Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_M/0/1/0/all/0/1\">Mingen Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tholoniat_P/0/1/0/all/0/1\">Pierre Tholoniat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cidon_A/0/1/0/all/0/1\">Asaf Cidon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geambasu_R/0/1/0/all/0/1\">Roxana Geambasu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lecuyer_M/0/1/0/all/0/1\">Mathias L&#xe9;cuyer</a>",
          "description": "Machine learning (ML) models trained on personal data have been shown to leak\ninformation about users. Differential privacy (DP) enables model training with\na guaranteed bound on this leakage. Each new model trained with DP increases\nthe bound on data leakage and can be seen as consuming part of a global privacy\nbudget that should not be exceeded. This budget is a scarce resource that must\nbe carefully managed to maximize the number of successfully trained models.\n\nWe describe PrivateKube, an extension to the popular Kubernetes datacenter\norchestrator that adds privacy as a new type of resource to be managed\nalongside other traditional compute resources, such as CPU, GPU, and memory.\nThe abstractions we design for the privacy resource mirror those defined by\nKubernetes for traditional resources, but there are also major differences. For\nexample, traditional compute resources are replenishable while privacy is not:\na CPU can be regained after a model finishes execution while privacy budget\ncannot. This distinction forces a re-design of the scheduler. We present DPF\n(Dominant Private Block Fairness) -- a variant of the popular Dominant Resource\nFairness (DRF) algorithm -- that is geared toward the non-replenishable privacy\nresource but enjoys similar theoretical properties as DRF.\n\nWe evaluate PrivateKube and DPF on microbenchmarks and an ML workload on\nAmazon Reviews data. Compared to existing baselines, DPF allows training more\nmodels under the same global privacy guarantee. This is especially true for DPF\nover R\\'enyi DP, a highly composable form of DP.",
          "link": "http://arxiv.org/abs/2106.15335",
          "publishedOn": "2021-06-30T02:01:03.304Z",
          "wordCount": 702,
          "title": "Privacy Budget Scheduling. (arXiv:2106.15335v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15338",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gabbur_P/0/1/0/all/0/1\">Prasad Gabbur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bilkhu_M/0/1/0/all/0/1\">Manjot Bilkhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Movellan_J/0/1/0/all/0/1\">Javier Movellan</a>",
          "description": "We provide a probabilistic interpretation of attention and show that the\nstandard dot-product attention in transformers is a special case of Maximum A\nPosteriori (MAP) inference. The proposed approach suggests the use of\nExpectation Maximization algorithms for online adaptation of key and value\nmodel parameters. This approach is useful for cases in which external agents,\ne.g., annotators, provide inference-time information about the correct values\nof some tokens, e.g, the semantic category of some pixels, and we need for this\nnew information to propagate to other tokens in a principled manner. We\nillustrate the approach on an interactive semantic segmentation task in which\nannotators and models collaborate online to improve annotation efficiency.\nUsing standard benchmarks, we observe that key adaptation boosts model\nperformance ($\\sim10\\%$ mIoU) in the low feedback regime and value propagation\nimproves model responsiveness in the high feedback regime. A PyTorch layer\nimplementation of our probabilistic attention model will be made publicly\navailable.",
          "link": "http://arxiv.org/abs/2106.15338",
          "publishedOn": "2021-06-30T02:01:03.298Z",
          "wordCount": 602,
          "title": "Probabilistic Attention for Interactive Segmentation. (arXiv:2106.15338v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15358",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Liu_Z/0/1/0/all/0/1\">Zhaoqiang Liu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ghosh_S/0/1/0/all/0/1\">Subhroshekhar Ghosh</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Scarlett_J/0/1/0/all/0/1\">Jonathan Scarlett</a>",
          "description": "Compressive phase retrieval is a popular variant of the standard compressive\nsensing problem, in which the measurements only contain magnitude information.\nIn this paper, motivated by recent advances in deep generative models, we\nprovide recovery guarantees with order-optimal sample complexity bounds for\nphase retrieval with generative priors. We first show that when using i.i.d.\nGaussian measurements and an $L$-Lipschitz continuous generative model with\nbounded $k$-dimensional inputs, roughly $O(k \\log L)$ samples suffice to\nguarantee that the signal is close to any vector that minimizes an\namplitude-based empirical loss function. Attaining this sample complexity with\na practical algorithm remains a difficult challenge, and a popular spectral\ninitialization method has been observed to pose a major bottleneck. To\npartially address this, we further show that roughly $O(k \\log L)$ samples\nensure sufficient closeness between the signal and any {\\em globally optimal}\nsolution to an optimization problem designed for spectral initialization\n(though finding such a solution may still be challenging). We adapt this result\nto sparse phase retrieval, and show that $O(s \\log n)$ samples are sufficient\nfor a similar guarantee when the underlying signal is $s$-sparse and\n$n$-dimensional, matching an information-theoretic lower bound. While our\nguarantees do not directly correspond to a practical algorithm, we propose a\npractical spectral initialization method motivated by our findings, and\nexperimentally observe significant performance gains over various existing\nspectral initialization methods of sparse phase retrieval.",
          "link": "http://arxiv.org/abs/2106.15358",
          "publishedOn": "2021-06-30T02:01:03.291Z",
          "wordCount": 670,
          "title": "Towards Sample-Optimal Compressive Phase Retrieval with Sparse and Generative Priors. (arXiv:2106.15358v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15379",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Ghojogh_B/0/1/0/all/0/1\">Benyamin Ghojogh</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ghodsi_A/0/1/0/all/0/1\">Ali Ghodsi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Karray_F/0/1/0/all/0/1\">Fakhri Karray</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Crowley_M/0/1/0/all/0/1\">Mark Crowley</a>",
          "description": "This is a tutorial and survey paper on unification of spectral dimensionality\nreduction methods, kernel learning by Semidefinite Programming (SDP), Maximum\nVariance Unfolding (MVU) or Semidefinite Embedding (SDE), and its variants. We\nfirst explain how the spectral dimensionality reduction methods can be unified\nas kernel Principal Component Analysis (PCA) with different kernels. This\nunification can be interpreted as eigenfunction learning or representation of\nkernel in terms of distance matrix. Then, since the spectral methods are\nunified as kernel PCA, we say let us learn the best kernel for unfolding the\nmanifold of data to its maximum variance. We first briefly introduce kernel\nlearning by SDP for the transduction task. Then, we explain MVU in detail.\nVarious versions of supervised MVU using nearest neighbors graph, by class-wise\nunfolding, by Fisher criterion, and by colored MVU are explained. We also\nexplain out-of-sample extension of MVU using eigenfunctions and kernel mapping.\nFinally, we introduce other variants of MVU including action respecting\nembedding, relaxed MVU, and landmark MVU for big data.",
          "link": "http://arxiv.org/abs/2106.15379",
          "publishedOn": "2021-06-30T02:01:03.281Z",
          "wordCount": 644,
          "title": "Unified Framework for Spectral Dimensionality Reduction, Maximum Variance Unfolding, and Kernel Learning By Semidefinite Programming: Tutorial and Survey. (arXiv:2106.15379v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15498",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hagerer_G/0/1/0/all/0/1\">Gerhard Hagerer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_W/0/1/0/all/0/1\">Wenbin Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danner_H/0/1/0/all/0/1\">Hannah Danner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Groh_G/0/1/0/all/0/1\">Georg Groh</a>",
          "description": "Social media offer plenty of information to perform market research in order\nto meet the requirements of customers. One way how this research is conducted\nis that a domain expert gathers and categorizes user-generated content into a\ncomplex and fine-grained class structure. In many of such cases, little data\nmeets complex annotations. It is not yet fully understood how this can be\nleveraged successfully for classification. We examine the classification\naccuracy of expert labels when used with a) many fine-grained classes and b)\nfew abstract classes. For scenario b) we compare abstract class labels given by\nthe domain expert as baseline and by automatic hierarchical clustering. We\ncompare this to another baseline where the entire class structure is given by a\ncompletely unsupervised clustering approach. By doing so, this work can serve\nas an example of how complex expert annotations are potentially beneficial and\ncan be utilized in the most optimal way for opinion mining in highly specific\ndomains. By exploring across a range of techniques and experiments, we find\nthat automated class abstraction approaches in particular the unsupervised\napproach performs remarkably well against domain expert baseline on text\nclassification tasks. This has the potential to inspire opinion mining\napplications in order to support market researchers in practice and to inspire\nfine-grained automated content analysis on a large scale.",
          "link": "http://arxiv.org/abs/2106.15498",
          "publishedOn": "2021-06-30T02:01:03.265Z",
          "wordCount": 657,
          "title": "Classification of Consumer Belief Statements From Social Media. (arXiv:2106.15498v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.07098",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hallyburton_R/0/1/0/all/0/1\">R. Spencer Hallyburton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yupei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pajic_M/0/1/0/all/0/1\">Miroslav Pajic</a>",
          "description": "To enable safe and reliable decision-making, autonomous vehicles (AVs) feed\nsensor data to perception algorithms to understand the environment. Sensor\nfusion, and particularly semantic fusion, with multi-frame tracking is becoming\nincreasingly popular for detecting 3D objects. Recently, it was shown that\nLiDAR-based perception built on deep neural networks is vulnerable to LiDAR\nspoofing attacks. Thus, in this work, we perform the first analysis of\ncamera-LiDAR fusion under spoofing attacks and the first security analysis of\nsemantic fusion in any AV context. We find first that fusion is more successful\nthan existing defenses at guarding against naive spoofing. However, we then\ndefine the frustum attack as a new class of attacks on AVs and find that\nsemantic camera-LiDAR fusion exhibits widespread vulnerability to frustum\nattacks with between 70% and 90% success against target models. Importantly,\nthe attacker needs less than 20 random spoof points on average for successful\nattacks - an order of magnitude less than established maximum capability.\nFinally, we are the first to analyze the longitudinal impact of perception\nattacks by showing the impact of multi-frame attacks.",
          "link": "http://arxiv.org/abs/2106.07098",
          "publishedOn": "2021-06-30T02:01:02.870Z",
          "wordCount": 644,
          "title": "Security Analysis of Camera-LiDAR Semantic-Level Fusion Against Black-Box Attacks on Autonomous Vehicles. (arXiv:2106.07098v2 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14630",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Gao_Y/0/1/0/all/0/1\">Yue Gao</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Raskutti_G/0/1/0/all/0/1\">Garvesh Raskutti</a>",
          "description": "Network estimation from multi-variate point process or time series data is a\nproblem of fundamental importance. Prior work has focused on parametric\napproaches that require a known parametric model, which makes estimation\nprocedures less robust to model mis-specification, non-linearities and\nheterogeneities. In this paper, we develop a semi-parametric approach based on\nthe monotone single-index multi-variate autoregressive model (SIMAM) which\naddresses these challenges. We provide theoretical guarantees for dependent\ndata and an alternating projected gradient descent algorithm. Significantly we\ndo not explicitly assume mixing conditions on the process (although we do\nrequire conditions analogous to restricted strong convexity) and we achieve\nrates of the form $O(T^{-\\frac{1}{3}} \\sqrt{s\\log(TM)})$ (optimal in the\nindependent design case) where $s$ is the threshold for the maximum in-degree\nof the network that indicates the sparsity level, $M$ is the number of actors\nand $T$ is the number of time points. In addition, we demonstrate the superior\nperformance both on simulated data and two real data examples where our SIMAM\napproach out-performs state-of-the-art parametric methods both in terms of\nprediction and network estimation.",
          "link": "http://arxiv.org/abs/2106.14630",
          "publishedOn": "2021-06-30T02:01:02.865Z",
          "wordCount": 634,
          "title": "Improved Prediction and Network Estimation Using the Monotone Single Index Multi-variate Autoregressive Model. (arXiv:2106.14630v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.05648",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Larsson_J/0/1/0/all/0/1\">Johan Larsson</a>",
          "description": "The lasso is a popular method to induce shrinkage and sparsity in the\nsolution vector (coefficients) of regression problems, particularly when there\nare many predictors relative to the number of observations. Solving the lasso\nin this high-dimensional setting can, however, be computationally demanding.\nFortunately, this demand can be alleviated via the use of screening rules that\ndiscard predictors prior to fitting the model, leading to a reduced problem to\nbe solved. In this paper, we present a new screening strategy: look-ahead\nscreening. Our method uses safe screening rules to find a range of penalty\nvalues for which a given predictor cannot enter the model, thereby screening\npredictors along the remainder of the path. In experiments we show that these\nlook-ahead screening rules outperform the active warm-start version of the Gap\nSafe rules.",
          "link": "http://arxiv.org/abs/2105.05648",
          "publishedOn": "2021-06-30T02:01:02.858Z",
          "wordCount": 592,
          "title": "Look-Ahead Screening Rules for the Lasso. (arXiv:2105.05648v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.06251",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Akiyama_S/0/1/0/all/0/1\">Shunta Akiyama</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Suzuki_T/0/1/0/all/0/1\">Taiji Suzuki</a>",
          "description": "Deep learning empirically achieves high performance in many applications, but\nits training dynamics has not been fully understood theoretically. In this\npaper, we explore theoretical analysis on training two-layer ReLU neural\nnetworks in a teacher-student regression model, in which a student network\nlearns an unknown teacher network through its outputs. We show that with a\nspecific regularization and sufficient over-parameterization, the student\nnetwork can identify the parameters of the teacher network with high\nprobability via gradient descent with a norm dependent stepsize even though the\nobjective function is highly non-convex. The key theoretical tool is the\nmeasure representation of the neural networks and a novel application of a dual\ncertificate argument for sparse estimation on a measure space. We analyze the\nglobal minima and global convergence property in the measure space.",
          "link": "http://arxiv.org/abs/2106.06251",
          "publishedOn": "2021-06-30T02:01:02.808Z",
          "wordCount": 593,
          "title": "On Learnability via Gradient Method for Two-Layer ReLU Neural Networks in Teacher-Student Setting. (arXiv:2106.06251v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1905.13100",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zombori_Z/0/1/0/all/0/1\">Zsolt Zombori</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Csiszarik_A/0/1/0/all/0/1\">Adri&#xe1;n Csisz&#xe1;rik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Michalewski_H/0/1/0/all/0/1\">Henryk Michalewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaliszyk_C/0/1/0/all/0/1\">Cezary Kaliszyk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Urban_J/0/1/0/all/0/1\">Josef Urban</a>",
          "description": "We present a reinforcement learning (RL) based guidance system for automated\ntheorem proving geared towards Finding Longer Proofs (FLoP). Unlike most\nlearning based approaches, we focus on generalising from very little training\ndata and achieving near complete confidence. We use several simple, structured\ndatasets with very long proofs to show that FLoP can successfully generalise a\nsingle training proof to a large class of related problems. On these\nbenchmarks, FLoP is competitive with strong theorem provers despite using very\nlimited search, due to its ability to solve problems that are prohibitively\nlong for other systems.",
          "link": "http://arxiv.org/abs/1905.13100",
          "publishedOn": "2021-06-30T02:01:02.717Z",
          "wordCount": 569,
          "title": "Towards Finding Longer Proofs. (arXiv:1905.13100v2 [cs.LO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.04679",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1\">Qin Yang</a>",
          "description": "Distributed artificial intelligence (DAI) studies artificial intelligence\nentities working together to reason, plan, solve problems, organize behaviors\nand strategies, make collective decisions and learn. This Ph.D. research\nproposes a principled Multi-Agent Systems (MAS) cooperation framework,\nSelf-Adaptive Swarm System (SASS), to bridge the fourth level automation gap\nbetween perception, communication, planning, execution, decision-making, and\nlearning.",
          "link": "http://arxiv.org/abs/2106.04679",
          "publishedOn": "2021-06-30T02:01:02.712Z",
          "wordCount": 517,
          "title": "Self-Adaptive Swarm System (SASS). (arXiv:2106.04679v2 [cs.MA] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15587",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hanping Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1\">Yuhong Guo</a>",
          "description": "The generalization gap in reinforcement learning (RL) has been a significant\nobstacle that prevents the RL agent from learning general skills and adapting\nto varying environments. Increasing the generalization capacity of the RL\nsystems can significantly improve their performance on real-world working\nenvironments. In this work, we propose a novel policy-aware adversarial data\naugmentation method to augment the standard policy learning method with\nautomatically generated trajectory data. Different from the commonly used\nobservation transformation based data augmentations, our proposed method\nadversarially generates new trajectory data based on the policy gradient\nobjective and aims to more effectively increase the RL agent's generalization\nability with the policy-aware data augmentation. Moreover, we further deploy a\nmixup step to integrate the original and generated data to enhance the\ngeneralization capacity while mitigating the over-deviation of the adversarial\ndata. We conduct experiments on a number of RL tasks to investigate the\ngeneralization performance of the proposed method by comparing it with the\nstandard baselines and the state-of-the-art mixreg approach. The results show\nour method can generalize well with limited training diversity, and achieve the\nstate-of-the-art generalization test performance.",
          "link": "http://arxiv.org/abs/2106.15587",
          "publishedOn": "2021-06-30T02:01:02.707Z",
          "wordCount": 611,
          "title": "Generalization of Reinforcement Learning with Policy-Aware Adversarial Data Augmentation. (arXiv:2106.15587v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15561",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Tan_X/0/1/0/all/0/1\">Xu Tan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Qin_T/0/1/0/all/0/1\">Tao Qin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Soong_F/0/1/0/all/0/1\">Frank Soong</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_T/0/1/0/all/0/1\">Tie-Yan Liu</a>",
          "description": "Text to speech (TTS), or speech synthesis, which aims to synthesize\nintelligible and natural speech given text, is a hot research topic in speech,\nlanguage, and machine learning communities and has broad applications in the\nindustry. As the development of deep learning and artificial intelligence,\nneural network-based TTS has significantly improved the quality of synthesized\nspeech in recent years. In this paper, we conduct a comprehensive survey on\nneural TTS, aiming to provide a good understanding of current research and\nfuture trends. We focus on the key components in neural TTS, including text\nanalysis, acoustic models and vocoders, and several advanced topics, including\nfast TTS, low-resource TTS, robust TTS, expressive TTS, and adaptive TTS, etc.\nWe further summarize resources related to TTS (e.g., datasets, opensource\nimplementations) and discuss future research directions. This survey can serve\nboth academic researchers and industry practitioners working on TTS.",
          "link": "http://arxiv.org/abs/2106.15561",
          "publishedOn": "2021-06-30T02:01:02.702Z",
          "wordCount": 607,
          "title": "A Survey on Neural Speech Synthesis. (arXiv:2106.15561v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2008.13578",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yijue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chenghong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zigeng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shanglin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_J/0/1/0/all/0/1\">Jinbo Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1\">Caiwen Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajasekaran_S/0/1/0/all/0/1\">Sanguthevar Rajasekaran</a>",
          "description": "The large model size, high computational operations, and vulnerability\nagainst membership inference attack (MIA) have impeded deep learning or deep\nneural networks (DNNs) popularity, especially on mobile devices. To address the\nchallenge, we envision that the weight pruning technique will help DNNs against\nMIA while reducing model storage and computational operation. In this work, we\npropose a pruning algorithm, and we show that the proposed algorithm can find a\nsubnetwork that can prevent privacy leakage from MIA and achieves competitive\naccuracy with the original DNNs. We also verify our theoretical insights with\nexperiments. Our experimental results illustrate that the attack accuracy using\nmodel compression is up to 13.6% and 10% lower than that of the baseline and\nMin-Max game, accordingly.",
          "link": "http://arxiv.org/abs/2008.13578",
          "publishedOn": "2021-06-30T02:01:02.697Z",
          "wordCount": 620,
          "title": "Against Membership Inference Attack: Pruning is All You Need. (arXiv:2008.13578v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.13826",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Raffy_P/0/1/0/all/0/1\">Philippe Raffy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pambrun_J/0/1/0/all/0/1\">Jean-Fran&#xe7;ois Pambrun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kumar_A/0/1/0/all/0/1\">Ashish Kumar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dubois_D/0/1/0/all/0/1\">David Dubois</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Patti_J/0/1/0/all/0/1\">Jay Waldron Patti</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cairns_R/0/1/0/all/0/1\">Robyn Alexandra Cairns</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Young_R/0/1/0/all/0/1\">Ryan Young</a>",
          "description": "Standardized body region labelling of individual images provides data that\ncan improve human and computer use of medical images. A CNN-based classifier\nwas developed to identify body regions in CT and MRI. 17 CT (18 MRI) body\nregions covering the entire human body were defined for the classification\ntask. Three retrospective databases were built for the AI model training,\nvalidation, and testing, with a balanced distribution of studies per body\nregion. The test databases originated from a different healthcare network.\nAccuracy, recall and precision of the classifier was evaluated for patient age,\npatient gender, institution, scanner manufacturer, contrast, slice thickness,\nMRI sequence, and CT kernel. The data included a retrospective cohort of 2,934\nanonymized CT cases (training: 1,804 studies, validation: 602 studies, test:\n528 studies) and 3,185 anonymized MRI cases (training: 1,911 studies,\nvalidation: 636 studies, test: 638 studies). 27 institutions from primary care\nhospitals, community hospitals and imaging centers contributed to the test\ndatasets. The data included cases of all genders in equal proportions and\nsubjects aged from a few months old to +90 years old. An image-level prediction\naccuracy of 91.9% (90.2 - 92.1) for CT, and 94.2% (92.0 - 95.6) for MRI was\nachieved. The classification results were robust across all body regions and\nconfounding factors. Due to limited data, performance results for subjects\nunder 10 years-old could not be reliably evaluated. We show that deep learning\nmodels can classify CT and MRI images by body region including lower and upper\nextremities with high accuracy.",
          "link": "http://arxiv.org/abs/2104.13826",
          "publishedOn": "2021-06-30T02:01:02.682Z",
          "wordCount": 732,
          "title": "Deep Learning Body Region Classification of MRI and CT examinations. (arXiv:2104.13826v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15590",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Birhane_A/0/1/0/all/0/1\">Abeba Birhane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalluri_P/0/1/0/all/0/1\">Pratyusha Kalluri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Card_D/0/1/0/all/0/1\">Dallas Card</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agnew_W/0/1/0/all/0/1\">William Agnew</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dotan_R/0/1/0/all/0/1\">Ravit Dotan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_M/0/1/0/all/0/1\">Michelle Bao</a>",
          "description": "Machine learning (ML) currently exerts an outsized influence on the world,\nincreasingly affecting communities and institutional practices. It is therefore\ncritical that we question vague conceptions of the field as value-neutral or\nuniversally beneficial, and investigate what specific values the field is\nadvancing. In this paper, we present a rigorous examination of the values of\nthe field by quantitatively and qualitatively analyzing 100 highly cited ML\npapers published at premier ML conferences, ICML and NeurIPS. We annotate key\nfeatures of papers which reveal their values: how they justify their choice of\nproject, which aspects they uplift, their consideration of potential negative\nconsequences, and their institutional affiliations and funding sources. We find\nthat societal needs are typically very loosely connected to the choice of\nproject, if mentioned at all, and that consideration of negative consequences\nis extremely rare. We identify 67 values that are uplifted in machine learning\nresearch, and, of these, we find that papers most frequently justify and assess\nthemselves based on performance, generalization, efficiency, researcher\nunderstanding, novelty, and building on previous work. We present extensive\ntextual evidence and analysis of how these values are operationalized. Notably,\nwe find that each of these top values is currently being defined and applied\nwith assumptions and implications generally supporting the centralization of\npower. Finally, we find increasingly close ties between these highly cited\npapers and tech companies and elite universities.",
          "link": "http://arxiv.org/abs/2106.15590",
          "publishedOn": "2021-06-30T02:01:02.676Z",
          "wordCount": 678,
          "title": "The Values Encoded in Machine Learning Research. (arXiv:2106.15590v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15324",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Beck_N/0/1/0/all/0/1\">Nathan Beck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sivasubramanian_D/0/1/0/all/0/1\">Durga Sivasubramanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dani_A/0/1/0/all/0/1\">Apurva Dani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramakrishnan_G/0/1/0/all/0/1\">Ganesh Ramakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyer_R/0/1/0/all/0/1\">Rishabh Iyer</a>",
          "description": "With the goal of making deep learning more label-efficient, a growing number\nof papers have been studying active learning (AL) for deep models. However,\nthere are a number of issues in the prevalent experimental settings, mainly\nstemming from a lack of unified implementation and benchmarking. Issues in the\ncurrent literature include sometimes contradictory observations on the\nperformance of different AL algorithms, unintended exclusion of important\ngeneralization approaches such as data augmentation and SGD for optimization, a\nlack of study of evaluation facets like the labeling efficiency of AL, and\nlittle or no clarity on the scenarios in which AL outperforms random sampling\n(RS). In this work, we present a unified re-implementation of state-of-the-art\nAL algorithms in the context of image classification, and we carefully study\nthese issues as facets of effective evaluation. On the positive side, we show\nthat AL techniques are 2x to 4x more label-efficient compared to RS with the\nuse of data augmentation. Surprisingly, when data augmentation is included,\nthere is no longer a consistent gain in using BADGE, a state-of-the-art\napproach, over simple uncertainty sampling. We then do a careful analysis of\nhow existing approaches perform with varying amounts of redundancy and number\nof examples per class. Finally, we provide several insights for AL\npractitioners to consider in future work, such as the effect of the AL batch\nsize, the effect of initialization, the importance of retraining a new model at\nevery round, and other insights.",
          "link": "http://arxiv.org/abs/2106.15324",
          "publishedOn": "2021-06-30T02:01:02.671Z",
          "wordCount": 726,
          "title": "Effective Evaluation of Deep Active Learning on Image Classification Tasks. (arXiv:2106.15324v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2104.14380",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mo_F/0/1/0/all/0/1\">Fan Mo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haddadi_H/0/1/0/all/0/1\">Hamed Haddadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katevas_K/0/1/0/all/0/1\">Kleomenis Katevas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marin_E/0/1/0/all/0/1\">Eduard Marin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perino_D/0/1/0/all/0/1\">Diego Perino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kourtellis_N/0/1/0/all/0/1\">Nicolas Kourtellis</a>",
          "description": "We propose and implement a Privacy-preserving Federated Learning ($PPFL$)\nframework for mobile systems to limit privacy leakages in federated learning.\nLeveraging the widespread presence of Trusted Execution Environments (TEEs) in\nhigh-end and mobile devices, we utilize TEEs on clients for local training, and\non servers for secure aggregation, so that model/gradient updates are hidden\nfrom adversaries. Challenged by the limited memory size of current TEEs, we\nleverage greedy layer-wise training to train each model's layer inside the\ntrusted area until its convergence. The performance evaluation of our\nimplementation shows that $PPFL$ can significantly improve privacy while\nincurring small system overheads at the client-side. In particular, $PPFL$ can\nsuccessfully defend the trained model against data reconstruction, property\ninference, and membership inference attacks. Furthermore, it can achieve\ncomparable model utility with fewer communication rounds (0.54$\\times$) and a\nsimilar amount of network traffic (1.002$\\times$) compared to the standard\nfederated learning of a complete model. This is achieved while only introducing\nup to ~15% CPU time, ~18% memory usage, and ~21% energy consumption overhead in\n$PPFL$'s client-side.",
          "link": "http://arxiv.org/abs/2104.14380",
          "publishedOn": "2021-06-30T02:01:02.635Z",
          "wordCount": 658,
          "title": "PPFL: Privacy-preserving Federated Learning with Trusted Execution Environments. (arXiv:2104.14380v2 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14642",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Meng_L/0/1/0/all/0/1\">Li Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yazidi_A/0/1/0/all/0/1\">Anis Yazidi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goodwin_M/0/1/0/all/0/1\">Morten Goodwin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Engelstad_P/0/1/0/all/0/1\">Paal Engelstad</a>",
          "description": "We propose a novel algorithm named Expert Q-learning. Expert Q-learning was\ninspired by Dueling Q-learning and aimed at incorporating the ideas from\nsemi-supervised learning into reinforcement learning through splitting Q-values\ninto state values and action advantages. Different from Generative Adversarial\nImitation Learning and Deep Q-Learning from Demonstrations, the offline expert\nwe have used only predicts the value of a state from {-1, 0, 1}, indicating\nwhether this is a bad, neutral or good state. An expert network was designed in\naddition to the Q-network, which updates each time following the regular\noffline minibatch update whenever the expert example buffer is not empty. The\nQ-network plays the role of the advantage function only during the update. Our\nalgorithm also keeps asynchronous copies of the Q-network and expert network,\npredicting the target values using the same manner as of Double Q-learning.\n\nWe compared on the game of Othello our algorithm with the state-of-the-art\nQ-learning algorithm, which was a combination of Double Q-learning and Dueling\nQ-learning. The results showed that Expert Q-learning was indeed useful and\nmore resistant to the overestimation bias of Q-learning. The baseline\nQ-learning algorithm exhibited unstable and suboptimal behavior, especially\nwhen playing against a stochastic player, whereas Expert Q-learning\ndemonstrated more robust performance with higher scores. Expert Q-learning\nwithout using examples has also gained better results than the baseline\nalgorithm when trained and tested against a fixed player. On the other hand,\nExpert Q-learning without examples cannot win against the baseline Q-learning\nalgorithm in direct game competitions despite the fact that it has also shown\nthe strength of reducing the overestimation bias.",
          "link": "http://arxiv.org/abs/2106.14642",
          "publishedOn": "2021-06-30T02:01:02.629Z",
          "wordCount": 723,
          "title": "Expert Q-learning: Deep Q-learning With State Values From Expert Examples. (arXiv:2106.14642v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14861",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Din_Z/0/1/0/all/0/1\">Zainul Abi Din</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Venugopalan_H/0/1/0/all/0/1\">Hari Venugopalan</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Henry Lin</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Wushensky_A/0/1/0/all/0/1\">Adam Wushensky</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Steven Liu</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+King_S/0/1/0/all/0/1\">Samuel T. King</a> (1 and 2) ((1) University of California, Davis, (2) Bouncer Technologies)",
          "description": "App builders commonly use security challenges, a form of step-up\nauthentication, to add security to their apps. However, the ethical\nimplications of this type of architecture has not been studied previously. In\nthis paper, we present a large-scale measurement study of running an existing\nanti-fraud security challenge, Boxer, in real apps running on mobile devices.\nWe find that although Boxer does work well overall, it is unable to scan\neffectively on devices that run its machine learning models at less than one\nframe per second (FPS), blocking users who use inexpensive devices. With the\ninsights from our study, we design Daredevil, anew anti-fraud system for\nscanning payment cards that work swell across the broad range of performance\ncharacteristics and hardware configurations found on modern mobile devices.\nDaredevil reduces the number of devices that run at less than one FPS by an\norder of magnitude compared to Boxer, providing a more equitable system for\nfighting fraud. In total, we collect data from 5,085,444 real devices spread\nacross 496 real apps running production software and interacting with real\nusers.",
          "link": "http://arxiv.org/abs/2106.14861",
          "publishedOn": "2021-06-30T02:01:02.616Z",
          "wordCount": 667,
          "title": "Doing good by fighting fraud: Ethical anti-fraud systems for mobile payments. (arXiv:2106.14861v2 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.06075",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Barazandeh_B/0/1/0/all/0/1\">Babak Barazandeh</a>, <a href=\"http://arxiv.org/find/math/1/au:+Huang_T/0/1/0/all/0/1\">Tianjian Huang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Michailidis_G/0/1/0/all/0/1\">George Michailidis</a>",
          "description": "Min-max saddle point games have recently been intensely studied, due to their\nwide range of applications, including training Generative Adversarial Networks\n(GANs). However, most of the recent efforts for solving them are limited to\nspecial regimes such as convex-concave games. Further, it is customarily\nassumed that the underlying optimization problem is solved either by a single\nmachine or in the case of multiple machines connected in centralized fashion,\nwherein each one communicates with a central node. The latter approach becomes\nchallenging, when the underlying communications network has low bandwidth. In\naddition, privacy considerations may dictate that certain nodes can communicate\nwith a subset of other nodes. Hence, it is of interest to develop methods that\nsolve min-max games in a decentralized manner. To that end, we develop a\ndecentralized adaptive momentum (ADAM)-type algorithm for solving min-max\noptimization problem under the condition that the objective function satisfies\na Minty Variational Inequality condition, which is a generalization to\nconvex-concave case. The proposed method overcomes shortcomings of recent\nnon-adaptive gradient-based decentralized algorithms for min-max optimization\nproblems that do not perform well in practice and require careful tuning. In\nthis paper, we obtain non-asymptotic rates of convergence of the proposed\nalgorithm (coined DADAM$^3$) for finding a (stochastic) first-order Nash\nequilibrium point and subsequently evaluate its performance on training GANs.\nThe extensive empirical evaluation shows that DADAM$^3$ outperforms recently\ndeveloped methods, including decentralized optimistic stochastic gradient for\nsolving such min-max problems.",
          "link": "http://arxiv.org/abs/2106.06075",
          "publishedOn": "2021-06-30T02:01:02.611Z",
          "wordCount": 702,
          "title": "A Decentralized Adaptive Momentum Method for Solving a Class of Min-Max Optimization Problems. (arXiv:2106.06075v2 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15542",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Upadhyay_U/0/1/0/all/0/1\">Uddeshya Upadhyay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yanbei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hepp_T/0/1/0/all/0/1\">Tobias Hepp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gatidis_S/0/1/0/all/0/1\">Sergios Gatidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akata_Z/0/1/0/all/0/1\">Zeynep Akata</a>",
          "description": "Image-to-image translation plays a vital role in tackling various medical\nimaging tasks such as attenuation correction, motion correction, undersampled\nreconstruction, and denoising. Generative adversarial networks have been shown\nto achieve the state-of-the-art in generating high fidelity images for these\ntasks. However, the state-of-the-art GAN-based frameworks do not estimate the\nuncertainty in the predictions made by the network that is essential for making\ninformed medical decisions and subsequent revision by medical experts and has\nrecently been shown to improve the performance and interpretability of the\nmodel. In this work, we propose an uncertainty-guided progressive learning\nscheme for image-to-image translation. By incorporating aleatoric uncertainty\nas attention maps for GANs trained in a progressive manner, we generate images\nof increasing fidelity progressively. We demonstrate the efficacy of our model\non three challenging medical image translation tasks, including PET to CT\ntranslation, undersampled MRI reconstruction, and MRI motion artefact\ncorrection. Our model generalizes well in three different tasks and improves\nperformance over state of the art under full-supervision and weak-supervision\nwith limited data. Code is released here:\nhttps://github.com/ExplainableML/UncerGuidedI2I",
          "link": "http://arxiv.org/abs/2106.15542",
          "publishedOn": "2021-06-30T02:01:02.599Z",
          "wordCount": 634,
          "title": "Uncertainty-Guided Progressive GANs for Medical Image Translation. (arXiv:2106.15542v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2105.03746",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1\">Huangjie Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1\">Jiangchao Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hongxia Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chunyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Ya Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsang_I/0/1/0/all/0/1\">Ivor Tsang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jingren Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Mingyuan Zhou</a>",
          "description": "Contrastive learning (CL) is effective in learning data representations\nwithout label supervision, where the encoder needs to contrast each positive\nsample over multiple negative samples via a one-vs-many softmax cross-entropy\nloss. However, conventional CL is sensitive to how many negative samples are\nincluded and how they are selected. Proposed in this paper is a doubly CL\nstrategy that contrasts positive samples and negative ones within themselves\nseparately. We realize this strategy with contrastive attraction and\ncontrastive repulsion (CACR) makes the query not only exert a greater force to\nattract more distant positive samples but also do so to repel closer negative\nsamples. Theoretical analysis reveals the connection between CACR and CL from\nthe perspectives of both positive attraction and negative repulsion and shows\nthe benefits in both efficiency and robustness brought by separately\ncontrasting within the sampled positive and negative pairs. Extensive\nlarge-scale experiments on standard vision tasks show that CACR not only\nconsistently outperforms existing CL methods on benchmark datasets in\nrepresentation learning, but also provides interpretable contrastive weights,\ndemonstrating the efficacy of the proposed doubly contrastive strategy.",
          "link": "http://arxiv.org/abs/2105.03746",
          "publishedOn": "2021-06-30T02:01:02.593Z",
          "wordCount": 664,
          "title": "Contrastive Attraction and Contrastive Repulsion for Representation Learning. (arXiv:2105.03746v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.15301",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Banerjee_M/0/1/0/all/0/1\">Monami Banerjee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_R/0/1/0/all/0/1\">Rudrasis Chakraborty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bouza_J/0/1/0/all/0/1\">Jose Bouza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vemuri_B/0/1/0/all/0/1\">Baba C. Vemuri</a>",
          "description": "Convolutional neural networks have been highly successful in image-based\nlearning tasks due to their translation equivariance property. Recent work has\ngeneralized the traditional convolutional layer of a convolutional neural\nnetwork to non-Euclidean spaces and shown group equivariance of the generalized\nconvolution operation. In this paper, we present a novel higher order Volterra\nconvolutional neural network (VolterraNet) for data defined as samples of\nfunctions on Riemannian homogeneous spaces. Analagous to the result for\ntraditional convolutions, we prove that the Volterra functional convolutions\nare equivariant to the action of the isometry group admitted by the Riemannian\nhomogeneous spaces, and under some restrictions, any non-linear equivariant\nfunction can be expressed as our homogeneous space Volterra convolution,\ngeneralizing the non-linear shift equivariant characterization of Volterra\nexpansions in Euclidean space. We also prove that second order functional\nconvolution operations can be represented as cascaded convolutions which leads\nto an efficient implementation. Beyond this, we also propose a dilated\nVolterraNet model. These advances lead to large parameter reductions relative\nto baseline non-Euclidean CNNs. To demonstrate the efficacy of the VolterraNet\nperformance, we present several real data experiments involving classification\ntasks on spherical-MNIST, atomic energy, Shrec17 data sets, and group testing\non diffusion MRI data. Performance comparisons to the state-of-the-art are also\npresented.",
          "link": "http://arxiv.org/abs/2106.15301",
          "publishedOn": "2021-06-30T02:01:02.563Z",
          "wordCount": 665,
          "title": "VolterraNet: A higher order convolutional network with group equivariance for homogeneous manifolds. (arXiv:2106.15301v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15609",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Thakur_N/0/1/0/all/0/1\">Nirmalya Thakur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_C/0/1/0/all/0/1\">Chia Y. Han</a>",
          "description": "This framework for human behavior monitoring aims to take a holistic approach\nto study, track, monitor, and analyze human behavior during activities of daily\nliving (ADLs). The framework consists of two novel functionalities. First, it\ncan perform the semantic analysis of user interactions on the diverse\ncontextual parameters during ADLs to identify a list of distinct behavioral\npatterns associated with different complex activities. Second, it consists of\nan intelligent decision-making algorithm that can analyze these behavioral\npatterns and their relationships with the dynamic contextual and spatial\nfeatures of the environment to detect any anomalies in user behavior that could\nconstitute an emergency. These functionalities of this interdisciplinary\nframework were developed by integrating the latest advancements and\ntechnologies in human-computer interaction, machine learning, Internet of\nThings, pattern recognition, and ubiquitous computing. The framework was\nevaluated on a dataset of ADLs, and the performance accuracies of these two\nfunctionalities were found to be 76.71% and 83.87%, respectively. The presented\nand discussed results uphold the relevance and immense potential of this\nframework to contribute towards improving the quality of life and assisted\nliving of the aging population in the future of Internet of Things (IoT)-based\nubiquitous living environments, e.g., smart homes.",
          "link": "http://arxiv.org/abs/2106.15609",
          "publishedOn": "2021-06-30T02:01:02.549Z",
          "wordCount": 658,
          "title": "An Ambient Intelligence-Based Human Behavior Monitoring Framework for Ubiquitous Environments. (arXiv:2106.15609v1 [cs.HC])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15615",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Saunshi_N/0/1/0/all/0/1\">Nikunj Saunshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Arushi Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1\">Wei Hu</a>",
          "description": "An effective approach in meta-learning is to utilize multiple \"train tasks\"\nto learn a good initialization for model parameters that can help solve unseen\n\"test tasks\" with very few samples by fine-tuning from this initialization.\nAlthough successful in practice, theoretical understanding of such methods is\nlimited. This work studies an important aspect of these methods: splitting the\ndata from each task into train (support) and validation (query) sets during\nmeta-training. Inspired by recent work (Raghu et al., 2020), we view such\nmeta-learning methods through the lens of representation learning and argue\nthat the train-validation split encourages the learned representation to be\nlow-rank without compromising on expressivity, as opposed to the non-splitting\nvariant that encourages high-rank representations. Since sample efficiency\nbenefits from low-rankness, the splitting strategy will require very few\nsamples to solve unseen test tasks. We present theoretical results that\nformalize this idea for linear representation learning on a subspace\nmeta-learning instance, and experimentally verify this practical benefit of\nsplitting in simulations and on standard meta-learning benchmarks.",
          "link": "http://arxiv.org/abs/2106.15615",
          "publishedOn": "2021-06-30T02:01:02.544Z",
          "wordCount": 613,
          "title": "A Representation Learning Perspective on the Importance of Train-Validation Splitting in Meta-Learning. (arXiv:2106.15615v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15320",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kahu_S/0/1/0/all/0/1\">Sampanna Yashwant Kahu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ingram_W/0/1/0/all/0/1\">William A. Ingram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fox_E/0/1/0/all/0/1\">Edward A. Fox</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jian Wu</a>",
          "description": "We focus on electronic theses and dissertations (ETDs), aiming to improve\naccess and expand their utility, since more than 6 million are publicly\navailable, and they constitute an important corpus to aid research and\neducation across disciplines. The corpus is growing as new born-digital\ndocuments are included, and since millions of older theses and dissertations\nhave been converted to digital form to be disseminated electronically in\ninstitutional repositories. In ETDs, as with other scholarly works, figures and\ntables can communicate a large amount of information in a concise way. Although\nmethods have been proposed for extracting figures and tables from born-digital\nPDFs, they do not work well with scanned ETDs. Considering this problem, our\nassessment of state-of-the-art figure extraction systems is that the reason\nthey do not function well on scanned PDFs is that they have only been trained\non born-digital documents. To address this limitation, we present ScanBank, a\nnew dataset containing 10 thousand scanned page images, manually labeled by\nhumans as to the presence of the 3.3 thousand figures or tables found therein.\nWe use this dataset to train a deep neural network model based on YOLOv5 to\naccurately extract figures and tables from scanned ETDs. We pose and answer\nimportant research questions aimed at finding better methods for figure\nextraction from scanned documents. One of those concerns the value for\ntraining, of data augmentation techniques applied to born-digital documents\nwhich are used to train models better suited for figure extraction from scanned\ndocuments. To the best of our knowledge, ScanBank is the first manually\nannotated dataset for figure and table extraction for scanned ETDs. A\nYOLOv5-based model, trained on ScanBank, outperforms existing comparable\nopen-source and freely available baseline methods by a considerable margin.",
          "link": "http://arxiv.org/abs/2106.15320",
          "publishedOn": "2021-06-30T02:01:02.539Z",
          "wordCount": 756,
          "title": "ScanBank: A Benchmark Dataset for Figure Extraction from Scanned Electronic Theses and Dissertations. (arXiv:2106.15320v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15321",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Boucaud_L/0/1/0/all/0/1\">Laurent Boucaud</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aloise_D/0/1/0/all/0/1\">Daniel Aloise</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saunier_N/0/1/0/all/0/1\">Nicolas Saunier</a>",
          "description": "We consider the problem of predicting the future path of a pedestrian using\nits motion history and the motion history of the surrounding pedestrians,\ncalled social information. Since the seminal paper on Social-LSTM,\ndeep-learning has become the main tool used to model the impact of social\ninteractions on a pedestrian's motion. The demonstration that these models can\nlearn social interactions relies on an ablative study of these models. The\nmodels are compared with and without their social interactions module on two\nstandard metrics, the Average Displacement Error and Final Displacement Error.\nYet, these complex models were recently outperformed by a simple\nconstant-velocity approach. This questions if they actually allow to model\nsocial interactions as well as the validity of the proof. In this paper, we\nfocus on the deep-learning models with a soft-attention mechanism for social\ninteraction modeling and study whether they use social information at\nprediction time. We conduct two experiments across four state-of-the-art\napproaches on the ETH and UCY datasets, which were also used in previous work.\nFirst, the models are trained by replacing the social information with random\nnoise and compared to model trained with actual social information. Second, we\nuse a gating mechanism along with a $L_0$ penalty, allowing models to shut down\ntheir inner components. The models consistently learn to prune their\nsoft-attention mechanism. For both experiments, neither the course of the\nconvergence nor the prediction performance were altered. This demonstrates that\nthe soft-attention mechanism and therefore the social information are ignored\nby the models.",
          "link": "http://arxiv.org/abs/2106.15321",
          "publishedOn": "2021-06-30T02:01:02.478Z",
          "wordCount": 726,
          "title": "Soft Attention: Does it Actually Help to Learn Social Interactions in Pedestrian Trajectory Prediction?. (arXiv:2106.15321v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15306",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ruijters_D/0/1/0/all/0/1\">Daniel Ruijters</a>",
          "description": "Minimally invasive image guided treatment procedures often employ advanced\nimage processing algorithms. The recent developments of artificial intelligence\nalgorithms harbor potential to further enhance this domain. In this article we\nexplore several application areas within the minimally invasive treatment space\nand discuss the deployment of artificial intelligence within these areas.",
          "link": "http://arxiv.org/abs/2106.15306",
          "publishedOn": "2021-06-30T02:01:02.472Z",
          "wordCount": 496,
          "title": "Artificial Intelligence in Minimally Invasive Interventional Treatment. (arXiv:2106.15306v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15295",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Camero_A/0/1/0/all/0/1\">Andr&#xe9;s Camero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toutouh_J/0/1/0/all/0/1\">Jamal Toutouh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alba_E/0/1/0/all/0/1\">Enrique Alba</a>",
          "description": "This article introduces Random Error Sampling-based Neuroevolution (RESN), a\nnovel automatic method to optimize recurrent neural network architectures. RESN\ncombines an evolutionary algorithm with a training-free evaluation approach.\nThe results show that RESN achieves state-of-the-art error performance while\nreducing by half the computational time.",
          "link": "http://arxiv.org/abs/2106.15295",
          "publishedOn": "2021-06-30T02:01:02.458Z",
          "wordCount": 480,
          "title": "Reliable and Fast Recurrent Neural Network Architecture Optimization. (arXiv:2106.15295v1 [cs.NE])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15190",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Nguyen_T/0/1/0/all/0/1\">Thi Ngoc Tho Nguyen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Watcharasupat_K/0/1/0/all/0/1\">Karn Watcharasupat</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nguyen_N/0/1/0/all/0/1\">Ngoc Khanh Nguyen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jones_D/0/1/0/all/0/1\">Douglas L. Jones</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gan_W/0/1/0/all/0/1\">Woon Seng Gan</a>",
          "description": "Sound event localization and detection consists of two subtasks which are\nsound event detection and direction-of-arrival estimation. While sound event\ndetection mainly relies on time-frequency patterns to distinguish different\nsound classes, direction-of-arrival estimation uses magnitude or phase\ndifferences between microphones to estimate source directions. Therefore, it is\noften difficult to jointly train these two subtasks simultaneously. We propose\na novel feature called spatial cue-augmented log-spectrogram (SALSA) with exact\ntime-frequency mapping between the signal power and the source\ndirection-of-arrival. The feature includes multichannel log-spectrograms\nstacked along with the estimated direct-to-reverberant ratio and a normalized\nversion of the principal eigenvector of the spatial covariance matrix at each\ntime-frequency bin on the spectrograms. Experimental results on the DCASE 2021\ndataset for sound event localization and detection with directional\ninterference showed that the deep learning-based models trained on this new\nfeature outperformed the DCASE challenge baseline by a large margin. We\ncombined several models with slightly different architectures that were trained\non the new feature to further improve the system performances for the DCASE\nsound event localization and detection challenge.",
          "link": "http://arxiv.org/abs/2106.15190",
          "publishedOn": "2021-06-30T02:01:02.453Z",
          "wordCount": 658,
          "title": "DCASE 2021 Task 3: Spectrotemporally-aligned Features for Polyphonic Sound Event Localization and Detection. (arXiv:2106.15190v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15296",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pereg_D/0/1/0/all/0/1\">Deborah Pereg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_I/0/1/0/all/0/1\">Israel Cohen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vassiliou_A/0/1/0/all/0/1\">Anthony A. Vassiliou</a>",
          "description": "In sparse coding, we attempt to extract features of input vectors, assuming\nthat the data is inherently structured as a sparse superposition of basic\nbuilding blocks. Similarly, neural networks perform a given task by learning\nfeatures of the training data set. Recently both data-driven and model-driven\nfeature extracting methods have become extremely popular and have achieved\nremarkable results. Nevertheless, practical implementations are often too slow\nto be employed in real-life scenarios, especially for real-time applications.\nWe propose a speed-up upgraded version of the classic iterative thresholding\nalgorithm, that produces a good approximation of the convolutional sparse code\nwithin 2-5 iterations. The speed advantage is gained mostly from the\nobservation that most solvers are slowed down by inefficient global\nthresholding. The main idea is to normalize each data point by the local\nreceptive field energy, before applying a threshold. This way, the natural\ninclination towards strong feature expressions is suppressed, so that one can\nrely on a global threshold that can be easily approximated, or learned during\ntraining. The proposed algorithm can be employed with a known predetermined\ndictionary, or with a trained dictionary. The trained version is implemented as\na neural net designed as the unfolding of the proposed solver. The performance\nof the proposed solution is demonstrated via the seismic inversion problem in\nboth synthetic and real data scenarios. We also provide theoretical guarantees\nfor a stable support recovery. Namely, we prove that under certain conditions\nthe true support is perfectly recovered within the first iteration.",
          "link": "http://arxiv.org/abs/2106.15296",
          "publishedOn": "2021-06-30T02:01:02.448Z",
          "wordCount": 697,
          "title": "Convolutional Sparse Coding Fast Approximation with Application to Seismic Reflectivity Estimation. (arXiv:2106.15296v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15298",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Wang_Y/0/1/0/all/0/1\">Yuan Wang</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Fan_Y/0/1/0/all/0/1\">Yangxin Fan</a>",
          "description": "We believe that \"all men are created equal\". With the rise of the police\nshootings reported by media, more people in the U.S. think that police use\nexcessive force during law enforcement, especially to a specific group of\npeople. We want to apply multidimensional statistical analysis to reveal more\nfacts than the monotone mainstream media. Our paper has three parts. First, we\nproposed a new method to quantify fatal police shooting news reporting\ndeviation of mainstream media, which includes CNN, FOX, ABC, and NBC. Second,\nwe analyzed the most comprehensive US fatal police shooting dataset from\nWashington Post. We used FP-growth to reveal the frequent patterns and DBSCAN\nclustering to find fatal shooting hotspots. We brought multi-attributes (social\neconomics, demographics, political tendency, education, gun ownership rate,\npolice training hours, etc.) to reveal connections under the iceberg. We found\nthat the police shooting rate of a state depends on many variables. The top\nfour most relevant attributes were state joined year, state land area, gun\nownership rate, and violent crime rate. Third, we proposed four regression\nmodels to predict police shooting rates at the state level. The best model\nKstar could predict the fatal police shooting rate with about 88.53%\ncorrelation coefficient. We also proposed classification models, including\nGradient Boosting Machine, Multi-class Classifier, Logistic Regression, and\nNaive Bayes Classifier, to predict the race of fatal police shooting victims.\nOur classification models show no significant evidence to conclude that racial\ndiscrimination happened during fatal police shootings recorded by the WP\ndataset.",
          "link": "http://arxiv.org/abs/2106.15298",
          "publishedOn": "2021-06-30T02:01:02.443Z",
          "wordCount": 688,
          "title": "US Fatal Police Shooting Analysis and Prediction. (arXiv:2106.15298v1 [physics.soc-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15285",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Royston_S/0/1/0/all/0/1\">Sam Royston</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Greenberg_B/0/1/0/all/0/1\">Ben Greenberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tavasoli_O/0/1/0/all/0/1\">Omeed Tavasoli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotton_C/0/1/0/all/0/1\">Courtenay Cotton</a>",
          "description": "Voter eligibility in United States elections is determined by a patchwork of\nstate databases containing information about which citizens are eligible to\nvote. Administrators at the state and local level are faced with the\nexceedingly difficult task of ensuring that each of their jurisdictions is\nproperly managed, while also monitoring for improper modifications to the\ndatabase. Monitoring changes to Voter Registration Files (VRFs) is crucial,\ngiven that a malicious actor wishing to disrupt the democratic process in the\nUS would be well-advised to manipulate the contents of these files in order to\nachieve their goals. In 2020, we saw election officials perform admirably when\nfaced with administering one of the most contentious elections in US history,\nbut much work remains to secure and monitor the election systems Americans rely\non. Using data created by comparing snapshots taken of VRFs over time, we\npresent a set of methods that make use of machine learning to ease the burden\non analysts and administrators in protecting voter rolls. We first evaluate the\neffectiveness of multiple unsupervised anomaly detection methods in detecting\nVRF modifications by modeling anomalous changes as sparse additive noise. In\nthis setting we determine that statistical models comparing administrative\ndistricts within a short time span and non-negative matrix factorization are\nmost effective for surfacing anomalous events for review. These methods were\ndeployed during 2019-2020 in our organization's monitoring system and were used\nin collaboration with the office of the Iowa Secretary of State. Additionally,\nwe propose a newly deployed model which uses historical and demographic\nmetadata to label the likely root cause of database modifications. We hope to\nuse this model to predict which modifications have known causes and therefore\nbetter identify potentially anomalous modifications.",
          "link": "http://arxiv.org/abs/2106.15285",
          "publishedOn": "2021-06-30T02:01:02.426Z",
          "wordCount": 732,
          "title": "Anomaly Detection and Automated Labeling for Voter Registration File Changes. (arXiv:2106.15285v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15286",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kirsten_L/0/1/0/all/0/1\">Lucas N. Kirsten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piccoli_R/0/1/0/all/0/1\">Ricardo Piccoli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ribani_R/0/1/0/all/0/1\">Ricardo Ribani</a>",
          "description": "This work evaluates six state-of-the-art deep neural network (DNN)\narchitectures applied to the problem of enhancing camera-captured document\nimages. The results from each network were evaluated both qualitatively and\nquantitatively using Image Quality Assessment (IQA) metrics, and also compared\nwith an existing approach based on traditional computer vision techniques. The\nbest performing architectures generally produced good enhancement compared to\nthe existing algorithm, showing that it is possible to use DNNs for document\nimage enhancement. Furthermore, the best performing architectures could work as\na baseline for future investigations on document enhancement using deep\nlearning techniques. The main contributions of this paper are: a baseline of\ndeep learning techniques that can be further improved to provide better\nresults, and a evaluation methodology using IQA metrics for quantitatively\ncomparing the produced images from the neural networks to a ground truth.",
          "link": "http://arxiv.org/abs/2106.15286",
          "publishedOn": "2021-06-30T02:01:02.421Z",
          "wordCount": 597,
          "title": "Evaluating Deep Neural Networks for Image Document Enhancement. (arXiv:2106.15286v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15268",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Soares_D/0/1/0/all/0/1\">Daniel de Barros Soares</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Andrieux_F/0/1/0/all/0/1\">Fran&#xe7;ois Andrieux</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Hell_B/0/1/0/all/0/1\">Bastien Hell</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Lenhardt_J/0/1/0/all/0/1\">Julien Lenhardt</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Badosa_J/0/1/0/all/0/1\">Jordi Badosa</a> (3), <a href=\"http://arxiv.org/find/cs/1/au:+Gavoille_S/0/1/0/all/0/1\">Sylvain Gavoille</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Gaiffas_S/0/1/0/all/0/1\">St&#xe9;phane Gaiffas</a> (1, 4 and 5), <a href=\"http://arxiv.org/find/cs/1/au:+Bacry_E/0/1/0/all/0/1\">Emmanuel Bacry</a> (1 and 6), ((1) namR, Paris, France, (2) ENSTA Paris, France, (3) LMD, Ecole polytechnique, IP Paris, Palaiseau, France, (4) LPSM, Universit&#xe9; de Paris, France, (5) DMA, Ecole normale sup&#xe9;rieure, Paris, France, (6) CEREMADE, Universit&#xe9; Paris Dauphine, Paris, France)",
          "description": "Estimating the amount of electricity that can be produced by rooftop\nphotovoltaic systems is a time-consuming process that requires on-site\nmeasurements, a difficult task to achieve on a large scale. In this paper, we\npresent an approach to estimate the solar potential of rooftops based on their\nlocation and architectural characteristics, as well as the amount of solar\nradiation they receive annually. Our technique uses computer vision to achieve\nsemantic segmentation of roof sections and roof objects on the one hand, and a\nmachine learning model based on structured building features to predict roof\npitch on the other hand. We then compute the azimuth and maximum number of\nsolar panels that can be installed on a rooftop with geometric approaches.\nFinally, we compute precise shading masks and combine them with solar\nirradiation data that enables us to estimate the yearly solar potential of a\nrooftop.",
          "link": "http://arxiv.org/abs/2106.15268",
          "publishedOn": "2021-06-30T02:01:02.416Z",
          "wordCount": 655,
          "title": "Predicting the Solar Potential of Rooftops using Image Segmentation and Structured Data. (arXiv:2106.15268v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15278",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1\">Geeho Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1\">Bohyung Han</a>",
          "description": "Visual recognition tasks are often limited to dealing with a small subset of\nclasses simply because the labels for the remaining classes are unavailable. We\nare interested in identifying novel concepts in a dataset through\nrepresentation learning based on the examples in both labeled and unlabeled\nclasses, and extending the horizon of recognition to both known and novel\nclasses. To address this challenging task, we propose a combinatorial learning\napproach, which naturally clusters the examples in unseen classes using the\ncompositional knowledge given by multiple supervised meta-classifiers on\nheterogeneous label spaces. We also introduce a metric learning strategy to\nestimate pairwise pseudo-labels for improving representations of unlabeled\nexamples, which preserves semantic relations across known and novel classes\neffectively. The proposed algorithm discovers novel concepts via a joint\noptimization of enhancing the discrimitiveness of unseen classes as well as\nlearning the representations of known classes generalizable to novel ones. Our\nextensive experiments demonstrate remarkable performance gains by the proposed\napproach in multiple image retrieval and novel class discovery benchmarks.",
          "link": "http://arxiv.org/abs/2106.15278",
          "publishedOn": "2021-06-30T02:01:02.411Z",
          "wordCount": 609,
          "title": "Open-Set Representation Learning through Combinatorial Embedding. (arXiv:2106.15278v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15239",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zahirnia_K/0/1/0/all/0/1\">Kiarash Zahirnia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sakhuja_A/0/1/0/all/0/1\">Ankita Sakhuja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schulte_O/0/1/0/all/0/1\">Oliver Schulte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nadaf_P/0/1/0/all/0/1\">Parmis Nadaf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1\">Ke Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xia Hu</a>",
          "description": "Recent work on graph generative models has made remarkable progress towards\ngenerating increasingly realistic graphs, as measured by global graph features\nsuch as degree distribution, density, and clustering coefficients. Deep\ngenerative models have also made significant advances through better modelling\nof the local correlations in the graph topology, which have been very useful\nfor predicting unobserved graph components, such as the existence of a link or\nthe class of a node, from nearby observed graph components. A complete\nscientific understanding of graph data should address both global and local\nstructure. In this paper, we propose a joint model for both as complementary\nobjectives in a graph VAE framework. Global structure is captured by\nincorporating graph kernels in a probabilistic model whose loss function is\nclosely related to the maximum mean discrepancy(MMD) between the global\nstructures of the reconstructed and the input graphs. The ELBO objective\nderived from the model regularizes a standard local link reconstruction term\nwith an MMD term. Our experiments demonstrate a significant improvement in the\nrealism of the generated graph structures, typically by 1-2 orders of magnitude\nof graph structure metrics, compared to leading graph VAEand GAN models. Local\nlink reconstruction improves as well in many cases.",
          "link": "http://arxiv.org/abs/2106.15239",
          "publishedOn": "2021-06-30T02:01:02.405Z",
          "wordCount": 633,
          "title": "Generating the Graph Gestalt: Kernel-Regularized Graph Representation Learning. (arXiv:2106.15239v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15130",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Conti_M/0/1/0/all/0/1\">Mauro Conti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milani_S/0/1/0/all/0/1\">Simone Milani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nowroozi_E/0/1/0/all/0/1\">Ehsan Nowroozi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orazi_G/0/1/0/all/0/1\">Gabriele Orazi</a>",
          "description": "The last-generation video conferencing software allows users to utilize a\nvirtual background to conceal their personal environment due to privacy\nconcerns, especially in official meetings with other employers. On the other\nhand, users maybe want to fool people in the meeting by considering the virtual\nbackground to conceal where they are. In this case, developing tools to\nunderstand the virtual background utilize for fooling people in meeting plays\nan important role. Besides, such detectors must prove robust against different\nkinds of attacks since a malicious user can fool the detector by applying a set\nof adversarial editing steps on the video to conceal any revealing footprint.\nIn this paper, we study the feasibility of an efficient tool to detect whether\na videoconferencing user background is real. In particular, we provide the\nfirst tool which computes pixel co-occurrences matrices and uses them to search\nfor inconsistencies among spectral and spatial bands. Our experiments confirm\nthat cross co-occurrences matrices improve the robustness of the detector\nagainst different kinds of attacks. This work's performance is especially\nnoteworthy with regard to color SPAM features. Moreover, the performance\nespecially is significant with regard to robustness versus post-processing,\nlike geometric transformations, filtering, contrast enhancement, and JPEG\ncompression with different quality factors.",
          "link": "http://arxiv.org/abs/2106.15130",
          "publishedOn": "2021-06-30T02:01:02.400Z",
          "wordCount": 669,
          "title": "Do Not Deceive Your Employer with a Virtual Background: A Video Conferencing Manipulation-Detection System. (arXiv:2106.15130v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14993",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chang_M/0/1/0/all/0/1\">Michael Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaushik_S/0/1/0/all/0/1\">Sidhant Kaushik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1\">Sergey Levine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Griffiths_T/0/1/0/all/0/1\">Thomas L. Griffiths</a>",
          "description": "Many transfer problems require re-using previously optimal decisions for\nsolving new tasks, which suggests the need for learning algorithms that can\nmodify the mechanisms for choosing certain actions independently of those for\nchoosing others. However, there is currently no formalism nor theory for how to\nachieve this kind of modular credit assignment. To answer this question, we\ndefine modular credit assignment as a constraint on minimizing the algorithmic\nmutual information among feedback signals for different decisions. We introduce\nwhat we call the modularity criterion for testing whether a learning algorithm\nsatisfies this constraint by performing causal analysis on the algorithm\nitself. We generalize the recently proposed societal decision-making framework\nas a more granular formalism than the Markov decision process to prove that for\ndecision sequences that do not contain cycles, certain single-step temporal\ndifference action-value methods meet this criterion while all policy-gradient\nmethods do not. Empirical evidence suggests that such action-value methods are\nmore sample efficient than policy-gradient methods on transfer problems that\nrequire only sparse changes to a sequence of previously optimal decisions.",
          "link": "http://arxiv.org/abs/2106.14993",
          "publishedOn": "2021-06-30T02:01:02.386Z",
          "wordCount": 647,
          "title": "Modularity in Reinforcement Learning via Algorithmic Independence in Credit Assignment. (arXiv:2106.14993v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15083",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kulits_P/0/1/0/all/0/1\">Peter Kulits</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wall_J/0/1/0/all/0/1\">Jake Wall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bedetti_A/0/1/0/all/0/1\">Anka Bedetti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henley_M/0/1/0/all/0/1\">Michelle Henley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beery_S/0/1/0/all/0/1\">Sara Beery</a>",
          "description": "African elephants are vital to their ecosystems, but their populations are\nthreatened by a rise in human-elephant conflict and poaching. Monitoring\npopulation dynamics is essential in conservation efforts; however, tracking\nelephants is a difficult task, usually relying on the invasive and sometimes\ndangerous placement of GPS collars. Although there have been many recent\nsuccesses in the use of computer vision techniques for automated identification\nof other species, identification of elephants is extremely difficult and\ntypically requires expertise as well as familiarity with elephants in the\npopulation. We have built and deployed a web-based platform and database for\nhuman-in-the-loop re-identification of elephants combining manual attribute\nlabeling and state-of-the-art computer vision algorithms, known as\nElephantBook. Our system is currently in use at the Mara Elephant Project,\nhelping monitor the protected and at-risk population of elephants in the\nGreater Maasai Mara ecosystem. ElephantBook makes elephant re-identification\nusable by non-experts and scalable for use by multiple conservation NGOs.",
          "link": "http://arxiv.org/abs/2106.15083",
          "publishedOn": "2021-06-30T02:01:02.381Z",
          "wordCount": 595,
          "title": "ElephantBook: A Semi-Automated Human-in-the-Loop System for Elephant Re-Identification. (arXiv:2106.15083v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15230",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Demir_C/0/1/0/all/0/1\">Caglar Demir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moussallem_D/0/1/0/all/0/1\">Diego Moussallem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heindorf_S/0/1/0/all/0/1\">Stefan Heindorf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ngomo_A/0/1/0/all/0/1\">Axel-Cyrille Ngonga Ngomo</a>",
          "description": "Knowledge graph embedding research has mainly focused on the two smallest\nnormed division algebras, $\\mathbb{R}$ and $\\mathbb{C}$. Recent results suggest\nthat trilinear products of quaternion-valued embeddings can be a more effective\nmeans to tackle link prediction. In addition, models based on convolutions on\nreal-valued embeddings often yield state-of-the-art results for link\nprediction. In this paper, we investigate a composition of convolution\noperations with hypercomplex multiplications. We propose the four approaches\nQMult, OMult, ConvQ and ConvO to tackle the link prediction problem. QMult and\nOMult can be considered as quaternion and octonion extensions of previous\nstate-of-the-art approaches, including DistMult and ComplEx. ConvQ and ConvO\nbuild upon QMult and OMult by including convolution operations in a way\ninspired by the residual learning framework. We evaluated our approaches on\nseven link prediction datasets including WN18RR, FB15K-237 and YAGO3-10.\nExperimental results suggest that the benefits of learning hypercomplex-valued\nvector representations become more apparent as the size and complexity of the\nknowledge graph grows. ConvO outperforms state-of-the-art approaches on\nFB15K-237 in MRR, Hit@1 and Hit@3, while QMult, OMult, ConvQ and ConvO\noutperform state-of-the-approaches on YAGO3-10 in all metrics. Results also\nsuggest that link prediction performances can be further improved via\nprediction averaging. To foster reproducible research, we provide an\nopen-source implementation of approaches, including training and evaluation\nscripts as well as pretrained models.",
          "link": "http://arxiv.org/abs/2106.15230",
          "publishedOn": "2021-06-30T02:01:02.376Z",
          "wordCount": 648,
          "title": "Convolutional Hypercomplex Embeddings for Link Prediction. (arXiv:2106.15230v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15292",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Patel_D/0/1/0/all/0/1\">Deep Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sastry_P/0/1/0/all/0/1\">P.S. Sastry</a>",
          "description": "Deep Neural Networks (DNNs) have been shown to be susceptible to memorization\nor overfitting in the presence of noisily labelled data. For the problem of\nrobust learning under such noisy data, several algorithms have been proposed. A\nprominent class of algorithms rely on sample selection strategies, motivated by\ncurriculum learning. For example, many algorithms use the `small loss trick'\nwherein a fraction of samples with loss values below a certain threshold are\nselected for training. These algorithms are sensitive to such thresholds, and\nit is difficult to fix or learn these thresholds. Often, these algorithms also\nrequire information such as label noise rates which are typically unavailable\nin practice. In this paper, we propose a data-dependent, adaptive sample\nselection strategy that relies only on batch statistics of a given mini-batch\nto provide robustness against label noise. The algorithm does not have any\nadditional hyperparameters for sample selection, does not need any information\non noise rates, and does not need access to separate data with clean labels. We\nempirically demonstrate the effectiveness of our algorithm on benchmark\ndatasets.",
          "link": "http://arxiv.org/abs/2106.15292",
          "publishedOn": "2021-06-30T02:01:02.361Z",
          "wordCount": 620,
          "title": "Adaptive Sample Selection for Robust Learning under Label Noise. (arXiv:2106.15292v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15207",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sherman_U/0/1/0/all/0/1\">Uri Sherman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koren_T/0/1/0/all/0/1\">Tomer Koren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mansour_Y/0/1/0/all/0/1\">Yishay Mansour</a>",
          "description": "We study online convex optimization in the random order model, recently\nproposed by \\citet{garber2020online}, where the loss functions may be chosen by\nan adversary, but are then presented to the online algorithm in a uniformly\nrandom order. Focusing on the scenario where the cumulative loss function is\n(strongly) convex, yet individual loss functions are smooth but might be\nnon-convex, we give algorithms that achieve the optimal bounds and\nsignificantly outperform the results of \\citet{garber2020online}, completely\nremoving the dimension dependence and improving their scaling with respect to\nthe strong convexity parameter. Our analysis relies on novel connections\nbetween algorithmic stability and generalization for sampling\nwithout-replacement analogous to those studied in the with-replacement\ni.i.d.~setting, as well as on a refined average stability analysis of\nstochastic gradient descent.",
          "link": "http://arxiv.org/abs/2106.15207",
          "publishedOn": "2021-06-30T02:01:02.356Z",
          "wordCount": 563,
          "title": "Optimal Rates for Random Order Online Optimization. (arXiv:2106.15207v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15214",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Marmin_A/0/1/0/all/0/1\">Arthur Marmin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goulart_J/0/1/0/all/0/1\">Jos&#xe9; Henrique de Morais Goulart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fevotte_C/0/1/0/all/0/1\">C&#xe9;dric F&#xe9;votte</a>",
          "description": "This article proposes new multiplicative updates for nonnegative matrix\nfactorization (NMF) with the $\\beta$-divergence objective function. Our new\nupdates are derived from a joint majorization-minimization (MM) scheme, in\nwhich an auxiliary function (a tight upper bound of the objective function) is\nbuilt for the two factors jointly and minimized at each iteration. This is in\ncontrast with the classic approach in which the factors are optimized\nalternately and a MM scheme is applied to each factor individually. Like the\nclassic approach, our joint MM algorithm also results in multiplicative updates\nthat are simple to implement. They however yield a significant drop of\ncomputation time (for equally good solutions), in particular for some\n$\\beta$-divergences of important applicative interest, such as the squared\nEuclidean distance and the Kullback-Leibler or Itakura-Saito divergences. We\nreport experimental results using diverse datasets: face images, audio\nspectrograms, hyperspectral data and song play counts. Depending on the value\nof $\\beta$ and on the dataset, our joint MM approach yields a CPU time\nreduction of about $10\\%$ to $78\\%$ in comparison to the classic alternating\nscheme.",
          "link": "http://arxiv.org/abs/2106.15214",
          "publishedOn": "2021-06-30T02:01:02.350Z",
          "wordCount": 615,
          "title": "Joint Majorization-Minimization for Nonnegative Matrix Factorization with the $\\beta$-divergence. (arXiv:2106.15214v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15223",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Radstok_W/0/1/0/all/0/1\">Wessel Radstok</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chekol_M/0/1/0/all/0/1\">Mel Chekol</a>",
          "description": "The inclusion of temporal scopes of facts in knowledge graph embedding (KGE)\npresents significant opportunities for improving the resulting embeddings, and\nconsequently for increased performance in downstream applications. Yet, little\nresearch effort has focussed on this area and much of the carried out research\nreports only marginally improved results compared to models trained without\ntemporal scopes (static models). Furthermore, rather than leveraging existing\nwork on static models, they introduce new models specific to temporal knowledge\ngraphs. We propose a novel perspective that takes advantage of the power of\nexisting static embedding models by focussing effort on manipulating the data\ninstead. Our method, SpliMe, draws inspiration from the field of signal\nprocessing and early work in graph embedding. We show that SpliMe competes with\nor outperforms the current state of the art in temporal KGE. Additionally, we\nuncover issues with the procedure currently used to assess the performance of\nstatic models on temporal graphs and introduce two ways to counteract them.",
          "link": "http://arxiv.org/abs/2106.15223",
          "publishedOn": "2021-06-30T02:01:02.345Z",
          "wordCount": 598,
          "title": "Leveraging Static Models for Link Prediction in Temporal Knowledge Graphs. (arXiv:2106.15223v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15021",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Venieris_S/0/1/0/all/0/1\">Stylianos I. Venieris</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panopoulos_I/0/1/0/all/0/1\">Ioannis Panopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leontiadis_I/0/1/0/all/0/1\">Ilias Leontiadis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venieris_I/0/1/0/all/0/1\">Iakovos S. Venieris</a>",
          "description": "The unprecedented performance of deep neural networks (DNNs) has led to large\nstrides in various Artificial Intelligence (AI) inference tasks, such as object\nand speech recognition. Nevertheless, deploying such AI models across commodity\ndevices faces significant challenges: large computational cost, multiple\nperformance objectives, hardware heterogeneity and a common need for high\naccuracy, together pose critical problems to the deployment of DNNs across the\nvarious embedded and mobile devices in the wild. As such, we have yet to\nwitness the mainstream usage of state-of-the-art deep learning algorithms\nacross consumer devices. In this paper, we provide preliminary answers to this\npotentially game-changing question by presenting an array of design techniques\nfor efficient AI systems. We start by examining the major roadblocks when\ntargeting both programmable processors and custom accelerators. Then, we\npresent diverse methods for achieving real-time performance following a\ncross-stack approach. These span model-, system- and hardware-level techniques,\nand their combination. Our findings provide illustrative examples of AI systems\nthat do not overburden mobile hardware, while also indicating how they can\nimprove inference accuracy. Moreover, we showcase how custom ASIC- and\nFPGA-based accelerators can be an enabling factor for next-generation AI\napplications, such as multi-DNN systems. Collectively, these results highlight\nthe critical need for further exploration as to how the various cross-stack\nsolutions can be best combined in order to bring the latest advances in deep\nlearning close to users, in a robust and efficient manner.",
          "link": "http://arxiv.org/abs/2106.15021",
          "publishedOn": "2021-06-30T02:01:02.340Z",
          "wordCount": 707,
          "title": "How to Reach Real-Time AI on Consumer Devices? Solutions for Programmable and Custom Architectures. (arXiv:2106.15021v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15185",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Dongha Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yongchan Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kunwoong Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Yongdai Kim</a>",
          "description": "In many classification problems, collecting massive clean-annotated data is\nnot easy, and thus a lot of researches have been done to handle data with noisy\nlabels. Most recent state-of-art solutions for noisy label problems are built\non the small-loss strategy which exploits the memorization effect. While it is\na powerful tool, the memorization effect has several drawbacks. The\nperformances are sensitive to the choice of a training epoch required for\nutilizing the memorization effect. In addition, when the labels are heavily\ncontaminated or imbalanced, the memorization effect may not occur in which case\nthe methods based on the small-loss strategy fail to identify clean labeled\ndata. We introduce a new method called INN(Integration with the Nearest\nNeighborhoods) to refine clean labeled data from training data with noisy\nlabels. The proposed method is based on a new discovery that a prediction\npattern at neighbor regions of clean labeled data is consistently different\nfrom that of noisy labeled data regardless of training epochs. The INN method\nrequires more computation but is much stable and powerful than the small-loss\nstrategy. By carrying out various experiments, we demonstrate that the INN\nmethod resolves the shortcomings in the memorization effect successfully and\nthus is helpful to construct more accurate deep prediction models with training\ndata with noisy labels.",
          "link": "http://arxiv.org/abs/2106.15185",
          "publishedOn": "2021-06-30T02:01:02.334Z",
          "wordCount": 663,
          "title": "INN: A Method Identifying Clean-annotated Samples via Consistency Effect in Deep Neural Networks. (arXiv:2106.15185v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15212",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Spooner_T/0/1/0/all/0/1\">Thomas Spooner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dervovic_D/0/1/0/all/0/1\">Danial Dervovic</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_J/0/1/0/all/0/1\">Jason Long</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shepard_J/0/1/0/all/0/1\">Jon Shepard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiahao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Magazzeni_D/0/1/0/all/0/1\">Daniele Magazzeni</a>",
          "description": "We present a new method for counterfactual explanations (CFEs) based on\nBayesian optimisation that applies to both classification and regression\nmodels. Our method is a globally convergent search algorithm with support for\narbitrary regression models and constraints like feature sparsity and\nactionable recourse, and furthermore can answer multiple counterfactual\nquestions in parallel while learning from previous queries. We formulate CFE\nsearch for regression models in a rigorous mathematical framework using\ndifferentiable potentials, which resolves robustness issues in threshold-based\nobjectives. We prove that in this framework, (a) verifying the existence of\ncounterfactuals is NP-complete; and (b) that finding instances using such\npotentials is CLS-complete. We describe a unified algorithm for CFEs using a\nspecialised acquisition function that composes both expected improvement and an\nexponential-polynomial (EP) family with desirable properties. Our evaluation on\nreal-world benchmark domains demonstrate high sample-efficiency and precision.",
          "link": "http://arxiv.org/abs/2106.15212",
          "publishedOn": "2021-06-30T02:01:02.329Z",
          "wordCount": 584,
          "title": "Counterfactual Explanations for Arbitrary Regression Models. (arXiv:2106.15212v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14956",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Turan_B/0/1/0/all/0/1\">Berkay Turan</a>, <a href=\"http://arxiv.org/find/math/1/au:+Uribe_C/0/1/0/all/0/1\">Cesar A. Uribe</a>, <a href=\"http://arxiv.org/find/math/1/au:+Wai_H/0/1/0/all/0/1\">Hoi-To Wai</a>, <a href=\"http://arxiv.org/find/math/1/au:+Alizadeh_M/0/1/0/all/0/1\">Mahnoosh Alizadeh</a>",
          "description": "In this paper, we propose a first-order distributed optimization algorithm\nthat is provably robust to Byzantine failures-arbitrary and potentially\nadversarial behavior, where all the participating agents are prone to failure.\nWe model each agent's state over time as a two-state Markov chain that\nindicates Byzantine or trustworthy behaviors at different time instants. We set\nno restrictions on the maximum number of Byzantine agents at any given time. We\ndesign our method based on three layers of defense: 1) Temporal gradient\naveraging, 2) robust aggregation, and 3) gradient normalization. We study two\nsettings for stochastic optimization, namely Sample Average Approximation and\nStochastic Approximation, and prove that for strongly convex and smooth\nnon-convex cost functions, our algorithm achieves order-optimal statistical\nerror and convergence rates.",
          "link": "http://arxiv.org/abs/2106.14956",
          "publishedOn": "2021-06-30T02:01:02.324Z",
          "wordCount": 578,
          "title": "Robust Distributed Optimization With Randomly Corrupted Gradients. (arXiv:2106.14956v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15159",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Jayasundara_S/0/1/0/all/0/1\">Shyaman Jayasundara</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Lokuge_S/0/1/0/all/0/1\">Sandali Lokuge</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Ihalagedara_P/0/1/0/all/0/1\">Puwasuru Ihalagedara</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Herath_D/0/1/0/all/0/1\">Damayanthi Herath</a>",
          "description": "MicroRNAs (miRNAs) are endogenous small non-coding RNAs that play an\nimportant role in post-transcriptional gene regulation. However, the\nexperimental determination of miRNA sequence and structure is both expensive\nand time-consuming. Therefore, computational and machine learning-based\napproaches have been adopted to predict novel microRNAs. With the involvement\nof data science and machine learning in biology, multiple research studies have\nbeen conducted to find microRNAs with different computational methods and\ndifferent miRNA features. Multiple approaches are discussed in detail\nconsidering the learning algorithm/s used, features considered, dataset/s used\nand the criteria used in evaluations. This systematic review focuses on the\nmachine learning methods developed for miRNA identification in plants. This\nwill help researchers to gain a detailed idea about past studies and identify\nnovel paths that solve drawbacks occurred in past studies. Our findings\nhighlight the need for plant-specific computational methods for miRNA\nidentification.",
          "link": "http://arxiv.org/abs/2106.15159",
          "publishedOn": "2021-06-30T02:01:02.302Z",
          "wordCount": 577,
          "title": "Machine learning for plant microRNA prediction: A systematic review. (arXiv:2106.15159v1 [q-bio.GN])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15067",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1\">Xiang Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zihang He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Heng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yong Li</a>",
          "description": "Attention Mechanism is a widely used method for improving the performance of\nconvolutional neural networks (CNNs) on computer vision tasks. Despite its\npervasiveness, we have a poor understanding of what its effectiveness stems\nfrom. It is popularly believed that its effectiveness stems from the visual\nattention explanation, advocating focusing on the important part of input data\nrather than ingesting the entire input. In this paper, we find that there is\nonly a weak consistency between the attention weights of features and their\nimportance. Instead, we verify the crucial role of feature map multiplication\nin attention mechanism and uncover a fundamental impact of feature map\nmultiplication on the learned landscapes of CNNs: with the high order\nnon-linearity brought by the feature map multiplication, it played a\nregularization role on CNNs, which made them learn smoother and more stable\nlandscapes near real samples compared to vanilla CNNs. This smoothness and\nstability induce a more predictive and stable behavior in-between real samples,\nand make CNNs generate better. Moreover, motivated by the proposed\neffectiveness of feature map multiplication, we design feature map\nmultiplication network (FMMNet) by simply replacing the feature map addition in\nResNet with feature map multiplication. FMMNet outperforms ResNet on various\ndatasets, and this indicates that feature map multiplication plays a vital role\nin improving the performance even without finely designed attention mechanism\nin existing methods.",
          "link": "http://arxiv.org/abs/2106.15067",
          "publishedOn": "2021-06-30T02:01:02.295Z",
          "wordCount": 662,
          "title": "Towards Understanding the Effectiveness of Attention Mechanism. (arXiv:2106.15067v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15176",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pucci_R/0/1/0/all/0/1\">Rita Pucci</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martinel_N/0/1/0/all/0/1\">Niki Martinel</a>",
          "description": "Automatic image colourisation is the computer vision research path that\nstudies how to colourise greyscale images (for restoration). Deep learning\ntechniques improved image colourisation yielding astonishing results. These\ndiffer by various factors, such as structural differences, input types, user\nassistance, etc. Most of them, base the architectural structure on\nconvolutional layers with no emphasis on layers specialised in object features\nextraction. We introduce a novel downsampling upsampling architecture named\nTUCaN (Tiny UCapsNet) that exploits the collaboration of convolutional layers\nand capsule layers to obtain a neat colourisation of entities present in every\nsingle image. This is obtained by enforcing collaboration among such layers by\nskip and residual connections. We pose the problem as a per pixel colour\nclassification task that identifies colours as a bin in a quantized space. To\ntrain the network, in contrast with the standard end to end learning method, we\npropose the progressive learning scheme to extract the context of objects by\nonly manipulating the learning process without changing the model. In this\nscheme, the upsampling starts from the reconstruction of low resolution images\nand progressively grows to high resolution images throughout the training\nphase. Experimental results on three benchmark datasets show that our approach\nwith ImageNet10k dataset outperforms existing methods on standard quality\nmetrics and achieves state of the art performances on image colourisation. We\nperformed a user study to quantify the perceptual realism of the colourisation\nresults demonstrating: that progressive learning let the TUCaN achieve better\ncolours than the end to end scheme; and pointing out the limitations of the\nexisting evaluation metrics.",
          "link": "http://arxiv.org/abs/2106.15176",
          "publishedOn": "2021-06-30T02:01:02.285Z",
          "wordCount": 691,
          "title": "TUCaN: Progressively Teaching Colourisation to Capsules. (arXiv:2106.15176v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15057",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yuntao Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yinghao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_F/0/1/0/all/0/1\">Fengli Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaowen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chongjun Wang</a>",
          "description": "Unsupervised domain adaptation aims to transfer knowledge from a labeled\nsource domain to an unlabeled target domain. Previous methods focus on learning\ndomain-invariant features to decrease the discrepancy between the feature\ndistributions as well as minimizing the source error and have made remarkable\nprogress. However, a recently proposed theory reveals that such a strategy is\nnot sufficient for a successful domain adaptation. It shows that besides a\nsmall source error, both the discrepancy between the feature distributions and\nthe discrepancy between the labeling functions should be small across domains.\nThe discrepancy between the labeling functions is essentially the cross-domain\nerrors which are ignored by existing methods. To overcome this issue, in this\npaper, a novel method is proposed to integrate all the objectives into a\nunified optimization framework. Moreover, the incorrect pseudo labels widely\nused in previous methods can lead to error accumulation during learning. To\nalleviate this problem, the pseudo labels are obtained by utilizing structural\ninformation of the target domain besides source classifier and we propose a\ncurriculum learning based strategy to select the target samples with more\naccurate pseudo-labels during training. Comprehensive experiments are\nconducted, and the results validate that our approach outperforms\nstate-of-the-art methods.",
          "link": "http://arxiv.org/abs/2106.15057",
          "publishedOn": "2021-06-30T02:01:02.277Z",
          "wordCount": 632,
          "title": "Cross-domain error minimization for unsupervised domain adaptation. (arXiv:2106.15057v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15123",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Bak_T/0/1/0/all/0/1\">Taejun Bak</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bae_J/0/1/0/all/0/1\">Jae-Sung Bae</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bae_H/0/1/0/all/0/1\">Hanbin Bae</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_Y/0/1/0/all/0/1\">Young-Ik Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cho_H/0/1/0/all/0/1\">Hoon-Young Cho</a>",
          "description": "Methods for modeling and controlling prosody with acoustic features have been\nproposed for neural text-to-speech (TTS) models. Prosodic speech can be\ngenerated by conditioning acoustic features. However, synthesized speech with a\nlarge pitch-shift scale suffers from audio quality degradation, and speaker\ncharacteristics deformation. To address this problem, we propose a feed-forward\nTransformer based TTS model that is designed based on the source-filter theory.\nThis model, called FastPitchFormant, has a unique structure that handles text\nand acoustic features in parallel. With modeling each feature separately, the\ntendency that the model learns the relationship between two features can be\nmitigated.",
          "link": "http://arxiv.org/abs/2106.15123",
          "publishedOn": "2021-06-30T02:01:02.270Z",
          "wordCount": 554,
          "title": "FastPitchFormant: Source-filter based Decomposed Modeling for Speech Synthesis. (arXiv:2106.15123v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15020",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bjork_S/0/1/0/all/0/1\">Sara Bj&#xf6;rk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anfinsen_S/0/1/0/all/0/1\">Stian Normann Anfinsen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naesset_E/0/1/0/all/0/1\">Erik N&#xe6;sset</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gobakken_T/0/1/0/all/0/1\">Terje Gobakken</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zahabu_E/0/1/0/all/0/1\">Eliakimu Zahabu</a>",
          "description": "This paper studies construction of above-ground biomass (AGB) prediction maps\nfrom synthetic aperture radar (SAR) intensity images. The purpose is to improve\ntraditional regression models based on SAR intensity, trained with a limited\namount of AGB in situ measurements. Although it is costly to collect, data from\nairborne laser scanning (ALS) sensors are highly correlated with AGB.\nTherefore, we propose using AGB predictions based on ALS data as surrogate\nresponse variables for SAR data in a sequential modelling fashion. This\nincreases the amount of training data dramatically. To model the regression\nfunction between SAR intensity and ALS-predicted AGB we propose to utilise a\nconditional generative adversarial network (cGAN), i.e. the Pix2Pix\nconvolutional neural network. This enables the recreation of existing ALS-based\nAGB prediction maps. The generated synthesised ALS-based AGB predictions are\nevaluated qualitatively and quantitatively against ALS-based AGB predictions\nretrieved from a traditional non-sequential regression model trained in the\nsame area. Results show that the proposed architecture manages to capture\ncharacteristics of the actual data. This suggests that the use of ALS-guided\ngenerative models is a promising avenue for AGB prediction from SAR intensity.\nFurther research on this area has the potential of providing both large-scale\nand low-cost predictions of AGB.",
          "link": "http://arxiv.org/abs/2106.15020",
          "publishedOn": "2021-06-30T02:01:02.247Z",
          "wordCount": 658,
          "title": "Constructing Forest Biomass Prediction Maps from Radar Backscatter by Sequential Regression with a Conditional Generative Adversarial Network. (arXiv:2106.15020v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15178",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alloulah_M/0/1/0/all/0/1\">Mohammed Alloulah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arnold_M/0/1/0/all/0/1\">Maximilian Arnold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Isopoussu_A/0/1/0/all/0/1\">Anton Isopoussu</a>",
          "description": "Autonomous navigation in uninstrumented and unprepared environments is a\nfundamental demand for next generation indoor and outdoor location-based\nservices. To bring about such ambition, a suite of collaborative sensing\nmodalities is required in order to sustain performance irrespective of\nchallenging dynamic conditions. Of the many modalities on offer, inertial\ntracking plays a key role under momentary unfavourable operational conditions\nowing to its independence of the surrounding environment. However, inertial\ntracking has traditionally (i) suffered from excessive error growth and (ii)\nrequired extensive and cumbersome tuning. Both of these issues have limited the\nappeal and utility of inertial tracking. In this paper, we present DIT: a novel\nDeep learning Inertial Tracking system that overcomes prior limitations;\nnamely, by (i) significantly reducing tracking drift and (ii) seamlessly\nconstructing robust and generalisable learned models. DIT describes two core\ncontributions: (i) DIT employs a robotic platform augmented with a mechanical\nslider subsystem that automatically samples inertial signal variabilities\narising from different sensor mounting geometries. We use the platform to\ncurate in-house a 7.2 million sample dataset covering an aggregate distance of\n21 kilometres split into 11 indexed sensor mounting geometries. (ii) DIT uses\ndeep learning, optimal transport, and domain adaptation (DA) to create a model\nwhich is robust to variabilities in sensor mounting geometry. The overall\nsystem synthesises high-performance and generalisable inertial navigation\nmodels in an end-to-end, robotic-learning fashion. In our evaluation, DIT\noutperforms an industrial-grade sensor fusion baseline by 10x (90th percentile)\nand a state-of-the-art adversarial DA technique by > 2.5x in performance (90th\npercentile) and >10x in training time.",
          "link": "http://arxiv.org/abs/2106.15178",
          "publishedOn": "2021-06-30T02:01:02.234Z",
          "wordCount": 690,
          "title": "Towards Generalisable Deep Inertial Tracking via Geometry-Aware Learning. (arXiv:2106.15178v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14999",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Mummadi_C/0/1/0/all/0/1\">Chaithanya Kumar Mummadi</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Hutmacher_R/0/1/0/all/0/1\">Robin Hutmacher</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Rambach_K/0/1/0/all/0/1\">Kilian Rambach</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Levinkov_E/0/1/0/all/0/1\">Evgeny Levinkov</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Brox_T/0/1/0/all/0/1\">Thomas Brox</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Metzen_J/0/1/0/all/0/1\">Jan Hendrik Metzen</a>",
          "description": "Deep neural networks often exhibit poor performance on data that is unlikely\nunder the train-time data distribution, for instance data affected by\ncorruptions. Previous works demonstrate that test-time adaptation to data\nshift, for instance using entropy minimization, effectively improves\nperformance on such shifted distributions. This paper focuses on the fully\ntest-time adaptation setting, where only unlabeled data from the target\ndistribution is required. This allows adapting arbitrary pretrained networks.\nSpecifically, we propose a novel loss that improves test-time adaptation by\naddressing both premature convergence and instability of entropy minimization.\nThis is achieved by replacing the entropy by a non-saturating surrogate and\nadding a diversity regularizer based on batch-wise entropy maximization that\nprevents convergence to trivial collapsed solutions. Moreover, we propose to\nprepend an input transformation module to the network that can partially undo\ntest-time distribution shifts. Surprisingly, this preprocessing can be learned\nsolely using the fully test-time adaptation loss in an end-to-end fashion\nwithout any target domain labels or source domain data. We show that our\napproach outperforms previous work in improving the robustness of publicly\navailable pretrained image classifiers to common corruptions on such\nchallenging benchmarks as ImageNet-C.",
          "link": "http://arxiv.org/abs/2106.14999",
          "publishedOn": "2021-06-30T02:01:02.221Z",
          "wordCount": 642,
          "title": "Test-Time Adaptation to Distribution Shift by Confidence Maximization and Input Transformation. (arXiv:2106.14999v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15216",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Su_L/0/1/0/all/0/1\">Lili Su</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Xu_J/0/1/0/all/0/1\">Jiaming Xu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Yang_P/0/1/0/all/0/1\">Pengkun Yang</a>",
          "description": "Federated Learning (FL) is a promising framework that has great potentials in\nprivacy preservation and in lowering the computation load at the cloud. FedAvg\nand FedProx are two widely adopted algorithms. However, recent work raised\nconcerns on these two methods: (1) their fixed points do not correspond to the\nstationary points of the original optimization problem, and (2) the common\nmodel found might not generalize well locally.\n\nIn this paper, we alleviate these concerns. Towards this, we adopt the\nstatistical learning perspective yet allow the distributions to be\nheterogeneous and the local data to be unbalanced. We show, in the general\nkernel regression setting, that both FedAvg and FedProx converge to the\nminimax-optimal error rates. Moreover, when the kernel function has a finite\nrank, the convergence is exponentially fast. Our results further analytically\nquantify the impact of the model heterogeneity and characterize the federation\ngain - the reduction of the estimation error for a worker to join the federated\nlearning compared to the best local estimator. To the best of our knowledge, we\nare the first to show the achievability of minimax error rates under FedAvg and\nFedProx, and the first to characterize the gains in joining FL. Numerical\nexperiments further corroborate our theoretical findings on the statistical\noptimality of FedAvg and FedProx and the federation gains.",
          "link": "http://arxiv.org/abs/2106.15216",
          "publishedOn": "2021-06-30T02:01:02.215Z",
          "wordCount": 664,
          "title": "Achieving Statistical Optimality of Federated Learning: Beyond Stationary Points. (arXiv:2106.15216v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15153",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Yang_J/0/1/0/all/0/1\">Jinhyeok Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bae_J/0/1/0/all/0/1\">Jae-Sung Bae</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bak_T/0/1/0/all/0/1\">Taejun Bak</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kim_Y/0/1/0/all/0/1\">Youngik Kim</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cho_H/0/1/0/all/0/1\">Hoon-Young Cho</a>",
          "description": "Recent advances in neural multi-speaker text-to-speech (TTS) models have\nenabled the generation of reasonably good speech quality with a single model\nand made it possible to synthesize the speech of a speaker with limited\ntraining data. Fine-tuning to the target speaker data with the multi-speaker\nmodel can achieve better quality, however, there still exists a gap compared to\nthe real speech sample and the model depends on the speaker. In this work, we\npropose GANSpeech, which is a high-fidelity multi-speaker TTS model that adopts\nthe adversarial training method to a non-autoregressive multi-speaker TTS\nmodel. In addition, we propose simple but efficient automatic scaling methods\nfor feature matching loss used in adversarial training. In the subjective\nlistening tests, GANSpeech significantly outperformed the baseline\nmulti-speaker FastSpeech and FastSpeech2 models, and showed a better MOS score\nthan the speaker-specific fine-tuned FastSpeech2.",
          "link": "http://arxiv.org/abs/2106.15153",
          "publishedOn": "2021-06-30T02:01:02.210Z",
          "wordCount": 595,
          "title": "GANSpeech: Adversarial Training for High-Fidelity Multi-Speaker Speech Synthesis. (arXiv:2106.15153v1 [eess.AS])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15023",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bryniarski_O/0/1/0/all/0/1\">Oliver Bryniarski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hingun_N/0/1/0/all/0/1\">Nabeel Hingun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pachuca_P/0/1/0/all/0/1\">Pedro Pachuca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_V/0/1/0/all/0/1\">Vincent Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carlini_N/0/1/0/all/0/1\">Nicholas Carlini</a>",
          "description": "Evading adversarial example detection defenses requires finding adversarial\nexamples that must simultaneously (a) be misclassified by the model and (b) be\ndetected as non-adversarial. We find that existing attacks that attempt to\nsatisfy multiple simultaneous constraints often over-optimize against one\nconstraint at the cost of satisfying another. We introduce Orthogonal Projected\nGradient Descent, an improved attack technique to generate adversarial examples\nthat avoids this problem by orthogonalizing the gradients when running standard\ngradient-based attacks. We use our technique to evade four state-of-the-art\ndetection defenses, reducing their accuracy to 0% while maintaining a 0%\ndetection rate.",
          "link": "http://arxiv.org/abs/2106.15023",
          "publishedOn": "2021-06-30T02:01:02.175Z",
          "wordCount": 536,
          "title": "Evading Adversarial Example Detection Defenses with Orthogonal Projected Gradient Descent. (arXiv:2106.15023v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15017",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1\">Rex Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fazio_S/0/1/0/all/0/1\">Sarina A Fazio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Huanle Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramli_A/0/1/0/all/0/1\">Albara Ah Ramli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adams_J/0/1/0/all/0/1\">Jason Yeates Adams</a>",
          "description": "With the development of the Internet of Things(IoT) and Artificial\nIntelligence(AI) technologies, human activity recognition has enabled various\napplications, such as smart homes and assisted living. In this paper, we target\na new healthcare application of human activity recognition, early mobility\nrecognition for Intensive Care Unit(ICU) patients. Early mobility is essential\nfor ICU patients who suffer from long-time immobilization. Our system includes\naccelerometer-based data collection from ICU patients and an AI model to\nrecognize patients' early mobility. To improve the model accuracy and\nstability, we identify features that are insensitive to sensor orientations and\npropose a segment voting process that leverages a majority voting strategy to\nrecognize each segment's activity. Our results show that our system improves\nmodel accuracy from 77.78\\% to 81.86\\% and reduces the model instability\n(standard deviation) from 16.69\\% to 6.92\\%, compared to the same AI model\nwithout our feature engineering and segment voting process.",
          "link": "http://arxiv.org/abs/2106.15017",
          "publishedOn": "2021-06-30T02:01:02.164Z",
          "wordCount": 593,
          "title": "Early Mobility Recognition for Intensive Care Unit Patients Using Accelerometers. (arXiv:2106.15017v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15049",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Al_Rakhami_M/0/1/0/all/0/1\">Mabrook S. Al-Rakhami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gumaei1_A/0/1/0/all/0/1\">Abdu Gumaei1</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Altaf_M/0/1/0/all/0/1\">Meteb Altaf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassan_M/0/1/0/all/0/1\">Mohammad Mehedi Hassan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alkhamees_B/0/1/0/all/0/1\">Bader Fahad Alkhamees</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muhammad_K/0/1/0/all/0/1\">Khan Muhammad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fortino_G/0/1/0/all/0/1\">Giancarlo Fortino</a>",
          "description": "Fall prevalence is high among elderly people, which is challenging due to the\nsevere consequences of falling. This is why rapid assistance is a critical\ntask. Ambient assisted living (AAL) uses recent technologies such as 5G\nnetworks and the internet of medical things (IoMT) to address this research\narea. Edge computing can reduce the cost of cloud communication, including high\nlatency and bandwidth use, by moving conventional healthcare services and\napplications closer to end-users. Artificial intelligence (AI) techniques such\nas deep learning (DL) have been used recently for automatic fall detection, as\nwell as supporting healthcare services. However, DL requires a vast amount of\ndata and substantial processing power to improve its performance for the IoMT\nlinked to the traditional edge computing environment. This research proposes an\neffective fall detection framework based on DL algorithms and mobile edge\ncomputing (MEC) within 5G wireless networks, the aim being to empower\nIoMT-based healthcare applications. We also propose the use of a deep gated\nrecurrent unit (DGRU) neural network to improve the accuracy of existing\nDL-based fall detection methods. DGRU has the advantage of dealing with\ntime-series IoMT data, and it can reduce the number of parameters and avoid the\nvanishing gradient problem. The experimental results on two public datasets\nshow that the DGRU model of the proposed framework achieves higher accuracy\nrates compared to the current related works on the same datasets.",
          "link": "http://arxiv.org/abs/2106.15049",
          "publishedOn": "2021-06-30T02:01:02.149Z",
          "wordCount": 680,
          "title": "FallDeF5: A Fall Detection Framework Using 5G-based Deep Gated Recurrent Unit Networks. (arXiv:2106.15049v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15002",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Siegel_J/0/1/0/all/0/1\">Jonathan W. Siegel</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Xu_J/0/1/0/all/0/1\">Jinchao Xu</a>",
          "description": "We consider the variation space corresponding to a dictionary of functions in\n$L^2(\\Omega)$ and present the basic theory of approximation in these spaces.\nSpecifically, we compare the definition based on integral representations with\nthe definition in terms of convex hulls. We show that in many cases, including\nthe dictionaries corresponding to shallow ReLU$^k$ networks and a dictionary of\ndecaying Fourier modes, that the two definitions coincide. We also give a\npartial characterization of the variation space for shallow ReLU$^k$ networks\nand show that the variation space with respect to the dictionary of decaying\nFourier modes corresponds to the Barron spectral space.",
          "link": "http://arxiv.org/abs/2106.15002",
          "publishedOn": "2021-06-30T02:01:02.109Z",
          "wordCount": 546,
          "title": "Characterization of the Variation Spaces Corresponding to Shallow Neural Networks. (arXiv:2106.15002v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14976",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Song_Y/0/1/0/all/0/1\">Yifei Song</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chang_H/0/1/0/all/0/1\">Hao-Hsuan Chang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhou Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jere_S/0/1/0/all/0/1\">Shashank Jere</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_L/0/1/0/all/0/1\">Lingjia Liu</a>",
          "description": "Due to the growing volume of data traffic produced by the surge of Internet\nof Things (IoT) devices, the demand for radio spectrum resources is approaching\ntheir limitation defined by Federal Communications Commission (FCC). To this\nend, Dynamic Spectrum Access (DSA) is considered as a promising technology to\nhandle this spectrum scarcity. However, standard DSA techniques often rely on\nanalytical modeling wireless networks, making its application intractable in\nunder-measured network environments. Therefore, utilizing neural networks to\napproximate the network dynamics is an alternative approach. In this article,\nwe introduce a Federated Learning (FL) based framework for the task of DSA,\nwhere FL is a distributive machine learning framework that can reserve the\nprivacy of network terminals under heterogeneous data distributions. We discuss\nthe opportunities, challenges, and opening problems of this framework. To\nevaluate its feasibility, we implement a Multi-Agent Reinforcement Learning\n(MARL)-based FL as a realization associated with its initial evaluation\nresults.",
          "link": "http://arxiv.org/abs/2106.14976",
          "publishedOn": "2021-06-30T02:01:02.101Z",
          "wordCount": 584,
          "title": "Federated Dynamic Spectrum Access. (arXiv:2106.14976v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14942",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bergman_A/0/1/0/all/0/1\">Alexander W. Bergman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kellnhofer_P/0/1/0/all/0/1\">Petr Kellnhofer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wetzstein_G/0/1/0/all/0/1\">Gordon Wetzstein</a>",
          "description": "Novel view synthesis is a long-standing problem in machine learning and\ncomputer vision. Significant progress has recently been made in developing\nneural scene representations and rendering techniques that synthesize\nphotorealistic images from arbitrary views. These representations, however, are\nextremely slow to train and often also slow to render. Inspired by neural\nvariants of image-based rendering, we develop a new neural rendering approach\nwith the goal of quickly learning a high-quality representation which can also\nbe rendered in real-time. Our approach, MetaNLR++, accomplishes this by using a\nunique combination of a neural shape representation and 2D CNN-based image\nfeature extraction, aggregation, and re-projection. To push representation\nconvergence times down to minutes, we leverage meta learning to learn neural\nshape and image feature priors which accelerate training. The optimized shape\nand image features can then be extracted using traditional graphics techniques\nand rendered in real time. We show that MetaNLR++ achieves similar or better\nnovel view synthesis results in a fraction of the time that competing methods\nrequire.",
          "link": "http://arxiv.org/abs/2106.14942",
          "publishedOn": "2021-06-30T02:01:02.063Z",
          "wordCount": 616,
          "title": "Fast Training of Neural Lumigraph Representations using Meta Learning. (arXiv:2106.14942v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15013",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Stoger_D/0/1/0/all/0/1\">Dominik St&#xf6;ger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soltanolkotabi_M/0/1/0/all/0/1\">Mahdi Soltanolkotabi</a>",
          "description": "Recently there has been significant theoretical progress on understanding the\nconvergence and generalization of gradient-based methods on nonconvex losses\nwith overparameterized models. Nevertheless, many aspects of optimization and\ngeneralization and in particular the critical role of small random\ninitialization are not fully understood. In this paper, we take a step towards\ndemystifying this role by proving that small random initialization followed by\na few iterations of gradient descent behaves akin to popular spectral methods.\nWe also show that this implicit spectral bias from small random initialization,\nwhich is provably more prominent for overparameterized models, also puts the\ngradient descent iterations on a particular trajectory towards solutions that\nare not only globally optimal but also generalize well. Concretely, we focus on\nthe problem of reconstructing a low-rank matrix from a few measurements via a\nnatural nonconvex formulation. In this setting, we show that the trajectory of\nthe gradient descent iterations from small random initialization can be\napproximately decomposed into three phases: (I) a spectral or alignment phase\nwhere we show that that the iterates have an implicit spectral bias akin to\nspectral initialization allowing us to show that at the end of this phase the\ncolumn space of the iterates and the underlying low-rank matrix are\nsufficiently aligned, (II) a saddle avoidance/refinement phase where we show\nthat the trajectory of the gradient iterates moves away from certain degenerate\nsaddle points, and (III) a local refinement phase where we show that after\navoiding the saddles the iterates converge quickly to the underlying low-rank\nmatrix. Underlying our analysis are insights for the analysis of\noverparameterized nonconvex optimization schemes that may have implications for\ncomputational problems beyond low-rank reconstruction.",
          "link": "http://arxiv.org/abs/2106.15013",
          "publishedOn": "2021-06-30T02:01:02.030Z",
          "wordCount": 743,
          "title": "Small random initialization is akin to spectral learning: Optimization and generalization guarantees for overparameterized low-rank matrix reconstruction. (arXiv:2106.15013v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15058",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Z/0/1/0/all/0/1\">Zihao Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1\">Chilin Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1\">Yinpeng Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1\">Wei Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaolu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jun Zhu</a>",
          "description": "Face recognition is greatly improved by deep convolutional neural networks\n(CNNs). Recently, these face recognition models have been used for identity\nauthentication in security sensitive applications. However, deep CNNs are\nvulnerable to adversarial patches, which are physically realizable and\nstealthy, raising new security concerns on the real-world applications of these\nmodels. In this paper, we evaluate the robustness of face recognition models\nusing adversarial patches based on transferability, where the attacker has\nlimited accessibility to the target models. First, we extend the existing\ntransfer-based attack techniques to generate transferable adversarial patches.\nHowever, we observe that the transferability is sensitive to initialization and\ndegrades when the perturbation magnitude is large, indicating the overfitting\nto the substitute models. Second, we propose to regularize the adversarial\npatches on the low dimensional data manifold. The manifold is represented by\ngenerative models pre-trained on legitimate human face images. Using face-like\nfeatures as adversarial perturbations through optimization on the manifold, we\nshow that the gaps between the responses of substitute models and the target\nmodels dramatically decrease, exhibiting a better transferability. Extensive\ndigital world experiments are conducted to demonstrate the superiority of the\nproposed method in the black-box setting. We apply the proposed method in the\nphysical world as well.",
          "link": "http://arxiv.org/abs/2106.15058",
          "publishedOn": "2021-06-30T02:01:01.998Z",
          "wordCount": 673,
          "title": "Improving Transferability of Adversarial Patches on Face Recognition with Generative Models. (arXiv:2106.15058v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15108",
          "author": "<a href=\"http://arxiv.org/find/cond-mat/1/au:+Bae_Y/0/1/0/all/0/1\">Youngkyoung Bae</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Kim_D/0/1/0/all/0/1\">Dong-Kyum Kim</a>, <a href=\"http://arxiv.org/find/cond-mat/1/au:+Jeong_H/0/1/0/all/0/1\">Hawoong Jeong</a>",
          "description": "Quantifying entropy production (EP) is essential to understand stochastic\nsystems at mesoscopic scales, such as living organisms or biological\nassemblies. However, without tracking the relevant variables, it is challenging\nto figure out where and to what extent EP occurs from recorded time-series\nimage data from experiments. Here, applying a convolutional neural network\n(CNN), a powerful tool for image processing, we develop an estimation method\nfor EP through an unsupervised learning algorithm that calculates only from\nmovies. Together with an attention map of the CNN's last layer, our method can\nnot only quantify stochastic EP but also produce the spatiotemporal pattern of\nthe EP (dissipation map). We show that our method accurately measures the EP\nand creates a dissipation map in two nonequilibrium systems, the bead-spring\nmodel and a network of elastic filaments. We further confirm high performance\neven with noisy, low spatial resolution data, and partially observed\nsituations. Our method will provide a practical way to obtain dissipation maps\nand ultimately contribute to uncovering the nonequilibrium nature of complex\nsystems.",
          "link": "http://arxiv.org/abs/2106.15108",
          "publishedOn": "2021-06-30T02:01:01.974Z",
          "wordCount": 626,
          "title": "Attaining entropy production and dissipation maps from Brownian movies via neural networks. (arXiv:2106.15108v1 [cond-mat.stat-mech])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14984",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+McDonald_A/0/1/0/all/0/1\">Andrew McDonald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_L/0/1/0/all/0/1\">Lai Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_V/0/1/0/all/0/1\">Vaibhav Srivastava</a>",
          "description": "Heterogeneous multi-robot sensing systems are able to characterize physical\nprocesses more comprehensively than homogeneous systems. Access to multiple\nmodalities of sensory data allow such systems to fuse information between\ncomplementary sources and learn richer representations of a phenomenon of\ninterest. Often, these data are correlated but vary in fidelity, i.e., accuracy\n(bias) and precision (noise). Low-fidelity data may be more plentiful, while\nhigh-fidelity data may be more trustworthy. In this paper, we address the\nproblem of multi-robot online estimation and coverage control by combining low-\nand high-fidelity data to learn and cover a sensory function of interest. We\npropose two algorithms for this task of heterogeneous learning and coverage --\nnamely Stochastic Sequencing of Multi-fidelity Learning and Coverage (SMLC) and\nDeterministic Sequencing of Multi-fidelity Learning and Coverage (DMLC) -- and\nprove that they converge asymptotically. In addition, we demonstrate the\nempirical efficacy of SMLC and DMLC through numerical simulations.",
          "link": "http://arxiv.org/abs/2106.14984",
          "publishedOn": "2021-06-30T02:01:01.969Z",
          "wordCount": 603,
          "title": "Online Estimation and Coverage Control with Heterogeneous Sensing Information. (arXiv:2106.14984v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14979",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hron_J/0/1/0/all/0/1\">Jiri Hron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krauth_K/0/1/0/all/0/1\">Karl Krauth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jordan_M/0/1/0/all/0/1\">Michael I. Jordan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kilbertus_N/0/1/0/all/0/1\">Niki Kilbertus</a>",
          "description": "Thanks to their scalability, two-stage recommenders are used by many of\ntoday's largest online platforms, including YouTube, LinkedIn, and Pinterest.\nThese systems produce recommendations in two steps: (i) multiple nominators --\ntuned for low prediction latency -- preselect a small subset of candidates from\nthe whole item pool; (ii)~a slower but more accurate ranker further narrows\ndown the nominated items, and serves to the user. Despite their popularity, the\nliterature on two-stage recommenders is relatively scarce, and the algorithms\nare often treated as the sum of their parts. Such treatment presupposes that\nthe two-stage performance is explained by the behavior of individual components\nif they were deployed independently. This is not the case: using synthetic and\nreal-world data, we demonstrate that interactions between the ranker and the\nnominators substantially affect the overall performance. Motivated by these\nfindings, we derive a generalization lower bound which shows that careful\nchoice of each nominator's training set is sometimes the only difference\nbetween a poor and an optimal two-stage recommender. Since searching for a good\nchoice manually is difficult, we learn one instead. In particular, using a\nMixture-of-Experts approach, we train the nominators (experts) to specialize on\ndifferent subsets of the item pool. This significantly improves performance.",
          "link": "http://arxiv.org/abs/2106.14979",
          "publishedOn": "2021-06-30T02:01:01.947Z",
          "wordCount": 641,
          "title": "On component interactions in two-stage recommender systems. (arXiv:2106.14979v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14997",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Siegel_J/0/1/0/all/0/1\">Jonathan W. Siegel</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Xu_J/0/1/0/all/0/1\">Jinchao Xu</a>",
          "description": "We consider the approximation rates of shallow neural networks with respect\nto the variation norm. Upper bounds on these rates have been established for\nsigmoidal and ReLU activation functions, but it has remained an important open\nproblem whether these rates are sharp. In this article, we provide a solution\nto this problem by proving sharp lower bounds on the approximation rates for\nshallow neural networks, which are obtained by lower bounding the $L^2$-metric\nentropy of the convex hull of the neural network basis functions. In addition,\nour methods also give sharp lower bounds on the Kolmogorov $n$-widths of this\nconvex hull, which show that the variation spaces corresponding to shallow\nneural networks cannot be efficiently approximated by linear methods. These\nlower bounds apply to both sigmoidal activation functions with bounded\nvariation and to activation functions which are a power of the ReLU. Our\nresults also quantify how much stronger the Barron spectral norm is than the\nvariation norm and, combined with previous results, give the asymptotics of the\n$L^\\infty$-metric entropy up to logarithmic factors in the case of the ReLU\nactivation function.",
          "link": "http://arxiv.org/abs/2106.14997",
          "publishedOn": "2021-06-30T02:01:01.941Z",
          "wordCount": 639,
          "title": "Sharp Lower Bounds on the Approximation Rate of Shallow Neural Networks. (arXiv:2106.14997v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15577",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wever_F/0/1/0/all/0/1\">Fiorella Wever</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keller_T/0/1/0/all/0/1\">T. Anderson Keller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_V/0/1/0/all/0/1\">Victor Garcia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Symul_L/0/1/0/all/0/1\">Laura Symul</a>",
          "description": "High levels of sparsity and strong class imbalance are ubiquitous challenges\nthat are often presented simultaneously in real-world time series data. While\nmost methods tackle each problem separately, our proposed approach handles both\nin conjunction, while imposing fewer assumptions on the data. In this work, we\npropose leveraging a self-supervised learning method, specifically\nAutoregressive Predictive Coding (APC), to learn relevant hidden\nrepresentations of time series data in the context of both missing data and\nclass imbalance. We apply APC using either a GRU or GRU-D encoder on two\nreal-world datasets, and show that applying one-step-ahead prediction with APC\nimproves the classification results in all settings. In fact, by applying GRU-D\n- APC, we achieve state-of-the-art AUPRC results on the Physionet benchmark.",
          "link": "http://arxiv.org/abs/2106.15577",
          "publishedOn": "2021-06-30T02:01:01.929Z",
          "wordCount": 597,
          "title": "As easy as APC: Leveraging self-supervised learning in the context of time series classification with varying levels of sparsity and severe class imbalance. (arXiv:2106.15577v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15283",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chenglin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tong_C/0/1/0/all/0/1\">Carrie Lu Tong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niu_D/0/1/0/all/0/1\">Di Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_B/0/1/0/all/0/1\">Bei Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuo_X/0/1/0/all/0/1\">Xiao Zuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1\">Lei Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_J/0/1/0/all/0/1\">Jian Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Jianming Yang</a>",
          "description": "Deep learning models for human activity recognition (HAR) based on sensor\ndata have been heavily studied recently. However, the generalization ability of\ndeep models on complex real-world HAR data is limited by the availability of\nhigh-quality labeled activity data, which are hard to obtain. In this paper, we\ndesign a similarity embedding neural network that maps input sensor signals\nonto real vectors through carefully designed convolutional and LSTM layers. The\nembedding network is trained with a pairwise similarity loss, encouraging the\nclustering of samples from the same class in the embedded real space, and can\nbe effectively trained on a small dataset and even on a noisy dataset with\nmislabeled samples. Based on the learned embeddings, we further propose both\nnonparametric and parametric approaches for activity recognition. Extensive\nevaluation based on two public datasets has shown that the proposed similarity\nembedding network significantly outperforms state-of-the-art deep models on HAR\nclassification tasks, is robust to mislabeled samples in the training set, and\ncan also be used to effectively denoise a noisy dataset.",
          "link": "http://arxiv.org/abs/2106.15283",
          "publishedOn": "2021-06-30T02:01:01.847Z",
          "wordCount": 626,
          "title": "Similarity Embedding Networks for Robust Human Activity Recognition. (arXiv:2106.15283v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15350",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dogaru_R/0/1/0/all/0/1\">Radu Dogaru</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dogaru_I/0/1/0/all/0/1\">Ioana Dogaru</a>",
          "description": "Light binary convolutional neural networks (LB-CNN) are particularly useful\nwhen implemented in low-energy computing platforms as required in many\nindustrial applications. Herein, a framework for optimizing compact LB-CNN is\nintroduced and its effectiveness is evaluated. The framework is freely\navailable and may run on free-access cloud platforms, thus requiring no major\ninvestments. The optimized model is saved in the standardized .h5 format and\ncan be used as input to specialized tools for further deployment into specific\ntechnologies, thus enabling the rapid development of various intelligent image\nsensors. The main ingredient in accelerating the optimization of our model,\nparticularly the selection of binary convolution kernels, is the Chainer/Cupy\nmachine learning library offering significant speed-ups for training the output\nlayer as an extreme-learning machine. Additional training of the output layer\nusing Keras/Tensorflow is included, as it allows an increase in accuracy.\nResults for widely used datasets including MNIST, GTSRB, ORL, VGG show very\ngood compromise between accuracy and complexity. Particularly, for face\nrecognition problems a carefully optimized LB-CNN model provides up to 100%\naccuracies. Such TinyML solutions are well suited for industrial applications\nrequiring image recognition with low energy consumption.",
          "link": "http://arxiv.org/abs/2106.15350",
          "publishedOn": "2021-06-30T02:01:01.731Z",
          "wordCount": 657,
          "title": "LB-CNN: An Open Source Framework for Fast Training of Light Binary Convolutional Neural Networks using Chainer and Cupy. (arXiv:2106.15350v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15274",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aliyev_N/0/1/0/all/0/1\">Namig Aliyev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sezer_O/0/1/0/all/0/1\">Oguzhan Sezer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guzel_M/0/1/0/all/0/1\">Mehmet Turan Guzel</a>",
          "description": "Autonomous systems require identifying the environment and it has a long way\nto go before putting it safely into practice. In autonomous driving systems,\nthe detection of obstacles and traffic lights are of importance as well as lane\ntracking. In this study, an autonomous driving system is developed and tested\nin the experimental environment designed for this purpose. In this system, a\nmodel vehicle having a camera is used to trace the lanes and avoid obstacles to\nexperimentally study autonomous driving behavior. Convolutional Neural Network\nmodels were trained for Lane tracking. For the vehicle to avoid obstacles,\ncorner detection, optical flow, focus of expansion, time to collision, balance\ncalculation, and decision mechanism were created, respectively.",
          "link": "http://arxiv.org/abs/2106.15274",
          "publishedOn": "2021-06-30T02:01:01.706Z",
          "wordCount": 573,
          "title": "Autonomous Driving Implementation in an Experimental Environment. (arXiv:2106.15274v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15347",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaoqi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yen_K/0/1/0/all/0/1\">Kevin Yen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1\">Yifan Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1\">Han-Wei Shen</a>",
          "description": "In the past decades, many graph drawing techniques have been proposed for\ngenerating aesthetically pleasing graph layouts. However, it remains a\nchallenging task since different layout methods tend to highlight different\ncharacteristics of the graphs. Recently, studies on deep learning based graph\ndrawing algorithm have emerged but they are often not generalizable to\narbitrary graphs without re-training. In this paper, we propose a Convolutional\nGraph Neural Network based deep learning framework, DeepGD, which can draw\narbitrary graphs once trained. It attempts to generate layouts by compromising\namong multiple pre-specified aesthetics considering a good graph layout usually\ncomplies with multiple aesthetics simultaneously. In order to balance the\ntrade-off, we propose two adaptive training strategies which adjust the weight\nfactor of each aesthetic dynamically during training. The quantitative and\nqualitative assessment of DeepGD demonstrates that it is capable of drawing\narbitrary graphs effectively, while being flexible at accommodating different\naesthetic criteria.",
          "link": "http://arxiv.org/abs/2106.15347",
          "publishedOn": "2021-06-30T02:01:01.700Z",
          "wordCount": 582,
          "title": "DeepGD: A Deep Learning Framework for Graph Drawing Using GNN. (arXiv:2106.15347v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15282",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ho_J/0/1/0/all/0/1\">Jonathan Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saharia_C/0/1/0/all/0/1\">Chitwan Saharia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_W/0/1/0/all/0/1\">William Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fleet_D/0/1/0/all/0/1\">David J. Fleet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Norouzi_M/0/1/0/all/0/1\">Mohammad Norouzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salimans_T/0/1/0/all/0/1\">Tim Salimans</a>",
          "description": "We show that cascaded diffusion models are capable of generating high\nfidelity images on the class-conditional ImageNet generation challenge, without\nany assistance from auxiliary image classifiers to boost sample quality. A\ncascaded diffusion model comprises a pipeline of multiple diffusion models that\ngenerate images of increasing resolution, beginning with a standard diffusion\nmodel at the lowest resolution, followed by one or more super-resolution\ndiffusion models that successively upsample the image and add higher resolution\ndetails. We find that the sample quality of a cascading pipeline relies\ncrucially on conditioning augmentation, our proposed method of data\naugmentation of the lower resolution conditioning inputs to the\nsuper-resolution models. Our experiments show that conditioning augmentation\nprevents compounding error during sampling in a cascaded model, helping us to\ntrain cascading pipelines achieving FID scores of 1.48 at 64x64, 3.52 at\n128x128 and 4.88 at 256x256 resolutions, outperforming BigGAN-deep.",
          "link": "http://arxiv.org/abs/2106.15282",
          "publishedOn": "2021-06-30T02:01:01.686Z",
          "wordCount": 593,
          "title": "Cascaded Diffusion Models for High Fidelity Image Generation. (arXiv:2106.15282v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15281",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rosso_M/0/1/0/all/0/1\">Maria Pia Del Rosso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sebastianelli_A/0/1/0/all/0/1\">Alessandro Sebastianelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spiller_D/0/1/0/all/0/1\">Dario Spiller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathieu_P/0/1/0/all/0/1\">Pierre Philippe Mathieu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ullo_S/0/1/0/all/0/1\">Silvia Liberata Ullo</a>",
          "description": "In recent years, the growth of Machine Learning algorithms in a variety of\ndifferent applications has raised numerous studies on the applicability of\nthese algorithms in real scenarios. Among all, one of the hardest scenarios,\ndue to its physical requirements, is the aerospace one. In this context, the\nauthors of this work aim to propose a first prototype and a study of\nfeasibility for an AI model to be 'loaded' on board. As a case study, the\nauthors decided to investigate the detection of volcanic eruptions as a method\nto swiftly produce alerts. Two Convolutional Neural Networks have been proposed\nand created, also showing how to correctly implement them on real hardware and\nhow the complexity of a CNN can be adapted to fit computational requirements.",
          "link": "http://arxiv.org/abs/2106.15281",
          "publishedOn": "2021-06-30T02:01:01.671Z",
          "wordCount": 597,
          "title": "On-board Volcanic Eruption Detection through CNNs and Satellite Multispectral Imagery. (arXiv:2106.15281v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15257",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kashi_M/0/1/0/all/0/1\">Mohammad Amin Kashi</a>",
          "description": "Depth perception is fundamental for robots to understand the surrounding\nenvironment. As the view of cognitive neuroscience, visual depth perception\nmethods are divided into three categories, namely binocular, active, and\npictorial. The first two categories have been studied for decades in detail.\nHowever, research for the exploration of the third category is still in its\ninfancy and has got momentum by the advent of deep learning methods in recent\nyears. In cognitive neuroscience, it is known that pictorial depth perception\nmechanisms are dependent on the perception of seen objects. Inspired by this\nfact, in this thesis, we investigated the relation of perception of objects and\ndepth estimation convolutional neural networks. For this purpose, we developed\nnew network structures based on a simple depth estimation network that only\nused a single image at its input. Our proposed structures use both an image and\na semantic label of the image as their input. We used semantic labels as the\noutput of object perception. The obtained results of performance comparison\nbetween the developed network and original network showed that our novel\nstructures can improve the performance of depth estimation by 52\\% of relative\nerror of distance in the examined cases. Most of the experimental studies were\ncarried out on synthetic datasets that were generated by game engines to\nisolate the performance comparison from the effect of inaccurate depth and\nsemantic labels of non-synthetic datasets. It is shown that particular\nsynthetic datasets may be used for training of depth networks in cases that an\nappropriate dataset is not available. Furthermore, we showed that in these\ncases, usage of semantic labels improves the robustness of the network against\ndomain shift from synthetic training data to non-synthetic test data.",
          "link": "http://arxiv.org/abs/2106.15257",
          "publishedOn": "2021-06-30T02:01:01.665Z",
          "wordCount": 748,
          "title": "Predicting Depth from Semantic Segmentation using Game Engine Dataset. (arXiv:2106.15257v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15262",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Pasandi_H/0/1/0/all/0/1\">Hannaneh Barahouei Pasandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nadeem_T/0/1/0/all/0/1\">Tamer Nadeem</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amirpour_H/0/1/0/all/0/1\">Hadi Amirpour</a>",
          "description": "Over the last decade, the bandwidth expansion and MU-MIMO spectral efficiency\nhave promised to increase data throughput by allowing concurrent communication\nbetween one Access Point and multiple users. However, we are still a long way\nfrom enjoying such MU-MIMO MAC protocol improvements for bandwidth hungry\napplications such as video streaming in practical WiFi network settings due to\nheterogeneous channel conditions and devices, unreliable transmissions, and\nlack of useful feedback exchange among the lower and upper layers'\nrequirements. This paper introduces MuViS, a novel dual-phase optimization\nframework that proposes a Quality of Experience (QoE) aware MU-MIMO\noptimization for multi-user video streaming over IEEE 802.11ac. MuViS first\nemploys reinforcement learning to optimize the MU-MIMO user group and mode\nselection for users based on their PHY/MAC layer characteristics. The video\nbitrate is then optimized based on the user's mode (Multi-User (MU) or\nSingle-User (SU)). We present our design and its evaluation on smartphones and\nlaptops using 802.11ac WiFi. Our experimental results in various indoor\nenvironments and configurations show a scalable framework that can support a\nlarge number of users with streaming at high video rates and satisfying QoE\nrequirements.",
          "link": "http://arxiv.org/abs/2106.15262",
          "publishedOn": "2021-06-30T02:01:01.641Z",
          "wordCount": 632,
          "title": "MuViS: Online MU-MIMO Grouping for Multi-User Applications Over Commodity WiFi. (arXiv:2106.15262v1 [cs.NI])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15147",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bahri_D/0/1/0/all/0/1\">Dara Bahri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Heinrich Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1\">Yi Tay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metzler_D/0/1/0/all/0/1\">Donald Metzler</a>",
          "description": "Self-supervised contrastive representation learning has proved incredibly\nsuccessful in the vision and natural language domains, enabling\nstate-of-the-art performance with orders of magnitude less labeled data.\nHowever, such methods are domain-specific and little has been done to leverage\nthis technique on real-world tabular datasets. We propose SCARF, a simple,\nwidely-applicable technique for contrastive learning, where views are formed by\ncorrupting a random subset of features. When applied to pre-train deep neural\nnetworks on the 69 real-world, tabular classification datasets from the\nOpenML-CC18 benchmark, SCARF not only improves classification accuracy in the\nfully-supervised setting but does so also in the presence of label noise and in\nthe semi-supervised setting where only a fraction of the available training\ndata is labeled. We show that SCARF complements existing strategies and\noutperforms alternatives like autoencoders. We conduct comprehensive ablations,\ndetailing the importance of a range of factors.",
          "link": "http://arxiv.org/abs/2106.15147",
          "publishedOn": "2021-06-30T02:01:01.606Z",
          "wordCount": 575,
          "title": "SCARF: Self-Supervised Contrastive Learning using Random Feature Corruption. (arXiv:2106.15147v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15093",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mahadevan_A/0/1/0/all/0/1\">Ananth Mahadevan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathioudakis_M/0/1/0/all/0/1\">Michael Mathioudakis</a>",
          "description": "Machine unlearning is the task of updating machine learning (ML) models after\na subset of the training data they were trained on is deleted. Methods for the\ntask are desired to combine effectiveness and efficiency, i.e., they should\neffectively \"unlearn\" deleted data, but in a way that does not require\nexcessive computation effort (e.g., a full retraining) for a small amount of\ndeletions. Such a combination is typically achieved by tolerating some amount\nof approximation in the unlearning. In addition, laws and regulations in the\nspirit of \"the right to be forgotten\" have given rise to requirements for\ncertifiability, i.e., the ability to demonstrate that the deleted data has\nindeed been unlearned by the ML model.\n\nIn this paper, we present an experimental study of the three state-of-the-art\napproximate unlearning methods for linear models and demonstrate the trade-offs\nbetween efficiency, effectiveness and certifiability offered by each method. In\nimplementing the study, we extend some of the existing works and describe a\ncommon ML pipeline to compare and evaluate the unlearning methods on six\nreal-world datasets and a variety of settings. We provide insights into the\neffect of the quantity and distribution of the deleted data on ML models and\nthe performance of each unlearning method in different settings. We also\npropose a practical online strategy to determine when the accumulated error\nfrom approximate unlearning is large enough to warrant a full retrain of the ML\nmodel.",
          "link": "http://arxiv.org/abs/2106.15093",
          "publishedOn": "2021-06-30T02:01:01.591Z",
          "wordCount": 661,
          "title": "Certifiable Machine Unlearning for Linear Models. (arXiv:2106.15093v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14952",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Braverman_V/0/1/0/all/0/1\">Vladimir Braverman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassidim_A/0/1/0/all/0/1\">Avinatan Hassidim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matias_Y/0/1/0/all/0/1\">Yossi Matias</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schain_M/0/1/0/all/0/1\">Mariano Schain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Silwal_S/0/1/0/all/0/1\">Sandeep Silwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Samson Zhou</a>",
          "description": "In this paper, we introduce adversarially robust streaming algorithms for\ncentral machine learning and algorithmic tasks, such as regression and\nclustering, as well as their more general counterparts, subspace embedding,\nlow-rank approximation, and coreset construction. For regression and other\nnumerical linear algebra related tasks, we consider the row arrival streaming\nmodel. Our results are based on a simple, but powerful, observation that many\nimportance sampling-based algorithms give rise to adversarial robustness which\nis in contrast to sketching based algorithms, which are very prevalent in the\nstreaming literature but suffer from adversarial attacks. In addition, we show\nthat the well-known merge and reduce paradigm in streaming is adversarially\nrobust. Since the merge and reduce paradigm allows coreset constructions in the\nstreaming setting, we thus obtain robust algorithms for $k$-means, $k$-median,\n$k$-center, Bregman clustering, projective clustering, principal component\nanalysis (PCA) and non-negative matrix factorization. To the best of our\nknowledge, these are the first adversarially robust results for these problems\nyet require no new algorithmic implementations. Finally, we empirically confirm\nthe robustness of our algorithms on various adversarial attacks and demonstrate\nthat by contrast, some common existing algorithms are not robust.\n\n(Abstract shortened to meet arXiv limits)",
          "link": "http://arxiv.org/abs/2106.14952",
          "publishedOn": "2021-06-30T02:01:01.573Z",
          "wordCount": 634,
          "title": "Adversarial Robustness of Streaming Algorithms through Importance Sampling. (arXiv:2106.14952v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15098",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kong_X/0/1/0/all/0/1\">Xiangzhe Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1\">Zhixing Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>",
          "description": "Molecular graph generation is a fundamental but challenging task in various\napplications such as drug discovery and material science, which requires\ngenerating valid molecules with desired properties. Auto-regressive models,\nwhich usually construct graphs following sequential actions of adding nodes and\nedges at the atom-level, have made rapid progress in recent years. However,\nthese atom-level models ignore high-frequency subgraphs that not only capture\nthe regularities of atomic combination in molecules but also are often related\nto desired chemical properties. In this paper, we propose a method to\nautomatically discover such common substructures, which we call {\\em graph\npieces}, from given molecular graphs. Based on graph pieces, we leverage a\nvariational autoencoder to generate molecules in two phases: piece-level graph\ngeneration followed by bond completion. Experiments show that our graph piece\nvariational autoencoder achieves better performance over state-of-the-art\nbaselines on property optimization and constrained property optimization tasks\nwith higher computational efficiency.",
          "link": "http://arxiv.org/abs/2106.15098",
          "publishedOn": "2021-06-30T02:01:01.567Z",
          "wordCount": 589,
          "title": "GraphPiece: Efficiently Generating High-Quality Molecular Graph with Substructures. (arXiv:2106.15098v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15127",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Blanco_Mulero_D/0/1/0/all/0/1\">David Blanco-Mulero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heinonen_M/0/1/0/all/0/1\">Markus Heinonen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kyrki_V/0/1/0/all/0/1\">Ville Kyrki</a>",
          "description": "Graph Gaussian Processes (GGPs) provide a data-efficient solution on graph\nstructured domains. Existing approaches have focused on static structures,\nwhereas many real graph data represent a dynamic structure, limiting the\napplications of GGPs. To overcome this we propose evolving-Graph Gaussian\nProcesses (e-GGPs). The proposed method is capable of learning the transition\nfunction of graph vertices over time with a neighbourhood kernel to model the\nconnectivity and interaction changes between vertices. We assess the\nperformance of our method on time-series regression problems where graphs\nevolve over time. We demonstrate the benefits of e-GGPs over static graph\nGaussian Process approaches.",
          "link": "http://arxiv.org/abs/2106.15127",
          "publishedOn": "2021-06-30T02:01:01.552Z",
          "wordCount": 533,
          "title": "Evolving-Graph Gaussian Processes. (arXiv:2106.15127v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14947",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Fabian_Z/0/1/0/all/0/1\">Zalan Fabian</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Heckel_R/0/1/0/all/0/1\">Reinhard Heckel</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Soltanolkotabi_M/0/1/0/all/0/1\">Mahdi Soltanolkotabi</a>",
          "description": "Deep neural networks have emerged as very successful tools for image\nrestoration and reconstruction tasks. These networks are often trained\nend-to-end to directly reconstruct an image from a noisy or corrupted\nmeasurement of that image. To achieve state-of-the-art performance, training on\nlarge and diverse sets of images is considered critical. However, it is often\ndifficult and/or expensive to collect large amounts of training images.\nInspired by the success of Data Augmentation (DA) for classification problems,\nin this paper, we propose a pipeline for data augmentation for accelerated MRI\nreconstruction and study its effectiveness at reducing the required training\ndata in a variety of settings. Our DA pipeline, MRAugment, is specifically\ndesigned to utilize the invariances present in medical imaging measurements as\nnaive DA strategies that neglect the physics of the problem fail. Through\nextensive studies on multiple datasets we demonstrate that in the low-data\nregime DA prevents overfitting and can match or even surpass the state of the\nart while using significantly fewer training data, whereas in the high-data\nregime it has diminishing returns. Furthermore, our findings show that DA can\nimprove the robustness of the model against various shifts in the test\ndistribution.",
          "link": "http://arxiv.org/abs/2106.14947",
          "publishedOn": "2021-06-30T02:01:01.540Z",
          "wordCount": 663,
          "title": "Data augmentation for deep learning based accelerated MRI reconstruction with limited data. (arXiv:2106.14947v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15128",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yichi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1\">Shihong Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Huishuai Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jun Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tie-Yan Liu</a>",
          "description": "Balancing exploration and exploitation (EE) is a fundamental problem in\ncontex-tual bandit. One powerful principle for EE trade-off isOptimism in Face\nof Uncer-tainty(OFU), in which the agent takes the action according to an upper\nconfidencebound (UCB) of reward. OFU has achieved (near-)optimal regret bound\nfor lin-ear/kernel contextual bandits. However, it is in general unknown how to\nderiveefficient and effective EE trade-off methods for non-linearcomplex tasks,\nsuchas contextual bandit with deep neural network as the reward function. In\nthispaper, we propose a novel OFU algorithm namedregularized OFU(ROFU). InROFU,\nwe measure the uncertainty of the reward by a differentiable function\nandcompute the upper confidence bound by solving a regularized optimization\nprob-lem. We prove that, for multi-armed bandit, kernel contextual bandit and\nneuraltangent kernel bandit, ROFU achieves (near-)optimal regret bounds with\ncertainuncertainty measure, which theoretically justifies its effectiveness on\nEE trade-off.Importantly, ROFU admits a very efficient implementation with\ngradient-basedoptimizer, which easily extends to general deep neural network\nmodels beyondneural tangent kernel, in sharp contrast with previous OFU\nmethods. The em-pirical evaluation demonstrates that ROFU works extremelywell\nfor contextualbandits under various settings.",
          "link": "http://arxiv.org/abs/2106.15128",
          "publishedOn": "2021-06-30T02:01:01.532Z",
          "wordCount": 619,
          "title": "Regularized OFU: an Efficient UCB Estimator forNon-linear Contextual Bandit. (arXiv:2106.15128v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15133",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Iwata_T/0/1/0/all/0/1\">Tomoharu Iwata</a>",
          "description": "We propose a method that meta-learns a knowledge on matrix factorization from\nvarious matrices, and uses the knowledge for factorizing unseen matrices. The\nproposed method uses a neural network that takes a matrix as input, and\ngenerates prior distributions of factorized matrices of the given matrix. The\nneural network is meta-learned such that the expected imputation error is\nminimized when the factorized matrices are adapted to each matrix by a maximum\na posteriori (MAP) estimation. We use a gradient descent method for the MAP\nestimation, which enables us to backpropagate the expected imputation error\nthrough the gradient descent steps for updating neural network parameters since\neach gradient descent step is written in a closed form and is differentiable.\nThe proposed method can meta-learn from matrices even when their rows and\ncolumns are not shared, and their sizes are different from each other. In our\nexperiments with three user-item rating datasets, we demonstrate that our\nproposed method can impute the missing values from a limited number of\nobservations in unseen matrices after being trained with different matrices.",
          "link": "http://arxiv.org/abs/2106.15133",
          "publishedOn": "2021-06-30T02:01:01.527Z",
          "wordCount": 610,
          "title": "Meta-learning for Matrix Factorization without Shared Rows or Columns. (arXiv:2106.15133v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15146",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jingzheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Hailong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiyi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhijun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tao_R/0/1/0/all/0/1\">Renshuai Tao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1\">Yufei Ge</a>",
          "description": "Learning from multiple annotators aims to induce a high-quality classifier\nfrom training instances, where each of them is associated with a set of\npossibly noisy labels provided by multiple annotators under the influence of\ntheir varying abilities and own biases. In modeling the probability transition\nprocess from latent true labels to observed labels, most existing methods adopt\nclass-level confusion matrices of annotators that observed labels do not depend\non the instance features, just determined by the true labels. It may limit the\nperformance that the classifier can achieve. In this work, we propose the noise\ntransition matrix, which incorporates the influence of instance features on\nannotators' performance based on confusion matrices. Furthermore, we propose a\nsimple yet effective learning framework, which consists of a classifier module\nand a noise transition matrix module in a unified neural network architecture.\nExperimental results demonstrate the superiority of our method in comparison\nwith state-of-the-art methods.",
          "link": "http://arxiv.org/abs/2106.15146",
          "publishedOn": "2021-06-30T02:01:01.505Z",
          "wordCount": 584,
          "title": "Learning from Multiple Annotators by Incorporating Instance Features. (arXiv:2106.15146v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.15158",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Aoudia_F/0/1/0/all/0/1\">Fay&#xe7;al Ait Aoudia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoydis_J/0/1/0/all/0/1\">Jakob Hoydis</a>",
          "description": "As communication systems are foreseen to enable new services such as joint\ncommunication and sensing and utilize parts of the sub-THz spectrum, the design\nof novel waveforms that can support these emerging applications becomes\nincreasingly challenging. We present in this work an end-to-end learning\napproach to design waveforms through joint learning of pulse shaping and\nconstellation geometry, together with a neural network (NN)-based receiver.\nOptimization is performed to maximize an achievable information rate, while\nsatisfying constraints on out-of-band emission and power envelope. Our results\nshow that the proposed approach enables up to orders of magnitude smaller\nadjacent channel leakage ratios (ACLRs) with peak-to-average power ratios\n(PAPRs) competitive with traditional filters, without significant loss of\ninformation rate on an additive white Gaussian noise (AWGN) channel, and no\nadditional complexity at the transmitter.",
          "link": "http://arxiv.org/abs/2106.15158",
          "publishedOn": "2021-06-30T02:01:01.493Z",
          "wordCount": 576,
          "title": "End-to-end Waveform Learning Through Joint Optimization of Pulse and Constellation Shaping. (arXiv:2106.15158v1 [cs.IT])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14941",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kamalov_F/0/1/0/all/0/1\">Firuz Kamalov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moussa_S/0/1/0/all/0/1\">Sherif Moussa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zgheib_R/0/1/0/all/0/1\">Rita Zgheib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mashaal_O/0/1/0/all/0/1\">Omar Mashaal</a>",
          "description": "In this paper, we analyze existing feature selection methods to identify the\nkey elements of network traffic data that allow intrusion detection. In\naddition, we propose a new feature selection method that addresses the\nchallenge of considering continuous input features and discrete target values.\nWe show that the proposed method performs well against the benchmark selection\nmethods. We use our findings to develop a highly effective machine\nlearning-based detection systems that achieves 99.9% accuracy in distinguishing\nbetween DDoS and benign signals. We believe that our results can be useful to\nexperts who are interested in designing and building automated intrusion\ndetection systems.",
          "link": "http://arxiv.org/abs/2106.14941",
          "publishedOn": "2021-06-30T02:01:01.439Z",
          "wordCount": 552,
          "title": "Feature selection for intrusion detection systems. (arXiv:2106.14941v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2102.10456",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Farsang_M/0/1/0/all/0/1\">M&#xf3;nika Farsang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szegletes_L/0/1/0/all/0/1\">Luca Szegletes</a>",
          "description": "Proximal Policy Optimization (PPO) is among the most widely used algorithms\nin reinforcement learning, which achieves state-of-the-art performance in many\nchallenging problems. The keys to its success are the reliable policy updates\nthrough the clipping mechanism and the multiple epochs of minibatch updates.\nThe aim of this research is to give new simple but effective alternatives to\nthe former. For this, we propose linearly and exponentially decaying clipping\nrange approaches throughout the training. With these, we would like to provide\nhigher exploration at the beginning and stronger restrictions at the end of the\nlearning phase. We investigate their performance in several classical control\nand locomotive robotic environments. During the analysis, we found that they\ninfluence the achieved rewards and are effective alternatives to the constant\nclipping method in many reinforcement learning tasks.",
          "link": "http://arxiv.org/abs/2102.10456",
          "publishedOn": "2021-06-29T01:55:20.694Z",
          "wordCount": 600,
          "title": "Decaying Clipping Range in Proximal Policy Optimization. (arXiv:2102.10456v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.10447",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Farsang_M/0/1/0/all/0/1\">M&#xf3;nika Farsang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szegletes_L/0/1/0/all/0/1\">Luca Szegletes</a>",
          "description": "An in-depth understanding of the particular environment is crucial in\nreinforcement learning (RL). To address this challenge, the decision-making\nprocess of a mobile collaborative robotic assistant modeled by the Markov\ndecision process (MDP) framework is studied in this paper. The optimal\nstate-action combinations of the MDP are calculated with the non-linear Bellman\noptimality equations. This system of equations can be solved with relative ease\nby the computational power of Wolfram Mathematica, where the obtained optimal\naction-values point to the optimal policy. Unlike other RL algorithms, this\nmethodology does not approximate the optimal behavior, it gives the exact,\nexplicit solution, which provides a strong foundation for our study. With this,\nwe offer new insights into understanding the action selection mechanisms in RL\nby presenting various small modifications on the very same schema that lead to\ndifferent optimal policies.",
          "link": "http://arxiv.org/abs/2102.10447",
          "publishedOn": "2021-06-29T01:55:20.689Z",
          "wordCount": 615,
          "title": "Importance of Environment Design in Reinforcement Learning: A Study of a Robotic Environment. (arXiv:2102.10447v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.02322",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xue Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Derezinski_M/0/1/0/all/0/1\">Micha&#x142; Derezi&#x144;ski</a>",
          "description": "Consider a regression problem where the learner is given a large collection\nof $d$-dimensional data points, but can only query a small subset of the\nreal-valued labels. How many queries are needed to obtain a $1+\\epsilon$\nrelative error approximation of the optimum? While this problem has been\nextensively studied for least squares regression, little is known for other\nlosses. An important example is least absolute deviation regression ($\\ell_1$\nregression) which enjoys superior robustness to outliers compared to least\nsquares. We develop a new framework for analyzing importance sampling methods\nin regression problems, which enables us to show that the query complexity of\nleast absolute deviation regression is $\\Theta(d/\\epsilon^2)$ up to logarithmic\nfactors. We further extend our techniques to show the first bounds on the query\ncomplexity for any $\\ell_p$ loss with $p\\in(1,2)$. As a key novelty in our\nanalysis, we introduce the notion of robust uniform convergence, which is a new\napproximation guarantee for the empirical loss. While it is inspired by uniform\nconvergence in statistical learning, our approach additionally incorporates a\ncorrection term to avoid unnecessary variance due to outliers. This can be\nviewed as a new connection between statistical learning theory and variance\nreduction techniques in stochastic optimization, which should be of independent\ninterest.",
          "link": "http://arxiv.org/abs/2102.02322",
          "publishedOn": "2021-06-29T01:55:20.671Z",
          "wordCount": 672,
          "title": "Query Complexity of Least Absolute Deviation Regression via Robust Uniform Convergence. (arXiv:2102.02322v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.06930",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1\">Qingyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zixuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adeli_E/0/1/0/all/0/1\">Ehsan Adeli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pohl_K/0/1/0/all/0/1\">Kilian M. Pohl</a>",
          "description": "Machine learning analysis of longitudinal neuroimaging data is typically\nbased on supervised learning, which requires a large number of ground-truth\nlabels to be informative. As ground-truth labels are often missing or expensive\nto obtain in neuroscience, we avoid them in our analysis by combing factor\ndisentanglement with self-supervised learning to identify changes and\nconsistencies across the multiple MRIs acquired of each individual over time.\nSpecifically, we propose a new definition of disentanglement by formulating a\nmultivariate mapping between factors (e.g., brain age) associated with an MRI\nand a latent image representation. Then, factors that evolve across\nacquisitions of longitudinal sequences are disentangled from that mapping by\nself-supervised learning in such a way that changes in a single factor induce\nchange along one direction in the representation space. We implement this\nmodel, named Longitudinal Self-Supervised Learning (LSSL), via a standard\nautoencoding structure with a cosine loss to disentangle brain age from the\nimage representation. We apply LSSL to two longitudinal neuroimaging studies to\nhighlight its strength in extracting the brain-age information from MRI and\nrevealing informative characteristics associated with neurodegenerative and\nneuropsychological disorders. Moreover, the representations learned by LSSL\nfacilitate supervised classification by recording faster convergence and higher\n(or similar) prediction accuracy compared to several other representation\nlearning techniques.",
          "link": "http://arxiv.org/abs/2006.06930",
          "publishedOn": "2021-06-29T01:55:20.665Z",
          "wordCount": 670,
          "title": "Longitudinal Self-Supervised Learning. (arXiv:2006.06930v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.08483",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lobacheva_E/0/1/0/all/0/1\">Ekaterina Lobacheva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chirkova_N/0/1/0/all/0/1\">Nadezhda Chirkova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kodryan_M/0/1/0/all/0/1\">Maxim Kodryan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vetrov_D/0/1/0/all/0/1\">Dmitry Vetrov</a>",
          "description": "Ensembles of deep neural networks are known to achieve state-of-the-art\nperformance in uncertainty estimation and lead to accuracy improvement. In this\nwork, we focus on a classification problem and investigate the behavior of both\nnon-calibrated and calibrated negative log-likelihood (CNLL) of a deep ensemble\nas a function of the ensemble size and the member network size. We indicate the\nconditions under which CNLL follows a power law w.r.t. ensemble size or member\nnetwork size, and analyze the dynamics of the parameters of the discovered\npower laws. Our important practical finding is that one large network may\nperform worse than an ensemble of several medium-size networks with the same\ntotal number of parameters (we call this ensemble a memory split). Using the\ndetected power law-like dependencies, we can predict (1) the possible gain from\nthe ensembling of networks with given structure, (2) the optimal memory split\ngiven a memory budget, based on a relatively small number of trained networks.\n\nWe describe the memory split advantage effect in more details in\narXiv:2005.07292",
          "link": "http://arxiv.org/abs/2007.08483",
          "publishedOn": "2021-06-29T01:55:20.659Z",
          "wordCount": 648,
          "title": "On Power Laws in Deep Ensembles. (arXiv:2007.08483v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.08405",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Shlezinger_N/0/1/0/all/0/1\">Nir Shlezinger</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Whang_J/0/1/0/all/0/1\">Jay Whang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Eldar_Y/0/1/0/all/0/1\">Yonina C. Eldar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dimakis_A/0/1/0/all/0/1\">Alexandros G. Dimakis</a>",
          "description": "Signal processing, communications, and control have traditionally relied on\nclassical statistical modeling techniques. Such model-based methods utilize\nmathematical formulations that represent the underlying physics, prior\ninformation and additional domain knowledge. Simple classical models are useful\nbut sensitive to inaccuracies and may lead to poor performance when real\nsystems display complex or dynamic behavior. On the other hand, purely\ndata-driven approaches that are model-agnostic are becoming increasingly\npopular as datasets become abundant and the power of modern deep learning\npipelines increases. Deep neural networks (DNNs) use generic architectures\nwhich learn to operate from data, and demonstrate excellent performance,\nespecially for supervised problems. However, DNNs typically require massive\namounts of data and immense computational resources, limiting their\napplicability for some signal processing scenarios. We are interested in hybrid\ntechniques that combine principled mathematical models with data-driven systems\nto benefit from the advantages of both approaches. Such model-based deep\nlearning methods exploit both partial domain knowledge, via mathematical\nstructures designed for specific problems, as well as learning from limited\ndata. In this article we survey the leading approaches for studying and\ndesigning model-based deep learning systems. We divide hybrid\nmodel-based/data-driven systems into categories based on their inference\nmechanism. We provide a comprehensive review of the leading approaches for\ncombining model-based algorithms with deep learning in a systematic manner,\nalong with concrete guidelines and detailed signal processing oriented examples\nfrom recent literature. Our aim is to facilitate the design and study of future\nsystems on the intersection of signal processing and machine learning that\nincorporate the advantages of both domains.",
          "link": "http://arxiv.org/abs/2012.08405",
          "publishedOn": "2021-06-29T01:55:20.653Z",
          "wordCount": 703,
          "title": "Model-Based Deep Learning. (arXiv:2012.08405v2 [eess.SP] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.11533",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Baek_J/0/1/0/all/0/1\">Jinheon Baek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_M/0/1/0/all/0/1\">Minki Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1\">Sung Ju Hwang</a>",
          "description": "Graph neural networks have been widely used on modeling graph data, achieving\nimpressive results on node classification and link prediction tasks. Yet,\nobtaining an accurate representation for a graph further requires a pooling\nfunction that maps a set of node representations into a compact form. A simple\nsum or average over all node representations considers all node features\nequally without consideration of their task relevance, and any structural\ndependencies among them. Recently proposed hierarchical graph pooling methods,\non the other hand, may yield the same representation for two different graphs\nthat are distinguished by the Weisfeiler-Lehman test, as they suboptimally\npreserve information from the node features. To tackle these limitations of\nexisting graph pooling methods, we first formulate the graph pooling problem as\na multiset encoding problem with auxiliary information about the graph\nstructure, and propose a Graph Multiset Transformer (GMT) which is a multi-head\nattention based global pooling layer that captures the interaction between\nnodes according to their structural dependencies. We show that GMT satisfies\nboth injectiveness and permutation invariance, such that it is at most as\npowerful as the Weisfeiler-Lehman graph isomorphism test. Moreover, our methods\ncan be easily extended to the previous node clustering approaches for\nhierarchical graph pooling. Our experimental results show that GMT\nsignificantly outperforms state-of-the-art graph pooling methods on graph\nclassification benchmarks with high memory and time efficiency, and obtains\neven larger performance gain on graph reconstruction and generation tasks.",
          "link": "http://arxiv.org/abs/2102.11533",
          "publishedOn": "2021-06-29T01:55:20.646Z",
          "wordCount": 716,
          "title": "Accurate Learning of Graph Representations with Graph Multiset Pooling. (arXiv:2102.11533v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14352",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Khamaru_K/0/1/0/all/0/1\">Koulik Khamaru</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Xia_E/0/1/0/all/0/1\">Eric Xia</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Wainwright_M/0/1/0/all/0/1\">Martin J. Wainwright</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Jordan_M/0/1/0/all/0/1\">Michael I. Jordan</a>",
          "description": "Various algorithms in reinforcement learning exhibit dramatic variability in\ntheir convergence rates and ultimate accuracy as a function of the problem\nstructure. Such instance-specific behavior is not captured by existing global\nminimax bounds, which are worst-case in nature. We analyze the problem of\nestimating optimal $Q$-value functions for a discounted Markov decision process\nwith discrete states and actions and identify an instance-dependent functional\nthat controls the difficulty of estimation in the $\\ell_\\infty$-norm. Using a\nlocal minimax framework, we show that this functional arises in lower bounds on\nthe accuracy on any estimation procedure. In the other direction, we establish\nthe sharpness of our lower bounds, up to factors logarithmic in the state and\naction spaces, by analyzing a variance-reduced version of $Q$-learning. Our\ntheory provides a precise way of distinguishing \"easy\" problems from \"hard\"\nones in the context of $Q$-learning, as illustrated by an ensemble with a\ncontinuum of difficulty.",
          "link": "http://arxiv.org/abs/2106.14352",
          "publishedOn": "2021-06-29T01:55:20.627Z",
          "wordCount": 587,
          "title": "Instance-optimality in optimal value estimation: Adaptivity via variance-reduced Q-learning. (arXiv:2106.14352v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2005.07115",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Haoyan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1\">Runjian Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1\">Yunsheng Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_Z/0/1/0/all/0/1\">Ziheng Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jie Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yizhou Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Wei Wang</a>",
          "description": "In this work, we focus on large graph similarity computation problem and\npropose a novel \"embedding-coarsening-matching\" learning framework, which\noutperforms state-of-the-art methods in this task and has significant\nimprovement in time efficiency. Graph similarity computation for metrics such\nas Graph Edit Distance (GED) is typically NP-hard, and existing\nheuristics-based algorithms usually achieves a unsatisfactory trade-off between\naccuracy and efficiency. Recently the development of deep learning techniques\nprovides a promising solution for this problem by a data-driven approach which\ntrains a network to encode graphs to their own feature vectors and computes\nsimilarity based on feature vectors. These deep-learning methods can be\nclassified to two categories, embedding models and matching models. Embedding\nmodels such as GCN-Mean and GCN-Max, which directly map graphs to respective\nfeature vectors, run faster but the performance is usually poor due to the lack\nof interactions across graphs. Matching models such as GMN, whose encoding\nprocess involves interaction across the two graphs, are more accurate but\ninteraction between whole graphs brings a significant increase in time\nconsumption (at least quadratic time complexity over number of nodes). Inspired\nby large biological molecular identification where the whole molecular is first\nmapped to functional groups and then identified based on these functional\ngroups, our \"embedding-coarsening-matching\" learning framework first embeds and\ncoarsens large graphs to coarsened graphs with denser local topology and then\nmatching mechanism is deployed on the coarsened graphs for the final similarity\nscores. Detailed experiments have been conducted and the results demonstrate\nthe efficiency and effectiveness of our proposed framework.",
          "link": "http://arxiv.org/abs/2005.07115",
          "publishedOn": "2021-06-29T01:55:20.620Z",
          "wordCount": 761,
          "title": "Hierarchical Large-scale Graph Similarity Computation via Graph Coarsening and Matching. (arXiv:2005.07115v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1810.01256",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_G/0/1/0/all/0/1\">Guanxiong Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_B/0/1/0/all/0/1\">Bo Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Shan Yu</a>",
          "description": "Deep neural networks (DNNs) are powerful tools in learning sophisticated but\nfixed mapping rules between inputs and outputs, thereby limiting their\napplication in more complex and dynamic situations in which the mapping rules\nare not kept the same but changing according to different contexts. To lift\nsuch limits, we developed a novel approach involving a learning algorithm,\ncalled orthogonal weights modification (OWM), with the addition of a\ncontext-dependent processing (CDP) module. We demonstrated that with OWM to\novercome the problem of catastrophic forgetting, and the CDP module to learn\nhow to reuse a feature representation and a classifier for different contexts,\na single network can acquire numerous context-dependent mapping rules in an\nonline and continual manner, with as few as $\\sim$10 samples to learn each.\nThis should enable highly compact systems to gradually learn myriad\nregularities of the real world and eventually behave appropriately within it.",
          "link": "http://arxiv.org/abs/1810.01256",
          "publishedOn": "2021-06-29T01:55:20.614Z",
          "wordCount": 634,
          "title": "Continual Learning of Context-dependent Processing in Neural Networks. (arXiv:1810.01256v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.11788",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rawal_K/0/1/0/all/0/1\">Kaivalya Rawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamar_E/0/1/0/all/0/1\">Ece Kamar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakkaraju_H/0/1/0/all/0/1\">Himabindu Lakkaraju</a>",
          "description": "As predictive models are increasingly being deployed to make a variety of\nconsequential decisions, there is a growing emphasis on designing algorithms\nthat can provide recourse to affected individuals. Existing recourse algorithms\nfunction under the assumption that the underlying predictive model does not\nchange. However, models are regularly updated in practice for several reasons\nincluding data distribution shifts. In this work, we make the first attempt at\nunderstanding how model updates resulting from data distribution shifts impact\nthe algorithmic recourses generated by state-of-the-art algorithms. We carry\nout a rigorous theoretical and empirical analysis to address the above\nquestion. Our theoretical results establish a lower bound on the probability of\nrecourse invalidation due to model shifts, and show the existence of a tradeoff\nbetween this invalidation probability and typical notions of \"cost\" minimized\nby modern recourse generation algorithms. We experiment with multiple synthetic\nand real world datasets, capturing different kinds of distribution shifts\nincluding temporal shifts, geospatial shifts, and shifts due to data\ncorrection. These experiments demonstrate that model updation due to all the\naforementioned distribution shifts can potentially invalidate recourses\ngenerated by state-of-the-art algorithms. Our findings thus not only expose\npreviously unknown flaws in the current recourse generation paradigm, but also\npave the way for fundamentally rethinking the design and development of\nrecourse generation algorithms.",
          "link": "http://arxiv.org/abs/2012.11788",
          "publishedOn": "2021-06-29T01:55:20.599Z",
          "wordCount": 692,
          "title": "Algorithmic Recourse in the Wild: Understanding the Impact of Data and Model Shifts. (arXiv:2012.11788v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1907.02513",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Stemmer_U/0/1/0/all/0/1\">Uri Stemmer</a>",
          "description": "We design a new algorithm for the Euclidean $k$-means problem that operates\nin the local model of differential privacy. Unlike in the non-private\nliterature, differentially private algorithms for the $k$-means objective incur\nboth additive and multiplicative errors. Our algorithm significantly reduces\nthe additive error while keeping the multiplicative error the same as in\nprevious state-of-the-art results. Specifically, on a database of size $n$, our\nalgorithm guarantees $O(1)$ multiplicative error and $\\approx n^{1/2+a}$\nadditive error for an arbitrarily small constant $a>0$. All previous algorithms\nin the local model had additive error $\\approx n^{2/3+a}$. Our techniques\nextend to $k$-median clustering.\n\nWe show that the additive error we obtain is almost optimal in terms of its\ndependency on the database size $n$. Specifically, we give a simple lower bound\nshowing that every locally-private algorithm for the $k$-means objective must\nhave additive error at least $\\approx\\sqrt{n}$.",
          "link": "http://arxiv.org/abs/1907.02513",
          "publishedOn": "2021-06-29T01:55:20.593Z",
          "wordCount": 601,
          "title": "Locally Private k-Means Clustering. (arXiv:1907.02513v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1906.02944",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Han-Jia Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1\">Hexiang Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_D/0/1/0/all/0/1\">De-Chuan Zhan</a>",
          "description": "Object recognition in the real-world requires handling long-tailed or even\nopen-ended data. An ideal visual system needs to recognize the populated head\nvisual concepts reliably and meanwhile efficiently learn about emerging new\ntail categories with a few training instances. Class-balanced many-shot\nlearning and few-shot learning tackle one side of this problem, by either\nlearning strong classifiers for head or learning to learn few-shot classifiers\nfor the tail. In this paper, we investigate the problem of generalized few-shot\nlearning (GFSL) -- a model during the deployment is required to learn about\ntail categories with few shots and simultaneously classify the head classes. We\npropose the ClAssifier SynThesis LEarning (CASTLE), a learning framework that\nlearns how to synthesize calibrated few-shot classifiers in addition to the\nmulti-class classifiers of head classes with a shared neural dictionary,\nshedding light upon the inductive GFSL. Furthermore, we propose an adaptive\nversion of CASTLE (ACASTLE) that adapts the head classifiers conditioned on the\nincoming tail training examples, yielding a framework that allows effective\nbackward knowledge transfer. As a consequence, ACASTLE can handle GFSL with\nclasses from heterogeneous domains effectively. CASTLE and ACASTLE demonstrate\nsuperior performances than existing GFSL algorithms and strong baselines on\nMiniImageNet as well as TieredImageNet datasets. More interestingly, they\noutperform previous state-of-the-art methods when evaluated with standard\nfew-shot learning criteria.",
          "link": "http://arxiv.org/abs/1906.02944",
          "publishedOn": "2021-06-29T01:55:20.579Z",
          "wordCount": 722,
          "title": "Learning Adaptive Classifiers Synthesis for Generalized Few-Shot Learning. (arXiv:1906.02944v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2003.11723",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yuntao Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruiting Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaowen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yirong Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1\">Hengyang Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chongjun Wang</a>",
          "description": "Although achieving remarkable progress, it is very difficult to induce a\nsupervised classifier without any labeled data. Unsupervised domain adaptation\nis able to overcome this challenge by transferring knowledge from a labeled\nsource domain to an unlabeled target domain. Transferability and\ndiscriminability are two key criteria for characterizing the superiority of\nfeature representations to enable successful domain adaptation. In this paper,\na novel method called \\textit{learning TransFerable and Discriminative Features\nfor unsupervised domain adaptation} (TFDF) is proposed to optimize these two\nobjectives simultaneously. On the one hand, distribution alignment is performed\nto reduce domain discrepancy and learn more transferable representations.\nInstead of adopting \\textit{Maximum Mean Discrepancy} (MMD) which only captures\nthe first-order statistical information to measure distribution discrepancy, we\nadopt a recently proposed statistic called \\textit{Maximum Mean and Covariance\nDiscrepancy} (MMCD), which can not only capture the first-order statistical\ninformation but also capture the second-order statistical information in the\nreproducing kernel Hilbert space (RKHS). On the other hand, we propose to\nexplore both local discriminative information via manifold regularization and\nglobal discriminative information via minimizing the proposed \\textit{class\nconfusion} objective to learn more discriminative features, respectively. We\nintegrate these two objectives into the \\textit{Structural Risk Minimization}\n(RSM) framework and learn a domain-invariant classifier. Comprehensive\nexperiments are conducted on five real-world datasets and the results verify\nthe effectiveness of the proposed method.",
          "link": "http://arxiv.org/abs/2003.11723",
          "publishedOn": "2021-06-29T01:55:20.573Z",
          "wordCount": 693,
          "title": "Learning transferable and discriminative features for unsupervised domain adaptation. (arXiv:2003.11723v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.07175",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Su_L/0/1/0/all/0/1\">Lang Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1\">Chuqing Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guofa Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_D/0/1/0/all/0/1\">Dongpu Cao</a>",
          "description": "Multimodal learning mimics the reasoning process of the human multi-sensory\nsystem, which is used to perceive the surrounding world. While making a\nprediction, the human brain tends to relate crucial cues from multiple sources\nof information. In this work, we propose a novel multimodal fusion module that\nlearns to emphasize more contributive features across all modalities.\nSpecifically, the proposed Multimodal Split Attention Fusion (MSAF) module\nsplits each modality into channel-wise equal feature blocks and creates a joint\nrepresentation that is used to generate soft attention for each channel across\nthe feature blocks. Further, the MSAF module is designed to be compatible with\nfeatures of various spatial dimensions and sequence lengths, suitable for both\nCNNs and RNNs. Thus, MSAF can be easily added to fuse features of any unimodal\nnetworks and utilize existing pretrained unimodal model weights. To demonstrate\nthe effectiveness of our fusion module, we design three multimodal networks\nwith MSAF for emotion recognition, sentiment analysis, and action recognition\ntasks. Our approach achieves competitive results in each task and outperforms\nother application-specific networks and multimodal fusion benchmarks.",
          "link": "http://arxiv.org/abs/2012.07175",
          "publishedOn": "2021-06-29T01:55:20.566Z",
          "wordCount": 637,
          "title": "MSAF: Multimodal Split Attention Fusion. (arXiv:2012.07175v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1302.6808",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Geiger_D/0/1/0/all/0/1\">Dan Geiger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heckerman_D/0/1/0/all/0/1\">David Heckerman</a>",
          "description": "We describe algorithms for learning Bayesian networks from a combination of\nuser knowledge and statistical data. The algorithms have two components: a\nscoring metric and a search procedure. The scoring metric takes a network\nstructure, statistical data, and a user's prior knowledge, and returns a score\nproportional to the posterior probability of the network structure given the\ndata. The search procedure generates networks for evaluation by the scoring\nmetric. Previous work has concentrated on metrics for domains containing only\ndiscrete variables, under the assumption that data represents a multinomial\nsample. In this paper, we extend this work, developing scoring metrics for\ndomains containing all continuous variables or a mixture of discrete and\ncontinuous variables, under the assumption that continuous data is sampled from\na multivariate normal distribution. Our work extends traditional statistical\napproaches for identifying vanishing regression coefficients in that we\nidentify two important assumptions, called event equivalence and parameter\nmodularity, that when combined allow the construction of prior distributions\nfor multivariate normal parameters from a single prior Bayesian network\nspecified by a user.",
          "link": "http://arxiv.org/abs/1302.6808",
          "publishedOn": "2021-06-29T01:55:20.560Z",
          "wordCount": 653,
          "title": "Learning Gaussian Networks. (arXiv:1302.6808v3 [cs.AI] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14289",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Ye_T/0/1/0/all/0/1\">Tian Ye</a>, <a href=\"http://arxiv.org/find/math/1/au:+Du_S/0/1/0/all/0/1\">Simon S. Du</a>",
          "description": "We study the asymmetric low-rank factorization problem: \\[\\min_{\\mathbf{U}\n\\in \\mathbb{R}^{m \\times d}, \\mathbf{V} \\in \\mathbb{R}^{n \\times d}}\n\\frac{1}{2}\\|\\mathbf{U}\\mathbf{V}^\\top -\\mathbf{\\Sigma}\\|_F^2\\] where\n$\\mathbf{\\Sigma}$ is a given matrix of size $m \\times n$ and rank $d$. This is\na canonical problem that admits two difficulties in optimization: 1)\nnon-convexity and 2) non-smoothness (due to unbalancedness of $\\mathbf{U}$ and\n$\\mathbf{V}$). This is also a prototype for more complex problems such as\nasymmetric matrix sensing and matrix completion. Despite being non-convex and\nnon-smooth, it has been observed empirically that the randomly initialized\ngradient descent algorithm can solve this problem in polynomial time. Existing\ntheories to explain this phenomenon all require artificial modifications of the\nalgorithm, such as adding noise in each iteration and adding a balancing\nregularizer to balance the $\\mathbf{U}$ and $\\mathbf{V}$.\n\nThis paper presents the first proof that shows randomly initialized gradient\ndescent converges to a global minimum of the asymmetric low-rank factorization\nproblem with a polynomial rate. For the proof, we develop 1) a new\nsymmetrization technique to capture the magnitudes of the symmetry and\nasymmetry, and 2) a quantitative perturbation analysis to approximate matrix\nderivatives. We believe both are useful for other related non-convex problems.",
          "link": "http://arxiv.org/abs/2106.14289",
          "publishedOn": "2021-06-29T01:55:20.554Z",
          "wordCount": 637,
          "title": "Global Convergence of Gradient Descent for Asymmetric Low-Rank Matrix Factorization. (arXiv:2106.14289v1 [math.OC])"
        },
        {
          "id": "http://arxiv.org/abs/1904.06366",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Zhu_Y/0/1/0/all/0/1\">Yifan Zhu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Dai_F/0/1/0/all/0/1\">Fan Dai</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Maitra_R/0/1/0/all/0/1\">Ranjan Maitra</a>",
          "description": "We develop methodology for three-dimensional (3D) radial visualization\n(RadViz) of multidimensional datasets. Our tool is called RadViz3D and extends\nthe classical two-dimensional (2D) RadViz that visualizes multivariate data in\nthe 2D plane by mapping every observation to a point inside the unit circle. We\nshow that distributing anchor points uniformly on the 3D unit sphere provides\nthe best visualization with minimal artificial visual correlation for data with\nuncorrelated variables. However, anchor points can be placed exactly\nequi-distant from each other only for the five Platonic solids. We provide\nequi-distant anchor points for these five settings, and approximately\nequi-distant anchor points via a Fibonacci grid for the other cases. Our\nmethodology, implemented in the R package $radviz3d$, makes fully 3D RadViz\npossible and is shown to improve clarity of this nonlinear display technique on\nsimulated and real datasets.",
          "link": "http://arxiv.org/abs/1904.06366",
          "publishedOn": "2021-06-29T01:55:20.539Z",
          "wordCount": 601,
          "title": "Fully Three-dimensional Radial Visualization. (arXiv:1904.06366v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13956",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gunasekaran_V/0/1/0/all/0/1\">V. Gunasekaran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kovi_K/0/1/0/all/0/1\">K.K. Kovi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arja_S/0/1/0/all/0/1\">S. Arja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chimata_R/0/1/0/all/0/1\">R. Chimata</a>",
          "description": "Renewable energy forecasting is attaining greater importance due to its\nconstant increase in contribution to the electrical power grids. Solar energy\nis one of the most significant contributors to renewable energy and is\ndependent on solar irradiation. For the effective management of electrical\npower grids, forecasting models that predict solar irradiation, with high\naccuracy, are needed. In the current study, Machine Learning techniques such as\nLinear Regression, Extreme Gradient Boosting and Genetic Algorithm Optimization\nare used to forecast solar irradiation. The data used for training and\nvalidation is recorded from across three different geographical stations in the\nUnited States that are part of the SURFRAD network. A Global Horizontal Index\n(GHI) is predicted for the models built and compared. Genetic Algorithm\nOptimization is applied to XGB to further improve the accuracy of solar\nirradiation prediction.",
          "link": "http://arxiv.org/abs/2106.13956",
          "publishedOn": "2021-06-29T01:55:20.533Z",
          "wordCount": 577,
          "title": "Solar Irradiation Forecasting using Genetic Algorithms. (arXiv:2106.13956v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14406",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1\">Robert Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saxena_N/0/1/0/all/0/1\">Nayan Saxena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_R/0/1/0/all/0/1\">Rohan Jain</a>",
          "description": "Deep learning has proven to be a highly effective problem-solving tool for\nobject detection and image segmentation across various domains such as\nhealthcare and autonomous driving. At the heart of this performance lies neural\narchitecture design which relies heavily on domain knowledge and prior\nexperience on the researchers' behalf. More recently, this process of finding\nthe most optimal architectures, given an initial search space of possible\noperations, was automated by Neural Architecture Search (NAS). In this paper,\nwe evaluate the robustness of one such algorithm known as Efficient NAS (ENAS)\nagainst data agnostic poisoning attacks on the original search space with\ncarefully designed ineffective operations. By evaluating algorithm performance\non the CIFAR-10 dataset, we empirically demonstrate how our novel search space\npoisoning (SSP) approach and multiple-instance poisoning attacks exploit design\nflaws in the ENAS controller to result in inflated prediction error rates for\nchild networks. Our results provide insights into the challenges to surmount in\nusing NAS for more adversarially robust architecture search.",
          "link": "http://arxiv.org/abs/2106.14406",
          "publishedOn": "2021-06-29T01:55:20.498Z",
          "wordCount": 630,
          "title": "Poisoning the Search Space in Neural Architecture Search. (arXiv:2106.14406v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2010.05780",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xian_L/0/1/0/all/0/1\">Lu Xian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adams_H/0/1/0/all/0/1\">Henry Adams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Topaz_C/0/1/0/all/0/1\">Chad M. Topaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ziegelmeier_L/0/1/0/all/0/1\">Lori Ziegelmeier</a>",
          "description": "One approach to understanding complex data is to study its shape through the\nlens of algebraic topology. While the early development of topological data\nanalysis focused primarily on static data, in recent years, theoretical and\napplied studies have turned to data that varies in time. A time-varying\ncollection of metric spaces as formed, for example, by a moving school of fish\nor flock of birds, can contain a vast amount of information. There is often a\nneed to simplify or summarize the dynamic behavior. We provide an introduction\nto topological summaries of time-varying metric spaces including vineyards\n[19], crocker plots [56], and multiparameter rank functions [37]. We then\nintroduce a new tool to summarize time-varying metric spaces: a crocker stack.\nCrocker stacks are convenient for visualization, amenable to machine learning,\nand satisfy a desirable continuity property which we prove. We demonstrate the\nutility of crocker stacks for a parameter identification task involving an\ninfluential model of biological aggregations [58]. Altogether, we aim to bring\nthe broader applied mathematics community up-to-date on topological summaries\nof time-varying metric spaces.",
          "link": "http://arxiv.org/abs/2010.05780",
          "publishedOn": "2021-06-29T01:55:18.547Z",
          "wordCount": 658,
          "title": "Capturing Dynamics of Time-Varying Data via Topology. (arXiv:2010.05780v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14190",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gowdra_N/0/1/0/all/0/1\">Nidhi Gowdra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sinha_R/0/1/0/all/0/1\">Roopak Sinha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+MacDonell_S/0/1/0/all/0/1\">Stephen MacDonell</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_W/0/1/0/all/0/1\">Wei Qi Yan</a>",
          "description": "Convolutional Neural Networks (CNNs) such as ResNet-50, DenseNet-40 and\nResNeXt-56 are severely over-parameterized, necessitating a consequent increase\nin the computational resources required for model training which scales\nexponentially for increments in model depth. In this paper, we propose an\nEntropy-Based Convolutional Layer Estimation (EBCLE) heuristic which is robust\nand simple, yet effective in resolving the problem of over-parameterization\nwith regards to network depth of CNN model. The EBCLE heuristic employs a\npriori knowledge of the entropic data distribution of input datasets to\ndetermine an upper bound for convolutional network depth, beyond which identity\ntransformations are prevalent offering insignificant contributions for\nenhancing model performance. Restricting depth redundancies by forcing feature\ncompression and abstraction restricts over-parameterization while decreasing\ntraining time by 24.99% - 78.59% without degradation in model performance. We\npresent empirical evidence to emphasize the relative effectiveness of broader,\nyet shallower models trained using the EBCLE heuristic, which maintains or\noutperforms baseline classification accuracies of narrower yet deeper models.\nThe EBCLE heuristic is architecturally agnostic and EBCLE based CNN models\nrestrict depth redundancies resulting in enhanced utilization of the available\ncomputational resources. The proposed EBCLE heuristic is a compelling technique\nfor researchers to analytically justify their HyperParameter (HP) choices for\nCNNs. Empirical validation of the EBCLE heuristic in training CNN models was\nestablished on five benchmarking datasets (ImageNet32, CIFAR-10/100, STL-10,\nMNIST) and four network architectures (DenseNet, ResNet, ResNeXt and\nEfficientNet B0-B2) with appropriate statistical tests employed to infer any\nconclusive claims presented in this paper.",
          "link": "http://arxiv.org/abs/2106.14190",
          "publishedOn": "2021-06-29T01:55:18.531Z",
          "wordCount": 721,
          "title": "Mitigating severe over-parameterization in deep convolutional neural networks through forced feature abstraction and compression with an entropy-based heuristic. (arXiv:2106.14190v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2011.00731",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Law_H/0/1/0/all/0/1\">Ho Law</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_G/0/1/0/all/0/1\">Gary P. T. Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_K/0/1/0/all/0/1\">Ka Chun Lam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lui_L/0/1/0/all/0/1\">Lok Ming Lui</a>",
          "description": "Image registration has been widely studied over the past several decades,\nwith numerous applications in science, engineering and medicine. Most of the\nconventional mathematical models for large deformation image registration rely\non prescribed landmarks, which usually require tedious manual labeling and are\nprone to error. In recent years, there has been a surge of interest in the use\nof machine learning for image registration. In this paper, we develop a novel\nmethod for large deformation image registration by a fusion of quasiconformal\ntheory and convolutional neural network (CNN). More specifically, we propose a\nquasiconformal energy model with a novel fidelity term that incorporates the\nfeatures extracted using a pre-trained CNN, thereby allowing us to obtain\nmeaningful registration results without any guidance of prescribed landmarks.\nMoreover, unlike many prior image registration methods, the bijectivity of our\nmethod is guaranteed by quasiconformal theory. Experimental results are\npresented to demonstrate the effectiveness of the proposed method. More\nbroadly, our work sheds light on how rigorous mathematical theories and\npractical machine learning approaches can be integrated for developing\ncomputational methods with improved performance.",
          "link": "http://arxiv.org/abs/2011.00731",
          "publishedOn": "2021-06-29T01:55:18.517Z",
          "wordCount": 681,
          "title": "Quasiconformal model with CNN features for large deformation image registration. (arXiv:2011.00731v3 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.01807",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Hurwitz_C/0/1/0/all/0/1\">Cole Hurwitz</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Kudryashova_N/0/1/0/all/0/1\">Nina Kudryashova</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Onken_A/0/1/0/all/0/1\">Arno Onken</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Hennig_M/0/1/0/all/0/1\">Matthias H. Hennig</a>",
          "description": "Modern recording technologies now enable simultaneous recording from large\nnumbers of neurons. This has driven the development of new statistical models\nfor analyzing and interpreting neural population activity. Here we provide a\nbroad overview of recent developments in this area. We compare and contrast\ndifferent approaches, highlight strengths and limitations, and discuss\nbiological and mechanistic insights that these methods provide.",
          "link": "http://arxiv.org/abs/2102.01807",
          "publishedOn": "2021-06-29T01:55:18.510Z",
          "wordCount": 527,
          "title": "Building population models for large-scale neural recordings: opportunities and pitfalls. (arXiv:2102.01807v3 [q-bio.NC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14265",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Witt_L/0/1/0/all/0/1\">Leon Witt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zafar_U/0/1/0/all/0/1\">Usama Zafar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_K/0/1/0/all/0/1\">KuoYeh Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sattler_F/0/1/0/all/0/1\">Felix Sattler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samek_W/0/1/0/all/0/1\">Wojciech Samek</a>",
          "description": "The recent advent of various forms of Federated Knowledge Distillation (FD)\npaves the way for a new generation of robust and communication-efficient\nFederated Learning (FL), where mere soft-labels are aggregated, rather than\nwhole gradients of Deep Neural Networks (DNN) as done in previous FL schemes.\nThis security-per-design approach in combination with increasingly performant\nInternet of Things (IoT) and mobile devices opens up a new realm of\npossibilities to utilize private data from industries as well as from\nindividuals as input for artificial intelligence model training. Yet in\nprevious FL systems, lack of trust due to the imbalance of power between\nworkers and a central authority, the assumption of altruistic worker\nparticipation and the inability to correctly measure and compare contributions\nof workers hinder this technology from scaling beyond small groups of already\nentrusted entities towards mass adoption. This work aims to mitigate the\naforementioned issues by introducing a novel decentralized federated learning\nframework where heavily compressed 1-bit soft-labels, resembling 1-hot label\npredictions, are aggregated on a smart contract. In a context where workers'\ncontributions are now easily comparable, we modify the Peer Truth Serum for\nCrowdsourcing mechanism (PTSC) for FD to reward honest participation based on\npeer consistency in an incentive compatible fashion. Due to heavy reductions of\nboth computational complexity and storage, our framework is a fully\non-blockchain FL system that is feasible on simple smart contracts and\ntherefore blockchain agnostic. We experimentally test our new framework and\nvalidate its theoretical properties.",
          "link": "http://arxiv.org/abs/2106.14265",
          "publishedOn": "2021-06-29T01:55:18.492Z",
          "wordCount": 687,
          "title": "Reward-Based 1-bit Compressed Federated Distillation on Blockchain. (arXiv:2106.14265v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14323",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Deziderio_N/0/1/0/all/0/1\">Nathalie Deziderio</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Carvalho_H/0/1/0/all/0/1\">Hugo Tremonte de Carvalho</a>",
          "description": "This work was developed aiming to employ Statistical techniques to the field\nof Music Emotion Recognition, a well-recognized area within the Signal\nProcessing world, but hardly explored from the statistical point of view. Here,\nwe opened several possibilities within the field, applying modern Bayesian\nStatistics techniques and developing efficient algorithms, focusing on the\napplicability of the results obtained. Although the motivation for this project\nwas the development of a emotion-based music recommendation system, its main\ncontribution is a highly adaptable multivariate model that can be useful\ninterpreting any database where there is an interest in applying regularization\nin an efficient manner. Broadly speaking, we will explore what role a sound\ntheoretical statistical analysis can play in the modeling of an algorithm that\nis able to understand a well-known database and what can be gained with this\nkind of approach.",
          "link": "http://arxiv.org/abs/2106.14323",
          "publishedOn": "2021-06-29T01:55:18.485Z",
          "wordCount": 580,
          "title": "Use of Variational Inference in Music Emotion Recognition. (arXiv:2106.14323v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14195",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Macke_J/0/1/0/all/0/1\">J. Macke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sedlar_J/0/1/0/all/0/1\">J. Sedlar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Olsak_M/0/1/0/all/0/1\">M. Olsak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Urban_J/0/1/0/all/0/1\">J. Urban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sivic_J/0/1/0/all/0/1\">J. Sivic</a>",
          "description": "We describe a purely image-based method for finding geometric constructions\nwith a ruler and compass in the Euclidea geometric game. The method is based on\nadapting the Mask R-CNN state-of-the-art image processing neural architecture\nand adding a tree-based search procedure to it. In a supervised setting, the\nmethod learns to solve all 68 kinds of geometric construction problems from the\nfirst six level packs of Euclidea with an average 92% accuracy. When evaluated\non new kinds of problems, the method can solve 31 of the 68 kinds of Euclidea\nproblems. We believe that this is the first time that a purely image-based\nlearning has been trained to solve geometric construction problems of this\ndifficulty.",
          "link": "http://arxiv.org/abs/2106.14195",
          "publishedOn": "2021-06-29T01:55:18.473Z",
          "wordCount": 576,
          "title": "Learning to solve geometric construction problems from images. (arXiv:2106.14195v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14305",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jaekyeom Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Seohong Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1\">Gunhee Kim</a>",
          "description": "Having the ability to acquire inherent skills from environments without any\nexternal rewards or supervision like humans is an important problem. We propose\na novel unsupervised skill discovery method named Information Bottleneck Option\nLearning (IBOL). On top of the linearization of environments that promotes more\nvarious and distant state transitions, IBOL enables the discovery of diverse\nskills. It provides the abstraction of the skills learned with the information\nbottleneck framework for the options with improved stability and encouraged\ndisentanglement. We empirically demonstrate that IBOL outperforms multiple\nstate-of-the-art unsupervised skill discovery methods on the\ninformation-theoretic evaluations and downstream tasks in MuJoCo environments,\nincluding Ant, HalfCheetah, Hopper and D'Kitty.",
          "link": "http://arxiv.org/abs/2106.14305",
          "publishedOn": "2021-06-29T01:55:18.467Z",
          "wordCount": 550,
          "title": "Unsupervised Skill Discovery with Bottleneck Option Learning. (arXiv:2106.14305v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14413",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cha_H/0/1/0/all/0/1\">Hyuntak Cha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jaeho Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1\">Jinwoo Shin</a>",
          "description": "Recent breakthroughs in self-supervised learning show that such algorithms\nlearn visual representations that can be transferred better to unseen tasks\nthan joint-training methods relying on task-specific supervision. In this\npaper, we found that the similar holds in the continual learning con-text:\ncontrastively learned representations are more robust against the catastrophic\nforgetting than jointly trained representations. Based on this novel\nobservation, we propose a rehearsal-based continual learning algorithm that\nfocuses on continually learning and maintaining transferable representations.\nMore specifically, the proposed scheme (1) learns representations using the\ncontrastive learning objective, and (2) preserves learned representations using\na self-supervised distillation step. We conduct extensive experimental\nvalidations under popular benchmark image classification datasets, where our\nmethod sets the new state-of-the-art performance.",
          "link": "http://arxiv.org/abs/2106.14413",
          "publishedOn": "2021-06-29T01:55:18.454Z",
          "wordCount": 550,
          "title": "Co$^2$L: Contrastive Continual Learning. (arXiv:2106.14413v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14277",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Takhanov_R/0/1/0/all/0/1\">Rustem Takhanov</a>",
          "description": "We present a new way of study of Mercer kernels, by corresponding to a\nspecial kernel $K$ a pseudo-differential operator $p({\\mathbf x}, D)$ such that\n$\\mathcal{F} p({\\mathbf x}, D)^\\dag p({\\mathbf x}, D) \\mathcal{F}^{-1}$ acts on\nsmooth functions in the same way as an integral operator associated with $K$\n(where $\\mathcal{F}$ is the Fourier transform). We show that kernels defined by\npseudo-differential operators are able to approximate uniformly any continuous\nMercer kernel on a compact set.\n\nThe symbol $p({\\mathbf x}, {\\mathbf y})$ encapsulates a lot of useful\ninformation about the structure of the Maximum Mean Discrepancy distance\ndefined by the kernel $K$. We approximate $p({\\mathbf x}, {\\mathbf y})$ with\nthe sum of the first $r$ terms of the Singular Value Decomposition of $p$,\ndenoted by $p_r({\\mathbf x}, {\\mathbf y})$. If ordered singular values of the\nintegral operator associated with $p({\\mathbf x}, {\\mathbf y})$ die down\nrapidly, the MMD distance defined by the new symbol $p_r$ differs from the\ninitial one only slightly. Moreover, the new MMD distance can be interpreted as\nan aggregated result of comparing $r$ local moments of two probability\ndistributions.\n\nThe latter results holds under the condition that right singular vectors of\nthe integral operator associated with $p$ are uniformly bounded. But even if\nthis is not satisfied we can still hold that the Hilbert-Schmidt distance\nbetween $p$ and $p_r$ vanishes. Thus, we report an interesting phenomenon: the\nMMD distance measures the difference of two probability distributions with\nrespect to a certain number of local moments, $r^\\ast$, and this number\n$r^\\ast$ depends on the speed with which singular values of $p$ die down.",
          "link": "http://arxiv.org/abs/2106.14277",
          "publishedOn": "2021-06-29T01:55:18.447Z",
          "wordCount": 690,
          "title": "How many moments does MMD compare?. (arXiv:2106.14277v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1911.04293",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Tao_T/0/1/0/all/0/1\">Ting Tao</a>, <a href=\"http://arxiv.org/find/math/1/au:+Pan_S/0/1/0/all/0/1\">Shaohua Pan</a>, <a href=\"http://arxiv.org/find/math/1/au:+Bi_S/0/1/0/all/0/1\">Shujun Bi</a>",
          "description": "This paper is concerned with the squared F(robenius)-norm regularized\nfactorization form for noisy low-rank matrix recovery problems. Under a\nsuitable assumption on the restricted condition number of the Hessian for the\nloss function, we derive an error bound to the true matrix for the non-strict\ncritical points with rank not more than that of the true matrix. Then, for the\nsquared F-norm regularized factorized least squares loss function, under the\nnoisy and full sample setting we establish its KL property of exponent $1/2$ on\nits global minimizer set, and under the noisy and partial sample setting\nachieve this property for a class of critical points. These theoretical\nfindings are also confirmed by solving the squared F-norm regularized\nfactorization problem with an accelerated alternating minimization method.",
          "link": "http://arxiv.org/abs/1911.04293",
          "publishedOn": "2021-06-29T01:55:18.442Z",
          "wordCount": 598,
          "title": "Error bound of critical points and KL property of exponent $1/2$ for squared F-norm regularized factorization. (arXiv:1911.04293v2 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.12255",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Abheesht Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pandey_H/0/1/0/all/0/1\">Harshit Pandey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chhablani_G/0/1/0/all/0/1\">Gunjan Chhablani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhartia_Y/0/1/0/all/0/1\">Yash Bhartia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dash_T/0/1/0/all/0/1\">Tirtharaj Dash</a>",
          "description": "In this article, we present our methodologies for SemEval-2021 Task-4:\nReading Comprehension of Abstract Meaning. Given a fill-in-the-blank-type\nquestion and a corresponding context, the task is to predict the most suitable\nword from a list of 5 options. There are three sub-tasks within this task:\nImperceptibility (subtask-I), Non-Specificity (subtask-II), and Intersection\n(subtask-III). We use encoders of transformers-based models pre-trained on the\nmasked language modelling (MLM) task to build our Fill-in-the-blank (FitB)\nmodels. Moreover, to model imperceptibility, we define certain linguistic\nfeatures, and to model non-specificity, we leverage information from hypernyms\nand hyponyms provided by a lexical database. Specifically, for non-specificity,\nwe try out augmentation techniques, and other statistical techniques. We also\npropose variants, namely Chunk Voting and Max Context, to take care of input\nlength restrictions for BERT, etc. Additionally, we perform a thorough ablation\nstudy, and use Integrated Gradients to explain our predictions on a few\nsamples. Our best submissions achieve accuracies of 75.31% and 77.84%, on the\ntest sets for subtask-I and subtask-II, respectively. For subtask-III, we\nachieve accuracies of 65.64% and 62.27%.",
          "link": "http://arxiv.org/abs/2102.12255",
          "publishedOn": "2021-06-29T01:55:18.392Z",
          "wordCount": 666,
          "title": "LRG at SemEval-2021 Task 4: Improving Reading Comprehension with Abstract Words using Augmentation, Linguistic Features and Voting. (arXiv:2102.12255v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14343",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cutkosky_A/0/1/0/all/0/1\">Ashok Cutkosky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehta_H/0/1/0/all/0/1\">Harsh Mehta</a>",
          "description": "We consider non-convex stochastic optimization using first-order algorithms\nfor which the gradient estimates may have heavy tails. We show that a\ncombination of gradient clipping, momentum, and normalized gradient descent\nyields convergence to critical points in high-probability with best-known rates\nfor smooth losses when the gradients only have bounded $\\mathfrak{p}$th moments\nfor some $\\mathfrak{p}\\in(1,2]$. We then consider the case of second-order\nsmooth losses, which to our knowledge have not been studied in this setting,\nand again obtain high-probability bounds for any $\\mathfrak{p}$. Moreover, our\nresults hold for arbitrary smooth norms, in contrast to the typical SGD\nanalysis which requires a Hilbert space norm. Further, we show that after a\nsuitable \"burn-in\" period, the objective value will monotonically decrease for\nevery iteration until a critical point is identified, which provides intuition\nbehind the popular practice of learning rate \"warm-up\" and also yields a\nlast-iterate guarantee.",
          "link": "http://arxiv.org/abs/2106.14343",
          "publishedOn": "2021-06-29T01:55:18.379Z",
          "wordCount": 583,
          "title": "High-probability Bounds for Non-Convex Stochastic Optimization with Heavy Tails. (arXiv:2106.14343v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2009.00162",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guan_Y/0/1/0/all/0/1\">Yue Guan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qifan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsiotras_P/0/1/0/all/0/1\">Panagiotis Tsiotras</a>",
          "description": "We explore the use of policy approximations to reduce the computational cost\nof learning Nash equilibria in zero-sum stochastic games. We propose a new\nQ-learning type algorithm that uses a sequence of entropy-regularized soft\npolicies to approximate the Nash policy during the Q-function updates. We prove\nthat under certain conditions, by updating the regularized Q-function, the\nalgorithm converges to a Nash equilibrium. We also demonstrate the proposed\nalgorithm's ability to transfer previous training experiences, enabling the\nagents to adapt quickly to new environments. We provide a dynamic\nhyper-parameter scheduling scheme to further expedite convergence. Empirical\nresults applied to a number of stochastic games verify that the proposed\nalgorithm converges to the Nash equilibrium, while exhibiting a major speed-up\nover existing algorithms.",
          "link": "http://arxiv.org/abs/2009.00162",
          "publishedOn": "2021-06-29T01:55:18.373Z",
          "wordCount": 606,
          "title": "Learning Nash Equilibria in Zero-Sum Stochastic Games via Entropy-Regularized Policy Approximation. (arXiv:2009.00162v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14300",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1\">Ren Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tianqi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_P/0/1/0/all/0/1\">Philip Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Sijia Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajapakse_I/0/1/0/all/0/1\">Indika Rajapakse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hero_A/0/1/0/all/0/1\">Alfred Hero</a>",
          "description": "K-Nearest Neighbor (kNN)-based deep learning methods have been applied to\nmany applications due to their simplicity and geometric interpretability.\nHowever, the robustness of kNN-based classification models has not been\nthoroughly explored and kNN attack strategies are underdeveloped. In this\npaper, we propose an Adversarial Soft kNN (ASK) loss to both design more\neffective kNN attack strategies and to develop better defenses against them.\nOur ASK loss approach has two advantages. First, ASK loss can better\napproximate the kNN's probability of classification error than objectives\nproposed in previous works. Second, the ASK loss is interpretable: it preserves\nthe mutual information between the perturbed input and the kNN of the\nunperturbed input. We use the ASK loss to generate a novel attack method called\nthe ASK-Attack (ASK-Atk), which shows superior attack efficiency and accuracy\ndegradation relative to previous kNN attacks. Based on the ASK-Atk, we then\nderive an ASK-Defense (ASK-Def) method that optimizes the worst-case training\nloss induced by ASK-Atk.",
          "link": "http://arxiv.org/abs/2106.14300",
          "publishedOn": "2021-06-29T01:55:18.367Z",
          "wordCount": 601,
          "title": "ASK: Adversarial Soft k-Nearest Neighbor Attack and Defense. (arXiv:2106.14300v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/1907.12727",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1\">Qingyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adeli_E/0/1/0/all/0/1\">Ehsan Adeli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfefferbaum_A/0/1/0/all/0/1\">Adolf Pfefferbaum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sullivan_E/0/1/0/all/0/1\">Edith V. Sullivan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pohl_K/0/1/0/all/0/1\">Kilian M. Pohl</a>",
          "description": "With recent advances in deep learning, neuroimaging studies increasingly rely\non convolutional networks (ConvNets) to predict diagnosis based on MR images.\nTo gain a better understanding of how a disease impacts the brain, the studies\nvisualize the salience maps of the ConvNet highlighting voxels within the brain\nmajorly contributing to the prediction. However, these salience maps are\ngenerally confounded, i.e., some salient regions are more predictive of\nconfounding variables (such as age) than the diagnosis. To avoid such\nmisinterpretation, we propose in this paper an approach that aims to visualize\nconfounder-free saliency maps that only highlight voxels predictive of the\ndiagnosis. The approach incorporates univariate statistical tests to identify\nconfounding effects within the intermediate features learned by ConvNet. The\ninfluence from the subset of confounded features is then removed by a novel\npartial back-propagation procedure. We use this two-step approach to visualize\nconfounder-free saliency maps extracted from synthetic and two real datasets.\nThese experiments reveal the potential of our visualization in producing\nunbiased model-interpretation.",
          "link": "http://arxiv.org/abs/1907.12727",
          "publishedOn": "2021-06-29T01:55:18.353Z",
          "wordCount": 638,
          "title": "Confounder-Aware Visualization of ConvNets. (arXiv:1907.12727v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14350",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kovalerchuk_B/0/1/0/all/0/1\">Boris Kovalerchuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalla_D/0/1/0/all/0/1\">Divya Chandrika Kalla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_B/0/1/0/all/0/1\">Bedant Agarwal</a>",
          "description": "Powerful deep learning algorithms open an opportunity for solving non-image\nMachine Learning (ML) problems by transforming these problems to into the image\nrecognition problems. The CPC-R algorithm presented in this chapter converts\nnon-image data into images by visualizing non-image data. Then deep learning\nCNN algorithms solve the learning problems on these images. The design of the\nCPC-R algorithm allows preserving all high-dimensional information in 2-D\nimages. The use of pair values mapping instead of single value mapping used in\nthe alternative approaches allows encoding each n-D point with 2 times fewer\nvisual elements. The attributes of an n-D point are divided into pairs of its\nvalues and each pair is visualized as 2-D points in the same 2-D Cartesian\ncoordinates. Next, grey scale or color intensity values are assigned to each\npair to encode the order of pairs. This is resulted in the heatmap image. The\ncomputational experiments with CPC-R are conducted for different CNN\narchitectures, and methods to optimize the CPC-R images showing that the\ncombined CPC-R and deep learning CNN algorithms are able to solve non-image ML\nproblems reaching high accuracy on the benchmark datasets. This chapter expands\nour prior work by adding more experiments to test accuracy of classification,\nexploring saliency and informativeness of discovered features to test their\ninterpretability, and generalizing the approach.",
          "link": "http://arxiv.org/abs/2106.14350",
          "publishedOn": "2021-06-29T01:55:18.348Z",
          "wordCount": 655,
          "title": "Deep Learning Image Recognition for Non-images. (arXiv:2106.14350v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14295",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Azimi_F/0/1/0/all/0/1\">Fatemeh Azimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raue_F/0/1/0/all/0/1\">Federico Raue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hees_J/0/1/0/all/0/1\">Joern Hees</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dengel_A/0/1/0/all/0/1\">Andreas Dengel</a>",
          "description": "Spatial Transformer Networks (STN) can generate geometric transformations\nwhich modify input images to improve the classifier's performance. In this\nwork, we combine the idea of STN with Reinforcement Learning (RL). To this end,\nwe break the affine transformation down into a sequence of simple and discrete\ntransformations. We formulate the task as a Markovian Decision Process (MDP)\nand use RL to solve this sequential decision-making problem. STN architectures\nlearn the transformation parameters by minimizing the classification error and\nbackpropagating the gradients through a sub-differentiable sampling module. In\nour method, we are not bound to the differentiability of the sampling modules.\nMoreover, we have freedom in designing the objective rather than only\nminimizing the error; e.g., we can directly set the target as maximizing the\naccuracy. We design multiple experiments to verify the effectiveness of our\nmethod using cluttered MNIST and Fashion-MNIST datasets and show that our\nmethod outperforms STN with a proper definition of MDP components.",
          "link": "http://arxiv.org/abs/2106.14295",
          "publishedOn": "2021-06-29T01:55:18.338Z",
          "wordCount": 587,
          "title": "A Reinforcement Learning Approach for Sequential Spatial Transformer Networks. (arXiv:2106.14295v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2006.06365",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Juditsky_A/0/1/0/all/0/1\">Anatoli Juditsky</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Kulunchakov_A/0/1/0/all/0/1\">Andrei Kulunchakov</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Tsyntseus_H/0/1/0/all/0/1\">Hlib Tsyntseus</a>",
          "description": "In this paper, we discuss application of iterative Stochastic Optimization\nroutines to the problem of sparse signal recovery from noisy observation. Using\nStochastic Mirror Descent algorithm as a building block, we develop a\nmultistage procedure for recovery of sparse solutions to Stochastic\nOptimization problem under assumption of smoothness and quadratic minoration on\nthe expected objective. An interesting feature of the proposed algorithm is\nlinear convergence of the approximate solution during the preliminary phase of\nthe routine when the component of stochastic error in the gradient observation\nwhich is due to bad initial approximation of the optimal solution is larger\nthan the \"ideal\" asymptotic error component owing to observation noise \"at the\noptimal solution.\" We also show how one can straightforwardly enhance\nreliability of the corresponding solution by using Median-of-Means like\ntechniques.\n\nWe illustrate the performance of the proposed algorithms in application to\nclassical problems of recovery of sparse and low rank signals in linear\nregression framework. We show, under rather weak assumption on the regressor\nand noise distributions, how they lead to parameter estimates which obey (up to\nfactors which are logarithmic in problem dimension and confidence level) the\nbest known to us accuracy bounds.",
          "link": "http://arxiv.org/abs/2006.06365",
          "publishedOn": "2021-06-29T01:55:18.333Z",
          "wordCount": 643,
          "title": "Sparse recovery by reduced variance stochastic approximation. (arXiv:2006.06365v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.00413",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Qin_J/0/1/0/all/0/1\">Jiancheng Qin</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_J/0/1/0/all/0/1\">Jin Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1\">Ying Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ye_Q/0/1/0/all/0/1\">Qiang Ye</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1\">Hua Li</a>",
          "description": "Two-stage ensemble-based forecasting methods have been studied extensively in\nthe wind power forecasting field. However, deep learning-based wind power\nforecasting studies have not investigated two aspects. In the first stage,\ndifferent learning structures considering multiple inputs and multiple outputs\nhave not been discussed. In the second stage, the model extrapolation issue has\nnot been investigated. Therefore, we develop four deep neural networks for the\nfirst stage to learn data features considering the input-and-output structure.\nWe then explore the model extrapolation issue in the second stage using\ndifferent modeling methods. Considering the overfitting issue, we propose a new\nmoving window-based algorithm using a validation set in the first stage to\nupdate the training data in both stages with two different moving window\nprocesses.Experiments were conducted at three wind farms, and the results\ndemonstrate that the model with single input multiple output structure obtains\nbetter forecasting accuracy compared to existing models. In addition, the ridge\nregression method results in a better ensemble model that can further improve\nforecasting accuracy compared to existing machine learning methods. Finally,\nthe proposed two-stage forecasting algorithm can generate more accurate and\nstable results than existing algorithms.",
          "link": "http://arxiv.org/abs/2006.00413",
          "publishedOn": "2021-06-29T01:55:18.318Z",
          "wordCount": 658,
          "title": "Two-stage framework for short-term wind power forecasting using different feature-learning models. (arXiv:2006.00413v2 [eess.SP] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.08239",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Liu_Z/0/1/0/all/0/1\">Zixuan Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Adeli_E/0/1/0/all/0/1\">Ehsan Adeli</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Pohl_K/0/1/0/all/0/1\">Kilian M. Pohl</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhao_Q/0/1/0/all/0/1\">Qingyu Zhao</a>",
          "description": "Interpretability is a critical factor in applying complex deep learning\nmodels to advance the understanding of brain disorders in neuroimaging studies.\nTo interpret the decision process of a trained classifier, existing techniques\ntypically rely on saliency maps to quantify the voxel-wise or feature-level\nimportance for classification through partial derivatives. Despite providing\nsome level of localization, these maps are not human-understandable from the\nneuroscience perspective as they do not inform the specific meaning of the\nalteration linked to the brain disorder. Inspired by the image-to-image\ntranslation scheme, we propose to train simulator networks that can warp a\ngiven image to inject or remove patterns of the disease. These networks are\ntrained such that the classifier produces consistently increased or decreased\nprediction logits for the simulated images. Moreover, we propose to couple all\nthe simulators into a unified model based on conditional convolution. We\napplied our approach to interpreting classifiers trained on a synthetic dataset\nand two neuroimaging datasets to visualize the effect of the Alzheimer's\ndisease and alcohol use disorder. Compared to the saliency maps generated by\nbaseline approaches, our simulations and visualizations based on the Jacobian\ndeterminants of the warping field reveal meaningful and understandable patterns\nrelated to the diseases.",
          "link": "http://arxiv.org/abs/2102.08239",
          "publishedOn": "2021-06-29T01:55:18.305Z",
          "wordCount": 673,
          "title": "Going Beyond Saliency Maps: Training Deep Models to Interpret Deep Models. (arXiv:2102.08239v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.06107",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Brennan_M/0/1/0/all/0/1\">Matthew Brennan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bresler_G/0/1/0/all/0/1\">Guy Bresler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hopkins_S/0/1/0/all/0/1\">Samuel B. Hopkins</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jerry Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schramm_T/0/1/0/all/0/1\">Tselil Schramm</a>",
          "description": "Researchers currently use a number of approaches to predict and substantiate\ninformation-computation gaps in high-dimensional statistical estimation\nproblems. A prominent approach is to characterize the limits of restricted\nmodels of computation, which on the one hand yields strong computational lower\nbounds for powerful classes of algorithms and on the other hand helps guide the\ndevelopment of efficient algorithms. In this paper, we study two of the most\npopular restricted computational models, the statistical query framework and\nlow-degree polynomials, in the context of high-dimensional hypothesis testing.\nOur main result is that under mild conditions on the testing problem, the two\nclasses of algorithms are essentially equivalent in power. As corollaries, we\nobtain new statistical query lower bounds for sparse PCA, tensor PCA and\nseveral variants of the planted clique problem.",
          "link": "http://arxiv.org/abs/2009.06107",
          "publishedOn": "2021-06-29T01:55:18.298Z",
          "wordCount": 639,
          "title": "Statistical Query Algorithms and Low-Degree Tests Are Almost Equivalent. (arXiv:2009.06107v3 [cs.CC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14338",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tranos_D/0/1/0/all/0/1\">Damianos Tranos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Proutiere_A/0/1/0/all/0/1\">Alexandre Proutiere</a>",
          "description": "We consider Markov Decision Processes (MDPs) with deterministic transitions\nand study the problem of regret minimization, which is central to the analysis\nand design of optimal learning algorithms. We present logarithmic\nproblem-specific regret lower bounds that explicitly depend on the system\nparameter (in contrast to previous minimax approaches) and thus, truly quantify\nthe fundamental limit of performance achievable by any learning algorithm.\nDeterministic MDPs can be interpreted as graphs and analyzed in terms of their\ncycles, a fact which we leverage in order to identify a class of deterministic\nMDPs whose regret lower bound can be determined numerically. We further\nexemplify this result on a deterministic line search problem, and a\ndeterministic MDP with state-dependent rewards, whose regret lower bounds we\ncan state explicitly. These bounds share similarities with the known\nproblem-specific bound of the multi-armed bandit problem and suggest that\nnavigation on a deterministic MDP need not have an effect on the performance of\na learning algorithm.",
          "link": "http://arxiv.org/abs/2106.14338",
          "publishedOn": "2021-06-29T01:55:18.023Z",
          "wordCount": 585,
          "title": "Regret Analysis in Deterministic Reinforcement Learning. (arXiv:2106.14338v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13914",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jiawei Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_S/0/1/0/all/0/1\">Steve Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venkatesan_R/0/1/0/all/0/1\">Rangharajan Venkatesan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1\">Ming-Yu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khailany_B/0/1/0/all/0/1\">Brucek Khailany</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dally_B/0/1/0/all/0/1\">Bill Dally</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1\">Anima Anandkumar</a>",
          "description": "Training large-scale deep neural networks (DNNs) currently requires a\nsignificant amount of energy, leading to serious environmental impacts. One\npromising approach to reduce the energy costs is representing DNNs with\nlow-precision numbers. While it is common to train DNNs with forward and\nbackward propagation in low-precision, training directly over low-precision\nweights, without keeping a copy of weights in high-precision, still remains to\nbe an unsolved problem. This is due to complex interactions between learning\nalgorithms and low-precision number systems. To address this, we jointly design\na low-precision training framework involving a logarithmic number system (LNS)\nand a multiplicative weight update training method, termed LNS-Madam. LNS has a\nhigh dynamic range even in a low-bitwidth setting, leading to high energy\nefficiency and making it relevant for on-board training in energy-constrained\nedge devices. We design LNS to have the flexibility of choosing different bases\nfor weights and gradients, as they usually require different quantization gaps\nand dynamic ranges during training. By drawing the connection between LNS and\nmultiplicative update, LNS-Madam ensures low quantization error during weight\nupdate, leading to a stable convergence even if the bitwidth is limited.\nCompared to using a fixed-point or floating-point number system and training\nwith popular learning algorithms such as SGD and Adam, our joint design with\nLNS and LNS-Madam optimizer achieves better accuracy while requiring smaller\nbitwidth. Notably, with only 5-bit for gradients, the proposed training\nframework achieves accuracy comparable to full-precision state-of-the-art\nmodels such as ResNet-50 and BERT. After conducting energy estimations by\nanalyzing the math datapath units during training, the results show that our\ndesign achieves over 60x energy reduction compared to FP32 on BERT models.",
          "link": "http://arxiv.org/abs/2106.13914",
          "publishedOn": "2021-06-29T01:55:18.010Z",
          "wordCount": 716,
          "title": "Low-Precision Training in Logarithmic Number System using Multiplicative Weight Update. (arXiv:2106.13914v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14326",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1\">Chung-Wei Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kroer_C/0/1/0/all/0/1\">Christian Kroer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1\">Haipeng Luo</a>",
          "description": "Regret-based algorithms are highly efficient at finding approximate Nash\nequilibria in sequential games such as poker games. However, most regret-based\nalgorithms, including counterfactual regret minimization (CFR) and its\nvariants, rely on iterate averaging to achieve convergence. Inspired by recent\nadvances on last-iterate convergence of optimistic algorithms in zero-sum\nnormal-form games, we study this phenomenon in sequential games, and provide a\ncomprehensive study of last-iterate convergence for zero-sum extensive-form\ngames with perfect recall (EFGs), using various optimistic regret-minimization\nalgorithms over treeplexes. This includes algorithms using the vanilla entropy\nor squared Euclidean norm regularizers, as well as their dilated versions which\nadmit more efficient implementation. In contrast to CFR, we show that all of\nthese algorithms enjoy last-iterate convergence, with some of them even\nconverging exponentially fast. We also provide experiments to further support\nour theoretical results.",
          "link": "http://arxiv.org/abs/2106.14326",
          "publishedOn": "2021-06-29T01:55:17.976Z",
          "wordCount": 557,
          "title": "Last-iterate Convergence in Extensive-Form Games. (arXiv:2106.14326v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2011.12026",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Skorokhodov_I/0/1/0/all/0/1\">Ivan Skorokhodov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ignatyev_S/0/1/0/all/0/1\">Savva Ignatyev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elhoseiny_M/0/1/0/all/0/1\">Mohamed Elhoseiny</a>",
          "description": "In most existing learning systems, images are typically viewed as 2D pixel\narrays. However, in another paradigm gaining popularity, a 2D image is\nrepresented as an implicit neural representation (INR) - an MLP that predicts\nan RGB pixel value given its (x,y) coordinate. In this paper, we propose two\nnovel architectural techniques for building INR-based image decoders:\nfactorized multiplicative modulation and multi-scale INRs, and use them to\nbuild a state-of-the-art continuous image GAN. Previous attempts to adapt INRs\nfor image generation were limited to MNIST-like datasets and do not scale to\ncomplex real-world data. Our proposed INR-GAN architecture improves the\nperformance of continuous image generators by several times, greatly reducing\nthe gap between continuous image GANs and pixel-based ones. Apart from that, we\nexplore several exciting properties of the INR-based decoders, like\nout-of-the-box superresolution, meaningful image-space interpolation,\naccelerated inference of low-resolution images, an ability to extrapolate\noutside of image boundaries, and strong geometric prior. The project page is\nlocated at https://universome.github.io/inr-gan.",
          "link": "http://arxiv.org/abs/2011.12026",
          "publishedOn": "2021-06-29T01:55:17.967Z",
          "wordCount": 629,
          "title": "Adversarial Generation of Continuous Images. (arXiv:2011.12026v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14120",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rubinstein_B/0/1/0/all/0/1\">Boris Rubinstein</a>",
          "description": "Neural networks mapping sequences to sequences (seq2seq) lead to significant\nprogress in machine translation and speech recognition. Their traditional\narchitecture includes two recurrent networks (RNs) followed by a linear\npredictor. In this manuscript we perform analysis of a corresponding algorithm\nand show that the parameters of the RNs of the well trained predictive network\nare not independent of each other. Their dependence can be used to\nsignificantly improve the network effectiveness. The traditional seq2seq\nalgorithms require short term memory of a size proportional to the predicted\nsequence length. This requirement is quite difficult to implement in a\nneuroscience context. We present a novel memoryless algorithm for seq2seq\npredictive networks and compare it to the traditional one in the context of\ntime series prediction. We show that the new algorithm is more robust and makes\npredictions with higher accuracy than the traditional one.",
          "link": "http://arxiv.org/abs/2106.14120",
          "publishedOn": "2021-06-29T01:55:17.961Z",
          "wordCount": 578,
          "title": "On a novel training algorithm for sequence-to-sequence predictive recurrent networks. (arXiv:2106.14120v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14365",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Arseniev_Koehler_A/0/1/0/all/0/1\">Alina Arseniev-Koehler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cochran_S/0/1/0/all/0/1\">Susan D. Cochran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mays_V/0/1/0/all/0/1\">Vickie M. Mays</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foster_J/0/1/0/all/0/1\">Jacob Gates Foster</a>",
          "description": "There is an escalating need for methods to identify latent patterns in text\ndata from many domains. We introduce a new method to identify topics in a\ncorpus and represent documents as topic sequences. Discourse Atom Topic\nModeling draws on advances in theoretical machine learning to integrate topic\nmodeling and word embedding, capitalizing on the distinct capabilities of each.\nWe first identify a set of vectors (\"discourse atoms\") that provide a sparse\nrepresentation of an embedding space. Atom vectors can be interpreted as latent\ntopics: Through a generative model, atoms map onto distributions over words;\none can also infer the topic that generated a sequence of words. We illustrate\nour method with a prominent example of underutilized text: the U.S. National\nViolent Death Reporting System (NVDRS). The NVDRS summarizes violent death\nincidents with structured variables and unstructured narratives. We identify\n225 latent topics in the narratives (e.g., preparation for death and physical\naggression); many of these topics are not captured by existing structured\nvariables. Motivated by known patterns in suicide and homicide by gender, and\nrecent research on gender biases in semantic space, we identify the gender bias\nof our topics (e.g., a topic about pain medication is feminine). We then\ncompare the gender bias of topics to their prevalence in narratives of female\nversus male victims. Results provide a detailed quantitative picture of\nreporting about lethal violence and its gendered nature. Our method offers a\nflexible and broadly applicable approach to model topics in text data.",
          "link": "http://arxiv.org/abs/2106.14365",
          "publishedOn": "2021-06-29T01:55:17.956Z",
          "wordCount": 698,
          "title": "Integrating topic modeling and word embedding to characterize violent deaths. (arXiv:2106.14365v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2010.01155",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Koehler_F/0/1/0/all/0/1\">Frederic Koehler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mehta_V/0/1/0/all/0/1\">Viraj Mehta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Risteski_A/0/1/0/all/0/1\">Andrej Risteski</a>",
          "description": "Normalizing flows are among the most popular paradigms in generative\nmodeling, especially for images, primarily because we can efficiently evaluate\nthe likelihood of a data point. This is desirable both for evaluating the fit\nof a model, and for ease of training, as maximizing the likelihood can be done\nby gradient descent. However, training normalizing flows comes with\ndifficulties as well: models which produce good samples typically need to be\nextremely deep -- which comes with accompanying vanishing/exploding gradient\nproblems. A very related problem is that they are often poorly conditioned:\nsince they are parametrized as invertible maps from $\\mathbb{R}^d \\to\n\\mathbb{R}^d$, and typical training data like images intuitively is\nlower-dimensional, the learned maps often have Jacobians that are close to\nbeing singular.\n\nIn our paper, we tackle representational aspects around depth and\nconditioning of normalizing flows: both for general invertible architectures,\nand for a particular common architecture, affine couplings. We prove that\n$\\Theta(1)$ affine coupling layers suffice to exactly represent a permutation\nor $1 \\times 1$ convolution, as used in GLOW, showing that representationally\nthe choice of partition is not a bottleneck for depth. We also show that\nshallow affine coupling networks are universal approximators in Wasserstein\ndistance if ill-conditioning is allowed, and experimentally investigate related\nphenomena involving padding. Finally, we show a depth lower bound for general\nflow architectures with few neurons per layer and bounded Lipschitz constant.",
          "link": "http://arxiv.org/abs/2010.01155",
          "publishedOn": "2021-06-29T01:55:17.950Z",
          "wordCount": 698,
          "title": "Representational aspects of depth and conditioning in normalizing flows. (arXiv:2010.01155v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.04053",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Meirman_T/0/1/0/all/0/1\">Tomer Meirman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stern_R/0/1/0/all/0/1\">Roni Stern</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katz_G/0/1/0/all/0/1\">Gilad Katz</a>",
          "description": "In data systems, activities or events are continuously collected in the field\nto trace their proper executions. Logging, which means recording sequences of\nevents, can be used for analyzing system failures and malfunctions, and\nidentifying the causes and locations of such issues. In our research we focus\non creating an Anomaly detection models for system logs. The task of anomaly\ndetection is identifying unexpected events in dataset, which differ from the\nnormal behavior. Anomaly detection models also assist in data systems analysis\ntasks.\n\nModern systems may produce such a large amount of events monitoring every\nindividual event is not feasible. In such cases, the events are often\naggregated over a fixed period of time, reporting the number of times every\nevent has occurred in that time period. This aggregation facilitates scaling,\nbut requires a different approach for anomaly detection. In this research, we\npresent a thorough analysis of the aggregated data and the relationships\nbetween aggregated events. Based on the initial phase of our research we\npresent graphs representations of our aggregated dataset, which represent the\ndifferent relationships between aggregated instances in the same context.\n\nUsing the graph representation, we propose Multiple-graphs autoencoder MGAE,\na novel convolutional graphs-autoencoder model which exploits the relationships\nof the aggregated instances in our unique dataset. MGAE outperforms standard\ngraph-autoencoder models and the different experiments. With our novel MGAE we\npresent 60% decrease in reconstruction error in comparison to standard graph\nautoencoder, which is expressed in reconstructing high-degree relationships.",
          "link": "http://arxiv.org/abs/2101.04053",
          "publishedOn": "2021-06-29T01:55:17.935Z",
          "wordCount": 738,
          "title": "Anomaly Detection for Aggregated Data Using Multi-Graph Autoencoder. (arXiv:2101.04053v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14045",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Ning_N/0/1/0/all/0/1\">Ning Ning</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Qiu_J/0/1/0/all/0/1\">Jinwen Qiu</a>",
          "description": "The multivariate Bayesian structural time series (MBSTS) model\n\\citep{qiu2018multivariate,Jammalamadaka2019Predicting} as a generalized\nversion of many structural time series models, deals with inference and\nprediction for multiple correlated time series, where one also has the choice\nof using a different candidate pool of contemporaneous predictors for each\ntarget series. The MBSTS model has wide applications and is ideal for feature\nselection, time series forecasting, nowcasting, inferring causal impact, and\nothers. This paper demonstrates how to use the R package \\pkg{mbsts} for MBSTS\nmodeling, establishing a bridge between user-friendly and developer-friendly\nfunctions in package and the corresponding methodology. A simulated dataset and\nobject-oriented functions in the \\pkg{mbsts} package are explained in the way\nthat enables users to flexibly add or deduct some components, as well as to\nsimplify or complicate some settings.",
          "link": "http://arxiv.org/abs/2106.14045",
          "publishedOn": "2021-06-29T01:55:17.930Z",
          "wordCount": 574,
          "title": "The mbsts package: Multivariate Bayesian Structural Time Series Models in R. (arXiv:2106.14045v1 [stat.ME])"
        },
        {
          "id": "http://arxiv.org/abs/2012.03074",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Meyer_A/0/1/0/all/0/1\">Angela Meyer</a>",
          "description": "The trend towards larger wind turbines and remote locations of wind farms\nfuels the demand for automated condition monitoring strategies that can reduce\nthe operating cost and avoid unplanned downtime. Normal behaviour modelling has\nbeen introduced to detect anomalous deviations from normal operation based on\nthe turbine's SCADA data. A growing number of machine learning models of the\nnormal behaviour of turbine subsystems are being developed by wind farm\nmanagers to this end. However, these models need to be kept track of, be\nmaintained and require frequent updates. This research explores multi-target\nmodels as a new approach to capturing a wind turbine's normal behaviour. We\npresent an overview of multi-target regression methods, motivate their\napplication and benefits in wind turbine condition monitoring, and assess their\nperformance in a wind farm case study. We find that multi-target models are\nadvantageous in comparison to single-target modelling in that they can reduce\nthe cost and effort of practical condition monitoring without compromising on\nthe accuracy. We also outline some areas of future research.",
          "link": "http://arxiv.org/abs/2012.03074",
          "publishedOn": "2021-06-29T01:55:17.925Z",
          "wordCount": 637,
          "title": "Multi-target normal behaviour models for wind farm condition monitoring. (arXiv:2012.03074v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.16955",
          "author": "<a href=\"http://arxiv.org/find/q-bio/1/au:+Cieplinski_T/0/1/0/all/0/1\">Tobiasz Cieplinski</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Danel_T/0/1/0/all/0/1\">Tomasz Danel</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Podlewska_S/0/1/0/all/0/1\">Sabina Podlewska</a>, <a href=\"http://arxiv.org/find/q-bio/1/au:+Jastrzebski_S/0/1/0/all/0/1\">Stanislaw Jastrzebski</a>",
          "description": "Designing compounds with desired properties is a key element of the drug\ndiscovery process. However, measuring progress in the field has been\nchallenging due to the lack of realistic retrospective benchmarks, and the\nlarge cost of prospective validation. To close this gap, we propose a benchmark\nbased on docking, a popular computational method for assessing molecule binding\nto a protein. Concretely, the goal is to generate drug-like molecules that are\nscored highly by SMINA, a popular docking software. We observe that popular\ngraph-based generative models fail to generate molecules with a high docking\nscore when trained using a realistically sized training set. This suggests a\nlimitation of the current incarnation of models for de novo drug design.\nFinally, we propose a simplified version of the benchmark based on a simpler\nscoring function, and show that the tested models are able to partially solve\nit. We release the benchmark as an easy to use package available at\nhttps://github.com/cieplinski-tobiasz/smina-docking-benchmark. We hope that our\nbenchmark will serve as a stepping stone towards the goal of automatically\ngenerating promising drug candidates.",
          "link": "http://arxiv.org/abs/2006.16955",
          "publishedOn": "2021-06-29T01:55:17.919Z",
          "wordCount": 660,
          "title": "We Should at Least Be Able to Design Molecules That Dock Well. (arXiv:2006.16955v4 [q-bio.BM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13871",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Valle_Perez_G/0/1/0/all/0/1\">Guillermo Valle-P&#xe9;rez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henter_G/0/1/0/all/0/1\">Gustav Eje Henter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beskow_J/0/1/0/all/0/1\">Jonas Beskow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holzapfel_A/0/1/0/all/0/1\">Andr&#xe9; Holzapfel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oudeyer_P/0/1/0/all/0/1\">Pierre-Yves Oudeyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alexanderson_S/0/1/0/all/0/1\">Simon Alexanderson</a>",
          "description": "Dance requires skillful composition of complex movements that follow\nrhythmic, tonal and timbral features of music. Formally, generating dance\nconditioned on a piece of music can be expressed as a problem of modelling a\nhigh-dimensional continuous motion signal, conditioned on an audio signal. In\nthis work we make two contributions to tackle this problem. First, we present a\nnovel probabilistic autoregressive architecture that models the distribution\nover future poses with a normalizing flow conditioned on previous poses as well\nas music context, using a multimodal transformer encoder. Second, we introduce\nthe currently largest 3D dance-motion dataset, obtained with a variety of\nmotion-capture technologies, and including both professional and casual\ndancers. Using this dataset, we compare our new model against two baselines,\nvia objective metrics and a user study, and show that both the ability to model\na probability distribution, as well as being able to attend over a large motion\nand music context are necessary to produce interesting, diverse, and realistic\ndance that matches the music.",
          "link": "http://arxiv.org/abs/2106.13871",
          "publishedOn": "2021-06-29T01:55:17.913Z",
          "wordCount": 612,
          "title": "Transflower: probabilistic autoregressive dance generation with multimodal attention. (arXiv:2106.13871v1 [cs.SD])"
        },
        {
          "id": "http://arxiv.org/abs/2102.03432",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Noack_M/0/1/0/all/0/1\">Marcus M. Noack</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Sethian_J/0/1/0/all/0/1\">James A. Sethian</a>",
          "description": "Gaussian process regression is a widely-applied method for function\napproximation and uncertainty quantification. The technique has gained\npopularity recently in the machine learning community due to its robustness and\ninterpretability. The mathematical methods we discuss in this paper are an\nextension of the Gaussian-process framework. We are proposing advanced kernel\ndesigns that only allow for functions with certain desirable characteristics to\nbe elements of the reproducing kernel Hilbert space (RKHS) that underlies all\nkernel methods and serves as the sample space for Gaussian process regression.\nThese desirable characteristics reflect the underlying physics; two obvious\nexamples are symmetry and periodicity constraints. In addition, non-stationary\nkernel designs can be defined in the same framework to yield flexible\nmulti-task Gaussian processes. We will show the impact of advanced kernel\ndesigns on Gaussian processes using several synthetic and two scientific data\nsets. The results show that including domain knowledge, communicated through\nadvanced kernel designs, has a significant impact on the accuracy and relevance\nof the function approximation.",
          "link": "http://arxiv.org/abs/2102.03432",
          "publishedOn": "2021-06-29T01:55:17.898Z",
          "wordCount": 616,
          "title": "Advanced Stationary and Non-Stationary Kernel Designs for Domain-Aware Gaussian Processes. (arXiv:2102.03432v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2007.08926",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Abadi_M/0/1/0/all/0/1\">Martin Abadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plotkin_G/0/1/0/all/0/1\">Gordon Plotkin</a>",
          "description": "Describing systems in terms of choices and their resulting costs and rewards\noffers the promise of freeing algorithm designers and programmers from\nspecifying how those choices should be made; in implementations, the choices\ncan be realized by optimization techniques and,increasingly, by\nmachine-learning methods. We study this approach from a programming-language\nperspective. We define two small languages that support decision-making\nabstractions: one with choices and rewards, and the other additionally with\nprobabilities. We give both operational and denotational semantics.\n\nIn the case of the second language we consider three denotational semantics,\nwith varying degrees of correlation between possible program values and\nexpected rewards. The operational semantics combine the usual semantics of\nstandard constructs with optimization over spaces of possible execution\nstrategies. The denotational semantics, which are compositional rely on the\nselection monad, to handle choice, augmented with an auxiliary monad to handle\nother effects, such as rewards or probability.\n\nWe establish adequacy theorems that the two semantics coincide in all cases.\nWe also prove full abstraction at base types, with varying notions of\nobservation in the probabilistic case corresponding to the various degrees of\ncorrelation. We present axioms for choice combined with rewards and\nprobability, establishing completeness at base types for the case of rewards\nwithout probability.",
          "link": "http://arxiv.org/abs/2007.08926",
          "publishedOn": "2021-06-29T01:55:17.892Z",
          "wordCount": 701,
          "title": "Smart Choices and the Selection Monad. (arXiv:2007.08926v5 [cs.LO] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.16318",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wan_Y/0/1/0/all/0/1\">Yi Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naik_A/0/1/0/all/0/1\">Abhishek Naik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sutton_R/0/1/0/all/0/1\">Richard S. Sutton</a>",
          "description": "We introduce learning and planning algorithms for average-reward MDPs,\nincluding 1) the first general proven-convergent off-policy model-free control\nalgorithm without reference states, 2) the first proven-convergent off-policy\nmodel-free prediction algorithm, and 3) the first off-policy learning algorithm\nthat converges to the actual value function rather than to the value function\nplus an offset. All of our algorithms are based on using the\ntemporal-difference error rather than the conventional error when updating the\nestimate of the average reward. Our proof techniques are a slight\ngeneralization of those by Abounadi, Bertsekas, and Borkar (2001). In\nexperiments with an Access-Control Queuing Task, we show some of the\ndifficulties that can arise when using methods that rely on reference states\nand argue that our new algorithms can be significantly easier to use.",
          "link": "http://arxiv.org/abs/2006.16318",
          "publishedOn": "2021-06-29T01:55:17.886Z",
          "wordCount": 605,
          "title": "Learning and Planning in Average-Reward Markov Decision Processes. (arXiv:2006.16318v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2101.08925",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Wang_P/0/1/0/all/0/1\">Puyu Wang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lei_Y/0/1/0/all/0/1\">Yunwen Lei</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ying_Y/0/1/0/all/0/1\">Yiming Ying</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zhang_H/0/1/0/all/0/1\">Hai Zhang</a>",
          "description": "In this paper, we are concerned with differentially private {stochastic\ngradient descent (SGD)} algorithms in the setting of stochastic convex\noptimization (SCO). Most of the existing work requires the loss to be Lipschitz\ncontinuous and strongly smooth, and the model parameter to be uniformly\nbounded. However, these assumptions are restrictive as many popular losses\nviolate these conditions including the hinge loss for SVM, the absolute loss in\nrobust regression, and even the least square loss in an unbounded domain. We\nsignificantly relax these restrictive assumptions and establish privacy and\ngeneralization (utility) guarantees for private SGD algorithms using output and\ngradient perturbations associated with non-smooth convex losses. Specifically,\nthe loss function is relaxed to have an $\\alpha$-H\\\"{o}lder continuous gradient\n(referred to as $\\alpha$-H\\\"{o}lder smoothness) which instantiates the\nLipschitz continuity ($\\alpha=0$) and the strong smoothness ($\\alpha=1$). We\nprove that noisy SGD with $\\alpha$-H\\\"older smooth losses using gradient\nperturbation can guarantee $(\\epsilon,\\delta)$-differential privacy (DP) and\nattain optimal excess population risk\n$\\mathcal{O}\\Big(\\frac{\\sqrt{d\\log(1/\\delta)}}{n\\epsilon}+\\frac{1}{\\sqrt{n}}\\Big)$,\nup to logarithmic terms, with the gradient complexity $ \\mathcal{O}(\nn^{2-\\alpha\\over 1+\\alpha}+ n).$ This shows an important trade-off between\n$\\alpha$-H\\\"older smoothness of the loss and the computational complexity for\nprivate SGD with statistically optimal performance. In particular, our results\nindicate that $\\alpha$-H\\\"older smoothness with $\\alpha\\ge {1/2}$ is sufficient\nto guarantee $(\\epsilon,\\delta)$-DP of noisy SGD algorithms while achieving\noptimal excess risk with the linear gradient complexity $\\mathcal{O}(n).$",
          "link": "http://arxiv.org/abs/2101.08925",
          "publishedOn": "2021-06-29T01:55:17.881Z",
          "wordCount": 680,
          "title": "Differentially Private SGD with Non-Smooth Losses. (arXiv:2101.08925v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2008.11193",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Feldman_V/0/1/0/all/0/1\">Vitaly Feldman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zrnic_T/0/1/0/all/0/1\">Tijana Zrnic</a>",
          "description": "We consider a sequential setting in which a single dataset of individuals is\nused to perform adaptively-chosen analyses, while ensuring that the\ndifferential privacy loss of each participant does not exceed a pre-specified\nprivacy budget. The standard approach to this problem relies on bounding a\nworst-case estimate of the privacy loss over all individuals and all possible\nvalues of their data, for every single analysis. Yet, in many scenarios this\napproach is overly conservative, especially for \"typical\" data points which\nincur little privacy loss by participation in most of the analyses. In this\nwork, we give a method for tighter privacy loss accounting based on the value\nof a personalized privacy loss estimate for each individual in each analysis.\nTo implement the accounting method we design a filter for R\\'enyi differential\nprivacy. A filter is a tool that ensures that the privacy parameter of a\ncomposed sequence of algorithms with adaptively-chosen privacy parameters does\nnot exceed a pre-specified budget. Our filter is simpler and tighter than the\nknown filter for $(\\epsilon,\\delta)$-differential privacy by Rogers et al. We\napply our results to the analysis of noisy gradient descent and show that\npersonalized accounting can be practical, easy to implement, and can only make\nthe privacy-utility tradeoff tighter.",
          "link": "http://arxiv.org/abs/2008.11193",
          "publishedOn": "2021-06-29T01:55:17.873Z",
          "wordCount": 677,
          "title": "Individual Privacy Accounting via a Renyi Filter. (arXiv:2008.11193v3 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13823",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Shangnan_Z/0/1/0/all/0/1\">Zhou Shangnan</a>",
          "description": "Quantum machine learning is an emerging field at the intersection of machine\nlearning and quantum computing. A central quantity for the theoretical\nfoundation of quantum machine learning is the quantum cross entropy. In this\npaper, we present one operational interpretation of this quantity, that the\nquantum cross entropy is the compression rate for sub-optimal quantum source\ncoding. To do so, we give a simple, universal quantum data compression\nprotocol, which is developed based on quantum generalization of variable-length\ncoding, as well as quantum strong typicality.",
          "link": "http://arxiv.org/abs/2106.13823",
          "publishedOn": "2021-06-29T01:55:17.858Z",
          "wordCount": 532,
          "title": "Quantum Data Compression and Quantum Cross Entropy. (arXiv:2106.13823v1 [quant-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14269",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1\">Shuo Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1\">Jianxi Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jie Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Magee_C/0/1/0/all/0/1\">Christopher L. Magee</a>",
          "description": "In large technology companies, the requirements for managing and organizing\ntechnical documents created by engineers and managers in supporting relevant\ndecision making have increased dramatically in recent years, which has led to a\nhigher demand for more scalable, accurate, and automated document\nclassification. Prior studies have primarily focused on processing text for\nclassification and small-scale databases. This paper describes a novel\nmultimodal deep learning architecture, called TechDoc, for technical document\nclassification, which utilizes both natural language and descriptive images to\ntrain hierarchical classifiers. The architecture synthesizes convolutional\nneural networks and recurrent neural networks through an integrated training\nprocess. We applied the architecture to a large multimodal technical document\ndatabase and trained the model for classifying documents based on the\nhierarchical International Patent Classification system. Our results show that\nthe trained neural network presents a greater classification accuracy than\nthose using a single modality and several earlier text classification methods.\nThe trained model can potentially be scaled to millions of real-world technical\ndocuments with both text and figures, which is useful for data and knowledge\nmanagement in large technology companies and organizations.",
          "link": "http://arxiv.org/abs/2106.14269",
          "publishedOn": "2021-06-29T01:55:17.851Z",
          "wordCount": 625,
          "title": "Deep Learning for Technical Document Classification. (arXiv:2106.14269v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2011.10510",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Nasim_M/0/1/0/all/0/1\">M Quamer Nasim</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Maiti_T/0/1/0/all/0/1\">Tannistha Maiti</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Srivastava_A/0/1/0/all/0/1\">Ayush Srivastava</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Singh_T/0/1/0/all/0/1\">Tarry Singh</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Mei_J/0/1/0/all/0/1\">Jie Mei</a>",
          "description": "Deep neural networks (DNNs) can learn accurately from large quantities of\nlabeled input data, but DNNs sometimes fail to generalize to test data sampled\nfrom different input distributions. Unsupervised Deep Domain Adaptation (DDA)\nproves useful when no input labels are available, and distribution shifts are\nobserved in the target domain (TD). Experiments are performed on seismic images\nof the F3 block 3D dataset from offshore Netherlands (source domain; SD) and\nPenobscot 3D survey data from Canada (target domain; TD). Three geological\nclasses from SD and TD that have similar reflection patterns are considered. In\nthe present study, an improved deep neural network architecture named\nEarthAdaptNet (EAN) is proposed to semantically segment the seismic images. We\nspecifically use a transposed residual unit to replace the traditional dilated\nconvolution in the decoder block. The EAN achieved a pixel-level accuracy >84%\nand an accuracy of ~70% for the minority classes, showing improved performance\ncompared to existing architectures. In addition, we introduced the CORAL\n(Correlation Alignment) method to the EAN to create an unsupervised deep domain\nadaptation network (EAN-DDA) for the classification of seismic reflections\nfromF3 and Penobscot. Maximum class accuracy achieved was ~99% for class 2 of\nPenobscot with >50% overall accuracy. Taken together, EAN-DDA has the potential\nto classify target domain seismic facies classes with high accuracy.",
          "link": "http://arxiv.org/abs/2011.10510",
          "publishedOn": "2021-06-29T01:55:17.845Z",
          "wordCount": 701,
          "title": "Seismic Facies Analysis: A Deep Domain Adaptation Approach. (arXiv:2011.10510v2 [physics.geo-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14342",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bai_S/0/1/0/all/0/1\">Shaojie Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koltun_V/0/1/0/all/0/1\">Vladlen Koltun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kolter_J/0/1/0/all/0/1\">J. Zico Kolter</a>",
          "description": "Deep equilibrium networks (DEQs) are a new class of models that eschews\ntraditional depth in favor of finding the fixed point of a single nonlinear\nlayer. These models have been shown to achieve performance competitive with the\nstate-of-the-art deep networks while using significantly less memory. Yet they\nare also slower, brittle to architectural choices, and introduce potential\ninstability to the model. In this paper, we propose a regularization scheme for\nDEQ models that explicitly regularizes the Jacobian of the fixed-point update\nequations to stabilize the learning of equilibrium models. We show that this\nregularization adds only minimal computational cost, significantly stabilizes\nthe fixed-point convergence in both forward and backward passes, and scales\nwell to high-dimensional, realistic domains (e.g., WikiText-103 language\nmodeling and ImageNet classification). Using this method, we demonstrate, for\nthe first time, an implicit-depth model that runs with approximately the same\nspeed and level of performance as popular conventional deep networks such as\nResNet-101, while still maintaining the constant memory footprint and\narchitectural simplicity of DEQs. Code is available at\nhttps://github.com/locuslab/deq .",
          "link": "http://arxiv.org/abs/2106.14342",
          "publishedOn": "2021-06-29T01:55:17.829Z",
          "wordCount": 610,
          "title": "Stabilizing Equilibrium Models by Jacobian Regularization. (arXiv:2106.14342v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2012.02976",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yuezhang_L/0/1/0/all/0/1\">Liu Yuezhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qifeng Chen</a>",
          "description": "It is well known that artificial neural networks are vulnerable to\nadversarial examples, in which great efforts have been made to improve the\nrobustness. However, such examples are usually imperceptible to humans, and\nthus their effect on biological neural circuits is largely unknown. This paper\nwill investigate the adversarial robustness in a simulated cerebellum, a\nwell-studied supervised learning system in computational neuroscience.\nSpecifically, we propose to study three unique characteristics revealed in the\ncerebellum: (i) network width; (ii) long-term depression on the parallel\nfiber-Purkinje cell synapses; (iii) sparse connectivity in the granule layer,\nand hypothesize that they will be beneficial for improving robustness. To the\nbest of our knowledge, this is the first attempt to examine the adversarial\nrobustness in simulated cerebellum models.\n\nThe results are negative in the experimental phase -- no significant\nimprovements in robustness are discovered from the proposed three mechanisms.\nConsequently, the cerebellum is expected to be vulnerable to adversarial\nexamples as the deep neural networks under batch training. Neuroscientists are\nencouraged to fool the biological system in experiments with adversarial\nattacks.",
          "link": "http://arxiv.org/abs/2012.02976",
          "publishedOn": "2021-06-29T01:55:17.807Z",
          "wordCount": 645,
          "title": "Evaluating adversarial robustness in simulated cerebellum. (arXiv:2012.02976v2 [cs.NE] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13955",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ashfahani_A/0/1/0/all/0/1\">Andri Ashfahani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pratama_M/0/1/0/all/0/1\">Mahardhika Pratama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lughofer_E/0/1/0/all/0/1\">Edwin Lughofer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yee_E/0/1/0/all/0/1\">Edward Yapp Kien Yee</a>",
          "description": "The common practice of quality monitoring in industry relies on manual\ninspection well-known to be slow, error-prone and operator-dependent. This\nissue raises strong demand for automated real-time quality monitoring developed\nfrom data-driven approaches thus alleviating from operator dependence and\nadapting to various process uncertainties. Nonetheless, current approaches do\nnot take into account the streaming nature of sensory information while relying\nheavily on hand-crafted features making them application-specific. This paper\nproposes the online quality monitoring methodology developed from recently\ndeveloped deep learning algorithms for data streams, Neural Networks with\nDynamically Evolved Capacity (NADINE), namely NADINE++. It features the\nintegration of 1-D and 2-D convolutional layers to extract natural features of\ntime-series and visual data streams captured from sensors and cameras of the\ninjection molding machines from our own project. Real-time experiments have\nbeen conducted where the online quality monitoring task is simulated on the fly\nunder the prequential test-then-train fashion - the prominent data stream\nevaluation protocol. Comparison with the state-of-the-art techniques clearly\nexhibits the advantage of NADINE++ with 4.68\\% improvement on average for the\nquality monitoring task in streaming environments. To support the reproducible\nresearch initiative, codes, results of NADINE++ along with supplementary\nmaterials and injection molding dataset are made available in\n\\url{https://github.com/ContinualAL/NADINE-IJCNN2021}.",
          "link": "http://arxiv.org/abs/2106.13955",
          "publishedOn": "2021-06-29T01:55:17.801Z",
          "wordCount": 659,
          "title": "Autonomous Deep Quality Monitoring in Streaming Environments. (arXiv:2106.13955v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14043",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vakilian_A/0/1/0/all/0/1\">Ali Vakilian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yalciner_M/0/1/0/all/0/1\">Mustafa Yal&#xe7;&#x131;ner</a>",
          "description": "We consider the $k$-clustering problem with $\\ell_p$-norm cost, which\nincludes $k$-median, $k$-means and $k$-center cost functions, under an\nindividual notion of fairness proposed by Jung et al. [2020]: given a set of\npoints $P$ of size $n$, a set of $k$ centers induces a fair clustering if for\nevery point $v\\in P$, $v$ can find a center among its $n/k$ closest neighbors.\nRecently, Mahabadi and Vakilian [2020] showed how to get a\n$(p^{O(p)},7)$-bicriteria approximation for the problem of fair $k$-clustering\nwith $\\ell_p$-norm cost: every point finds a center within distance at most $7$\ntimes its distance to its $(n/k)$-th closest neighbor and the $\\ell_p$-norm\ncost of the solution is at most $p^{O(p)}$ times the cost of an optimal fair\nsolution. In this work, for any $\\varepsilon>0$, we present an improved $(16^p\n+\\varepsilon,3)$-bicriteria approximation for the fair $k$-clustering with\n$\\ell_p$-norm cost. To achieve our guarantees, we extend the framework of\n[Charikar et al., 2002, Swamy, 2016] and devise a $16^p$-approximation\nalgorithm for the facility location with $\\ell_p$-norm cost under matroid\nconstraint which might be of an independent interest. Besides, our approach\nsuggests a reduction from our individually fair clustering to a clustering with\na group fairness requirement proposed by Kleindessner et al. [2019], which is\nessentially the median matroid problem [Krishnaswamy et al., 2011].",
          "link": "http://arxiv.org/abs/2106.14043",
          "publishedOn": "2021-06-29T01:55:17.792Z",
          "wordCount": 655,
          "title": "Improved Approximation Algorithms for Individually Fair Clustering. (arXiv:2106.14043v1 [cs.DS])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14089",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Que_Z/0/1/0/all/0/1\">Zhiqiang Que</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_E/0/1/0/all/0/1\">Erwei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marikar_U/0/1/0/all/0/1\">Umar Marikar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreno_E/0/1/0/all/0/1\">Eric Moreno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ngadiuba_J/0/1/0/all/0/1\">Jennifer Ngadiuba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Javed_H/0/1/0/all/0/1\">Hamza Javed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borzyszkowski_B/0/1/0/all/0/1\">Bart&#x142;omiej Borzyszkowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aarrestad_T/0/1/0/all/0/1\">Thea Aarrestad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Loncar_V/0/1/0/all/0/1\">Vladimir Loncar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Summers_S/0/1/0/all/0/1\">Sioni Summers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pierini_M/0/1/0/all/0/1\">Maurizio Pierini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheung_P/0/1/0/all/0/1\">Peter Y Cheung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luk_W/0/1/0/all/0/1\">Wayne Luk</a>",
          "description": "This paper presents novel reconfigurable architectures for reducing the\nlatency of recurrent neural networks (RNNs) that are used for detecting\ngravitational waves. Gravitational interferometers such as the LIGO detectors\ncapture cosmic events such as black hole mergers which happen at unknown times\nand of varying durations, producing time-series data. We have developed a new\narchitecture capable of accelerating RNN inference for analyzing time-series\ndata from LIGO detectors. This architecture is based on optimizing the\ninitiation intervals (II) in a multi-layer LSTM (Long Short-Term Memory)\nnetwork, by identifying appropriate reuse factors for each layer. A\ncustomizable template for this architecture has been designed, which enables\nthe generation of low-latency FPGA designs with efficient resource utilization\nusing high-level synthesis tools. The proposed approach has been evaluated\nbased on two LSTM models, targeting a ZYNQ 7045 FPGA and a U250 FPGA.\nExperimental results show that with balanced II, the number of DSPs can be\nreduced up to 42% while achieving the same IIs. When compared to other\nFPGA-based LSTM designs, our design can achieve about 4.92 to 12.4 times lower\nlatency.",
          "link": "http://arxiv.org/abs/2106.14089",
          "publishedOn": "2021-06-29T01:55:17.786Z",
          "wordCount": 655,
          "title": "Accelerating Recurrent Neural Networks for Gravitational Wave Experiments. (arXiv:2106.14089v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13899",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Monteiro_J/0/1/0/all/0/1\">Joao Monteiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gibert_X/0/1/0/all/0/1\">Xavier Gibert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jianqiao Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dumoulin_V/0/1/0/all/0/1\">Vincent Dumoulin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dar-Shyang Lee</a>",
          "description": "Learning guarantees often rely on assumptions of i.i.d. data, which will\nlikely be violated in practice once predictors are deployed to perform\nreal-world tasks. Domain adaptation approaches thus appeared as a useful\nframework yielding extra flexibility in that distinct train and test data\ndistributions are supported, provided that other assumptions are satisfied such\nas covariate shift, which expects the conditional distributions over labels to\nbe independent of the underlying data distribution. Several approaches were\nintroduced in order to induce generalization across varying train and test data\nsources, and those often rely on the general idea of domain-invariance, in such\na way that the data-generating distributions are to be disregarded by the\nprediction model. In this contribution, we tackle the problem of generalizing\nacross data sources by approaching it from the opposite direction: we consider\na conditional modeling approach in which predictions, in addition to being\ndependent on the input data, use information relative to the underlying\ndata-generating distribution. For instance, the model has an explicit mechanism\nto adapt to changing environments and/or new data sources. We argue that such\nan approach is more generally applicable than current domain adaptation methods\nsince it does not require extra assumptions such as covariate shift and further\nyields simpler training algorithms that avoid a common source of training\ninstabilities caused by minimax formulations, often employed in\ndomain-invariant methods.",
          "link": "http://arxiv.org/abs/2106.13899",
          "publishedOn": "2021-06-29T01:55:17.780Z",
          "wordCount": 673,
          "title": "Domain Conditional Predictors for Domain Adaptation. (arXiv:2106.13899v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13970",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yue Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_C/0/1/0/all/0/1\">Chenzhuang Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tiejun Li</a>",
          "description": "In vision-based reinforcement learning (RL) tasks, it is prevalent to assign\nthe auxiliary task with a surrogate self-supervised loss so as to obtain more\nsemantic representations and improve sample efficiency. However, abundant\ninformation in self-supervised auxiliary tasks has been disregarded, since the\nrepresentation learning part and the decision-making part are separated. To\nsufficiently utilize information in the auxiliary task, we present a simple yet\neffective idea to employ self-supervised loss as an intrinsic reward, called\nIntrinsically Motivated Self-Supervised learning in Reinforcement learning\n(IM-SSR). We formally show that the self-supervised loss can be decomposed as\nexploration for novel states and robustness improvement from nuisance\nelimination. IM-SSR can be effortlessly plugged into any reinforcement learning\nwith self-supervised auxiliary objectives with nearly no additional cost.\nCombined with IM-SSR, the previous underlying algorithms achieve salient\nimprovements on both sample efficiency and generalization in various\nvision-based robotics tasks from the DeepMind Control Suite, especially when\nthe reward signal is sparse.",
          "link": "http://arxiv.org/abs/2106.13970",
          "publishedOn": "2021-06-29T01:55:17.762Z",
          "wordCount": 587,
          "title": "Intrinsically Motivated Self-supervised Learning in Reinforcement Learning. (arXiv:2106.13970v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14103",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ren_P/0/1/0/all/0/1\">Pu Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rao_C/0/1/0/all/0/1\">Chengping Rao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jianxun Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Hao Sun</a>",
          "description": "Partial differential equations (PDEs) play a fundamental role in modeling and\nsimulating problems across a wide range of disciplines. Recent advances in deep\nlearning have shown the great potential of physics-informed neural networks\n(PINNs) to solve PDEs as a basis for data-driven modeling and inverse analysis.\nHowever, the majority of existing PINN methods, based on fully-connected NNs,\npose intrinsic limitations to low-dimensional spatiotemporal parameterizations.\nMoreover, since the initial/boundary conditions (I/BCs) are softly imposed via\npenalty, the solution quality heavily relies on hyperparameter tuning. To this\nend, we propose the novel physics-informed convolutional-recurrent learning\narchitectures (PhyCRNet and PhyCRNet-s) for solving PDEs without any labeled\ndata. Specifically, an encoder-decoder convolutional long short-term memory\nnetwork is proposed for low-dimensional spatial feature extraction and temporal\nevolution learning. The loss function is defined as the aggregated discretized\nPDE residuals, while the I/BCs are hard-encoded in the network to ensure\nforcible satisfaction (e.g., periodic boundary padding). The networks are\nfurther enhanced by autoregressive and residual connections that explicitly\nsimulate time marching. The performance of our proposed methods has been\nassessed by solving three nonlinear PDEs (e.g., 2D Burgers' equations, the\n$\\lambda$-$\\omega$ and FitzHugh Nagumo reaction-diffusion equations), and\ncompared against the start-of-the-art baseline algorithms. The numerical\nresults demonstrate the superiority of our proposed methodology in the context\nof solution accuracy, extrapolability and generalizability.",
          "link": "http://arxiv.org/abs/2106.14103",
          "publishedOn": "2021-06-29T01:55:17.755Z",
          "wordCount": 662,
          "title": "PhyCRNet: Physics-informed Convolutional-Recurrent Network for Solving Spatiotemporal PDEs. (arXiv:2106.14103v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13919",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">David W. Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burghouts_G/0/1/0/all/0/1\">Gertjan J. Burghouts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Snoek_C/0/1/0/all/0/1\">Cees G. M. Snoek</a>",
          "description": "This work considers predicting the relational structure of a hypergraph for a\ngiven set of vertices, as common for applications in particle physics,\nbiological systems and other complex combinatorial problems. A problem arises\nfrom the number of possible multi-way relationships, or hyperedges, scaling in\n$\\mathcal{O}(2^n)$ for a set of $n$ elements. Simply storing an indicator\ntensor for all relationships is already intractable for moderately sized $n$,\nprompting previous approaches to restrict the number of vertices a hyperedge\nconnects. Instead, we propose a recurrent hypergraph neural network that\npredicts the incidence matrix by iteratively refining an initial guess of the\nsolution. We leverage the property that most hypergraphs of interest are\nsparsely connected and reduce the memory requirement to $\\mathcal{O}(nk)$,\nwhere $k$ is the maximum number of positive edges, i.e., edges that actually\nexist. In order to counteract the linearly growing memory cost from training a\nlengthening sequence of refinement steps, we further propose an algorithm that\napplies backpropagation through time on randomly sampled subsequences. We\nempirically show that our method can match an increase in the intrinsic\ncomplexity without a performance decrease and demonstrate superior performance\ncompared to state-of-the-art models.",
          "link": "http://arxiv.org/abs/2106.13919",
          "publishedOn": "2021-06-29T01:55:17.749Z",
          "wordCount": 612,
          "title": "Recurrently Predicting Hypergraphs. (arXiv:2106.13919v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13884",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tsimpoukelli_M/0/1/0/all/0/1\">Maria Tsimpoukelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menick_J/0/1/0/all/0/1\">Jacob Menick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cabi_S/0/1/0/all/0/1\">Serkan Cabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eslami_S/0/1/0/all/0/1\">S.M. Ali Eslami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vinyals_O/0/1/0/all/0/1\">Oriol Vinyals</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hill_F/0/1/0/all/0/1\">Felix Hill</a>",
          "description": "When trained at sufficient scale, auto-regressive language models exhibit the\nnotable ability to learn a new language task after being prompted with just a\nfew examples. Here, we present a simple, yet effective, approach for\ntransferring this few-shot learning ability to a multimodal setting (vision and\nlanguage). Using aligned image and caption data, we train a vision encoder to\nrepresent each image as a sequence of continuous embeddings, such that a\npre-trained, frozen language model prompted with this prefix generates the\nappropriate caption. The resulting system is a multimodal few-shot learner,\nwith the surprising ability to learn a variety of new tasks when conditioned on\nexamples, represented as a sequence of multiple interleaved image and text\nembeddings. We demonstrate that it can rapidly learn words for new objects and\nnovel visual categories, do visual question-answering with only a handful of\nexamples, and make use of outside knowledge, by measuring a single model on a\nvariety of established and new benchmarks.",
          "link": "http://arxiv.org/abs/2106.13884",
          "publishedOn": "2021-06-29T01:55:17.742Z",
          "wordCount": 608,
          "title": "Multimodal Few-Shot Learning with Frozen Language Models. (arXiv:2106.13884v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13876",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Majumder_B/0/1/0/all/0/1\">Bodhisattwa Prasad Majumder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camburu_O/0/1/0/all/0/1\">Oana-Maria Camburu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lukasiewicz_T/0/1/0/all/0/1\">Thomas Lukasiewicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McAuley_J/0/1/0/all/0/1\">Julian McAuley</a>",
          "description": "Explainable machine learning models primarily justify predicted labels using\neither extractive rationales (i.e., subsets of input features) or free-text\nnatural language explanations (NLEs) as abstractive justifications. While NLEs\ncan be more comprehensive than extractive rationales, machine-generated NLEs\nhave been shown to sometimes lack commonsense knowledge. Here, we show that\ncommonsense knowledge can act as a bridge between extractive rationales and\nNLEs, rendering both types of explanations better. More precisely, we introduce\na unified framework, called RExC (Rationale-Inspired Explanations with\nCommonsense), that (1) extracts rationales as a set of features responsible for\nmachine predictions, (2) expands the extractive rationales using available\ncommonsense resources, and (3) uses the expanded knowledge to generate natural\nlanguage explanations. Our framework surpasses by a large margin the previous\nstate-of-the-art in generating NLEs across five tasks in both natural language\nprocessing and vision-language understanding, with human annotators\nconsistently rating the explanations generated by RExC to be more\ncomprehensive, grounded in commonsense, and overall preferred compared to\nprevious state-of-the-art models. Moreover, our work shows that\ncommonsense-grounded explanations can enhance both task performance and\nrationales extraction capabilities.",
          "link": "http://arxiv.org/abs/2106.13876",
          "publishedOn": "2021-06-29T01:55:17.726Z",
          "wordCount": 615,
          "title": "Rationale-Inspired Natural Language Explanations with Commonsense. (arXiv:2106.13876v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14472",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Atigh_M/0/1/0/all/0/1\">Mina Ghadimi Atigh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keller_Ressel_M/0/1/0/all/0/1\">Martin Keller-Ressel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mettes_P/0/1/0/all/0/1\">Pascal Mettes</a>",
          "description": "Hyperbolic space has become a popular choice of manifold for representation\nlearning of arbitrary data, from tree-like structures and text to graphs.\nBuilding on the success of deep learning with prototypes in Euclidean and\nhyperspherical spaces, a few recent works have proposed hyperbolic prototypes\nfor classification. Such approaches enable effective learning in\nlow-dimensional output spaces and can exploit hierarchical relations amongst\nclasses, but require privileged information about class labels to position the\nhyperbolic prototypes. In this work, we propose Hyperbolic Busemann Learning.\nThe main idea behind our approach is to position prototypes on the ideal\nboundary of the Poincare ball, which does not require prior label knowledge. To\nbe able to compute proximities to ideal prototypes, we introduce the penalised\nBusemann loss. We provide theory supporting the use of ideal prototypes and the\nproposed loss by proving its equivalence to logistic regression in the\none-dimensional case. Empirically, we show that our approach provides a natural\ninterpretation of classification confidence, while outperforming recent\nhyperspherical and hyperbolic prototype approaches.",
          "link": "http://arxiv.org/abs/2106.14472",
          "publishedOn": "2021-06-29T01:55:17.720Z",
          "wordCount": 593,
          "title": "Hyperbolic Busemann Learning with Ideal Prototypes. (arXiv:2106.14472v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14144",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Xu_S/0/1/0/all/0/1\">Shichao Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fu_Y/0/1/0/all/0/1\">Yangyang Fu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1\">Yixuan Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+ONeill_Z/0/1/0/all/0/1\">Zheng O&#x27;Neill</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_Q/0/1/0/all/0/1\">Qi Zhu</a>",
          "description": "As people spend up to 87% of their time indoors, intelligent Heating,\nVentilation, and Air Conditioning (HVAC) systems in buildings are essential for\nmaintaining occupant comfort and reducing energy consumption. Those HVAC\nsystems in modern smart buildings rely on real-time sensor readings, which in\npractice often suffer from various faults and could also be vulnerable to\nmalicious attacks. Such faulty sensor inputs may lead to the violation of\nindoor environment requirements (e.g., temperature, humidity, etc.) and the\nincrease of energy consumption. While many model-based approaches have been\nproposed in the literature for building HVAC control, it is costly to develop\naccurate physical models for ensuring their performance and even more\nchallenging to address the impact of sensor faults. In this work, we present a\nnovel learning-based framework for sensor fault-tolerant HVAC control, which\nincludes three deep learning based components for 1) generating temperature\nproposals with the consideration of possible sensor faults, 2) selecting one of\nthe proposals based on the assessment of their accuracy, and 3) applying\nreinforcement learning with the selected temperature proposal. Moreover, to\naddress the challenge of training data insufficiency in building-related tasks,\nwe propose a model-assisted learning method leveraging an abstract model of\nbuilding physical dynamics. Through extensive numerical experiments, we\ndemonstrate that the proposed fault-tolerant HVAC control framework can\nsignificantly reduce building temperature violations under a variety of sensor\nfault patterns while maintaining energy efficiency.",
          "link": "http://arxiv.org/abs/2106.14144",
          "publishedOn": "2021-06-29T01:55:17.687Z",
          "wordCount": 675,
          "title": "Model-assisted Learning-based Framework for Sensor Fault-Tolerant Building HVAC Control. (arXiv:2106.14144v1 [eess.SY])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14229",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1\">Haoming Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1\">Xiaojun Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1\">Dian Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1\">Zhi Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>",
          "description": "In this letter, we propose a multi-task over-theair federated learning\n(MOAFL) framework, where multiple learning tasks share edge devices for data\ncollection and learning models under the coordination of a edge server (ES).\nSpecially, the model updates for all the tasks are transmitted and\nsuperpositioned concurrently over a non-orthogonal uplink channel via\nover-the-air computation, and the aggregation results of all the tasks are\nreconstructed at the ES through an extended version of the turbo compressed\nsensing algorithm. Both the convergence analysis and numerical results\ndemonstrate that the MOAFL framework can significantly reduce the uplink\nbandwidth consumption of multiple tasks without causing substantial learning\nperformance degradation.",
          "link": "http://arxiv.org/abs/2106.14229",
          "publishedOn": "2021-06-29T01:55:17.679Z",
          "wordCount": 543,
          "title": "Multi-task Over-the-Air Federated Learning: A Non-Orthogonal Transmission Approach. (arXiv:2106.14229v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13851",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Matheny_M/0/1/0/all/0/1\">Michael Matheny</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phillips_J/0/1/0/all/0/1\">Jeff M. Phillips</a>",
          "description": "Consider the geometric range space $(X, \\mathcal{H}_d)$ where $X \\subset\n\\mathbb{R}^d$ and $\\mathcal{H}_d$ is the set of ranges defined by\n$d$-dimensional halfspaces. In this setting we consider that $X$ is the\ndisjoint union of a red and blue set. For each halfspace $h \\in \\mathcal{H}_d$\ndefine a function $\\Phi(h)$ that measures the \"difference\" between the fraction\nof red and fraction of blue points which fall in the range $h$. In this context\nthe maximum discrepancy problem is to find the $h^* = \\arg \\max_{h \\in (X,\n\\mathcal{H}_d)} \\Phi(h)$. We aim to instead find an $\\hat{h}$ such that\n$\\Phi(h^*) - \\Phi(\\hat{h}) \\le \\varepsilon$. This is the central problem in\nlinear classification for machine learning, in spatial scan statistics for\nspatial anomaly detection, and shows up in many other areas. We provide a\nsolution for this problem in $O(|X| + (1/\\varepsilon^d) \\log^4\n(1/\\varepsilon))$ time, which improves polynomially over the previous best\nsolutions. For $d=2$ we show that this is nearly tight through conditional\nlower bounds. For different classes of $\\Phi$ we can either provide a\n$\\Omega(|X|^{3/2 - o(1)})$ time lower bound for the exact solution with a\nreduction to APSP, or an $\\Omega(|X| + 1/\\varepsilon^{2-o(1)})$ lower bound for\nthe approximate solution with a reduction to 3SUM.\n\nA key technical result is a $\\varepsilon$-approximate halfspace range\ncounting data structure of size $O(1/\\varepsilon^d)$ with $O(\\log\n(1/\\varepsilon))$ query time, which we can build in $O(|X| + (1/\\varepsilon^d)\n\\log^4 (1/\\varepsilon))$ time.",
          "link": "http://arxiv.org/abs/2106.13851",
          "publishedOn": "2021-06-29T01:55:17.672Z",
          "wordCount": 658,
          "title": "Approximate Maximum Halfspace Discrepancy. (arXiv:2106.13851v1 [cs.CG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13895",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Roy_K/0/1/0/all/0/1\">Kaushik Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaur_M/0/1/0/all/0/1\">Manas Gaur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheth_A/0/1/0/all/0/1\">Amit Sheth</a>",
          "description": "Contextual Bandits find important use cases in various real-life scenarios\nsuch as online advertising, recommendation systems, healthcare, etc. However,\nmost of the algorithms use flat feature vectors to represent context whereas,\nin the real world, there is a varying number of objects and relations among\nthem to model in the context. For example, in a music recommendation system,\nthe user context contains what music they listen to, which artists create this\nmusic, the artist albums, etc. Adding richer relational context representations\nalso introduces a much larger context space making exploration-exploitation\nharder. To improve the efficiency of exploration-exploitation knowledge about\nthe context can be infused to guide the exploration-exploitation strategy.\nRelational context representations allow a natural way for humans to specify\nknowledge owing to their descriptive nature. We propose an adaptation of\nKnowledge Infused Policy Gradients to the Contextual Bandit setting and a novel\nKnowledge Infused Policy Gradients Upper Confidence Bound algorithm and perform\nan experimental analysis of a simulated music recommendation dataset and\nvarious real-life datasets where expert knowledge can drastically reduce the\ntotal regret and where it cannot.",
          "link": "http://arxiv.org/abs/2106.13895",
          "publishedOn": "2021-06-29T01:55:17.649Z",
          "wordCount": 629,
          "title": "Knowledge Infused Policy Gradients with Upper Confidence Bound for Relational Bandits. (arXiv:2106.13895v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14405",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Szot_A/0/1/0/all/0/1\">Andrew Szot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clegg_A/0/1/0/all/0/1\">Alex Clegg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Undersander_E/0/1/0/all/0/1\">Eric Undersander</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wijmans_E/0/1/0/all/0/1\">Erik Wijmans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yili Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turner_J/0/1/0/all/0/1\">John Turner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maestre_N/0/1/0/all/0/1\">Noah Maestre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukadam_M/0/1/0/all/0/1\">Mustafa Mukadam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaplot_D/0/1/0/all/0/1\">Devendra Chaplot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maksymets_O/0/1/0/all/0/1\">Oleksandr Maksymets</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gokaslan_A/0/1/0/all/0/1\">Aaron Gokaslan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vondrus_V/0/1/0/all/0/1\">Vladimir Vondrus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dharur_S/0/1/0/all/0/1\">Sameer Dharur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meier_F/0/1/0/all/0/1\">Franziska Meier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galuba_W/0/1/0/all/0/1\">Wojciech Galuba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_A/0/1/0/all/0/1\">Angel Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kira_Z/0/1/0/all/0/1\">Zsolt Kira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koltun_V/0/1/0/all/0/1\">Vladlen Koltun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malik_J/0/1/0/all/0/1\">Jitendra Malik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savva_M/0/1/0/all/0/1\">Manolis Savva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Batra_D/0/1/0/all/0/1\">Dhruv Batra</a>",
          "description": "We introduce Habitat 2.0 (H2.0), a simulation platform for training virtual\nrobots in interactive 3D environments and complex physics-enabled scenarios. We\nmake comprehensive contributions to all levels of the embodied AI stack - data,\nsimulation, and benchmark tasks. Specifically, we present: (i) ReplicaCAD: an\nartist-authored, annotated, reconfigurable 3D dataset of apartments (matching\nreal spaces) with articulated objects (e.g. cabinets and drawers that can\nopen/close); (ii) H2.0: a high-performance physics-enabled 3D simulator with\nspeeds exceeding 25,000 simulation steps per second (850x real-time) on an\n8-GPU node, representing 100x speed-ups over prior work; and, (iii) Home\nAssistant Benchmark (HAB): a suite of common tasks for assistive robots (tidy\nthe house, prepare groceries, set the table) that test a range of mobile\nmanipulation capabilities. These large-scale engineering contributions allow us\nto systematically compare deep reinforcement learning (RL) at scale and\nclassical sense-plan-act (SPA) pipelines in long-horizon structured tasks, with\nan emphasis on generalization to new objects, receptacles, and layouts. We find\nthat (1) flat RL policies struggle on HAB compared to hierarchical ones; (2) a\nhierarchy with independent skills suffers from 'hand-off problems', and (3) SPA\npipelines are more brittle than RL policies.",
          "link": "http://arxiv.org/abs/2106.14405",
          "publishedOn": "2021-06-29T01:55:17.630Z",
          "wordCount": 659,
          "title": "Habitat 2.0: Training Home Assistants to Rearrange their Habitat. (arXiv:2106.14405v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14117",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Morad_S/0/1/0/all/0/1\">Steven D. Morad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liwicki_S/0/1/0/all/0/1\">Stephan Liwicki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prorok_A/0/1/0/all/0/1\">Amanda Prorok</a>",
          "description": "Solving partially-observable Markov decision processes (POMDPs) is critical\nwhen applying deep reinforcement learning (DRL) to real-world robotics\nproblems, where agents have an incomplete view of the world. We present graph\nconvolutional memory (GCM) for solving POMDPs using deep reinforcement\nlearning. Unlike recurrent neural networks (RNNs) or transformers, GCM embeds\ndomain-specific priors into the memory recall process via a knowledge graph. By\nencapsulating priors in the graph, GCM adapts to specific tasks but remains\napplicable to any DRL task. Using graph convolutions, GCM extracts hierarchical\ngraph features, analogous to image features in a convolutional neural network\n(CNN). We show GCM outperforms long short-term memory (LSTM), gated\ntransformers for reinforcement learning (GTrXL), and differentiable neural\ncomputers (DNCs) on control, long-term non-sequential recall, and 3D navigation\ntasks while using significantly fewer parameters.",
          "link": "http://arxiv.org/abs/2106.14117",
          "publishedOn": "2021-06-29T01:55:17.545Z",
          "wordCount": 562,
          "title": "Graph Convolutional Memory for Deep Reinforcement Learning. (arXiv:2106.14117v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13834",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Li-Ping Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_R/0/1/0/all/0/1\">Ruiyuan Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xiaozhe Hu</a>",
          "description": "Polynomial functions have plenty of useful analytical properties, but they\nare rarely used as learning models because their function class is considered\nto be restricted. This work shows that when trained properly polynomial\nfunctions can be strong learning models. Particularly this work constructs\npolynomial feedforward neural networks using the product activation, a new\nactivation function constructed from multiplications. The new neural network is\na polynomial function and provides accurate control of its polynomial order. It\ncan be trained by standard training techniques such as batch normalization and\ndropout. This new feedforward network covers several previous polynomial models\nas special cases. Compared with common feedforward neural networks, the\npolynomial feedforward network has closed-form calculations of a few\ninteresting quantities, which are very useful in Bayesian learning. In a series\nof regression and classification tasks in the empirical study, the proposed\nmodel outperforms previous polynomial models.",
          "link": "http://arxiv.org/abs/2106.13834",
          "publishedOn": "2021-06-29T01:55:17.504Z",
          "wordCount": 590,
          "title": "Ladder Polynomial Neural Networks. (arXiv:2106.13834v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14015",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Besbes_O/0/1/0/all/0/1\">Omar Besbes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fonseca_Y/0/1/0/all/0/1\">Yuri Fonseca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lobel_I/0/1/0/all/0/1\">Ilan Lobel</a>",
          "description": "We study the problems of offline and online contextual optimization with\nfeedback information, where instead of observing the loss, we observe,\nafter-the-fact, the optimal action an oracle with full knowledge of the\nobjective function would have taken. We aim to minimize regret, which is\ndefined as the difference between our losses and the ones incurred by an\nall-knowing oracle. In the offline setting, the decision-maker has information\navailable from past periods and needs to make one decision, while in the online\nsetting, the decision-maker optimizes decisions dynamically over time based a\nnew set of feasible actions and contextual functions in each period. For the\noffline setting, we characterize the optimal minimax policy, establishing the\nperformance that can be achieved as a function of the underlying geometry of\nthe information induced by the data. In the online setting, we leverage this\ngeometric characterization to optimize the cumulative regret. We develop an\nalgorithm that yields the first regret bound for this problem that is\nlogarithmic in the time horizon.",
          "link": "http://arxiv.org/abs/2106.14015",
          "publishedOn": "2021-06-29T01:55:17.498Z",
          "wordCount": 603,
          "title": "Contextual Inverse Optimization: Offline and Online Learning. (arXiv:2106.14015v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13924",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Finn_T/0/1/0/all/0/1\">Tobias Sebastian Finn</a>",
          "description": "Ensemble data from Earth system models has to be calibrated and\npost-processed. I propose a novel member-by-member post-processing approach\nwith neural networks. I bridge ideas from ensemble data assimilation with\nself-attention, resulting into the self-attentive ensemble transformer. Here,\ninteractions between ensemble members are represented as additive and dynamic\nself-attentive part. As proof-of-concept, global ECMWF ensemble forecasts are\nregressed to 2-metre-temperature fields from the ERA5 reanalysis. I demonstrate\nthat the ensemble transformer can calibrate the ensemble spread and extract\nadditional information from the ensemble. Furthermore, the ensemble transformer\ndirectly outputs multivariate and spatially-coherent ensemble members.\nTherefore, self-attention and the transformer technique can be a missing piece\nfor a member-by-member post-processing of ensemble data with neural networks.",
          "link": "http://arxiv.org/abs/2106.13924",
          "publishedOn": "2021-06-29T01:55:17.378Z",
          "wordCount": 581,
          "title": "Self-Attentive Ensemble Transformer: Representing Ensemble Interactions in Neural Networks for Earth System Models. (arXiv:2106.13924v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14367",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_C/0/1/0/all/0/1\">Chao Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_C/0/1/0/all/0/1\">Chang-E Ren</a>",
          "description": "Broad learning system (BLS) has been proposed for a few years. It\ndemonstrates an effective learning capability for many classification and\nregression problems. However, BLS and its improved versions are mainly used to\ndeal with unsupervised, supervised and semi-supervised learning problems in a\nsingle domain. As far as we know, a little attention is paid to the\ncross-domain learning ability of BLS. Therefore, we introduce BLS into the\nfield of transfer learning and propose a novel algorithm called domain\nadaptation broad learning system based on locally linear embedding (DABLS-LLE).\nThe proposed algorithm can learn a robust classification model by using a small\npart of labeled data from the target domain and all labeled data from the\nsource domain. The proposed algorithm inherits the computational efficiency and\nlearning capability of BLS. Experiments on benchmark dataset\n(Office-Caltech-10) verify the effectiveness of our approach. The results show\nthat our approach can get better classification accuracy with less running time\nthan many existing transfer learning approaches. It shows that our approach can\nbring a new superiority for BLS.",
          "link": "http://arxiv.org/abs/2106.14367",
          "publishedOn": "2021-06-29T01:55:17.311Z",
          "wordCount": 603,
          "title": "Domain Adaptation Broad Learning System Based on Locally Linear Embedding. (arXiv:2106.14367v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13861",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1\">Junwen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yeye He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhuri_S/0/1/0/all/0/1\">Surajit Chaudhuri</a>",
          "description": "Recent work has made significant progress in helping users to automate single\ndata preparation steps, such as string-transformations and table-manipulation\noperators (e.g., Join, GroupBy, Pivot, etc.). We in this work propose to\nautomate multiple such steps end-to-end, by synthesizing complex data pipelines\nwith both string transformations and table-manipulation operators. We propose a\nnovel \"by-target\" paradigm that allows users to easily specify the desired\npipeline, which is a significant departure from the traditional by-example\nparadigm. Using by-target, users would provide input tables (e.g., csv or json\nfiles), and point us to a \"target table\" (e.g., an existing database table or\nBI dashboard) to demonstrate how the output from the desired pipeline would\nschematically \"look like\". While the problem is seemingly underspecified, our\nunique insight is that implicit table constraints such as FDs and keys can be\nexploited to significantly constrain the space to make the problem tractable.\nWe develop an Auto-Pipeline system that learns to synthesize pipelines using\nreinforcement learning and search. Experiments on large numbers of real\npipelines crawled from GitHub suggest that Auto-Pipeline can successfully\nsynthesize 60-70% of these complex pipelines (up to 10 steps) in 10-20 seconds\non average.",
          "link": "http://arxiv.org/abs/2106.13861",
          "publishedOn": "2021-06-29T01:55:17.305Z",
          "wordCount": 624,
          "title": "AutoPipeline: Synthesize Data Pipelines By-Target Using Reinforcement Learning and Search. (arXiv:2106.13861v1 [cs.DB])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13906",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jothimurugan_K/0/1/0/all/0/1\">Kishor Jothimurugan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_S/0/1/0/all/0/1\">Suguman Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bastani_O/0/1/0/all/0/1\">Osbert Bastani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alur_R/0/1/0/all/0/1\">Rajeev Alur</a>",
          "description": "We study the problem of learning control policies for complex tasks given by\nlogical specifications. Recent approaches automatically generate a reward\nfunction from a given specification and use a suitable reinforcement learning\nalgorithm to learn a policy that maximizes the expected reward. These\napproaches, however, scale poorly to complex tasks that require high-level\nplanning. In this work, we develop a compositional learning approach, called\nDiRL, that interleaves high-level planning and reinforcement learning. First,\nDiRL encodes the specification as an abstract graph; intuitively, vertices and\nedges of the graph correspond to regions of the state space and simpler\nsub-tasks, respectively. Our approach then incorporates reinforcement learning\nto learn neural network policies for each edge (sub-task) within a\nDijkstra-style planning algorithm to compute a high-level plan in the graph. An\nevaluation of the proposed approach on a set of challenging control benchmarks\nwith continuous state and action spaces demonstrates that it outperforms\nstate-of-the-art baselines.",
          "link": "http://arxiv.org/abs/2106.13906",
          "publishedOn": "2021-06-29T01:55:16.958Z",
          "wordCount": 582,
          "title": "Compositional Reinforcement Learning from Logical Specifications. (arXiv:2106.13906v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13959",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Chatterjee_A/0/1/0/all/0/1\">Avishek Chatterjee</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Mazumder_S/0/1/0/all/0/1\">Satyaki Mazumder</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Das_K/0/1/0/all/0/1\">Koel Das</a>",
          "description": "In recent times, functional data analysis (FDA) has been successfully applied\nin the field of high dimensional data classification. In this paper, we present\na novel classification framework using functional data and classwise Principal\nComponent Analysis (PCA). Our proposed method can be used in high dimensional\ntime series data which typically suffers from small sample size problem. Our\nmethod extracts a piece wise linear functional feature space and is\nparticularly suitable for hard classification problems.The proposed framework\nconverts time series data into functional data and uses classwise functional\nPCA for feature extraction followed by classification using a Bayesian linear\nclassifier. We demonstrate the efficacy of our proposed method by applying it\nto both synthetic data sets and real time series data from diverse fields\nincluding but not limited to neuroscience, food science, medical sciences and\nchemometrics.",
          "link": "http://arxiv.org/abs/2106.13959",
          "publishedOn": "2021-06-29T01:55:16.935Z",
          "wordCount": 577,
          "title": "Functional Classwise Principal Component Analysis: A Novel Classification Framework. (arXiv:2106.13959v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13997",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tyukin_I/0/1/0/all/0/1\">Ivan Y. Tyukin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Higham_D/0/1/0/all/0/1\">Desmond J. Higham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woldegeorgis_E/0/1/0/all/0/1\">Eliyas Woldegeorgis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gorban_A/0/1/0/all/0/1\">Alexander N. Gorban</a>",
          "description": "We develop and study new adversarial perturbations that enable an attacker to\ngain control over decisions in generic Artificial Intelligence (AI) systems\nincluding deep learning neural networks. In contrast to adversarial data\nmodification, the attack mechanism we consider here involves alterations to the\nAI system itself. Such a stealth attack could be conducted by a mischievous,\ncorrupt or disgruntled member of a software development team. It could also be\nmade by those wishing to exploit a \"democratization of AI\" agenda, where\nnetwork architectures and trained parameter sets are shared publicly. Building\non work by [Tyukin et al., International Joint Conference on Neural Networks,\n2020], we develop a range of new implementable attack strategies with\naccompanying analysis, showing that with high probability a stealth attack can\nbe made transparent, in the sense that system performance is unchanged on a\nfixed validation set which is unknown to the attacker, while evoking any\ndesired output on a trigger input of interest. The attacker only needs to have\nestimates of the size of the validation set and the spread of the AI's relevant\nlatent space. In the case of deep learning neural networks, we show that a one\nneuron attack is possible - a modification to the weights and bias associated\nwith a single neuron - revealing a vulnerability arising from\nover-parameterization. We illustrate these concepts in a realistic setting.\nGuided by the theory and computational results, we also propose strategies to\nguard against stealth attacks.",
          "link": "http://arxiv.org/abs/2106.13997",
          "publishedOn": "2021-06-29T01:55:16.889Z",
          "wordCount": 688,
          "title": "The Feasibility and Inevitability of Stealth Attacks. (arXiv:2106.13997v1 [cs.CR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13869",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kou_W/0/1/0/all/0/1\">Wenjun Kou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carlson_D/0/1/0/all/0/1\">Dustin A. Carlson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baumann_A/0/1/0/all/0/1\">Alexandra J. Baumann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Donnan_E/0/1/0/all/0/1\">Erica N. Donnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schauer_J/0/1/0/all/0/1\">Jacob M. Schauer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Etemadi_M/0/1/0/all/0/1\">Mozziyar Etemadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pandolfino_J/0/1/0/all/0/1\">John E. Pandolfino</a>",
          "description": "High-resolution manometry (HRM) is the primary procedure used to diagnose\nesophageal motility disorders. Its interpretation and classification includes\nan initial evaluation of swallow-level outcomes and then derivation of a\nstudy-level diagnosis based on Chicago Classification (CC), using a tree-like\nalgorithm. This diagnostic approach on motility disordered using HRM was\nmirrored using a multi-stage modeling framework developed using a combination\nof various machine learning approaches. Specifically, the framework includes\ndeep-learning models at the swallow-level stage and feature-based machine\nlearning models at the study-level stage. In the swallow-level stage, three\nmodels based on convolutional neural networks (CNNs) were developed to predict\nswallow type, swallow pressurization, and integrated relaxation pressure (IRP).\nAt the study-level stage, model selection from families of the\nexpert-knowledge-based rule models, xgboost models and artificial neural\nnetwork(ANN) models were conducted, with the latter two model designed and\naugmented with motivation from the export knowledge. A simple model-agnostic\nstrategy of model balancing motivated by Bayesian principles was utilized,\nwhich gave rise to model averaging weighted by precision scores. The averaged\n(blended) models and individual models were compared and evaluated, of which\nthe best performance on test dataset is 0.81 in top-1 prediction, 0.92 in top-2\npredictions. This is the first artificial-intelligence-style model to\nautomatically predict CC diagnosis of HRM study from raw multi-swallow data.\nMoreover, the proposed modeling framework could be easily extended to\nmulti-modal tasks, such as diagnosis of esophageal patients based on clinical\ndata from both HRM and functional luminal imaging probe panometry (FLIP).",
          "link": "http://arxiv.org/abs/2106.13869",
          "publishedOn": "2021-06-29T01:55:16.802Z",
          "wordCount": 693,
          "title": "A multi-stage machine learning model on diagnosis of esophageal manometry. (arXiv:2106.13869v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13867",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Huang_C/0/1/0/all/0/1\">Chao Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fan_J/0/1/0/all/0/1\">Jiameng Fan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1\">Xin Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_W/0/1/0/all/0/1\">Wenchao Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhu_Q/0/1/0/all/0/1\">Qi Zhu</a>",
          "description": "We propose POLAR, a \\textbf{pol}ynomial \\textbf{ar}ithmetic framework that\nleverages polynomial overapproximations with interval remainders for\nbounded-time reachability analysis of neural network-controlled systems\n(NNCSs). Compared with existing arithmetic approaches that use standard Taylor\nmodels, our framework uses a novel approach to iteratively overapproximate the\nneuron output ranges layer-by-layer with a combination of Bernstein polynomial\ninterpolation for continuous activation functions and Taylor model arithmetic\nfor the other operations. This approach can overcome the main drawback in the\nstandard Taylor model arithmetic, i.e. its inability to handle functions that\ncannot be well approximated by Taylor polynomials, and significantly improve\nthe accuracy and efficiency of reachable states computation for NNCSs. To\nfurther tighten the overapproximation, our method keeps the Taylor model\nremainders symbolic under the linear mappings when estimating the output range\nof a neural network. We show that POLAR can be seamlessly integrated with\nexisting Taylor model flowpipe construction techniques, and demonstrate that\nPOLAR significantly outperforms the current state-of-the-art techniques on a\nsuite of benchmarks.",
          "link": "http://arxiv.org/abs/2106.13867",
          "publishedOn": "2021-06-29T01:55:16.795Z",
          "wordCount": 608,
          "title": "POLAR: A Polynomial Arithmetic Framework for Verifying Neural-Network Controlled Systems. (arXiv:2106.13867v1 [eess.SY])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14384",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Kang_Y/0/1/0/all/0/1\">Yihuang Kang</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Chiu_Y/0/1/0/all/0/1\">Yi-Wen Chiu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lin_M/0/1/0/all/0/1\">Ming-Yen Lin</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Su_F/0/1/0/all/0/1\">Fang-yi Su</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Huang_S/0/1/0/all/0/1\">Sheng-Tai Huang</a>",
          "description": "Machine Learning (ML) and its applications have been transforming our lives\nbut it is also creating issues related to the development of fair, accountable,\ntransparent, and ethical Artificial Intelligence. As the ML models are not\nfully comprehensible yet, it is obvious that we still need humans to be part of\nalgorithmic decision-making processes. In this paper, we consider a ML\nframework that may accelerate model learning and improve its interpretability\nby incorporating human experts into the model learning loop. We propose a novel\nhuman-in-the-loop ML framework aimed at dealing with learning problems that the\ncost of data annotation is high and the lack of appropriate data to model the\nassociation between the target tasks and the input features. With an\napplication to precision dosing, our experimental results show that the\napproach can learn interpretable rules from data and may potentially lower\nexperts' workload by replacing data annotation with rule representation\nediting. The approach may also help remove algorithmic bias by introducing\nexperts' feedback into the iterative model learning process.",
          "link": "http://arxiv.org/abs/2106.14384",
          "publishedOn": "2021-06-29T01:55:16.783Z",
          "wordCount": 605,
          "title": "Towards Model-informed Precision Dosing with Expert-in-the-loop Machine Learning. (arXiv:2106.14384v1 [stat.AP])"
        },
        {
          "id": "http://arxiv.org/abs/2012.06330",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tan_Y/0/1/0/all/0/1\">Yi Xiang Marcus Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chong_P/0/1/0/all/0/1\">Penny Chong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jiamei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheung_N/0/1/0/all/0/1\">Ngai-Man Cheung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elovici_Y/0/1/0/all/0/1\">Yuval Elovici</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Binder_A/0/1/0/all/0/1\">Alexander Binder</a>",
          "description": "Few-shot classifiers excel under limited training samples, making them useful\nin applications with sparsely user-provided labels. Their unique relative\nprediction setup offers opportunities for novel attacks, such as targeting\nsupport sets required to categorise unseen test samples, which are not\navailable in other machine learning setups. In this work, we propose a\ndetection strategy to identify adversarial support sets, aimed at destroying\nthe understanding of a few-shot classifier for a certain class. We achieve this\nby introducing the concept of self-similarity of a support set and by employing\nfiltering of supports. Our method is attack-agnostic, and we are the first to\nexplore adversarial detection for support sets of few-shot classifiers to the\nbest of our knowledge. Our evaluation of the miniImagenet (MI) and CUB datasets\nexhibits good attack detection performance despite conceptual simplicity,\nshowing high AUROC scores. We show that self-similarity and filtering for\nadversarial detection can be paired with other filtering functions,\nconstituting a generalisable concept.",
          "link": "http://arxiv.org/abs/2012.06330",
          "publishedOn": "2021-06-29T01:55:16.777Z",
          "wordCount": 651,
          "title": "Detection of Adversarial Supports in Few-shot Classifiers Using Self-Similarity and Filtering. (arXiv:2012.06330v2 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14080",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Modhe_N/0/1/0/all/0/1\">Nirbhay Modhe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamath_H/0/1/0/all/0/1\">Harish Kamath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Batra_D/0/1/0/all/0/1\">Dhruv Batra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalyan_A/0/1/0/all/0/1\">Ashwin Kalyan</a>",
          "description": "Model-based Reinforcement Learning (MBRL) algorithms have been traditionally\ndesigned with the goal of learning accurate dynamics of the environment. This\nintroduces a mismatch between the objectives of model-learning and the overall\nlearning problem of finding an optimal policy. Value-aware model learning, an\nalternative model-learning paradigm to maximum likelihood, proposes to inform\nmodel-learning through the value function of the learnt policy. While this\nparadigm is theoretically sound, it does not scale beyond toy settings. In this\nwork, we propose a novel value-aware objective that is an upper bound on the\nabsolute performance difference of a policy across two models. Further, we\npropose a general purpose algorithm that modifies the standard MBRL pipeline --\nenabling learning with value aware objectives. Our proposed objective, in\nconjunction with this algorithm, is the first successful instantiation of\nvalue-aware MBRL on challenging continuous control environments, outperforming\nprevious value-aware objectives and with competitive performance w.r.t.\nMLE-based MBRL approaches.",
          "link": "http://arxiv.org/abs/2106.14080",
          "publishedOn": "2021-06-29T01:55:16.755Z",
          "wordCount": 585,
          "title": "Model-Advantage Optimization for Model-Based Reinforcement Learning. (arXiv:2106.14080v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2102.06697",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Noormandipour_M/0/1/0/all/0/1\">Mohammadreza Noormandipour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hanchen Wang</a>",
          "description": "In this work, we propose a parameterised quantum circuit learning approach to\npoint set matching problem. In contrast to previous annealing-based methods, we\npropose a quantum circuit-based framework whose parameters are optimised via\ndescending the gradients w.r.t a kernel-based loss function. We formulate the\nshape matching problem into a distribution learning task; that is, to learn the\ndistribution of the optimal transformation parameters. We show that this\nframework is able to find multiple optimal solutions for symmetric shapes and\nis more accurate, scalable and robust than the previous annealing-based method.\nCode, data and pre-trained weights are available at the project page:\n\\href{https://hansen7.github.io/qKC}{https://hansen7.github.io/qKC}",
          "link": "http://arxiv.org/abs/2102.06697",
          "publishedOn": "2021-06-29T01:55:16.741Z",
          "wordCount": 579,
          "title": "Matching Point Sets with Quantum Circuit Learning. (arXiv:2102.06697v2 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13814",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Campos_E/0/1/0/all/0/1\">E. Campos</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Rabinovich_D/0/1/0/all/0/1\">D. Rabinovich</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Akshay_V/0/1/0/all/0/1\">V. Akshay</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Biamonte_J/0/1/0/all/0/1\">J. Biamonte</a>",
          "description": "Quantum Approximate Optimisation (QAOA) is the most studied gate based\nvariational quantum algorithm today. We train QAOA one layer at a time to\nmaximize overlap with an $n$ qubit target state. Doing so we discovered that\nsuch training always saturates -- called \\textit{training saturation} -- at\nsome depth $p^*$, meaning that past a certain depth, overlap can not be\nimproved by adding subsequent layers. We formulate necessary conditions for\nsaturation. Numerically, we find layerwise QAOA reaches its maximum overlap at\ndepth $p^*=n$. The addition of coherent dephasing errors to training removes\nsaturation, recovering robustness to layerwise training. This study sheds new\nlight on the performance limitations and prospects of QAOA.",
          "link": "http://arxiv.org/abs/2106.13814",
          "publishedOn": "2021-06-29T01:55:16.734Z",
          "wordCount": 554,
          "title": "Training Saturation in Layerwise Quantum Approximate Optimisation. (arXiv:2106.13814v1 [quant-ph])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14210",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fanuel_M/0/1/0/all/0/1\">Micha&#xeb;l Fanuel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bardenet_R/0/1/0/all/0/1\">R&#xe9;mi Bardenet</a>",
          "description": "Determinantal Point Process (DPPs) are statistical models for repulsive point\npatterns. Both sampling and inference are tractable for DPPs, a rare feature\namong models with negative dependence that explains their popularity in machine\nlearning and spatial statistics. Parametric and nonparametric inference methods\nhave been proposed in the finite case, i.e. when the point patterns live in a\nfinite ground set. In the continuous case, only parametric methods have been\ninvestigated, while nonparametric maximum likelihood for DPPs -- an\noptimization problem over trace-class operators -- has remained an open\nquestion. In this paper, we show that a restricted version of this maximum\nlikelihood (MLE) problem falls within the scope of a recent representer theorem\nfor nonnegative functions in an RKHS. This leads to a finite-dimensional\nproblem, with strong statistical ties to the original MLE. Moreover, we\npropose, analyze, and demonstrate a fixed point algorithm to solve this\nfinite-dimensional problem. Finally, we also provide a controlled estimate of\nthe correlation kernel of the DPP, thus providing more interpretability.",
          "link": "http://arxiv.org/abs/2106.14210",
          "publishedOn": "2021-06-29T01:55:16.725Z",
          "wordCount": 600,
          "title": "Nonparametric estimation of continuous DPPs with kernel methods. (arXiv:2106.14210v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13954",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fubing_M/0/1/0/all/0/1\">Mao Fubing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weiwei_W/0/1/0/all/0/1\">Weng Weiwei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pratama_M/0/1/0/all/0/1\">Mahardhika Pratama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yee_E/0/1/0/all/0/1\">Edward Yapp Kien Yee</a>",
          "description": "Learning from streaming tasks leads a model to catastrophically erase unique\nexperiences it absorbs from previous episodes. While regularization techniques\nsuch as LWF, SI, EWC have proven themselves as an effective avenue to overcome\nthis issue by constraining important parameters of old tasks from changing when\naccepting new concepts, these approaches do not exploit common information of\neach task which can be shared to existing neurons. As a result, they do not\nscale well to large-scale problems since the parameter importance variables\nquickly explode. An Inter-Task Synaptic Mapping (ISYANA) is proposed here to\nunderpin knowledge retention for continual learning. ISYANA combines\ntask-to-neuron relationship as well as concept-to-concept relationship such\nthat it prevents a neuron to embrace distinct concepts while merely accepting\nrelevant concept. Numerical study in the benchmark continual learning problems\nhas been carried out followed by comparison against prominent continual\nlearning algorithms. ISYANA exhibits competitive performance compared to state\nof the arts. Codes of ISYANA is made available in\n\\url{https://github.com/ContinualAL/ISYANAKBS}.",
          "link": "http://arxiv.org/abs/2106.13954",
          "publishedOn": "2021-06-29T01:55:16.709Z",
          "wordCount": 611,
          "title": "Continual Learning via Inter-Task Synaptic Mapping. (arXiv:2106.13954v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14347",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dogga_P/0/1/0/all/0/1\">Pradeep Dogga</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Narasimhan_K/0/1/0/all/0/1\">Karthik Narasimhan</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Sivaraman_A/0/1/0/all/0/1\">Anirudh Sivaraman</a> (3), <a href=\"http://arxiv.org/find/cs/1/au:+Saini_S/0/1/0/all/0/1\">Shiv Kumar Saini</a> (4), <a href=\"http://arxiv.org/find/cs/1/au:+Varghese_G/0/1/0/all/0/1\">George Varghese</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Netravali_R/0/1/0/all/0/1\">Ravi Netravali</a> (2) ((1) UCLA, (2) Princeton University, (3) NYU, (4) Adobe Research, India)",
          "description": "A major difficulty in debugging distributed systems lies in manually\ndetermining which of the many available debugging tools to use and how to query\nits logs. Our own study of a production debugging workflow confirms the\nmagnitude of this burden. This paper explores whether a machine-learning model\ncan assist developers in distributed systems debugging. We present Revelio, a\ndebugging assistant which takes user reports and system logs as input, and\noutputs debugging queries that developers can use to find a bug's root cause.\nThe key challenges lie in (1) combining inputs of different types (e.g.,\nnatural language reports and quantitative logs) and (2) generalizing to unseen\nfaults. Revelio addresses these by employing deep neural networks to uniformly\nembed diverse input sources and potential queries into a high-dimensional\nvector space. In addition, it exploits observations from production systems to\nfactorize query generation into two computationally and statistically simpler\nlearning tasks. To evaluate Revelio, we built a testbed with multiple\ndistributed applications and debugging tools. By injecting faults and training\non logs and reports from 800 Mechanical Turkers, we show that Revelio includes\nthe most helpful query in its predicted list of top-3 relevant queries 96% of\nthe time. Our developer study confirms the utility of Revelio.",
          "link": "http://arxiv.org/abs/2106.14347",
          "publishedOn": "2021-06-29T01:55:16.703Z",
          "wordCount": 665,
          "title": "Revelio: ML-Generated Debugging Queries for Distributed Systems. (arXiv:2106.14347v1 [cs.DC])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13870",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tsuei_S/0/1/0/all/0/1\">Stephanie Tsuei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Golatkar_A/0/1/0/all/0/1\">Aditya Golatkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1\">Stefano Soatto</a>",
          "description": "We propose a method to estimate the uncertainty of the outcome of an image\nclassifier on a given input datum. Deep neural networks commonly used for image\nclassification are deterministic maps from an input image to an output class.\nAs such, their outcome on a given datum involves no uncertainty, so we must\nspecify what variability we are referring to when defining, measuring and\ninterpreting \"confidence.\" To this end, we introduce the Wellington Posterior,\nwhich is the distribution of outcomes that would have been obtained in response\nto data that could have been generated by the same scene that produced the\ngiven image. Since there are infinitely many scenes that could have generated\nthe given image, the Wellington Posterior requires induction from scenes other\nthan the one portrayed. We explore alternate methods using data augmentation,\nensembling, and model linearization. Additional alternatives include generative\nadversarial networks, conditional prior networks, and supervised single-view\nreconstruction. We test these alternatives against the empirical posterior\nobtained by inferring the class of temporally adjacent frames in a video. These\ndevelopments are only a small step towards assessing the reliability of deep\nnetwork classifiers in a manner that is compatible with safety-critical\napplications.",
          "link": "http://arxiv.org/abs/2106.13870",
          "publishedOn": "2021-06-29T01:55:16.691Z",
          "wordCount": 649,
          "title": "Scene Uncertainty and the Wellington Posterior of Deterministic Image Classifiers. (arXiv:2106.13870v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14320",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Hajimohammadi_Z/0/1/0/all/0/1\">Zeinab Hajimohammadi</a>, <a href=\"http://arxiv.org/find/math/1/au:+Parand_K/0/1/0/all/0/1\">Kourosh Parand</a>, <a href=\"http://arxiv.org/find/math/1/au:+Ghodsi_A/0/1/0/all/0/1\">Ali Ghodsi</a>",
          "description": "Various phenomena in biology, physics, and engineering are modeled by\ndifferential equations. These differential equations including partial\ndifferential equations and ordinary differential equations can be converted and\nrepresented as integral equations. In particular, Volterra Fredholm Hammerstein\nintegral equations are the main type of these integral equations and\nresearchers are interested in investigating and solving these equations. In\nthis paper, we propose Legendre Deep Neural Network (LDNN) for solving\nnonlinear Volterra Fredholm Hammerstein integral equations (VFHIEs). LDNN\nutilizes Legendre orthogonal polynomials as activation functions of the Deep\nstructure. We present how LDNN can be used to solve nonlinear VFHIEs. We show\nusing the Gaussian quadrature collocation method in combination with LDNN\nresults in a novel numerical solution for nonlinear VFHIEs. Several examples\nare given to verify the performance and accuracy of LDNN.",
          "link": "http://arxiv.org/abs/2106.14320",
          "publishedOn": "2021-06-29T01:55:16.685Z",
          "wordCount": 582,
          "title": "Legendre Deep Neural Network (LDNN) and its application for approximation of nonlinear Volterra Fredholm Hammerstein integral equations. (arXiv:2106.14320v1 [math.NA])"
        },
        {
          "id": "http://arxiv.org/abs/2012.04830",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1\">Xiaoqing Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hu_Y/0/1/0/all/0/1\">Yan Hu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Fang_J/0/1/0/all/0/1\">Jiansheng Fang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xiao_Z/0/1/0/all/0/1\">Zunjie Xiao</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Higashita_R/0/1/0/all/0/1\">Risa Higashita</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1\">Jiang Liu</a>",
          "description": "Cataract is one of the leading causes of reversible visual impairment and\nblindness globally. Over the years, researchers have achieved significant\nprogress in developing state-of-the-art artificial intelligence techniques for\nautomatic cataract classification and grading, helping clinicians prevent and\ntreat cataract in time. This paper provides a comprehensive survey of recent\nadvances in machine learning for cataract classification and grading based on\nophthalmic images. We summarize existing literature from two research\ndirections: conventional machine learning techniques and deep learning\ntechniques. This paper also provides insights into existing works of both\nmerits and limitations. In addition, we discuss several challenges of automatic\ncataract classification and grading based on machine learning techniques and\npresent possible solutions to these challenges for future research.",
          "link": "http://arxiv.org/abs/2012.04830",
          "publishedOn": "2021-06-29T01:55:16.671Z",
          "wordCount": 612,
          "title": "Machine Learning for Cataract Classification and Grading on Ophthalmic Imaging Modalities: A Survey. (arXiv:2012.04830v3 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1902.03717",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1\">Qingyu Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Honnorat_N/0/1/0/all/0/1\">Nicolas Honnorat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adeli_E/0/1/0/all/0/1\">Ehsan Adeli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pohl_K/0/1/0/all/0/1\">Kilian M. Pohl</a>",
          "description": "Variation Autoencoder (VAE) has become a powerful tool in modeling the\nnon-linear generative process of data from a low-dimensional latent space.\nRecently, several studies have proposed to use VAE for unsupervised clustering\nby using mixture models to capture the multi-modal structure of latent\nrepresentations. This strategy, however, is ineffective when there are outlier\ndata samples whose latent representations are meaningless, yet contaminating\nthe estimation of key major clusters in the latent space. This exact problem\narises in the context of resting-state fMRI (rs-fMRI) analysis, where\nclustering major functional connectivity patterns is often hindered by heavy\nnoise of rs-fMRI and many minor clusters (rare connectivity patterns) of no\ninterest to analysis. In this paper we propose a novel generative process, in\nwhich we use a Gaussian-mixture to model a few major clusters in the data, and\nuse a non-informative uniform distribution to capture the remaining data. We\nembed this truncated Gaussian-Mixture model in a Variational AutoEncoder\nframework to obtain a general joint clustering and outlier detection approach,\ncalled tGM-VAE. We demonstrated the applicability of tGM-VAE on the MNIST\ndataset and further validated it in the context of rs-fMRI connectivity\nanalysis.",
          "link": "http://arxiv.org/abs/1902.03717",
          "publishedOn": "2021-06-29T01:55:16.665Z",
          "wordCount": 656,
          "title": "Truncated Gaussian-Mixture Variational AutoEncoder. (arXiv:1902.03717v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.06073",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Royset_J/0/1/0/all/0/1\">Johannes O. Royset</a>",
          "description": "A basic requirement for a mathematical model is often that its solution\n(output) shouldn't change much if the model's parameters (input) are perturbed.\nThis is important because the exact values of parameters may not be known and\none would like to avoid being mislead by an output obtained using incorrect\nvalues. Thus, it's rarely enough to address an application by formulating a\nmodel, solving the resulting optimization problem and presenting the solution\nas the answer. One would need to confirm that the model is suitable, i.e.,\n\"good,\" and this can, at least in part, be achieved by considering a family of\noptimization problems constructed by perturbing parameters of concern. The\nresulting sensitivity analysis uncovers troubling situations with unstable\nsolutions, which we referred to as \"bad\" models, and indicates better model\nformulations. Embedding an actual problem of interest within a family of\nproblems is also a primary path to optimality conditions as well as\ncomputationally attractive, alternative problems, which under ideal\ncircumstances, and when properly tuned, may even furnish the minimum value of\nthe actual problem. The tuning of these alternative problems turns out to be\nintimately tied to finding multipliers in optimality conditions and thus\nemerges as a main component of several optimization algorithms. In fact, the\ntuning amounts to solving certain dual optimization problems. In this tutorial,\nwe'll discuss the opportunities and insights afforded by this broad\nperspective.",
          "link": "http://arxiv.org/abs/2105.06073",
          "publishedOn": "2021-06-29T01:55:16.660Z",
          "wordCount": 678,
          "title": "Good and Bad Optimization Models: Insights from Rockafellians. (arXiv:2105.06073v2 [math.OC] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.00351",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Chung_M/0/1/0/all/0/1\">Moo K. Chung</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ombao_H/0/1/0/all/0/1\">Hernando Ombao</a>",
          "description": "Topological data analysis, including persistent homology, has undergone\nsignificant development in recent years. However, one outstanding challenge is\nto build a coherent statistical inference procedure on persistent diagrams. The\npaired dependent data structure, which are the births and deaths in persistent\ndiagrams, adds complexity to statistical inference. In this paper, we present a\nnew lattice path representation for persistent diagrams. A new exact\nstatistical inference procedure is developed for lattice paths via\ncombinatorial enumerations. The proposed lattice path method is applied to\nstudy the topological characterization of the protein structures of the\nCOVID-19 virus. We demonstrate that there are topological changes during the\nconformational change of spike proteins, a necessary step in infecting host\ncells.",
          "link": "http://arxiv.org/abs/2105.00351",
          "publishedOn": "2021-06-29T01:55:16.652Z",
          "wordCount": 638,
          "title": "Lattice Paths for Persistent Diagrams with Application to COVID-19 Virus Spike Proteins. (arXiv:2105.00351v4 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.02825",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Luca_M/0/1/0/all/0/1\">Massimiliano Luca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barlacchi_G/0/1/0/all/0/1\">Gianni Barlacchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lepri_B/0/1/0/all/0/1\">Bruno Lepri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pappalardo_L/0/1/0/all/0/1\">Luca Pappalardo</a>",
          "description": "The study of human mobility is crucial due to its impact on several aspects\nof our society, such as disease spreading, urban planning, well-being,\npollution, and more. The proliferation of digital mobility data, such as phone\nrecords, GPS traces, and social media posts, combined with the predictive power\nof artificial intelligence, triggered the application of deep learning to human\nmobility. Existing surveys focus on single tasks, data sources, mechanistic or\ntraditional machine learning approaches, while a comprehensive description of\ndeep learning solutions is missing. This survey provides a taxonomy of mobility\ntasks, a discussion on the challenges related to each task and how deep\nlearning may overcome the limitations of traditional models, a description of\nthe most relevant solutions to the mobility tasks described above and the\nrelevant challenges for the future. Our survey is a guide to the leading deep\nlearning solutions to next-location prediction, crowd flow prediction,\ntrajectory generation, and flow generation. At the same time, it helps deep\nlearning scientists and practitioners understand the fundamental concepts and\nthe open challenges of the study of human mobility.",
          "link": "http://arxiv.org/abs/2012.02825",
          "publishedOn": "2021-06-29T01:55:16.623Z",
          "wordCount": 654,
          "title": "A Survey on Deep Learning for Human Mobility. (arXiv:2012.02825v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14126",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_G/0/1/0/all/0/1\">Guangmeng Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Ke Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1\">Qi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yi Zhao</a>",
          "description": "In multi-party collaborative learning, the parameter server sends a global\nmodel to each data holder for local training and then aggregates committed\nmodels globally to achieve privacy protection. However, both the dragger issue\nof synchronous collaborative learning and the staleness issue of asynchronous\ncollaborative learning make collaborative learning inefficient in real-world\nheterogeneous environments. We propose a novel and efficient collaborative\nlearning framework named AdaptCL, which generates an adaptive sub-model\ndynamically from the global base model for each data holder, without any prior\ninformation about worker capability. All workers (data holders) achieve\napproximately identical update time as the fastest worker by equipping them\nwith capability-adapted pruned models. Thus the training process can be\ndramatically accelerated. Besides, we tailor the efficient pruned rate learning\nalgorithm and pruning approach for AdaptCL. Meanwhile, AdaptCL provides a\nmechanism for handling the trade-off between accuracy and time overhead and can\nbe combined with other techniques to accelerate training further. Empirical\nresults show that AdaptCL introduces little computing and communication\noverhead. AdaptCL achieves time savings of more than 41\\% on average and\nimproves accuracy in a low heterogeneous environment. In a highly heterogeneous\nenvironment, AdaptCL achieves a training speedup of 6.2x with a slight loss of\naccuracy.",
          "link": "http://arxiv.org/abs/2106.14126",
          "publishedOn": "2021-06-29T01:55:16.616Z",
          "wordCount": 641,
          "title": "AdaptCL: Efficient Collaborative Learning with Dynamic and Adaptive Pruning. (arXiv:2106.14126v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.10759",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Moharrer_A/0/1/0/all/0/1\">Armin Moharrer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamran_K/0/1/0/all/0/1\">Khashayar Kamran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeh_E/0/1/0/all/0/1\">Edmund Yeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ioannidis_S/0/1/0/all/0/1\">Stratis Ioannidis</a>",
          "description": "The mean squared error loss is widely used in many applications, including\nauto-encoders, multi-target regression, and matrix factorization, to name a\nfew. Despite computational advantages due to its differentiability, it is not\nrobust to outliers. In contrast, l_p norms are known to be robust, but cannot\nbe optimized via, e.g., stochastic gradient descent, as they are\nnon-differentiable. We propose an algorithm inspired by so-called model-based\noptimization (MBO) [35, 36], which replaces a non-convex objective with a\nconvex model function and alternates between optimizing the model function and\nupdating the solution. We apply this to robust regression, proposing SADM, a\nstochastic variant of the Online Alternating Direction Method of Multipliers\n(OADM) [50] to solve the inner optimization in MBO. We show that SADM converges\nwith the rate O(log T/T). Finally, we demonstrate experimentally (a) the\nrobustness of l_p norms to outliers and (b) the efficiency of our proposed\nmodel-based algorithms in comparison with gradient methods on autoencoders and\nmulti-target regression.",
          "link": "http://arxiv.org/abs/2106.10759",
          "publishedOn": "2021-06-29T01:55:16.608Z",
          "wordCount": 617,
          "title": "Robust Regression via Model Based Methods. (arXiv:2106.10759v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14473",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Ryck_T/0/1/0/all/0/1\">Tim De Ryck</a>, <a href=\"http://arxiv.org/find/math/1/au:+Mishra_S/0/1/0/all/0/1\">Siddhartha Mishra</a>",
          "description": "Physics informed neural networks approximate solutions of PDEs by minimizing\npointwise residuals. We derive rigorous bounds on the error, incurred by PINNs\nin approximating the solutions of a large class of linear parabolic PDEs,\nnamely Kolmogorov equations that include the heat equation and Black-Scholes\nequation of option pricing, as examples. We construct neural networks, whose\nPINN residual (generalization error) can be made as small as desired. We also\nprove that the total $L^2$-error can be bounded by the generalization error,\nwhich in turn is bounded in terms of the training error, provided that a\nsufficient number of randomly chosen training (collocation) points is used.\nMoreover, we prove that the size of the PINNs and the number of training\nsamples only grow polynomially with the underlying dimension, enabling PINNs to\novercome the curse of dimensionality in this context. These results enable us\nto provide a comprehensive error analysis for PINNs in approximating Kolmogorov\nPDEs.",
          "link": "http://arxiv.org/abs/2106.14473",
          "publishedOn": "2021-06-29T01:55:16.603Z",
          "wordCount": 600,
          "title": "Error analysis for physics informed neural networks (PINNs) approximating Kolmogorov PDEs. (arXiv:2106.14473v1 [math.NA])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14340",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Vicuna_M/0/1/0/all/0/1\">Marc Vicuna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khannouz_M/0/1/0/all/0/1\">Martin Khannouz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiar_G/0/1/0/all/0/1\">Gregory Kiar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chatelain_Y/0/1/0/all/0/1\">Yohan Chatelain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glatard_T/0/1/0/all/0/1\">Tristan Glatard</a>",
          "description": "Mondrian Forests are a powerful data stream classification method, but their\nlarge memory footprint makes them ill-suited for low-resource platforms such as\nconnected objects. We explored using reduced-precision floating-point\nrepresentations to lower memory consumption and evaluated its effect on\nclassification performance. We applied the Mondrian Forest implementation\nprovided by OrpailleCC, a C++ collection of data stream algorithms, to two\ncanonical datasets in human activity recognition: Recofit and Banos \\emph{et\nal}. Results show that the precision of floating-point values used by tree\nnodes can be reduced from 64 bits to 8 bits with no significant difference in\nF1 score. In some cases, reduced precision was shown to improve classification\nperformance, presumably due to its regularization effect. We conclude that\nnumerical precision is a relevant hyperparameter in the Mondrian Forest, and\nthat commonly-used double precision values may not be necessary for optimal\nperformance. Future work will evaluate the generalizability of these findings\nto other data stream classifiers.",
          "link": "http://arxiv.org/abs/2106.14340",
          "publishedOn": "2021-06-29T01:55:16.597Z",
          "wordCount": 611,
          "title": "Reducing numerical precision preserves classification accuracy in Mondrian Forests. (arXiv:2106.14340v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14207",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Khandakar_A/0/1/0/all/0/1\">Amith Khandakar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chowdhury_M/0/1/0/all/0/1\">Muhammad E. H. Chowdhury</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Reaz_M/0/1/0/all/0/1\">Mamun Bin Ibne Reaz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ali_S/0/1/0/all/0/1\">Sawal Hamid Md Ali</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hasan_M/0/1/0/all/0/1\">Md Anwarul Hasan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Kiranyaz_S/0/1/0/all/0/1\">Serkan Kiranyaz</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rahman_T/0/1/0/all/0/1\">Tawsifur Rahman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Alfkey_R/0/1/0/all/0/1\">Rashad Alfkey</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bakar_A/0/1/0/all/0/1\">Ahmad Ashrif A. Bakar</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Malik_R/0/1/0/all/0/1\">Rayaz A. Malik</a>",
          "description": "Diabetes foot ulceration (DFU) and amputation are a cause of significant\nmorbidity. The prevention of DFU may be achieved by the identification of\npatients at risk of DFU and the institution of preventative measures through\neducation and offloading. Several studies have reported that thermogram images\nmay help to detect an increase in plantar temperature prior to DFU. However,\nthe distribution of plantar temperature may be heterogeneous, making it\ndifficult to quantify and utilize to predict outcomes. We have compared a\nmachine learning-based scoring technique with feature selection and\noptimization techniques and learning classifiers to several state-of-the-art\nConvolutional Neural Networks (CNNs) on foot thermogram images and propose a\nrobust solution to identify the diabetic foot. A comparatively shallow CNN\nmodel, MobilenetV2 achieved an F1 score of ~95% for a two-feet thermogram\nimage-based classification and the AdaBoost Classifier used 10 features and\nachieved an F1 score of 97 %. A comparison of the inference time for the\nbest-performing networks confirmed that the proposed algorithm can be deployed\nas a smartphone application to allow the user to monitor the progression of the\nDFU in a home setting.",
          "link": "http://arxiv.org/abs/2106.14207",
          "publishedOn": "2021-06-29T01:55:16.567Z",
          "wordCount": 670,
          "title": "A Machine Learning Model for Early Detection of Diabetic Foot using Thermogram Images. (arXiv:2106.14207v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14344",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_J/0/1/0/all/0/1\">Jun Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasan_M/0/1/0/all/0/1\">Mohammad Al Hasan</a>",
          "description": "Supervised learning, while deployed in real-life scenarios, often encounters\ninstances of unknown classes. Conventional algorithms for training a supervised\nlearning model do not provide an option to detect such instances, so they\nmiss-classify such instances with 100% probability. Open Set Recognition (OSR)\nand Non-Exhaustive Learning (NEL) are potential solutions to overcome this\nproblem. Most existing methods of OSR first classify members of existing\nclasses and then identify instances of new classes. However, many of the\nexisting methods of OSR only makes a binary decision, i.e., they only identify\nthe existence of the unknown class. Hence, such methods cannot distinguish test\ninstances belonging to incremental unseen classes. On the other hand, the\nmajority of NEL methods often make a parametric assumption over the data\ndistribution, which either fail to return good results, due to the reason that\nreal-life complex datasets may not follow a well-known data distribution. In\nthis paper, we propose a new online non-exhaustive learning model, namely,\nNon-Exhaustive Gaussian Mixture Generative Adversarial Networks (NE-GM-GAN) to\naddress these issues. Our proposed model synthesizes Gaussian mixture based\nlatent representation over a deep generative model, such as GAN, for\nincremental detection of instances of emerging classes in the test data.\nExtensive experimental results on several benchmark datasets show that\nNE-GM-GAN significantly outperforms the state-of-the-art methods in detecting\ninstances of novel classes in streaming data.",
          "link": "http://arxiv.org/abs/2106.14344",
          "publishedOn": "2021-06-29T01:55:16.551Z",
          "wordCount": 653,
          "title": "Non-Exhaustive Learning Using Gaussian Mixture Generative Adversarial Networks. (arXiv:2106.14344v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2105.10358",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Shoji_T/0/1/0/all/0/1\">Taku Shoji</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yoshida_N/0/1/0/all/0/1\">Noboru Yoshida</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tanaka_T/0/1/0/all/0/1\">Toshihisa Tanaka</a>",
          "description": "Electroencephalography (EEG) is essential for the diagnosis of epilepsy, but\nit requires expertise and experience to identify abnormalities. It is thus\ncrucial to develop automated models for the detection of abnormalities in EEGs\nrelated to epilepsy. This paper describes the development of a novel class of\ncompact convolutional neural networks (CNNs) for detecting abnormal patterns\nand electrodes in EEGs for epilepsy. The designed model is inspired by a CNN\ndeveloped for brain-computer interfacing called multichannel EEGNet (mEEGNet).\nUnlike the EEGNet, the proposed model, mEEGNet, has the same number of\nelectrode inputs and outputs to detect abnormal patterns. The mEEGNet was\nevaluated with a clinical dataset consisting of 29 cases of juvenile and\nchildhood absence epilepsy labeled by a clinical expert. The labels were given\nto paroxysmal discharges visually observed in both ictal (seizure) and\ninterictal (nonseizure) durations. Results showed that the mEEGNet detected\nabnormalities with the area under the curve, F1-values, and sensitivity\nequivalent to or higher than those of existing CNNs. Moreover, the number of\nparameters is much smaller than other CNN models. To our knowledge, the dataset\nof absence epilepsy validated with machine learning through this research is\nthe largest in the literature.",
          "link": "http://arxiv.org/abs/2105.10358",
          "publishedOn": "2021-06-29T01:55:16.545Z",
          "wordCount": 666,
          "title": "Automated Detection of Abnormalities from an EEG Recording of Epilepsy Patients With a Compact Convolutional Neural Network. (arXiv:2105.10358v2 [eess.SP] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2002.12301",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ito_R/0/1/0/all/0/1\">Rei Ito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsukada_M/0/1/0/all/0/1\">Mineto Tsukada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matsutani_H/0/1/0/all/0/1\">Hiroki Matsutani</a>",
          "description": "Most edge AI focuses on prediction tasks on resource-limited edge devices\nwhile the training is done at server machines. However, retraining or\ncustomizing a model is required at edge devices as the model is becoming\noutdated due to environmental changes over time. To follow such a concept\ndrift, a neural-network based on-device learning approach is recently proposed,\nso that edge devices train incoming data at runtime to update their model. In\nthis case, since a training is done at distributed edge devices, the issue is\nthat only a limited amount of training data can be used for each edge device.\nTo address this issue, one approach is a cooperative learning or federated\nlearning, where edge devices exchange their trained results and update their\nmodel by using those collected from the other devices. In this paper, as an\non-device learning algorithm, we focus on OS-ELM (Online Sequential Extreme\nLearning Machine) to sequentially train a model based on recent samples and\ncombine it with autoencoder for anomaly detection. We extend it for an\non-device federated learning so that edge devices can exchange their trained\nresults and update their model by using those collected from the other edge\ndevices. This cooperative model update is one-shot while it can be repeatedly\napplied to synchronize their model. Our approach is evaluated with anomaly\ndetection tasks generated from a driving dataset of cars, a human activity\ndataset, and MNIST dataset. The results demonstrate that the proposed on-device\nfederated learning can produce a merged model by integrating trained results\nfrom multiple edge devices as accurately as traditional backpropagation based\nneural networks and a traditional federated learning approach with lower\ncomputation or communication cost.",
          "link": "http://arxiv.org/abs/2002.12301",
          "publishedOn": "2021-06-29T01:55:16.531Z",
          "wordCount": 786,
          "title": "An On-Device Federated Learning Approach for Cooperative Model Update between Edge Devices. (arXiv:2002.12301v5 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14232",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mufei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jinjing Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Jiajing Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_W/0/1/0/all/0/1\">Wenxuan Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yangkang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yaxin Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karypis_G/0/1/0/all/0/1\">George Karypis</a>",
          "description": "Graph neural networks (GNNs) constitute a class of deep learning methods for\ngraph data. They have wide applications in chemistry and biology, such as\nmolecular property prediction, reaction prediction and drug-target interaction\nprediction. Despite the interest, GNN-based modeling is challenging as it\nrequires graph data pre-processing and modeling in addition to programming and\ndeep learning. Here we present DGL-LifeSci, an open-source package for deep\nlearning on graphs in life science. DGL-LifeSci is a python toolkit based on\nRDKit, PyTorch and Deep Graph Library (DGL). DGL-LifeSci allows GNN-based\nmodeling on custom datasets for molecular property prediction, reaction\nprediction and molecule generation. With its command-line interfaces, users can\nperform modeling without any background in programming and deep learning. We\ntest the command-line interfaces using standard benchmarks MoleculeNet, USPTO,\nand ZINC. Compared with previous implementations, DGL-LifeSci achieves a speed\nup by up to 6x. For modeling flexibility, DGL-LifeSci provides well-optimized\nmodules for various stages of the modeling pipeline. In addition, DGL-LifeSci\nprovides pre-trained models for reproducing the test experiment results and\napplying models without training. The code is distributed under an Apache-2.0\nLicense and is freely accessible at https://github.com/awslabs/dgl-lifesci.",
          "link": "http://arxiv.org/abs/2106.14232",
          "publishedOn": "2021-06-29T01:55:16.516Z",
          "wordCount": 637,
          "title": "DGL-LifeSci: An Open-Source Toolkit for Deep Learning on Graphs in Life Science. (arXiv:2106.14232v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2010.05154",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ramanath_R/0/1/0/all/0/1\">Rohan Ramanath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Salomatin_K/0/1/0/all/0/1\">Konstantin Salomatin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gee_J/0/1/0/all/0/1\">Jeffrey D. Gee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talanine_K/0/1/0/all/0/1\">Kirill Talanine</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dalal_O/0/1/0/all/0/1\">Onkar Dalal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Polatkan_G/0/1/0/all/0/1\">Gungor Polatkan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smoot_S/0/1/0/all/0/1\">Sara Smoot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_D/0/1/0/all/0/1\">Deepak Kumar</a>",
          "description": "One of the most well-established applications of machine learning is in\ndeciding what content to show website visitors. When observation data comes\nfrom high-velocity, user-generated data streams, machine learning methods\nperform a balancing act between model complexity, training time, and\ncomputational costs. Furthermore, when model freshness is critical, the\ntraining of models becomes time-constrained. Parallelized batch offline\ntraining, although horizontally scalable, is often not time-considerate or\ncost-effective. In this paper, we propose Lambda Learner, a new framework for\ntraining models by incremental updates in response to mini-batches from data\nstreams. We show that the resulting model of our framework closely estimates a\nperiodically updated model trained on offline data and outperforms it when\nmodel updates are time-sensitive. We provide theoretical proof that the\nincremental learning updates improve the loss-function over a stale batch\nmodel. We present a large-scale deployment on the sponsored content platform\nfor a large social network, serving hundreds of millions of users across\ndifferent channels (e.g., desktop, mobile). We address challenges and\ncomplexities from both algorithms and infrastructure perspectives, and\nillustrate the system details for computation, storage, and streaming\nproduction of training data.",
          "link": "http://arxiv.org/abs/2010.05154",
          "publishedOn": "2021-06-29T01:55:16.476Z",
          "wordCount": 680,
          "title": "Lambda Learner: Fast Incremental Learning on Data Streams. (arXiv:2010.05154v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14448",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaobo Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Lijun Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Juntao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Q/0/1/0/all/0/1\">Qi Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1\">Tao Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Min Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tie-Yan Liu</a>",
          "description": "Dropout is a powerful and widely used technique to regularize the training of\ndeep neural networks. In this paper, we introduce a simple regularization\nstrategy upon dropout in model training, namely R-Drop, which forces the output\ndistributions of different sub models generated by dropout to be consistent\nwith each other. Specifically, for each training sample, R-Drop minimizes the\nbidirectional KL-divergence between the output distributions of two sub models\nsampled by dropout. Theoretical analysis reveals that R-Drop reduces the\nfreedom of the model parameters and complements dropout. Experiments on\n$\\bf{5}$ widely used deep learning tasks ($\\bf{18}$ datasets in total),\nincluding neural machine translation, abstractive summarization, language\nunderstanding, language modeling, and image classification, show that R-Drop is\nuniversally effective. In particular, it yields substantial improvements when\napplied to fine-tune large-scale pre-trained models, e.g., ViT, RoBERTa-large,\nand BART, and achieves state-of-the-art (SOTA) performances with the vanilla\nTransformer model on WMT14 English$\\to$German translation ($\\bf{30.91}$ BLEU)\nand WMT14 English$\\to$French translation ($\\bf{43.95}$ BLEU), even surpassing\nmodels trained with extra large-scale data and expert-designed advanced\nvariants of Transformer models. Our code is available at\nGitHub{\\url{https://github.com/dropreg/R-Drop}}.",
          "link": "http://arxiv.org/abs/2106.14448",
          "publishedOn": "2021-06-29T01:55:16.470Z",
          "wordCount": 616,
          "title": "R-Drop: Regularized Dropout for Neural Networks. (arXiv:2106.14448v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2102.09701",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Aounon Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1\">Tom Goldstein</a>",
          "description": "Randomized smoothing has been successfully applied to classification tasks on\nhigh-dimensional inputs, such as images, to obtain models that are provably\nrobust against adversarial perturbations of the input. We extend this technique\nto produce provable robustness for functions that map inputs into an arbitrary\nmetric space rather than discrete classes. Such functions are used in many\nmachine learning problems like image reconstruction, dimensionality reduction,\nfacial recognition, etc. Our robustness certificates guarantee that the change\nin the output of the smoothed model as measured by the distance metric remains\nsmall for any norm-bounded perturbation of the input. We can certify robustness\nunder a variety of different output metrics, such as total variation distance,\nJaccard distance, perceptual metrics, etc. In our experiments, we apply our\nprocedure to create certifiably robust models with disparate output spaces --\nfrom sets to images -- and show that it yields meaningful certificates without\nsignificantly degrading the performance of the base model. The code for our\nexperiments is available at: https://github.com/aounon/center-smoothing.",
          "link": "http://arxiv.org/abs/2102.09701",
          "publishedOn": "2021-06-29T01:55:16.459Z",
          "wordCount": 620,
          "title": "Center Smoothing: Provable Robustness for Functions with Metric-Space Outputs. (arXiv:2102.09701v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.06506",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Joon Sik Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plumb_G/0/1/0/all/0/1\">Gregory Plumb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talwalkar_A/0/1/0/all/0/1\">Ameet Talwalkar</a>",
          "description": "Saliency methods are a popular class of feature attribution tools that aim to\ncapture a model's predictive reasoning by identifying \"important\" pixels in an\ninput image. However, the development and adoption of saliency methods are\ncurrently hindered by the lack of access to underlying model reasoning, which\nprevents accurate method evaluation. In this work, we design a synthetic\nevaluation framework, SMERF, that allows us to perform ground-truth-based\nevaluation of saliency methods while controlling the underlying complexity of\nmodel reasoning. Experimental evaluations via SMERF reveal significant\nlimitations in existing saliency methods, especially given the relative\nsimplicity of SMERF's synthetic evaluation tasks. Moreover, the SMERF\nbenchmarking suite represents a useful tool in the development of new saliency\nmethods to potentially overcome these limitations.",
          "link": "http://arxiv.org/abs/2105.06506",
          "publishedOn": "2021-06-29T01:55:16.381Z",
          "wordCount": 573,
          "title": "Sanity Simulations for Saliency Methods. (arXiv:2105.06506v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2009.01040",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Khanmohammadi_R/0/1/0/all/0/1\">Reza Khanmohammadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mirroshandel_S/0/1/0/all/0/1\">Seyed Abolghasem Mirroshandel</a>",
          "description": "Recent developments in Text Style Transfer have led this field to be more\nhighlighted than ever. The task of transferring an input's style to another is\naccompanied by plenty of challenges (e.g., fluency and content preservation)\nthat need to be taken care of. In this research, we introduce PGST, a novel\npolyglot text style transfer approach in the gender domain, composed of\ndifferent constitutive elements. In contrast to prior studies, it is feasible\nto apply a style transfer method in multiple languages by fulfilling our\nmethod's predefined elements. We have proceeded with a pre-trained word\nembedding for token replacement purposes, a character-based token classifier\nfor gender exchange purposes, and a beam search algorithm for extracting the\nmost fluent combination. Since different approaches are introduced in our\nresearch, we determine a trade-off value for evaluating different models'\nsuccess in faking our gender identification model with transferred text. To\ndemonstrate our method's multilingual applicability, we applied our method on\nboth English and Persian corpora and ended up defeating our proposed gender\nidentification model by 45.6% and 39.2%, respectively. While this research's\nfocus is not limited to a specific language, our obtained evaluation results\nare highly competitive in an analogy among English state of the art methods.",
          "link": "http://arxiv.org/abs/2009.01040",
          "publishedOn": "2021-06-29T01:55:16.365Z",
          "wordCount": 673,
          "title": "PGST: a Polyglot Gender Style Transfer method. (arXiv:2009.01040v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2012.00073",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bento_J/0/1/0/all/0/1\">Jo&#xe3;o Bento</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saleiro_P/0/1/0/all/0/1\">Pedro Saleiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cruz_A/0/1/0/all/0/1\">Andr&#xe9; F. Cruz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Figueiredo_M/0/1/0/all/0/1\">M&#xe1;rio A.T. Figueiredo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bizarro_P/0/1/0/all/0/1\">Pedro Bizarro</a>",
          "description": "Although recurrent neural networks (RNNs) are state-of-the-art in numerous\nsequential decision-making tasks, there has been little research on explaining\ntheir predictions. In this work, we present TimeSHAP, a model-agnostic\nrecurrent explainer that builds upon KernelSHAP and extends it to the\nsequential domain. TimeSHAP computes feature-, timestep-, and cell-level\nattributions. As sequences may be arbitrarily long, we further propose a\npruning method that is shown to dramatically decrease both its computational\ncost and the variance of its attributions. We use TimeSHAP to explain the\npredictions of a real-world bank account takeover fraud detection RNN model,\nand draw key insights from its explanations: i) the model identifies important\nfeatures and events aligned with what fraud analysts consider cues for account\ntakeover; ii) positive predicted sequences can be pruned to only 10% of the\noriginal length, as older events have residual attribution values; iii) the\nmost recent input event of positive predictions only contributes on average to\n41% of the model's score; iv) notably high attribution to client's age,\nsuggesting a potential discriminatory reasoning, later confirmed as higher\nfalse positive rates for older clients.",
          "link": "http://arxiv.org/abs/2012.00073",
          "publishedOn": "2021-06-29T01:55:16.359Z",
          "wordCount": 653,
          "title": "TimeSHAP: Explaining Recurrent Models through Sequence Perturbations. (arXiv:2012.00073v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14421",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Gasse_M/0/1/0/all/0/1\">Maxime Gasse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grasset_D/0/1/0/all/0/1\">Damien Grasset</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaudron_G/0/1/0/all/0/1\">Guillaume Gaudron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oudeyer_P/0/1/0/all/0/1\">Pierre-Yves Oudeyer</a>",
          "description": "Learning efficiently a causal model of the environment is a key challenge of\nmodel-based RL agents operating in POMDPs. We consider here a scenario where\nthe learning agent has the ability to collect online experiences through direct\ninteractions with the environment (interventional data), but has also access to\na large collection of offline experiences, obtained by observing another agent\ninteracting with the environment (observational data). A key ingredient, that\nmakes this situation non-trivial, is that we allow the observed agent to\ninteract with the environment based on hidden information, which is not\nobserved by the learning agent. We then ask the following questions: can the\nonline and offline experiences be safely combined for learning a causal model ?\nAnd can we expect the offline experiences to improve the agent's performances ?\nTo answer these questions, we import ideas from the well-established causal\nframework of do-calculus, and we express model-based reinforcement learning as\na causal inference problem. Then, we propose a general yet simple methodology\nfor leveraging offline data during learning. In a nutshell, the method relies\non learning a latent-based causal transition model that explains both the\ninterventional and observational regimes, and then using the recovered latent\nvariable to infer the standard POMDP transition model via deconfounding. We\nprove our method is correct and efficient in the sense that it attains better\ngeneralization guarantees due to the offline data (in the asymptotic case), and\nwe illustrate its effectiveness empirically on synthetic toy problems. Our\ncontribution aims at bridging the gap between the fields of reinforcement\nlearning and causality.",
          "link": "http://arxiv.org/abs/2106.14421",
          "publishedOn": "2021-06-29T01:55:16.353Z",
          "wordCount": 687,
          "title": "Causal Reinforcement Learning using Observational and Interventional Data. (arXiv:2106.14421v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14031",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1\">Xu Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hongshen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yonghao Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xiaofang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1\">Zhuoye Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zhen He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Long_B/0/1/0/all/0/1\">Bo Long</a>",
          "description": "Most sequential recommendation models capture the features of consecutive\nitems in a user-item interaction history. Though effective, their\nrepresentation expressiveness is still hindered by the sparse learning signals.\nAs a result, the sequential recommender is prone to make inconsistent\npredictions. In this paper, we propose a model, \\textbf{SSI}, to improve\nsequential recommendation consistency with Self-Supervised Imitation.\nPrecisely, we extract the consistency knowledge by utilizing three\nself-supervised pre-training tasks, where temporal consistency and persona\nconsistency capture user-interaction dynamics in terms of the chronological\norder and persona sensitivities, respectively. Furthermore, to provide the\nmodel with a global perspective, global session consistency is introduced by\nmaximizing the mutual information among global and local interaction sequences.\nFinally, to comprehensively take advantage of all three independent aspects of\nconsistency-enhanced knowledge, we establish an integrated imitation learning\nframework. The consistency knowledge is effectively internalized and\ntransferred to the student model by imitating the conventional prediction logit\nas well as the consistency-enhanced item representations. In addition, the\nflexible self-supervised imitation framework can also benefit other student\nrecommenders. Experiments on four real-world datasets show that SSI effectively\noutperforms the state-of-the-art sequential recommendation methods.",
          "link": "http://arxiv.org/abs/2106.14031",
          "publishedOn": "2021-06-29T01:55:16.347Z",
          "wordCount": 623,
          "title": "Improving Sequential Recommendation Consistency with Self-Supervised Imitation. (arXiv:2106.14031v1 [cs.IR])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14257",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Gupta_K/0/1/0/all/0/1\">Kanhaiya Gupta</a>",
          "description": "In recent years, artificial neural networks (ANNs) have won numerous contests\nin pattern recognition and machine learning. ANNS have been applied to problems\nranging from speech recognition to prediction of protein secondary structure,\nclassification of cancers, and gene prediction. Here, we intend to maximize the\nchances of finding the Higgs boson decays to two $\\tau$ leptons in the pseudo\ndataset using a Machine Learning technique to classify the recorded events as\nsignal or background.",
          "link": "http://arxiv.org/abs/2106.14257",
          "publishedOn": "2021-06-29T01:55:16.341Z",
          "wordCount": 528,
          "title": "Use of Machine Learning Technique to maximize the signal over background for $H \\rightarrow \\tau \\tau$. (arXiv:2106.14257v1 [physics.data-an])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13897",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dandi_Y/0/1/0/all/0/1\">Yatin Dandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barba_L/0/1/0/all/0/1\">Luis Barba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jaggi_M/0/1/0/all/0/1\">Martin Jaggi</a>",
          "description": "A major obstacle to achieving global convergence in distributed and federated\nlearning is the misalignment of gradients across clients, or mini-batches due\nto heterogeneity and stochasticity of the distributed data. One way to\nalleviate this problem is to encourage the alignment of gradients across\ndifferent clients throughout training. Our analysis reveals that this goal can\nbe accomplished by utilizing the right optimization method that replicates the\nimplicit regularization effect of SGD, leading to gradient alignment as well as\nimprovements in test accuracies. Since the existence of this regularization in\nSGD completely relies on the sequential use of different mini-batches during\ntraining, it is inherently absent when training with large mini-batches. To\nobtain the generalization benefits of this regularization while increasing\nparallelism, we propose a novel GradAlign algorithm that induces the same\nimplicit regularization while allowing the use of arbitrarily large batches in\neach update. We experimentally validate the benefit of our algorithm in\ndifferent distributed and federated learning settings.",
          "link": "http://arxiv.org/abs/2106.13897",
          "publishedOn": "2021-06-29T01:55:16.327Z",
          "wordCount": 592,
          "title": "Implicit Gradient Alignment in Distributed and Federated Learning. (arXiv:2106.13897v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2103.01846",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Buyl_M/0/1/0/all/0/1\">Maarten Buyl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bie_T/0/1/0/all/0/1\">Tijl De Bie</a>",
          "description": "Learning and reasoning over graphs is increasingly done by means of\nprobabilistic models, e.g. exponential random graph models, graph embedding\nmodels, and graph neural networks. When graphs are modeling relations between\npeople, however, they will inevitably reflect biases, prejudices, and other\nforms of inequity and inequality. An important challenge is thus to design\naccurate graph modeling approaches while guaranteeing fairness according to the\nspecific notion of fairness that the problem requires. Yet, past work on the\ntopic remains scarce, is limited to debiasing specific graph modeling methods,\nand often aims to ensure fairness in an indirect manner.\n\nWe propose a generic approach applicable to most probabilistic graph modeling\napproaches. Specifically, we first define the class of fair graph models\ncorresponding to a chosen set of fairness criteria. Given this, we propose a\nfairness regularizer defined as the KL-divergence between the graph model and\nits I-projection onto the set of fair models. We demonstrate that using this\nfairness regularizer in combination with existing graph modeling approaches\nefficiently trades-off fairness with accuracy, whereas the state-of-the-art\nmodels can only make this trade-off for the fairness criterion that they were\nspecifically designed for.",
          "link": "http://arxiv.org/abs/2103.01846",
          "publishedOn": "2021-06-29T01:55:16.321Z",
          "wordCount": 655,
          "title": "The KL-Divergence between a Graph Model and its Fair I-Projection as a Fairness Regularizer. (arXiv:2103.01846v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14274",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhiqin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tagliasacchi_A/0/1/0/all/0/1\">Andrea Tagliasacchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>",
          "description": "Polygonal meshes are ubiquitous, but have only played a relatively minor role\nin the deep learning revolution. State-of-the-art neural generative models for\n3D shapes learn implicit functions and generate meshes via expensive\niso-surfacing. We overcome these challenges by employing a classical spatial\ndata structure from computer graphics, Binary Space Partitioning (BSP), to\nfacilitate 3D learning. The core operation of BSP involves recursive\nsubdivision of 3D space to obtain convex sets. By exploiting this property, we\ndevise BSP-Net, a network that learns to represent a 3D shape via convex\ndecomposition without supervision. The network is trained to reconstruct a\nshape using a set of convexes obtained from a BSP-tree built over a set of\nplanes, where the planes and convexes are both defined by learned network\nweights. BSP-Net directly outputs polygonal meshes from the inferred convexes.\nThe generated meshes are watertight, compact (i.e., low-poly), and well suited\nto represent sharp geometry. We show that the reconstruction quality by BSP-Net\nis competitive with those from state-of-the-art methods while using much fewer\nprimitives. We also explore variations to BSP-Net including using a more\ngeneric decoder for reconstruction, more general primitives than planes, as\nwell as training a generative model with variational auto-encoders. Code is\navailable at https://github.com/czq142857/BSP-NET-original.",
          "link": "http://arxiv.org/abs/2106.14274",
          "publishedOn": "2021-06-29T01:55:16.313Z",
          "wordCount": 663,
          "title": "Learning Mesh Representations via Binary Space Partitioning Tree Networks. (arXiv:2106.14274v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.02800",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Q/0/1/0/all/0/1\">Qian Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sampani_K/0/1/0/all/0/1\">Konstantina Sampani</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_M/0/1/0/all/0/1\">Mengjia Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Cai_S/0/1/0/all/0/1\">Shengze Cai</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Deng_Y/0/1/0/all/0/1\">Yixiang Deng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1\">He Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Sun_J/0/1/0/all/0/1\">Jennifer K. Sun</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Karniadakis_G/0/1/0/all/0/1\">George Em Karniadakis</a>",
          "description": "Microaneurysms (MAs) are one of the earliest signs of diabetic retinopathy\n(DR), a frequent complication of diabetes that can lead to visual impairment\nand blindness. Adaptive optics scanning laser ophthalmoscopy (AOSLO) provides\nreal-time retinal images with resolution down to 2 $\\mu m$ and thus allows\ndetection of the morphologies of individual MAs, a potential marker that might\ndictate MA pathology and affect the progression of DR. In contrast to the\nnumerous automatic models developed for assessing the number of MAs on fundus\nphotographs, currently there is no high throughput image protocol available for\nautomatic analysis of AOSLO photographs. To address this urgency, we introduce\nAOSLO-net, a deep neural network framework with customized training policies to\nautomatically segment MAs from AOSLO images. We evaluate the performance of\nAOSLO-net using 87 DR AOSLO images and our results demonstrate that the\nproposed model outperforms the state-of-the-art segmentation model both in\naccuracy and cost and enables correct MA morphological classification.",
          "link": "http://arxiv.org/abs/2106.02800",
          "publishedOn": "2021-06-29T01:55:16.308Z",
          "wordCount": 651,
          "title": "AOSLO-net: A deep learning-based method for automatic segmentation of retinal microaneurysms from adaptive optics scanning laser ophthalmoscope images. (arXiv:2106.02800v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07597",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Banbury_C/0/1/0/all/0/1\">Colby Banbury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddi_V/0/1/0/all/0/1\">Vijay Janapa Reddi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torelli_P/0/1/0/all/0/1\">Peter Torelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holleman_J/0/1/0/all/0/1\">Jeremy Holleman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeffries_N/0/1/0/all/0/1\">Nat Jeffries</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiraly_C/0/1/0/all/0/1\">Csaba Kiraly</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Montino_P/0/1/0/all/0/1\">Pietro Montino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanter_D/0/1/0/all/0/1\">David Kanter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_S/0/1/0/all/0/1\">Sebastian Ahmed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pau_D/0/1/0/all/0/1\">Danilo Pau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thakker_U/0/1/0/all/0/1\">Urmish Thakker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torrini_A/0/1/0/all/0/1\">Antonio Torrini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Warden_P/0/1/0/all/0/1\">Peter Warden</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cordaro_J/0/1/0/all/0/1\">Jay Cordaro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guglielmo_G/0/1/0/all/0/1\">Giuseppe Di Guglielmo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duarte_J/0/1/0/all/0/1\">Javier Duarte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gibellini_S/0/1/0/all/0/1\">Stephen Gibellini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parekh_V/0/1/0/all/0/1\">Videet Parekh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_H/0/1/0/all/0/1\">Honson Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_N/0/1/0/all/0/1\">Nhan Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wenxu_N/0/1/0/all/0/1\">Niu Wenxu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xuesong_X/0/1/0/all/0/1\">Xu Xuesong</a>",
          "description": "Advancements in ultra-low-power tiny machine learning (TinyML) systems\npromise to unlock an entirely new class of smart applications. However,\ncontinued progress is limited by the lack of a widely accepted and easily\nreproducible benchmark for these systems. To meet this need, we present MLPerf\nTiny, the first industry-standard benchmark suite for ultra-low-power tiny\nmachine learning systems. The benchmark suite is the collaborative effort of\nmore than 50 organizations from industry and academia and reflects the needs of\nthe community. MLPerf Tiny measures the accuracy, latency, and energy of\nmachine learning inference to properly evaluate the tradeoffs between systems.\nAdditionally, MLPerf Tiny implements a modular design that enables benchmark\nsubmitters to show the benefits of their product, regardless of where it falls\non the ML deployment stack, in a fair and reproducible manner. The suite\nfeatures four benchmarks: keyword spotting, visual wake words, image\nclassification, and anomaly detection.",
          "link": "http://arxiv.org/abs/2106.07597",
          "publishedOn": "2021-06-29T01:55:16.297Z",
          "wordCount": 629,
          "title": "MLPerf Tiny Benchmark. (arXiv:2106.07597v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1904.07272",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Slivkins_A/0/1/0/all/0/1\">Aleksandrs Slivkins</a>",
          "description": "Multi-armed bandits a simple but very powerful framework for algorithms that\nmake decisions over time under uncertainty. An enormous body of work has\naccumulated over the years, covered in several books and surveys. This book\nprovides a more introductory, textbook-like treatment of the subject. Each\nchapter tackles a particular line of work, providing a self-contained,\nteachable technical introduction and a brief review of the further\ndevelopments; many of the chapters conclude with exercises.\n\nThe book is structured as follows. The first four chapters are on IID\nrewards, from the basic model to impossibility results to Bayesian priors to\nLipschitz rewards. The next three chapters cover adversarial rewards, from the\nfull-feedback version to adversarial bandits to extensions with linear rewards\nand combinatorially structured actions. Chapter 8 is on contextual bandits, a\nmiddle ground between IID and adversarial bandits in which the change in reward\ndistributions is completely explained by observable contexts. The last three\nchapters cover connections to economics, from learning in repeated games to\nbandits with supply/budget constraints to exploration in the presence of\nincentives. The appendix provides sufficient background on concentration and\nKL-divergence.\n\nThe chapters on \"bandits with similarity information\", \"bandits with\nknapsacks\" and \"bandits and agents\" can also be consumed as standalone surveys\non the respective topics.",
          "link": "http://arxiv.org/abs/1904.07272",
          "publishedOn": "2021-06-29T01:55:16.281Z",
          "wordCount": 748,
          "title": "Introduction to Multi-Armed Bandits. (arXiv:1904.07272v6 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14186",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Ferla_M/0/1/0/all/0/1\">Michele La Ferla</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Montebello_M/0/1/0/all/0/1\">Matthew Montebello</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Seychell_D/0/1/0/all/0/1\">Dylan Seychell</a>",
          "description": "During the last decade or so, there has been an insurgence in the deep\nlearning community to solve health-related issues, particularly breast cancer.\nFollowing the Camelyon-16 challenge in 2016, several researchers have dedicated\ntheir time to build Convolutional Neural Networks (CNNs) to help radiologists\nand other clinicians diagnose breast cancer. In particular, there has been an\nemphasis on Ductal Carcinoma in Situ (DCIS); the clinical term for early-stage\nbreast cancer. Large companies have given their fair share of research into\nthis subject, among these Google Deepmind who developed a model in 2020 that\nhas proven to be better than radiologists themselves to diagnose breast cancer\ncorrectly.\n\nWe found that among the issues which exist, there is a need for an\nexplanatory system that goes through the hidden layers of a CNN to highlight\nthose pixels that contributed to the classification of a mammogram. We then\nchose an open-source, reasonably successful project developed by Prof. Shen,\nusing the CBIS-DDSM image database to run our experiments on. It was later\nimproved using the Resnet-50 and VGG-16 patch-classifiers, analytically\ncomparing the outcome of both. The results showed that the Resnet-50 one\nconverged earlier in the experiments.\n\nFollowing the research by Montavon and Binder, we used the DeepTaylor\nLayer-wise Relevance Propagation (LRP) model to highlight those pixels and\nregions within a mammogram which contribute most to its classification. This is\nrepresented as a map of those pixels in the original image, which contribute to\nthe diagnosis and the extent to which they contribute to the final\nclassification. The most significant advantage of this algorithm is that it\nperforms exceptionally well with the Resnet-50 patch classifier architecture.",
          "link": "http://arxiv.org/abs/2106.14186",
          "publishedOn": "2021-06-29T01:55:16.275Z",
          "wordCount": 739,
          "title": "An XAI Approach to Deep Learning Models in the Detection of Ductal Carcinoma in Situ. (arXiv:2106.14186v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2102.12254",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chhablani_G/0/1/0/all/0/1\">Gunjan Chhablani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Abheesht Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pandey_H/0/1/0/all/0/1\">Harshit Pandey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhartia_Y/0/1/0/all/0/1\">Yash Bhartia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suthaharan_S/0/1/0/all/0/1\">Shan Suthaharan</a>",
          "description": "Toxicity detection of text has been a popular NLP task in the recent years.\nIn SemEval-2021 Task-5 Toxic Spans Detection, the focus is on detecting toxic\nspans within passages. Most state-of-the-art span detection approaches employ\nvarious techniques, each of which can be broadly classified into Token\nClassification or Span Prediction approaches. In our paper, we explore simple\nversions of both of these approaches and their performance on the task.\nSpecifically, we use BERT-based models -- BERT, RoBERTa, and SpanBERT for both\napproaches. We also combine these approaches and modify them to bring\nimprovements for Toxic Spans prediction. To this end, we investigate results on\nfour hybrid approaches -- Multi-Span, Span+Token, LSTM-CRF, and a combination\nof predicted offsets using union/intersection. Additionally, we perform a\nthorough ablative analysis and analyze our observed results. Our best\nsubmission -- a combination of SpanBERT Span Predictor and RoBERTa Token\nClassifier predictions -- achieves an F1 score of 0.6753 on the test set. Our\nbest post-eval F1 score is 0.6895 on intersection of predicted offsets from\ntop-3 RoBERTa Token Classification checkpoints. These approaches improve the\nperformance by 3% on average than those of the shared baseline models -- RNNSL\nand SpaCy NER.",
          "link": "http://arxiv.org/abs/2102.12254",
          "publishedOn": "2021-06-29T01:55:16.264Z",
          "wordCount": 685,
          "title": "NLRG at SemEval-2021 Task 5: Toxic Spans Detection Leveraging BERT-based Token Classification and Span Prediction Techniques. (arXiv:2102.12254v2 [cs.CL] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.08671",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Saeed_W/0/1/0/all/0/1\">Waddah Saeed</a>",
          "description": "Short Message Service (SMS) is a very popular service used for communication\nby mobile users. However, this popular service can be abused by executing\nillegal activities and influencing security risks. Nowadays, many automatic\nmachine learning (AutoML) tools exist which can help domain experts and lay\nusers to build high-quality ML models with little or no machine learning\nknowledge. In this work, a classification performance comparison was conducted\nbetween three automatic ML tools for SMS spam message filtering. These tools\nare mljar-supervised AutoML, H2O AutoML, and Tree-based Pipeline Optimization\nTool (TPOT) AutoML. Experimental results showed that ensemble models achieved\nthe best classification performance. The Stacked Ensemble model, which was\nbuilt using H2O AutoML, achieved the best performance in terms of Log Loss\n(0.8370), true positive (1088/1116), and true negative (281/287) metrics. There\nis a 19.05\\% improvement in Log Loss with respect to TPOT AutoML and 5.56\\%\nimprovement with respect to mljar-supervised AutoML. The satisfactory filtering\nperformance achieved with AutoML tools provides a potential application for\nAutoML tools to automatically determine the best ML model that can perform best\nfor SMS spam message filtering.",
          "link": "http://arxiv.org/abs/2106.08671",
          "publishedOn": "2021-06-29T01:55:16.250Z",
          "wordCount": 633,
          "title": "Comparison of Automated Machine Learning Tools for SMS Spam Message Filtering. (arXiv:2106.08671v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.07566",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wang_F/0/1/0/all/0/1\">Fanyi Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Hu_H/0/1/0/all/0/1\">Haotian Hu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shen_C/0/1/0/all/0/1\">Cheng Shen</a>",
          "description": "Attention mechanism has shown enormous potential for single image\nsuper-resolution (SISR). However, existing works only proposed some attention\nmechanism for a specific network. A universal attention mechanism for SISR,\nwhich could further improve the performance of networks without attention and\nprovide a baseline for networks with attention, is still lacking. To fit this\ngap, we propose a lightweight and efficient Balanced Attention Mechanism (BAM),\nwhich consists of Avgpool Channel Attention Module (ACAM) and Maxpool Spatial\nAttention Module (MSAM) in parallel. The information extraction mechanism of\nACAM and MSAM effectively filters redundant information, making the overall\nstructure of BAM very lightweight. Owing to the parallel structure, during the\ngradient backpropagation process of BAM, ACAM and MSAM not only conduct\nself-optimization, but also mutual optimization so as to generate more balanced\nattention information. To verify the effectiveness and robustness of BAM, we\napplied it to 12 state-ofthe-art SISR networks. The results on 4 benchmark\ndatasets demonstrate that BAM can efficiently improve the networks'\nperformance, and for those with attention, the substitution with BAM further\nreduces the amount of parameters and increase the inference speed. Moreover,\nablation experiments were conducted to prove the minimalism of BAM.",
          "link": "http://arxiv.org/abs/2104.07566",
          "publishedOn": "2021-06-29T01:55:16.244Z",
          "wordCount": 670,
          "title": "BAM: A Lightweight and Efficient Balanced Attention Mechanism for Single Image Super Resolution. (arXiv:2104.07566v2 [eess.IV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2011.14870",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Henriques_L/0/1/0/all/0/1\">Luis Felipe M.O. Henriques</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morgan_E/0/1/0/all/0/1\">Eduardo Morgan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Colcher_S/0/1/0/all/0/1\">Sergio Colcher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milidiu_R/0/1/0/all/0/1\">Ruy Luiz Milidi&#xfa;</a>",
          "description": "Non-Intrusive Load Monitoring (NILM) is a computational technique to estimate\nthe power loads' appliance-by-appliance from the whole consumption measured by\na single meter. In this paper, we propose a conditional density estimation\nmodel, based on deep neural networks, that joins a Conditional Variational\nAutoencoder with a Conditional Invertible Normalizing Flow model to estimate\nthe individual appliance's power demand. The resulting model is called Prior\nFlow Variational Autoencoder or, for simplicity PFVAE. Thus, instead of having\none model per appliance, the resulting model is responsible for estimating the\npower demand, appliance-by-appliance, at once. We train and evaluate our\nproposed model in a publicly available dataset composed of power demand\nmeasures from a poultry feed factory located in Brazil. The proposed model's\nquality is evaluated by comparing the obtained normalized disaggregation error\n(NDE) and signal aggregated error (SAE) with the previous work values on the\nsame dataset. Our proposal achieves highly competitive results, and for six of\nthe eight machines belonging to the dataset, we observe consistent improvements\nthat go from 28% up to 81% in NDE and from 27% up to 86% in SAE.",
          "link": "http://arxiv.org/abs/2011.14870",
          "publishedOn": "2021-06-29T01:55:16.238Z",
          "wordCount": 663,
          "title": "Prior Flow Variational Autoencoder: A density estimation model for Non-Intrusive Load Monitoring. (arXiv:2011.14870v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13948",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Francis_J/0/1/0/all/0/1\">Jonathan Francis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kitamura_N/0/1/0/all/0/1\">Nariaki Kitamura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Labelle_F/0/1/0/all/0/1\">Felix Labelle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Xiaopeng Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Navarro_I/0/1/0/all/0/1\">Ingrid Navarro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1\">Jean Oh</a>",
          "description": "Recent advances in the areas of multimodal machine learning and artificial\nintelligence (AI) have led to the development of challenging tasks at the\nintersection of Computer Vision, Natural Language Processing, and Embodied AI.\nWhereas many approaches and previous survey pursuits have characterised one or\ntwo of these dimensions, there has not been a holistic analysis at the center\nof all three. Moreover, even when combinations of these topics are considered,\nmore focus is placed on describing, e.g., current architectural methods, as\nopposed to also illustrating high-level challenges and opportunities for the\nfield. In this survey paper, we discuss Embodied Vision-Language Planning\n(EVLP) tasks, a family of prominent embodied navigation and manipulation\nproblems that jointly use computer vision and natural language. We propose a\ntaxonomy to unify these tasks and provide an in-depth analysis and comparison\nof the new and current algorithmic approaches, metrics, simulated environments,\nas well as the datasets used for EVLP tasks. Finally, we present the core\nchallenges that we believe new EVLP works should seek to address, and we\nadvocate for task construction that enables model generalizability and furthers\nreal-world deployment.",
          "link": "http://arxiv.org/abs/2106.13948",
          "publishedOn": "2021-06-29T01:55:16.232Z",
          "wordCount": 628,
          "title": "Core Challenges in Embodied Vision-Language Planning. (arXiv:2106.13948v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14308",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chandak_S/0/1/0/all/0/1\">Siddharth Chandak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Borkar_V/0/1/0/all/0/1\">Vivek S. Borkar</a>",
          "description": "Using a martingale concentration inequality, concentration bounds `from time\n$n_0$ on' are derived for stochastic approximation algorithms with contractive\nmaps and both martingale difference and Markov noises. These are applied to\nreinforcement learning algorithms, in particular to asynchronous Q-learning and\nTD(0).",
          "link": "http://arxiv.org/abs/2106.14308",
          "publishedOn": "2021-06-29T01:55:16.227Z",
          "wordCount": 481,
          "title": "Concentration of Contractive Stochastic Approximation and Reinforcement Learning. (arXiv:2106.14308v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.09994",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Muzellec_B/0/1/0/all/0/1\">Boris Muzellec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bach_F/0/1/0/all/0/1\">Francis Bach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rudi_A/0/1/0/all/0/1\">Alessandro Rudi</a>",
          "description": "Kernel mean embeddings are a popular tool that consists in representing\nprobability measures by their infinite-dimensional mean embeddings in a\nreproducing kernel Hilbert space. When the kernel is characteristic, mean\nembeddings can be used to define a distance between probability measures, known\nas the maximum mean discrepancy (MMD). A well-known advantage of mean\nembeddings and MMD is their low computational cost and low sample complexity.\nHowever, kernel mean embeddings have had limited applications to problems that\nconsist in optimizing distributions, due to the difficulty of characterizing\nwhich Hilbert space vectors correspond to a probability distribution. In this\nnote, we propose to leverage the kernel sums-of-squares parameterization of\npositive functions of Marteau-Ferey et al. [2020] to fit distributions in the\nMMD geometry. First, we show that when the kernel is characteristic,\ndistributions with a kernel sum-of-squares density are dense. Then, we provide\nalgorithms to optimize such distributions in the finite-sample setting, which\nwe illustrate in a density fitting numerical experiment.",
          "link": "http://arxiv.org/abs/2106.09994",
          "publishedOn": "2021-06-29T01:55:16.221Z",
          "wordCount": 610,
          "title": "A Note on Optimizing Distributions using Kernel Mean Embeddings. (arXiv:2106.09994v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.10293",
          "author": "<a href=\"http://arxiv.org/find/quant-ph/1/au:+Sharir_O/0/1/0/all/0/1\">Or Sharir</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Shashua_A/0/1/0/all/0/1\">Amnon Shashua</a>, <a href=\"http://arxiv.org/find/quant-ph/1/au:+Carleo_G/0/1/0/all/0/1\">Giuseppe Carleo</a>",
          "description": "We establish a direct connection between general tensor networks and deep\nfeed-forward artificial neural networks. The core of our results is the\nconstruction of neural-network layers that efficiently perform tensor\ncontractions, and that use commonly adopted non-linear activation functions.\nThe resulting deep networks feature a number of edges that closely matches the\ncontraction complexity of the tensor networks to be approximated. In the\ncontext of many-body quantum states, this result establishes that\nneural-network states have strictly the same or higher expressive power than\npractically usable variational tensor networks. As an example, we show that all\nmatrix product states can be efficiently written as neural-network states with\na number of edges polynomial in the bond dimension and depth logarithmic in the\nsystem size. The opposite instead does not hold true, and our results imply\nthat there exist quantum states that are not efficiently expressible in terms\nof matrix product states or practically usable PEPS, but that are instead\nefficiently expressible with neural network states.",
          "link": "http://arxiv.org/abs/2103.10293",
          "publishedOn": "2021-06-29T01:55:16.210Z",
          "wordCount": 625,
          "title": "Neural tensor contractions and the expressive power of deep neural quantum states. (arXiv:2103.10293v2 [quant-ph] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.04621",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bashivan_P/0/1/0/all/0/1\">Pouya Bashivan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bayat_R/0/1/0/all/0/1\">Reza Bayat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ibrahim_A/0/1/0/all/0/1\">Adam Ibrahim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahuja_K/0/1/0/all/0/1\">Kartik Ahuja</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faramarzi_M/0/1/0/all/0/1\">Mojtaba Faramarzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laleh_T/0/1/0/all/0/1\">Touraj Laleh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Richards_B/0/1/0/all/0/1\">Blake Aaron Richards</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rish_I/0/1/0/all/0/1\">Irina Rish</a>",
          "description": "Neural networks are known to be vulnerable to adversarial attacks -- slight\nbut carefully constructed perturbations of the inputs which can drastically\nimpair the network's performance. Many defense methods have been proposed for\nimproving robustness of deep networks by training them on adversarially\nperturbed inputs. However, these models often remain vulnerable to new types of\nattacks not seen during training, and even to slightly stronger versions of\npreviously seen attacks. In this work, we propose a novel approach to\nadversarial robustness, which builds upon the insights from the domain\nadaptation field. Our method, called Adversarial Feature Desensitization (AFD),\naims at learning features that are invariant towards adversarial perturbations\nof the inputs. This is achieved through a game where we learn features that are\nboth predictive and robust (insensitive to adversarial attacks), i.e. cannot be\nused to discriminate between natural and adversarial data. Empirical results on\nseveral benchmarks demonstrate the effectiveness of the proposed approach\nagainst a wide range of attack types and attack strengths.",
          "link": "http://arxiv.org/abs/2006.04621",
          "publishedOn": "2021-06-29T01:55:16.143Z",
          "wordCount": 630,
          "title": "Adversarial Feature Desensitization. (arXiv:2006.04621v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.17236",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+He_Z/0/1/0/all/0/1\">Zichang He</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>",
          "description": "Fabrication process variations can significantly influence the performance\nand yield of nano-scale electronic and photonic circuits. Stochastic spectral\nmethods have achieved great success in quantifying the impact of process\nvariations, but they suffer from the curse of dimensionality. Recently,\nlow-rank tensor methods have been developed to mitigate this issue, but two\nfundamental challenges remain open: how to automatically determine the tensor\nrank and how to adaptively pick the informative simulation samples. This paper\nproposes a novel tensor regression method to address these two challenges. We\nuse a $\\ell_{q}/ \\ell_{2}$ group-sparsity regularization to determine the\ntensor rank. The resulting optimization problem can be efficiently solved via\nan alternating minimization solver. We also propose a two-stage adaptive\nsampling method to reduce the simulation cost. Our method considers both\nexploration and exploitation via the estimated Voronoi cell volume and\nnonlinearity measurement respectively. The proposed model is verified with\nsynthetic and some realistic circuit benchmarks, on which our method can well\ncapture the uncertainty caused by 19 to 100 random variables with only 100 to\n600 simulation samples.",
          "link": "http://arxiv.org/abs/2103.17236",
          "publishedOn": "2021-06-29T01:55:16.137Z",
          "wordCount": 646,
          "title": "High-Dimensional Uncertainty Quantification via Tensor Regression with Rank Determination and Adaptive Sampling. (arXiv:2103.17236v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13973",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Basu_P/0/1/0/all/0/1\">Priyam Basu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_T/0/1/0/all/0/1\">Tiasa Singha Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naidu_R/0/1/0/all/0/1\">Rakshit Naidu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muftuoglu_Z/0/1/0/all/0/1\">Zumrut Muftuoglu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sahib Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mireshghallah_F/0/1/0/all/0/1\">Fatemehsadat Mireshghallah</a>",
          "description": "Natural Language Processing (NLP) techniques can be applied to help with the\ndiagnosis of medical conditions such as depression, using a collection of a\nperson's utterances. Depression is a serious medical illness that can have\nadverse effects on how one feels, thinks, and acts, which can lead to emotional\nand physical problems. Due to the sensitive nature of such data, privacy\nmeasures need to be taken for handling and training models with such data. In\nthis work, we study the effects that the application of Differential Privacy\n(DP) has, in both a centralized and a Federated Learning (FL) setup, on\ntraining contextualized language models (BERT, ALBERT, RoBERTa and DistilBERT).\nWe offer insights on how to privately train NLP models and what architectures\nand setups provide more desirable privacy utility trade-offs. We envisage this\nwork to be used in future healthcare and mental health studies to keep medical\nhistory private. Therefore, we provide an open-source implementation of this\nwork.",
          "link": "http://arxiv.org/abs/2106.13973",
          "publishedOn": "2021-06-29T01:55:16.131Z",
          "wordCount": 612,
          "title": "Benchmarking Differential Privacy and Federated Learning for BERT Models. (arXiv:2106.13973v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2105.00771",
          "author": "<a href=\"http://arxiv.org/find/physics/1/au:+Mandralis_I/0/1/0/all/0/1\">Ioannis Mandralis</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Weber_P/0/1/0/all/0/1\">Pascal Weber</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Novati_G/0/1/0/all/0/1\">Guido Novati</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Koumoutsakos_P/0/1/0/all/0/1\">Petros Koumoutsakos</a>",
          "description": "Swimming organisms can escape their predators by creating and harnessing\nunsteady flow fields through their body motions. Stochastic optimization and\nflow simulations have identified escape patterns that are consistent with those\nobserved in natural larval swimmers. However, these patterns have been limited\nby the specification of a particular cost function and depend on a prescribed\nfunctional form of the body motion. Here, we deploy reinforcement learning to\ndiscover swimmer escape patterns for larval fish under energy constraints. The\nidentified patterns include the C-start mechanism, in addition to more\nenergetically efficient escapes. We find that maximizing distance with limited\nenergy requires swimming via short bursts of accelerating motion interlinked\nwith phases of gliding. The present, data efficient, reinforcement learning\nalgorithm results in an array of patterns that reveal practical flow\noptimization principles for efficient swimming and the methodology can be\ntransferred to the control of aquatic robotic devices operating under energy\nconstraints.",
          "link": "http://arxiv.org/abs/2105.00771",
          "publishedOn": "2021-06-29T01:55:16.126Z",
          "wordCount": 613,
          "title": "Learning swimming escape patterns for larval fish under energy constraints. (arXiv:2105.00771v2 [physics.flu-dyn] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.01987",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1\">Jinshuo Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_A/0/1/0/all/0/1\">Aaron Roth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_W/0/1/0/all/0/1\">Weijie J. Su</a>",
          "description": "In this rejoinder, we aim to address two broad issues that cover most\ncomments made in the discussion. First, we discuss some theoretical aspects of\nour work and comment on how this work might impact the theoretical foundation\nof privacy-preserving data analysis. Taking a practical viewpoint, we next\ndiscuss how f-differential privacy (f-DP) and Gaussian differential privacy\n(GDP) can make a difference in a range of applications.",
          "link": "http://arxiv.org/abs/2104.01987",
          "publishedOn": "2021-06-29T01:55:16.098Z",
          "wordCount": 555,
          "title": "Rejoinder: Gaussian Differential Privacy. (arXiv:2104.01987v2 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.14625",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Bartz_Beielstein_T/0/1/0/all/0/1\">Thomas Bartz-Beielstein</a>",
          "description": "A surrogate model based hyperparameter tuning approach for deep learning is\npresented. This article demonstrates how the architecture-level parameters\n(hyperparameters) of deep learning models that were implemented in\nKeras/tensorflow can be optimized. The implementation of the tuning procedure\nis 100% accessible from R, the software environment for statistical computing.\nWith a few lines of code, existing R packages (tfruns and SPOT) can be combined\nto perform hyperparameter tuning. An elementary hyperparameter tuning task\n(neural network and the MNIST data) is used to exemplify this approach",
          "link": "http://arxiv.org/abs/2105.14625",
          "publishedOn": "2021-06-29T01:55:16.092Z",
          "wordCount": 552,
          "title": "Surrogate Model Based Hyperparameter Tuning for Deep Learning with SPOT. (arXiv:2105.14625v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.06958",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Sameni_R/0/1/0/all/0/1\">Reza Sameni</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Jutten_C/0/1/0/all/0/1\">Christian Jutten</a>",
          "description": "The extraction of nonstationary signals from blind and semi-blind\nmultivariate observations is a recurrent problem. Numerous algorithms have been\ndeveloped for this problem, which are based on the exact or approximate joint\ndiagonalization of second or higher order cumulant matrices/tensors of\nmultichannel data. While a great body of research has been dedicated to joint\ndiagonalization algorithms, the selection of the diagonalized matrix/tensor set\nremains highly problem-specific. Herein, various methods for nonstationarity\nidentification are reviewed and a new general framework based on hypothesis\ntesting is proposed, which results in a classification/clustering perspective\nto semi-blind source separation of nonstationary components. The proposed\nmethod is applied to noninvasive fetal ECG extraction, as case study.",
          "link": "http://arxiv.org/abs/2105.06958",
          "publishedOn": "2021-06-29T01:55:16.086Z",
          "wordCount": 566,
          "title": "A Hypothesis Testing Approach to Nonstationary Source Separation. (arXiv:2105.06958v2 [eess.SP] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.11503",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Setlur_A/0/1/0/all/0/1\">Amrith Setlur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_O/0/1/0/all/0/1\">Oscar Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_V/0/1/0/all/0/1\">Virginia Smith</a>",
          "description": "We categorize meta-learning evaluation into two settings:\n$\\textit{in-distribution}$ [ID], in which the train and test tasks are sampled\n$\\textit{iid}$ from the same underlying task distribution, and\n$\\textit{out-of-distribution}$ [OOD], in which they are not. While most\nmeta-learning theory and some FSL applications follow the ID setting, we\nidentify that most existing few-shot classification benchmarks instead reflect\nOOD evaluation, as they use disjoint sets of train (base) and test (novel)\nclasses for task generation. This discrepancy is problematic because -- as we\nshow on numerous benchmarks -- meta-learning methods that perform better on\nexisting OOD datasets may perform significantly worse in the ID setting. In\naddition, in the OOD setting, even though current FSL benchmarks seem\nbefitting, our study highlights concerns in 1) reliably performing model\nselection for a given meta-learning method, and 2) consistently comparing the\nperformance of different methods. To address these concerns, we provide\nsuggestions on how to construct FSL benchmarks to allow for ID evaluation as\nwell as more reliable OOD evaluation. Our work aims to inform the meta-learning\ncommunity about the importance and distinction of ID vs. OOD evaluation, as\nwell as the subtleties of OOD evaluation with current benchmarks.",
          "link": "http://arxiv.org/abs/2102.11503",
          "publishedOn": "2021-06-29T01:55:16.080Z",
          "wordCount": 652,
          "title": "Two Sides of Meta-Learning Evaluation: In vs. Out of Distribution. (arXiv:2102.11503v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13865",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Peng_H/0/1/0/all/0/1\">Hsuan-Tung Peng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lederman_J/0/1/0/all/0/1\">Joshua Lederman</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Xu_L/0/1/0/all/0/1\">Lei Xu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lima_T/0/1/0/all/0/1\">Thomas Ferreira de Lima</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Huang_C/0/1/0/all/0/1\">Chaoran Huang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shastri_B/0/1/0/all/0/1\">Bhavin Shastri</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Rosenbluth_D/0/1/0/all/0/1\">David Rosenbluth</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Prucnal_P/0/1/0/all/0/1\">Paul Prucnal</a>",
          "description": "Machine learning (ML) methods are ubiquitous in wireless communication\nsystems and have proven powerful for applications including radio-frequency\n(RF) fingerprinting, automatic modulation classification, and cognitive radio.\nHowever, the large size of ML models can make them difficult to implement on\nedge devices for latency-sensitive downstream tasks. In wireless communication\nsystems, ML data processing at a sub-millisecond scale will enable real-time\nnetwork monitoring to improve security and prevent infiltration. In addition,\ncompact and integratable hardware platforms which can implement ML models at\nthe chip scale will find much broader application to wireless communication\nnetworks. Toward real-time wireless signal classification at the edge, we\npropose a novel compact deep network that consists of a\nphotonic-hardware-inspired recurrent neural network model in combination with a\nsimplified convolutional classifier, and we demonstrate its application to the\nidentification of RF emitters by their random transmissions. With the proposed\nmodel, we achieve 96.32% classification accuracy over a set of 30 identical\nZigBee devices when using 50 times fewer training parameters than an existing\nstate-of-the-art CNN classifier. Thanks to the large reduction in network size,\nwe demonstrate real-time RF fingerprinting with 0.219 ms latency using a\nsmall-scale FPGA board, the PYNQ-Z1.",
          "link": "http://arxiv.org/abs/2106.13865",
          "publishedOn": "2021-06-29T01:55:16.064Z",
          "wordCount": 658,
          "title": "A Photonic-Circuits-Inspired Compact Network: Toward Real-Time Wireless Signal Classification at the Edge. (arXiv:2106.13865v1 [eess.SP])"
        },
        {
          "id": "http://arxiv.org/abs/2105.08769",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Walton_N/0/1/0/all/0/1\">Neil Walton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kuang Xu</a>",
          "description": "We review the role of information and learning in the stability and\noptimization of queueing systems. In recent years, techniques from supervised\nlearning, bandit learning and reinforcement learning have been applied to\nqueueing systems supported by increasing role of information in decision\nmaking. We present observations and new results that help rationalize the\napplication of these areas to queueing systems.\n\nWe prove that the MaxWeight and BackPressure policies are an application of\nBlackwell's Approachability Theorem. This connects queueing theoretic results\nwith adversarial learning. We then discuss the requirements of statistical\nlearning for service parameter estimation. As an example, we show how queue\nsize regret can be bounded when applying a perceptron algorithm to classify\nservice. Next, we discuss the role of state information in improved decision\nmaking. Here we contrast the roles of epistemic information (information on\nuncertain parameters) and aleatoric information (information on an uncertain\nstate). Finally we review recent advances in the theory of reinforcement\nlearning and queueing, as well as, provide discussion on current research\nchallenges.",
          "link": "http://arxiv.org/abs/2105.08769",
          "publishedOn": "2021-06-29T01:55:16.051Z",
          "wordCount": 635,
          "title": "Learning and Information in Stochastic Networks and Queues. (arXiv:2105.08769v3 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.04623",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Tack_J/0/1/0/all/0/1\">Jihoon Tack</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1\">Sihyun Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeong_J/0/1/0/all/0/1\">Jongheon Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Minseon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1\">Sung Ju Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1\">Jinwoo Shin</a>",
          "description": "Adversarial training (AT) is currently one of the most successful methods to\nobtain the adversarial robustness of deep neural networks. However, the\nphenomenon of robust overfitting, i.e., the robustness starts to decrease\nsignificantly during AT, has been problematic, not only making practitioners\nconsider a bag of tricks for a successful training, e.g., early stopping, but\nalso incurring a significant generalization gap in the robustness. In this\npaper, we propose an effective regularization technique that prevents robust\noverfitting by optimizing an auxiliary 'consistency' regularization loss during\nAT. Specifically, it forces the predictive distributions after attacking from\ntwo different augmentations of the same instance to be similar with each other.\nOur experimental results demonstrate that such a simple regularization\ntechnique brings significant improvements in the test robust accuracy of a wide\nrange of AT methods. More remarkably, we also show that our method could\nsignificantly help the model to generalize its robustness against unseen\nadversaries, e.g., other types or larger perturbations compared to those used\nduring training. Code is available at\nhttps://github.com/alinlab/consistency-adversarial.",
          "link": "http://arxiv.org/abs/2103.04623",
          "publishedOn": "2021-06-29T01:55:16.022Z",
          "wordCount": 637,
          "title": "Consistency Regularization for Adversarial Robustness. (arXiv:2103.04623v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.02477",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Oberst_M/0/1/0/all/0/1\">Michael Oberst</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thams_N/0/1/0/all/0/1\">Nikolaj Thams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peters_J/0/1/0/all/0/1\">Jonas Peters</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sontag_D/0/1/0/all/0/1\">David Sontag</a>",
          "description": "We propose a method for learning linear models whose predictive performance\nis robust to causal interventions on unobserved variables, when noisy proxies\nof those variables are available. Our approach takes the form of a\nregularization term that trades off between in-distribution performance and\nrobustness to interventions. Under the assumption of a linear structural causal\nmodel, we show that a single proxy can be used to create estimators that are\nprediction optimal under interventions of bounded strength. This strength\ndepends on the magnitude of the measurement noise in the proxy, which is, in\ngeneral, not identifiable. In the case of two proxy variables, we propose a\nmodified estimator that is prediction optimal under interventions up to a known\nstrength. We further show how to extend these estimators to scenarios where\nadditional information about the \"test time\" intervention is available during\ntraining. We evaluate our theoretical findings in synthetic experiments and\nusing real data of hourly pollution levels across several cities in China.",
          "link": "http://arxiv.org/abs/2103.02477",
          "publishedOn": "2021-06-29T01:55:16.003Z",
          "wordCount": 628,
          "title": "Regularizing towards Causal Invariance: Linear Models with Proxies. (arXiv:2103.02477v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.13202",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Dimakopoulou_M/0/1/0/all/0/1\">Maria Dimakopoulou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Z/0/1/0/all/0/1\">Zhimei Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhengyuan Zhou</a>",
          "description": "During online decision making in Multi-Armed Bandits (MAB), one needs to\nconduct inference on the true mean reward of each arm based on data collected\nso far at each step. However, since the arms are adaptively selected--thereby\nyielding non-iid data--conducting inference accurately is not straightforward.\nIn particular, sample averaging, which is used in the family of UCB and\nThompson sampling (TS) algorithms, does not provide a good choice as it suffers\nfrom bias and a lack of good statistical properties (e.g. asymptotic\nnormality). Our thesis in this paper is that more sophisticated inference\nschemes that take into account the adaptive nature of the sequentially\ncollected data can unlock further performance gains, even though both UCB and\nTS type algorithms are optimal in the worst case. In particular, we propose a\nvariant of TS-style algorithms--which we call doubly adaptive TS--that\nleverages recent advances in causal inference and adaptively reweights the\nterms of a doubly robust estimator on the true mean reward of each arm. Through\n20 synthetic domain experiments and a semi-synthetic experiment based on data\nfrom an A/B test of a web service, we demonstrate that using an adaptive\ninferential scheme (while still retaining the exploration efficacy of TS)\nprovides clear benefits in online decision making: the proposed DATS algorithm\nhas superior empirical performance to existing baselines (UCB and TS) in terms\nof regret and sample complexity in identifying the best arm. In addition, we\nalso provide a finite-time regret bound of doubly adaptive TS that matches (up\nto log factors) those of UCB and TS algorithms, thereby establishing that its\nimproved practical benefits do not come at the expense of worst-case\nsuboptimality.",
          "link": "http://arxiv.org/abs/2102.13202",
          "publishedOn": "2021-06-29T01:55:15.996Z",
          "wordCount": 733,
          "title": "Online Multi-Armed Bandits with Adaptive Inference. (arXiv:2102.13202v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.07594",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1\">Yuning You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tianlong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yang Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhangyang Wang</a>",
          "description": "Self-supervised learning on graph-structured data has drawn recent interest\nfor learning generalizable, transferable and robust representations from\nunlabeled graphs. Among many, graph contrastive learning (GraphCL) has emerged\nwith promising representation learning performance. Unfortunately, unlike its\ncounterpart on image data, the effectiveness of GraphCL hinges on ad-hoc data\naugmentations, which have to be manually picked per dataset, by either rules of\nthumb or trial-and-errors, owing to the diverse nature of graph data. That\nsignificantly limits the more general applicability of GraphCL. Aiming to fill\nin this crucial gap, this paper proposes a unified bi-level optimization\nframework to automatically, adaptively and dynamically select data\naugmentations when performing GraphCL on specific graph data. The general\nframework, dubbed JOint Augmentation Optimization (JOAO), is instantiated as\nmin-max optimization. The selections of augmentations made by JOAO are shown to\nbe in general aligned with previous \"best practices\" observed from handcrafted\ntuning: yet now being automated, more flexible and versatile. Moreover, we\npropose a new augmentation-aware projection head mechanism, which will route\noutput features through different projection heads corresponding to different\naugmentations chosen at each training step. Extensive experiments demonstrate\nthat JOAO performs on par with or sometimes better than the state-of-the-art\ncompetitors including GraphCL, on multiple graph datasets of various scales and\ntypes, yet without resorting to any laborious dataset-specific tuning on\naugmentation selection. We release the code at\nhttps://github.com/Shen-Lab/GraphCL_Automated.",
          "link": "http://arxiv.org/abs/2106.07594",
          "publishedOn": "2021-06-29T01:55:15.970Z",
          "wordCount": 676,
          "title": "Graph Contrastive Learning Automated. (arXiv:2106.07594v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.02876",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Schell_A/0/1/0/all/0/1\">Alexander Schell</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Oberhauser_H/0/1/0/all/0/1\">Harald Oberhauser</a>",
          "description": "We study the classical problem of recovering a multidimensional source\nprocess from observations of nonlinear mixtures of this process. Assuming\nstatistical independence of the coordinate processes of the source, we show\nthat this recovery is possible for many popular models of stochastic processes\n(up to order and monotone scaling of their coordinates) if the mixture is given\nby a sufficiently differentiable, invertible function. Key to our approach is\nthe combination of tools from stochastic analysis and recent contrastive\nlearning approaches to nonlinear ICA. This yields a scalable method with widely\napplicable theoretical guarantees for which our experiments indicate good\nperformance.",
          "link": "http://arxiv.org/abs/2102.02876",
          "publishedOn": "2021-06-29T01:55:15.939Z",
          "wordCount": 561,
          "title": "Nonlinear Independent Component Analysis for Continuous-Time Signals. (arXiv:2102.02876v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.00442",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rajeswar_S/0/1/0/all/0/1\">Sai Rajeswar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ibrahim_C/0/1/0/all/0/1\">Cyril Ibrahim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Surya_N/0/1/0/all/0/1\">Nitin Surya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Golemo_F/0/1/0/all/0/1\">Florian Golemo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vazquez_D/0/1/0/all/0/1\">David Vazquez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Courville_A/0/1/0/all/0/1\">Aaron Courville</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pinheiro_P/0/1/0/all/0/1\">Pedro O. Pinheiro</a>",
          "description": "Robots in many real-world settings have access to force/torque sensors in\ntheir gripper and tactile sensing is often necessary in tasks that involve\ncontact-rich motion. In this work, we leverage surprise from mismatches in\ntouch feedback to guide exploration in hard sparse-reward reinforcement\nlearning tasks. Our approach, Touch-based Curiosity (ToC), learns what visible\nobjects interactions are supposed to \"feel\" like. We encourage exploration by\nrewarding interactions where the expectation and the experience don't match. In\nour proposed method, an initial task-independent exploration phase is followed\nby an on-task learning phase, in which the original interactions are relabeled\nwith on-task rewards. We test our approach on a range of touch-intensive robot\narm tasks (e.g. pushing objects, opening doors), which we also release as part\nof this work. Across multiple experiments in a simulated setting, we\ndemonstrate that our method is able to learn these difficult tasks through\nsparse reward and curiosity alone. We compare our cross-modal approach to\nsingle-modality (touch- or vision-only) approaches as well as other\ncuriosity-based methods and find that our method performs better and is more\nsample-efficient.",
          "link": "http://arxiv.org/abs/2104.00442",
          "publishedOn": "2021-06-29T01:55:15.934Z",
          "wordCount": 655,
          "title": "Touch-based Curiosity for Sparse-Reward Tasks. (arXiv:2104.00442v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.13795",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1\">Mengying Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Guizhong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yuanchao Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xinliang Wu</a>",
          "description": "In representation learning on the graph-structured data, under heterophily\n(or low homophily), many popular GNNs may fail to capture long-range\ndependencies, which leads to their performance degradation. To solve the\nabove-mentioned issue, we propose a graph convolutional networks with structure\nlearning (GCN-SL), and furthermore, the proposed approach can be applied to\nnode classification. The proposed GCN-SL contains two improvements:\ncorresponding to node features and edges, respectively. In the aspect of node\nfeatures, we propose an efficient-spectral-clustering (ESC) and an ESC with\nanchors (ESC-ANCH) algorithms to efficiently aggregate feature representations\nfrom all similar nodes. In the aspect of edges, we build a re-connected\nadjacency matrix by using a special data preprocessing technique and similarity\nlearning, and the re-connected adjacency matrix can be optimized directly along\nwith GCN-SL parameters. Considering that the original adjacency matrix may\nprovide misleading information for aggregation in GCN, especially the graphs\nbeing with a low level of homophily. The proposed GCN-SL can aggregate feature\nrepresentations from nearby nodes via re-connected adjacency matrix and is\napplied to graphs with various levels of homophily. Experimental results on a\nwide range of benchmark datasets illustrate that the proposed GCN-SL\noutperforms the stateof-the-art GNN counterparts.",
          "link": "http://arxiv.org/abs/2105.13795",
          "publishedOn": "2021-06-29T01:55:15.919Z",
          "wordCount": 657,
          "title": "GCN-SL: Graph Convolutional Networks with Structure Learning for Graphs under Heterophily. (arXiv:2105.13795v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2102.13376",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_R/0/1/0/all/0/1\">Rajesh Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">WenYong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_J/0/1/0/all/0/1\">Jay Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zakria/0/1/0/all/0/1\">Zakria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1\">Ting Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ali_W/0/1/0/all/0/1\">Waqar Ali</a>",
          "description": "The widespread significance of Android IoT devices is due to its flexibility\nand hardware support features which revolutionized the digital world by\nintroducing exciting applications almost in all walks of daily life, such as\nhealthcare, smart cities, smart environments, safety, remote sensing, and many\nmore. Such versatile applicability gives incentive for more malware attacks. In\nthis paper, we propose a framework which continuously aggregates multiple user\ntrained models on non-overlapping data into single model. Specifically for\nmalware detection task, (i) we propose a novel user (local) neural network\n(LNN) which trains on local distribution and (ii) then to assure the model\nauthenticity and quality, we propose a novel smart contract which enable\naggregation process over blokchain platform. The LNN model analyzes various\nstatic and dynamic features of both malware and benign whereas the smart\ncontract verifies the malicious applications both for uploading and downloading\nprocesses in the network using stored aggregated features of local models. In\nthis way, the proposed model not only improves malware detection accuracy using\ndecentralized model network but also model efficacy with blockchain. We\nevaluate our approach with three state-of-the-art models and performed deep\nanalyses of extracted features of the relative model.",
          "link": "http://arxiv.org/abs/2102.13376",
          "publishedOn": "2021-06-29T01:55:15.913Z",
          "wordCount": 671,
          "title": "Collective Intelligence: Decentralized Learning for Android Malware Detection in IoT with Blockchain. (arXiv:2102.13376v2 [cs.CR] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.04668",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Yoneyama_R/0/1/0/all/0/1\">Reo Yoneyama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yi-Chiao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toda_T/0/1/0/all/0/1\">Tomoki Toda</a>",
          "description": "We propose a unified approach to data-driven source-filter modeling using a\nsingle neural network for developing a neural vocoder capable of generating\nhigh-quality synthetic speech waveforms while retaining flexibility of the\nsource-filter model to control their voice characteristics. Our proposed\nnetwork called unified source-filter generative adversarial networks (uSFGAN)\nis developed by factorizing quasi-periodic parallel WaveGAN (QPPWG), one of the\nneural vocoders based on a single neural network, into a source excitation\ngeneration network and a vocal tract resonance filtering network by\nadditionally implementing a regularization loss. Moreover, inspired by neural\nsource filter (NSF), only a sinusoidal waveform is additionally used as the\nsimplest clue to generate a periodic source excitation waveform while\nminimizing the effect of approximations in the source filter model. The\nexperimental results demonstrate that uSFGAN outperforms conventional neural\nvocoders, such as QPPWG and NSF in both speech quality and pitch\ncontrollability.",
          "link": "http://arxiv.org/abs/2104.04668",
          "publishedOn": "2021-06-29T01:55:15.879Z",
          "wordCount": 632,
          "title": "Unified Source-Filter GAN: Unified Source-filter Network Based On Factorization of Quasi-Periodic Parallel WaveGAN. (arXiv:2104.04668v3 [cs.SD] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13880",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kang_Z/0/1/0/all/0/1\">Zhao Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hongfei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiangxin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaofeng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_L/0/1/0/all/0/1\">Ling Tian</a>",
          "description": "Principal Component Analysis (PCA) has been widely used for dimensionality\nreduction and feature extraction. Robust PCA (RPCA), under different robust\ndistance metrics, such as l1-norm and l2, p-norm, can deal with noise or\noutliers to some extent. However, real-world data may display structures that\ncan not be fully captured by these simple functions. In addition, existing\nmethods treat complex and simple samples equally. By contrast, a learning\npattern typically adopted by human beings is to learn from simple to complex\nand less to more. Based on this principle, we propose a novel method called\nSelf-paced PCA (SPCA) to further reduce the effect of noise and outliers.\nNotably, the complexity of each sample is calculated at the beginning of each\niteration in order to integrate samples from simple to more complex into\ntraining. Based on an alternating optimization, SPCA finds an optimal\nprojection matrix and filters out outliers iteratively. Theoretical analysis is\npresented to show the rationality of SPCA. Extensive experiments on popular\ndata sets demonstrate that the proposed method can improve the state of-the-art\nresults considerably.",
          "link": "http://arxiv.org/abs/2106.13880",
          "publishedOn": "2021-06-29T01:55:15.726Z",
          "wordCount": 617,
          "title": "Self-paced Principal Component Analysis. (arXiv:2106.13880v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14112",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Eldele_E/0/1/0/all/0/1\">Emadeldeen Eldele</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ragab_M/0/1/0/all/0/1\">Mohamed Ragab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhenghua Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1\">Min Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwoh_C/0/1/0/all/0/1\">Chee Keong Kwoh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoli Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guan_C/0/1/0/all/0/1\">Cuntai Guan</a>",
          "description": "Learning decent representations from unlabeled time-series data with temporal\ndynamics is a very challenging task. In this paper, we propose an unsupervised\nTime-Series representation learning framework via Temporal and Contextual\nContrasting (TS-TCC), to learn time-series representation from unlabeled data.\nFirst, the raw time-series data are transformed into two different yet\ncorrelated views by using weak and strong augmentations. Second, we propose a\nnovel temporal contrasting module to learn robust temporal representations by\ndesigning a tough cross-view prediction task. Last, to further learn\ndiscriminative representations, we propose a contextual contrasting module\nbuilt upon the contexts from the temporal contrasting module. It attempts to\nmaximize the similarity among different contexts of the same sample while\nminimizing similarity among contexts of different samples. Experiments have\nbeen carried out on three real-world time-series datasets. The results manifest\nthat training a linear classifier on top of the features learned by our\nproposed TS-TCC performs comparably with the supervised training. Additionally,\nour proposed TS-TCC shows high efficiency in few-labeled data and transfer\nlearning scenarios. The code is publicly available at\nhttps://github.com/emadeldeen24/TS-TCC.",
          "link": "http://arxiv.org/abs/2106.14112",
          "publishedOn": "2021-06-29T01:55:15.720Z",
          "wordCount": 628,
          "title": "Time-Series Representation Learning via Temporal and Contextual Contrasting. (arXiv:2106.14112v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14251",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Maass_W/0/1/0/all/0/1\">Wolfgang Maass</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Storey_V/0/1/0/all/0/1\">Veda C. Storey</a>",
          "description": "Both conceptual modeling and machine learning have long been recognized as\nimportant areas of research. With the increasing emphasis on digitizing and\nprocessing large amounts of data for business and other applications, it would\nbe helpful to consider how these areas of research can complement each other.\nTo understand how they can be paired, we provide an overview of machine\nlearning foundations and development cycle. We then examine how conceptual\nmodeling can be applied to machine learning and propose a framework for\nincorporating conceptual modeling into data science projects. The framework is\nillustrated by applying it to a healthcare application. For the inverse\npairing, machine learning can impact conceptual modeling through text and rule\nmining, as well as knowledge graphs. The pairing of conceptual modeling and\nmachine learning in this this way should help lay the foundations for future\nresearch.",
          "link": "http://arxiv.org/abs/2106.14251",
          "publishedOn": "2021-06-29T01:55:15.703Z",
          "wordCount": 579,
          "title": "Pairing Conceptual Modeling with Machine Learning. (arXiv:2106.14251v1 [cs.SE])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14465",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Hossain_S/0/1/0/all/0/1\">Sk Imran Hossain</a> (LIMOS), <a href=\"http://arxiv.org/find/eess/1/au:+Herve_J/0/1/0/all/0/1\">Jocelyn de Go&#xeb;r de Herve</a> (INRAE), <a href=\"http://arxiv.org/find/eess/1/au:+Hassan_M/0/1/0/all/0/1\">Md Shahriar Hassan</a> (LIMOS), <a href=\"http://arxiv.org/find/eess/1/au:+Martineau_D/0/1/0/all/0/1\">Delphine Martineau</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Petrosyan_E/0/1/0/all/0/1\">Evelina Petrosyan</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Corbain_V/0/1/0/all/0/1\">Violaine Corbain</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Beytout_J/0/1/0/all/0/1\">Jean Beytout</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Lebert_I/0/1/0/all/0/1\">Isabelle Lebert</a> (INRAE), <a href=\"http://arxiv.org/find/eess/1/au:+Baux_E/0/1/0/all/0/1\">Elisabeth Baux</a> (CHRU Nancy), <a href=\"http://arxiv.org/find/eess/1/au:+Cazorla_C/0/1/0/all/0/1\">C&#xe9;line Cazorla</a> (CHU de Saint-Etienne), <a href=\"http://arxiv.org/find/eess/1/au:+Eldin_C/0/1/0/all/0/1\">Carole Eldin</a> (IHU M&#xe9;diterran&#xe9;e Infection), <a href=\"http://arxiv.org/find/eess/1/au:+Hansmann_Y/0/1/0/all/0/1\">Yves Hansmann</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Patrat_Delon_S/0/1/0/all/0/1\">Solene Patrat-Delon</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Prazuck_T/0/1/0/all/0/1\">Thierry Prazuck</a> (CHR), <a href=\"http://arxiv.org/find/eess/1/au:+Raffetin_A/0/1/0/all/0/1\">Alice Raffetin</a> (CHIV), <a href=\"http://arxiv.org/find/eess/1/au:+Tattevin_P/0/1/0/all/0/1\">Pierre Tattevin</a> (CHU Rennes), <a href=\"http://arxiv.org/find/eess/1/au:+VourcH_G/0/1/0/all/0/1\">Gwena&#xeb;l Vourc&#x27;H</a> (INRAE), <a href=\"http://arxiv.org/find/eess/1/au:+Lesens_O/0/1/0/all/0/1\">Olivier Lesens</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Nguifo_E/0/1/0/all/0/1\">Engelbert Nguifo</a> (LIMOS)",
          "description": "Lyme disease is one of the most common infectious vector-borne diseases in\nthe world. In the early stage, the disease manifests itself in most cases with\nerythema migrans (EM) skin lesions. Better diagnosis of these early forms would\nallow improving the prognosis by preventing the transition to a severe late\nform thanks to appropriate antibiotic therapy. Recent studies show that\nconvolutional neural networks (CNNs) perform very well to identify skin lesions\nfrom the image but, there is not much work for Lyme disease prediction from EM\nlesion images. The main objective of this study is to extensively analyze the\neffectiveness of CNNs for diagnosing Lyme disease from images and to find out\nthe best CNN architecture for the purpose. There is no publicly available EM\nimage dataset for Lyme dis…",
          "link": "http://arxiv.org/abs/2106.14465",
          "publishedOn": "2021-06-29T01:55:15.690Z",
          "wordCount": 857,
          "title": "Benchmarking convolutional neural networks for diagnosing Lyme disease from images. (arXiv:2106.14465v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14463",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Saahil Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agrawal_A/0/1/0/all/0/1\">Ashwin Agrawal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saporta_A/0/1/0/all/0/1\">Adriel Saporta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Truong_S/0/1/0/all/0/1\">Steven QH Truong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duong_D/0/1/0/all/0/1\">Du Nguyen Duong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1\">Tan Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chambon_P/0/1/0/all/0/1\">Pierre Chambon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuhao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lungren_M/0/1/0/all/0/1\">Matthew P. Lungren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_A/0/1/0/all/0/1\">Andrew Y. Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Langlotz_C/0/1/0/all/0/1\">Curtis P. Langlotz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajpurkar_P/0/1/0/all/0/1\">Pranav Rajpurkar</a>",
          "description": "Extracting structured clinical information from free-text radiology reports\ncan enable the use of radiology report information for a variety of critical\nhealthcare applications. In our work, we present RadGraph, a dataset of\nentities and relations in full-text chest X-ray radiology reports based on a\nnovel information extraction schema we designed to structure radiology reports.\nWe release a development dataset, which contains board-certified radiologist\nannotations for 500 radiology reports from the MIMIC-CXR dataset (14,579\nentities and 10,889 relations), and a test dataset, which contains two\nindependent sets of board-certified radiologist annotations for 100 radiology\nreports split equally across the MIMIC-CXR and CheXpert datasets. Using these\ndatasets, we train and test a deep learning model, RadGraph Benchmark, that\nachieves a micro F1 of 0.82 and 0.73 on relation extraction on the MIMIC-CXR\nand CheXpert test sets respectively. Additionally, we release an inference\ndataset, which contains annotations automatically generated by RadGraph\nBenchmark across 220,763 MIMIC-CXR reports (around 6 million entities and 4\nmillion relations) and 500 CheXpert reports (13,783 entities and 9,908\nrelations) with mappings to associated chest radiographs. Our freely available\ndataset can facilitate a wide range of research in medical natural language\nprocessing, as well as computer vision and multi-modal learning when linked to\nchest radiographs.",
          "link": "http://arxiv.org/abs/2106.14463",
          "publishedOn": "2021-06-29T01:55:15.682Z",
          "wordCount": 674,
          "title": "RadGraph: Extracting Clinical Entities and Relations from Radiology Reports. (arXiv:2106.14463v1 [cs.CL])"
        },
        {
          "id": "http://arxiv.org/abs/2008.10271",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Comandur_B/0/1/0/all/0/1\">Bharath Comandur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kak_A/0/1/0/all/0/1\">Avinash C. Kak</a>",
          "description": "We present a novel multi-view training framework and CNN architecture for\ncombining information from multiple overlapping satellite images and noisy\ntraining labels derived from OpenStreetMap (OSM) to semantically label\nbuildings and roads across large geographic regions (100 km$^2$). Our approach\nto multi-view semantic segmentation yields a 4-7% improvement in the per-class\nIoU scores compared to the traditional approaches that use the views\nindependently of one another. A unique (and, perhaps, surprising) property of\nour system is that modifications that are added to the tail-end of the CNN for\nlearning from the multi-view data can be discarded at the time of inference\nwith a relatively small penalty in the overall performance. This implies that\nthe benefits of training using multiple views are absorbed by all the layers of\nthe network. Additionally, our approach only adds a small overhead in terms of\nthe GPU-memory consumption even when training with as many as 32 views per\nscene. The system we present is end-to-end automated, which facilitates\ncomparing the classifiers trained directly on true orthophotos vis-a-vis first\ntraining them on the off-nadir images and subsequently translating the\npredicted labels to geographical coordinates. With no human supervision, our\nIoU scores for the buildings and roads classes are 0.8 and 0.64 respectively\nwhich are better than state-of-the-art approaches that use OSM labels and that\nare not completely automated.",
          "link": "http://arxiv.org/abs/2008.10271",
          "publishedOn": "2021-06-29T01:55:15.675Z",
          "wordCount": 774,
          "title": "Semantic Labeling of Large-Area Geographic Regions Using Multi-View and Multi-Date Satellite Images and Noisy OSM Training Labels. (arXiv:2008.10271v5 [cs.CV] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13935",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fang_K/0/1/0/all/0/1\">Kuan Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yuke Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Savarese_S/0/1/0/all/0/1\">Silvio Savarese</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1\">Li Fei-Fei</a>",
          "description": "The learning efficiency and generalization ability of an intelligent agent\ncan be greatly improved by utilizing a useful set of skills. However, the\ndesign of robot skills can often be intractable in real-world applications due\nto the prohibitive amount of effort and expertise that it requires. In this\nwork, we introduce Skill Learning In Diversified Environments (SLIDE), a method\nto discover generalizable skills via automated generation of a diverse set of\ntasks. As opposed to prior work on unsupervised discovery of skills which\nincentivizes the skills to produce different outcomes in the same environment,\nour method pairs each skill with a unique task produced by a trainable task\ngenerator. To encourage generalizable skills to emerge, our method trains each\nskill to specialize in the paired task and maximizes the diversity of the\ngenerated tasks. A task discriminator defined on the robot behaviors in the\ngenerated tasks is jointly trained to estimate the evidence lower bound of the\ndiversity objective. The learned skills can then be composed in a hierarchical\nreinforcement learning algorithm to solve unseen target tasks. We demonstrate\nthat the proposed method can effectively learn a variety of robot skills in two\ntabletop manipulation domains. Our results suggest that the learned skills can\neffectively improve the robot's performance in various unseen target tasks\ncompared to existing reinforcement learning and skill learning methods.",
          "link": "http://arxiv.org/abs/2106.13935",
          "publishedOn": "2021-06-29T01:55:15.661Z",
          "wordCount": 664,
          "title": "Discovering Generalizable Skills via Automated Generation of Diverse Tasks. (arXiv:2106.13935v1 [cs.RO])"
        },
        {
          "id": "http://arxiv.org/abs/2011.00810",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Fotakis_D/0/1/0/all/0/1\">Dimitris Fotakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kalavasis_A/0/1/0/all/0/1\">Alkis Kalavasis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stavropoulos_K/0/1/0/all/0/1\">Konstantinos Stavropoulos</a>",
          "description": "We consider the problem of learning the true ordering of a set of\nalternatives from largely incomplete and noisy rankings. We introduce a natural\ngeneralization of both the classical Mallows model of ranking distributions and\nthe extensively studied model of noisy pairwise comparisons. Our selective\nMallows model outputs a noisy ranking on any given subset of alternatives,\nbased on an underlying Mallows distribution. Assuming a sequence of subsets\nwhere each pair of alternatives appears frequently enough, we obtain strong\nasymptotically tight upper and lower bounds on the sample complexity of\nlearning the underlying complete ranking and the (identities and the) ranking\nof the top-k alternatives from selective Mallows rankings. Moreover, building\non the work of (Braverman and Mossel, 2009), we show how to efficiently compute\nthe maximum likelihood complete ranking from selective Mallows rankings.",
          "link": "http://arxiv.org/abs/2011.00810",
          "publishedOn": "2021-06-29T01:55:15.655Z",
          "wordCount": 637,
          "title": "Aggregating Incomplete and Noisy Rankings. (arXiv:2011.00810v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13863",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Melnyk_P/0/1/0/all/0/1\">Pavlo Melnyk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Felsberg_M/0/1/0/all/0/1\">Michael Felsberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wadenback_M/0/1/0/all/0/1\">M&#xe5;rten Wadenb&#xe4;ck</a>",
          "description": "Emerging from low-level vision theory, steerable filters found their\ncounterpart in deep learning. Earlier works used the steering theorems and\npresented convolutional networks equivariant to rigid transformations. In our\nwork, we propose a steerable feed-forward learning-based approach that consists\nof spherical decision surfaces and operates on point clouds. Due to the\ninherent geometric 3D structure of our theory, we derive a 3D steerability\nconstraint for its atomic parts, the hypersphere neurons. Exploiting the\nrotational equivariance, we show how the model parameters are fully steerable\nat inference time. The proposed spherical filter banks enable to make\nequivariant and, after online optimization, invariant class predictions for\nknown synthetic point sets in unknown orientations.",
          "link": "http://arxiv.org/abs/2106.13863",
          "publishedOn": "2021-06-29T01:55:15.649Z",
          "wordCount": 543,
          "title": "Fully Steerable 3D Spherical Neurons. (arXiv:2106.13863v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2007.15421",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Saha_A/0/1/0/all/0/1\">Arkajyoti Saha</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Basu_S/0/1/0/all/0/1\">Sumanta Basu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Datta_A/0/1/0/all/0/1\">Abhirup Datta</a>",
          "description": "Random forest (RF) is one of the most popular methods for estimating\nregression functions. The local nature of the RF algorithm, based on intra-node\nmeans and variances, is ideal when errors are i.i.d. For dependent error\nprocesses like time series and spatial settings where data in all the nodes\nwill be correlated, operating locally ignores this dependence. Also, RF will\ninvolve resampling of correlated data, violating the principles of bootstrap.\nTheoretically, consistency of RF has been established for i.i.d. errors, but\nlittle is known about the case of dependent errors.\n\nWe propose RF-GLS, a novel extension of RF for dependent error processes in\nthe same way Generalized Least Squares (GLS) fundamentally extends Ordinary\nLeast Squares (OLS) for linear models under dependence. The key to this\nextension is the equivalent representation of the local decision-making in a\nregression tree as a global OLS optimization which is then replaced with a GLS\nloss to create a GLS-style regression tree. This also synergistically addresses\nthe resampling issue, as the use of GLS loss amounts to resampling uncorrelated\ncontrasts (pre-whitened data) instead of the correlated data. For spatial\nsettings, RF-GLS can be used in conjunction with Gaussian Process correlated\nerrors to generate kriging predictions at new locations. RF becomes a special\ncase of RF-GLS with an identity working covariance matrix.\n\nWe establish consistency of RF-GLS under beta- (absolutely regular) mixing\nerror processes and show that this general result subsumes important cases like\nautoregressive time series and spatial Matern Gaussian Processes. As a\nbyproduct, we also establish consistency of RF for beta-mixing processes, which\nto our knowledge, is the first such result for RF under dependence.\n\nWe empirically demonstrate the improvement achieved by RF-GLS over RF for\nboth estimation and prediction under dependence.",
          "link": "http://arxiv.org/abs/2007.15421",
          "publishedOn": "2021-06-29T01:55:15.643Z",
          "wordCount": 740,
          "title": "Random Forests for dependent data. (arXiv:2007.15421v2 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/1909.02373",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Liu_Y/0/1/0/all/0/1\">Yanbin Liu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Yamada_M/0/1/0/all/0/1\">Makoto Yamada</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Tsai_Y/0/1/0/all/0/1\">Yao-Hung Hubert Tsai</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Le_T/0/1/0/all/0/1\">Tam Le</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Salakhutdinov_R/0/1/0/all/0/1\">Ruslan Salakhutdinov</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Yang_Y/0/1/0/all/0/1\">Yi Yang</a>",
          "description": "Estimating mutual information is an important statistics and machine learning\nproblem. To estimate the mutual information from data, a common practice is\npreparing a set of paired samples $\\{(\\mathbf{x}_i,\\mathbf{y}_i)\\}_{i=1}^n\n\\stackrel{\\mathrm{i.i.d.}}{\\sim} p(\\mathbf{x},\\mathbf{y})$. However, in many\nsituations, it is difficult to obtain a large number of data pairs. To address\nthis problem, we propose the semi-supervised Squared-loss Mutual Information\n(SMI) estimation method using a small number of paired samples and the\navailable unpaired ones. We first represent SMI through the density ratio\nfunction, where the expectation is approximated by the samples from marginals\nand its assignment parameters. The objective is formulated using the optimal\ntransport problem and quadratic programming. Then, we introduce the\nLeast-Squares Mutual Information with Sinkhorn (LSMI-Sinkhorn) algorithm for\nefficient optimization. Through experiments, we first demonstrate that the\nproposed method can estimate the SMI without a large number of paired samples.\nThen, we show the effectiveness of the proposed LSMI-Sinkhorn algorithm on\nvarious types of machine learning problems such as image matching and photo\nalbum summarization. Code can be found at\nhttps://github.com/csyanbin/LSMI-Sinkhorn.",
          "link": "http://arxiv.org/abs/1909.02373",
          "publishedOn": "2021-06-29T01:55:15.637Z",
          "wordCount": 642,
          "title": "LSMI-Sinkhorn: Semi-supervised Mutual Information Estimation with Optimal Transport. (arXiv:1909.02373v3 [stat.ML] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14077",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kato_M/0/1/0/all/0/1\">Masahiro Kato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ariu_K/0/1/0/all/0/1\">Kaito Ariu</a>",
          "description": "We study the best-arm identification problem with fixed confidence when\ncontextual (covariate) information is available in stochastic bandits. Although\nwe can use contextual information in each round, we are interested in the\nmarginalized mean reward over the contextual distribution. Our goal is to\nidentify the best arm with a minimal number of samplings under a given value of\nthe error rate. We show the instance-specific sample complexity lower bounds\nfor the problem. Then, we propose a context-aware version of the\n\"Track-and-Stop\" strategy, wherein the proportion of the arm draws tracks the\nset of optimal allocations and prove that the expected number of arm draws\nmatches the lower bound asymptotically. We demonstrate that contextual\ninformation can be used to improve the efficiency of the identification of the\nbest marginalized mean reward compared with the results of Garivier & Kaufmann\n(2016). We experimentally confirm that context information contributes to\nfaster best-arm identification.",
          "link": "http://arxiv.org/abs/2106.14077",
          "publishedOn": "2021-06-29T01:55:15.620Z",
          "wordCount": 596,
          "title": "The Role of Contextual Information in Best Arm Identification. (arXiv:2106.14077v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2104.10314",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Xue_Y/0/1/0/all/0/1\">Ye Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lau_V/0/1/0/all/0/1\">Vincent Lau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_S/0/1/0/all/0/1\">Songfu Cai</a>",
          "description": "Sparse coding is a class of unsupervised methods for learning a sparse\nrepresentation of the input data in the form of a linear combination of a\ndictionary and a sparse code. This learning framework has led to\nstate-of-the-art results in various image and video processing tasks. However,\nclassical methods learn the dictionary and the sparse code based on alternative\noptimizations, usually without theoretical guarantees for either optimality or\nconvergence due to non-convexity of the problem. Recent works on sparse coding\nwith a complete dictionary provide strong theoretical guarantees thanks to the\ndevelopment of the non-convex optimization. However, initial non-convex\napproaches learn the dictionary in the sparse coding problem sequentially in an\natom-by-atom manner, which leads to a long execution time. More recent works\nseek to directly learn the entire dictionary at once, which substantially\nreduces the execution time. However, the associated recovery performance is\ndegraded with a finite number of data samples. In this paper, we propose an\nefficient sparse coding scheme with a two-stage optimization. The proposed\nscheme leverages the global and local Riemannian geometry of the two-stage\noptimization problem and facilitates fast implementation for superb dictionary\nrecovery performance by a finite number of samples without atom-by-atom\ncalculation. We further prove that, with high probability, the proposed scheme\ncan exactly recover any atom in the target dictionary with a finite number of\nsamples if it is adopted to recover one atom of the dictionary. An application\non wireless sensor data compression is also proposed. Experiments on both\nsynthetic and real-world data verify the efficiency and effectiveness of the\nproposed scheme.",
          "link": "http://arxiv.org/abs/2104.10314",
          "publishedOn": "2021-06-29T01:55:15.614Z",
          "wordCount": 735,
          "title": "Efficient Sparse Coding using Hierarchical Riemannian Pursuit. (arXiv:2104.10314v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.13898",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Hasani_R/0/1/0/all/0/1\">Ramin Hasani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lechner_M/0/1/0/all/0/1\">Mathias Lechner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Amini_A/0/1/0/all/0/1\">Alexander Amini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liebenwein_L/0/1/0/all/0/1\">Lucas Liebenwein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tschaikowski_M/0/1/0/all/0/1\">Max Tschaikowski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teschl_G/0/1/0/all/0/1\">Gerald Teschl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rus_D/0/1/0/all/0/1\">Daniela Rus</a>",
          "description": "Continuous-depth neural models, where the derivative of the model's hidden\nstate is defined by a neural network, have enabled strong sequential data\nprocessing capabilities. However, these models rely on advanced numerical\ndifferential equation (DE) solvers resulting in a significant overhead both in\nterms of computational cost and model complexity. In this paper, we present a\nnew family of models, termed Closed-form Continuous-depth (CfC) networks, that\nare simple to describe and at least one order of magnitude faster while\nexhibiting equally strong modeling abilities compared to their ODE-based\ncounterparts. The models are hereby derived from the analytical closed-form\nsolution of an expressive subset of time-continuous models, thus alleviating\nthe need for complex DE solvers all together. In our experimental evaluations,\nwe demonstrate that CfC networks outperform advanced, recurrent models over a\ndiverse set of time-series prediction tasks, including those with long-term\ndependencies and irregularly sampled data. We believe our findings open new\nopportunities to train and deploy rich, continuous neural models in\nresource-constrained settings, which demand both performance and efficiency.",
          "link": "http://arxiv.org/abs/2106.13898",
          "publishedOn": "2021-06-29T01:55:15.608Z",
          "wordCount": 615,
          "title": "Closed-form Continuous-Depth Models. (arXiv:2106.13898v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14122",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Liu_L/0/1/0/all/0/1\">Lang Liu</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Salmon_J/0/1/0/all/0/1\">Joseph Salmon</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Harchaoui_Z/0/1/0/all/0/1\">Zaid Harchaoui</a>",
          "description": "The widespread use of machine learning algorithms calls for automatic change\ndetection algorithms to monitor their behavior over time. As a machine learning\nalgorithm learns from a continuous, possibly evolving, stream of data, it is\ndesirable and often critical to supplement it with a companion change detection\nalgorithm to facilitate its monitoring and control. We present a generic\nscore-based change detection method that can detect a change in any number of\ncomponents of a machine learning model trained via empirical risk minimization.\nThis proposed statistical hypothesis test can be readily implemented for such\nmodels designed within a differentiable programming framework. We establish the\nconsistency of the hypothesis test and show how to calibrate it to achieve a\nprescribed false alarm rate. We illustrate the versatility of the approach on\nsynthetic and real data.",
          "link": "http://arxiv.org/abs/2106.14122",
          "publishedOn": "2021-06-29T01:55:15.603Z",
          "wordCount": 563,
          "title": "Score-Based Change Detection for Gradient-Based Learning Machines. (arXiv:2106.14122v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14447",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_L/0/1/0/all/0/1\">Le Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1\">Zhiyu Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_B/0/1/0/all/0/1\">Bo He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xin_J/0/1/0/all/0/1\">Jingyu Xin</a>",
          "description": "With rapidly evolving internet technologies and emerging tools, sports\nrelated videos generated online are increasing at an unprecedentedly fast pace.\nTo automate sports video editing/highlight generation process, a key task is to\nprecisely recognize and locate the events in the long untrimmed videos. In this\ntech report, we present a two-stage paradigm to detect what and when events\nhappen in soccer broadcast videos. Specifically, we fine-tune multiple action\nrecognition models on soccer data to extract high-level semantic features, and\ndesign a transformer based temporal detection module to locate the target\nevents. This approach achieved the state-of-the-art performance in both two\ntasks, i.e., action spotting and replay grounding, in the SoccerNet-v2\nChallenge, under CVPR 2021 ActivityNet workshop. Our soccer embedding features\nare released at https://github.com/baidu-research/vidpress-sports. By sharing\nthese features with the broader community, we hope to accelerate the research\ninto soccer video understanding.",
          "link": "http://arxiv.org/abs/2106.14447",
          "publishedOn": "2021-06-29T01:55:15.597Z",
          "wordCount": 611,
          "title": "Feature Combination Meets Attention: Baidu Soccer Embeddings and Transformer based Temporal Detection. (arXiv:2106.14447v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14324",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Zhou_W/0/1/0/all/0/1\">Weimin Zhou</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bhadra_S/0/1/0/all/0/1\">Sayantan Bhadra</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Brooks_F/0/1/0/all/0/1\">Frank J. Brooks</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1\">Hua Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Anastasio_M/0/1/0/all/0/1\">Mark A. Anastasio</a>",
          "description": "In order to objectively assess new medical imaging technologies via\ncomputer-simulations, it is important to account for all sources of variability\nthat contribute to image data. One important source of variability that can\nsignificantly limit observer performance is associated with the variability in\nthe ensemble of objects to-be-imaged. This source of variability can be\ndescribed by stochastic object models (SOMs), which are generative models that\ncan be employed to sample from a distribution of to-be-virtually-imaged\nobjects. It is generally desirable to establish SOMs from experimental imaging\nmeasurements acquired by use of a well-characterized imaging system, but this\ntask has remained challenging. Deep generative neural networks, such as\ngenerative adversarial networks (GANs) hold potential for such tasks. To\nestablish SOMs from imaging measurements, an AmbientGAN has been proposed that\naugments a GAN with a measurement operator. However, the original AmbientGAN\ncould not immediately benefit from modern training procedures and GAN\narchitectures, which limited its ability to be applied to realistically sized\nmedical image data. To circumvent this, in this work, a modified AmbientGAN\ntraining strategy is proposed that is suitable for modern progressive or\nmulti-resolution training approaches such as employed in the Progressive\nGrowing of GANs and Style-based GANs. AmbientGANs established by use of the\nproposed training procedure are systematically validated in a controlled way by\nuse of computer-simulated measurement data corresponding to a stylized imaging\nsystem. Finally, emulated single-coil experimental magnetic resonance imaging\ndata are employed to demonstrate the methods under less stylized conditions.",
          "link": "http://arxiv.org/abs/2106.14324",
          "publishedOn": "2021-06-29T01:55:15.581Z",
          "wordCount": 728,
          "title": "Learning stochastic object models from medical imaging measurements by use of advanced AmbientGANs. (arXiv:2106.14324v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14432",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Muravev_N/0/1/0/all/0/1\">Nikita Muravev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Petiushko_A/0/1/0/all/0/1\">Aleksandr Petiushko</a>",
          "description": "We propose a novel approach of randomized smoothing over multiplicative\nparameters. Using this method we construct certifiably robust classifiers with\nrespect to a gamma-correction perturbation and compare the result with\nclassifiers obtained via Gaussian smoothing. To the best of our knowledge it is\nthe first work concerning certified robustness against the multiplicative\ngamma-correction transformation.",
          "link": "http://arxiv.org/abs/2106.14432",
          "publishedOn": "2021-06-29T01:55:15.575Z",
          "wordCount": 480,
          "title": "Certified Robustness via Randomized Smoothing over Multiplicative Parameters. (arXiv:2106.14432v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14178",
          "author": "<a href=\"http://arxiv.org/find/eess/1/au:+Wang_Q/0/1/0/all/0/1\">Quanziang Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Wang_R/0/1/0/all/0/1\">Renzhen Wang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1\">Yuexiang Li</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Ma_K/0/1/0/all/0/1\">Kai Ma</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zheng_Y/0/1/0/all/0/1\">Yefeng Zheng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Meng_D/0/1/0/all/0/1\">Deyu Meng</a>",
          "description": "Location information is proven to benefit the deep learning models on\ncapturing the manifold structure of target objects, and accordingly boosts the\naccuracy of medical image segmentation. However, most existing methods encode\nthe location information in an implicit way, e.g. the distance transform maps,\nwhich describe the relative distance from each pixel to the contour boundary,\nfor the network to learn. These implicit approaches do not fully exploit the\nposition information (i.e. absolute location) of targets. In this paper, we\npropose a novel loss function, namely residual moment (RM) loss, to explicitly\nembed the location information of segmentation targets during the training of\ndeep learning networks. Particularly, motivated by image moments, the\nsegmentation prediction map and ground-truth map are weighted by coordinate\ninformation. Then our RM loss encourages the networks to maintain the\nconsistency between the two weighted maps, which promotes the segmentation\nnetworks to easily locate the targets and extract manifold-structure-related\nfeatures. We validate the proposed RM loss by conducting extensive experiments\non two publicly available datasets, i.e., 2D optic cup and disk segmentation\nand 3D left atrial segmentation. The experimental results demonstrate the\neffectiveness of our RM loss, which significantly boosts the accuracy of\nsegmentation networks.",
          "link": "http://arxiv.org/abs/2106.14178",
          "publishedOn": "2021-06-29T01:55:15.569Z",
          "wordCount": 648,
          "title": "Residual Moment Loss for Medical Image Segmentation. (arXiv:2106.14178v1 [eess.IV])"
        },
        {
          "id": "http://arxiv.org/abs/2002.05505",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Youngduck Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Youngnam Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1\">Junghyun Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baek_J/0/1/0/all/0/1\">Jineon Baek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_D/0/1/0/all/0/1\">Dongmin Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1\">Hangyeol Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shim_Y/0/1/0/all/0/1\">Yugeun Shim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seewoo Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1\">Jonghun Shin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bae_C/0/1/0/all/0/1\">Chan Bae</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1\">Byungsoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heo_J/0/1/0/all/0/1\">Jaewe Heo</a>",
          "description": "Like many other domains in Artificial Intelligence (AI), there are specific\ntasks in the field of AI in Education (AIEd) for which labels are scarce and\nexpensive, such as predicting exam score or review correctness. A common way of\ncircumventing label-scarce problems is pre-training a model to learn\nrepresentations of the contents of learning items. However, such methods fail\nto utilize the full range of student interaction data available and do not\nmodel student learning behavior. To this end, we propose Assessment Modeling, a\nclass of fundamental pre-training tasks for general interactive educational\nsystems. An assessment is a feature of student-system interactions which can\nserve as a pedagogical evaluation. Examples include the correctness and\ntimeliness of a student's answer. Assessment Modeling is the prediction of\nassessments conditioned on the surrounding context of interactions. Although it\nis natural to pre-train on interactive features available in large amounts,\nlimiting the prediction targets to assessments focuses the tasks' relevance to\nthe label-scarce educational problems and reduces less-relevant noise. While\nthe effectiveness of different combinations of assessments is open for\nexploration, we suggest Assessment Modeling as a first-order guiding principle\nfor selecting proper pre-training tasks for label-scarce educational problems.",
          "link": "http://arxiv.org/abs/2002.05505",
          "publishedOn": "2021-06-29T01:55:15.563Z",
          "wordCount": 717,
          "title": "Assessment Modeling: Fundamental Pre-training Tasks for Interactive Educational Systems. (arXiv:2002.05505v6 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14008",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhihua Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1\">Dingquan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1\">Kede Ma</a>",
          "description": "Ensemble methods are generally regarded to be better than a single model if\nthe base learners are deemed to be \"accurate\" and \"diverse.\" Here we\ninvestigate a semi-supervised ensemble learning strategy to produce\ngeneralizable blind image quality assessment models. We train a multi-head\nconvolutional network for quality prediction by maximizing the accuracy of the\nensemble (as well as the base learners) on labeled data, and the disagreement\n(i.e., diversity) among them on unlabeled data, both implemented by the\nfidelity loss. We conduct extensive experiments to demonstrate the advantages\nof employing unlabeled data for BIQA, especially in model generalization and\nfailure identification.",
          "link": "http://arxiv.org/abs/2106.14008",
          "publishedOn": "2021-06-29T01:55:15.557Z",
          "wordCount": 552,
          "title": "Semi-Supervised Deep Ensembles for Blind Image Quality Assessment. (arXiv:2106.14008v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2103.11037",
          "author": "<a href=\"http://arxiv.org/find/math/1/au:+Cai_H/0/1/0/all/0/1\">HanQin Cai</a>, <a href=\"http://arxiv.org/find/math/1/au:+Hamm_K/0/1/0/all/0/1\">Keaton Hamm</a>, <a href=\"http://arxiv.org/find/math/1/au:+Huang_L/0/1/0/all/0/1\">Longxiu Huang</a>, <a href=\"http://arxiv.org/find/math/1/au:+Needell_D/0/1/0/all/0/1\">Deanna Needell</a>",
          "description": "Low rank tensor approximation is a fundamental tool in modern machine\nlearning and data science. In this paper, we study the characterization,\nperturbation analysis, and an efficient sampling strategy for two primary\ntensor CUR approximations, namely Chidori and Fiber CUR. We characterize exact\ntensor CUR decompositions for low multilinear rank tensors. We also present\ntheoretical error bounds of the tensor CUR approximations when (adversarial or\nGaussian) noise appears. Moreover, we show that low cost uniform sampling is\nsufficient for tensor CUR approximations if the tensor has an incoherent\nstructure. Empirical performance evaluations, with both synthetic and\nreal-world datasets, establish the speed advantage of the tensor CUR\napproximations over other state-of-the-art low multilinear rank tensor\napproximations.",
          "link": "http://arxiv.org/abs/2103.11037",
          "publishedOn": "2021-06-29T01:55:15.540Z",
          "wordCount": 590,
          "title": "Mode-wise Tensor Decompositions: Multi-dimensional Generalizations of CUR Decompositions. (arXiv:2103.11037v2 [math.NA] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2105.07674",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Cossu_A/0/1/0/all/0/1\">Andrea Cossu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bacciu_D/0/1/0/all/0/1\">Davide Bacciu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carta_A/0/1/0/all/0/1\">Antonio Carta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gallicchio_C/0/1/0/all/0/1\">Claudio Gallicchio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lomonaco_V/0/1/0/all/0/1\">Vincenzo Lomonaco</a>",
          "description": "Continual Learning (CL) refers to a learning setup where data is non\nstationary and the model has to learn without forgetting existing knowledge.\nThe study of CL for sequential patterns revolves around trained recurrent\nnetworks. In this work, instead, we introduce CL in the context of Echo State\nNetworks (ESNs), where the recurrent component is kept fixed. We provide the\nfirst evaluation of catastrophic forgetting in ESNs and we highlight the\nbenefits in using CL strategies which are not applicable to trained recurrent\nmodels. Our results confirm the ESN as a promising model for CL and open to its\nuse in streaming scenarios.",
          "link": "http://arxiv.org/abs/2105.07674",
          "publishedOn": "2021-06-29T01:55:15.533Z",
          "wordCount": 572,
          "title": "Continual Learning with Echo State Networks. (arXiv:2105.07674v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2103.11956",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wolpert_D/0/1/0/all/0/1\">David H. Wolpert</a>",
          "description": "The important recent book by G. Schurz appreciates that the no-free-lunch\ntheorems (NFL) have major implications for the problem of (meta) induction.\nHere I review the NFL theorems, emphasizing that they do not only concern the\ncase where there is a uniform prior -- they prove that there are \"as many\npriors\" (loosely speaking) for which any induction algorithm $A$\nout-generalizes some induction algorithm $B$ as vice-versa. Importantly though,\nin addition to the NFL theorems, there are many \\textit{free lunch} theorems.\nIn particular, the NFL theorems can only be used to compare the\n\\textit{marginal} expected performance of an induction algorithm $A$ with the\nmarginal expected performance of an induction algorithm $B$. There is a rich\nset of free lunches which instead concern the statistical correlations among\nthe generalization errors of induction algorithms. As I describe, the\nmeta-induction algorithms that Schurz advocate as a \"solution to Hume's\nproblem\" are just an example of such a free lunch based on correlations among\nthe generalization errors of induction algorithms. I end by pointing out that\nthe prior that Schurz advocates, which is uniform over bit frequencies rather\nthan bit patterns, is contradicted by thousands of experiments in statistical\nphysics and by the great success of the maximum entropy procedure in inductive\ninference.",
          "link": "http://arxiv.org/abs/2103.11956",
          "publishedOn": "2021-06-29T01:55:15.528Z",
          "wordCount": 664,
          "title": "The Implications of the No-Free-Lunch Theorems for Meta-induction. (arXiv:2103.11956v2 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2006.07988",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Chien_E/0/1/0/all/0/1\">Eli Chien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1\">Jianhao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Pan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milenkovic_O/0/1/0/all/0/1\">Olgica Milenkovic</a>",
          "description": "In many important graph data processing applications the acquired information\nincludes both node features and observations of the graph topology. Graph\nneural networks (GNNs) are designed to exploit both sources of evidence but\nthey do not optimally trade-off their utility and integrate them in a manner\nthat is also universal. Here, universality refers to independence on homophily\nor heterophily graph assumptions. We address these issues by introducing a new\nGeneralized PageRank (GPR) GNN architecture that adaptively learns the GPR\nweights so as to jointly optimize node feature and topological information\nextraction, regardless of the extent to which the node labels are homophilic or\nheterophilic. Learned GPR weights automatically adjust to the node label\npattern, irrelevant on the type of initialization, and thereby guarantee\nexcellent learning performance for label patterns that are usually hard to\nhandle. Furthermore, they allow one to avoid feature over-smoothing, a process\nwhich renders feature information nondiscriminative, without requiring the\nnetwork to be shallow. Our accompanying theoretical analysis of the GPR-GNN\nmethod is facilitated by novel synthetic benchmark datasets generated by the\nso-called contextual stochastic block model. We also compare the performance of\nour GNN architecture with that of several state-of-the-art GNNs on the problem\nof node-classification, using well-known benchmark homophilic and heterophilic\ndatasets. The results demonstrate that GPR-GNN offers significant performance\nimprovement compared to existing techniques on both synthetic and benchmark\ndata.",
          "link": "http://arxiv.org/abs/2006.07988",
          "publishedOn": "2021-06-29T01:55:15.522Z",
          "wordCount": 716,
          "title": "Adaptive Universal Generalized PageRank Graph Neural Network. (arXiv:2006.07988v4 [cs.LG] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2104.12909",
          "author": "<a href=\"http://arxiv.org/find/econ/1/au:+Narita_Y/0/1/0/all/0/1\">Yusuke Narita</a>, <a href=\"http://arxiv.org/find/econ/1/au:+Yata_K/0/1/0/all/0/1\">Kohei Yata</a>",
          "description": "Algorithms produce a growing portion of decisions and recommendations both in\npolicy and business. Such algorithmic decisions are natural experiments\n(conditionally quasi-randomly assigned instruments) since the algorithms make\ndecisions based only on observable input variables. We use this observation to\ndevelop a treatment-effect estimator for a class of stochastic and\ndeterministic decision-making algorithms. Our estimator is shown to be\nconsistent and asymptotically normal for well-defined causal effects. A key\nspecial case of our estimator is a multidimensional regression discontinuity\ndesign. We apply our estimator to evaluate the effect of the Coronavirus Aid,\nRelief, and Economic Security (CARES) Act, where more than \\$175 billion worth\nof relief funding is allocated to hospitals via an algorithmic rule. Our\nestimates suggest that the relief funding has little effect on COVID-19-related\nhospital activity levels. Naive OLS and IV estimates exhibit substantial\nselection bias.",
          "link": "http://arxiv.org/abs/2104.12909",
          "publishedOn": "2021-06-29T01:55:15.515Z",
          "wordCount": 642,
          "title": "Algorithm is Experiment: Machine Learning, Market Design, and Policy Eligibility Rules. (arXiv:2104.12909v2 [econ.EM] UPDATED)"
        },
        {
          "id": "http://arxiv.org/abs/2106.14052",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Andresel_M/0/1/0/all/0/1\">Medina Andresel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Domokos_C/0/1/0/all/0/1\">Csaba Domokos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stepanova_D/0/1/0/all/0/1\">Daria Stepanova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1\">Trung-Kien Tran</a>",
          "description": "Recently, low-dimensional vector space representations of knowledge graphs\n(KGs) have been applied to find answers to conjunctive queries (CQs) over\nincomplete KGs. However, the current methods only focus on inductive reasoning,\ni.e. answering CQs by predicting facts based on patterns learned from the data,\nand lack the ability of deductive reasoning by applying external domain\nknowledge. Such (expert or commonsense) domain knowledge is an invaluable\nresource which can be used to advance machine intelligence. To address this\nshortcoming, we introduce a neural-symbolic method for ontology-mediated CQ\nanswering over incomplete KGs that operates in the embedding space. More\nspecifically, we propose various data augmentation strategies to generate\ntraining queries using query-rewriting based methods and then exploit a novel\nloss function for training the model. The experimental results demonstrate the\neffectiveness of our training strategies and the new loss function, i.e., our\nmethod significantly outperforms the baseline in the settings that require both\ninductive and deductive reasoning.",
          "link": "http://arxiv.org/abs/2106.14052",
          "publishedOn": "2021-06-29T01:55:15.493Z",
          "wordCount": 589,
          "title": "A Neural-symbolic Approach for Ontology-mediated Query Answering. (arXiv:2106.14052v1 [cs.AI])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14213",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Mansoor_M/0/1/0/all/0/1\">Muvazima Mansoor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chandar_S/0/1/0/all/0/1\">Srikanth Chandar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinath_R/0/1/0/all/0/1\">Ramamoorthy Srinath</a>",
          "description": "In this paper, we propose an architecture to solve a novel problem statement\nthat has stemmed more so in recent times with an increase in demand for virtual\ncontent delivery due to the COVID-19 pandemic. All educational institutions,\nworkplaces, research centers, etc. are trying to bridge the gap of\ncommunication during these socially distanced times with the use of online\ncontent delivery. The trend now is to create presentations, and then\nsubsequently deliver the same using various virtual meeting platforms. The time\nbeing spent in such creation of presentations and delivering is what we try to\nreduce and eliminate through this paper which aims to use Machine Learning (ML)\nalgorithms and Natural Language Processing (NLP) modules to automate the\nprocess of creating a slides-based presentation from a document, and then use\nstate-of-the-art voice cloning models to deliver the content in the desired\nauthor's voice. We consider a structured document such as a research paper to\nbe the content that has to be presented. The research paper is first summarized\nusing BERT summarization techniques and condensed into bullet points that go\ninto the slides. Tacotron inspired architecture with Encoder, Synthesizer, and\na Generative Adversarial Network (GAN) based vocoder, is used to convey the\ncontents of the slides in the author's voice (or any customized voice). Almost\nall learning has now been shifted to online mode, and professionals are now\nworking from the comfort of their homes. Due to the current situation, teachers\nand professionals have shifted to presentations to help them in imparting\ninformation. In this paper, we aim to reduce the considerable amount of time\nthat is taken in creating a presentation by automating this process and\nsubsequently delivering this presentation in a customized voice, using a\ncontent delivery mechanism that can clone any voice using a short audio clip.",
          "link": "http://arxiv.org/abs/2106.14213",
          "publishedOn": "2021-06-29T01:55:15.485Z",
          "wordCount": 783,
          "title": "AI based Presentation Creator With Customized Audio Content Delivery. (arXiv:2106.14213v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13913",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1\">Hongyu Guo</a>",
          "description": "Label Smoothing (LS) improves model generalization through penalizing models\nfrom generating overconfident output distributions. For each training sample\nthe LS strategy smooths the one-hot encoded training signal by distributing its\ndistribution mass over the non-ground truth classes. We extend this technique\nby considering example pairs, coined PLS. PLS first creates midpoint samples by\naveraging random sample pairs and then learns a smoothing distribution during\ntraining for each of these midpoint samples, resulting in midpoints with high\nuncertainty labels for training. We empirically show that PLS significantly\noutperforms LS, achieving up to 30% of relative classification error reduction.\nWe also visualize that PLS produces very low winning softmax scores for both in\nand out of distribution samples.",
          "link": "http://arxiv.org/abs/2106.13913",
          "publishedOn": "2021-06-29T01:55:15.473Z",
          "wordCount": 566,
          "title": "Midpoint Regularization: from High Uncertainty Training to Conservative Classification. (arXiv:2106.13913v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14102",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Kalgaonkar_P/0/1/0/all/0/1\">Priyank Kalgaonkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+El_Sharkawy_M/0/1/0/all/0/1\">Mohamed El-Sharkawy</a>",
          "description": "In this paper, we demonstrate the implementation of our ultra-efficient deep\nconvolutional neural network architecture: CondenseNeXt on NXP BlueBox, an\nautonomous driving development platform developed for self-driving vehicles. We\nshow that CondenseNeXt is remarkably efficient in terms of FLOPs, designed for\nARM-based embedded computing platforms with limited computational resources and\ncan perform image classification without the need of a CUDA enabled GPU.\nCondenseNeXt utilizes the state-of-the-art depthwise separable convolution and\nmodel compression techniques to achieve a remarkable computational efficiency.\nExtensive analyses are conducted on CIFAR-10, CIFAR-100 and ImageNet datasets\nto verify the performance of CondenseNeXt Convolutional Neural Network (CNN)\narchitecture. It achieves state-of-the-art image classification performance on\nthree benchmark datasets including CIFAR-10 (4.79% top-1 error), CIFAR-100\n(21.98% top-1 error) and ImageNet (7.91% single model, single crop top-5\nerror). CondenseNeXt achieves final trained model size improvement of 2.9+ MB\nand up to 59.98% reduction in forward FLOPs compared to CondenseNet and can\nperform image classification on ARM-Based computing platforms without needing a\nCUDA enabled GPU support, with outstanding efficiency.",
          "link": "http://arxiv.org/abs/2106.14102",
          "publishedOn": "2021-06-29T01:55:15.467Z",
          "wordCount": 620,
          "title": "Image Classification with CondenseNeXt for ARM-Based Computing Platforms. (arXiv:2106.14102v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14131",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Valipour_M/0/1/0/all/0/1\">Mojtaba Valipour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+You_B/0/1/0/all/0/1\">Bowen You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panju_M/0/1/0/all/0/1\">Maysum Panju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghodsi_A/0/1/0/all/0/1\">Ali Ghodsi</a>",
          "description": "Symbolic regression is the task of identifying a mathematical expression that\nbest fits a provided dataset of input and output values. Due to the richness of\nthe space of mathematical expressions, symbolic regression is generally a\nchallenging problem. While conventional approaches based on genetic evolution\nalgorithms have been used for decades, deep learning-based methods are\nrelatively new and an active research area. In this work, we present\nSymbolicGPT, a novel transformer-based language model for symbolic regression.\nThis model exploits the advantages of probabilistic language models like GPT,\nincluding strength in performance and flexibility. Through comprehensive\nexperiments, we show that our model performs strongly compared to competing\nmodels with respect to the accuracy, running time, and data efficiency.",
          "link": "http://arxiv.org/abs/2106.14131",
          "publishedOn": "2021-06-29T01:55:15.451Z",
          "wordCount": 560,
          "title": "SymbolicGPT: A Generative Transformer Model for Symbolic Regression. (arXiv:2106.14131v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14238",
          "author": "<a href=\"http://arxiv.org/find/stat/1/au:+Wilson_J/0/1/0/all/0/1\">James D. Wilson</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Lee_J/0/1/0/all/0/1\">Jihui Lee</a>",
          "description": "We consider the problem of interpretable network representation learning for\nsamples of network-valued data. We propose the Principal Component Analysis for\nNetworks (PCAN) algorithm to identify statistically meaningful low-dimensional\nrepresentations of a network sample via subgraph count statistics. The PCAN\nprocedure provides an interpretable framework for which one can readily\nvisualize, explore, and formulate predictive models for network samples. We\nfurthermore introduce a fast sampling-based algorithm, sPCAN, which is\nsignificantly more computationally efficient than its counterpart, but still\nenjoys advantages of interpretability. We investigate the relationship between\nthese two methods and analyze their large-sample properties under the common\nregime where the sample of networks is a collection of kernel-based random\ngraphs. We show that under this regime, the embeddings of the sPCAN method\nenjoy a central limit theorem and moreover that the population level embeddings\nof PCAN and sPCAN are equivalent. We assess PCAN's ability to visualize,\ncluster, and classify observations in network samples arising in nature,\nincluding functional connectivity network samples and dynamic networks\ndescribing the political co-voting habits of the U.S. Senate. Our analyses\nreveal that our proposed algorithm provides informative and discriminatory\nfeatures describing the networks in each sample. The PCAN and sPCAN methods\nbuild on the current literature of network representation learning and set the\nstage for a new line of research in interpretable learning on network-valued\ndata. Publicly available software for the PCAN and sPCAN methods are available\nat https://www.github.com/jihuilee/.",
          "link": "http://arxiv.org/abs/2106.14238",
          "publishedOn": "2021-06-29T01:55:15.445Z",
          "wordCount": 678,
          "title": "Interpretable Network Representation Learning with Principal Component Analysis. (arXiv:2106.14238v1 [stat.ML])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14174",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Rahmani_S/0/1/0/all/0/1\">Sana Rahmani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hosseini_S/0/1/0/all/0/1\">Saeid Hosseini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zall_R/0/1/0/all/0/1\">Raziyeh Zall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kangavari_M/0/1/0/all/0/1\">Mohammad Reza Kangavari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kamran_S/0/1/0/all/0/1\">Sara Kamran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_W/0/1/0/all/0/1\">Wen Hua</a>",
          "description": "Multimodal sentiment analysis benefits various applications such as\nhuman-computer interaction and recommendation systems. It aims to infer the\nusers' bipolar ideas using visual, textual, and acoustic signals. Although\nresearchers affirm the association between cognitive cues and emotional\nmanifestations, most of the current multimodal approaches in sentiment analysis\ndisregard user-specific aspects. To tackle this issue, we devise a novel method\nto perform multimodal sentiment prediction using cognitive cues, such as\npersonality. Our framework constructs an adaptive tree by hierarchically\ndividing users and trains the LSTM-based submodels, utilizing an\nattention-based fusion to transfer cognitive-oriented knowledge within the\ntree. Subsequently, the framework consumes the conclusive agglomerative\nknowledge from the adaptive tree to predict final sentiments. We also devise a\ndynamic dropout method to facilitate data sharing between neighboring nodes,\nreducing data sparsity. The empirical results on real-world datasets determine\nthat our proposed model for sentiment prediction can surpass trending rivals.\nMoreover, compared to other ensemble approaches, the proposed transfer-based\nalgorithm can better utilize the latent cognitive cues and foster the\nprediction outcomes. Based on the given extrinsic and intrinsic analysis\nresults, we note that compared to other theoretical-based techniques, the\nproposed hierarchical clustering approach can better group the users within the\nadaptive tree.",
          "link": "http://arxiv.org/abs/2106.14174",
          "publishedOn": "2021-06-29T01:55:15.437Z",
          "wordCount": 664,
          "title": "Transfer-based adaptive tree for multimodal sentiment analysis based on user latent aspects. (arXiv:2106.14174v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13881",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Baptista_A/0/1/0/all/0/1\">Andr&#xe9; Baptista</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baghoussi_Y/0/1/0/all/0/1\">Yassine Baghoussi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soares_C/0/1/0/all/0/1\">Carlos Soares</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mendes_Moreira_J/0/1/0/all/0/1\">Jo&#xe3;o Mendes-Moreira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arantes_M/0/1/0/all/0/1\">Miguel Arantes</a>",
          "description": "Forecasting accuracy is reliant on the quality of available past data. Data\ndisruptions can adversely affect the quality of the generated model (e.g.\nunexpected events such as out-of-stock products when forecasting demand). We\naddress this problem by pastcasting: predicting how data should have been in\nthe past to explain the future better. We propose Pastprop-LSTM, a data-centric\nbackpropagation algorithm that assigns part of the responsibility for errors to\nthe training data and changes it accordingly. We test three variants of\nPastprop-LSTM on forecasting competition datasets, M4 and M5, plus the Numenta\nAnomaly Benchmark. Empirical evaluation indicates that the proposed method can\nimprove forecasting accuracy, especially when the prediction errors of standard\nLSTM are high. It also demonstrates the potential of the algorithm on datasets\ncontaining anomalies.",
          "link": "http://arxiv.org/abs/2106.13881",
          "publishedOn": "2021-06-29T01:55:15.430Z",
          "wordCount": 561,
          "title": "Pastprop-RNN: improved predictions of the future by correcting the past. (arXiv:2106.13881v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.14148",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Alimi_R/0/1/0/all/0/1\">Roger Alimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ivry_A/0/1/0/all/0/1\">Amir Ivry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fisher_E/0/1/0/all/0/1\">Elad Fisher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weiss_E/0/1/0/all/0/1\">Eyal Weiss</a>",
          "description": "Modern magnetic sensor arrays conventionally utilize state of the art low\npower magnetometers such as parallel and orthogonal fluxgates. Low power\nfluxgates tend to have large Barkhausen jumps that appear as a dc jump in the\nfluxgate output. This phenomenon deteriorates the signal fidelity and\neffectively increases the internal sensor noise. Even if sensors that are more\nprone to dc jumps can be screened during production, the conventional noise\nmeasurement does not always catch the dc jump because of its sparsity.\nMoreover, dc jumps persist in almost all the sensor cores although at a slower\nbut still intolerable rate. Even if dc jumps can be easily detected in a\nshielded environment, when deployed in presence of natural noise and clutter,\nit can be hard to positively detect them. This work fills this gap and presents\nalgorithms that distinguish dc jumps embedded in natural magnetic field data.\nTo improve robustness to noise, we developed two machine learning algorithms\nthat employ temporal and statistical physical-based features of a pre-acquired\nand well-known experimental data set. The first algorithm employs a support\nvector machine classifier, while the second is based on a neural network\narchitecture. We compare these new approaches to a more classical kernel-based\nmethod. To that purpose, the receiver operating characteristic curve is\ngenerated, which allows diagnosis ability of the different classifiers by\ncomparing their performances across various operation points. The accuracy of\nthe machine learning-based algorithms over the classic method is highly\nemphasized. In addition, high generalization and robustness of the neural\nnetwork can be concluded, based on the rapid convergence of the corresponding\nreceiver operating characteristic curves.",
          "link": "http://arxiv.org/abs/2106.14148",
          "publishedOn": "2021-06-29T01:55:15.424Z",
          "wordCount": 733,
          "title": "Machine Learning Detection Algorithm for Large Barkhausen Jumps in Cluttered Environment. (arXiv:2106.14148v1 [cs.LG])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13952",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Di Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_B/0/1/0/all/0/1\">Bo Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Liangpei Zhang</a>",
          "description": "In this paper, we propose a spectral-spatial graph reasoning network (SSGRN)\nfor hyperspectral image (HSI) classification. Concretely, this network contains\ntwo parts that separately named spatial graph reasoning subnetwork (SAGRN) and\nspectral graph reasoning subnetwork (SEGRN) to capture the spatial and spectral\ngraph contexts, respectively. Different from the previous approaches\nimplementing superpixel segmentation on the original image or attempting to\nobtain the category features under the guide of label image, we perform the\nsuperpixel segmentation on intermediate features of the network to adaptively\nproduce the homogeneous regions to get the effective descriptors. Then, we\nadopt a similar idea in spectral part that reasonably aggregating the channels\nto generate spectral descriptors for spectral graph contexts capturing. All\ngraph reasoning procedures in SAGRN and SEGRN are achieved through graph\nconvolution. To guarantee the global perception ability of the proposed\nmethods, all adjacent matrices in graph reasoning are obtained with the help of\nnon-local self-attention mechanism. At last, by combining the extracted spatial\nand spectral graph contexts, we obtain the SSGRN to achieve a high accuracy\nclassification. Extensive quantitative and qualitative experiments on three\npublic HSI benchmarks demonstrate the competitiveness of the proposed methods\ncompared with other state-of-the-art approaches.",
          "link": "http://arxiv.org/abs/2106.13952",
          "publishedOn": "2021-06-29T01:55:15.400Z",
          "wordCount": 641,
          "title": "Spectral-Spatial Graph Reasoning Network for Hyperspectral Image Classification. (arXiv:2106.13952v1 [cs.CV])"
        },
        {
          "id": "http://arxiv.org/abs/2106.13911",
          "author": "<a href=\"http://arxiv.org/find/cs/1/au:+Ovalle_A/0/1/0/all/0/1\">Alvaro Ovalle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lucas_S/0/1/0/all/0/1\">Simon M. Lucas</a>",
          "description": "A large part of the interest in model-based reinforcement learning derives\nfrom the potential utility to acquire a forward model capable of strategic long\nterm decision making. Assuming that an agent succeeds in learning a useful\npredictive model, it still requires a mechanism to harness it to generate and\nselect among competing simulated plans. In this paper, we explore this theme\ncombining evolutionary algorithmic planning techniques with models learned via\ndeep learning and variational inference. We demonstrate the approach with an\nagent that reliably performs online planning in a set of visual navigation\ntasks.",
          "link": "http://arxiv.org/abs/2106.13911",
          "publishedOn": "2021-06-29T01:55:15.394Z",
          "wordCount": 548,
          "title": "Predictive Control Using Learned State Space Models via Rolling Horizon Evolution. (arXiv:2106.13911v1 [cs.LG])"
        }
      ]
    }
  ],
  "cliVersion": "1.11.0"
}